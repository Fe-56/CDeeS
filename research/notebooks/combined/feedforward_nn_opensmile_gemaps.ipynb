{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combined Dataset - Feed Forward Neural Network\n",
    "## openSMILE GeMAPS Featureset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import relevant libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torcheval.metrics import R2Score, MeanSquaredError\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import math\n",
    "\n",
    "import sys\n",
    "sys.path.insert(1, '../../utils')\n",
    "from paths import *\n",
    "\n",
    "sys.path.insert(1, '../../models')\n",
    "from feedforward_nn_combined import NeuralNetworkCombined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change featureset here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureset = NORMALISED_OPENSMILE_GEMAPS_FEATURES_CSV\n",
    "featureset_path = f'{COMBINED_EXTRACTED_FEATURES_FOLDER}/scaled/{featureset}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import annotations dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_annotations = pd.read_csv(f'{COMBINED_STATIC_ANNOTATIONS_CSV}')\n",
    "df_annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the featureset\n",
    "\n",
    "This is where you should change between normalised and standardised, and untouched featuresets!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_opensmile_gemaps = pd.read_csv(featureset_path)\n",
    "\n",
    "df_opensmile_gemaps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare dataframes for the neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform splitting of the dataframe into training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df_opensmile_gemaps.drop('song_id', axis=1)\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = df_annotations.drop('song_id', axis=1)\n",
    "targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform 80-20 train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## FIXME: Rewrote the dataset as a PyTorch Dataset Class\n",
    "class MusicEmoDataset(Dataset):\n",
    "\n",
    "    def __init__(self, df_features, df_targets, train=True):\n",
    "        self.features = df_features\n",
    "        self.targets = df_targets\n",
    "\n",
    "        # Train-Test Split (80/20) consisting of features and targets dataset\n",
    "        X_train, X_test, y_train, y_test = train_test_split(self.features, self.targets, test_size=0.2, random_state=42)\n",
    "\n",
    "        if train:\n",
    "            self.X, self.y = torch.tensor(X_train.values, dtype=torch.float), torch.tensor(y_train.values, dtype=torch.float)\n",
    "        else:\n",
    "            self.X, self.y = torch.tensor(X_test.values, dtype=torch.float), torch.tensor(y_test.values, dtype=torch.float)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        if len(self.X) == len(self.y):\n",
    "            return len(self.X)\n",
    "        else:\n",
    "            raise Exception(\"Size of Features and Targets do not match.\")        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## FIXME: Instantiate DEAM datasets for Train and Test split\n",
    "MusicEmo_train = MusicEmoDataset(features, targets, train=True)\n",
    "MusicEmo_test = MusicEmoDataset(features, targets, train=False)\n",
    "print(len(MusicEmo_train))\n",
    "print(len(MusicEmo_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## FIXME: Setup Dataloader for both splits\n",
    "train_dataloader = DataLoader(MusicEmo_train, batch_size=64, shuffle=True) # batch_size=len(MusicEmo_train)\n",
    "test_dataloader = DataLoader(MusicEmo_test, batch_size=32) # batch_size=len(MusicEmo_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(test_dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define neural network parameters and instantitate neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## FIXME: What is the purpose of defining this parameters if most of them are not being used? Removed Hidden size as a variable as it is implicitly defined via input_size\n",
    "\n",
    "input_size = features.shape[1]\n",
    "output_size = targets.shape[1]  # Output size for valence and arousal\n",
    "num_epochs = 2000\n",
    "\n",
    "print(input_size, output_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set a random seed to ensure consistent initial weights of the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set the seed\n",
    "# seed = 42\n",
    "# torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_loop(model, train_dataloader, optimiser, criterion, device):\n",
    "    # Training Phase\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_count = 0\n",
    "    \n",
    "    for batch in train_dataloader:\n",
    "        # Zero gradients\n",
    "        optimiser.zero_grad()\n",
    "\n",
    "        # Unpack batch\n",
    "        inputs, targets = batch\n",
    "        inputs_re = inputs.to(device)\n",
    "        outputs_re = targets.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        preds = model(inputs_re)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = criterion(preds, outputs_re)\n",
    "        \n",
    "        # Add to total losses\n",
    "        train_loss += loss.item() * outputs_re.shape[0]\n",
    "        train_count += outputs_re.shape[0]\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Update weights\n",
    "        optimiser.step()\n",
    "    \n",
    "    # Compute total training loss (RMSE)\n",
    "    train_loss /= train_count\n",
    "    train_rmse = math.sqrt(train_loss)\n",
    "\n",
    "    return train_rmse # Need to check whether to return optimiser, model and criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_one_loop(model, test_dataloader, device=\"cpu\"):\n",
    "    # Evaluation Phase\n",
    "    model.eval()\n",
    "\n",
    "    full_preds = []\n",
    "    full_outputs = []\n",
    "    num_features = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in test_dataloader:\n",
    "            # Unpack batch\n",
    "            inputs, targets = batch\n",
    "            inputs_re = inputs.to(device)\n",
    "            outputs_re = targets.to(device)\n",
    "\n",
    "            # Capture number of features\n",
    "            num_features = inputs.shape[1]\n",
    "\n",
    "            # Forward pass\n",
    "            preds = model(inputs_re)\n",
    "            \n",
    "            # Append outputs together\n",
    "            full_preds.append(preds)\n",
    "            full_outputs.append(outputs_re)\n",
    "\n",
    "    # Combine into a single tensor for preds and outputs\n",
    "    preds_tensor = torch.cat(full_preds, 0)\n",
    "    outputs_tensor = torch.cat(full_outputs, 0)\n",
    "    \n",
    "    # Calculate Mean Squared Error\n",
    "    mse_metric = MeanSquaredError(multioutput=\"raw_values\")\n",
    "    mse_metric.update(preds_tensor, outputs_tensor)\n",
    "    mse = mse_metric.compute()\n",
    "    print(\"Test MSE Metric:\", mse)\n",
    "\n",
    "    average_rmse = torch.sqrt(torch.mean(mse))\n",
    "    print(\"Test RMSE:\", average_rmse.item())\n",
    "\n",
    "    valence_rmse = torch.sqrt(mse[0])\n",
    "    print(\"Valence RMSE:\", valence_rmse.item())\n",
    "\n",
    "    arousal_rmse = torch.sqrt(mse[1])\n",
    "    print(\"Arousal RMSE:\", arousal_rmse.item())\n",
    "\n",
    "    # Calculate R^2 Score\n",
    "    r2_metric = R2Score(multioutput=\"raw_values\") # can be adjusted using multioutput and num_regressors (adjusted r^2 score)\n",
    "    r2_metric.update(preds_tensor, outputs_tensor)\n",
    "    r2_score = r2_metric.compute()\n",
    "    print(f\"Test R^2 Score (Valence, Arousal): {r2_score}\")\n",
    "\n",
    "    combined_r2_score = torch.mean(r2_score)\n",
    "    print(f\"Test R^2 Score (combined): {combined_r2_score.item():.4f}\")\n",
    "    \n",
    "    # Calculate Adjusted R^2 Score\n",
    "    r2_metric = R2Score(multioutput=\"raw_values\", num_regressors=num_features) # can be adjusted using multioutput and num_regressors (adjusted r^2 score)\n",
    "    r2_metric.update(preds_tensor, outputs_tensor)\n",
    "    adjusted_r2_score = r2_metric.compute()\n",
    "    print(f\"Adjusted Test R^2 Score (Valence, Arousal): {adjusted_r2_score}\")\n",
    "\n",
    "    valence_adj_r2_score = adjusted_r2_score[0]\n",
    "    arousal_adj_r2_score = adjusted_r2_score[1]\n",
    "\n",
    "    combined_adj_r2_score = torch.mean(adjusted_r2_score)\n",
    "    print(f\"Adjusted Test R^2 Score (combined): {combined_adj_r2_score.item():.4f}\")\n",
    "\n",
    "    return average_rmse.item(), combined_adj_r2_score.item(), valence_adj_r2_score.item(), arousal_adj_r2_score.item(), preds_tensor, outputs_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model_kwargs, train_dataloader, test_dataloader, num_epochs, criterion=None, optimiser=None, device=\"cpu\"):\n",
    "    # Set the seed\n",
    "    torch.manual_seed(seed=42)\n",
    "\n",
    "    model = NeuralNetworkCombined(**model_kwargs)\n",
    "\n",
    "    if optimiser is None:\n",
    "        optimiser = torch.optim.Adam(model.parameters(), lr=1e-3, betas=(0.9, 0.999), eps=1e-8)\n",
    "    \n",
    "    if criterion is None:\n",
    "        criterion = nn.MSELoss()\n",
    "    \n",
    "    rmse_list = []\n",
    "    r2_scores_list = []\n",
    "    adjusted_r2_scores_valence_list = []\n",
    "    adjusted_r2_scores_arousal_list = []\n",
    "\n",
    "    for epoch in range(num_epochs):    \n",
    "        train_rmse = train_one_loop(model, train_dataloader, optimiser, criterion, device)\n",
    "        test_rmse, adj_r2_score, valence_adj_r2_score, arousal_adj_r2_score, predictions, targets = test_one_loop(model, test_dataloader, device)\n",
    "        \n",
    "        rmse_list.append(test_rmse)\n",
    "        r2_scores_list.append(adj_r2_score)\n",
    "        adjusted_r2_scores_valence_list.append(valence_adj_r2_score)\n",
    "        adjusted_r2_scores_arousal_list.append(arousal_adj_r2_score)\n",
    "\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}, Training Loss (RMSE): {train_rmse:.4f}, Test Loss (RMSE): {test_rmse:.4f}, Adjusted Test R^2 Score: {adj_r2_score:.4f}', end=\"\\n\\n\")\n",
    "\n",
    "    print(\"Training completed.\")\n",
    "    return model, rmse_list, r2_scores_list, adjusted_r2_scores_valence_list, adjusted_r2_scores_arousal_list, predictions, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict = {\"input_size\": input_size, \"output_size\":output_size, \"dropout_prob\":0.01}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model, rmse_list, r2_scores_list, adjusted_r2_scores_valence_list, adjusted_r2_scores_arousal_list, predictions, targets = train_model(model_dict, train_dataloader, test_dataloader, num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_rmse, adj_r2_score, valence_adj_r2_score, arousal_adj_r2_score, preds_tensor, outputs_tensor = test_one_loop(trained_model, test_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse relationship between epochs and r^2 score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the graph to visualise the relationship the evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs_list = [i for i in range(num_epochs)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RMSE vs. num_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(num_epochs_list, rmse_list, color='b', linestyle='-')\n",
    "plt.title('num_epochs vs. Test RMSE')\n",
    "plt.xlabel('num_epochs')\n",
    "plt.ylabel('Tets RMSE') \n",
    "plt.grid(True)\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_rmse = min(rmse_list)\n",
    "corresponding_r2_score = r2_scores_list[rmse_list.index(min_rmse)]\n",
    "corresponding_num_epochs = num_epochs_list[rmse_list.index(min_rmse)] + 1 \n",
    "\n",
    "print(f'Min RMSE score: {min_rmse}')\n",
    "print(f'Corresponding R^2 SCore: {corresponding_r2_score}')\n",
    "print(f'Corresponding num_epochs: {corresponding_num_epochs}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Adjusted R^2 Score vs. num_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(num_epochs_list, r2_scores_list, color='b', linestyle='-')\n",
    "plt.title('num_epochs vs. Test Adjusted R^2 Score')\n",
    "plt.xlabel('num_epochs')\n",
    "plt.ylabel('Test R^2 SCore') \n",
    "plt.grid(True)\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_r2_score = max(r2_scores_list)\n",
    "corresponding_rmse = rmse_list[r2_scores_list.index(max_r2_score)]\n",
    "corresponding_num_epochs = num_epochs_list[r2_scores_list.index(max_r2_score)] + 1\n",
    "optimal_num_epocs = corresponding_num_epochs\n",
    "\n",
    "print(f'Max R^2 score: {max_r2_score}')\n",
    "print(f'Corresponding RMSE: {corresponding_rmse}')\n",
    "print(f'Corresponding num_epochs: {corresponding_num_epochs}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Adjusted R^2 Score (Valence) vs. num_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(num_epochs_list, adjusted_r2_scores_valence_list, color='b', linestyle='-')\n",
    "plt.title('num_epochs vs. Test Adjusted R^2 Score (Valence)')\n",
    "plt.xlabel('num_epochs')\n",
    "plt.ylabel('Test Adjusted R^2 Score (Valence)') \n",
    "plt.grid(True)\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_r2_score_valence = max(adjusted_r2_scores_valence_list)\n",
    "corresponding_rmse = rmse_list[adjusted_r2_scores_valence_list.index(max_r2_score_valence)]\n",
    "corresponding_num_epochs = num_epochs_list[adjusted_r2_scores_valence_list.index(max_r2_score_valence)] + 1\n",
    "\n",
    "print(f'Max R^2 score: {max_r2_score_valence}')\n",
    "print(f'Corresponding RMSE: {corresponding_rmse}')\n",
    "print(f'Corresponding num_epochs: {corresponding_num_epochs}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Adjusted R^2 Score (Arousal) vs. num_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(num_epochs_list, adjusted_r2_scores_arousal_list, color='b', linestyle='-')\n",
    "plt.title('num_epochs vs. Test Adjusted R^2 Score (Arousal)')\n",
    "plt.xlabel('num_epochs')\n",
    "plt.ylabel('Test Adjusted R^2 SCore (Arousal)') \n",
    "plt.grid(True)\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_r2_score_arousal = max(adjusted_r2_scores_arousal_list)\n",
    "corresponding_rmse = rmse_list[adjusted_r2_scores_arousal_list.index(max_r2_score_arousal)]\n",
    "corresponding_num_epochs = num_epochs_list[adjusted_r2_scores_arousal_list.index(max_r2_score_arousal)] + 1\n",
    "\n",
    "print(f'Max R^2 score: {max_r2_score_arousal}')\n",
    "print(f'Corresponding RMSE: {corresponding_rmse}')\n",
    "print(f'Corresponding num_epochs: {corresponding_num_epochs}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the model with the highest test adjusted R^2 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model, rmse_list, r2_scores_list, adjusted_r2_scores_valence_list, adjusted_r2_scores_arousal_list, predictions, targets = train_model(model_dict, train_dataloader, test_dataloader, optimal_num_epocs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the predicted values vs true values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure and axis\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "# Plot the tensors from the predictions in orange\n",
    "for tensor in predictions:\n",
    "    predictions_scatter = ax.scatter(tensor[0], tensor[1], color='orange')\n",
    "\n",
    "# Plot the tensors from the targets in green\n",
    "for tensor in targets:\n",
    "    targets_scatter = ax.scatter(tensor[0], tensor[1], color='green')\n",
    "\n",
    "# Set the axis labels and title\n",
    "ax.set_xlabel('Valence')\n",
    "ax.set_ylabel('Arousal')\n",
    "ax.set_title('Scatter Plot of Predictions vs. Targets')\n",
    "\n",
    "# set the legend\n",
    "legend_elements = [\n",
    "    predictions_scatter,\n",
    "    targets_scatter\n",
    "]\n",
    "ax.legend(legend_elements, ['Predictions', 'True Values'], loc='upper left')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(trained_model.state_dict(), f'../../models/opensmile_gemaps_normalised/combined_feedforward_nn_opensmile_gemaps_normalised.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
