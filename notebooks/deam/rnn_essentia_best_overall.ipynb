{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEAM Dataset - CNN with Inception-GRU Residual Structure\n",
    "## Essentia Best Overall Featureset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import relevant libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "# from torchaudio.models.wav2vec2 import Wav2Vec2Model\n",
    "from transformers import Wav2Vec2Model\n",
    "from torcheval.metrics import R2Score\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import math\n",
    "\n",
    "import sys\n",
    "sys.path.insert(1, '../../utils')\n",
    "from paths import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the annotations dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = pd.read_csv(get_deam_path('processed/annotations/deam_static_annotations.csv'))\n",
    "targets = targets.drop('song_id', axis=1)\n",
    "targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the song_id of all .mp3 files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_path = get_deam_path('MEMD_audio')\n",
    "song_ids_temp = []\n",
    "\n",
    "# Iterate through all files in the directory\n",
    "for filename in os.listdir(audio_path):\n",
    "    # Check if the path is a file (not a subdirectory)\n",
    "    if os.path.isfile(os.path.join(audio_path, filename)):\n",
    "        song_ids_temp.append(int(filename[:-4]))\n",
    "\n",
    "song_ids_temp.sort()\n",
    "song_ids = []\n",
    "\n",
    "# remove all song_ids from 2015 (song_id 2001 onwards)\n",
    "for song_id in song_ids_temp:\n",
    "    if song_id <= 2000:\n",
    "        song_ids.append(song_id)\n",
    "\n",
    "print(song_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform 80-20 train-test split and create tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(song_ids, targets, test_size=0.2, random_state=42)\n",
    "\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float64)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float64)\n",
    "\n",
    "target_train_labels = y_train_tensor\n",
    "target_test_labels = y_test_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DEAMDataset(Dataset):\n",
    "    def __init__(self, song_ids):\n",
    "        self.song_ids = song_ids\n",
    "        self.mp3s = [f'{get_deam_path(\"MEMD_audio\")}/{song_id}.mp3' for song_id in self.song_ids]\n",
    "\n",
    "        waveforms = []\n",
    "        \n",
    "        for mp3 in self.mp3s:\n",
    "            print(f'Sampling {mp3}')\n",
    "            waveform, sample_rate = torchaudio.load(mp3, format='mp3')\n",
    "            waveforms.append(waveform)\n",
    "\n",
    "        self.waveform_lengths = [waveform.shape[-1] for waveform in waveforms]\n",
    "        self.max_waveform_length = max(self.waveform_lengths)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.song_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        mp3_file = self.mp3s[idx]\n",
    "        waveform, sample_rate = torchaudio.load(mp3_file, format='mp3')\n",
    "        waveform = F.pad(waveform, (0, self.max_waveform_length - waveform.size(-1)), mode='constant')\n",
    "        return waveform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InceptionBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(InceptionBlock, self).__init__()\n",
    "        self.branch1 = nn.Conv1d(in_channels, out_channels // 4, kernel_size=1)\n",
    "        self.branch2 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels, out_channels // 4, kernel_size=1),\n",
    "            nn.Conv1d(out_channels // 4, out_channels // 4, kernel_size=3, padding=1)\n",
    "        )\n",
    "        self.branch3 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels, out_channels // 4, kernel_size=1),\n",
    "            nn.Conv1d(out_channels // 4, out_channels // 4, kernel_size=5, padding=2)\n",
    "        )\n",
    "        self.branch4 = nn.MaxPool1d(kernel_size=3, stride=1, padding=1)\n",
    "        self.branch4_conv = nn.Conv1d(in_channels, out_channels // 4, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        branch1 = self.branch1(x)\n",
    "        branch2 = self.branch2(x)\n",
    "        branch3 = self.branch3(x)\n",
    "        branch4 = self.branch4_conv(self.branch4(x))\n",
    "        return torch.cat([branch1, branch2, branch3, branch4], dim=1)\n",
    "\n",
    "class MusicEmotionRegressionModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(MusicEmotionRegressionModel, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(input_size, 64, kernel_size=3, padding=1)\n",
    "        self.inception1 = InceptionBlock(64, 128)\n",
    "        self.inception2 = InceptionBlock(128, 256)\n",
    "        self.gru = nn.GRU(256, hidden_size, num_layers, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(2 * hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = x.unsqueeze(1)  # Add channel dimension\n",
    "        # x = x.squeeze(1)  # Squeeze the channel dimension\n",
    "        x = self.conv1(x)\n",
    "        x = self.inception1(x)\n",
    "        x = self.inception2(x)\n",
    "        x = x.permute(0, 2, 1)  # Reshape for GRU input\n",
    "        _, hidden = self.gru(x)\n",
    "        hidden = torch.cat((hidden[-2, :, :], hidden[-1, :, :]), dim=1)\n",
    "        output = self.fc(hidden)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 2  # Single-channel audio input\n",
    "hidden_size = 128\n",
    "num_layers = 2\n",
    "output_size = 2  # Valence and arousal regression targets\n",
    "learning_rate = 0.001\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set a random seed to ensure consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the seed\n",
    "seed = 42\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantitate the dataset and dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare your dataset and dataloader\n",
    "dataset = DEAMDataset(X_train)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(num_epochs):\n",
    "  model = MusicEmotionRegressionModel(input_size, hidden_size, num_layers, output_size)\n",
    "  optimiser = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "  \n",
    "  for epoch in range(num_epochs):\n",
    "    for waveforms in dataloader:\n",
    "      optimiser.zero_grad()\n",
    "      \n",
    "      # forward pass\n",
    "      output = model(waveforms)\n",
    "\n",
    "      # calculate loss\n",
    "      loss = torch.sqrt(criterion(output.float(), target_train_labels.float()))\n",
    "\n",
    "      # backward pass\n",
    "      loss.backward()\n",
    "      # update weights\n",
    "      optimiser.step()\n",
    "\n",
    "    print(f'Epoch {epoch + 1}, Loss: {math.sqrt(loss.item())}')\n",
    "\n",
    "  print(\"Training completed.\")\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = train_model(num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
