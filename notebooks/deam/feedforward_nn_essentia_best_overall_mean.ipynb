{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEAM Dataset - Feed Forward Neural Network\n",
    "## Essentia Best Overall Mean Featureset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import relevant libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torcheval.metrics import R2Score\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import math\n",
    "\n",
    "import sys\n",
    "sys.path.insert(1, '../../utils')\n",
    "from paths import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import annotations dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>song_id</th>\n",
       "      <th>valence_mean_mapped</th>\n",
       "      <th>arousal_mean_mapped</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>-0.475</td>\n",
       "      <td>-0.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>-0.375</td>\n",
       "      <td>-0.425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>0.175</td>\n",
       "      <td>0.125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>-0.150</td>\n",
       "      <td>0.075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1739</th>\n",
       "      <td>1996</td>\n",
       "      <td>-0.275</td>\n",
       "      <td>0.225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1740</th>\n",
       "      <td>1997</td>\n",
       "      <td>0.075</td>\n",
       "      <td>-0.275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1741</th>\n",
       "      <td>1998</td>\n",
       "      <td>0.350</td>\n",
       "      <td>0.300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1742</th>\n",
       "      <td>1999</td>\n",
       "      <td>-0.100</td>\n",
       "      <td>0.100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1743</th>\n",
       "      <td>2000</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.250</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1744 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      song_id  valence_mean_mapped  arousal_mean_mapped\n",
       "0           2               -0.475               -0.500\n",
       "1           3               -0.375               -0.425\n",
       "2           4                0.175                0.125\n",
       "3           5               -0.150                0.075\n",
       "4           7                0.200                0.350\n",
       "...       ...                  ...                  ...\n",
       "1739     1996               -0.275                0.225\n",
       "1740     1997                0.075               -0.275\n",
       "1741     1998                0.350                0.300\n",
       "1742     1999               -0.100                0.100\n",
       "1743     2000                0.200                0.250\n",
       "\n",
       "[1744 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_annotations = pd.read_csv(get_deam_path('processed/annotations/deam_static_annotations.csv'))\n",
    "df_annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the featureset\n",
    "\n",
    "This is where you should change between normalised and standardised, and untouched featuresets!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>song_id</th>\n",
       "      <th>lowlevel.melbands_kurtosis.mean</th>\n",
       "      <th>lowlevel.melbands_skewness.mean</th>\n",
       "      <th>lowlevel.spectral_energy.mean</th>\n",
       "      <th>tonal.chords_strength.mean</th>\n",
       "      <th>tonal.hpcp_entropy.mean</th>\n",
       "      <th>tonal.key_edma.strength</th>\n",
       "      <th>tonal.key_temperley.strength</th>\n",
       "      <th>rhythm.beats_loudness_band_ratio.mean_0</th>\n",
       "      <th>rhythm.beats_loudness_band_ratio.mean_1</th>\n",
       "      <th>...</th>\n",
       "      <th>tonal.chords_histogram_14</th>\n",
       "      <th>tonal.chords_histogram_15</th>\n",
       "      <th>tonal.chords_histogram_16</th>\n",
       "      <th>tonal.chords_histogram_17</th>\n",
       "      <th>tonal.chords_histogram_18</th>\n",
       "      <th>tonal.chords_histogram_19</th>\n",
       "      <th>tonal.chords_histogram_20</th>\n",
       "      <th>tonal.chords_histogram_21</th>\n",
       "      <th>tonal.chords_histogram_22</th>\n",
       "      <th>tonal.chords_histogram_23</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>-0.058571</td>\n",
       "      <td>0.380991</td>\n",
       "      <td>-0.069127</td>\n",
       "      <td>-0.452484</td>\n",
       "      <td>0.809781</td>\n",
       "      <td>-0.085112</td>\n",
       "      <td>-0.288833</td>\n",
       "      <td>0.569221</td>\n",
       "      <td>-0.098158</td>\n",
       "      <td>...</td>\n",
       "      <td>0.231912</td>\n",
       "      <td>-0.169071</td>\n",
       "      <td>0.693755</td>\n",
       "      <td>-0.215917</td>\n",
       "      <td>0.174944</td>\n",
       "      <td>-0.382824</td>\n",
       "      <td>-0.398789</td>\n",
       "      <td>0.323740</td>\n",
       "      <td>-0.466431</td>\n",
       "      <td>-0.596685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>1.479718</td>\n",
       "      <td>2.085477</td>\n",
       "      <td>3.025151</td>\n",
       "      <td>-0.386129</td>\n",
       "      <td>0.202518</td>\n",
       "      <td>0.233522</td>\n",
       "      <td>0.222782</td>\n",
       "      <td>2.000293</td>\n",
       "      <td>-0.968809</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.220673</td>\n",
       "      <td>-0.312868</td>\n",
       "      <td>0.297680</td>\n",
       "      <td>-0.477019</td>\n",
       "      <td>-0.368661</td>\n",
       "      <td>-0.434446</td>\n",
       "      <td>-0.398789</td>\n",
       "      <td>-0.487958</td>\n",
       "      <td>-0.563641</td>\n",
       "      <td>-0.386679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>-0.208423</td>\n",
       "      <td>0.235235</td>\n",
       "      <td>0.521816</td>\n",
       "      <td>1.264373</td>\n",
       "      <td>0.275493</td>\n",
       "      <td>-0.282729</td>\n",
       "      <td>-0.424737</td>\n",
       "      <td>1.035227</td>\n",
       "      <td>-0.435947</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.220673</td>\n",
       "      <td>-0.292305</td>\n",
       "      <td>-0.231753</td>\n",
       "      <td>-0.477019</td>\n",
       "      <td>0.214373</td>\n",
       "      <td>-0.434446</td>\n",
       "      <td>-0.398789</td>\n",
       "      <td>-0.487958</td>\n",
       "      <td>-0.747808</td>\n",
       "      <td>-0.701512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>-0.262338</td>\n",
       "      <td>-0.207226</td>\n",
       "      <td>-0.341279</td>\n",
       "      <td>1.199370</td>\n",
       "      <td>-0.827397</td>\n",
       "      <td>1.617190</td>\n",
       "      <td>1.568023</td>\n",
       "      <td>-0.764753</td>\n",
       "      <td>0.850362</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.220673</td>\n",
       "      <td>-0.312868</td>\n",
       "      <td>-0.275872</td>\n",
       "      <td>-0.477019</td>\n",
       "      <td>-0.368661</td>\n",
       "      <td>0.004792</td>\n",
       "      <td>-0.398789</td>\n",
       "      <td>-0.439206</td>\n",
       "      <td>0.248861</td>\n",
       "      <td>-0.510704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>2.154544</td>\n",
       "      <td>2.097370</td>\n",
       "      <td>2.747921</td>\n",
       "      <td>-1.793781</td>\n",
       "      <td>0.606843</td>\n",
       "      <td>-1.168573</td>\n",
       "      <td>-1.264489</td>\n",
       "      <td>2.137213</td>\n",
       "      <td>-1.194825</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.220673</td>\n",
       "      <td>-0.312868</td>\n",
       "      <td>-0.275872</td>\n",
       "      <td>0.206820</td>\n",
       "      <td>-0.368661</td>\n",
       "      <td>0.998061</td>\n",
       "      <td>1.904098</td>\n",
       "      <td>0.729588</td>\n",
       "      <td>-0.747808</td>\n",
       "      <td>-0.568093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1739</th>\n",
       "      <td>1996</td>\n",
       "      <td>-0.321333</td>\n",
       "      <td>-0.139660</td>\n",
       "      <td>0.382933</td>\n",
       "      <td>0.081083</td>\n",
       "      <td>0.775046</td>\n",
       "      <td>0.819838</td>\n",
       "      <td>0.782967</td>\n",
       "      <td>-0.920655</td>\n",
       "      <td>2.131480</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.220673</td>\n",
       "      <td>-0.312868</td>\n",
       "      <td>-0.275872</td>\n",
       "      <td>-0.477019</td>\n",
       "      <td>-0.368661</td>\n",
       "      <td>-0.434446</td>\n",
       "      <td>-0.398789</td>\n",
       "      <td>-0.487958</td>\n",
       "      <td>-0.671897</td>\n",
       "      <td>-0.520047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1740</th>\n",
       "      <td>1997</td>\n",
       "      <td>-0.346532</td>\n",
       "      <td>-0.099817</td>\n",
       "      <td>0.297799</td>\n",
       "      <td>0.051395</td>\n",
       "      <td>0.428232</td>\n",
       "      <td>-0.546505</td>\n",
       "      <td>-0.387822</td>\n",
       "      <td>0.794940</td>\n",
       "      <td>-0.035303</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.220673</td>\n",
       "      <td>-0.312868</td>\n",
       "      <td>-0.275872</td>\n",
       "      <td>-0.477019</td>\n",
       "      <td>-0.368661</td>\n",
       "      <td>-0.240465</td>\n",
       "      <td>-0.398789</td>\n",
       "      <td>0.878506</td>\n",
       "      <td>0.998161</td>\n",
       "      <td>1.772008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1741</th>\n",
       "      <td>1998</td>\n",
       "      <td>-0.242062</td>\n",
       "      <td>0.036691</td>\n",
       "      <td>-0.052204</td>\n",
       "      <td>0.025573</td>\n",
       "      <td>0.971682</td>\n",
       "      <td>0.513806</td>\n",
       "      <td>0.621963</td>\n",
       "      <td>0.807291</td>\n",
       "      <td>-0.616616</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.220673</td>\n",
       "      <td>-0.065850</td>\n",
       "      <td>-0.275872</td>\n",
       "      <td>3.771523</td>\n",
       "      <td>-0.368661</td>\n",
       "      <td>2.682184</td>\n",
       "      <td>-0.398789</td>\n",
       "      <td>-0.487958</td>\n",
       "      <td>-0.444162</td>\n",
       "      <td>-0.701502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1742</th>\n",
       "      <td>1999</td>\n",
       "      <td>-0.098127</td>\n",
       "      <td>0.390945</td>\n",
       "      <td>-0.085379</td>\n",
       "      <td>-1.326377</td>\n",
       "      <td>0.769227</td>\n",
       "      <td>0.185179</td>\n",
       "      <td>0.364287</td>\n",
       "      <td>0.474512</td>\n",
       "      <td>0.050776</td>\n",
       "      <td>...</td>\n",
       "      <td>1.114688</td>\n",
       "      <td>-0.086435</td>\n",
       "      <td>4.781004</td>\n",
       "      <td>-0.427183</td>\n",
       "      <td>1.810241</td>\n",
       "      <td>0.044041</td>\n",
       "      <td>-0.398789</td>\n",
       "      <td>4.066921</td>\n",
       "      <td>0.043842</td>\n",
       "      <td>0.883837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1743</th>\n",
       "      <td>2000</td>\n",
       "      <td>0.026676</td>\n",
       "      <td>0.422818</td>\n",
       "      <td>0.057740</td>\n",
       "      <td>-0.558929</td>\n",
       "      <td>1.518386</td>\n",
       "      <td>0.074597</td>\n",
       "      <td>-0.306991</td>\n",
       "      <td>0.797629</td>\n",
       "      <td>-0.683647</td>\n",
       "      <td>...</td>\n",
       "      <td>0.333627</td>\n",
       "      <td>-0.312868</td>\n",
       "      <td>-0.275872</td>\n",
       "      <td>-0.477019</td>\n",
       "      <td>-0.368661</td>\n",
       "      <td>-0.434446</td>\n",
       "      <td>-0.398789</td>\n",
       "      <td>-0.439156</td>\n",
       "      <td>-0.726119</td>\n",
       "      <td>-0.682401</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1744 rows Ã— 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      song_id  lowlevel.melbands_kurtosis.mean  \\\n",
       "0           2                        -0.058571   \n",
       "1           3                         1.479718   \n",
       "2           4                        -0.208423   \n",
       "3           5                        -0.262338   \n",
       "4           7                         2.154544   \n",
       "...       ...                              ...   \n",
       "1739     1996                        -0.321333   \n",
       "1740     1997                        -0.346532   \n",
       "1741     1998                        -0.242062   \n",
       "1742     1999                        -0.098127   \n",
       "1743     2000                         0.026676   \n",
       "\n",
       "      lowlevel.melbands_skewness.mean  lowlevel.spectral_energy.mean  \\\n",
       "0                            0.380991                      -0.069127   \n",
       "1                            2.085477                       3.025151   \n",
       "2                            0.235235                       0.521816   \n",
       "3                           -0.207226                      -0.341279   \n",
       "4                            2.097370                       2.747921   \n",
       "...                               ...                            ...   \n",
       "1739                        -0.139660                       0.382933   \n",
       "1740                        -0.099817                       0.297799   \n",
       "1741                         0.036691                      -0.052204   \n",
       "1742                         0.390945                      -0.085379   \n",
       "1743                         0.422818                       0.057740   \n",
       "\n",
       "      tonal.chords_strength.mean  tonal.hpcp_entropy.mean  \\\n",
       "0                      -0.452484                 0.809781   \n",
       "1                      -0.386129                 0.202518   \n",
       "2                       1.264373                 0.275493   \n",
       "3                       1.199370                -0.827397   \n",
       "4                      -1.793781                 0.606843   \n",
       "...                          ...                      ...   \n",
       "1739                    0.081083                 0.775046   \n",
       "1740                    0.051395                 0.428232   \n",
       "1741                    0.025573                 0.971682   \n",
       "1742                   -1.326377                 0.769227   \n",
       "1743                   -0.558929                 1.518386   \n",
       "\n",
       "      tonal.key_edma.strength  tonal.key_temperley.strength  \\\n",
       "0                   -0.085112                     -0.288833   \n",
       "1                    0.233522                      0.222782   \n",
       "2                   -0.282729                     -0.424737   \n",
       "3                    1.617190                      1.568023   \n",
       "4                   -1.168573                     -1.264489   \n",
       "...                       ...                           ...   \n",
       "1739                 0.819838                      0.782967   \n",
       "1740                -0.546505                     -0.387822   \n",
       "1741                 0.513806                      0.621963   \n",
       "1742                 0.185179                      0.364287   \n",
       "1743                 0.074597                     -0.306991   \n",
       "\n",
       "      rhythm.beats_loudness_band_ratio.mean_0  \\\n",
       "0                                    0.569221   \n",
       "1                                    2.000293   \n",
       "2                                    1.035227   \n",
       "3                                   -0.764753   \n",
       "4                                    2.137213   \n",
       "...                                       ...   \n",
       "1739                                -0.920655   \n",
       "1740                                 0.794940   \n",
       "1741                                 0.807291   \n",
       "1742                                 0.474512   \n",
       "1743                                 0.797629   \n",
       "\n",
       "      rhythm.beats_loudness_band_ratio.mean_1  ...  tonal.chords_histogram_14  \\\n",
       "0                                   -0.098158  ...                   0.231912   \n",
       "1                                   -0.968809  ...                  -0.220673   \n",
       "2                                   -0.435947  ...                  -0.220673   \n",
       "3                                    0.850362  ...                  -0.220673   \n",
       "4                                   -1.194825  ...                  -0.220673   \n",
       "...                                       ...  ...                        ...   \n",
       "1739                                 2.131480  ...                  -0.220673   \n",
       "1740                                -0.035303  ...                  -0.220673   \n",
       "1741                                -0.616616  ...                  -0.220673   \n",
       "1742                                 0.050776  ...                   1.114688   \n",
       "1743                                -0.683647  ...                   0.333627   \n",
       "\n",
       "      tonal.chords_histogram_15  tonal.chords_histogram_16  \\\n",
       "0                     -0.169071                   0.693755   \n",
       "1                     -0.312868                   0.297680   \n",
       "2                     -0.292305                  -0.231753   \n",
       "3                     -0.312868                  -0.275872   \n",
       "4                     -0.312868                  -0.275872   \n",
       "...                         ...                        ...   \n",
       "1739                  -0.312868                  -0.275872   \n",
       "1740                  -0.312868                  -0.275872   \n",
       "1741                  -0.065850                  -0.275872   \n",
       "1742                  -0.086435                   4.781004   \n",
       "1743                  -0.312868                  -0.275872   \n",
       "\n",
       "      tonal.chords_histogram_17  tonal.chords_histogram_18  \\\n",
       "0                     -0.215917                   0.174944   \n",
       "1                     -0.477019                  -0.368661   \n",
       "2                     -0.477019                   0.214373   \n",
       "3                     -0.477019                  -0.368661   \n",
       "4                      0.206820                  -0.368661   \n",
       "...                         ...                        ...   \n",
       "1739                  -0.477019                  -0.368661   \n",
       "1740                  -0.477019                  -0.368661   \n",
       "1741                   3.771523                  -0.368661   \n",
       "1742                  -0.427183                   1.810241   \n",
       "1743                  -0.477019                  -0.368661   \n",
       "\n",
       "      tonal.chords_histogram_19  tonal.chords_histogram_20  \\\n",
       "0                     -0.382824                  -0.398789   \n",
       "1                     -0.434446                  -0.398789   \n",
       "2                     -0.434446                  -0.398789   \n",
       "3                      0.004792                  -0.398789   \n",
       "4                      0.998061                   1.904098   \n",
       "...                         ...                        ...   \n",
       "1739                  -0.434446                  -0.398789   \n",
       "1740                  -0.240465                  -0.398789   \n",
       "1741                   2.682184                  -0.398789   \n",
       "1742                   0.044041                  -0.398789   \n",
       "1743                  -0.434446                  -0.398789   \n",
       "\n",
       "      tonal.chords_histogram_21  tonal.chords_histogram_22  \\\n",
       "0                      0.323740                  -0.466431   \n",
       "1                     -0.487958                  -0.563641   \n",
       "2                     -0.487958                  -0.747808   \n",
       "3                     -0.439206                   0.248861   \n",
       "4                      0.729588                  -0.747808   \n",
       "...                         ...                        ...   \n",
       "1739                  -0.487958                  -0.671897   \n",
       "1740                   0.878506                   0.998161   \n",
       "1741                  -0.487958                  -0.444162   \n",
       "1742                   4.066921                   0.043842   \n",
       "1743                  -0.439156                  -0.726119   \n",
       "\n",
       "      tonal.chords_histogram_23  \n",
       "0                     -0.596685  \n",
       "1                     -0.386679  \n",
       "2                     -0.701512  \n",
       "3                     -0.510704  \n",
       "4                     -0.568093  \n",
       "...                         ...  \n",
       "1739                  -0.520047  \n",
       "1740                   1.772008  \n",
       "1741                  -0.701502  \n",
       "1742                   0.883837  \n",
       "1743                  -0.682401  \n",
       "\n",
       "[1744 rows x 38 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_essentia_best_overall_features_mean = pd.read_csv(get_deam_path('processed/features/standardised_essentia_best_overall_features.csv'))\n",
    "\n",
    "# drop Unnamed:0 column\n",
    "df_essentia_best_overall_features_mean = df_essentia_best_overall_features_mean[df_essentia_best_overall_features_mean.columns[1:]]\n",
    "\n",
    "df_essentia_best_overall_features_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1744 entries, 0 to 1743\n",
      "Data columns (total 38 columns):\n",
      " #   Column                                   Non-Null Count  Dtype  \n",
      "---  ------                                   --------------  -----  \n",
      " 0   song_id                                  1744 non-null   int64  \n",
      " 1   lowlevel.melbands_kurtosis.mean          1744 non-null   float64\n",
      " 2   lowlevel.melbands_skewness.mean          1744 non-null   float64\n",
      " 3   lowlevel.spectral_energy.mean            1744 non-null   float64\n",
      " 4   tonal.chords_strength.mean               1744 non-null   float64\n",
      " 5   tonal.hpcp_entropy.mean                  1744 non-null   float64\n",
      " 6   tonal.key_edma.strength                  1744 non-null   float64\n",
      " 7   tonal.key_temperley.strength             1744 non-null   float64\n",
      " 8   rhythm.beats_loudness_band_ratio.mean_0  1744 non-null   float64\n",
      " 9   rhythm.beats_loudness_band_ratio.mean_1  1744 non-null   float64\n",
      " 10  rhythm.beats_loudness_band_ratio.mean_2  1744 non-null   float64\n",
      " 11  rhythm.beats_loudness_band_ratio.mean_3  1744 non-null   float64\n",
      " 12  rhythm.beats_loudness_band_ratio.mean_4  1744 non-null   float64\n",
      " 13  rhythm.beats_loudness_band_ratio.mean_5  1744 non-null   float64\n",
      " 14  tonal.chords_histogram_0                 1744 non-null   float64\n",
      " 15  tonal.chords_histogram_1                 1744 non-null   float64\n",
      " 16  tonal.chords_histogram_2                 1744 non-null   float64\n",
      " 17  tonal.chords_histogram_3                 1744 non-null   float64\n",
      " 18  tonal.chords_histogram_4                 1744 non-null   float64\n",
      " 19  tonal.chords_histogram_5                 1744 non-null   float64\n",
      " 20  tonal.chords_histogram_6                 1744 non-null   float64\n",
      " 21  tonal.chords_histogram_7                 1744 non-null   float64\n",
      " 22  tonal.chords_histogram_8                 1744 non-null   float64\n",
      " 23  tonal.chords_histogram_9                 1744 non-null   float64\n",
      " 24  tonal.chords_histogram_10                1744 non-null   float64\n",
      " 25  tonal.chords_histogram_11                1744 non-null   float64\n",
      " 26  tonal.chords_histogram_12                1744 non-null   float64\n",
      " 27  tonal.chords_histogram_13                1744 non-null   float64\n",
      " 28  tonal.chords_histogram_14                1744 non-null   float64\n",
      " 29  tonal.chords_histogram_15                1744 non-null   float64\n",
      " 30  tonal.chords_histogram_16                1744 non-null   float64\n",
      " 31  tonal.chords_histogram_17                1744 non-null   float64\n",
      " 32  tonal.chords_histogram_18                1744 non-null   float64\n",
      " 33  tonal.chords_histogram_19                1744 non-null   float64\n",
      " 34  tonal.chords_histogram_20                1744 non-null   float64\n",
      " 35  tonal.chords_histogram_21                1744 non-null   float64\n",
      " 36  tonal.chords_histogram_22                1744 non-null   float64\n",
      " 37  tonal.chords_histogram_23                1744 non-null   float64\n",
      "dtypes: float64(37), int64(1)\n",
      "memory usage: 517.9 KB\n"
     ]
    }
   ],
   "source": [
    "df_essentia_best_overall_features_mean.info(verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join both the featureset and annotation set together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lowlevel.melbands_kurtosis.mean</th>\n",
       "      <th>lowlevel.melbands_skewness.mean</th>\n",
       "      <th>lowlevel.spectral_energy.mean</th>\n",
       "      <th>tonal.chords_strength.mean</th>\n",
       "      <th>tonal.hpcp_entropy.mean</th>\n",
       "      <th>tonal.key_edma.strength</th>\n",
       "      <th>tonal.key_temperley.strength</th>\n",
       "      <th>rhythm.beats_loudness_band_ratio.mean_0</th>\n",
       "      <th>rhythm.beats_loudness_band_ratio.mean_1</th>\n",
       "      <th>rhythm.beats_loudness_band_ratio.mean_2</th>\n",
       "      <th>...</th>\n",
       "      <th>tonal.chords_histogram_16</th>\n",
       "      <th>tonal.chords_histogram_17</th>\n",
       "      <th>tonal.chords_histogram_18</th>\n",
       "      <th>tonal.chords_histogram_19</th>\n",
       "      <th>tonal.chords_histogram_20</th>\n",
       "      <th>tonal.chords_histogram_21</th>\n",
       "      <th>tonal.chords_histogram_22</th>\n",
       "      <th>tonal.chords_histogram_23</th>\n",
       "      <th>valence_mean_mapped</th>\n",
       "      <th>arousal_mean_mapped</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.058571</td>\n",
       "      <td>0.380991</td>\n",
       "      <td>-0.069127</td>\n",
       "      <td>-0.452484</td>\n",
       "      <td>0.809781</td>\n",
       "      <td>-0.085112</td>\n",
       "      <td>-0.288833</td>\n",
       "      <td>0.569221</td>\n",
       "      <td>-0.098158</td>\n",
       "      <td>-0.312716</td>\n",
       "      <td>...</td>\n",
       "      <td>0.693755</td>\n",
       "      <td>-0.215917</td>\n",
       "      <td>0.174944</td>\n",
       "      <td>-0.382824</td>\n",
       "      <td>-0.398789</td>\n",
       "      <td>0.323740</td>\n",
       "      <td>-0.466431</td>\n",
       "      <td>-0.596685</td>\n",
       "      <td>-0.475</td>\n",
       "      <td>-0.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.479718</td>\n",
       "      <td>2.085477</td>\n",
       "      <td>3.025151</td>\n",
       "      <td>-0.386129</td>\n",
       "      <td>0.202518</td>\n",
       "      <td>0.233522</td>\n",
       "      <td>0.222782</td>\n",
       "      <td>2.000293</td>\n",
       "      <td>-0.968809</td>\n",
       "      <td>-1.278967</td>\n",
       "      <td>...</td>\n",
       "      <td>0.297680</td>\n",
       "      <td>-0.477019</td>\n",
       "      <td>-0.368661</td>\n",
       "      <td>-0.434446</td>\n",
       "      <td>-0.398789</td>\n",
       "      <td>-0.487958</td>\n",
       "      <td>-0.563641</td>\n",
       "      <td>-0.386679</td>\n",
       "      <td>-0.375</td>\n",
       "      <td>-0.425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.208423</td>\n",
       "      <td>0.235235</td>\n",
       "      <td>0.521816</td>\n",
       "      <td>1.264373</td>\n",
       "      <td>0.275493</td>\n",
       "      <td>-0.282729</td>\n",
       "      <td>-0.424737</td>\n",
       "      <td>1.035227</td>\n",
       "      <td>-0.435947</td>\n",
       "      <td>-0.816900</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.231753</td>\n",
       "      <td>-0.477019</td>\n",
       "      <td>0.214373</td>\n",
       "      <td>-0.434446</td>\n",
       "      <td>-0.398789</td>\n",
       "      <td>-0.487958</td>\n",
       "      <td>-0.747808</td>\n",
       "      <td>-0.701512</td>\n",
       "      <td>0.175</td>\n",
       "      <td>0.125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.262338</td>\n",
       "      <td>-0.207226</td>\n",
       "      <td>-0.341279</td>\n",
       "      <td>1.199370</td>\n",
       "      <td>-0.827397</td>\n",
       "      <td>1.617190</td>\n",
       "      <td>1.568023</td>\n",
       "      <td>-0.764753</td>\n",
       "      <td>0.850362</td>\n",
       "      <td>0.883425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.275872</td>\n",
       "      <td>-0.477019</td>\n",
       "      <td>-0.368661</td>\n",
       "      <td>0.004792</td>\n",
       "      <td>-0.398789</td>\n",
       "      <td>-0.439206</td>\n",
       "      <td>0.248861</td>\n",
       "      <td>-0.510704</td>\n",
       "      <td>-0.150</td>\n",
       "      <td>0.075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.154544</td>\n",
       "      <td>2.097370</td>\n",
       "      <td>2.747921</td>\n",
       "      <td>-1.793781</td>\n",
       "      <td>0.606843</td>\n",
       "      <td>-1.168573</td>\n",
       "      <td>-1.264489</td>\n",
       "      <td>2.137213</td>\n",
       "      <td>-1.194825</td>\n",
       "      <td>-1.209289</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.275872</td>\n",
       "      <td>0.206820</td>\n",
       "      <td>-0.368661</td>\n",
       "      <td>0.998061</td>\n",
       "      <td>1.904098</td>\n",
       "      <td>0.729588</td>\n",
       "      <td>-0.747808</td>\n",
       "      <td>-0.568093</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1739</th>\n",
       "      <td>-0.321333</td>\n",
       "      <td>-0.139660</td>\n",
       "      <td>0.382933</td>\n",
       "      <td>0.081083</td>\n",
       "      <td>0.775046</td>\n",
       "      <td>0.819838</td>\n",
       "      <td>0.782967</td>\n",
       "      <td>-0.920655</td>\n",
       "      <td>2.131480</td>\n",
       "      <td>-0.013774</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.275872</td>\n",
       "      <td>-0.477019</td>\n",
       "      <td>-0.368661</td>\n",
       "      <td>-0.434446</td>\n",
       "      <td>-0.398789</td>\n",
       "      <td>-0.487958</td>\n",
       "      <td>-0.671897</td>\n",
       "      <td>-0.520047</td>\n",
       "      <td>-0.275</td>\n",
       "      <td>0.225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1740</th>\n",
       "      <td>-0.346532</td>\n",
       "      <td>-0.099817</td>\n",
       "      <td>0.297799</td>\n",
       "      <td>0.051395</td>\n",
       "      <td>0.428232</td>\n",
       "      <td>-0.546505</td>\n",
       "      <td>-0.387822</td>\n",
       "      <td>0.794940</td>\n",
       "      <td>-0.035303</td>\n",
       "      <td>-0.473535</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.275872</td>\n",
       "      <td>-0.477019</td>\n",
       "      <td>-0.368661</td>\n",
       "      <td>-0.240465</td>\n",
       "      <td>-0.398789</td>\n",
       "      <td>0.878506</td>\n",
       "      <td>0.998161</td>\n",
       "      <td>1.772008</td>\n",
       "      <td>0.075</td>\n",
       "      <td>-0.275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1741</th>\n",
       "      <td>-0.242062</td>\n",
       "      <td>0.036691</td>\n",
       "      <td>-0.052204</td>\n",
       "      <td>0.025573</td>\n",
       "      <td>0.971682</td>\n",
       "      <td>0.513806</td>\n",
       "      <td>0.621963</td>\n",
       "      <td>0.807291</td>\n",
       "      <td>-0.616616</td>\n",
       "      <td>-0.307099</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.275872</td>\n",
       "      <td>3.771523</td>\n",
       "      <td>-0.368661</td>\n",
       "      <td>2.682184</td>\n",
       "      <td>-0.398789</td>\n",
       "      <td>-0.487958</td>\n",
       "      <td>-0.444162</td>\n",
       "      <td>-0.701502</td>\n",
       "      <td>0.350</td>\n",
       "      <td>0.300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1742</th>\n",
       "      <td>-0.098127</td>\n",
       "      <td>0.390945</td>\n",
       "      <td>-0.085379</td>\n",
       "      <td>-1.326377</td>\n",
       "      <td>0.769227</td>\n",
       "      <td>0.185179</td>\n",
       "      <td>0.364287</td>\n",
       "      <td>0.474512</td>\n",
       "      <td>0.050776</td>\n",
       "      <td>-0.278295</td>\n",
       "      <td>...</td>\n",
       "      <td>4.781004</td>\n",
       "      <td>-0.427183</td>\n",
       "      <td>1.810241</td>\n",
       "      <td>0.044041</td>\n",
       "      <td>-0.398789</td>\n",
       "      <td>4.066921</td>\n",
       "      <td>0.043842</td>\n",
       "      <td>0.883837</td>\n",
       "      <td>-0.100</td>\n",
       "      <td>0.100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1743</th>\n",
       "      <td>0.026676</td>\n",
       "      <td>0.422818</td>\n",
       "      <td>0.057740</td>\n",
       "      <td>-0.558929</td>\n",
       "      <td>1.518386</td>\n",
       "      <td>0.074597</td>\n",
       "      <td>-0.306991</td>\n",
       "      <td>0.797629</td>\n",
       "      <td>-0.683647</td>\n",
       "      <td>-0.700831</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.275872</td>\n",
       "      <td>-0.477019</td>\n",
       "      <td>-0.368661</td>\n",
       "      <td>-0.434446</td>\n",
       "      <td>-0.398789</td>\n",
       "      <td>-0.439156</td>\n",
       "      <td>-0.726119</td>\n",
       "      <td>-0.682401</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.250</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1744 rows Ã— 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      lowlevel.melbands_kurtosis.mean  lowlevel.melbands_skewness.mean  \\\n",
       "0                           -0.058571                         0.380991   \n",
       "1                            1.479718                         2.085477   \n",
       "2                           -0.208423                         0.235235   \n",
       "3                           -0.262338                        -0.207226   \n",
       "4                            2.154544                         2.097370   \n",
       "...                               ...                              ...   \n",
       "1739                        -0.321333                        -0.139660   \n",
       "1740                        -0.346532                        -0.099817   \n",
       "1741                        -0.242062                         0.036691   \n",
       "1742                        -0.098127                         0.390945   \n",
       "1743                         0.026676                         0.422818   \n",
       "\n",
       "      lowlevel.spectral_energy.mean  tonal.chords_strength.mean  \\\n",
       "0                         -0.069127                   -0.452484   \n",
       "1                          3.025151                   -0.386129   \n",
       "2                          0.521816                    1.264373   \n",
       "3                         -0.341279                    1.199370   \n",
       "4                          2.747921                   -1.793781   \n",
       "...                             ...                         ...   \n",
       "1739                       0.382933                    0.081083   \n",
       "1740                       0.297799                    0.051395   \n",
       "1741                      -0.052204                    0.025573   \n",
       "1742                      -0.085379                   -1.326377   \n",
       "1743                       0.057740                   -0.558929   \n",
       "\n",
       "      tonal.hpcp_entropy.mean  tonal.key_edma.strength  \\\n",
       "0                    0.809781                -0.085112   \n",
       "1                    0.202518                 0.233522   \n",
       "2                    0.275493                -0.282729   \n",
       "3                   -0.827397                 1.617190   \n",
       "4                    0.606843                -1.168573   \n",
       "...                       ...                      ...   \n",
       "1739                 0.775046                 0.819838   \n",
       "1740                 0.428232                -0.546505   \n",
       "1741                 0.971682                 0.513806   \n",
       "1742                 0.769227                 0.185179   \n",
       "1743                 1.518386                 0.074597   \n",
       "\n",
       "      tonal.key_temperley.strength  rhythm.beats_loudness_band_ratio.mean_0  \\\n",
       "0                        -0.288833                                 0.569221   \n",
       "1                         0.222782                                 2.000293   \n",
       "2                        -0.424737                                 1.035227   \n",
       "3                         1.568023                                -0.764753   \n",
       "4                        -1.264489                                 2.137213   \n",
       "...                            ...                                      ...   \n",
       "1739                      0.782967                                -0.920655   \n",
       "1740                     -0.387822                                 0.794940   \n",
       "1741                      0.621963                                 0.807291   \n",
       "1742                      0.364287                                 0.474512   \n",
       "1743                     -0.306991                                 0.797629   \n",
       "\n",
       "      rhythm.beats_loudness_band_ratio.mean_1  \\\n",
       "0                                   -0.098158   \n",
       "1                                   -0.968809   \n",
       "2                                   -0.435947   \n",
       "3                                    0.850362   \n",
       "4                                   -1.194825   \n",
       "...                                       ...   \n",
       "1739                                 2.131480   \n",
       "1740                                -0.035303   \n",
       "1741                                -0.616616   \n",
       "1742                                 0.050776   \n",
       "1743                                -0.683647   \n",
       "\n",
       "      rhythm.beats_loudness_band_ratio.mean_2  ...  tonal.chords_histogram_16  \\\n",
       "0                                   -0.312716  ...                   0.693755   \n",
       "1                                   -1.278967  ...                   0.297680   \n",
       "2                                   -0.816900  ...                  -0.231753   \n",
       "3                                    0.883425  ...                  -0.275872   \n",
       "4                                   -1.209289  ...                  -0.275872   \n",
       "...                                       ...  ...                        ...   \n",
       "1739                                -0.013774  ...                  -0.275872   \n",
       "1740                                -0.473535  ...                  -0.275872   \n",
       "1741                                -0.307099  ...                  -0.275872   \n",
       "1742                                -0.278295  ...                   4.781004   \n",
       "1743                                -0.700831  ...                  -0.275872   \n",
       "\n",
       "      tonal.chords_histogram_17  tonal.chords_histogram_18  \\\n",
       "0                     -0.215917                   0.174944   \n",
       "1                     -0.477019                  -0.368661   \n",
       "2                     -0.477019                   0.214373   \n",
       "3                     -0.477019                  -0.368661   \n",
       "4                      0.206820                  -0.368661   \n",
       "...                         ...                        ...   \n",
       "1739                  -0.477019                  -0.368661   \n",
       "1740                  -0.477019                  -0.368661   \n",
       "1741                   3.771523                  -0.368661   \n",
       "1742                  -0.427183                   1.810241   \n",
       "1743                  -0.477019                  -0.368661   \n",
       "\n",
       "      tonal.chords_histogram_19  tonal.chords_histogram_20  \\\n",
       "0                     -0.382824                  -0.398789   \n",
       "1                     -0.434446                  -0.398789   \n",
       "2                     -0.434446                  -0.398789   \n",
       "3                      0.004792                  -0.398789   \n",
       "4                      0.998061                   1.904098   \n",
       "...                         ...                        ...   \n",
       "1739                  -0.434446                  -0.398789   \n",
       "1740                  -0.240465                  -0.398789   \n",
       "1741                   2.682184                  -0.398789   \n",
       "1742                   0.044041                  -0.398789   \n",
       "1743                  -0.434446                  -0.398789   \n",
       "\n",
       "      tonal.chords_histogram_21  tonal.chords_histogram_22  \\\n",
       "0                      0.323740                  -0.466431   \n",
       "1                     -0.487958                  -0.563641   \n",
       "2                     -0.487958                  -0.747808   \n",
       "3                     -0.439206                   0.248861   \n",
       "4                      0.729588                  -0.747808   \n",
       "...                         ...                        ...   \n",
       "1739                  -0.487958                  -0.671897   \n",
       "1740                   0.878506                   0.998161   \n",
       "1741                  -0.487958                  -0.444162   \n",
       "1742                   4.066921                   0.043842   \n",
       "1743                  -0.439156                  -0.726119   \n",
       "\n",
       "      tonal.chords_histogram_23  valence_mean_mapped  arousal_mean_mapped  \n",
       "0                     -0.596685               -0.475               -0.500  \n",
       "1                     -0.386679               -0.375               -0.425  \n",
       "2                     -0.701512                0.175                0.125  \n",
       "3                     -0.510704               -0.150                0.075  \n",
       "4                     -0.568093                0.200                0.350  \n",
       "...                         ...                  ...                  ...  \n",
       "1739                  -0.520047               -0.275                0.225  \n",
       "1740                   1.772008                0.075               -0.275  \n",
       "1741                  -0.701502                0.350                0.300  \n",
       "1742                   0.883837               -0.100                0.100  \n",
       "1743                  -0.682401                0.200                0.250  \n",
       "\n",
       "[1744 rows x 39 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_essentia_best_overall_features_mean_whole = pd.merge(df_essentia_best_overall_features_mean, df_annotations, how='inner', on='song_id')\n",
    "df_essentia_best_overall_features_mean_whole = df_essentia_best_overall_features_mean_whole.drop('song_id', axis=1)\n",
    "df_essentia_best_overall_features_mean_whole"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare dataframes for the neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform splitting of the dataframe into training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lowlevel.melbands_kurtosis.mean</th>\n",
       "      <th>lowlevel.melbands_skewness.mean</th>\n",
       "      <th>lowlevel.spectral_energy.mean</th>\n",
       "      <th>tonal.chords_strength.mean</th>\n",
       "      <th>tonal.hpcp_entropy.mean</th>\n",
       "      <th>tonal.key_edma.strength</th>\n",
       "      <th>tonal.key_temperley.strength</th>\n",
       "      <th>rhythm.beats_loudness_band_ratio.mean_0</th>\n",
       "      <th>rhythm.beats_loudness_band_ratio.mean_1</th>\n",
       "      <th>rhythm.beats_loudness_band_ratio.mean_2</th>\n",
       "      <th>...</th>\n",
       "      <th>tonal.chords_histogram_14</th>\n",
       "      <th>tonal.chords_histogram_15</th>\n",
       "      <th>tonal.chords_histogram_16</th>\n",
       "      <th>tonal.chords_histogram_17</th>\n",
       "      <th>tonal.chords_histogram_18</th>\n",
       "      <th>tonal.chords_histogram_19</th>\n",
       "      <th>tonal.chords_histogram_20</th>\n",
       "      <th>tonal.chords_histogram_21</th>\n",
       "      <th>tonal.chords_histogram_22</th>\n",
       "      <th>tonal.chords_histogram_23</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.058571</td>\n",
       "      <td>0.380991</td>\n",
       "      <td>-0.069127</td>\n",
       "      <td>-0.452484</td>\n",
       "      <td>0.809781</td>\n",
       "      <td>-0.085112</td>\n",
       "      <td>-0.288833</td>\n",
       "      <td>0.569221</td>\n",
       "      <td>-0.098158</td>\n",
       "      <td>-0.312716</td>\n",
       "      <td>...</td>\n",
       "      <td>0.231912</td>\n",
       "      <td>-0.169071</td>\n",
       "      <td>0.693755</td>\n",
       "      <td>-0.215917</td>\n",
       "      <td>0.174944</td>\n",
       "      <td>-0.382824</td>\n",
       "      <td>-0.398789</td>\n",
       "      <td>0.323740</td>\n",
       "      <td>-0.466431</td>\n",
       "      <td>-0.596685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.479718</td>\n",
       "      <td>2.085477</td>\n",
       "      <td>3.025151</td>\n",
       "      <td>-0.386129</td>\n",
       "      <td>0.202518</td>\n",
       "      <td>0.233522</td>\n",
       "      <td>0.222782</td>\n",
       "      <td>2.000293</td>\n",
       "      <td>-0.968809</td>\n",
       "      <td>-1.278967</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.220673</td>\n",
       "      <td>-0.312868</td>\n",
       "      <td>0.297680</td>\n",
       "      <td>-0.477019</td>\n",
       "      <td>-0.368661</td>\n",
       "      <td>-0.434446</td>\n",
       "      <td>-0.398789</td>\n",
       "      <td>-0.487958</td>\n",
       "      <td>-0.563641</td>\n",
       "      <td>-0.386679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.208423</td>\n",
       "      <td>0.235235</td>\n",
       "      <td>0.521816</td>\n",
       "      <td>1.264373</td>\n",
       "      <td>0.275493</td>\n",
       "      <td>-0.282729</td>\n",
       "      <td>-0.424737</td>\n",
       "      <td>1.035227</td>\n",
       "      <td>-0.435947</td>\n",
       "      <td>-0.816900</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.220673</td>\n",
       "      <td>-0.292305</td>\n",
       "      <td>-0.231753</td>\n",
       "      <td>-0.477019</td>\n",
       "      <td>0.214373</td>\n",
       "      <td>-0.434446</td>\n",
       "      <td>-0.398789</td>\n",
       "      <td>-0.487958</td>\n",
       "      <td>-0.747808</td>\n",
       "      <td>-0.701512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.262338</td>\n",
       "      <td>-0.207226</td>\n",
       "      <td>-0.341279</td>\n",
       "      <td>1.199370</td>\n",
       "      <td>-0.827397</td>\n",
       "      <td>1.617190</td>\n",
       "      <td>1.568023</td>\n",
       "      <td>-0.764753</td>\n",
       "      <td>0.850362</td>\n",
       "      <td>0.883425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.220673</td>\n",
       "      <td>-0.312868</td>\n",
       "      <td>-0.275872</td>\n",
       "      <td>-0.477019</td>\n",
       "      <td>-0.368661</td>\n",
       "      <td>0.004792</td>\n",
       "      <td>-0.398789</td>\n",
       "      <td>-0.439206</td>\n",
       "      <td>0.248861</td>\n",
       "      <td>-0.510704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.154544</td>\n",
       "      <td>2.097370</td>\n",
       "      <td>2.747921</td>\n",
       "      <td>-1.793781</td>\n",
       "      <td>0.606843</td>\n",
       "      <td>-1.168573</td>\n",
       "      <td>-1.264489</td>\n",
       "      <td>2.137213</td>\n",
       "      <td>-1.194825</td>\n",
       "      <td>-1.209289</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.220673</td>\n",
       "      <td>-0.312868</td>\n",
       "      <td>-0.275872</td>\n",
       "      <td>0.206820</td>\n",
       "      <td>-0.368661</td>\n",
       "      <td>0.998061</td>\n",
       "      <td>1.904098</td>\n",
       "      <td>0.729588</td>\n",
       "      <td>-0.747808</td>\n",
       "      <td>-0.568093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1739</th>\n",
       "      <td>-0.321333</td>\n",
       "      <td>-0.139660</td>\n",
       "      <td>0.382933</td>\n",
       "      <td>0.081083</td>\n",
       "      <td>0.775046</td>\n",
       "      <td>0.819838</td>\n",
       "      <td>0.782967</td>\n",
       "      <td>-0.920655</td>\n",
       "      <td>2.131480</td>\n",
       "      <td>-0.013774</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.220673</td>\n",
       "      <td>-0.312868</td>\n",
       "      <td>-0.275872</td>\n",
       "      <td>-0.477019</td>\n",
       "      <td>-0.368661</td>\n",
       "      <td>-0.434446</td>\n",
       "      <td>-0.398789</td>\n",
       "      <td>-0.487958</td>\n",
       "      <td>-0.671897</td>\n",
       "      <td>-0.520047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1740</th>\n",
       "      <td>-0.346532</td>\n",
       "      <td>-0.099817</td>\n",
       "      <td>0.297799</td>\n",
       "      <td>0.051395</td>\n",
       "      <td>0.428232</td>\n",
       "      <td>-0.546505</td>\n",
       "      <td>-0.387822</td>\n",
       "      <td>0.794940</td>\n",
       "      <td>-0.035303</td>\n",
       "      <td>-0.473535</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.220673</td>\n",
       "      <td>-0.312868</td>\n",
       "      <td>-0.275872</td>\n",
       "      <td>-0.477019</td>\n",
       "      <td>-0.368661</td>\n",
       "      <td>-0.240465</td>\n",
       "      <td>-0.398789</td>\n",
       "      <td>0.878506</td>\n",
       "      <td>0.998161</td>\n",
       "      <td>1.772008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1741</th>\n",
       "      <td>-0.242062</td>\n",
       "      <td>0.036691</td>\n",
       "      <td>-0.052204</td>\n",
       "      <td>0.025573</td>\n",
       "      <td>0.971682</td>\n",
       "      <td>0.513806</td>\n",
       "      <td>0.621963</td>\n",
       "      <td>0.807291</td>\n",
       "      <td>-0.616616</td>\n",
       "      <td>-0.307099</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.220673</td>\n",
       "      <td>-0.065850</td>\n",
       "      <td>-0.275872</td>\n",
       "      <td>3.771523</td>\n",
       "      <td>-0.368661</td>\n",
       "      <td>2.682184</td>\n",
       "      <td>-0.398789</td>\n",
       "      <td>-0.487958</td>\n",
       "      <td>-0.444162</td>\n",
       "      <td>-0.701502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1742</th>\n",
       "      <td>-0.098127</td>\n",
       "      <td>0.390945</td>\n",
       "      <td>-0.085379</td>\n",
       "      <td>-1.326377</td>\n",
       "      <td>0.769227</td>\n",
       "      <td>0.185179</td>\n",
       "      <td>0.364287</td>\n",
       "      <td>0.474512</td>\n",
       "      <td>0.050776</td>\n",
       "      <td>-0.278295</td>\n",
       "      <td>...</td>\n",
       "      <td>1.114688</td>\n",
       "      <td>-0.086435</td>\n",
       "      <td>4.781004</td>\n",
       "      <td>-0.427183</td>\n",
       "      <td>1.810241</td>\n",
       "      <td>0.044041</td>\n",
       "      <td>-0.398789</td>\n",
       "      <td>4.066921</td>\n",
       "      <td>0.043842</td>\n",
       "      <td>0.883837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1743</th>\n",
       "      <td>0.026676</td>\n",
       "      <td>0.422818</td>\n",
       "      <td>0.057740</td>\n",
       "      <td>-0.558929</td>\n",
       "      <td>1.518386</td>\n",
       "      <td>0.074597</td>\n",
       "      <td>-0.306991</td>\n",
       "      <td>0.797629</td>\n",
       "      <td>-0.683647</td>\n",
       "      <td>-0.700831</td>\n",
       "      <td>...</td>\n",
       "      <td>0.333627</td>\n",
       "      <td>-0.312868</td>\n",
       "      <td>-0.275872</td>\n",
       "      <td>-0.477019</td>\n",
       "      <td>-0.368661</td>\n",
       "      <td>-0.434446</td>\n",
       "      <td>-0.398789</td>\n",
       "      <td>-0.439156</td>\n",
       "      <td>-0.726119</td>\n",
       "      <td>-0.682401</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1744 rows Ã— 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      lowlevel.melbands_kurtosis.mean  lowlevel.melbands_skewness.mean  \\\n",
       "0                           -0.058571                         0.380991   \n",
       "1                            1.479718                         2.085477   \n",
       "2                           -0.208423                         0.235235   \n",
       "3                           -0.262338                        -0.207226   \n",
       "4                            2.154544                         2.097370   \n",
       "...                               ...                              ...   \n",
       "1739                        -0.321333                        -0.139660   \n",
       "1740                        -0.346532                        -0.099817   \n",
       "1741                        -0.242062                         0.036691   \n",
       "1742                        -0.098127                         0.390945   \n",
       "1743                         0.026676                         0.422818   \n",
       "\n",
       "      lowlevel.spectral_energy.mean  tonal.chords_strength.mean  \\\n",
       "0                         -0.069127                   -0.452484   \n",
       "1                          3.025151                   -0.386129   \n",
       "2                          0.521816                    1.264373   \n",
       "3                         -0.341279                    1.199370   \n",
       "4                          2.747921                   -1.793781   \n",
       "...                             ...                         ...   \n",
       "1739                       0.382933                    0.081083   \n",
       "1740                       0.297799                    0.051395   \n",
       "1741                      -0.052204                    0.025573   \n",
       "1742                      -0.085379                   -1.326377   \n",
       "1743                       0.057740                   -0.558929   \n",
       "\n",
       "      tonal.hpcp_entropy.mean  tonal.key_edma.strength  \\\n",
       "0                    0.809781                -0.085112   \n",
       "1                    0.202518                 0.233522   \n",
       "2                    0.275493                -0.282729   \n",
       "3                   -0.827397                 1.617190   \n",
       "4                    0.606843                -1.168573   \n",
       "...                       ...                      ...   \n",
       "1739                 0.775046                 0.819838   \n",
       "1740                 0.428232                -0.546505   \n",
       "1741                 0.971682                 0.513806   \n",
       "1742                 0.769227                 0.185179   \n",
       "1743                 1.518386                 0.074597   \n",
       "\n",
       "      tonal.key_temperley.strength  rhythm.beats_loudness_band_ratio.mean_0  \\\n",
       "0                        -0.288833                                 0.569221   \n",
       "1                         0.222782                                 2.000293   \n",
       "2                        -0.424737                                 1.035227   \n",
       "3                         1.568023                                -0.764753   \n",
       "4                        -1.264489                                 2.137213   \n",
       "...                            ...                                      ...   \n",
       "1739                      0.782967                                -0.920655   \n",
       "1740                     -0.387822                                 0.794940   \n",
       "1741                      0.621963                                 0.807291   \n",
       "1742                      0.364287                                 0.474512   \n",
       "1743                     -0.306991                                 0.797629   \n",
       "\n",
       "      rhythm.beats_loudness_band_ratio.mean_1  \\\n",
       "0                                   -0.098158   \n",
       "1                                   -0.968809   \n",
       "2                                   -0.435947   \n",
       "3                                    0.850362   \n",
       "4                                   -1.194825   \n",
       "...                                       ...   \n",
       "1739                                 2.131480   \n",
       "1740                                -0.035303   \n",
       "1741                                -0.616616   \n",
       "1742                                 0.050776   \n",
       "1743                                -0.683647   \n",
       "\n",
       "      rhythm.beats_loudness_band_ratio.mean_2  ...  tonal.chords_histogram_14  \\\n",
       "0                                   -0.312716  ...                   0.231912   \n",
       "1                                   -1.278967  ...                  -0.220673   \n",
       "2                                   -0.816900  ...                  -0.220673   \n",
       "3                                    0.883425  ...                  -0.220673   \n",
       "4                                   -1.209289  ...                  -0.220673   \n",
       "...                                       ...  ...                        ...   \n",
       "1739                                -0.013774  ...                  -0.220673   \n",
       "1740                                -0.473535  ...                  -0.220673   \n",
       "1741                                -0.307099  ...                  -0.220673   \n",
       "1742                                -0.278295  ...                   1.114688   \n",
       "1743                                -0.700831  ...                   0.333627   \n",
       "\n",
       "      tonal.chords_histogram_15  tonal.chords_histogram_16  \\\n",
       "0                     -0.169071                   0.693755   \n",
       "1                     -0.312868                   0.297680   \n",
       "2                     -0.292305                  -0.231753   \n",
       "3                     -0.312868                  -0.275872   \n",
       "4                     -0.312868                  -0.275872   \n",
       "...                         ...                        ...   \n",
       "1739                  -0.312868                  -0.275872   \n",
       "1740                  -0.312868                  -0.275872   \n",
       "1741                  -0.065850                  -0.275872   \n",
       "1742                  -0.086435                   4.781004   \n",
       "1743                  -0.312868                  -0.275872   \n",
       "\n",
       "      tonal.chords_histogram_17  tonal.chords_histogram_18  \\\n",
       "0                     -0.215917                   0.174944   \n",
       "1                     -0.477019                  -0.368661   \n",
       "2                     -0.477019                   0.214373   \n",
       "3                     -0.477019                  -0.368661   \n",
       "4                      0.206820                  -0.368661   \n",
       "...                         ...                        ...   \n",
       "1739                  -0.477019                  -0.368661   \n",
       "1740                  -0.477019                  -0.368661   \n",
       "1741                   3.771523                  -0.368661   \n",
       "1742                  -0.427183                   1.810241   \n",
       "1743                  -0.477019                  -0.368661   \n",
       "\n",
       "      tonal.chords_histogram_19  tonal.chords_histogram_20  \\\n",
       "0                     -0.382824                  -0.398789   \n",
       "1                     -0.434446                  -0.398789   \n",
       "2                     -0.434446                  -0.398789   \n",
       "3                      0.004792                  -0.398789   \n",
       "4                      0.998061                   1.904098   \n",
       "...                         ...                        ...   \n",
       "1739                  -0.434446                  -0.398789   \n",
       "1740                  -0.240465                  -0.398789   \n",
       "1741                   2.682184                  -0.398789   \n",
       "1742                   0.044041                  -0.398789   \n",
       "1743                  -0.434446                  -0.398789   \n",
       "\n",
       "      tonal.chords_histogram_21  tonal.chords_histogram_22  \\\n",
       "0                      0.323740                  -0.466431   \n",
       "1                     -0.487958                  -0.563641   \n",
       "2                     -0.487958                  -0.747808   \n",
       "3                     -0.439206                   0.248861   \n",
       "4                      0.729588                  -0.747808   \n",
       "...                         ...                        ...   \n",
       "1739                  -0.487958                  -0.671897   \n",
       "1740                   0.878506                   0.998161   \n",
       "1741                  -0.487958                  -0.444162   \n",
       "1742                   4.066921                   0.043842   \n",
       "1743                  -0.439156                  -0.726119   \n",
       "\n",
       "      tonal.chords_histogram_23  \n",
       "0                     -0.596685  \n",
       "1                     -0.386679  \n",
       "2                     -0.701512  \n",
       "3                     -0.510704  \n",
       "4                     -0.568093  \n",
       "...                         ...  \n",
       "1739                  -0.520047  \n",
       "1740                   1.772008  \n",
       "1741                  -0.701502  \n",
       "1742                   0.883837  \n",
       "1743                  -0.682401  \n",
       "\n",
       "[1744 rows x 37 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = df_essentia_best_overall_features_mean.drop('song_id', axis=1)\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>valence_mean_mapped</th>\n",
       "      <th>arousal_mean_mapped</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.475</td>\n",
       "      <td>-0.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.375</td>\n",
       "      <td>-0.425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.175</td>\n",
       "      <td>0.125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.150</td>\n",
       "      <td>0.075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.200</td>\n",
       "      <td>0.350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1739</th>\n",
       "      <td>-0.275</td>\n",
       "      <td>0.225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1740</th>\n",
       "      <td>0.075</td>\n",
       "      <td>-0.275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1741</th>\n",
       "      <td>0.350</td>\n",
       "      <td>0.300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1742</th>\n",
       "      <td>-0.100</td>\n",
       "      <td>0.100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1743</th>\n",
       "      <td>0.200</td>\n",
       "      <td>0.250</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1744 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      valence_mean_mapped  arousal_mean_mapped\n",
       "0                  -0.475               -0.500\n",
       "1                  -0.375               -0.425\n",
       "2                   0.175                0.125\n",
       "3                  -0.150                0.075\n",
       "4                   0.200                0.350\n",
       "...                   ...                  ...\n",
       "1739               -0.275                0.225\n",
       "1740                0.075               -0.275\n",
       "1741                0.350                0.300\n",
       "1742               -0.100                0.100\n",
       "1743                0.200                0.250\n",
       "\n",
       "[1744 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets = df_annotations.drop('song_id', axis=1)\n",
    "targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform 80-20 train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(features, targets, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create tensors for X_train and X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tensor = torch.tensor(X_train.values, dtype=torch.float64)\n",
    "X_test_tensor = torch.tensor(X_test.values, dtype=torch.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create tensors for Y_train and Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float64)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define neural network parameters and instantitate neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 1\n",
    "hidden_size = 20 \n",
    "output_size = 2  # Output size for valence and arousal\n",
    "learning_rate = 0.001\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 164"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set a random seed to ensure consistent initial weights of the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x108a2fe50>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the seed\n",
    "seed = 42\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_size, math.ceil((input_size**0.5) * 2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(math.ceil((input_size**0.5) * 2), math.ceil((input_size**0.5) * 2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(math.ceil((input_size**0.5) * 2), math.ceil((input_size**0.5) * 2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(math.ceil((input_size**0.5) * 2), math.ceil((input_size**0.5) * 2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(math.ceil((input_size**0.5) * 2), math.ceil((input_size**0.5) * 2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(math.ceil((input_size**0.5) * 2), math.ceil((input_size**0.5) * 2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(math.ceil((input_size**0.5) * 2), math.ceil((input_size**0.5) * 2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(math.ceil((input_size**0.5) * 2), math.ceil((input_size**0.5) * 2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(math.ceil((input_size**0.5) * 2), math.ceil((input_size**0.5) * 2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(math.ceil((input_size**0.5) * 2), math.ceil((input_size**0.5) * 2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(math.ceil((input_size**0.5) * 2), math.ceil((input_size**0.5) * 2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(math.ceil((input_size**0.5) * 2), 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare input_train_data and target_train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1395, 37])\n"
     ]
    }
   ],
   "source": [
    "input_train_data = X_train_tensor.float()\n",
    "\n",
    "# input_train_data = input_train_data.view(input_train_data.shape[1], -1)\n",
    "print(input_train_data.shape)\n",
    "\n",
    "target_train_labels = y_train_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(num_epochs):\n",
    "  model = NeuralNetwork(input_size=input_train_data.shape[1])\n",
    "  optimiser = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "  \n",
    "  for epoch in range(num_epochs):\n",
    "    optimiser.zero_grad()\n",
    "    \n",
    "    # forward pass\n",
    "    output = model(input_train_data)\n",
    "\n",
    "    # calculate loss\n",
    "    loss = torch.sqrt(criterion(output.float(), target_train_labels.float()))\n",
    "\n",
    "    # backward pass\n",
    "    loss.backward()\n",
    "    # update weights\n",
    "    optimiser.step()\n",
    "\n",
    "    print(f'Epoch {epoch + 1}, Loss: {math.sqrt(loss.item())}')\n",
    "\n",
    "  print(\"Training completed.\")\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Epoch 121, Loss: 0.5116998508836337\n",
      "Epoch 122, Loss: 0.5111129902664598\n",
      "Epoch 123, Loss: 0.5105405741436312\n",
      "Epoch 124, Loss: 0.5099849010083686\n",
      "Epoch 125, Loss: 0.5094623759868415\n",
      "Epoch 126, Loss: 0.5089652841754122\n",
      "Epoch 127, Loss: 0.5084717799132713\n",
      "Epoch 128, Loss: 0.5079809643002384\n",
      "Epoch 129, Loss: 0.507507731608986\n",
      "Epoch 130, Loss: 0.5070515727009828\n",
      "Epoch 131, Loss: 0.5065962678643066\n",
      "Epoch 132, Loss: 0.5061448812256618\n",
      "Epoch 133, Loss: 0.5057069703510908\n",
      "Epoch 134, Loss: 0.5052756693952567\n",
      "Epoch 135, Loss: 0.5048445017462813\n",
      "Epoch 136, Loss: 0.504425284231979\n",
      "Epoch 137, Loss: 0.504022717877823\n",
      "Epoch 138, Loss: 0.5036312802042249\n",
      "Epoch 139, Loss: 0.5032368731068207\n",
      "Epoch 140, Loss: 0.5028511060044271\n",
      "Epoch 141, Loss: 0.5024804933282331\n",
      "Epoch 142, Loss: 0.5021182727396313\n",
      "Epoch 143, Loss: 0.5017581664990297\n",
      "Epoch 144, Loss: 0.5014050827892457\n",
      "Epoch 145, Loss: 0.501047675902349\n",
      "Epoch 146, Loss: 0.500690966247362\n",
      "Epoch 147, Loss: 0.5003476722262875\n",
      "Epoch 148, Loss: 0.5000068544871652\n",
      "Epoch 149, Loss: 0.49966338867073357\n",
      "Epoch 150, Loss: 0.4993280873087794\n",
      "Epoch 151, Loss: 0.49900257940428383\n",
      "Epoch 152, Loss: 0.4986865554458311\n",
      "Epoch 153, Loss: 0.49836949389755036\n",
      "Epoch 154, Loss: 0.49805375636713634\n",
      "Epoch 155, Loss: 0.49773873165141763\n",
      "Epoch 156, Loss: 0.49742224924400874\n",
      "Epoch 157, Loss: 0.49711425826812494\n",
      "Epoch 158, Loss: 0.4968010973402242\n",
      "Epoch 159, Loss: 0.4964901399315246\n",
      "Epoch 160, Loss: 0.49617675026847124\n",
      "Epoch 161, Loss: 0.4958670390985117\n",
      "Epoch 162, Loss: 0.4955578109301908\n",
      "Epoch 163, Loss: 0.4952510223984696\n",
      "Epoch 164, Loss: 0.49494577484111674\n",
      "Training completed.\n"
     ]
    }
   ],
   "source": [
    "model = train_model(num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare input_test_data and target_test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([349, 37])\n"
     ]
    }
   ],
   "source": [
    "input_test_data = X_test_tensor.float()\n",
    "\n",
    "# input_test_data = input_test_data.view(input_test_data.shape[1], -1)\n",
    "print(input_test_data.shape)\n",
    "\n",
    "target_test_labels = y_test_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(trained_model):\n",
    "  with torch.no_grad():\n",
    "    test_pred = trained_model(input_test_data)\n",
    "    test_loss = criterion(test_pred.float(), target_test_labels)\n",
    "\n",
    "    # Separate the output into valence and arousal\n",
    "    valence_pred = test_pred[:, 0]\n",
    "    arousal_pred = test_pred[:, 1]\n",
    "        \n",
    "    valence_target = target_test_labels[:, 0]\n",
    "    arousal_target = target_test_labels[:, 1]\n",
    "\n",
    "     # Calculate RMSE for valence and arousal separately\n",
    "    valence_rmse = math.sqrt(mean_squared_error(valence_pred, valence_target))\n",
    "    arousal_rmse = math.sqrt(mean_squared_error(arousal_pred, arousal_target))\n",
    "\n",
    "  rmse = math.sqrt(test_loss.item())\n",
    "  print(f'Test RMSE: {rmse}')\n",
    "\n",
    "  print(f'Valence RMSE: {valence_rmse}')\n",
    "  print(f'Arousal RMSE: {arousal_rmse}')\n",
    "\n",
    "  metric = R2Score(multioutput=\"raw_values\")\n",
    "  metric.update(test_pred, target_test_labels)\n",
    "  adjusted_r2_score = metric.compute()\n",
    "  print(f'Test R^2 score: {adjusted_r2_score}')\n",
    "\n",
    "  metric = R2Score(multioutput=\"raw_values\", num_regressors=input_test_data.shape[1])\n",
    "  metric.update(test_pred, target_test_labels)\n",
    "  adjusted_r2_score = metric.compute()\n",
    "  print(f'Test Adjusted R^2 score: {adjusted_r2_score}')\n",
    "\n",
    "  metric = R2Score()\n",
    "  metric.update(test_pred, target_test_labels)\n",
    "  r2_score = metric.compute()\n",
    "  print(f'Test R^2 score (overall): {r2_score}')\n",
    "  return test_pred, rmse, adjusted_r2_score, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE: 0.2612383610913183\n",
      "Valence RMSE: 0.24607732401040297\n",
      "Arousal RMSE: 0.2755665313844103\n",
      "Test R^2 score: tensor([0.2661, 0.2481], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1788, 0.1586], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.2571213472447835\n"
     ]
    }
   ],
   "source": [
    "test_pred, rmse, adjusted_r2_score, r2_score = test_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), '../../models/deam_feedforward_nn_essentia_best_overall_mean_standardised.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "True values (test set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1500, -0.1500],\n",
       "        [-0.3000, -0.1000],\n",
       "        [ 0.2000,  0.3500],\n",
       "        [ 0.2250,  0.4500],\n",
       "        [-0.1750, -0.2000],\n",
       "        [-0.5250, -0.3000],\n",
       "        [-0.2500, -0.7750],\n",
       "        [ 0.3000,  0.3000],\n",
       "        [-0.1750, -0.4000],\n",
       "        [ 0.4500,  0.1500],\n",
       "        [ 0.1750,  0.0250],\n",
       "        [-0.1750, -0.0250],\n",
       "        [-0.0500, -0.3000],\n",
       "        [ 0.1250,  0.3000],\n",
       "        [-0.0750, -0.1500],\n",
       "        [-0.2000, -0.2750],\n",
       "        [-0.6000, -0.2250],\n",
       "        [ 0.1500, -0.2000],\n",
       "        [ 0.2750,  0.6000],\n",
       "        [-0.1500, -0.4500],\n",
       "        [-0.2250, -0.6250],\n",
       "        [-0.0250, -0.4500],\n",
       "        [-0.5250, -0.1250],\n",
       "        [ 0.0000,  0.3250],\n",
       "        [ 0.1250,  0.3750],\n",
       "        [ 0.1500, -0.2500],\n",
       "        [ 0.4500,  0.3250],\n",
       "        [ 0.2500,  0.2250],\n",
       "        [-0.1000,  0.0750],\n",
       "        [ 0.4250,  0.1250],\n",
       "        [-0.4500, -0.3500],\n",
       "        [-0.0500,  0.3750],\n",
       "        [-0.4750, -0.2000],\n",
       "        [-0.2750, -0.4000],\n",
       "        [-0.4000, -0.2250],\n",
       "        [ 0.1000, -0.4500],\n",
       "        [-0.2250, -0.6750],\n",
       "        [ 0.3000,  0.1250],\n",
       "        [-0.2000, -0.2250],\n",
       "        [ 0.2500,  0.3750],\n",
       "        [-0.3250, -0.4750],\n",
       "        [ 0.2250,  0.2000],\n",
       "        [ 0.0500,  0.1250],\n",
       "        [-0.5750, -0.6000],\n",
       "        [-0.1250, -0.3500],\n",
       "        [ 0.5000,  0.6000],\n",
       "        [-0.1500,  0.3250],\n",
       "        [-0.1750,  0.0250],\n",
       "        [-0.2750, -0.3250],\n",
       "        [ 0.2500,  0.3500],\n",
       "        [-0.3250, -0.7500],\n",
       "        [ 0.3000,  0.4000],\n",
       "        [ 0.0250,  0.2000],\n",
       "        [ 0.3750,  0.2250],\n",
       "        [-0.4250, -0.3750],\n",
       "        [-0.4250, -0.2500],\n",
       "        [-0.5250, -0.1750],\n",
       "        [-0.0500, -0.1500],\n",
       "        [ 0.1250, -0.1000],\n",
       "        [-0.3250, -0.5000],\n",
       "        [-0.4000, -0.0750],\n",
       "        [ 0.1500, -0.0500],\n",
       "        [-0.3000, -0.6500],\n",
       "        [-0.7000, -0.3750],\n",
       "        [ 0.5500,  0.2500],\n",
       "        [-0.2000, -0.1500],\n",
       "        [ 0.0750,  0.0750],\n",
       "        [-0.3000, -0.4000],\n",
       "        [-0.4250, -0.3750],\n",
       "        [-0.5750, -0.1000],\n",
       "        [ 0.2750, -0.1250],\n",
       "        [-0.1750, -0.2000],\n",
       "        [-0.2750, -0.6250],\n",
       "        [-0.4750, -0.3750],\n",
       "        [ 0.2750,  0.1250],\n",
       "        [ 0.3250,  0.4250],\n",
       "        [-0.3000, -0.1500],\n",
       "        [ 0.0500,  0.0750],\n",
       "        [ 0.2750, -0.2500],\n",
       "        [-0.3000, -0.6500],\n",
       "        [-0.3000,  0.2250],\n",
       "        [-0.4000, -0.0500],\n",
       "        [-0.0250, -0.3500],\n",
       "        [ 0.0000, -0.0500],\n",
       "        [-0.3000, -0.1000],\n",
       "        [ 0.3500, -0.3750],\n",
       "        [ 0.0250,  0.2000],\n",
       "        [-0.2500, -0.2250],\n",
       "        [ 0.0000, -0.3250],\n",
       "        [ 0.1500,  0.0000],\n",
       "        [-0.3500, -0.4750],\n",
       "        [-0.1750, -0.1250],\n",
       "        [-0.6750, -0.6000],\n",
       "        [ 0.2500,  0.2750],\n",
       "        [-0.3000, -0.5750],\n",
       "        [-0.1750, -0.5250],\n",
       "        [ 0.2750,  0.3250],\n",
       "        [ 0.3250, -0.1000],\n",
       "        [ 0.1000,  0.1750],\n",
       "        [-0.0750,  0.2000],\n",
       "        [ 0.2250, -0.3250],\n",
       "        [ 0.3750,  0.5500],\n",
       "        [-0.5250, -0.2500],\n",
       "        [-0.1000,  0.1500],\n",
       "        [ 0.1250,  0.1000],\n",
       "        [-0.3750, -0.3250],\n",
       "        [-0.4750, -0.3500],\n",
       "        [-0.2750, -0.2750],\n",
       "        [-0.2000, -0.0750],\n",
       "        [ 0.2750,  0.6000],\n",
       "        [-0.0500, -0.2500],\n",
       "        [-0.0500,  0.2500],\n",
       "        [-0.4750, -0.2000],\n",
       "        [-0.0250,  0.2000],\n",
       "        [-0.0750,  0.1500],\n",
       "        [ 0.6000,  0.6500],\n",
       "        [ 0.3250,  0.1500],\n",
       "        [-0.3500,  0.0000],\n",
       "        [ 0.2500,  0.2000],\n",
       "        [-0.1500,  0.3750],\n",
       "        [ 0.2250,  0.1000],\n",
       "        [ 0.2750, -0.4250],\n",
       "        [-0.0750,  0.6250],\n",
       "        [-0.1750, -0.3000],\n",
       "        [-0.0750, -0.6500],\n",
       "        [-0.1250,  0.1000],\n",
       "        [ 0.0000, -0.0250],\n",
       "        [ 0.0500,  0.0500],\n",
       "        [-0.1000,  0.0250],\n",
       "        [ 0.2000, -0.0750],\n",
       "        [-0.2750,  0.2250],\n",
       "        [-0.3750,  0.1250],\n",
       "        [-0.0750,  0.0500],\n",
       "        [ 0.3000,  0.0000],\n",
       "        [ 0.2000,  0.3000],\n",
       "        [ 0.3500,  0.1000],\n",
       "        [ 0.5750,  0.6000],\n",
       "        [-0.2750,  0.1250],\n",
       "        [ 0.0750, -0.3500],\n",
       "        [-0.0750, -0.3750],\n",
       "        [ 0.3750,  0.1000],\n",
       "        [ 0.0500, -0.0750],\n",
       "        [-0.4000, -0.0500],\n",
       "        [-0.1750, -0.2750],\n",
       "        [ 0.2000, -0.1750],\n",
       "        [ 0.2750,  0.1000],\n",
       "        [-0.0500, -0.2750],\n",
       "        [-0.3000, -0.4000],\n",
       "        [-0.1250, -0.0500],\n",
       "        [ 0.0250, -0.3500],\n",
       "        [-0.2000, -0.8500],\n",
       "        [ 0.1500,  0.0250],\n",
       "        [ 0.2250, -0.4000],\n",
       "        [-0.1250, -0.2000],\n",
       "        [ 0.3500,  0.1000],\n",
       "        [-0.2000,  0.0000],\n",
       "        [ 0.0250, -0.3500],\n",
       "        [-0.2250,  0.0000],\n",
       "        [-0.0750,  0.1250],\n",
       "        [-0.1000,  0.2000],\n",
       "        [-0.2500, -0.6000],\n",
       "        [ 0.2250,  0.1000],\n",
       "        [ 0.0250,  0.4250],\n",
       "        [-0.2250, -0.2500],\n",
       "        [ 0.1750,  0.3000],\n",
       "        [-0.1500,  0.0500],\n",
       "        [-0.3500, -0.0500],\n",
       "        [-0.4000,  0.2250],\n",
       "        [-0.1000,  0.1500],\n",
       "        [ 0.0000, -0.4750],\n",
       "        [-0.1500, -0.4500],\n",
       "        [ 0.1500,  0.2250],\n",
       "        [ 0.2250,  0.0750],\n",
       "        [ 0.3500,  0.0750],\n",
       "        [ 0.5250,  0.3750],\n",
       "        [ 0.2500,  0.2000],\n",
       "        [ 0.3500,  0.2000],\n",
       "        [-0.0250, -0.3000],\n",
       "        [-0.4000,  0.0750],\n",
       "        [-0.1500,  0.4750],\n",
       "        [-0.4750, -0.6750],\n",
       "        [-0.0750, -0.1750],\n",
       "        [-0.5250, -0.3750],\n",
       "        [ 0.2750, -0.2750],\n",
       "        [ 0.6000,  0.4000],\n",
       "        [-0.4250, -0.5500],\n",
       "        [-0.1500, -0.5750],\n",
       "        [ 0.2250,  0.4500],\n",
       "        [ 0.6500,  0.7000],\n",
       "        [ 0.1750,  0.3250],\n",
       "        [-0.2750, -0.1250],\n",
       "        [-0.2500, -0.4000],\n",
       "        [-0.0250, -0.1250],\n",
       "        [-0.0250, -0.2500],\n",
       "        [-0.1500, -0.3750],\n",
       "        [ 0.3500,  0.4000],\n",
       "        [ 0.5750, -0.4250],\n",
       "        [-0.0750,  0.0750],\n",
       "        [-0.0500, -0.1250],\n",
       "        [ 0.0750,  0.2000],\n",
       "        [-0.1500, -0.0500],\n",
       "        [-0.1500, -0.3000],\n",
       "        [-0.0500,  0.2500],\n",
       "        [-0.3000, -0.4250],\n",
       "        [-0.3000, -0.3500],\n",
       "        [-0.4000, -0.1000],\n",
       "        [-0.1500, -0.5750],\n",
       "        [-0.0750, -0.3750],\n",
       "        [-0.3500, -0.3500],\n",
       "        [ 0.2250,  0.2250],\n",
       "        [ 0.1250,  0.0000],\n",
       "        [-0.1750, -0.2000],\n",
       "        [-0.0750, -0.3000],\n",
       "        [ 0.5000,  0.4000],\n",
       "        [-0.2000, -0.2250],\n",
       "        [-0.2000, -0.4750],\n",
       "        [ 0.3000,  0.1750],\n",
       "        [ 0.1250,  0.0000],\n",
       "        [ 0.1750,  0.4750],\n",
       "        [ 0.1750, -0.2500],\n",
       "        [-0.1250,  0.4250],\n",
       "        [ 0.2000,  0.4750],\n",
       "        [-0.3000, -0.4000],\n",
       "        [-0.1250, -0.5250],\n",
       "        [-0.5750, -0.0750],\n",
       "        [ 0.1750, -0.0250],\n",
       "        [ 0.4000,  0.3500],\n",
       "        [-0.2500,  0.0000],\n",
       "        [-0.4750, -0.3000],\n",
       "        [ 0.1250,  0.2750],\n",
       "        [ 0.0750,  0.1750],\n",
       "        [ 0.3750,  0.1500],\n",
       "        [-0.1750, -0.2250],\n",
       "        [ 0.1250,  0.2750],\n",
       "        [-0.4500, -0.3250],\n",
       "        [ 0.3000,  0.0750],\n",
       "        [-0.3000,  0.0250],\n",
       "        [-0.3250, -0.5250],\n",
       "        [-0.1250, -0.0250],\n",
       "        [ 0.1250,  0.2000],\n",
       "        [-0.3750,  0.0500],\n",
       "        [-0.3250, -0.0500],\n",
       "        [ 0.0500,  0.3000],\n",
       "        [-0.5500, -0.3250],\n",
       "        [-0.2750, -0.3000],\n",
       "        [-0.2750, -0.5500],\n",
       "        [-0.1750, -0.5750],\n",
       "        [ 0.4500,  0.3000],\n",
       "        [-0.2500,  0.1250],\n",
       "        [-0.1000, -0.3250],\n",
       "        [ 0.1250,  0.2250],\n",
       "        [ 0.4750,  0.2750],\n",
       "        [-0.2250, -0.0250],\n",
       "        [ 0.3750,  0.3500],\n",
       "        [ 0.0000,  0.1750],\n",
       "        [-0.4250, -0.1000],\n",
       "        [ 0.1000, -0.1000],\n",
       "        [ 0.2000,  0.1500],\n",
       "        [ 0.1250,  0.0250],\n",
       "        [-0.2500,  0.1750],\n",
       "        [-0.3250, -0.6500],\n",
       "        [-0.0750, -0.3000],\n",
       "        [ 0.0750, -0.1500],\n",
       "        [ 0.5250,  0.5250],\n",
       "        [-0.0750,  0.2250],\n",
       "        [-0.1750,  0.0000],\n",
       "        [ 0.3000, -0.0500],\n",
       "        [-0.3500, -0.4000],\n",
       "        [-0.2250, -0.2000],\n",
       "        [ 0.4750,  0.5500],\n",
       "        [ 0.1000, -0.4750],\n",
       "        [-0.1250,  0.0000],\n",
       "        [-0.3000,  0.0000],\n",
       "        [-0.2750, -0.4500],\n",
       "        [-0.1500, -0.3000],\n",
       "        [ 0.1500, -0.1250],\n",
       "        [ 0.0500, -0.1250],\n",
       "        [ 0.0750,  0.1750],\n",
       "        [-0.1250, -0.1250],\n",
       "        [ 0.5750,  0.2500],\n",
       "        [-0.3750, -0.0500],\n",
       "        [ 0.2250,  0.1000],\n",
       "        [ 0.3250, -0.2000],\n",
       "        [ 0.4750,  0.4500],\n",
       "        [-0.1750, -0.4000],\n",
       "        [ 0.3500,  0.3750],\n",
       "        [-0.4000,  0.1250],\n",
       "        [-0.1500, -0.2750],\n",
       "        [ 0.5750, -0.3750],\n",
       "        [-0.2500,  0.2000],\n",
       "        [ 0.0000,  0.1500],\n",
       "        [ 0.4500, -0.1250],\n",
       "        [-0.1000, -0.0250],\n",
       "        [ 0.1500,  0.0750],\n",
       "        [ 0.2000, -0.1000],\n",
       "        [ 0.0500,  0.0250],\n",
       "        [ 0.3500,  0.4250],\n",
       "        [-0.3500, -0.5500],\n",
       "        [-0.4250, -0.6000],\n",
       "        [ 0.1750,  0.5500],\n",
       "        [ 0.2000,  0.0250],\n",
       "        [-0.2250, -0.1250],\n",
       "        [ 0.2500,  0.1750],\n",
       "        [-0.3750, -0.0500],\n",
       "        [-0.4750, -0.4500],\n",
       "        [-0.3250, -0.5500],\n",
       "        [-0.1250,  0.1750],\n",
       "        [-0.2500, -0.0500],\n",
       "        [ 0.0000,  0.1250],\n",
       "        [-0.6250, -0.1500],\n",
       "        [-0.4250, -0.5500],\n",
       "        [ 0.0250, -0.2000],\n",
       "        [ 0.3250,  0.3750],\n",
       "        [ 0.1750,  0.1500],\n",
       "        [-0.1750, -0.6500],\n",
       "        [ 0.0750,  0.4250],\n",
       "        [-0.4500, -0.3750],\n",
       "        [-0.1250, -0.1750],\n",
       "        [ 0.0500, -0.3000],\n",
       "        [-0.0500,  0.3750],\n",
       "        [-0.2750,  0.0500],\n",
       "        [-0.4750, -0.3250],\n",
       "        [ 0.0000, -0.3000],\n",
       "        [ 0.3750, -0.1000],\n",
       "        [ 0.1750,  0.1000],\n",
       "        [-0.2250, -0.5500],\n",
       "        [ 0.1500,  0.2250],\n",
       "        [-0.6250, -0.5750],\n",
       "        [ 0.0750, -0.2750],\n",
       "        [ 0.3500,  0.6250],\n",
       "        [-0.0500,  0.2500],\n",
       "        [ 0.0750,  0.3500],\n",
       "        [ 0.2000, -0.3000],\n",
       "        [ 0.2000, -0.1500],\n",
       "        [-0.0250, -0.1500],\n",
       "        [-0.3000, -0.1000],\n",
       "        [-0.1250,  0.1000],\n",
       "        [-0.6750, -0.6250],\n",
       "        [-0.0750, -0.1250],\n",
       "        [ 0.2750,  0.2000],\n",
       "        [ 0.0250,  0.0500],\n",
       "        [-0.4750, -0.1500],\n",
       "        [-0.1500, -0.0500],\n",
       "        [ 0.2000,  0.0250],\n",
       "        [-0.1750,  0.1750],\n",
       "        [ 0.2750,  0.2250],\n",
       "        [ 0.2500,  0.2500],\n",
       "        [-0.1750, -0.2250],\n",
       "        [-0.5750, -0.7000]], dtype=torch.float64)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_test_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicted values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1671,  0.1682],\n",
       "        [-0.2498, -0.3022],\n",
       "        [-0.1794, -0.2300],\n",
       "        [ 0.1664,  0.1672],\n",
       "        [-0.2510, -0.3032],\n",
       "        [-0.2976, -0.3492],\n",
       "        [-0.4330, -0.4802],\n",
       "        [ 0.1669,  0.1691],\n",
       "        [-0.0569, -0.0996],\n",
       "        [ 0.1674,  0.1695],\n",
       "        [ 0.1675,  0.1691],\n",
       "        [-0.1709, -0.2213],\n",
       "        [-0.3711, -0.4203],\n",
       "        [ 0.1668,  0.1677],\n",
       "        [ 0.1667,  0.1676],\n",
       "        [ 0.1674,  0.1689],\n",
       "        [-0.2965, -0.3482],\n",
       "        [ 0.1675,  0.1693],\n",
       "        [ 0.1664,  0.1672],\n",
       "        [-0.2207, -0.2722],\n",
       "        [-0.2399, -0.2919],\n",
       "        [-0.2020, -0.2530],\n",
       "        [-0.2398, -0.2919],\n",
       "        [ 0.1669,  0.1679],\n",
       "        [-0.1417, -0.1915],\n",
       "        [-0.1503, -0.2002],\n",
       "        [ 0.1667,  0.1675],\n",
       "        [ 0.0445,  0.0304],\n",
       "        [ 0.1651,  0.1658],\n",
       "        [ 0.1671,  0.1682],\n",
       "        [-0.2767, -0.3290],\n",
       "        [ 0.1671,  0.1681],\n",
       "        [ 0.1123,  0.1101],\n",
       "        [ 0.1675,  0.1695],\n",
       "        [-0.3936, -0.4421],\n",
       "        [ 0.1002,  0.0963],\n",
       "        [-0.4021, -0.4503],\n",
       "        [ 0.1675,  0.1692],\n",
       "        [ 0.1672,  0.1685],\n",
       "        [-0.2214, -0.2730],\n",
       "        [-0.1814, -0.2320],\n",
       "        [-0.1704, -0.2208],\n",
       "        [ 0.1675,  0.1690],\n",
       "        [ 0.1673,  0.1687],\n",
       "        [-0.1543, -0.2043],\n",
       "        [ 0.1674,  0.1695],\n",
       "        [ 0.1671,  0.1682],\n",
       "        [-0.1984, -0.2493],\n",
       "        [-0.1729, -0.2234],\n",
       "        [ 0.1675,  0.1693],\n",
       "        [-0.2736, -0.3261],\n",
       "        [ 0.1667,  0.1676],\n",
       "        [-0.3057, -0.3571],\n",
       "        [ 0.1661,  0.1669],\n",
       "        [ 0.1665,  0.1690],\n",
       "        [ 0.1673,  0.1695],\n",
       "        [-0.3008, -0.3524],\n",
       "        [ 0.1671,  0.1683],\n",
       "        [ 0.1667,  0.1675],\n",
       "        [-0.3055, -0.3570],\n",
       "        [-0.2103, -0.2615],\n",
       "        [-0.2132, -0.2647],\n",
       "        [-0.3062, -0.3576],\n",
       "        [-0.4045, -0.4528],\n",
       "        [ 0.1672,  0.1685],\n",
       "        [ 0.1672,  0.1695],\n",
       "        [ 0.1671,  0.1682],\n",
       "        [ 0.1665,  0.1673],\n",
       "        [ 0.1673,  0.1686],\n",
       "        [-0.2457, -0.2982],\n",
       "        [-0.2395, -0.2915],\n",
       "        [-0.1121, -0.1613],\n",
       "        [-0.4196, -0.4672],\n",
       "        [-0.2308, -0.2825],\n",
       "        [ 0.1671,  0.1683],\n",
       "        [ 0.1669,  0.1678],\n",
       "        [-0.2796, -0.3318],\n",
       "        [ 0.1674,  0.1687],\n",
       "        [ 0.1419,  0.1440],\n",
       "        [-0.3970, -0.4453],\n",
       "        [-0.0376, -0.0748],\n",
       "        [ 0.1503,  0.1539],\n",
       "        [ 0.1675,  0.1695],\n",
       "        [ 0.1667,  0.1676],\n",
       "        [-0.1628, -0.2130],\n",
       "        [ 0.1674,  0.1688],\n",
       "        [-0.0868, -0.1355],\n",
       "        [ 0.1170,  0.1156],\n",
       "        [ 0.1672,  0.1685],\n",
       "        [ 0.1669,  0.1691],\n",
       "        [-0.4588, -0.5051],\n",
       "        [ 0.1654,  0.1660],\n",
       "        [-0.4207, -0.4682],\n",
       "        [ 0.1671,  0.1681],\n",
       "        [-0.2647, -0.3174],\n",
       "        [-0.2531, -0.3053],\n",
       "        [ 0.1674,  0.1689],\n",
       "        [ 0.1674,  0.1688],\n",
       "        [ 0.1668,  0.1677],\n",
       "        [ 0.1663,  0.1672],\n",
       "        [ 0.1675,  0.1691],\n",
       "        [ 0.1672,  0.1685],\n",
       "        [-0.3127, -0.3639],\n",
       "        [ 0.1671,  0.1683],\n",
       "        [-0.0661, -0.1112],\n",
       "        [-0.2398, -0.2920],\n",
       "        [-0.1517, -0.2017],\n",
       "        [-0.2810, -0.3332],\n",
       "        [-0.2671, -0.3198],\n",
       "        [ 0.1672,  0.1684],\n",
       "        [-0.2079, -0.2590],\n",
       "        [ 0.1648,  0.1655],\n",
       "        [-0.2593, -0.3120],\n",
       "        [ 0.1652,  0.1680],\n",
       "        [-0.2316, -0.2833],\n",
       "        [ 0.1674,  0.1693],\n",
       "        [ 0.1675,  0.1692],\n",
       "        [-0.1128, -0.1619],\n",
       "        [ 0.1671,  0.1683],\n",
       "        [ 0.1669,  0.1678],\n",
       "        [ 0.1672,  0.1684],\n",
       "        [-0.1710, -0.2214],\n",
       "        [ 0.1637,  0.1647],\n",
       "        [-0.3251, -0.3759],\n",
       "        [-0.0923, -0.1411],\n",
       "        [ 0.1674,  0.1689],\n",
       "        [ 0.1673,  0.1686],\n",
       "        [ 0.1666,  0.1675],\n",
       "        [ 0.1675,  0.1690],\n",
       "        [ 0.1675,  0.1691],\n",
       "        [-0.1299, -0.1794],\n",
       "        [-0.0033, -0.0308],\n",
       "        [ 0.1669,  0.1678],\n",
       "        [ 0.1664,  0.1673],\n",
       "        [ 0.1663,  0.1671],\n",
       "        [ 0.1671,  0.1683],\n",
       "        [ 0.1666,  0.1675],\n",
       "        [ 0.1667,  0.1676],\n",
       "        [-0.1219, -0.1713],\n",
       "        [ 0.1675,  0.1694],\n",
       "        [ 0.1627,  0.1666],\n",
       "        [ 0.1667,  0.1675],\n",
       "        [-0.1629, -0.2131],\n",
       "        [-0.2922, -0.3441],\n",
       "        [ 0.1667,  0.1675],\n",
       "        [ 0.1675,  0.1691],\n",
       "        [ 0.1672,  0.1685],\n",
       "        [-0.3686, -0.4177],\n",
       "        [ 0.1673,  0.1686],\n",
       "        [-0.1510, -0.2010],\n",
       "        [ 0.1669,  0.1678],\n",
       "        [ 0.1675,  0.1696],\n",
       "        [ 0.1670,  0.1681],\n",
       "        [-0.2751, -0.3274],\n",
       "        [ 0.1675,  0.1692],\n",
       "        [-0.1493, -0.1992],\n",
       "        [-0.2322, -0.2839],\n",
       "        [-0.0980, -0.1468],\n",
       "        [ 0.1673,  0.1687],\n",
       "        [-0.1398, -0.1895],\n",
       "        [-0.3857, -0.4345],\n",
       "        [ 0.1673,  0.1686],\n",
       "        [ 0.1666,  0.1675],\n",
       "        [ 0.1671,  0.1682],\n",
       "        [-0.2466, -0.2986],\n",
       "        [-0.1427, -0.1925],\n",
       "        [-0.2311, -0.2827],\n",
       "        [ 0.1672,  0.1685],\n",
       "        [-0.1536, -0.2041],\n",
       "        [-0.2573, -0.3099],\n",
       "        [-0.2922, -0.3441],\n",
       "        [ 0.1671,  0.1682],\n",
       "        [ 0.1670,  0.1681],\n",
       "        [ 0.1672,  0.1684],\n",
       "        [ 0.1675,  0.1692],\n",
       "        [-0.0577, -0.1005],\n",
       "        [-0.0231, -0.0561],\n",
       "        [ 0.1673,  0.1687],\n",
       "        [-0.2653, -0.3179],\n",
       "        [ 0.1665,  0.1673],\n",
       "        [-0.2918, -0.3437],\n",
       "        [ 0.1667,  0.1690],\n",
       "        [-0.2882, -0.3400],\n",
       "        [ 0.1675,  0.1693],\n",
       "        [ 0.1673,  0.1687],\n",
       "        [ 0.1675,  0.1696],\n",
       "        [-0.0388, -0.0762],\n",
       "        [ 0.1673,  0.1686],\n",
       "        [ 0.1671,  0.1682],\n",
       "        [ 0.1665,  0.1673],\n",
       "        [-0.2480, -0.3006],\n",
       "        [-0.1674, -0.2180],\n",
       "        [-0.2394, -0.2913],\n",
       "        [-0.0508, -0.0916],\n",
       "        [ 0.1673,  0.1687],\n",
       "        [ 0.1669,  0.1678],\n",
       "        [ 0.1674,  0.1687],\n",
       "        [-0.1609, -0.2110],\n",
       "        [ 0.1674,  0.1693],\n",
       "        [-0.2080, -0.2593],\n",
       "        [ 0.1673,  0.1687],\n",
       "        [ 0.1636,  0.1671],\n",
       "        [ 0.1672,  0.1684],\n",
       "        [-0.4298, -0.4770],\n",
       "        [ 0.1609,  0.1651],\n",
       "        [-0.0087, -0.0378],\n",
       "        [-0.3196, -0.3705],\n",
       "        [-0.2518, -0.3041],\n",
       "        [ 0.1671,  0.1693],\n",
       "        [ 0.1671,  0.1682],\n",
       "        [ 0.1645,  0.1653],\n",
       "        [-0.0831, -0.1313],\n",
       "        [-0.2430, -0.2954],\n",
       "        [ 0.1672,  0.1684],\n",
       "        [-0.2759, -0.3282],\n",
       "        [-0.0655, -0.1102],\n",
       "        [-0.1796, -0.2302],\n",
       "        [ 0.1670,  0.1681],\n",
       "        [-0.2407, -0.2929],\n",
       "        [-0.1055, -0.1545],\n",
       "        [ 0.1666,  0.1675],\n",
       "        [-0.2345, -0.2863],\n",
       "        [-0.1977, -0.2487],\n",
       "        [ 0.1672,  0.1684],\n",
       "        [-0.2919, -0.3439],\n",
       "        [ 0.1674,  0.1688],\n",
       "        [ 0.1672,  0.1684],\n",
       "        [ 0.1674,  0.1688],\n",
       "        [-0.1928, -0.2437],\n",
       "        [ 0.1630,  0.1641],\n",
       "        [-0.1801, -0.2307],\n",
       "        [ 0.1664,  0.1672],\n",
       "        [-0.1481, -0.1981],\n",
       "        [-0.1784, -0.2289],\n",
       "        [-0.2902, -0.3421],\n",
       "        [ 0.1672,  0.1685],\n",
       "        [-0.2424, -0.2945],\n",
       "        [-0.1591, -0.2092],\n",
       "        [ 0.1673,  0.1686],\n",
       "        [ 0.1669,  0.1679],\n",
       "        [-0.2767, -0.3291],\n",
       "        [-0.2914, -0.3433],\n",
       "        [ 0.1668,  0.1677],\n",
       "        [-0.2275, -0.2796],\n",
       "        [-0.3304, -0.3810],\n",
       "        [-0.1710, -0.2214],\n",
       "        [-0.2278, -0.2797],\n",
       "        [ 0.1671,  0.1682],\n",
       "        [ 0.1675,  0.1691],\n",
       "        [ 0.1675,  0.1693],\n",
       "        [ 0.1675,  0.1690],\n",
       "        [ 0.1670,  0.1681],\n",
       "        [ 0.1672,  0.1685],\n",
       "        [ 0.1673,  0.1686],\n",
       "        [ 0.1672,  0.1685],\n",
       "        [-0.2567, -0.3091],\n",
       "        [-0.0945, -0.1433],\n",
       "        [-0.1424, -0.1922],\n",
       "        [-0.1527, -0.2026],\n",
       "        [-0.2900, -0.3419],\n",
       "        [-0.2791, -0.3314],\n",
       "        [-0.2677, -0.3205],\n",
       "        [ 0.1612,  0.1628],\n",
       "        [ 0.1671,  0.1683],\n",
       "        [ 0.1672,  0.1685],\n",
       "        [ 0.1674,  0.1688],\n",
       "        [-0.1737, -0.2242],\n",
       "        [ 0.1671,  0.1681],\n",
       "        [-0.1412, -0.1909],\n",
       "        [ 0.1667,  0.1676],\n",
       "        [-0.2151, -0.2663],\n",
       "        [ 0.1663,  0.1689],\n",
       "        [-0.2413, -0.2934],\n",
       "        [-0.1713, -0.2217],\n",
       "        [-0.2334, -0.2854],\n",
       "        [ 0.1660,  0.1686],\n",
       "        [ 0.1672,  0.1685],\n",
       "        [ 0.1666,  0.1675],\n",
       "        [ 0.1668,  0.1677],\n",
       "        [ 0.1674,  0.1696],\n",
       "        [ 0.1674,  0.1689],\n",
       "        [ 0.1667,  0.1675],\n",
       "        [ 0.1675,  0.1690],\n",
       "        [ 0.1673,  0.1686],\n",
       "        [-0.2130, -0.2644],\n",
       "        [ 0.1669,  0.1678],\n",
       "        [-0.2121, -0.2635],\n",
       "        [-0.1730, -0.2236],\n",
       "        [ 0.1671,  0.1681],\n",
       "        [ 0.1668,  0.1677],\n",
       "        [ 0.1644,  0.1652],\n",
       "        [ 0.1672,  0.1684],\n",
       "        [-0.0646, -0.1091],\n",
       "        [ 0.1663,  0.1671],\n",
       "        [ 0.1672,  0.1694],\n",
       "        [ 0.1673,  0.1695],\n",
       "        [ 0.1665,  0.1674],\n",
       "        [-0.4334, -0.4805],\n",
       "        [-0.3311, -0.3817],\n",
       "        [ 0.1665,  0.1674],\n",
       "        [ 0.0383,  0.0224],\n",
       "        [ 0.1674,  0.1695],\n",
       "        [ 0.1672,  0.1684],\n",
       "        [-0.1451, -0.1954],\n",
       "        [-0.2802, -0.3322],\n",
       "        [-0.2603, -0.3130],\n",
       "        [ 0.1671,  0.1683],\n",
       "        [ 0.1672,  0.1685],\n",
       "        [ 0.1673,  0.1686],\n",
       "        [ 0.1675,  0.1690],\n",
       "        [-0.1932, -0.2441],\n",
       "        [-0.1053, -0.1543],\n",
       "        [ 0.1671,  0.1683],\n",
       "        [ 0.1675,  0.1690],\n",
       "        [-0.1918, -0.2427],\n",
       "        [ 0.1671,  0.1682],\n",
       "        [-0.3067, -0.3580],\n",
       "        [ 0.1675,  0.1693],\n",
       "        [ 0.1673,  0.1686],\n",
       "        [-0.3794, -0.4283],\n",
       "        [ 0.1675,  0.1692],\n",
       "        [-0.3206, -0.3715],\n",
       "        [-0.2023, -0.2534],\n",
       "        [ 0.1671,  0.1694],\n",
       "        [ 0.1652,  0.1659],\n",
       "        [-0.2631, -0.3157],\n",
       "        [ 0.1670,  0.1680],\n",
       "        [ 0.0863,  0.0802],\n",
       "        [ 0.0161, -0.0060],\n",
       "        [-0.2019, -0.2529],\n",
       "        [-0.3260, -0.3767],\n",
       "        [ 0.1674,  0.1695],\n",
       "        [ 0.1665,  0.1674],\n",
       "        [ 0.1674,  0.1689],\n",
       "        [ 0.1674,  0.1688],\n",
       "        [ 0.1675,  0.1691],\n",
       "        [-0.1847, -0.2354],\n",
       "        [ 0.0165, -0.0053],\n",
       "        [-0.2021, -0.2534],\n",
       "        [ 0.1666,  0.1675],\n",
       "        [-0.2287, -0.2803],\n",
       "        [-0.2634, -0.3161],\n",
       "        [ 0.0310,  0.0131],\n",
       "        [ 0.1673,  0.1686],\n",
       "        [-0.0793, -0.1269],\n",
       "        [-0.1889, -0.2402],\n",
       "        [ 0.1664,  0.1672],\n",
       "        [ 0.1673,  0.1686],\n",
       "        [-0.3378, -0.3880]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.2077, 0.1178], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "pred_valence = test_pred[:, 0]\n",
    "pred_arousal = test_pred[1]\n",
    "real_valence = target_test_labels[0]\n",
    "real_arousal = target_test_labels[1]\n",
    "\n",
    "\n",
    "metric = R2Score(multioutput='raw_values')\n",
    "metric.update(test_pred, target_test_labels)\n",
    "print(metric.compute())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse relationship between epochs and r^2 score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create lists to store the epochs and R^2 scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs_list = [i for i in range(1, 301)]\n",
    "adjusted_r2_scores_valence_list = []\n",
    "adjusted_r2_scores_arousal_list = []\n",
    "r2_scores_list = []\n",
    "rmse_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conduct training and testing for each num_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of epochs: 1\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3770084412793108\n",
      "Valence RMSE: 0.4223478050931573\n",
      "Arousal RMSE: 0.3254121404076737\n",
      "Test R^2 score: tensor([-1.1618, -0.0485], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-1.4189, -0.1733], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.6051387348535369\n",
      "Num of epochs: 2\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.37479346241247913\n",
      "Valence RMSE: 0.41863576960085513\n",
      "Arousal RMSE: 0.32509132769879995\n",
      "Test R^2 score: tensor([-1.1239, -0.0465], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-1.3766, -0.1710], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.5851892413055466\n",
      "Num of epochs: 3\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3726463628794824\n",
      "Valence RMSE: 0.4149991839143464\n",
      "Arousal RMSE: 0.32481733464370427\n",
      "Test R^2 score: tensor([-1.0872, -0.0447], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-1.3355, -0.1690], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.5659377600290598\n",
      "Num of epochs: 4\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3706784099754748\n",
      "Valence RMSE: 0.4116377864961444\n",
      "Arousal RMSE: 0.3245909733379026\n",
      "Test R^2 score: tensor([-1.0535, -0.0432], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-1.2978, -0.1673], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.5483727252871547\n",
      "Num of epochs: 5\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3687276043981212\n",
      "Valence RMSE: 0.40829197813682316\n",
      "Arousal RMSE: 0.324372861194451\n",
      "Test R^2 score: tensor([-1.0203, -0.0418], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-1.2606, -0.1658], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.5310487595464048\n",
      "Num of epochs: 6\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3667953078850143\n",
      "Valence RMSE: 0.4049655504397189\n",
      "Arousal RMSE: 0.32416122335958136\n",
      "Test R^2 score: tensor([-0.9875, -0.0405], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-1.2239, -0.1643], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.5139768244686426\n",
      "Num of epochs: 7\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3648826653763239\n",
      "Valence RMSE: 0.4016616158120229\n",
      "Arousal RMSE: 0.3239547273424723\n",
      "Test R^2 score: tensor([-0.9552, -0.0391], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-1.1878, -0.1628], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.4971654115579348\n",
      "Num of epochs: 8\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3629895753076952\n",
      "Valence RMSE: 0.3983807729614198\n",
      "Arousal RMSE: 0.32375241049107806\n",
      "Test R^2 score: tensor([-0.9234, -0.0379], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-1.1522, -0.1613], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.4806115919542516\n",
      "Num of epochs: 9\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3611154774868628\n",
      "Valence RMSE: 0.39512268799099964\n",
      "Arousal RMSE: 0.32355345400087604\n",
      "Test R^2 score: tensor([-0.8920, -0.0366], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-1.1171, -0.1599], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.4643083640294047\n",
      "Num of epochs: 10\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3592605830284345\n",
      "Valence RMSE: 0.3918883953108675\n",
      "Arousal RMSE: 0.3233571070141101\n",
      "Test R^2 score: tensor([-0.8612, -0.0353], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-1.0826, -0.1585], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.44825552078613995\n",
      "Num of epochs: 11\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.35790290794075563\n",
      "Valence RMSE: 0.3896976554299072\n",
      "Arousal RMSE: 0.32299337512916787\n",
      "Test R^2 score: tensor([-0.8404, -0.0330], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-1.0594, -0.1559], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.43671620084783214\n",
      "Num of epochs: 12\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3565670511200635\n",
      "Valence RMSE: 0.3875702860038411\n",
      "Arousal RMSE: 0.322597887928326\n",
      "Test R^2 score: tensor([-0.8204, -0.0305], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-1.0370, -0.1531], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.42543253867809794\n",
      "Num of epochs: 13\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.35520674842166555\n",
      "Valence RMSE: 0.3853898599698401\n",
      "Arousal RMSE: 0.3222085102554119\n",
      "Test R^2 score: tensor([-0.8000, -0.0280], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-1.0141, -0.1503], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.4139769438965696\n",
      "Num of epochs: 14\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3538346595933212\n",
      "Valence RMSE: 0.3831762024762537\n",
      "Arousal RMSE: 0.3218290392660675\n",
      "Test R^2 score: tensor([-0.7794, -0.0256], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.9911, -0.1476], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.4024577118355045\n",
      "Num of epochs: 15\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3524579870958845\n",
      "Valence RMSE: 0.38094016890896026\n",
      "Arousal RMSE: 0.3214620553766465\n",
      "Test R^2 score: tensor([-0.7587, -0.0232], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.9679, -0.1450], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.39093573322086295\n",
      "Num of epochs: 16\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3510815090985445\n",
      "Valence RMSE: 0.37868878191669325\n",
      "Arousal RMSE: 0.3211094182864681\n",
      "Test R^2 score: tensor([-0.7379, -0.0210], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.9447, -0.1424], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.37945083287792436\n",
      "Num of epochs: 17\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3497086649418089\n",
      "Valence RMSE: 0.3764270026304096\n",
      "Arousal RMSE: 0.320772524324404\n",
      "Test R^2 score: tensor([-0.7172, -0.0188], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.9215, -0.1400], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.36803118436849747\n",
      "Num of epochs: 18\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.34834203801313396\n",
      "Valence RMSE: 0.3741585036521751\n",
      "Arousal RMSE: 0.32045243802950313\n",
      "Test R^2 score: tensor([-0.6966, -0.0168], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.8984, -0.1378], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.3566975155716523\n",
      "Num of epochs: 19\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3469836410548369\n",
      "Valence RMSE: 0.3718861165781456\n",
      "Arousal RMSE: 0.3201499814395899\n",
      "Test R^2 score: tensor([-0.6760, -0.0149], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.8754, -0.1356], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.3454655883977741\n",
      "Num of epochs: 20\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.34563507580328073\n",
      "Valence RMSE: 0.36961207965000165\n",
      "Arousal RMSE: 0.31986578721063624\n",
      "Test R^2 score: tensor([-0.6556, -0.0131], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.8526, -0.1336], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.3343475952147077\n",
      "Num of epochs: 21\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3442976350651657\n",
      "Valence RMSE: 0.36733820258403355\n",
      "Arousal RMSE: 0.31960032532096017\n",
      "Test R^2 score: tensor([-0.6353, -0.0114], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.8299, -0.1317], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.32335306196100144\n",
      "Num of epochs: 22\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3429723910800979\n",
      "Valence RMSE: 0.36506600615595386\n",
      "Arousal RMSE: 0.3193539309852632\n",
      "Test R^2 score: tensor([-0.6151, -0.0098], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.8073, -0.1300], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.31248961595394353\n",
      "Num of epochs: 23\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.34166025203065004\n",
      "Valence RMSE: 0.3627968117798861\n",
      "Arousal RMSE: 0.319126822748644\n",
      "Test R^2 score: tensor([-0.5951, -0.0084], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.7849, -0.1284], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.30176347927058134\n",
      "Num of epochs: 24\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3403619760911809\n",
      "Valence RMSE: 0.3605317753016831\n",
      "Arousal RMSE: 0.31891909402731583\n",
      "Test R^2 score: tensor([-0.5753, -0.0071], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.7627, -0.1269], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.29117961933202197\n",
      "Num of epochs: 25\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3390782309671919\n",
      "Valence RMSE: 0.3582719892329323\n",
      "Arousal RMSE: 0.3187307251627278\n",
      "Test R^2 score: tensor([-0.5556, -0.0059], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.7406, -0.1256], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.28074225567628497\n",
      "Num of epochs: 26\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.33780959177406866\n",
      "Valence RMSE: 0.35601848865574415\n",
      "Arousal RMSE: 0.31856157383527217\n",
      "Test R^2 score: tensor([-0.5361, -0.0048], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.7188, -0.1244], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.2704548656335938\n",
      "Num of epochs: 27\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3365565485669128\n",
      "Valence RMSE: 0.353772269486752\n",
      "Arousal RMSE: 0.3184113724550968\n",
      "Test R^2 score: tensor([-0.5168, -0.0039], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.6972, -0.1233], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.26032026386135654\n",
      "Num of epochs: 28\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3353195491379117\n",
      "Valence RMSE: 0.3515343711782081\n",
      "Arousal RMSE: 0.3182797290881819\n",
      "Test R^2 score: tensor([-0.4976, -0.0031], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.6758, -0.1224], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.250340964299005\n",
      "Num of epochs: 29\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.33409897579998526\n",
      "Valence RMSE: 0.34930584828804434\n",
      "Arousal RMSE: 0.3181661132379877\n",
      "Test R^2 score: tensor([-0.4787, -0.0023], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.6546, -0.1216], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.2405190102832433\n",
      "Num of epochs: 30\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.33289514237920304\n",
      "Valence RMSE: 0.3470877927716726\n",
      "Arousal RMSE: 0.3180698284154412\n",
      "Test R^2 score: tensor([-0.4600, -0.0017], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.6337, -0.1209], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.23085598095568594\n",
      "Num of epochs: 31\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3317083121333945\n",
      "Valence RMSE: 0.34488134271206006\n",
      "Arousal RMSE: 0.3179900440672594\n",
      "Test R^2 score: tensor([-0.4415, -0.0012], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.6130, -0.1204], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.2213531242674125\n",
      "Num of epochs: 32\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3305386922420814\n",
      "Valence RMSE: 0.34268771088302735\n",
      "Arousal RMSE: 0.3179257569747399\n",
      "Test R^2 score: tensor([-0.4232, -0.0008], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.5925, -0.1199], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.21201134866578708\n",
      "Num of epochs: 33\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.32938639467579084\n",
      "Valence RMSE: 0.34050812608936304\n",
      "Arousal RMSE: 0.3178757777216474\n",
      "Test R^2 score: tensor([-0.4051, -0.0005], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.5723, -0.1195], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.202830929350722\n",
      "Num of epochs: 34\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3282514883954564\n",
      "Valence RMSE: 0.3383438944148873\n",
      "Arousal RMSE: 0.3178387773381853\n",
      "Test R^2 score: tensor([-3.8734e-01, -2.8246e-04], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.5524, -0.1193], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.19381190492966316\n",
      "Num of epochs: 35\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3271340612240203\n",
      "Valence RMSE: 0.3361964501671257\n",
      "Arousal RMSE: 0.3178133649185721\n",
      "Test R^2 score: tensor([-3.6979e-01, -1.2251e-04], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.5328, -0.1191], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.18495451992402\n",
      "Num of epochs: 36\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3260341477562881\n",
      "Valence RMSE: 0.3340672560185277\n",
      "Arousal RMSE: 0.31779804823596647\n",
      "Test R^2 score: tensor([-3.5249e-01, -2.6116e-05], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.5134, -0.1190], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.17625868096556885\n",
      "Num of epochs: 37\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.32495175743650556\n",
      "Valence RMSE: 0.33195782509757815\n",
      "Arousal RMSE: 0.317791270614271\n",
      "Test R^2 score: tensor([-3.3546e-01,  1.6538e-05], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.4943, -0.1190], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.1677241596421517\n",
      "Num of epochs: 38\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.32388694439053345\n",
      "Valence RMSE: 0.32986977629262026\n",
      "Arousal RMSE: 0.3177914979698634\n",
      "Test R^2 score: tensor([-3.1872e-01,  1.5107e-05], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.4756, -0.1190], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.15935108307468682\n",
      "Num of epochs: 39\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3228387634797252\n",
      "Valence RMSE: 0.3278030302917769\n",
      "Arousal RMSE: 0.3177969599315958\n",
      "Test R^2 score: tensor([-3.0224e-01, -1.9267e-05], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.4572, -0.1190], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.15113194212115066\n",
      "Num of epochs: 40\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.32180598365875396\n",
      "Valence RMSE: 0.3257571264687312\n",
      "Arousal RMSE: 0.31780572177352556\n",
      "Test R^2 score: tensor([-2.8604e-01, -7.4410e-05], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.4390, -0.1191], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.1430572302049008\n",
      "Num of epochs: 41\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3207893671559522\n",
      "Valence RMSE: 0.32373521773407143\n",
      "Arousal RMSE: 0.31781621254949527\n",
      "Test R^2 score: tensor([-2.7013e-01, -1.4044e-04], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.4212, -0.1191], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.1351328241360794\n",
      "Num of epochs: 42\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3197892261540317\n",
      "Valence RMSE: 0.32173926472432596\n",
      "Arousal RMSE: 0.3178272232881239\n",
      "Test R^2 score: tensor([-2.5451e-01, -2.0974e-04], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.4038, -0.1192], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.12736080069977018\n",
      "Num of epochs: 43\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3188056278263091\n",
      "Valence RMSE: 0.31977062343829454\n",
      "Arousal RMSE: 0.31783770237864906\n",
      "Test R^2 score: tensor([-0.2392, -0.0003], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.3866, -0.1193], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.11974122187228076\n",
      "Num of epochs: 44\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.31783861493933724\n",
      "Valence RMSE: 0.3178303893822329\n",
      "Arousal RMSE: 0.3178468402835725\n",
      "Test R^2 score: tensor([-0.2242, -0.0003], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.3699, -0.1193], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.1122738045359426\n",
      "Num of epochs: 45\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3168884805161307\n",
      "Valence RMSE: 0.31591994827701864\n",
      "Arousal RMSE: 0.31785406155701423\n",
      "Test R^2 score: tensor([-0.2095, -0.0004], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.3534, -0.1194], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.10496003875862969\n",
      "Num of epochs: 46\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3160138531058658\n",
      "Valence RMSE: 0.3141551287400096\n",
      "Arousal RMSE: 0.3178617086029387\n",
      "Test R^2 score: tensor([-0.1961, -0.0004], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.3384, -0.1194], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.098246133746968\n",
      "Num of epochs: 47\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.315208439064835\n",
      "Valence RMSE: 0.31252347980954975\n",
      "Arousal RMSE: 0.3178707200783198\n",
      "Test R^2 score: tensor([-0.1837, -0.0005], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.3245, -0.1195], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.09207854178711616\n",
      "Num of epochs: 48\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.31440937865096447\n",
      "Valence RMSE: 0.31090232211692403\n",
      "Arousal RMSE: 0.3178777451626262\n",
      "Test R^2 score: tensor([-0.1714, -0.0005], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.3108, -0.1196], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.08597649100886107\n",
      "Num of epochs: 49\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3136186106633498\n",
      "Valence RMSE: 0.3092960670888741\n",
      "Arousal RMSE: 0.31788238200972097\n",
      "Test R^2 score: tensor([-0.1594, -0.0006], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.2973, -0.1196], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.07995463328825148\n",
      "Num of epochs: 50\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3128380732385992\n",
      "Valence RMSE: 0.30770918037841527\n",
      "Arousal RMSE: 0.317884224909199\n",
      "Test R^2 score: tensor([-0.1475, -0.0006], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.2840, -0.1196], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.074027473855103\n",
      "Num of epochs: 51\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.31206976798332164\n",
      "Valence RMSE: 0.3061461912671344\n",
      "Arousal RMSE: 0.31788298122258063\n",
      "Test R^2 score: tensor([-0.1359, -0.0006], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.2710, -0.1196], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.06820977835634967\n",
      "Num of epochs: 52\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3113159205864041\n",
      "Valence RMSE: 0.30461196712703764\n",
      "Arousal RMSE: 0.31787852130038136\n",
      "Test R^2 score: tensor([-0.1245, -0.0005], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.2583, -0.1196], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.06251774903945706\n",
      "Num of epochs: 53\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.31057863490283427\n",
      "Valence RMSE: 0.30311120002394315\n",
      "Arousal RMSE: 0.31787069279230706\n",
      "Test R^2 score: tensor([-0.1134, -0.0005], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.2459, -0.1195], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.05696653802868834\n",
      "Num of epochs: 54\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3098552926805694\n",
      "Valence RMSE: 0.30164134956242317\n",
      "Arousal RMSE: 0.317857044972237\n",
      "Test R^2 score: tensor([-0.1027, -0.0004], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.2339, -0.1194], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.051537320648558205\n",
      "Num of epochs: 55\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3091469864189624\n",
      "Valence RMSE: 0.30019982362904957\n",
      "Arousal RMSE: 0.3178423891128113\n",
      "Test R^2 score: tensor([-0.0922, -0.0003], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.2221, -0.1193], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.04623415842775014\n",
      "Num of epochs: 56\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.30846089932551735\n",
      "Valence RMSE: 0.2988032203685235\n",
      "Arousal RMSE: 0.317825248088967\n",
      "Test R^2 score: tensor([-0.0820, -0.0002], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.2108, -0.1192], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.041111022250277296\n",
      "Num of epochs: 57\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.30779914492313665\n",
      "Valence RMSE: 0.2974572408450977\n",
      "Arousal RMSE: 0.317804683885637\n",
      "Test R^2 score: tensor([-7.2299e-02, -6.7878e-05], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.1999, -0.1190], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.03618323209781993\n",
      "Num of epochs: 58\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3071642617003358\n",
      "Valence RMSE: 0.2961674867902885\n",
      "Arousal RMSE: 0.3177807217251371\n",
      "Test R^2 score: tensor([-6.3020e-02,  8.2924e-05], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.1895, -0.1189], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.03146849795317719\n",
      "Num of epochs: 59\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3065587369706443\n",
      "Valence RMSE: 0.29493936405277904\n",
      "Arousal RMSE: 0.31775350502900135\n",
      "Test R^2 score: tensor([-0.0542,  0.0003], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.1796, -0.1187], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.026983959301261295\n",
      "Num of epochs: 60\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.30598356876200533\n",
      "Valence RMSE: 0.293775933900774\n",
      "Arousal RMSE: 0.3177225037125852\n",
      "Test R^2 score: tensor([-0.0459,  0.0004], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.1704, -0.1185], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.02273609812503602\n",
      "Num of epochs: 61\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3054373494184783\n",
      "Valence RMSE: 0.292676027778837\n",
      "Arousal RMSE: 0.31768646745363993\n",
      "Test R^2 score: tensor([-0.0381,  0.0007], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.1616, -0.1182], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.018714103627486822\n",
      "Num of epochs: 62\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.30491621149582004\n",
      "Valence RMSE: 0.2916349023748452\n",
      "Arousal RMSE: 0.31764268570634807\n",
      "Test R^2 score: tensor([-0.0307,  0.0010], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.1534, -0.1179], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.014890151153775122\n",
      "Num of epochs: 63\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.30442628375074876\n",
      "Valence RMSE: 0.29065985425881263\n",
      "Arousal RMSE: 0.3175965579140124\n",
      "Test R^2 score: tensor([-0.0239,  0.0012], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.1457, -0.1176], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.011304707383293033\n",
      "Num of epochs: 64\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3039739982182511\n",
      "Valence RMSE: 0.2897625355147239\n",
      "Arousal RMSE: 0.3175500845499132\n",
      "Test R^2 score: tensor([-0.0175,  0.0015], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.1386, -0.1173], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.008002640126994531\n",
      "Num of epochs: 65\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.30356021279464634\n",
      "Valence RMSE: 0.28894966126782484\n",
      "Arousal RMSE: 0.31749913202569746\n",
      "Test R^2 score: tensor([-0.0118,  0.0019], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.1322, -0.1169], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.004991933494956524\n",
      "Num of epochs: 66\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3031871177468974\n",
      "Valence RMSE: 0.28822398051508535\n",
      "Arousal RMSE: 0.3174457336166006\n",
      "Test R^2 score: tensor([-0.0068,  0.0022], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.1265, -0.1165], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.0022860918468273206\n",
      "Num of epochs: 67\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.30285584199855214\n",
      "Valence RMSE: 0.28758661382706946\n",
      "Arousal RMSE: 0.3173913382762422\n",
      "Test R^2 score: tensor([-0.0023,  0.0025], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.1216, -0.1161], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.00010872231276076727\n",
      "Num of epochs: 68\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3025675992199918\n",
      "Valence RMSE: 0.2870378821263607\n",
      "Arousal RMSE: 0.31733823976935505\n",
      "Test R^2 score: tensor([0.0015, 0.0029], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.1173, -0.1158], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.002186230087418639\n",
      "Num of epochs: 69\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.30232135468759275\n",
      "Valence RMSE: 0.2865744464855784\n",
      "Arousal RMSE: 0.31728770795882855\n",
      "Test R^2 score: tensor([0.0047, 0.0032], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.1137, -0.1154], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.003955808458810495\n",
      "Num of epochs: 70\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3021119497631685\n",
      "Valence RMSE: 0.28618532136494157\n",
      "Arousal RMSE: 0.31724000727312374\n",
      "Test R^2 score: tensor([0.0074, 0.0035], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.1107, -0.1151], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.0054561694645235415\n",
      "Num of epochs: 71\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3019337099289184\n",
      "Valence RMSE: 0.28585717676739364\n",
      "Arousal RMSE: 0.31719647676709073\n",
      "Test R^2 score: tensor([0.0097, 0.0038], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.1081, -0.1148], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.006730342964886504\n",
      "Num of epochs: 72\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.30177635830473865\n",
      "Valence RMSE: 0.2855687447519287\n",
      "Arousal RMSE: 0.31715679542483044\n",
      "Test R^2 score: tensor([0.0117, 0.0040], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.1059, -0.1145], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.00785367695420619\n",
      "Num of epochs: 73\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.30163224740211786\n",
      "Valence RMSE: 0.2853032359779248\n",
      "Arousal RMSE: 0.31712156799282737\n",
      "Test R^2 score: tensor([0.0135, 0.0042], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.1038, -0.1142], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.008882745836131023\n",
      "Num of epochs: 74\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.30149177458123594\n",
      "Valence RMSE: 0.2850433812463061\n",
      "Arousal RMSE: 0.31708808096167623\n",
      "Test R^2 score: tensor([0.0153, 0.0044], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.1018, -0.1140], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.009885952418796529\n",
      "Num of epochs: 75\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.30134495514015874\n",
      "Valence RMSE: 0.2847718683793715\n",
      "Arousal RMSE: 0.3170529087653518\n",
      "Test R^2 score: tensor([0.0172, 0.0047], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.0997, -0.1138], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.010933854704466839\n",
      "Num of epochs: 76\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3011837754827659\n",
      "Valence RMSE: 0.2844777734380483\n",
      "Arousal RMSE: 0.3170106144087872\n",
      "Test R^2 score: tensor([0.0192, 0.0049], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.0974, -0.1135], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.012081064317211898\n",
      "Num of epochs: 77\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3010013220562907\n",
      "Valence RMSE: 0.2841521942848674\n",
      "Arousal RMSE: 0.3169560257233875\n",
      "Test R^2 score: tensor([0.0215, 0.0053], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.0949, -0.1131], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.013374219534827814\n",
      "Num of epochs: 78\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3007886914854669\n",
      "Valence RMSE: 0.28378823315682683\n",
      "Arousal RMSE: 0.31687838767073606\n",
      "Test R^2 score: tensor([0.0240, 0.0058], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.0921, -0.1125], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.014870396898363336\n",
      "Num of epochs: 79\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.30053656351955094\n",
      "Valence RMSE: 0.28337775125012177\n",
      "Arousal RMSE: 0.3167672680702763\n",
      "Test R^2 score: tensor([0.0268, 0.0065], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.0890, -0.1118], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.016629707846167907\n",
      "Num of epochs: 80\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.30024132924972796\n",
      "Valence RMSE: 0.28291809704908827\n",
      "Arousal RMSE: 0.31661816426321754\n",
      "Test R^2 score: tensor([0.0300, 0.0074], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.0854, -0.1107], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.018674555010011806\n",
      "Num of epochs: 81\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2998906688275823\n",
      "Valence RMSE: 0.28239977723013165\n",
      "Arousal RMSE: 0.31641616949846496\n",
      "Test R^2 score: tensor([0.0335, 0.0087], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.0815, -0.1093], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.02108314363836128\n",
      "Num of epochs: 82\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.29947875923338374\n",
      "Valence RMSE: 0.2818195556390623\n",
      "Arousal RMSE: 0.3161531156312326\n",
      "Test R^2 score: tensor([0.0375, 0.0103], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.0770, -0.1074], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.02389067384693233\n",
      "Num of epochs: 83\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.29899669177186333\n",
      "Valence RMSE: 0.2811696543986187\n",
      "Arousal RMSE: 0.3158190444327236\n",
      "Test R^2 score: tensor([0.0419, 0.0124], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.0721, -0.1051], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.027153007387783867\n",
      "Num of epochs: 84\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.29843754529503935\n",
      "Valence RMSE: 0.2804429402760936\n",
      "Arousal RMSE: 0.31540718782671506\n",
      "Test R^2 score: tensor([0.0469, 0.0150], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.0665, -0.1022], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.030913174366104967\n",
      "Num of epochs: 85\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.29779304380013544\n",
      "Valence RMSE: 0.27962744310229265\n",
      "Arousal RMSE: 0.3149125067944632\n",
      "Test R^2 score: tensor([0.0524, 0.0181], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.0603, -0.0988], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.03522447168561754\n",
      "Num of epochs: 86\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2970617967276758\n",
      "Valence RMSE: 0.2787201875215223\n",
      "Arousal RMSE: 0.31433497931047605\n",
      "Test R^2 score: tensor([0.0585, 0.0216], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.0535, -0.0947], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.04009316874012114\n",
      "Num of epochs: 87\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2962352682883613\n",
      "Valence RMSE: 0.2777116846998511\n",
      "Arousal RMSE: 0.31366684322211247\n",
      "Test R^2 score: tensor([0.0653, 0.0258], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.0459, -0.0901], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.04557086185563808\n",
      "Num of epochs: 88\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2953158108881355\n",
      "Valence RMSE: 0.2766069144255978\n",
      "Arousal RMSE: 0.3129080874841429\n",
      "Test R^2 score: tensor([0.0728, 0.0305], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.0376, -0.0848], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.051635381513304324\n",
      "Num of epochs: 89\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2943009230243565\n",
      "Valence RMSE: 0.2754064881312921\n",
      "Arousal RMSE: 0.3120534135066701\n",
      "Test R^2 score: tensor([0.0808, 0.0358], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.0286, -0.0789], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.05829514981628531\n",
      "Num of epochs: 90\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2931971332628774\n",
      "Valence RMSE: 0.274123481482655\n",
      "Arousal RMSE: 0.3111035756897813\n",
      "Test R^2 score: tensor([0.0893, 0.0417], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.0190, -0.0724], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.06549779524141758\n",
      "Num of epochs: 91\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.29200371326998253\n",
      "Valence RMSE: 0.2727578928547285\n",
      "Arousal RMSE: 0.3100572028067801\n",
      "Test R^2 score: tensor([0.0984, 0.0481], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.0089, -0.0652], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.07324099584556759\n",
      "Num of epochs: 92\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2907314732829584\n",
      "Valence RMSE: 0.2713292309848462\n",
      "Arousal RMSE: 0.30891750926053235\n",
      "Test R^2 score: tensor([0.1078, 0.0551], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([ 0.0017, -0.0573], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.08144367175118994\n",
      "Num of epochs: 93\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2893926220930269\n",
      "Valence RMSE: 0.2698557447869391\n",
      "Arousal RMSE: 0.30769149557509917\n",
      "Test R^2 score: tensor([0.1175, 0.0626], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([ 0.0125, -0.0490], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.09001838060894779\n",
      "Num of epochs: 94\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2879960046835817\n",
      "Valence RMSE: 0.2683337940181673\n",
      "Arousal RMSE: 0.3063990411460633\n",
      "Test R^2 score: tensor([0.1274, 0.0704], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([ 0.0236, -0.0402], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.09891110590284996\n",
      "Num of epochs: 95\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.28654403414618884\n",
      "Valence RMSE: 0.2667690292465807\n",
      "Arousal RMSE: 0.30503975485890605\n",
      "Test R^2 score: tensor([0.1375, 0.0787], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([ 0.0349, -0.0310], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.10809952848207299\n",
      "Num of epochs: 96\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.28507394034283107\n",
      "Valence RMSE: 0.2652327188647707\n",
      "Arousal RMSE: 0.3036213229810738\n",
      "Test R^2 score: tensor([0.1474, 0.0872], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([ 0.0460, -0.0214], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.11732635565044292\n",
      "Num of epochs: 97\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.28360240478900745\n",
      "Valence RMSE: 0.26380029767527874\n",
      "Arousal RMSE: 0.30210933608654067\n",
      "Test R^2 score: tensor([0.1566, 0.0963], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([ 0.0563, -0.0112], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.12645249235852568\n",
      "Num of epochs: 98\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2822303075188916\n",
      "Valence RMSE: 0.26256121808629446\n",
      "Arousal RMSE: 0.30061520207976433\n",
      "Test R^2 score: tensor([0.1645, 0.1052], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([ 0.0651, -0.0013], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.13486300514027322\n",
      "Num of epochs: 99\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2809584373892211\n",
      "Valence RMSE: 0.2614733415525876\n",
      "Arousal RMSE: 0.2991771694794066\n",
      "Test R^2 score: tensor([0.1714, 0.1137], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.0729, 0.0083], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.14258763688615\n",
      "Num of epochs: 100\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2797774256728471\n",
      "Valence RMSE: 0.26046619485194733\n",
      "Arousal RMSE: 0.29783918004788823\n",
      "Test R^2 score: tensor([0.1778, 0.1216], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.0800, 0.0171], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.1497276700608094\n",
      "Num of epochs: 101\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2786885234787625\n",
      "Valence RMSE: 0.2594450051777783\n",
      "Arousal RMSE: 0.29668649366940875\n",
      "Test R^2 score: tensor([0.1842, 0.1284], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.0872, 0.0247], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.15633764574258874\n",
      "Num of epochs: 102\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.27767763362458925\n",
      "Valence RMSE: 0.25837374856805106\n",
      "Arousal RMSE: 0.2957240985810875\n",
      "Test R^2 score: tensor([0.1910, 0.1341], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.0947, 0.0311], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.16252158836898373\n",
      "Num of epochs: 103\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2768335331675108\n",
      "Valence RMSE: 0.2574279841032551\n",
      "Arousal RMSE: 0.294965155861754\n",
      "Test R^2 score: tensor([0.1969, 0.1385], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1013, 0.0360], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.16769703297170052\n",
      "Num of epochs: 104\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.27615119447142955\n",
      "Valence RMSE: 0.25668151080000073\n",
      "Arousal RMSE: 0.29433580555111494\n",
      "Test R^2 score: tensor([0.2015, 0.1422], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1065, 0.0401], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.17185863197387158\n",
      "Num of epochs: 105\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.27555995304139486\n",
      "Valence RMSE: 0.2561119621254756\n",
      "Arousal RMSE: 0.29372306395070585\n",
      "Test R^2 score: tensor([0.2051, 0.1457], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1105, 0.0441], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.17541230101955912\n",
      "Num of epochs: 106\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.27495939216291193\n",
      "Valence RMSE: 0.255638586866702\n",
      "Arousal RMSE: 0.2930089547812348\n",
      "Test R^2 score: tensor([0.2080, 0.1499], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1138, 0.0488], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.178954570715524\n",
      "Num of epochs: 107\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.27429913376706105\n",
      "Valence RMSE: 0.2552049829330747\n",
      "Arousal RMSE: 0.2921479869121969\n",
      "Test R^2 score: tensor([0.2107, 0.1549], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1168, 0.0543], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.18279101221348376\n",
      "Num of epochs: 108\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.273612699322764\n",
      "Valence RMSE: 0.2548269714632568\n",
      "Arousal RMSE: 0.29118899889288924\n",
      "Test R^2 score: tensor([0.2130, 0.1604], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1194, 0.0605], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.18672883605575985\n",
      "Num of epochs: 109\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2729136004646971\n",
      "Valence RMSE: 0.2544760785380467\n",
      "Arousal RMSE: 0.29018199821681995\n",
      "Test R^2 score: tensor([0.2152, 0.1662], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1218, 0.0670], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.19071015870075547\n",
      "Num of epochs: 110\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2722537426613349\n",
      "Valence RMSE: 0.25414898904023203\n",
      "Arousal RMSE: 0.2892274056102511\n",
      "Test R^2 score: tensor([0.2172, 0.1717], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1241, 0.0732], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.19445656194740868\n",
      "Num of epochs: 111\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2716583478053125\n",
      "Valence RMSE: 0.2538317686911994\n",
      "Arousal RMSE: 0.2883850708128313\n",
      "Test R^2 score: tensor([0.2192, 0.1765], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1263, 0.0785], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.19784179757113074\n",
      "Num of epochs: 112\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2711290165808847\n",
      "Valence RMSE: 0.25350049157455334\n",
      "Arousal RMSE: 0.2876793145773521\n",
      "Test R^2 score: tensor([0.2212, 0.1805], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1285, 0.0831], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.20087302130567314\n",
      "Num of epochs: 113\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.27064363031813654\n",
      "Valence RMSE: 0.2531457953052828\n",
      "Arousal RMSE: 0.2870769157957779\n",
      "Test R^2 score: tensor([0.2234, 0.1840], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1310, 0.0869], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.20367608820790845\n",
      "Num of epochs: 114\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.27021788430962873\n",
      "Valence RMSE: 0.25282928960764356\n",
      "Arousal RMSE: 0.28655324168125906\n",
      "Test R^2 score: tensor([0.2253, 0.1869], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1332, 0.0902], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.20613368934702037\n",
      "Num of epochs: 115\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2698502782800168\n",
      "Valence RMSE: 0.2525895931799301\n",
      "Arousal RMSE: 0.28607139457275765\n",
      "Test R^2 score: tensor([0.2268, 0.1897], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1348, 0.0933], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.20823380349598847\n",
      "Num of epochs: 116\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.26951650324115234\n",
      "Valence RMSE: 0.25243247101208416\n",
      "Arousal RMSE: 0.2855803540466491\n",
      "Test R^2 score: tensor([0.2278, 0.1925], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1359, 0.0964], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.21010434711448822\n",
      "Num of epochs: 117\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.26916119498236357\n",
      "Valence RMSE: 0.2522849169607534\n",
      "Arousal RMSE: 0.2850400295445775\n",
      "Test R^2 score: tensor([0.2287, 0.1955], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1369, 0.0998], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.21208206055522189\n",
      "Num of epochs: 118\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.26877196723674396\n",
      "Valence RMSE: 0.2521002165607664\n",
      "Arousal RMSE: 0.28446831379721965\n",
      "Test R^2 score: tensor([0.2298, 0.1987], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1381, 0.1034], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.21425854232599167\n",
      "Num of epochs: 119\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2683776048328613\n",
      "Valence RMSE: 0.2518965083663294\n",
      "Arousal RMSE: 0.2839035516236081\n",
      "Test R^2 score: tensor([0.2310, 0.2019], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1395, 0.1070], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.2164698550149896\n",
      "Num of epochs: 120\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.267995899469607\n",
      "Valence RMSE: 0.251674456035099\n",
      "Arousal RMSE: 0.28337884967739635\n",
      "Test R^2 score: tensor([0.2324, 0.2049], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1411, 0.1103], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.2186210609030887\n",
      "Num of epochs: 121\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Epoch 121, Loss: 0.5116998508836337\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2676488313360601\n",
      "Valence RMSE: 0.25147086884515374\n",
      "Arousal RMSE: 0.2829031564924312\n",
      "Test R^2 score: tensor([0.2336, 0.2075], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1424, 0.1132], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.2205753990954017\n",
      "Num of epochs: 122\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Epoch 121, Loss: 0.5116998508836337\n",
      "Epoch 122, Loss: 0.5111129902664598\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2673579622498822\n",
      "Valence RMSE: 0.2513245721105815\n",
      "Arousal RMSE: 0.2824827771922599\n",
      "Test R^2 score: tensor([0.2345, 0.2099], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1434, 0.1159], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.22219781814494016\n",
      "Num of epochs: 123\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Epoch 121, Loss: 0.5116998508836337\n",
      "Epoch 122, Loss: 0.5111129902664598\n",
      "Epoch 123, Loss: 0.5105405741436312\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.26713117165547146\n",
      "Valence RMSE: 0.25125331105788545\n",
      "Arousal RMSE: 0.28211681875155786\n",
      "Test R^2 score: tensor([0.2349, 0.2119], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1439, 0.1182], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.22343777690401517\n",
      "Num of epochs: 124\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Epoch 121, Loss: 0.5116998508836337\n",
      "Epoch 122, Loss: 0.5111129902664598\n",
      "Epoch 123, Loss: 0.5105405741436312\n",
      "Epoch 124, Loss: 0.5099849010083686\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2669390579089159\n",
      "Valence RMSE: 0.25123096120581095\n",
      "Arousal RMSE: 0.2817728258831988\n",
      "Test R^2 score: tensor([0.2351, 0.2138], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1441, 0.1203], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.2244661621160126\n",
      "Num of epochs: 125\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Epoch 121, Loss: 0.5116998508836337\n",
      "Epoch 122, Loss: 0.5111129902664598\n",
      "Epoch 123, Loss: 0.5105405741436312\n",
      "Epoch 124, Loss: 0.5099849010083686\n",
      "Epoch 125, Loss: 0.5094623759868415\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2667454523125892\n",
      "Valence RMSE: 0.25119443024779736\n",
      "Arousal RMSE: 0.28143850282322797\n",
      "Test R^2 score: tensor([0.2353, 0.2157], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1443, 0.1224], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.2255095947559687\n",
      "Num of epochs: 126\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Epoch 121, Loss: 0.5116998508836337\n",
      "Epoch 122, Loss: 0.5111129902664598\n",
      "Epoch 123, Loss: 0.5105405741436312\n",
      "Epoch 124, Loss: 0.5099849010083686\n",
      "Epoch 125, Loss: 0.5094623759868415\n",
      "Epoch 126, Loss: 0.5089652841754122\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2664964372510274\n",
      "Valence RMSE: 0.2510707143929072\n",
      "Arousal RMSE: 0.2810768551645915\n",
      "Test R^2 score: tensor([0.2361, 0.2177], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1452, 0.1247], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.2268932819086727\n",
      "Num of epochs: 127\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Epoch 121, Loss: 0.5116998508836337\n",
      "Epoch 122, Loss: 0.5111129902664598\n",
      "Epoch 123, Loss: 0.5105405741436312\n",
      "Epoch 124, Loss: 0.5099849010083686\n",
      "Epoch 125, Loss: 0.5094623759868415\n",
      "Epoch 126, Loss: 0.5089652841754122\n",
      "Epoch 127, Loss: 0.5084717799132713\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2661889232419731\n",
      "Valence RMSE: 0.2508434254695692\n",
      "Arousal RMSE: 0.28069674314486576\n",
      "Test R^2 score: tensor([0.2374, 0.2198], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1467, 0.1270], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.2286417335815054\n",
      "Num of epochs: 128\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Epoch 121, Loss: 0.5116998508836337\n",
      "Epoch 122, Loss: 0.5111129902664598\n",
      "Epoch 123, Loss: 0.5105405741436312\n",
      "Epoch 124, Loss: 0.5099849010083686\n",
      "Epoch 125, Loss: 0.5094623759868415\n",
      "Epoch 126, Loss: 0.5089652841754122\n",
      "Epoch 127, Loss: 0.5084717799132713\n",
      "Epoch 128, Loss: 0.5079809643002384\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2658554360513255\n",
      "Valence RMSE: 0.25056619130701224\n",
      "Arousal RMSE: 0.2803119860619209\n",
      "Test R^2 score: tensor([0.2391, 0.2220], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1486, 0.1294], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.23055270065708955\n",
      "Num of epochs: 129\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Epoch 121, Loss: 0.5116998508836337\n",
      "Epoch 122, Loss: 0.5111129902664598\n",
      "Epoch 123, Loss: 0.5105405741436312\n",
      "Epoch 124, Loss: 0.5099849010083686\n",
      "Epoch 125, Loss: 0.5094623759868415\n",
      "Epoch 126, Loss: 0.5089652841754122\n",
      "Epoch 127, Loss: 0.5084717799132713\n",
      "Epoch 128, Loss: 0.5079809643002384\n",
      "Epoch 129, Loss: 0.507507731608986\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.26555782057802507\n",
      "Valence RMSE: 0.2503146276148995\n",
      "Arousal RMSE: 0.27997231888584145\n",
      "Test R^2 score: tensor([0.2407, 0.2239], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1503, 0.1315], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.2322584134773103\n",
      "Num of epochs: 130\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Epoch 121, Loss: 0.5116998508836337\n",
      "Epoch 122, Loss: 0.5111129902664598\n",
      "Epoch 123, Loss: 0.5105405741436312\n",
      "Epoch 124, Loss: 0.5099849010083686\n",
      "Epoch 125, Loss: 0.5094623759868415\n",
      "Epoch 126, Loss: 0.5089652841754122\n",
      "Epoch 127, Loss: 0.5084717799132713\n",
      "Epoch 128, Loss: 0.5079809643002384\n",
      "Epoch 129, Loss: 0.507507731608986\n",
      "Epoch 130, Loss: 0.5070515727009828\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2652967505008135\n",
      "Valence RMSE: 0.2501047255005732\n",
      "Arousal RMSE: 0.27966472415173277\n",
      "Test R^2 score: tensor([0.2419, 0.2256], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1517, 0.1334], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.23374714214878833\n",
      "Num of epochs: 131\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Epoch 121, Loss: 0.5116998508836337\n",
      "Epoch 122, Loss: 0.5111129902664598\n",
      "Epoch 123, Loss: 0.5105405741436312\n",
      "Epoch 124, Loss: 0.5099849010083686\n",
      "Epoch 125, Loss: 0.5094623759868415\n",
      "Epoch 126, Loss: 0.5089652841754122\n",
      "Epoch 127, Loss: 0.5084717799132713\n",
      "Epoch 128, Loss: 0.5079809643002384\n",
      "Epoch 129, Loss: 0.507507731608986\n",
      "Epoch 130, Loss: 0.5070515727009828\n",
      "Epoch 131, Loss: 0.5065962678643066\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2650791168083288\n",
      "Valence RMSE: 0.2499580278325103\n",
      "Arousal RMSE: 0.27938299994425075\n",
      "Test R^2 score: tensor([0.2428, 0.2271], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1527, 0.1352], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.23497139887116303\n",
      "Num of epochs: 132\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Epoch 121, Loss: 0.5116998508836337\n",
      "Epoch 122, Loss: 0.5111129902664598\n",
      "Epoch 123, Loss: 0.5105405741436312\n",
      "Epoch 124, Loss: 0.5099849010083686\n",
      "Epoch 125, Loss: 0.5094623759868415\n",
      "Epoch 126, Loss: 0.5089652841754122\n",
      "Epoch 127, Loss: 0.5084717799132713\n",
      "Epoch 128, Loss: 0.5079809643002384\n",
      "Epoch 129, Loss: 0.507507731608986\n",
      "Epoch 130, Loss: 0.5070515727009828\n",
      "Epoch 131, Loss: 0.5065962678643066\n",
      "Epoch 132, Loss: 0.5061448812256618\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.26488303897439736\n",
      "Valence RMSE: 0.24982935049355084\n",
      "Arousal RMSE: 0.27912603659385626\n",
      "Test R^2 score: tensor([0.2436, 0.2285], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1536, 0.1368], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.23607161963440776\n",
      "Num of epochs: 133\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Epoch 121, Loss: 0.5116998508836337\n",
      "Epoch 122, Loss: 0.5111129902664598\n",
      "Epoch 123, Loss: 0.5105405741436312\n",
      "Epoch 124, Loss: 0.5099849010083686\n",
      "Epoch 125, Loss: 0.5094623759868415\n",
      "Epoch 126, Loss: 0.5089652841754122\n",
      "Epoch 127, Loss: 0.5084717799132713\n",
      "Epoch 128, Loss: 0.5079809643002384\n",
      "Epoch 129, Loss: 0.507507731608986\n",
      "Epoch 130, Loss: 0.5070515727009828\n",
      "Epoch 131, Loss: 0.5065962678643066\n",
      "Epoch 132, Loss: 0.5061448812256618\n",
      "Epoch 133, Loss: 0.5057069703510908\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2646611253046298\n",
      "Valence RMSE: 0.24964009327110384\n",
      "Arousal RMSE: 0.2788742482313857\n",
      "Test R^2 score: tensor([0.2447, 0.2299], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1549, 0.1383], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.23733999608769457\n",
      "Num of epochs: 134\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Epoch 121, Loss: 0.5116998508836337\n",
      "Epoch 122, Loss: 0.5111129902664598\n",
      "Epoch 123, Loss: 0.5105405741436312\n",
      "Epoch 124, Loss: 0.5099849010083686\n",
      "Epoch 125, Loss: 0.5094623759868415\n",
      "Epoch 126, Loss: 0.5089652841754122\n",
      "Epoch 127, Loss: 0.5084717799132713\n",
      "Epoch 128, Loss: 0.5079809643002384\n",
      "Epoch 129, Loss: 0.507507731608986\n",
      "Epoch 130, Loss: 0.5070515727009828\n",
      "Epoch 131, Loss: 0.5065962678643066\n",
      "Epoch 132, Loss: 0.5061448812256618\n",
      "Epoch 133, Loss: 0.5057069703510908\n",
      "Epoch 134, Loss: 0.5052756693952567\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.26439556253318736\n",
      "Valence RMSE: 0.2493813401582352\n",
      "Arousal RMSE: 0.27860182008623663\n",
      "Test R^2 score: tensor([0.2463, 0.2314], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1566, 0.1400], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.23887431359315042\n",
      "Num of epochs: 135\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Epoch 121, Loss: 0.5116998508836337\n",
      "Epoch 122, Loss: 0.5111129902664598\n",
      "Epoch 123, Loss: 0.5105405741436312\n",
      "Epoch 124, Loss: 0.5099849010083686\n",
      "Epoch 125, Loss: 0.5094623759868415\n",
      "Epoch 126, Loss: 0.5089652841754122\n",
      "Epoch 127, Loss: 0.5084717799132713\n",
      "Epoch 128, Loss: 0.5079809643002384\n",
      "Epoch 129, Loss: 0.507507731608986\n",
      "Epoch 130, Loss: 0.5070515727009828\n",
      "Epoch 131, Loss: 0.5065962678643066\n",
      "Epoch 132, Loss: 0.5061448812256618\n",
      "Epoch 133, Loss: 0.5057069703510908\n",
      "Epoch 134, Loss: 0.5052756693952567\n",
      "Epoch 135, Loss: 0.5048445017462813\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2641060707748241\n",
      "Valence RMSE: 0.2490826018311161\n",
      "Arousal RMSE: 0.2783197634112146\n",
      "Test R^2 score: tensor([0.2481, 0.2330], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1587, 0.1417], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.24055433019903505\n",
      "Num of epochs: 136\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Epoch 121, Loss: 0.5116998508836337\n",
      "Epoch 122, Loss: 0.5111129902664598\n",
      "Epoch 123, Loss: 0.5105405741436312\n",
      "Epoch 124, Loss: 0.5099849010083686\n",
      "Epoch 125, Loss: 0.5094623759868415\n",
      "Epoch 126, Loss: 0.5089652841754122\n",
      "Epoch 127, Loss: 0.5084717799132713\n",
      "Epoch 128, Loss: 0.5079809643002384\n",
      "Epoch 129, Loss: 0.507507731608986\n",
      "Epoch 130, Loss: 0.5070515727009828\n",
      "Epoch 131, Loss: 0.5065962678643066\n",
      "Epoch 132, Loss: 0.5061448812256618\n",
      "Epoch 133, Loss: 0.5057069703510908\n",
      "Epoch 134, Loss: 0.5052756693952567\n",
      "Epoch 135, Loss: 0.5048445017462813\n",
      "Epoch 136, Loss: 0.504425284231979\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.26382655008298783\n",
      "Valence RMSE: 0.24878978988838626\n",
      "Arousal RMSE: 0.27805132170999514\n",
      "Test R^2 score: tensor([0.2499, 0.2345], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1606, 0.1434], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.24217712569648953\n",
      "Num of epochs: 137\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Epoch 121, Loss: 0.5116998508836337\n",
      "Epoch 122, Loss: 0.5111129902664598\n",
      "Epoch 123, Loss: 0.5105405741436312\n",
      "Epoch 124, Loss: 0.5099849010083686\n",
      "Epoch 125, Loss: 0.5094623759868415\n",
      "Epoch 126, Loss: 0.5089652841754122\n",
      "Epoch 127, Loss: 0.5084717799132713\n",
      "Epoch 128, Loss: 0.5079809643002384\n",
      "Epoch 129, Loss: 0.507507731608986\n",
      "Epoch 130, Loss: 0.5070515727009828\n",
      "Epoch 131, Loss: 0.5065962678643066\n",
      "Epoch 132, Loss: 0.5061448812256618\n",
      "Epoch 133, Loss: 0.5057069703510908\n",
      "Epoch 134, Loss: 0.5052756693952567\n",
      "Epoch 135, Loss: 0.5048445017462813\n",
      "Epoch 136, Loss: 0.504425284231979\n",
      "Epoch 137, Loss: 0.504022717877823\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2636072514470406\n",
      "Valence RMSE: 0.248570311773124\n",
      "Arousal RMSE: 0.2778315427303721\n",
      "Test R^2 score: tensor([0.2512, 0.2357], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1621, 0.1448], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.24344342933008978\n",
      "Num of epochs: 138\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Epoch 121, Loss: 0.5116998508836337\n",
      "Epoch 122, Loss: 0.5111129902664598\n",
      "Epoch 123, Loss: 0.5105405741436312\n",
      "Epoch 124, Loss: 0.5099849010083686\n",
      "Epoch 125, Loss: 0.5094623759868415\n",
      "Epoch 126, Loss: 0.5089652841754122\n",
      "Epoch 127, Loss: 0.5084717799132713\n",
      "Epoch 128, Loss: 0.5079809643002384\n",
      "Epoch 129, Loss: 0.507507731608986\n",
      "Epoch 130, Loss: 0.5070515727009828\n",
      "Epoch 131, Loss: 0.5065962678643066\n",
      "Epoch 132, Loss: 0.5061448812256618\n",
      "Epoch 133, Loss: 0.5057069703510908\n",
      "Epoch 134, Loss: 0.5052756693952567\n",
      "Epoch 135, Loss: 0.5048445017462813\n",
      "Epoch 136, Loss: 0.504425284231979\n",
      "Epoch 137, Loss: 0.504022717877823\n",
      "Epoch 138, Loss: 0.5036312802042249\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2634712729233872\n",
      "Valence RMSE: 0.24843421019902384\n",
      "Arousal RMSE: 0.27769527636336777\n",
      "Test R^2 score: tensor([0.2520, 0.2364], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1630, 0.1456], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.24422808926363793\n",
      "Num of epochs: 139\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Epoch 121, Loss: 0.5116998508836337\n",
      "Epoch 122, Loss: 0.5111129902664598\n",
      "Epoch 123, Loss: 0.5105405741436312\n",
      "Epoch 124, Loss: 0.5099849010083686\n",
      "Epoch 125, Loss: 0.5094623759868415\n",
      "Epoch 126, Loss: 0.5089652841754122\n",
      "Epoch 127, Loss: 0.5084717799132713\n",
      "Epoch 128, Loss: 0.5079809643002384\n",
      "Epoch 129, Loss: 0.507507731608986\n",
      "Epoch 130, Loss: 0.5070515727009828\n",
      "Epoch 131, Loss: 0.5065962678643066\n",
      "Epoch 132, Loss: 0.5061448812256618\n",
      "Epoch 133, Loss: 0.5057069703510908\n",
      "Epoch 134, Loss: 0.5052756693952567\n",
      "Epoch 135, Loss: 0.5048445017462813\n",
      "Epoch 136, Loss: 0.504425284231979\n",
      "Epoch 137, Loss: 0.504022717877823\n",
      "Epoch 138, Loss: 0.5036312802042249\n",
      "Epoch 139, Loss: 0.5032368731068207\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2633693745787758\n",
      "Valence RMSE: 0.2483272860695265\n",
      "Arousal RMSE: 0.27759757550341574\n",
      "Test R^2 score: tensor([0.2527, 0.2370], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1638, 0.1462], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.24481854018182153\n",
      "Num of epochs: 140\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Epoch 121, Loss: 0.5116998508836337\n",
      "Epoch 122, Loss: 0.5111129902664598\n",
      "Epoch 123, Loss: 0.5105405741436312\n",
      "Epoch 124, Loss: 0.5099849010083686\n",
      "Epoch 125, Loss: 0.5094623759868415\n",
      "Epoch 126, Loss: 0.5089652841754122\n",
      "Epoch 127, Loss: 0.5084717799132713\n",
      "Epoch 128, Loss: 0.5079809643002384\n",
      "Epoch 129, Loss: 0.507507731608986\n",
      "Epoch 130, Loss: 0.5070515727009828\n",
      "Epoch 131, Loss: 0.5065962678643066\n",
      "Epoch 132, Loss: 0.5061448812256618\n",
      "Epoch 133, Loss: 0.5057069703510908\n",
      "Epoch 134, Loss: 0.5052756693952567\n",
      "Epoch 135, Loss: 0.5048445017462813\n",
      "Epoch 136, Loss: 0.504425284231979\n",
      "Epoch 137, Loss: 0.504022717877823\n",
      "Epoch 138, Loss: 0.5036312802042249\n",
      "Epoch 139, Loss: 0.5032368731068207\n",
      "Epoch 140, Loss: 0.5028511060044271\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.263266886557522\n",
      "Valence RMSE: 0.2482128543302116\n",
      "Arousal RMSE: 0.2775054703255982\n",
      "Test R^2 score: tensor([0.2534, 0.2375], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1645, 0.1468], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.24541596644759595\n",
      "Num of epochs: 141\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Epoch 121, Loss: 0.5116998508836337\n",
      "Epoch 122, Loss: 0.5111129902664598\n",
      "Epoch 123, Loss: 0.5105405741436312\n",
      "Epoch 124, Loss: 0.5099849010083686\n",
      "Epoch 125, Loss: 0.5094623759868415\n",
      "Epoch 126, Loss: 0.5089652841754122\n",
      "Epoch 127, Loss: 0.5084717799132713\n",
      "Epoch 128, Loss: 0.5079809643002384\n",
      "Epoch 129, Loss: 0.507507731608986\n",
      "Epoch 130, Loss: 0.5070515727009828\n",
      "Epoch 131, Loss: 0.5065962678643066\n",
      "Epoch 132, Loss: 0.5061448812256618\n",
      "Epoch 133, Loss: 0.5057069703510908\n",
      "Epoch 134, Loss: 0.5052756693952567\n",
      "Epoch 135, Loss: 0.5048445017462813\n",
      "Epoch 136, Loss: 0.504425284231979\n",
      "Epoch 137, Loss: 0.504022717877823\n",
      "Epoch 138, Loss: 0.5036312802042249\n",
      "Epoch 139, Loss: 0.5032368731068207\n",
      "Epoch 140, Loss: 0.5028511060044271\n",
      "Epoch 141, Loss: 0.5024804933282331\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.263131058518931\n",
      "Valence RMSE: 0.24806285887994475\n",
      "Arousal RMSE: 0.2773819135396084\n",
      "Test R^2 score: tensor([0.2543, 0.2382], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1655, 0.1475], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.24620646011749686\n",
      "Num of epochs: 142\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Epoch 121, Loss: 0.5116998508836337\n",
      "Epoch 122, Loss: 0.5111129902664598\n",
      "Epoch 123, Loss: 0.5105405741436312\n",
      "Epoch 124, Loss: 0.5099849010083686\n",
      "Epoch 125, Loss: 0.5094623759868415\n",
      "Epoch 126, Loss: 0.5089652841754122\n",
      "Epoch 127, Loss: 0.5084717799132713\n",
      "Epoch 128, Loss: 0.5079809643002384\n",
      "Epoch 129, Loss: 0.507507731608986\n",
      "Epoch 130, Loss: 0.5070515727009828\n",
      "Epoch 131, Loss: 0.5065962678643066\n",
      "Epoch 132, Loss: 0.5061448812256618\n",
      "Epoch 133, Loss: 0.5057069703510908\n",
      "Epoch 134, Loss: 0.5052756693952567\n",
      "Epoch 135, Loss: 0.5048445017462813\n",
      "Epoch 136, Loss: 0.504425284231979\n",
      "Epoch 137, Loss: 0.504022717877823\n",
      "Epoch 138, Loss: 0.5036312802042249\n",
      "Epoch 139, Loss: 0.5032368731068207\n",
      "Epoch 140, Loss: 0.5028511060044271\n",
      "Epoch 141, Loss: 0.5024804933282331\n",
      "Epoch 142, Loss: 0.5021182727396313\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2629742428406603\n",
      "Valence RMSE: 0.24790071019640755\n",
      "Arousal RMSE: 0.27722940442773\n",
      "Test R^2 score: tensor([0.2552, 0.2390], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1666, 0.1485], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.2471125222643995\n",
      "Num of epochs: 143\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Epoch 121, Loss: 0.5116998508836337\n",
      "Epoch 122, Loss: 0.5111129902664598\n",
      "Epoch 123, Loss: 0.5105405741436312\n",
      "Epoch 124, Loss: 0.5099849010083686\n",
      "Epoch 125, Loss: 0.5094623759868415\n",
      "Epoch 126, Loss: 0.5089652841754122\n",
      "Epoch 127, Loss: 0.5084717799132713\n",
      "Epoch 128, Loss: 0.5079809643002384\n",
      "Epoch 129, Loss: 0.507507731608986\n",
      "Epoch 130, Loss: 0.5070515727009828\n",
      "Epoch 131, Loss: 0.5065962678643066\n",
      "Epoch 132, Loss: 0.5061448812256618\n",
      "Epoch 133, Loss: 0.5057069703510908\n",
      "Epoch 134, Loss: 0.5052756693952567\n",
      "Epoch 135, Loss: 0.5048445017462813\n",
      "Epoch 136, Loss: 0.504425284231979\n",
      "Epoch 137, Loss: 0.504022717877823\n",
      "Epoch 138, Loss: 0.5036312802042249\n",
      "Epoch 139, Loss: 0.5032368731068207\n",
      "Epoch 140, Loss: 0.5028511060044271\n",
      "Epoch 141, Loss: 0.5024804933282331\n",
      "Epoch 142, Loss: 0.5021182727396313\n",
      "Epoch 143, Loss: 0.5017581664990297\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2628317801598701\n",
      "Valence RMSE: 0.24775704136020085\n",
      "Arousal RMSE: 0.2770875994707311\n",
      "Test R^2 score: tensor([0.2561, 0.2398], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1676, 0.1493], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.24793318286701604\n",
      "Num of epochs: 144\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Epoch 121, Loss: 0.5116998508836337\n",
      "Epoch 122, Loss: 0.5111129902664598\n",
      "Epoch 123, Loss: 0.5105405741436312\n",
      "Epoch 124, Loss: 0.5099849010083686\n",
      "Epoch 125, Loss: 0.5094623759868415\n",
      "Epoch 126, Loss: 0.5089652841754122\n",
      "Epoch 127, Loss: 0.5084717799132713\n",
      "Epoch 128, Loss: 0.5079809643002384\n",
      "Epoch 129, Loss: 0.507507731608986\n",
      "Epoch 130, Loss: 0.5070515727009828\n",
      "Epoch 131, Loss: 0.5065962678643066\n",
      "Epoch 132, Loss: 0.5061448812256618\n",
      "Epoch 133, Loss: 0.5057069703510908\n",
      "Epoch 134, Loss: 0.5052756693952567\n",
      "Epoch 135, Loss: 0.5048445017462813\n",
      "Epoch 136, Loss: 0.504425284231979\n",
      "Epoch 137, Loss: 0.504022717877823\n",
      "Epoch 138, Loss: 0.5036312802042249\n",
      "Epoch 139, Loss: 0.5032368731068207\n",
      "Epoch 140, Loss: 0.5028511060044271\n",
      "Epoch 141, Loss: 0.5024804933282331\n",
      "Epoch 142, Loss: 0.5021182727396313\n",
      "Epoch 143, Loss: 0.5017581664990297\n",
      "Epoch 144, Loss: 0.5014050827892457\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2627290710848155\n",
      "Valence RMSE: 0.24765313079214765\n",
      "Arousal RMSE: 0.2769856609917334\n",
      "Test R^2 score: tensor([0.2567, 0.2403], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1683, 0.1500], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.2485247461458327\n",
      "Num of epochs: 145\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Epoch 121, Loss: 0.5116998508836337\n",
      "Epoch 122, Loss: 0.5111129902664598\n",
      "Epoch 123, Loss: 0.5105405741436312\n",
      "Epoch 124, Loss: 0.5099849010083686\n",
      "Epoch 125, Loss: 0.5094623759868415\n",
      "Epoch 126, Loss: 0.5089652841754122\n",
      "Epoch 127, Loss: 0.5084717799132713\n",
      "Epoch 128, Loss: 0.5079809643002384\n",
      "Epoch 129, Loss: 0.507507731608986\n",
      "Epoch 130, Loss: 0.5070515727009828\n",
      "Epoch 131, Loss: 0.5065962678643066\n",
      "Epoch 132, Loss: 0.5061448812256618\n",
      "Epoch 133, Loss: 0.5057069703510908\n",
      "Epoch 134, Loss: 0.5052756693952567\n",
      "Epoch 135, Loss: 0.5048445017462813\n",
      "Epoch 136, Loss: 0.504425284231979\n",
      "Epoch 137, Loss: 0.504022717877823\n",
      "Epoch 138, Loss: 0.5036312802042249\n",
      "Epoch 139, Loss: 0.5032368731068207\n",
      "Epoch 140, Loss: 0.5028511060044271\n",
      "Epoch 141, Loss: 0.5024804933282331\n",
      "Epoch 142, Loss: 0.5021182727396313\n",
      "Epoch 143, Loss: 0.5017581664990297\n",
      "Epoch 144, Loss: 0.5014050827892457\n",
      "Epoch 145, Loss: 0.501047675902349\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.26266834010815937\n",
      "Valence RMSE: 0.247601576196066\n",
      "Arousal RMSE: 0.2769165456515285\n",
      "Test R^2 score: tensor([0.2570, 0.2407], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1686, 0.1504], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.24886899492623304\n",
      "Num of epochs: 146\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Epoch 121, Loss: 0.5116998508836337\n",
      "Epoch 122, Loss: 0.5111129902664598\n",
      "Epoch 123, Loss: 0.5105405741436312\n",
      "Epoch 124, Loss: 0.5099849010083686\n",
      "Epoch 125, Loss: 0.5094623759868415\n",
      "Epoch 126, Loss: 0.5089652841754122\n",
      "Epoch 127, Loss: 0.5084717799132713\n",
      "Epoch 128, Loss: 0.5079809643002384\n",
      "Epoch 129, Loss: 0.507507731608986\n",
      "Epoch 130, Loss: 0.5070515727009828\n",
      "Epoch 131, Loss: 0.5065962678643066\n",
      "Epoch 132, Loss: 0.5061448812256618\n",
      "Epoch 133, Loss: 0.5057069703510908\n",
      "Epoch 134, Loss: 0.5052756693952567\n",
      "Epoch 135, Loss: 0.5048445017462813\n",
      "Epoch 136, Loss: 0.504425284231979\n",
      "Epoch 137, Loss: 0.504022717877823\n",
      "Epoch 138, Loss: 0.5036312802042249\n",
      "Epoch 139, Loss: 0.5032368731068207\n",
      "Epoch 140, Loss: 0.5028511060044271\n",
      "Epoch 141, Loss: 0.5024804933282331\n",
      "Epoch 142, Loss: 0.5021182727396313\n",
      "Epoch 143, Loss: 0.5017581664990297\n",
      "Epoch 144, Loss: 0.5014050827892457\n",
      "Epoch 145, Loss: 0.501047675902349\n",
      "Epoch 146, Loss: 0.500690966247362\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2626156484745696\n",
      "Valence RMSE: 0.24756265744559552\n",
      "Arousal RMSE: 0.2768513830305208\n",
      "Test R^2 score: tensor([0.2573, 0.2411], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1689, 0.1508], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.24916441947506845\n",
      "Num of epochs: 147\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Epoch 121, Loss: 0.5116998508836337\n",
      "Epoch 122, Loss: 0.5111129902664598\n",
      "Epoch 123, Loss: 0.5105405741436312\n",
      "Epoch 124, Loss: 0.5099849010083686\n",
      "Epoch 125, Loss: 0.5094623759868415\n",
      "Epoch 126, Loss: 0.5089652841754122\n",
      "Epoch 127, Loss: 0.5084717799132713\n",
      "Epoch 128, Loss: 0.5079809643002384\n",
      "Epoch 129, Loss: 0.507507731608986\n",
      "Epoch 130, Loss: 0.5070515727009828\n",
      "Epoch 131, Loss: 0.5065962678643066\n",
      "Epoch 132, Loss: 0.5061448812256618\n",
      "Epoch 133, Loss: 0.5057069703510908\n",
      "Epoch 134, Loss: 0.5052756693952567\n",
      "Epoch 135, Loss: 0.5048445017462813\n",
      "Epoch 136, Loss: 0.504425284231979\n",
      "Epoch 137, Loss: 0.504022717877823\n",
      "Epoch 138, Loss: 0.5036312802042249\n",
      "Epoch 139, Loss: 0.5032368731068207\n",
      "Epoch 140, Loss: 0.5028511060044271\n",
      "Epoch 141, Loss: 0.5024804933282331\n",
      "Epoch 142, Loss: 0.5021182727396313\n",
      "Epoch 143, Loss: 0.5017581664990297\n",
      "Epoch 144, Loss: 0.5014050827892457\n",
      "Epoch 145, Loss: 0.501047675902349\n",
      "Epoch 146, Loss: 0.500690966247362\n",
      "Epoch 147, Loss: 0.5003476722262875\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.26254315868740896\n",
      "Valence RMSE: 0.24751150326791196\n",
      "Arousal RMSE: 0.2767595998284114\n",
      "Test R^2 score: tensor([0.2576, 0.2416], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1692, 0.1513], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.2495694399571058\n",
      "Num of epochs: 148\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Epoch 121, Loss: 0.5116998508836337\n",
      "Epoch 122, Loss: 0.5111129902664598\n",
      "Epoch 123, Loss: 0.5105405741436312\n",
      "Epoch 124, Loss: 0.5099849010083686\n",
      "Epoch 125, Loss: 0.5094623759868415\n",
      "Epoch 126, Loss: 0.5089652841754122\n",
      "Epoch 127, Loss: 0.5084717799132713\n",
      "Epoch 128, Loss: 0.5079809643002384\n",
      "Epoch 129, Loss: 0.507507731608986\n",
      "Epoch 130, Loss: 0.5070515727009828\n",
      "Epoch 131, Loss: 0.5065962678643066\n",
      "Epoch 132, Loss: 0.5061448812256618\n",
      "Epoch 133, Loss: 0.5057069703510908\n",
      "Epoch 134, Loss: 0.5052756693952567\n",
      "Epoch 135, Loss: 0.5048445017462813\n",
      "Epoch 136, Loss: 0.504425284231979\n",
      "Epoch 137, Loss: 0.504022717877823\n",
      "Epoch 138, Loss: 0.5036312802042249\n",
      "Epoch 139, Loss: 0.5032368731068207\n",
      "Epoch 140, Loss: 0.5028511060044271\n",
      "Epoch 141, Loss: 0.5024804933282331\n",
      "Epoch 142, Loss: 0.5021182727396313\n",
      "Epoch 143, Loss: 0.5017581664990297\n",
      "Epoch 144, Loss: 0.5014050827892457\n",
      "Epoch 145, Loss: 0.501047675902349\n",
      "Epoch 146, Loss: 0.500690966247362\n",
      "Epoch 147, Loss: 0.5003476722262875\n",
      "Epoch 148, Loss: 0.5000068544871652\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2624392865225576\n",
      "Valence RMSE: 0.24743006015231073\n",
      "Arousal RMSE: 0.27663536208149897\n",
      "Test R^2 score: tensor([0.2581, 0.2423], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1698, 0.1521], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.25015407863912176\n",
      "Num of epochs: 149\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Epoch 121, Loss: 0.5116998508836337\n",
      "Epoch 122, Loss: 0.5111129902664598\n",
      "Epoch 123, Loss: 0.5105405741436312\n",
      "Epoch 124, Loss: 0.5099849010083686\n",
      "Epoch 125, Loss: 0.5094623759868415\n",
      "Epoch 126, Loss: 0.5089652841754122\n",
      "Epoch 127, Loss: 0.5084717799132713\n",
      "Epoch 128, Loss: 0.5079809643002384\n",
      "Epoch 129, Loss: 0.507507731608986\n",
      "Epoch 130, Loss: 0.5070515727009828\n",
      "Epoch 131, Loss: 0.5065962678643066\n",
      "Epoch 132, Loss: 0.5061448812256618\n",
      "Epoch 133, Loss: 0.5057069703510908\n",
      "Epoch 134, Loss: 0.5052756693952567\n",
      "Epoch 135, Loss: 0.5048445017462813\n",
      "Epoch 136, Loss: 0.504425284231979\n",
      "Epoch 137, Loss: 0.504022717877823\n",
      "Epoch 138, Loss: 0.5036312802042249\n",
      "Epoch 139, Loss: 0.5032368731068207\n",
      "Epoch 140, Loss: 0.5028511060044271\n",
      "Epoch 141, Loss: 0.5024804933282331\n",
      "Epoch 142, Loss: 0.5021182727396313\n",
      "Epoch 143, Loss: 0.5017581664990297\n",
      "Epoch 144, Loss: 0.5014050827892457\n",
      "Epoch 145, Loss: 0.501047675902349\n",
      "Epoch 146, Loss: 0.500690966247362\n",
      "Epoch 147, Loss: 0.5003476722262875\n",
      "Epoch 148, Loss: 0.5000068544871652\n",
      "Epoch 149, Loss: 0.49966338867073357\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2623319052289919\n",
      "Valence RMSE: 0.2473315657634086\n",
      "Arousal RMSE: 0.27651971643838763\n",
      "Test R^2 score: tensor([0.2586, 0.2429], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1704, 0.1528], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.25076607062812933\n",
      "Num of epochs: 150\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Epoch 121, Loss: 0.5116998508836337\n",
      "Epoch 122, Loss: 0.5111129902664598\n",
      "Epoch 123, Loss: 0.5105405741436312\n",
      "Epoch 124, Loss: 0.5099849010083686\n",
      "Epoch 125, Loss: 0.5094623759868415\n",
      "Epoch 126, Loss: 0.5089652841754122\n",
      "Epoch 127, Loss: 0.5084717799132713\n",
      "Epoch 128, Loss: 0.5079809643002384\n",
      "Epoch 129, Loss: 0.507507731608986\n",
      "Epoch 130, Loss: 0.5070515727009828\n",
      "Epoch 131, Loss: 0.5065962678643066\n",
      "Epoch 132, Loss: 0.5061448812256618\n",
      "Epoch 133, Loss: 0.5057069703510908\n",
      "Epoch 134, Loss: 0.5052756693952567\n",
      "Epoch 135, Loss: 0.5048445017462813\n",
      "Epoch 136, Loss: 0.504425284231979\n",
      "Epoch 137, Loss: 0.504022717877823\n",
      "Epoch 138, Loss: 0.5036312802042249\n",
      "Epoch 139, Loss: 0.5032368731068207\n",
      "Epoch 140, Loss: 0.5028511060044271\n",
      "Epoch 141, Loss: 0.5024804933282331\n",
      "Epoch 142, Loss: 0.5021182727396313\n",
      "Epoch 143, Loss: 0.5017581664990297\n",
      "Epoch 144, Loss: 0.5014050827892457\n",
      "Epoch 145, Loss: 0.501047675902349\n",
      "Epoch 146, Loss: 0.500690966247362\n",
      "Epoch 147, Loss: 0.5003476722262875\n",
      "Epoch 148, Loss: 0.5000068544871652\n",
      "Epoch 149, Loss: 0.49966338867073357\n",
      "Epoch 150, Loss: 0.4993280873087794\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2622628195166935\n",
      "Valence RMSE: 0.24726174459592118\n",
      "Arousal RMSE: 0.2764510854763875\n",
      "Test R^2 score: tensor([0.2591, 0.2433], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1709, 0.1532], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.2511632126970724\n",
      "Num of epochs: 151\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Epoch 121, Loss: 0.5116998508836337\n",
      "Epoch 122, Loss: 0.5111129902664598\n",
      "Epoch 123, Loss: 0.5105405741436312\n",
      "Epoch 124, Loss: 0.5099849010083686\n",
      "Epoch 125, Loss: 0.5094623759868415\n",
      "Epoch 126, Loss: 0.5089652841754122\n",
      "Epoch 127, Loss: 0.5084717799132713\n",
      "Epoch 128, Loss: 0.5079809643002384\n",
      "Epoch 129, Loss: 0.507507731608986\n",
      "Epoch 130, Loss: 0.5070515727009828\n",
      "Epoch 131, Loss: 0.5065962678643066\n",
      "Epoch 132, Loss: 0.5061448812256618\n",
      "Epoch 133, Loss: 0.5057069703510908\n",
      "Epoch 134, Loss: 0.5052756693952567\n",
      "Epoch 135, Loss: 0.5048445017462813\n",
      "Epoch 136, Loss: 0.504425284231979\n",
      "Epoch 137, Loss: 0.504022717877823\n",
      "Epoch 138, Loss: 0.5036312802042249\n",
      "Epoch 139, Loss: 0.5032368731068207\n",
      "Epoch 140, Loss: 0.5028511060044271\n",
      "Epoch 141, Loss: 0.5024804933282331\n",
      "Epoch 142, Loss: 0.5021182727396313\n",
      "Epoch 143, Loss: 0.5017581664990297\n",
      "Epoch 144, Loss: 0.5014050827892457\n",
      "Epoch 145, Loss: 0.501047675902349\n",
      "Epoch 146, Loss: 0.500690966247362\n",
      "Epoch 147, Loss: 0.5003476722262875\n",
      "Epoch 148, Loss: 0.5000068544871652\n",
      "Epoch 149, Loss: 0.49966338867073357\n",
      "Epoch 150, Loss: 0.4993280873087794\n",
      "Epoch 151, Loss: 0.49900257940428383\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.26222494739894325\n",
      "Valence RMSE: 0.24721598369829567\n",
      "Arousal RMSE: 0.27642015751540466\n",
      "Test R^2 score: tensor([0.2593, 0.2434], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1712, 0.1534], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.25138498077732985\n",
      "Num of epochs: 152\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Epoch 121, Loss: 0.5116998508836337\n",
      "Epoch 122, Loss: 0.5111129902664598\n",
      "Epoch 123, Loss: 0.5105405741436312\n",
      "Epoch 124, Loss: 0.5099849010083686\n",
      "Epoch 125, Loss: 0.5094623759868415\n",
      "Epoch 126, Loss: 0.5089652841754122\n",
      "Epoch 127, Loss: 0.5084717799132713\n",
      "Epoch 128, Loss: 0.5079809643002384\n",
      "Epoch 129, Loss: 0.507507731608986\n",
      "Epoch 130, Loss: 0.5070515727009828\n",
      "Epoch 131, Loss: 0.5065962678643066\n",
      "Epoch 132, Loss: 0.5061448812256618\n",
      "Epoch 133, Loss: 0.5057069703510908\n",
      "Epoch 134, Loss: 0.5052756693952567\n",
      "Epoch 135, Loss: 0.5048445017462813\n",
      "Epoch 136, Loss: 0.504425284231979\n",
      "Epoch 137, Loss: 0.504022717877823\n",
      "Epoch 138, Loss: 0.5036312802042249\n",
      "Epoch 139, Loss: 0.5032368731068207\n",
      "Epoch 140, Loss: 0.5028511060044271\n",
      "Epoch 141, Loss: 0.5024804933282331\n",
      "Epoch 142, Loss: 0.5021182727396313\n",
      "Epoch 143, Loss: 0.5017581664990297\n",
      "Epoch 144, Loss: 0.5014050827892457\n",
      "Epoch 145, Loss: 0.501047675902349\n",
      "Epoch 146, Loss: 0.500690966247362\n",
      "Epoch 147, Loss: 0.5003476722262875\n",
      "Epoch 148, Loss: 0.5000068544871652\n",
      "Epoch 149, Loss: 0.49966338867073357\n",
      "Epoch 150, Loss: 0.4993280873087794\n",
      "Epoch 151, Loss: 0.49900257940428383\n",
      "Epoch 152, Loss: 0.4986865554458311\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.26220415565969085\n",
      "Valence RMSE: 0.2471833624125399\n",
      "Arousal RMSE: 0.27640988375391673\n",
      "Test R^2 score: tensor([0.2595, 0.2435], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1714, 0.1535], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.2515108270208005\n",
      "Num of epochs: 153\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Epoch 121, Loss: 0.5116998508836337\n",
      "Epoch 122, Loss: 0.5111129902664598\n",
      "Epoch 123, Loss: 0.5105405741436312\n",
      "Epoch 124, Loss: 0.5099849010083686\n",
      "Epoch 125, Loss: 0.5094623759868415\n",
      "Epoch 126, Loss: 0.5089652841754122\n",
      "Epoch 127, Loss: 0.5084717799132713\n",
      "Epoch 128, Loss: 0.5079809643002384\n",
      "Epoch 129, Loss: 0.507507731608986\n",
      "Epoch 130, Loss: 0.5070515727009828\n",
      "Epoch 131, Loss: 0.5065962678643066\n",
      "Epoch 132, Loss: 0.5061448812256618\n",
      "Epoch 133, Loss: 0.5057069703510908\n",
      "Epoch 134, Loss: 0.5052756693952567\n",
      "Epoch 135, Loss: 0.5048445017462813\n",
      "Epoch 136, Loss: 0.504425284231979\n",
      "Epoch 137, Loss: 0.504022717877823\n",
      "Epoch 138, Loss: 0.5036312802042249\n",
      "Epoch 139, Loss: 0.5032368731068207\n",
      "Epoch 140, Loss: 0.5028511060044271\n",
      "Epoch 141, Loss: 0.5024804933282331\n",
      "Epoch 142, Loss: 0.5021182727396313\n",
      "Epoch 143, Loss: 0.5017581664990297\n",
      "Epoch 144, Loss: 0.5014050827892457\n",
      "Epoch 145, Loss: 0.501047675902349\n",
      "Epoch 146, Loss: 0.500690966247362\n",
      "Epoch 147, Loss: 0.5003476722262875\n",
      "Epoch 148, Loss: 0.5000068544871652\n",
      "Epoch 149, Loss: 0.49966338867073357\n",
      "Epoch 150, Loss: 0.4993280873087794\n",
      "Epoch 151, Loss: 0.49900257940428383\n",
      "Epoch 152, Loss: 0.4986865554458311\n",
      "Epoch 153, Loss: 0.49836949389755036\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2621709315119088\n",
      "Valence RMSE: 0.24713180345778576\n",
      "Arousal RMSE: 0.276392956457552\n",
      "Test R^2 score: tensor([0.2598, 0.2436], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1718, 0.1536], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.25171158892429996\n",
      "Num of epochs: 154\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Epoch 121, Loss: 0.5116998508836337\n",
      "Epoch 122, Loss: 0.5111129902664598\n",
      "Epoch 123, Loss: 0.5105405741436312\n",
      "Epoch 124, Loss: 0.5099849010083686\n",
      "Epoch 125, Loss: 0.5094623759868415\n",
      "Epoch 126, Loss: 0.5089652841754122\n",
      "Epoch 127, Loss: 0.5084717799132713\n",
      "Epoch 128, Loss: 0.5079809643002384\n",
      "Epoch 129, Loss: 0.507507731608986\n",
      "Epoch 130, Loss: 0.5070515727009828\n",
      "Epoch 131, Loss: 0.5065962678643066\n",
      "Epoch 132, Loss: 0.5061448812256618\n",
      "Epoch 133, Loss: 0.5057069703510908\n",
      "Epoch 134, Loss: 0.5052756693952567\n",
      "Epoch 135, Loss: 0.5048445017462813\n",
      "Epoch 136, Loss: 0.504425284231979\n",
      "Epoch 137, Loss: 0.504022717877823\n",
      "Epoch 138, Loss: 0.5036312802042249\n",
      "Epoch 139, Loss: 0.5032368731068207\n",
      "Epoch 140, Loss: 0.5028511060044271\n",
      "Epoch 141, Loss: 0.5024804933282331\n",
      "Epoch 142, Loss: 0.5021182727396313\n",
      "Epoch 143, Loss: 0.5017581664990297\n",
      "Epoch 144, Loss: 0.5014050827892457\n",
      "Epoch 145, Loss: 0.501047675902349\n",
      "Epoch 146, Loss: 0.500690966247362\n",
      "Epoch 147, Loss: 0.5003476722262875\n",
      "Epoch 148, Loss: 0.5000068544871652\n",
      "Epoch 149, Loss: 0.49966338867073357\n",
      "Epoch 150, Loss: 0.4993280873087794\n",
      "Epoch 151, Loss: 0.49900257940428383\n",
      "Epoch 152, Loss: 0.4986865554458311\n",
      "Epoch 153, Loss: 0.49836949389755036\n",
      "Epoch 154, Loss: 0.49805375636713634\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.262115862876475\n",
      "Valence RMSE: 0.24704482679183198\n",
      "Arousal RMSE: 0.2763662510118626\n",
      "Test R^2 score: tensor([0.2604, 0.2437], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1724, 0.1538], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.2520451199935895\n",
      "Num of epochs: 155\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Epoch 121, Loss: 0.5116998508836337\n",
      "Epoch 122, Loss: 0.5111129902664598\n",
      "Epoch 123, Loss: 0.5105405741436312\n",
      "Epoch 124, Loss: 0.5099849010083686\n",
      "Epoch 125, Loss: 0.5094623759868415\n",
      "Epoch 126, Loss: 0.5089652841754122\n",
      "Epoch 127, Loss: 0.5084717799132713\n",
      "Epoch 128, Loss: 0.5079809643002384\n",
      "Epoch 129, Loss: 0.507507731608986\n",
      "Epoch 130, Loss: 0.5070515727009828\n",
      "Epoch 131, Loss: 0.5065962678643066\n",
      "Epoch 132, Loss: 0.5061448812256618\n",
      "Epoch 133, Loss: 0.5057069703510908\n",
      "Epoch 134, Loss: 0.5052756693952567\n",
      "Epoch 135, Loss: 0.5048445017462813\n",
      "Epoch 136, Loss: 0.504425284231979\n",
      "Epoch 137, Loss: 0.504022717877823\n",
      "Epoch 138, Loss: 0.5036312802042249\n",
      "Epoch 139, Loss: 0.5032368731068207\n",
      "Epoch 140, Loss: 0.5028511060044271\n",
      "Epoch 141, Loss: 0.5024804933282331\n",
      "Epoch 142, Loss: 0.5021182727396313\n",
      "Epoch 143, Loss: 0.5017581664990297\n",
      "Epoch 144, Loss: 0.5014050827892457\n",
      "Epoch 145, Loss: 0.501047675902349\n",
      "Epoch 146, Loss: 0.500690966247362\n",
      "Epoch 147, Loss: 0.5003476722262875\n",
      "Epoch 148, Loss: 0.5000068544871652\n",
      "Epoch 149, Loss: 0.49966338867073357\n",
      "Epoch 150, Loss: 0.4993280873087794\n",
      "Epoch 151, Loss: 0.49900257940428383\n",
      "Epoch 152, Loss: 0.4986865554458311\n",
      "Epoch 153, Loss: 0.49836949389755036\n",
      "Epoch 154, Loss: 0.49805375636713634\n",
      "Epoch 155, Loss: 0.49773873165141763\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2620552253363805\n",
      "Valence RMSE: 0.24693936079386872\n",
      "Arousal RMSE: 0.2763454981412178\n",
      "Test R^2 score: tensor([0.2610, 0.2438], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1731, 0.1539], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.2524175988597692\n",
      "Num of epochs: 156\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Epoch 121, Loss: 0.5116998508836337\n",
      "Epoch 122, Loss: 0.5111129902664598\n",
      "Epoch 123, Loss: 0.5105405741436312\n",
      "Epoch 124, Loss: 0.5099849010083686\n",
      "Epoch 125, Loss: 0.5094623759868415\n",
      "Epoch 126, Loss: 0.5089652841754122\n",
      "Epoch 127, Loss: 0.5084717799132713\n",
      "Epoch 128, Loss: 0.5079809643002384\n",
      "Epoch 129, Loss: 0.507507731608986\n",
      "Epoch 130, Loss: 0.5070515727009828\n",
      "Epoch 131, Loss: 0.5065962678643066\n",
      "Epoch 132, Loss: 0.5061448812256618\n",
      "Epoch 133, Loss: 0.5057069703510908\n",
      "Epoch 134, Loss: 0.5052756693952567\n",
      "Epoch 135, Loss: 0.5048445017462813\n",
      "Epoch 136, Loss: 0.504425284231979\n",
      "Epoch 137, Loss: 0.504022717877823\n",
      "Epoch 138, Loss: 0.5036312802042249\n",
      "Epoch 139, Loss: 0.5032368731068207\n",
      "Epoch 140, Loss: 0.5028511060044271\n",
      "Epoch 141, Loss: 0.5024804933282331\n",
      "Epoch 142, Loss: 0.5021182727396313\n",
      "Epoch 143, Loss: 0.5017581664990297\n",
      "Epoch 144, Loss: 0.5014050827892457\n",
      "Epoch 145, Loss: 0.501047675902349\n",
      "Epoch 146, Loss: 0.500690966247362\n",
      "Epoch 147, Loss: 0.5003476722262875\n",
      "Epoch 148, Loss: 0.5000068544871652\n",
      "Epoch 149, Loss: 0.49966338867073357\n",
      "Epoch 150, Loss: 0.4993280873087794\n",
      "Epoch 151, Loss: 0.49900257940428383\n",
      "Epoch 152, Loss: 0.4986865554458311\n",
      "Epoch 153, Loss: 0.49836949389755036\n",
      "Epoch 154, Loss: 0.49805375636713634\n",
      "Epoch 155, Loss: 0.49773873165141763\n",
      "Epoch 156, Loss: 0.49742224924400874\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2619901947676897\n",
      "Valence RMSE: 0.246827893208523\n",
      "Arousal RMSE: 0.27632176071215586\n",
      "Test R^2 score: tensor([0.2617, 0.2440], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1738, 0.1540], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.25281605740928736\n",
      "Num of epochs: 157\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Epoch 121, Loss: 0.5116998508836337\n",
      "Epoch 122, Loss: 0.5111129902664598\n",
      "Epoch 123, Loss: 0.5105405741436312\n",
      "Epoch 124, Loss: 0.5099849010083686\n",
      "Epoch 125, Loss: 0.5094623759868415\n",
      "Epoch 126, Loss: 0.5089652841754122\n",
      "Epoch 127, Loss: 0.5084717799132713\n",
      "Epoch 128, Loss: 0.5079809643002384\n",
      "Epoch 129, Loss: 0.507507731608986\n",
      "Epoch 130, Loss: 0.5070515727009828\n",
      "Epoch 131, Loss: 0.5065962678643066\n",
      "Epoch 132, Loss: 0.5061448812256618\n",
      "Epoch 133, Loss: 0.5057069703510908\n",
      "Epoch 134, Loss: 0.5052756693952567\n",
      "Epoch 135, Loss: 0.5048445017462813\n",
      "Epoch 136, Loss: 0.504425284231979\n",
      "Epoch 137, Loss: 0.504022717877823\n",
      "Epoch 138, Loss: 0.5036312802042249\n",
      "Epoch 139, Loss: 0.5032368731068207\n",
      "Epoch 140, Loss: 0.5028511060044271\n",
      "Epoch 141, Loss: 0.5024804933282331\n",
      "Epoch 142, Loss: 0.5021182727396313\n",
      "Epoch 143, Loss: 0.5017581664990297\n",
      "Epoch 144, Loss: 0.5014050827892457\n",
      "Epoch 145, Loss: 0.501047675902349\n",
      "Epoch 146, Loss: 0.500690966247362\n",
      "Epoch 147, Loss: 0.5003476722262875\n",
      "Epoch 148, Loss: 0.5000068544871652\n",
      "Epoch 149, Loss: 0.49966338867073357\n",
      "Epoch 150, Loss: 0.4993280873087794\n",
      "Epoch 151, Loss: 0.49900257940428383\n",
      "Epoch 152, Loss: 0.4986865554458311\n",
      "Epoch 153, Loss: 0.49836949389755036\n",
      "Epoch 154, Loss: 0.49805375636713634\n",
      "Epoch 155, Loss: 0.49773873165141763\n",
      "Epoch 156, Loss: 0.49742224924400874\n",
      "Epoch 157, Loss: 0.49711425826812494\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.26191896555087646\n",
      "Valence RMSE: 0.24672792388496\n",
      "Arousal RMSE: 0.27627598629975025\n",
      "Test R^2 score: tensor([0.2623, 0.2442], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1745, 0.1543], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.25324026640724173\n",
      "Num of epochs: 158\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Epoch 121, Loss: 0.5116998508836337\n",
      "Epoch 122, Loss: 0.5111129902664598\n",
      "Epoch 123, Loss: 0.5105405741436312\n",
      "Epoch 124, Loss: 0.5099849010083686\n",
      "Epoch 125, Loss: 0.5094623759868415\n",
      "Epoch 126, Loss: 0.5089652841754122\n",
      "Epoch 127, Loss: 0.5084717799132713\n",
      "Epoch 128, Loss: 0.5079809643002384\n",
      "Epoch 129, Loss: 0.507507731608986\n",
      "Epoch 130, Loss: 0.5070515727009828\n",
      "Epoch 131, Loss: 0.5065962678643066\n",
      "Epoch 132, Loss: 0.5061448812256618\n",
      "Epoch 133, Loss: 0.5057069703510908\n",
      "Epoch 134, Loss: 0.5052756693952567\n",
      "Epoch 135, Loss: 0.5048445017462813\n",
      "Epoch 136, Loss: 0.504425284231979\n",
      "Epoch 137, Loss: 0.504022717877823\n",
      "Epoch 138, Loss: 0.5036312802042249\n",
      "Epoch 139, Loss: 0.5032368731068207\n",
      "Epoch 140, Loss: 0.5028511060044271\n",
      "Epoch 141, Loss: 0.5024804933282331\n",
      "Epoch 142, Loss: 0.5021182727396313\n",
      "Epoch 143, Loss: 0.5017581664990297\n",
      "Epoch 144, Loss: 0.5014050827892457\n",
      "Epoch 145, Loss: 0.501047675902349\n",
      "Epoch 146, Loss: 0.500690966247362\n",
      "Epoch 147, Loss: 0.5003476722262875\n",
      "Epoch 148, Loss: 0.5000068544871652\n",
      "Epoch 149, Loss: 0.49966338867073357\n",
      "Epoch 150, Loss: 0.4993280873087794\n",
      "Epoch 151, Loss: 0.49900257940428383\n",
      "Epoch 152, Loss: 0.4986865554458311\n",
      "Epoch 153, Loss: 0.49836949389755036\n",
      "Epoch 154, Loss: 0.49805375636713634\n",
      "Epoch 155, Loss: 0.49773873165141763\n",
      "Epoch 156, Loss: 0.49742224924400874\n",
      "Epoch 157, Loss: 0.49711425826812494\n",
      "Epoch 158, Loss: 0.4968010973402242\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.26183784316848546\n",
      "Valence RMSE: 0.24662966500929304\n",
      "Arousal RMSE: 0.27620992119699955\n",
      "Test R^2 score: tensor([0.2628, 0.2446], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1751, 0.1547], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.25371471710109933\n",
      "Num of epochs: 159\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Epoch 121, Loss: 0.5116998508836337\n",
      "Epoch 122, Loss: 0.5111129902664598\n",
      "Epoch 123, Loss: 0.5105405741436312\n",
      "Epoch 124, Loss: 0.5099849010083686\n",
      "Epoch 125, Loss: 0.5094623759868415\n",
      "Epoch 126, Loss: 0.5089652841754122\n",
      "Epoch 127, Loss: 0.5084717799132713\n",
      "Epoch 128, Loss: 0.5079809643002384\n",
      "Epoch 129, Loss: 0.507507731608986\n",
      "Epoch 130, Loss: 0.5070515727009828\n",
      "Epoch 131, Loss: 0.5065962678643066\n",
      "Epoch 132, Loss: 0.5061448812256618\n",
      "Epoch 133, Loss: 0.5057069703510908\n",
      "Epoch 134, Loss: 0.5052756693952567\n",
      "Epoch 135, Loss: 0.5048445017462813\n",
      "Epoch 136, Loss: 0.504425284231979\n",
      "Epoch 137, Loss: 0.504022717877823\n",
      "Epoch 138, Loss: 0.5036312802042249\n",
      "Epoch 139, Loss: 0.5032368731068207\n",
      "Epoch 140, Loss: 0.5028511060044271\n",
      "Epoch 141, Loss: 0.5024804933282331\n",
      "Epoch 142, Loss: 0.5021182727396313\n",
      "Epoch 143, Loss: 0.5017581664990297\n",
      "Epoch 144, Loss: 0.5014050827892457\n",
      "Epoch 145, Loss: 0.501047675902349\n",
      "Epoch 146, Loss: 0.500690966247362\n",
      "Epoch 147, Loss: 0.5003476722262875\n",
      "Epoch 148, Loss: 0.5000068544871652\n",
      "Epoch 149, Loss: 0.49966338867073357\n",
      "Epoch 150, Loss: 0.4993280873087794\n",
      "Epoch 151, Loss: 0.49900257940428383\n",
      "Epoch 152, Loss: 0.4986865554458311\n",
      "Epoch 153, Loss: 0.49836949389755036\n",
      "Epoch 154, Loss: 0.49805375636713634\n",
      "Epoch 155, Loss: 0.49773873165141763\n",
      "Epoch 156, Loss: 0.49742224924400874\n",
      "Epoch 157, Loss: 0.49711425826812494\n",
      "Epoch 158, Loss: 0.4968010973402242\n",
      "Epoch 159, Loss: 0.4964901399315246\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.26172664667225387\n",
      "Valence RMSE: 0.2465114038671427\n",
      "Arousal RMSE: 0.2761046955777032\n",
      "Test R^2 score: tensor([0.2636, 0.2452], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1759, 0.1554], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.254355834724309\n",
      "Num of epochs: 160\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Epoch 121, Loss: 0.5116998508836337\n",
      "Epoch 122, Loss: 0.5111129902664598\n",
      "Epoch 123, Loss: 0.5105405741436312\n",
      "Epoch 124, Loss: 0.5099849010083686\n",
      "Epoch 125, Loss: 0.5094623759868415\n",
      "Epoch 126, Loss: 0.5089652841754122\n",
      "Epoch 127, Loss: 0.5084717799132713\n",
      "Epoch 128, Loss: 0.5079809643002384\n",
      "Epoch 129, Loss: 0.507507731608986\n",
      "Epoch 130, Loss: 0.5070515727009828\n",
      "Epoch 131, Loss: 0.5065962678643066\n",
      "Epoch 132, Loss: 0.5061448812256618\n",
      "Epoch 133, Loss: 0.5057069703510908\n",
      "Epoch 134, Loss: 0.5052756693952567\n",
      "Epoch 135, Loss: 0.5048445017462813\n",
      "Epoch 136, Loss: 0.504425284231979\n",
      "Epoch 137, Loss: 0.504022717877823\n",
      "Epoch 138, Loss: 0.5036312802042249\n",
      "Epoch 139, Loss: 0.5032368731068207\n",
      "Epoch 140, Loss: 0.5028511060044271\n",
      "Epoch 141, Loss: 0.5024804933282331\n",
      "Epoch 142, Loss: 0.5021182727396313\n",
      "Epoch 143, Loss: 0.5017581664990297\n",
      "Epoch 144, Loss: 0.5014050827892457\n",
      "Epoch 145, Loss: 0.501047675902349\n",
      "Epoch 146, Loss: 0.500690966247362\n",
      "Epoch 147, Loss: 0.5003476722262875\n",
      "Epoch 148, Loss: 0.5000068544871652\n",
      "Epoch 149, Loss: 0.49966338867073357\n",
      "Epoch 150, Loss: 0.4993280873087794\n",
      "Epoch 151, Loss: 0.49900257940428383\n",
      "Epoch 152, Loss: 0.4986865554458311\n",
      "Epoch 153, Loss: 0.49836949389755036\n",
      "Epoch 154, Loss: 0.49805375636713634\n",
      "Epoch 155, Loss: 0.49773873165141763\n",
      "Epoch 156, Loss: 0.49742224924400874\n",
      "Epoch 157, Loss: 0.49711425826812494\n",
      "Epoch 158, Loss: 0.4968010973402242\n",
      "Epoch 159, Loss: 0.4964901399315246\n",
      "Epoch 160, Loss: 0.49617675026847124\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.261576829994961\n",
      "Valence RMSE: 0.24637587314913323\n",
      "Arousal RMSE: 0.27594166976088935\n",
      "Test R^2 score: tensor([0.2644, 0.2460], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1768, 0.1563], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.2552061822319866\n",
      "Num of epochs: 161\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Epoch 121, Loss: 0.5116998508836337\n",
      "Epoch 122, Loss: 0.5111129902664598\n",
      "Epoch 123, Loss: 0.5105405741436312\n",
      "Epoch 124, Loss: 0.5099849010083686\n",
      "Epoch 125, Loss: 0.5094623759868415\n",
      "Epoch 126, Loss: 0.5089652841754122\n",
      "Epoch 127, Loss: 0.5084717799132713\n",
      "Epoch 128, Loss: 0.5079809643002384\n",
      "Epoch 129, Loss: 0.507507731608986\n",
      "Epoch 130, Loss: 0.5070515727009828\n",
      "Epoch 131, Loss: 0.5065962678643066\n",
      "Epoch 132, Loss: 0.5061448812256618\n",
      "Epoch 133, Loss: 0.5057069703510908\n",
      "Epoch 134, Loss: 0.5052756693952567\n",
      "Epoch 135, Loss: 0.5048445017462813\n",
      "Epoch 136, Loss: 0.504425284231979\n",
      "Epoch 137, Loss: 0.504022717877823\n",
      "Epoch 138, Loss: 0.5036312802042249\n",
      "Epoch 139, Loss: 0.5032368731068207\n",
      "Epoch 140, Loss: 0.5028511060044271\n",
      "Epoch 141, Loss: 0.5024804933282331\n",
      "Epoch 142, Loss: 0.5021182727396313\n",
      "Epoch 143, Loss: 0.5017581664990297\n",
      "Epoch 144, Loss: 0.5014050827892457\n",
      "Epoch 145, Loss: 0.501047675902349\n",
      "Epoch 146, Loss: 0.500690966247362\n",
      "Epoch 147, Loss: 0.5003476722262875\n",
      "Epoch 148, Loss: 0.5000068544871652\n",
      "Epoch 149, Loss: 0.49966338867073357\n",
      "Epoch 150, Loss: 0.4993280873087794\n",
      "Epoch 151, Loss: 0.49900257940428383\n",
      "Epoch 152, Loss: 0.4986865554458311\n",
      "Epoch 153, Loss: 0.49836949389755036\n",
      "Epoch 154, Loss: 0.49805375636713634\n",
      "Epoch 155, Loss: 0.49773873165141763\n",
      "Epoch 156, Loss: 0.49742224924400874\n",
      "Epoch 157, Loss: 0.49711425826812494\n",
      "Epoch 158, Loss: 0.4968010973402242\n",
      "Epoch 159, Loss: 0.4964901399315246\n",
      "Epoch 160, Loss: 0.49617675026847124\n",
      "Epoch 161, Loss: 0.4958670390985117\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.26143429328295015\n",
      "Valence RMSE: 0.24625134585785968\n",
      "Arousal RMSE: 0.27578262104763596\n",
      "Test R^2 score: tensor([0.2651, 0.2469], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1777, 0.1573], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.2560123467239053\n",
      "Num of epochs: 162\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Epoch 121, Loss: 0.5116998508836337\n",
      "Epoch 122, Loss: 0.5111129902664598\n",
      "Epoch 123, Loss: 0.5105405741436312\n",
      "Epoch 124, Loss: 0.5099849010083686\n",
      "Epoch 125, Loss: 0.5094623759868415\n",
      "Epoch 126, Loss: 0.5089652841754122\n",
      "Epoch 127, Loss: 0.5084717799132713\n",
      "Epoch 128, Loss: 0.5079809643002384\n",
      "Epoch 129, Loss: 0.507507731608986\n",
      "Epoch 130, Loss: 0.5070515727009828\n",
      "Epoch 131, Loss: 0.5065962678643066\n",
      "Epoch 132, Loss: 0.5061448812256618\n",
      "Epoch 133, Loss: 0.5057069703510908\n",
      "Epoch 134, Loss: 0.5052756693952567\n",
      "Epoch 135, Loss: 0.5048445017462813\n",
      "Epoch 136, Loss: 0.504425284231979\n",
      "Epoch 137, Loss: 0.504022717877823\n",
      "Epoch 138, Loss: 0.5036312802042249\n",
      "Epoch 139, Loss: 0.5032368731068207\n",
      "Epoch 140, Loss: 0.5028511060044271\n",
      "Epoch 141, Loss: 0.5024804933282331\n",
      "Epoch 142, Loss: 0.5021182727396313\n",
      "Epoch 143, Loss: 0.5017581664990297\n",
      "Epoch 144, Loss: 0.5014050827892457\n",
      "Epoch 145, Loss: 0.501047675902349\n",
      "Epoch 146, Loss: 0.500690966247362\n",
      "Epoch 147, Loss: 0.5003476722262875\n",
      "Epoch 148, Loss: 0.5000068544871652\n",
      "Epoch 149, Loss: 0.49966338867073357\n",
      "Epoch 150, Loss: 0.4993280873087794\n",
      "Epoch 151, Loss: 0.49900257940428383\n",
      "Epoch 152, Loss: 0.4986865554458311\n",
      "Epoch 153, Loss: 0.49836949389755036\n",
      "Epoch 154, Loss: 0.49805375636713634\n",
      "Epoch 155, Loss: 0.49773873165141763\n",
      "Epoch 156, Loss: 0.49742224924400874\n",
      "Epoch 157, Loss: 0.49711425826812494\n",
      "Epoch 158, Loss: 0.4968010973402242\n",
      "Epoch 159, Loss: 0.4964901399315246\n",
      "Epoch 160, Loss: 0.49617675026847124\n",
      "Epoch 161, Loss: 0.4958670390985117\n",
      "Epoch 162, Loss: 0.4955578109301908\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2613240778919143\n",
      "Valence RMSE: 0.24615704381388911\n",
      "Arousal RMSE: 0.2756578624906709\n",
      "Test R^2 score: tensor([0.2657, 0.2476], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1783, 0.1581], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.2566343227682221\n",
      "Num of epochs: 163\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Epoch 121, Loss: 0.5116998508836337\n",
      "Epoch 122, Loss: 0.5111129902664598\n",
      "Epoch 123, Loss: 0.5105405741436312\n",
      "Epoch 124, Loss: 0.5099849010083686\n",
      "Epoch 125, Loss: 0.5094623759868415\n",
      "Epoch 126, Loss: 0.5089652841754122\n",
      "Epoch 127, Loss: 0.5084717799132713\n",
      "Epoch 128, Loss: 0.5079809643002384\n",
      "Epoch 129, Loss: 0.507507731608986\n",
      "Epoch 130, Loss: 0.5070515727009828\n",
      "Epoch 131, Loss: 0.5065962678643066\n",
      "Epoch 132, Loss: 0.5061448812256618\n",
      "Epoch 133, Loss: 0.5057069703510908\n",
      "Epoch 134, Loss: 0.5052756693952567\n",
      "Epoch 135, Loss: 0.5048445017462813\n",
      "Epoch 136, Loss: 0.504425284231979\n",
      "Epoch 137, Loss: 0.504022717877823\n",
      "Epoch 138, Loss: 0.5036312802042249\n",
      "Epoch 139, Loss: 0.5032368731068207\n",
      "Epoch 140, Loss: 0.5028511060044271\n",
      "Epoch 141, Loss: 0.5024804933282331\n",
      "Epoch 142, Loss: 0.5021182727396313\n",
      "Epoch 143, Loss: 0.5017581664990297\n",
      "Epoch 144, Loss: 0.5014050827892457\n",
      "Epoch 145, Loss: 0.501047675902349\n",
      "Epoch 146, Loss: 0.500690966247362\n",
      "Epoch 147, Loss: 0.5003476722262875\n",
      "Epoch 148, Loss: 0.5000068544871652\n",
      "Epoch 149, Loss: 0.49966338867073357\n",
      "Epoch 150, Loss: 0.4993280873087794\n",
      "Epoch 151, Loss: 0.49900257940428383\n",
      "Epoch 152, Loss: 0.4986865554458311\n",
      "Epoch 153, Loss: 0.49836949389755036\n",
      "Epoch 154, Loss: 0.49805375636713634\n",
      "Epoch 155, Loss: 0.49773873165141763\n",
      "Epoch 156, Loss: 0.49742224924400874\n",
      "Epoch 157, Loss: 0.49711425826812494\n",
      "Epoch 158, Loss: 0.4968010973402242\n",
      "Epoch 159, Loss: 0.4964901399315246\n",
      "Epoch 160, Loss: 0.49617675026847124\n",
      "Epoch 161, Loss: 0.4958670390985117\n",
      "Epoch 162, Loss: 0.4955578109301908\n",
      "Epoch 163, Loss: 0.4952510223984696\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2612554662914226\n",
      "Valence RMSE: 0.24609781303205255\n",
      "Arousal RMSE: 0.2755806665118911\n",
      "Test R^2 score: tensor([0.2660, 0.2480], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1787, 0.1586], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.25702167243635726\n",
      "Num of epochs: 164\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Epoch 121, Loss: 0.5116998508836337\n",
      "Epoch 122, Loss: 0.5111129902664598\n",
      "Epoch 123, Loss: 0.5105405741436312\n",
      "Epoch 124, Loss: 0.5099849010083686\n",
      "Epoch 125, Loss: 0.5094623759868415\n",
      "Epoch 126, Loss: 0.5089652841754122\n",
      "Epoch 127, Loss: 0.5084717799132713\n",
      "Epoch 128, Loss: 0.5079809643002384\n",
      "Epoch 129, Loss: 0.507507731608986\n",
      "Epoch 130, Loss: 0.5070515727009828\n",
      "Epoch 131, Loss: 0.5065962678643066\n",
      "Epoch 132, Loss: 0.5061448812256618\n",
      "Epoch 133, Loss: 0.5057069703510908\n",
      "Epoch 134, Loss: 0.5052756693952567\n",
      "Epoch 135, Loss: 0.5048445017462813\n",
      "Epoch 136, Loss: 0.504425284231979\n",
      "Epoch 137, Loss: 0.504022717877823\n",
      "Epoch 138, Loss: 0.5036312802042249\n",
      "Epoch 139, Loss: 0.5032368731068207\n",
      "Epoch 140, Loss: 0.5028511060044271\n",
      "Epoch 141, Loss: 0.5024804933282331\n",
      "Epoch 142, Loss: 0.5021182727396313\n",
      "Epoch 143, Loss: 0.5017581664990297\n",
      "Epoch 144, Loss: 0.5014050827892457\n",
      "Epoch 145, Loss: 0.501047675902349\n",
      "Epoch 146, Loss: 0.500690966247362\n",
      "Epoch 147, Loss: 0.5003476722262875\n",
      "Epoch 148, Loss: 0.5000068544871652\n",
      "Epoch 149, Loss: 0.49966338867073357\n",
      "Epoch 150, Loss: 0.4993280873087794\n",
      "Epoch 151, Loss: 0.49900257940428383\n",
      "Epoch 152, Loss: 0.4986865554458311\n",
      "Epoch 153, Loss: 0.49836949389755036\n",
      "Epoch 154, Loss: 0.49805375636713634\n",
      "Epoch 155, Loss: 0.49773873165141763\n",
      "Epoch 156, Loss: 0.49742224924400874\n",
      "Epoch 157, Loss: 0.49711425826812494\n",
      "Epoch 158, Loss: 0.4968010973402242\n",
      "Epoch 159, Loss: 0.4964901399315246\n",
      "Epoch 160, Loss: 0.49617675026847124\n",
      "Epoch 161, Loss: 0.4958670390985117\n",
      "Epoch 162, Loss: 0.4955578109301908\n",
      "Epoch 163, Loss: 0.4952510223984696\n",
      "Epoch 164, Loss: 0.49494577484111674\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2612383610913183\n",
      "Valence RMSE: 0.24607732401040297\n",
      "Arousal RMSE: 0.2755665313844103\n",
      "Test R^2 score: tensor([0.2661, 0.2481], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1788, 0.1586], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.2571213472447835\n",
      "Num of epochs: 165\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Epoch 121, Loss: 0.5116998508836337\n",
      "Epoch 122, Loss: 0.5111129902664598\n",
      "Epoch 123, Loss: 0.5105405741436312\n",
      "Epoch 124, Loss: 0.5099849010083686\n",
      "Epoch 125, Loss: 0.5094623759868415\n",
      "Epoch 126, Loss: 0.5089652841754122\n",
      "Epoch 127, Loss: 0.5084717799132713\n",
      "Epoch 128, Loss: 0.5079809643002384\n",
      "Epoch 129, Loss: 0.507507731608986\n",
      "Epoch 130, Loss: 0.5070515727009828\n",
      "Epoch 131, Loss: 0.5065962678643066\n",
      "Epoch 132, Loss: 0.5061448812256618\n",
      "Epoch 133, Loss: 0.5057069703510908\n",
      "Epoch 134, Loss: 0.5052756693952567\n",
      "Epoch 135, Loss: 0.5048445017462813\n",
      "Epoch 136, Loss: 0.504425284231979\n",
      "Epoch 137, Loss: 0.504022717877823\n",
      "Epoch 138, Loss: 0.5036312802042249\n",
      "Epoch 139, Loss: 0.5032368731068207\n",
      "Epoch 140, Loss: 0.5028511060044271\n",
      "Epoch 141, Loss: 0.5024804933282331\n",
      "Epoch 142, Loss: 0.5021182727396313\n",
      "Epoch 143, Loss: 0.5017581664990297\n",
      "Epoch 144, Loss: 0.5014050827892457\n",
      "Epoch 145, Loss: 0.501047675902349\n",
      "Epoch 146, Loss: 0.500690966247362\n",
      "Epoch 147, Loss: 0.5003476722262875\n",
      "Epoch 148, Loss: 0.5000068544871652\n",
      "Epoch 149, Loss: 0.49966338867073357\n",
      "Epoch 150, Loss: 0.4993280873087794\n",
      "Epoch 151, Loss: 0.49900257940428383\n",
      "Epoch 152, Loss: 0.4986865554458311\n",
      "Epoch 153, Loss: 0.49836949389755036\n",
      "Epoch 154, Loss: 0.49805375636713634\n",
      "Epoch 155, Loss: 0.49773873165141763\n",
      "Epoch 156, Loss: 0.49742224924400874\n",
      "Epoch 157, Loss: 0.49711425826812494\n",
      "Epoch 158, Loss: 0.4968010973402242\n",
      "Epoch 159, Loss: 0.4964901399315246\n",
      "Epoch 160, Loss: 0.49617675026847124\n",
      "Epoch 161, Loss: 0.4958670390985117\n",
      "Epoch 162, Loss: 0.4955578109301908\n",
      "Epoch 163, Loss: 0.4952510223984696\n",
      "Epoch 164, Loss: 0.49494577484111674\n",
      "Epoch 165, Loss: 0.49464511374066544\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2612423526024008\n",
      "Valence RMSE: 0.24608101153591677\n",
      "Arousal RMSE: 0.2755708064144935\n",
      "Test R^2 score: tensor([0.2661, 0.2481], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1788, 0.1586], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.25709868537020036\n",
      "Num of epochs: 166\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Epoch 121, Loss: 0.5116998508836337\n",
      "Epoch 122, Loss: 0.5111129902664598\n",
      "Epoch 123, Loss: 0.5105405741436312\n",
      "Epoch 124, Loss: 0.5099849010083686\n",
      "Epoch 125, Loss: 0.5094623759868415\n",
      "Epoch 126, Loss: 0.5089652841754122\n",
      "Epoch 127, Loss: 0.5084717799132713\n",
      "Epoch 128, Loss: 0.5079809643002384\n",
      "Epoch 129, Loss: 0.507507731608986\n",
      "Epoch 130, Loss: 0.5070515727009828\n",
      "Epoch 131, Loss: 0.5065962678643066\n",
      "Epoch 132, Loss: 0.5061448812256618\n",
      "Epoch 133, Loss: 0.5057069703510908\n",
      "Epoch 134, Loss: 0.5052756693952567\n",
      "Epoch 135, Loss: 0.5048445017462813\n",
      "Epoch 136, Loss: 0.504425284231979\n",
      "Epoch 137, Loss: 0.504022717877823\n",
      "Epoch 138, Loss: 0.5036312802042249\n",
      "Epoch 139, Loss: 0.5032368731068207\n",
      "Epoch 140, Loss: 0.5028511060044271\n",
      "Epoch 141, Loss: 0.5024804933282331\n",
      "Epoch 142, Loss: 0.5021182727396313\n",
      "Epoch 143, Loss: 0.5017581664990297\n",
      "Epoch 144, Loss: 0.5014050827892457\n",
      "Epoch 145, Loss: 0.501047675902349\n",
      "Epoch 146, Loss: 0.500690966247362\n",
      "Epoch 147, Loss: 0.5003476722262875\n",
      "Epoch 148, Loss: 0.5000068544871652\n",
      "Epoch 149, Loss: 0.49966338867073357\n",
      "Epoch 150, Loss: 0.4993280873087794\n",
      "Epoch 151, Loss: 0.49900257940428383\n",
      "Epoch 152, Loss: 0.4986865554458311\n",
      "Epoch 153, Loss: 0.49836949389755036\n",
      "Epoch 154, Loss: 0.49805375636713634\n",
      "Epoch 155, Loss: 0.49773873165141763\n",
      "Epoch 156, Loss: 0.49742224924400874\n",
      "Epoch 157, Loss: 0.49711425826812494\n",
      "Epoch 158, Loss: 0.4968010973402242\n",
      "Epoch 159, Loss: 0.4964901399315246\n",
      "Epoch 160, Loss: 0.49617675026847124\n",
      "Epoch 161, Loss: 0.4958670390985117\n",
      "Epoch 162, Loss: 0.4955578109301908\n",
      "Epoch 163, Loss: 0.4952510223984696\n",
      "Epoch 164, Loss: 0.49494577484111674\n",
      "Epoch 165, Loss: 0.49464511374066544\n",
      "Epoch 166, Loss: 0.49434383269975535\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2612666082385379\n",
      "Valence RMSE: 0.24611115685703247\n",
      "Arousal RMSE: 0.2755898757781795\n",
      "Test R^2 score: tensor([0.2659, 0.2480], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1786, 0.1585], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.25695674410266434\n",
      "Num of epochs: 167\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Epoch 121, Loss: 0.5116998508836337\n",
      "Epoch 122, Loss: 0.5111129902664598\n",
      "Epoch 123, Loss: 0.5105405741436312\n",
      "Epoch 124, Loss: 0.5099849010083686\n",
      "Epoch 125, Loss: 0.5094623759868415\n",
      "Epoch 126, Loss: 0.5089652841754122\n",
      "Epoch 127, Loss: 0.5084717799132713\n",
      "Epoch 128, Loss: 0.5079809643002384\n",
      "Epoch 129, Loss: 0.507507731608986\n",
      "Epoch 130, Loss: 0.5070515727009828\n",
      "Epoch 131, Loss: 0.5065962678643066\n",
      "Epoch 132, Loss: 0.5061448812256618\n",
      "Epoch 133, Loss: 0.5057069703510908\n",
      "Epoch 134, Loss: 0.5052756693952567\n",
      "Epoch 135, Loss: 0.5048445017462813\n",
      "Epoch 136, Loss: 0.504425284231979\n",
      "Epoch 137, Loss: 0.504022717877823\n",
      "Epoch 138, Loss: 0.5036312802042249\n",
      "Epoch 139, Loss: 0.5032368731068207\n",
      "Epoch 140, Loss: 0.5028511060044271\n",
      "Epoch 141, Loss: 0.5024804933282331\n",
      "Epoch 142, Loss: 0.5021182727396313\n",
      "Epoch 143, Loss: 0.5017581664990297\n",
      "Epoch 144, Loss: 0.5014050827892457\n",
      "Epoch 145, Loss: 0.501047675902349\n",
      "Epoch 146, Loss: 0.500690966247362\n",
      "Epoch 147, Loss: 0.5003476722262875\n",
      "Epoch 148, Loss: 0.5000068544871652\n",
      "Epoch 149, Loss: 0.49966338867073357\n",
      "Epoch 150, Loss: 0.4993280873087794\n",
      "Epoch 151, Loss: 0.49900257940428383\n",
      "Epoch 152, Loss: 0.4986865554458311\n",
      "Epoch 153, Loss: 0.49836949389755036\n",
      "Epoch 154, Loss: 0.49805375636713634\n",
      "Epoch 155, Loss: 0.49773873165141763\n",
      "Epoch 156, Loss: 0.49742224924400874\n",
      "Epoch 157, Loss: 0.49711425826812494\n",
      "Epoch 158, Loss: 0.4968010973402242\n",
      "Epoch 159, Loss: 0.4964901399315246\n",
      "Epoch 160, Loss: 0.49617675026847124\n",
      "Epoch 161, Loss: 0.4958670390985117\n",
      "Epoch 162, Loss: 0.4955578109301908\n",
      "Epoch 163, Loss: 0.4952510223984696\n",
      "Epoch 164, Loss: 0.49494577484111674\n",
      "Epoch 165, Loss: 0.49464511374066544\n",
      "Epoch 166, Loss: 0.49434383269975535\n",
      "Epoch 167, Loss: 0.49404631909698943\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2613145064063336\n",
      "Valence RMSE: 0.24616588562824354\n",
      "Arousal RMSE: 0.27563181831862965\n",
      "Test R^2 score: tensor([0.2656, 0.2477], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1782, 0.1582], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.2566790291876197\n",
      "Num of epochs: 168\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Epoch 121, Loss: 0.5116998508836337\n",
      "Epoch 122, Loss: 0.5111129902664598\n",
      "Epoch 123, Loss: 0.5105405741436312\n",
      "Epoch 124, Loss: 0.5099849010083686\n",
      "Epoch 125, Loss: 0.5094623759868415\n",
      "Epoch 126, Loss: 0.5089652841754122\n",
      "Epoch 127, Loss: 0.5084717799132713\n",
      "Epoch 128, Loss: 0.5079809643002384\n",
      "Epoch 129, Loss: 0.507507731608986\n",
      "Epoch 130, Loss: 0.5070515727009828\n",
      "Epoch 131, Loss: 0.5065962678643066\n",
      "Epoch 132, Loss: 0.5061448812256618\n",
      "Epoch 133, Loss: 0.5057069703510908\n",
      "Epoch 134, Loss: 0.5052756693952567\n",
      "Epoch 135, Loss: 0.5048445017462813\n",
      "Epoch 136, Loss: 0.504425284231979\n",
      "Epoch 137, Loss: 0.504022717877823\n",
      "Epoch 138, Loss: 0.5036312802042249\n",
      "Epoch 139, Loss: 0.5032368731068207\n",
      "Epoch 140, Loss: 0.5028511060044271\n",
      "Epoch 141, Loss: 0.5024804933282331\n",
      "Epoch 142, Loss: 0.5021182727396313\n",
      "Epoch 143, Loss: 0.5017581664990297\n",
      "Epoch 144, Loss: 0.5014050827892457\n",
      "Epoch 145, Loss: 0.501047675902349\n",
      "Epoch 146, Loss: 0.500690966247362\n",
      "Epoch 147, Loss: 0.5003476722262875\n",
      "Epoch 148, Loss: 0.5000068544871652\n",
      "Epoch 149, Loss: 0.49966338867073357\n",
      "Epoch 150, Loss: 0.4993280873087794\n",
      "Epoch 151, Loss: 0.49900257940428383\n",
      "Epoch 152, Loss: 0.4986865554458311\n",
      "Epoch 153, Loss: 0.49836949389755036\n",
      "Epoch 154, Loss: 0.49805375636713634\n",
      "Epoch 155, Loss: 0.49773873165141763\n",
      "Epoch 156, Loss: 0.49742224924400874\n",
      "Epoch 157, Loss: 0.49711425826812494\n",
      "Epoch 158, Loss: 0.4968010973402242\n",
      "Epoch 159, Loss: 0.4964901399315246\n",
      "Epoch 160, Loss: 0.49617675026847124\n",
      "Epoch 161, Loss: 0.4958670390985117\n",
      "Epoch 162, Loss: 0.4955578109301908\n",
      "Epoch 163, Loss: 0.4952510223984696\n",
      "Epoch 164, Loss: 0.49494577484111674\n",
      "Epoch 165, Loss: 0.49464511374066544\n",
      "Epoch 166, Loss: 0.49434383269975535\n",
      "Epoch 167, Loss: 0.49404631909698943\n",
      "Epoch 168, Loss: 0.49374530645143583\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.26137983714845103\n",
      "Valence RMSE: 0.24623815645649708\n",
      "Arousal RMSE: 0.27569114755538887\n",
      "Test R^2 score: tensor([0.2652, 0.2474], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1778, 0.1579], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.25630145332414805\n",
      "Num of epochs: 169\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Epoch 121, Loss: 0.5116998508836337\n",
      "Epoch 122, Loss: 0.5111129902664598\n",
      "Epoch 123, Loss: 0.5105405741436312\n",
      "Epoch 124, Loss: 0.5099849010083686\n",
      "Epoch 125, Loss: 0.5094623759868415\n",
      "Epoch 126, Loss: 0.5089652841754122\n",
      "Epoch 127, Loss: 0.5084717799132713\n",
      "Epoch 128, Loss: 0.5079809643002384\n",
      "Epoch 129, Loss: 0.507507731608986\n",
      "Epoch 130, Loss: 0.5070515727009828\n",
      "Epoch 131, Loss: 0.5065962678643066\n",
      "Epoch 132, Loss: 0.5061448812256618\n",
      "Epoch 133, Loss: 0.5057069703510908\n",
      "Epoch 134, Loss: 0.5052756693952567\n",
      "Epoch 135, Loss: 0.5048445017462813\n",
      "Epoch 136, Loss: 0.504425284231979\n",
      "Epoch 137, Loss: 0.504022717877823\n",
      "Epoch 138, Loss: 0.5036312802042249\n",
      "Epoch 139, Loss: 0.5032368731068207\n",
      "Epoch 140, Loss: 0.5028511060044271\n",
      "Epoch 141, Loss: 0.5024804933282331\n",
      "Epoch 142, Loss: 0.5021182727396313\n",
      "Epoch 143, Loss: 0.5017581664990297\n",
      "Epoch 144, Loss: 0.5014050827892457\n",
      "Epoch 145, Loss: 0.501047675902349\n",
      "Epoch 146, Loss: 0.500690966247362\n",
      "Epoch 147, Loss: 0.5003476722262875\n",
      "Epoch 148, Loss: 0.5000068544871652\n",
      "Epoch 149, Loss: 0.49966338867073357\n",
      "Epoch 150, Loss: 0.4993280873087794\n",
      "Epoch 151, Loss: 0.49900257940428383\n",
      "Epoch 152, Loss: 0.4986865554458311\n",
      "Epoch 153, Loss: 0.49836949389755036\n",
      "Epoch 154, Loss: 0.49805375636713634\n",
      "Epoch 155, Loss: 0.49773873165141763\n",
      "Epoch 156, Loss: 0.49742224924400874\n",
      "Epoch 157, Loss: 0.49711425826812494\n",
      "Epoch 158, Loss: 0.4968010973402242\n",
      "Epoch 159, Loss: 0.4964901399315246\n",
      "Epoch 160, Loss: 0.49617675026847124\n",
      "Epoch 161, Loss: 0.4958670390985117\n",
      "Epoch 162, Loss: 0.4955578109301908\n",
      "Epoch 163, Loss: 0.4952510223984696\n",
      "Epoch 164, Loss: 0.49494577484111674\n",
      "Epoch 165, Loss: 0.49464511374066544\n",
      "Epoch 166, Loss: 0.49434383269975535\n",
      "Epoch 167, Loss: 0.49404631909698943\n",
      "Epoch 168, Loss: 0.49374530645143583\n",
      "Epoch 169, Loss: 0.49345064806438055\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2614475688023464\n",
      "Valence RMSE: 0.24629930351022675\n",
      "Arousal RMSE: 0.27576496433682895\n",
      "Test R^2 score: tensor([0.2648, 0.2470], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1774, 0.1574], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.2559174256864563\n",
      "Num of epochs: 170\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Epoch 121, Loss: 0.5116998508836337\n",
      "Epoch 122, Loss: 0.5111129902664598\n",
      "Epoch 123, Loss: 0.5105405741436312\n",
      "Epoch 124, Loss: 0.5099849010083686\n",
      "Epoch 125, Loss: 0.5094623759868415\n",
      "Epoch 126, Loss: 0.5089652841754122\n",
      "Epoch 127, Loss: 0.5084717799132713\n",
      "Epoch 128, Loss: 0.5079809643002384\n",
      "Epoch 129, Loss: 0.507507731608986\n",
      "Epoch 130, Loss: 0.5070515727009828\n",
      "Epoch 131, Loss: 0.5065962678643066\n",
      "Epoch 132, Loss: 0.5061448812256618\n",
      "Epoch 133, Loss: 0.5057069703510908\n",
      "Epoch 134, Loss: 0.5052756693952567\n",
      "Epoch 135, Loss: 0.5048445017462813\n",
      "Epoch 136, Loss: 0.504425284231979\n",
      "Epoch 137, Loss: 0.504022717877823\n",
      "Epoch 138, Loss: 0.5036312802042249\n",
      "Epoch 139, Loss: 0.5032368731068207\n",
      "Epoch 140, Loss: 0.5028511060044271\n",
      "Epoch 141, Loss: 0.5024804933282331\n",
      "Epoch 142, Loss: 0.5021182727396313\n",
      "Epoch 143, Loss: 0.5017581664990297\n",
      "Epoch 144, Loss: 0.5014050827892457\n",
      "Epoch 145, Loss: 0.501047675902349\n",
      "Epoch 146, Loss: 0.500690966247362\n",
      "Epoch 147, Loss: 0.5003476722262875\n",
      "Epoch 148, Loss: 0.5000068544871652\n",
      "Epoch 149, Loss: 0.49966338867073357\n",
      "Epoch 150, Loss: 0.4993280873087794\n",
      "Epoch 151, Loss: 0.49900257940428383\n",
      "Epoch 152, Loss: 0.4986865554458311\n",
      "Epoch 153, Loss: 0.49836949389755036\n",
      "Epoch 154, Loss: 0.49805375636713634\n",
      "Epoch 155, Loss: 0.49773873165141763\n",
      "Epoch 156, Loss: 0.49742224924400874\n",
      "Epoch 157, Loss: 0.49711425826812494\n",
      "Epoch 158, Loss: 0.4968010973402242\n",
      "Epoch 159, Loss: 0.4964901399315246\n",
      "Epoch 160, Loss: 0.49617675026847124\n",
      "Epoch 161, Loss: 0.4958670390985117\n",
      "Epoch 162, Loss: 0.4955578109301908\n",
      "Epoch 163, Loss: 0.4952510223984696\n",
      "Epoch 164, Loss: 0.49494577484111674\n",
      "Epoch 165, Loss: 0.49464511374066544\n",
      "Epoch 166, Loss: 0.49434383269975535\n",
      "Epoch 167, Loss: 0.49404631909698943\n",
      "Epoch 168, Loss: 0.49374530645143583\n",
      "Epoch 169, Loss: 0.49345064806438055\n",
      "Epoch 170, Loss: 0.49317001490258333\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2615084155833613\n",
      "Valence RMSE: 0.24633833402833466\n",
      "Arousal RMSE: 0.27584547853822916\n",
      "Test R^2 score: tensor([0.2646, 0.2466], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1771, 0.1569], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.25558103519487285\n",
      "Num of epochs: 171\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Epoch 121, Loss: 0.5116998508836337\n",
      "Epoch 122, Loss: 0.5111129902664598\n",
      "Epoch 123, Loss: 0.5105405741436312\n",
      "Epoch 124, Loss: 0.5099849010083686\n",
      "Epoch 125, Loss: 0.5094623759868415\n",
      "Epoch 126, Loss: 0.5089652841754122\n",
      "Epoch 127, Loss: 0.5084717799132713\n",
      "Epoch 128, Loss: 0.5079809643002384\n",
      "Epoch 129, Loss: 0.507507731608986\n",
      "Epoch 130, Loss: 0.5070515727009828\n",
      "Epoch 131, Loss: 0.5065962678643066\n",
      "Epoch 132, Loss: 0.5061448812256618\n",
      "Epoch 133, Loss: 0.5057069703510908\n",
      "Epoch 134, Loss: 0.5052756693952567\n",
      "Epoch 135, Loss: 0.5048445017462813\n",
      "Epoch 136, Loss: 0.504425284231979\n",
      "Epoch 137, Loss: 0.504022717877823\n",
      "Epoch 138, Loss: 0.5036312802042249\n",
      "Epoch 139, Loss: 0.5032368731068207\n",
      "Epoch 140, Loss: 0.5028511060044271\n",
      "Epoch 141, Loss: 0.5024804933282331\n",
      "Epoch 142, Loss: 0.5021182727396313\n",
      "Epoch 143, Loss: 0.5017581664990297\n",
      "Epoch 144, Loss: 0.5014050827892457\n",
      "Epoch 145, Loss: 0.501047675902349\n",
      "Epoch 146, Loss: 0.500690966247362\n",
      "Epoch 147, Loss: 0.5003476722262875\n",
      "Epoch 148, Loss: 0.5000068544871652\n",
      "Epoch 149, Loss: 0.49966338867073357\n",
      "Epoch 150, Loss: 0.4993280873087794\n",
      "Epoch 151, Loss: 0.49900257940428383\n",
      "Epoch 152, Loss: 0.4986865554458311\n",
      "Epoch 153, Loss: 0.49836949389755036\n",
      "Epoch 154, Loss: 0.49805375636713634\n",
      "Epoch 155, Loss: 0.49773873165141763\n",
      "Epoch 156, Loss: 0.49742224924400874\n",
      "Epoch 157, Loss: 0.49711425826812494\n",
      "Epoch 158, Loss: 0.4968010973402242\n",
      "Epoch 159, Loss: 0.4964901399315246\n",
      "Epoch 160, Loss: 0.49617675026847124\n",
      "Epoch 161, Loss: 0.4958670390985117\n",
      "Epoch 162, Loss: 0.4955578109301908\n",
      "Epoch 163, Loss: 0.4952510223984696\n",
      "Epoch 164, Loss: 0.49494577484111674\n",
      "Epoch 165, Loss: 0.49464511374066544\n",
      "Epoch 166, Loss: 0.49434383269975535\n",
      "Epoch 167, Loss: 0.49404631909698943\n",
      "Epoch 168, Loss: 0.49374530645143583\n",
      "Epoch 169, Loss: 0.49345064806438055\n",
      "Epoch 170, Loss: 0.49317001490258333\n",
      "Epoch 171, Loss: 0.4928927591216884\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.26158102485529966\n",
      "Valence RMSE: 0.24637451321522125\n",
      "Arousal RMSE: 0.27595083686530203\n",
      "Test R^2 score: tensor([0.2644, 0.2460], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1769, 0.1563], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.25518519518148264\n",
      "Num of epochs: 172\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Epoch 121, Loss: 0.5116998508836337\n",
      "Epoch 122, Loss: 0.5111129902664598\n",
      "Epoch 123, Loss: 0.5105405741436312\n",
      "Epoch 124, Loss: 0.5099849010083686\n",
      "Epoch 125, Loss: 0.5094623759868415\n",
      "Epoch 126, Loss: 0.5089652841754122\n",
      "Epoch 127, Loss: 0.5084717799132713\n",
      "Epoch 128, Loss: 0.5079809643002384\n",
      "Epoch 129, Loss: 0.507507731608986\n",
      "Epoch 130, Loss: 0.5070515727009828\n",
      "Epoch 131, Loss: 0.5065962678643066\n",
      "Epoch 132, Loss: 0.5061448812256618\n",
      "Epoch 133, Loss: 0.5057069703510908\n",
      "Epoch 134, Loss: 0.5052756693952567\n",
      "Epoch 135, Loss: 0.5048445017462813\n",
      "Epoch 136, Loss: 0.504425284231979\n",
      "Epoch 137, Loss: 0.504022717877823\n",
      "Epoch 138, Loss: 0.5036312802042249\n",
      "Epoch 139, Loss: 0.5032368731068207\n",
      "Epoch 140, Loss: 0.5028511060044271\n",
      "Epoch 141, Loss: 0.5024804933282331\n",
      "Epoch 142, Loss: 0.5021182727396313\n",
      "Epoch 143, Loss: 0.5017581664990297\n",
      "Epoch 144, Loss: 0.5014050827892457\n",
      "Epoch 145, Loss: 0.501047675902349\n",
      "Epoch 146, Loss: 0.500690966247362\n",
      "Epoch 147, Loss: 0.5003476722262875\n",
      "Epoch 148, Loss: 0.5000068544871652\n",
      "Epoch 149, Loss: 0.49966338867073357\n",
      "Epoch 150, Loss: 0.4993280873087794\n",
      "Epoch 151, Loss: 0.49900257940428383\n",
      "Epoch 152, Loss: 0.4986865554458311\n",
      "Epoch 153, Loss: 0.49836949389755036\n",
      "Epoch 154, Loss: 0.49805375636713634\n",
      "Epoch 155, Loss: 0.49773873165141763\n",
      "Epoch 156, Loss: 0.49742224924400874\n",
      "Epoch 157, Loss: 0.49711425826812494\n",
      "Epoch 158, Loss: 0.4968010973402242\n",
      "Epoch 159, Loss: 0.4964901399315246\n",
      "Epoch 160, Loss: 0.49617675026847124\n",
      "Epoch 161, Loss: 0.4958670390985117\n",
      "Epoch 162, Loss: 0.4955578109301908\n",
      "Epoch 163, Loss: 0.4952510223984696\n",
      "Epoch 164, Loss: 0.49494577484111674\n",
      "Epoch 165, Loss: 0.49464511374066544\n",
      "Epoch 166, Loss: 0.49434383269975535\n",
      "Epoch 167, Loss: 0.49404631909698943\n",
      "Epoch 168, Loss: 0.49374530645143583\n",
      "Epoch 169, Loss: 0.49345064806438055\n",
      "Epoch 170, Loss: 0.49317001490258333\n",
      "Epoch 171, Loss: 0.4928927591216884\n",
      "Epoch 172, Loss: 0.4926138499628376\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2616787406984771\n",
      "Valence RMSE: 0.2464276605914817\n",
      "Arousal RMSE: 0.27608863569964465\n",
      "Test R^2 score: tensor([0.2641, 0.2452], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1765, 0.1555], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.25464987764331304\n",
      "Num of epochs: 173\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Epoch 121, Loss: 0.5116998508836337\n",
      "Epoch 122, Loss: 0.5111129902664598\n",
      "Epoch 123, Loss: 0.5105405741436312\n",
      "Epoch 124, Loss: 0.5099849010083686\n",
      "Epoch 125, Loss: 0.5094623759868415\n",
      "Epoch 126, Loss: 0.5089652841754122\n",
      "Epoch 127, Loss: 0.5084717799132713\n",
      "Epoch 128, Loss: 0.5079809643002384\n",
      "Epoch 129, Loss: 0.507507731608986\n",
      "Epoch 130, Loss: 0.5070515727009828\n",
      "Epoch 131, Loss: 0.5065962678643066\n",
      "Epoch 132, Loss: 0.5061448812256618\n",
      "Epoch 133, Loss: 0.5057069703510908\n",
      "Epoch 134, Loss: 0.5052756693952567\n",
      "Epoch 135, Loss: 0.5048445017462813\n",
      "Epoch 136, Loss: 0.504425284231979\n",
      "Epoch 137, Loss: 0.504022717877823\n",
      "Epoch 138, Loss: 0.5036312802042249\n",
      "Epoch 139, Loss: 0.5032368731068207\n",
      "Epoch 140, Loss: 0.5028511060044271\n",
      "Epoch 141, Loss: 0.5024804933282331\n",
      "Epoch 142, Loss: 0.5021182727396313\n",
      "Epoch 143, Loss: 0.5017581664990297\n",
      "Epoch 144, Loss: 0.5014050827892457\n",
      "Epoch 145, Loss: 0.501047675902349\n",
      "Epoch 146, Loss: 0.500690966247362\n",
      "Epoch 147, Loss: 0.5003476722262875\n",
      "Epoch 148, Loss: 0.5000068544871652\n",
      "Epoch 149, Loss: 0.49966338867073357\n",
      "Epoch 150, Loss: 0.4993280873087794\n",
      "Epoch 151, Loss: 0.49900257940428383\n",
      "Epoch 152, Loss: 0.4986865554458311\n",
      "Epoch 153, Loss: 0.49836949389755036\n",
      "Epoch 154, Loss: 0.49805375636713634\n",
      "Epoch 155, Loss: 0.49773873165141763\n",
      "Epoch 156, Loss: 0.49742224924400874\n",
      "Epoch 157, Loss: 0.49711425826812494\n",
      "Epoch 158, Loss: 0.4968010973402242\n",
      "Epoch 159, Loss: 0.4964901399315246\n",
      "Epoch 160, Loss: 0.49617675026847124\n",
      "Epoch 161, Loss: 0.4958670390985117\n",
      "Epoch 162, Loss: 0.4955578109301908\n",
      "Epoch 163, Loss: 0.4952510223984696\n",
      "Epoch 164, Loss: 0.49494577484111674\n",
      "Epoch 165, Loss: 0.49464511374066544\n",
      "Epoch 166, Loss: 0.49434383269975535\n",
      "Epoch 167, Loss: 0.49404631909698943\n",
      "Epoch 168, Loss: 0.49374530645143583\n",
      "Epoch 169, Loss: 0.49345064806438055\n",
      "Epoch 170, Loss: 0.49317001490258333\n",
      "Epoch 171, Loss: 0.4928927591216884\n",
      "Epoch 172, Loss: 0.4926138499628376\n",
      "Epoch 173, Loss: 0.49233528219511824\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.26178940150795676\n",
      "Valence RMSE: 0.24649283533490188\n",
      "Arousal RMSE: 0.2762402280848127\n",
      "Test R^2 score: tensor([0.2637, 0.2444], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1761, 0.1545], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.25404068226824283\n",
      "Num of epochs: 174\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Epoch 121, Loss: 0.5116998508836337\n",
      "Epoch 122, Loss: 0.5111129902664598\n",
      "Epoch 123, Loss: 0.5105405741436312\n",
      "Epoch 124, Loss: 0.5099849010083686\n",
      "Epoch 125, Loss: 0.5094623759868415\n",
      "Epoch 126, Loss: 0.5089652841754122\n",
      "Epoch 127, Loss: 0.5084717799132713\n",
      "Epoch 128, Loss: 0.5079809643002384\n",
      "Epoch 129, Loss: 0.507507731608986\n",
      "Epoch 130, Loss: 0.5070515727009828\n",
      "Epoch 131, Loss: 0.5065962678643066\n",
      "Epoch 132, Loss: 0.5061448812256618\n",
      "Epoch 133, Loss: 0.5057069703510908\n",
      "Epoch 134, Loss: 0.5052756693952567\n",
      "Epoch 135, Loss: 0.5048445017462813\n",
      "Epoch 136, Loss: 0.504425284231979\n",
      "Epoch 137, Loss: 0.504022717877823\n",
      "Epoch 138, Loss: 0.5036312802042249\n",
      "Epoch 139, Loss: 0.5032368731068207\n",
      "Epoch 140, Loss: 0.5028511060044271\n",
      "Epoch 141, Loss: 0.5024804933282331\n",
      "Epoch 142, Loss: 0.5021182727396313\n",
      "Epoch 143, Loss: 0.5017581664990297\n",
      "Epoch 144, Loss: 0.5014050827892457\n",
      "Epoch 145, Loss: 0.501047675902349\n",
      "Epoch 146, Loss: 0.500690966247362\n",
      "Epoch 147, Loss: 0.5003476722262875\n",
      "Epoch 148, Loss: 0.5000068544871652\n",
      "Epoch 149, Loss: 0.49966338867073357\n",
      "Epoch 150, Loss: 0.4993280873087794\n",
      "Epoch 151, Loss: 0.49900257940428383\n",
      "Epoch 152, Loss: 0.4986865554458311\n",
      "Epoch 153, Loss: 0.49836949389755036\n",
      "Epoch 154, Loss: 0.49805375636713634\n",
      "Epoch 155, Loss: 0.49773873165141763\n",
      "Epoch 156, Loss: 0.49742224924400874\n",
      "Epoch 157, Loss: 0.49711425826812494\n",
      "Epoch 158, Loss: 0.4968010973402242\n",
      "Epoch 159, Loss: 0.4964901399315246\n",
      "Epoch 160, Loss: 0.49617675026847124\n",
      "Epoch 161, Loss: 0.4958670390985117\n",
      "Epoch 162, Loss: 0.4955578109301908\n",
      "Epoch 163, Loss: 0.4952510223984696\n",
      "Epoch 164, Loss: 0.49494577484111674\n",
      "Epoch 165, Loss: 0.49464511374066544\n",
      "Epoch 166, Loss: 0.49434383269975535\n",
      "Epoch 167, Loss: 0.49404631909698943\n",
      "Epoch 168, Loss: 0.49374530645143583\n",
      "Epoch 169, Loss: 0.49345064806438055\n",
      "Epoch 170, Loss: 0.49317001490258333\n",
      "Epoch 171, Loss: 0.4928927591216884\n",
      "Epoch 172, Loss: 0.4926138499628376\n",
      "Epoch 173, Loss: 0.49233528219511824\n",
      "Epoch 174, Loss: 0.49205852514110515\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2619142191388205\n",
      "Valence RMSE: 0.24655621142848733\n",
      "Arousal RMSE: 0.2764202434342088\n",
      "Test R^2 score: tensor([0.2633, 0.2434], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1756, 0.1534], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.2533587919227035\n",
      "Num of epochs: 175\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Epoch 121, Loss: 0.5116998508836337\n",
      "Epoch 122, Loss: 0.5111129902664598\n",
      "Epoch 123, Loss: 0.5105405741436312\n",
      "Epoch 124, Loss: 0.5099849010083686\n",
      "Epoch 125, Loss: 0.5094623759868415\n",
      "Epoch 126, Loss: 0.5089652841754122\n",
      "Epoch 127, Loss: 0.5084717799132713\n",
      "Epoch 128, Loss: 0.5079809643002384\n",
      "Epoch 129, Loss: 0.507507731608986\n",
      "Epoch 130, Loss: 0.5070515727009828\n",
      "Epoch 131, Loss: 0.5065962678643066\n",
      "Epoch 132, Loss: 0.5061448812256618\n",
      "Epoch 133, Loss: 0.5057069703510908\n",
      "Epoch 134, Loss: 0.5052756693952567\n",
      "Epoch 135, Loss: 0.5048445017462813\n",
      "Epoch 136, Loss: 0.504425284231979\n",
      "Epoch 137, Loss: 0.504022717877823\n",
      "Epoch 138, Loss: 0.5036312802042249\n",
      "Epoch 139, Loss: 0.5032368731068207\n",
      "Epoch 140, Loss: 0.5028511060044271\n",
      "Epoch 141, Loss: 0.5024804933282331\n",
      "Epoch 142, Loss: 0.5021182727396313\n",
      "Epoch 143, Loss: 0.5017581664990297\n",
      "Epoch 144, Loss: 0.5014050827892457\n",
      "Epoch 145, Loss: 0.501047675902349\n",
      "Epoch 146, Loss: 0.500690966247362\n",
      "Epoch 147, Loss: 0.5003476722262875\n",
      "Epoch 148, Loss: 0.5000068544871652\n",
      "Epoch 149, Loss: 0.49966338867073357\n",
      "Epoch 150, Loss: 0.4993280873087794\n",
      "Epoch 151, Loss: 0.49900257940428383\n",
      "Epoch 152, Loss: 0.4986865554458311\n",
      "Epoch 153, Loss: 0.49836949389755036\n",
      "Epoch 154, Loss: 0.49805375636713634\n",
      "Epoch 155, Loss: 0.49773873165141763\n",
      "Epoch 156, Loss: 0.49742224924400874\n",
      "Epoch 157, Loss: 0.49711425826812494\n",
      "Epoch 158, Loss: 0.4968010973402242\n",
      "Epoch 159, Loss: 0.4964901399315246\n",
      "Epoch 160, Loss: 0.49617675026847124\n",
      "Epoch 161, Loss: 0.4958670390985117\n",
      "Epoch 162, Loss: 0.4955578109301908\n",
      "Epoch 163, Loss: 0.4952510223984696\n",
      "Epoch 164, Loss: 0.49494577484111674\n",
      "Epoch 165, Loss: 0.49464511374066544\n",
      "Epoch 166, Loss: 0.49434383269975535\n",
      "Epoch 167, Loss: 0.49404631909698943\n",
      "Epoch 168, Loss: 0.49374530645143583\n",
      "Epoch 169, Loss: 0.49345064806438055\n",
      "Epoch 170, Loss: 0.49317001490258333\n",
      "Epoch 171, Loss: 0.4928927591216884\n",
      "Epoch 172, Loss: 0.4926138499628376\n",
      "Epoch 173, Loss: 0.49233528219511824\n",
      "Epoch 174, Loss: 0.49205852514110515\n",
      "Epoch 175, Loss: 0.4917773399684819\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.26207694101527423\n",
      "Valence RMSE: 0.2466437543661465\n",
      "Arousal RMSE: 0.2766505095892995\n",
      "Test R^2 score: tensor([0.2628, 0.2422], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1751, 0.1520], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.252466659295186\n",
      "Num of epochs: 176\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Epoch 121, Loss: 0.5116998508836337\n",
      "Epoch 122, Loss: 0.5111129902664598\n",
      "Epoch 123, Loss: 0.5105405741436312\n",
      "Epoch 124, Loss: 0.5099849010083686\n",
      "Epoch 125, Loss: 0.5094623759868415\n",
      "Epoch 126, Loss: 0.5089652841754122\n",
      "Epoch 127, Loss: 0.5084717799132713\n",
      "Epoch 128, Loss: 0.5079809643002384\n",
      "Epoch 129, Loss: 0.507507731608986\n",
      "Epoch 130, Loss: 0.5070515727009828\n",
      "Epoch 131, Loss: 0.5065962678643066\n",
      "Epoch 132, Loss: 0.5061448812256618\n",
      "Epoch 133, Loss: 0.5057069703510908\n",
      "Epoch 134, Loss: 0.5052756693952567\n",
      "Epoch 135, Loss: 0.5048445017462813\n",
      "Epoch 136, Loss: 0.504425284231979\n",
      "Epoch 137, Loss: 0.504022717877823\n",
      "Epoch 138, Loss: 0.5036312802042249\n",
      "Epoch 139, Loss: 0.5032368731068207\n",
      "Epoch 140, Loss: 0.5028511060044271\n",
      "Epoch 141, Loss: 0.5024804933282331\n",
      "Epoch 142, Loss: 0.5021182727396313\n",
      "Epoch 143, Loss: 0.5017581664990297\n",
      "Epoch 144, Loss: 0.5014050827892457\n",
      "Epoch 145, Loss: 0.501047675902349\n",
      "Epoch 146, Loss: 0.500690966247362\n",
      "Epoch 147, Loss: 0.5003476722262875\n",
      "Epoch 148, Loss: 0.5000068544871652\n",
      "Epoch 149, Loss: 0.49966338867073357\n",
      "Epoch 150, Loss: 0.4993280873087794\n",
      "Epoch 151, Loss: 0.49900257940428383\n",
      "Epoch 152, Loss: 0.4986865554458311\n",
      "Epoch 153, Loss: 0.49836949389755036\n",
      "Epoch 154, Loss: 0.49805375636713634\n",
      "Epoch 155, Loss: 0.49773873165141763\n",
      "Epoch 156, Loss: 0.49742224924400874\n",
      "Epoch 157, Loss: 0.49711425826812494\n",
      "Epoch 158, Loss: 0.4968010973402242\n",
      "Epoch 159, Loss: 0.4964901399315246\n",
      "Epoch 160, Loss: 0.49617675026847124\n",
      "Epoch 161, Loss: 0.4958670390985117\n",
      "Epoch 162, Loss: 0.4955578109301908\n",
      "Epoch 163, Loss: 0.4952510223984696\n",
      "Epoch 164, Loss: 0.49494577484111674\n",
      "Epoch 165, Loss: 0.49464511374066544\n",
      "Epoch 166, Loss: 0.49434383269975535\n",
      "Epoch 167, Loss: 0.49404631909698943\n",
      "Epoch 168, Loss: 0.49374530645143583\n",
      "Epoch 169, Loss: 0.49345064806438055\n",
      "Epoch 170, Loss: 0.49317001490258333\n",
      "Epoch 171, Loss: 0.4928927591216884\n",
      "Epoch 172, Loss: 0.4926138499628376\n",
      "Epoch 173, Loss: 0.49233528219511824\n",
      "Epoch 174, Loss: 0.49205852514110515\n",
      "Epoch 175, Loss: 0.4917773399684819\n",
      "Epoch 176, Loss: 0.49149629710928233\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.26225832733555937\n",
      "Valence RMSE: 0.24675180282011142\n",
      "Arousal RMSE: 0.27689783010835545\n",
      "Test R^2 score: tensor([0.2621, 0.2408], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1743, 0.1505], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.2514658345301876\n",
      "Num of epochs: 177\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Epoch 121, Loss: 0.5116998508836337\n",
      "Epoch 122, Loss: 0.5111129902664598\n",
      "Epoch 123, Loss: 0.5105405741436312\n",
      "Epoch 124, Loss: 0.5099849010083686\n",
      "Epoch 125, Loss: 0.5094623759868415\n",
      "Epoch 126, Loss: 0.5089652841754122\n",
      "Epoch 127, Loss: 0.5084717799132713\n",
      "Epoch 128, Loss: 0.5079809643002384\n",
      "Epoch 129, Loss: 0.507507731608986\n",
      "Epoch 130, Loss: 0.5070515727009828\n",
      "Epoch 131, Loss: 0.5065962678643066\n",
      "Epoch 132, Loss: 0.5061448812256618\n",
      "Epoch 133, Loss: 0.5057069703510908\n",
      "Epoch 134, Loss: 0.5052756693952567\n",
      "Epoch 135, Loss: 0.5048445017462813\n",
      "Epoch 136, Loss: 0.504425284231979\n",
      "Epoch 137, Loss: 0.504022717877823\n",
      "Epoch 138, Loss: 0.5036312802042249\n",
      "Epoch 139, Loss: 0.5032368731068207\n",
      "Epoch 140, Loss: 0.5028511060044271\n",
      "Epoch 141, Loss: 0.5024804933282331\n",
      "Epoch 142, Loss: 0.5021182727396313\n",
      "Epoch 143, Loss: 0.5017581664990297\n",
      "Epoch 144, Loss: 0.5014050827892457\n",
      "Epoch 145, Loss: 0.501047675902349\n",
      "Epoch 146, Loss: 0.500690966247362\n",
      "Epoch 147, Loss: 0.5003476722262875\n",
      "Epoch 148, Loss: 0.5000068544871652\n",
      "Epoch 149, Loss: 0.49966338867073357\n",
      "Epoch 150, Loss: 0.4993280873087794\n",
      "Epoch 151, Loss: 0.49900257940428383\n",
      "Epoch 152, Loss: 0.4986865554458311\n",
      "Epoch 153, Loss: 0.49836949389755036\n",
      "Epoch 154, Loss: 0.49805375636713634\n",
      "Epoch 155, Loss: 0.49773873165141763\n",
      "Epoch 156, Loss: 0.49742224924400874\n",
      "Epoch 157, Loss: 0.49711425826812494\n",
      "Epoch 158, Loss: 0.4968010973402242\n",
      "Epoch 159, Loss: 0.4964901399315246\n",
      "Epoch 160, Loss: 0.49617675026847124\n",
      "Epoch 161, Loss: 0.4958670390985117\n",
      "Epoch 162, Loss: 0.4955578109301908\n",
      "Epoch 163, Loss: 0.4952510223984696\n",
      "Epoch 164, Loss: 0.49494577484111674\n",
      "Epoch 165, Loss: 0.49464511374066544\n",
      "Epoch 166, Loss: 0.49434383269975535\n",
      "Epoch 167, Loss: 0.49404631909698943\n",
      "Epoch 168, Loss: 0.49374530645143583\n",
      "Epoch 169, Loss: 0.49345064806438055\n",
      "Epoch 170, Loss: 0.49317001490258333\n",
      "Epoch 171, Loss: 0.4928927591216884\n",
      "Epoch 172, Loss: 0.4926138499628376\n",
      "Epoch 173, Loss: 0.49233528219511824\n",
      "Epoch 174, Loss: 0.49205852514110515\n",
      "Epoch 175, Loss: 0.4917773399684819\n",
      "Epoch 176, Loss: 0.49149629710928233\n",
      "Epoch 177, Loss: 0.4912165647150265\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2624387164239051\n",
      "Valence RMSE: 0.2468727371126721\n",
      "Arousal RMSE: 0.2771317582431707\n",
      "Test R^2 score: tensor([0.2614, 0.2395], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1735, 0.1491], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.250462461663468\n",
      "Num of epochs: 178\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Epoch 121, Loss: 0.5116998508836337\n",
      "Epoch 122, Loss: 0.5111129902664598\n",
      "Epoch 123, Loss: 0.5105405741436312\n",
      "Epoch 124, Loss: 0.5099849010083686\n",
      "Epoch 125, Loss: 0.5094623759868415\n",
      "Epoch 126, Loss: 0.5089652841754122\n",
      "Epoch 127, Loss: 0.5084717799132713\n",
      "Epoch 128, Loss: 0.5079809643002384\n",
      "Epoch 129, Loss: 0.507507731608986\n",
      "Epoch 130, Loss: 0.5070515727009828\n",
      "Epoch 131, Loss: 0.5065962678643066\n",
      "Epoch 132, Loss: 0.5061448812256618\n",
      "Epoch 133, Loss: 0.5057069703510908\n",
      "Epoch 134, Loss: 0.5052756693952567\n",
      "Epoch 135, Loss: 0.5048445017462813\n",
      "Epoch 136, Loss: 0.504425284231979\n",
      "Epoch 137, Loss: 0.504022717877823\n",
      "Epoch 138, Loss: 0.5036312802042249\n",
      "Epoch 139, Loss: 0.5032368731068207\n",
      "Epoch 140, Loss: 0.5028511060044271\n",
      "Epoch 141, Loss: 0.5024804933282331\n",
      "Epoch 142, Loss: 0.5021182727396313\n",
      "Epoch 143, Loss: 0.5017581664990297\n",
      "Epoch 144, Loss: 0.5014050827892457\n",
      "Epoch 145, Loss: 0.501047675902349\n",
      "Epoch 146, Loss: 0.500690966247362\n",
      "Epoch 147, Loss: 0.5003476722262875\n",
      "Epoch 148, Loss: 0.5000068544871652\n",
      "Epoch 149, Loss: 0.49966338867073357\n",
      "Epoch 150, Loss: 0.4993280873087794\n",
      "Epoch 151, Loss: 0.49900257940428383\n",
      "Epoch 152, Loss: 0.4986865554458311\n",
      "Epoch 153, Loss: 0.49836949389755036\n",
      "Epoch 154, Loss: 0.49805375636713634\n",
      "Epoch 155, Loss: 0.49773873165141763\n",
      "Epoch 156, Loss: 0.49742224924400874\n",
      "Epoch 157, Loss: 0.49711425826812494\n",
      "Epoch 158, Loss: 0.4968010973402242\n",
      "Epoch 159, Loss: 0.4964901399315246\n",
      "Epoch 160, Loss: 0.49617675026847124\n",
      "Epoch 161, Loss: 0.4958670390985117\n",
      "Epoch 162, Loss: 0.4955578109301908\n",
      "Epoch 163, Loss: 0.4952510223984696\n",
      "Epoch 164, Loss: 0.49494577484111674\n",
      "Epoch 165, Loss: 0.49464511374066544\n",
      "Epoch 166, Loss: 0.49434383269975535\n",
      "Epoch 167, Loss: 0.49404631909698943\n",
      "Epoch 168, Loss: 0.49374530645143583\n",
      "Epoch 169, Loss: 0.49345064806438055\n",
      "Epoch 170, Loss: 0.49317001490258333\n",
      "Epoch 171, Loss: 0.4928927591216884\n",
      "Epoch 172, Loss: 0.4926138499628376\n",
      "Epoch 173, Loss: 0.49233528219511824\n",
      "Epoch 174, Loss: 0.49205852514110515\n",
      "Epoch 175, Loss: 0.4917773399684819\n",
      "Epoch 176, Loss: 0.49149629710928233\n",
      "Epoch 177, Loss: 0.4912165647150265\n",
      "Epoch 178, Loss: 0.49094181765497674\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.26262027867165333\n",
      "Valence RMSE: 0.24700620751832614\n",
      "Arousal RMSE: 0.2773567287566092\n",
      "Test R^2 score: tensor([0.2606, 0.2383], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1726, 0.1477], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.2494454449114245\n",
      "Num of epochs: 179\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Epoch 121, Loss: 0.5116998508836337\n",
      "Epoch 122, Loss: 0.5111129902664598\n",
      "Epoch 123, Loss: 0.5105405741436312\n",
      "Epoch 124, Loss: 0.5099849010083686\n",
      "Epoch 125, Loss: 0.5094623759868415\n",
      "Epoch 126, Loss: 0.5089652841754122\n",
      "Epoch 127, Loss: 0.5084717799132713\n",
      "Epoch 128, Loss: 0.5079809643002384\n",
      "Epoch 129, Loss: 0.507507731608986\n",
      "Epoch 130, Loss: 0.5070515727009828\n",
      "Epoch 131, Loss: 0.5065962678643066\n",
      "Epoch 132, Loss: 0.5061448812256618\n",
      "Epoch 133, Loss: 0.5057069703510908\n",
      "Epoch 134, Loss: 0.5052756693952567\n",
      "Epoch 135, Loss: 0.5048445017462813\n",
      "Epoch 136, Loss: 0.504425284231979\n",
      "Epoch 137, Loss: 0.504022717877823\n",
      "Epoch 138, Loss: 0.5036312802042249\n",
      "Epoch 139, Loss: 0.5032368731068207\n",
      "Epoch 140, Loss: 0.5028511060044271\n",
      "Epoch 141, Loss: 0.5024804933282331\n",
      "Epoch 142, Loss: 0.5021182727396313\n",
      "Epoch 143, Loss: 0.5017581664990297\n",
      "Epoch 144, Loss: 0.5014050827892457\n",
      "Epoch 145, Loss: 0.501047675902349\n",
      "Epoch 146, Loss: 0.500690966247362\n",
      "Epoch 147, Loss: 0.5003476722262875\n",
      "Epoch 148, Loss: 0.5000068544871652\n",
      "Epoch 149, Loss: 0.49966338867073357\n",
      "Epoch 150, Loss: 0.4993280873087794\n",
      "Epoch 151, Loss: 0.49900257940428383\n",
      "Epoch 152, Loss: 0.4986865554458311\n",
      "Epoch 153, Loss: 0.49836949389755036\n",
      "Epoch 154, Loss: 0.49805375636713634\n",
      "Epoch 155, Loss: 0.49773873165141763\n",
      "Epoch 156, Loss: 0.49742224924400874\n",
      "Epoch 157, Loss: 0.49711425826812494\n",
      "Epoch 158, Loss: 0.4968010973402242\n",
      "Epoch 159, Loss: 0.4964901399315246\n",
      "Epoch 160, Loss: 0.49617675026847124\n",
      "Epoch 161, Loss: 0.4958670390985117\n",
      "Epoch 162, Loss: 0.4955578109301908\n",
      "Epoch 163, Loss: 0.4952510223984696\n",
      "Epoch 164, Loss: 0.49494577484111674\n",
      "Epoch 165, Loss: 0.49464511374066544\n",
      "Epoch 166, Loss: 0.49434383269975535\n",
      "Epoch 167, Loss: 0.49404631909698943\n",
      "Epoch 168, Loss: 0.49374530645143583\n",
      "Epoch 169, Loss: 0.49345064806438055\n",
      "Epoch 170, Loss: 0.49317001490258333\n",
      "Epoch 171, Loss: 0.4928927591216884\n",
      "Epoch 172, Loss: 0.4926138499628376\n",
      "Epoch 173, Loss: 0.49233528219511824\n",
      "Epoch 174, Loss: 0.49205852514110515\n",
      "Epoch 175, Loss: 0.4917773399684819\n",
      "Epoch 176, Loss: 0.49149629710928233\n",
      "Epoch 177, Loss: 0.4912165647150265\n",
      "Epoch 178, Loss: 0.49094181765497674\n",
      "Epoch 179, Loss: 0.49066901222163783\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.262825339963342\n",
      "Valence RMSE: 0.24718440532500727\n",
      "Arousal RMSE: 0.2775863620890175\n",
      "Test R^2 score: tensor([0.2595, 0.2370], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1714, 0.1463], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.24828092066311003\n",
      "Num of epochs: 180\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Epoch 121, Loss: 0.5116998508836337\n",
      "Epoch 122, Loss: 0.5111129902664598\n",
      "Epoch 123, Loss: 0.5105405741436312\n",
      "Epoch 124, Loss: 0.5099849010083686\n",
      "Epoch 125, Loss: 0.5094623759868415\n",
      "Epoch 126, Loss: 0.5089652841754122\n",
      "Epoch 127, Loss: 0.5084717799132713\n",
      "Epoch 128, Loss: 0.5079809643002384\n",
      "Epoch 129, Loss: 0.507507731608986\n",
      "Epoch 130, Loss: 0.5070515727009828\n",
      "Epoch 131, Loss: 0.5065962678643066\n",
      "Epoch 132, Loss: 0.5061448812256618\n",
      "Epoch 133, Loss: 0.5057069703510908\n",
      "Epoch 134, Loss: 0.5052756693952567\n",
      "Epoch 135, Loss: 0.5048445017462813\n",
      "Epoch 136, Loss: 0.504425284231979\n",
      "Epoch 137, Loss: 0.504022717877823\n",
      "Epoch 138, Loss: 0.5036312802042249\n",
      "Epoch 139, Loss: 0.5032368731068207\n",
      "Epoch 140, Loss: 0.5028511060044271\n",
      "Epoch 141, Loss: 0.5024804933282331\n",
      "Epoch 142, Loss: 0.5021182727396313\n",
      "Epoch 143, Loss: 0.5017581664990297\n",
      "Epoch 144, Loss: 0.5014050827892457\n",
      "Epoch 145, Loss: 0.501047675902349\n",
      "Epoch 146, Loss: 0.500690966247362\n",
      "Epoch 147, Loss: 0.5003476722262875\n",
      "Epoch 148, Loss: 0.5000068544871652\n",
      "Epoch 149, Loss: 0.49966338867073357\n",
      "Epoch 150, Loss: 0.4993280873087794\n",
      "Epoch 151, Loss: 0.49900257940428383\n",
      "Epoch 152, Loss: 0.4986865554458311\n",
      "Epoch 153, Loss: 0.49836949389755036\n",
      "Epoch 154, Loss: 0.49805375636713634\n",
      "Epoch 155, Loss: 0.49773873165141763\n",
      "Epoch 156, Loss: 0.49742224924400874\n",
      "Epoch 157, Loss: 0.49711425826812494\n",
      "Epoch 158, Loss: 0.4968010973402242\n",
      "Epoch 159, Loss: 0.4964901399315246\n",
      "Epoch 160, Loss: 0.49617675026847124\n",
      "Epoch 161, Loss: 0.4958670390985117\n",
      "Epoch 162, Loss: 0.4955578109301908\n",
      "Epoch 163, Loss: 0.4952510223984696\n",
      "Epoch 164, Loss: 0.49494577484111674\n",
      "Epoch 165, Loss: 0.49464511374066544\n",
      "Epoch 166, Loss: 0.49434383269975535\n",
      "Epoch 167, Loss: 0.49404631909698943\n",
      "Epoch 168, Loss: 0.49374530645143583\n",
      "Epoch 169, Loss: 0.49345064806438055\n",
      "Epoch 170, Loss: 0.49317001490258333\n",
      "Epoch 171, Loss: 0.4928927591216884\n",
      "Epoch 172, Loss: 0.4926138499628376\n",
      "Epoch 173, Loss: 0.49233528219511824\n",
      "Epoch 174, Loss: 0.49205852514110515\n",
      "Epoch 175, Loss: 0.4917773399684819\n",
      "Epoch 176, Loss: 0.49149629710928233\n",
      "Epoch 177, Loss: 0.4912165647150265\n",
      "Epoch 178, Loss: 0.49094181765497674\n",
      "Epoch 179, Loss: 0.49066901222163783\n",
      "Epoch 180, Loss: 0.490393928005003\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.26302333722358323\n",
      "Valence RMSE: 0.24737207672844183\n",
      "Arousal RMSE: 0.2777941819108519\n",
      "Test R^2 score: tensor([0.2584, 0.2359], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1702, 0.1450], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.24714709232230775\n",
      "Num of epochs: 181\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Epoch 121, Loss: 0.5116998508836337\n",
      "Epoch 122, Loss: 0.5111129902664598\n",
      "Epoch 123, Loss: 0.5105405741436312\n",
      "Epoch 124, Loss: 0.5099849010083686\n",
      "Epoch 125, Loss: 0.5094623759868415\n",
      "Epoch 126, Loss: 0.5089652841754122\n",
      "Epoch 127, Loss: 0.5084717799132713\n",
      "Epoch 128, Loss: 0.5079809643002384\n",
      "Epoch 129, Loss: 0.507507731608986\n",
      "Epoch 130, Loss: 0.5070515727009828\n",
      "Epoch 131, Loss: 0.5065962678643066\n",
      "Epoch 132, Loss: 0.5061448812256618\n",
      "Epoch 133, Loss: 0.5057069703510908\n",
      "Epoch 134, Loss: 0.5052756693952567\n",
      "Epoch 135, Loss: 0.5048445017462813\n",
      "Epoch 136, Loss: 0.504425284231979\n",
      "Epoch 137, Loss: 0.504022717877823\n",
      "Epoch 138, Loss: 0.5036312802042249\n",
      "Epoch 139, Loss: 0.5032368731068207\n",
      "Epoch 140, Loss: 0.5028511060044271\n",
      "Epoch 141, Loss: 0.5024804933282331\n",
      "Epoch 142, Loss: 0.5021182727396313\n",
      "Epoch 143, Loss: 0.5017581664990297\n",
      "Epoch 144, Loss: 0.5014050827892457\n",
      "Epoch 145, Loss: 0.501047675902349\n",
      "Epoch 146, Loss: 0.500690966247362\n",
      "Epoch 147, Loss: 0.5003476722262875\n",
      "Epoch 148, Loss: 0.5000068544871652\n",
      "Epoch 149, Loss: 0.49966338867073357\n",
      "Epoch 150, Loss: 0.4993280873087794\n",
      "Epoch 151, Loss: 0.49900257940428383\n",
      "Epoch 152, Loss: 0.4986865554458311\n",
      "Epoch 153, Loss: 0.49836949389755036\n",
      "Epoch 154, Loss: 0.49805375636713634\n",
      "Epoch 155, Loss: 0.49773873165141763\n",
      "Epoch 156, Loss: 0.49742224924400874\n",
      "Epoch 157, Loss: 0.49711425826812494\n",
      "Epoch 158, Loss: 0.4968010973402242\n",
      "Epoch 159, Loss: 0.4964901399315246\n",
      "Epoch 160, Loss: 0.49617675026847124\n",
      "Epoch 161, Loss: 0.4958670390985117\n",
      "Epoch 162, Loss: 0.4955578109301908\n",
      "Epoch 163, Loss: 0.4952510223984696\n",
      "Epoch 164, Loss: 0.49494577484111674\n",
      "Epoch 165, Loss: 0.49464511374066544\n",
      "Epoch 166, Loss: 0.49434383269975535\n",
      "Epoch 167, Loss: 0.49404631909698943\n",
      "Epoch 168, Loss: 0.49374530645143583\n",
      "Epoch 169, Loss: 0.49345064806438055\n",
      "Epoch 170, Loss: 0.49317001490258333\n",
      "Epoch 171, Loss: 0.4928927591216884\n",
      "Epoch 172, Loss: 0.4926138499628376\n",
      "Epoch 173, Loss: 0.49233528219511824\n",
      "Epoch 174, Loss: 0.49205852514110515\n",
      "Epoch 175, Loss: 0.4917773399684819\n",
      "Epoch 176, Loss: 0.49149629710928233\n",
      "Epoch 177, Loss: 0.4912165647150265\n",
      "Epoch 178, Loss: 0.49094181765497674\n",
      "Epoch 179, Loss: 0.49066901222163783\n",
      "Epoch 180, Loss: 0.490393928005003\n",
      "Epoch 181, Loss: 0.4901079113527018\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.26320266072803744\n",
      "Valence RMSE: 0.24755118261304826\n",
      "Arousal RMSE: 0.2779742671822663\n",
      "Test R^2 score: tensor([0.2573, 0.2349], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1690, 0.1439], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.24611444784034792\n",
      "Num of epochs: 182\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Epoch 121, Loss: 0.5116998508836337\n",
      "Epoch 122, Loss: 0.5111129902664598\n",
      "Epoch 123, Loss: 0.5105405741436312\n",
      "Epoch 124, Loss: 0.5099849010083686\n",
      "Epoch 125, Loss: 0.5094623759868415\n",
      "Epoch 126, Loss: 0.5089652841754122\n",
      "Epoch 127, Loss: 0.5084717799132713\n",
      "Epoch 128, Loss: 0.5079809643002384\n",
      "Epoch 129, Loss: 0.507507731608986\n",
      "Epoch 130, Loss: 0.5070515727009828\n",
      "Epoch 131, Loss: 0.5065962678643066\n",
      "Epoch 132, Loss: 0.5061448812256618\n",
      "Epoch 133, Loss: 0.5057069703510908\n",
      "Epoch 134, Loss: 0.5052756693952567\n",
      "Epoch 135, Loss: 0.5048445017462813\n",
      "Epoch 136, Loss: 0.504425284231979\n",
      "Epoch 137, Loss: 0.504022717877823\n",
      "Epoch 138, Loss: 0.5036312802042249\n",
      "Epoch 139, Loss: 0.5032368731068207\n",
      "Epoch 140, Loss: 0.5028511060044271\n",
      "Epoch 141, Loss: 0.5024804933282331\n",
      "Epoch 142, Loss: 0.5021182727396313\n",
      "Epoch 143, Loss: 0.5017581664990297\n",
      "Epoch 144, Loss: 0.5014050827892457\n",
      "Epoch 145, Loss: 0.501047675902349\n",
      "Epoch 146, Loss: 0.500690966247362\n",
      "Epoch 147, Loss: 0.5003476722262875\n",
      "Epoch 148, Loss: 0.5000068544871652\n",
      "Epoch 149, Loss: 0.49966338867073357\n",
      "Epoch 150, Loss: 0.4993280873087794\n",
      "Epoch 151, Loss: 0.49900257940428383\n",
      "Epoch 152, Loss: 0.4986865554458311\n",
      "Epoch 153, Loss: 0.49836949389755036\n",
      "Epoch 154, Loss: 0.49805375636713634\n",
      "Epoch 155, Loss: 0.49773873165141763\n",
      "Epoch 156, Loss: 0.49742224924400874\n",
      "Epoch 157, Loss: 0.49711425826812494\n",
      "Epoch 158, Loss: 0.4968010973402242\n",
      "Epoch 159, Loss: 0.4964901399315246\n",
      "Epoch 160, Loss: 0.49617675026847124\n",
      "Epoch 161, Loss: 0.4958670390985117\n",
      "Epoch 162, Loss: 0.4955578109301908\n",
      "Epoch 163, Loss: 0.4952510223984696\n",
      "Epoch 164, Loss: 0.49494577484111674\n",
      "Epoch 165, Loss: 0.49464511374066544\n",
      "Epoch 166, Loss: 0.49434383269975535\n",
      "Epoch 167, Loss: 0.49404631909698943\n",
      "Epoch 168, Loss: 0.49374530645143583\n",
      "Epoch 169, Loss: 0.49345064806438055\n",
      "Epoch 170, Loss: 0.49317001490258333\n",
      "Epoch 171, Loss: 0.4928927591216884\n",
      "Epoch 172, Loss: 0.4926138499628376\n",
      "Epoch 173, Loss: 0.49233528219511824\n",
      "Epoch 174, Loss: 0.49205852514110515\n",
      "Epoch 175, Loss: 0.4917773399684819\n",
      "Epoch 176, Loss: 0.49149629710928233\n",
      "Epoch 177, Loss: 0.4912165647150265\n",
      "Epoch 178, Loss: 0.49094181765497674\n",
      "Epoch 179, Loss: 0.49066901222163783\n",
      "Epoch 180, Loss: 0.490393928005003\n",
      "Epoch 181, Loss: 0.4901079113527018\n",
      "Epoch 182, Loss: 0.48981984154671765\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2634219263572985\n",
      "Valence RMSE: 0.24775597923260168\n",
      "Arousal RMSE: 0.27820711228520856\n",
      "Test R^2 score: tensor([0.2561, 0.2336], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1676, 0.1424], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.24485863499584526\n",
      "Num of epochs: 183\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Epoch 121, Loss: 0.5116998508836337\n",
      "Epoch 122, Loss: 0.5111129902664598\n",
      "Epoch 123, Loss: 0.5105405741436312\n",
      "Epoch 124, Loss: 0.5099849010083686\n",
      "Epoch 125, Loss: 0.5094623759868415\n",
      "Epoch 126, Loss: 0.5089652841754122\n",
      "Epoch 127, Loss: 0.5084717799132713\n",
      "Epoch 128, Loss: 0.5079809643002384\n",
      "Epoch 129, Loss: 0.507507731608986\n",
      "Epoch 130, Loss: 0.5070515727009828\n",
      "Epoch 131, Loss: 0.5065962678643066\n",
      "Epoch 132, Loss: 0.5061448812256618\n",
      "Epoch 133, Loss: 0.5057069703510908\n",
      "Epoch 134, Loss: 0.5052756693952567\n",
      "Epoch 135, Loss: 0.5048445017462813\n",
      "Epoch 136, Loss: 0.504425284231979\n",
      "Epoch 137, Loss: 0.504022717877823\n",
      "Epoch 138, Loss: 0.5036312802042249\n",
      "Epoch 139, Loss: 0.5032368731068207\n",
      "Epoch 140, Loss: 0.5028511060044271\n",
      "Epoch 141, Loss: 0.5024804933282331\n",
      "Epoch 142, Loss: 0.5021182727396313\n",
      "Epoch 143, Loss: 0.5017581664990297\n",
      "Epoch 144, Loss: 0.5014050827892457\n",
      "Epoch 145, Loss: 0.501047675902349\n",
      "Epoch 146, Loss: 0.500690966247362\n",
      "Epoch 147, Loss: 0.5003476722262875\n",
      "Epoch 148, Loss: 0.5000068544871652\n",
      "Epoch 149, Loss: 0.49966338867073357\n",
      "Epoch 150, Loss: 0.4993280873087794\n",
      "Epoch 151, Loss: 0.49900257940428383\n",
      "Epoch 152, Loss: 0.4986865554458311\n",
      "Epoch 153, Loss: 0.49836949389755036\n",
      "Epoch 154, Loss: 0.49805375636713634\n",
      "Epoch 155, Loss: 0.49773873165141763\n",
      "Epoch 156, Loss: 0.49742224924400874\n",
      "Epoch 157, Loss: 0.49711425826812494\n",
      "Epoch 158, Loss: 0.4968010973402242\n",
      "Epoch 159, Loss: 0.4964901399315246\n",
      "Epoch 160, Loss: 0.49617675026847124\n",
      "Epoch 161, Loss: 0.4958670390985117\n",
      "Epoch 162, Loss: 0.4955578109301908\n",
      "Epoch 163, Loss: 0.4952510223984696\n",
      "Epoch 164, Loss: 0.49494577484111674\n",
      "Epoch 165, Loss: 0.49464511374066544\n",
      "Epoch 166, Loss: 0.49434383269975535\n",
      "Epoch 167, Loss: 0.49404631909698943\n",
      "Epoch 168, Loss: 0.49374530645143583\n",
      "Epoch 169, Loss: 0.49345064806438055\n",
      "Epoch 170, Loss: 0.49317001490258333\n",
      "Epoch 171, Loss: 0.4928927591216884\n",
      "Epoch 172, Loss: 0.4926138499628376\n",
      "Epoch 173, Loss: 0.49233528219511824\n",
      "Epoch 174, Loss: 0.49205852514110515\n",
      "Epoch 175, Loss: 0.4917773399684819\n",
      "Epoch 176, Loss: 0.49149629710928233\n",
      "Epoch 177, Loss: 0.4912165647150265\n",
      "Epoch 178, Loss: 0.49094181765497674\n",
      "Epoch 179, Loss: 0.49066901222163783\n",
      "Epoch 180, Loss: 0.490393928005003\n",
      "Epoch 181, Loss: 0.4901079113527018\n",
      "Epoch 182, Loss: 0.48981984154671765\n",
      "Epoch 183, Loss: 0.4895234443341763\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2636409125445553\n",
      "Valence RMSE: 0.24795737479212732\n",
      "Arousal RMSE: 0.27844245692934155\n",
      "Test R^2 score: tensor([0.2549, 0.2323], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1662, 0.1410], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.24360510593488383\n",
      "Num of epochs: 184\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Epoch 121, Loss: 0.5116998508836337\n",
      "Epoch 122, Loss: 0.5111129902664598\n",
      "Epoch 123, Loss: 0.5105405741436312\n",
      "Epoch 124, Loss: 0.5099849010083686\n",
      "Epoch 125, Loss: 0.5094623759868415\n",
      "Epoch 126, Loss: 0.5089652841754122\n",
      "Epoch 127, Loss: 0.5084717799132713\n",
      "Epoch 128, Loss: 0.5079809643002384\n",
      "Epoch 129, Loss: 0.507507731608986\n",
      "Epoch 130, Loss: 0.5070515727009828\n",
      "Epoch 131, Loss: 0.5065962678643066\n",
      "Epoch 132, Loss: 0.5061448812256618\n",
      "Epoch 133, Loss: 0.5057069703510908\n",
      "Epoch 134, Loss: 0.5052756693952567\n",
      "Epoch 135, Loss: 0.5048445017462813\n",
      "Epoch 136, Loss: 0.504425284231979\n",
      "Epoch 137, Loss: 0.504022717877823\n",
      "Epoch 138, Loss: 0.5036312802042249\n",
      "Epoch 139, Loss: 0.5032368731068207\n",
      "Epoch 140, Loss: 0.5028511060044271\n",
      "Epoch 141, Loss: 0.5024804933282331\n",
      "Epoch 142, Loss: 0.5021182727396313\n",
      "Epoch 143, Loss: 0.5017581664990297\n",
      "Epoch 144, Loss: 0.5014050827892457\n",
      "Epoch 145, Loss: 0.501047675902349\n",
      "Epoch 146, Loss: 0.500690966247362\n",
      "Epoch 147, Loss: 0.5003476722262875\n",
      "Epoch 148, Loss: 0.5000068544871652\n",
      "Epoch 149, Loss: 0.49966338867073357\n",
      "Epoch 150, Loss: 0.4993280873087794\n",
      "Epoch 151, Loss: 0.49900257940428383\n",
      "Epoch 152, Loss: 0.4986865554458311\n",
      "Epoch 153, Loss: 0.49836949389755036\n",
      "Epoch 154, Loss: 0.49805375636713634\n",
      "Epoch 155, Loss: 0.49773873165141763\n",
      "Epoch 156, Loss: 0.49742224924400874\n",
      "Epoch 157, Loss: 0.49711425826812494\n",
      "Epoch 158, Loss: 0.4968010973402242\n",
      "Epoch 159, Loss: 0.4964901399315246\n",
      "Epoch 160, Loss: 0.49617675026847124\n",
      "Epoch 161, Loss: 0.4958670390985117\n",
      "Epoch 162, Loss: 0.4955578109301908\n",
      "Epoch 163, Loss: 0.4952510223984696\n",
      "Epoch 164, Loss: 0.49494577484111674\n",
      "Epoch 165, Loss: 0.49464511374066544\n",
      "Epoch 166, Loss: 0.49434383269975535\n",
      "Epoch 167, Loss: 0.49404631909698943\n",
      "Epoch 168, Loss: 0.49374530645143583\n",
      "Epoch 169, Loss: 0.49345064806438055\n",
      "Epoch 170, Loss: 0.49317001490258333\n",
      "Epoch 171, Loss: 0.4928927591216884\n",
      "Epoch 172, Loss: 0.4926138499628376\n",
      "Epoch 173, Loss: 0.49233528219511824\n",
      "Epoch 174, Loss: 0.49205852514110515\n",
      "Epoch 175, Loss: 0.4917773399684819\n",
      "Epoch 176, Loss: 0.49149629710928233\n",
      "Epoch 177, Loss: 0.4912165647150265\n",
      "Epoch 178, Loss: 0.49094181765497674\n",
      "Epoch 179, Loss: 0.49066901222163783\n",
      "Epoch 180, Loss: 0.490393928005003\n",
      "Epoch 181, Loss: 0.4901079113527018\n",
      "Epoch 182, Loss: 0.48981984154671765\n",
      "Epoch 183, Loss: 0.4895234443341763\n",
      "Epoch 184, Loss: 0.48921459258368105\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2638481677314661\n",
      "Valence RMSE: 0.2481560821531234\n",
      "Arousal RMSE: 0.27865798054409996\n",
      "Test R^2 score: tensor([0.2537, 0.2311], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1649, 0.1397], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.24241331321502402\n",
      "Num of epochs: 185\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Epoch 121, Loss: 0.5116998508836337\n",
      "Epoch 122, Loss: 0.5111129902664598\n",
      "Epoch 123, Loss: 0.5105405741436312\n",
      "Epoch 124, Loss: 0.5099849010083686\n",
      "Epoch 125, Loss: 0.5094623759868415\n",
      "Epoch 126, Loss: 0.5089652841754122\n",
      "Epoch 127, Loss: 0.5084717799132713\n",
      "Epoch 128, Loss: 0.5079809643002384\n",
      "Epoch 129, Loss: 0.507507731608986\n",
      "Epoch 130, Loss: 0.5070515727009828\n",
      "Epoch 131, Loss: 0.5065962678643066\n",
      "Epoch 132, Loss: 0.5061448812256618\n",
      "Epoch 133, Loss: 0.5057069703510908\n",
      "Epoch 134, Loss: 0.5052756693952567\n",
      "Epoch 135, Loss: 0.5048445017462813\n",
      "Epoch 136, Loss: 0.504425284231979\n",
      "Epoch 137, Loss: 0.504022717877823\n",
      "Epoch 138, Loss: 0.5036312802042249\n",
      "Epoch 139, Loss: 0.5032368731068207\n",
      "Epoch 140, Loss: 0.5028511060044271\n",
      "Epoch 141, Loss: 0.5024804933282331\n",
      "Epoch 142, Loss: 0.5021182727396313\n",
      "Epoch 143, Loss: 0.5017581664990297\n",
      "Epoch 144, Loss: 0.5014050827892457\n",
      "Epoch 145, Loss: 0.501047675902349\n",
      "Epoch 146, Loss: 0.500690966247362\n",
      "Epoch 147, Loss: 0.5003476722262875\n",
      "Epoch 148, Loss: 0.5000068544871652\n",
      "Epoch 149, Loss: 0.49966338867073357\n",
      "Epoch 150, Loss: 0.4993280873087794\n",
      "Epoch 151, Loss: 0.49900257940428383\n",
      "Epoch 152, Loss: 0.4986865554458311\n",
      "Epoch 153, Loss: 0.49836949389755036\n",
      "Epoch 154, Loss: 0.49805375636713634\n",
      "Epoch 155, Loss: 0.49773873165141763\n",
      "Epoch 156, Loss: 0.49742224924400874\n",
      "Epoch 157, Loss: 0.49711425826812494\n",
      "Epoch 158, Loss: 0.4968010973402242\n",
      "Epoch 159, Loss: 0.4964901399315246\n",
      "Epoch 160, Loss: 0.49617675026847124\n",
      "Epoch 161, Loss: 0.4958670390985117\n",
      "Epoch 162, Loss: 0.4955578109301908\n",
      "Epoch 163, Loss: 0.4952510223984696\n",
      "Epoch 164, Loss: 0.49494577484111674\n",
      "Epoch 165, Loss: 0.49464511374066544\n",
      "Epoch 166, Loss: 0.49434383269975535\n",
      "Epoch 167, Loss: 0.49404631909698943\n",
      "Epoch 168, Loss: 0.49374530645143583\n",
      "Epoch 169, Loss: 0.49345064806438055\n",
      "Epoch 170, Loss: 0.49317001490258333\n",
      "Epoch 171, Loss: 0.4928927591216884\n",
      "Epoch 172, Loss: 0.4926138499628376\n",
      "Epoch 173, Loss: 0.49233528219511824\n",
      "Epoch 174, Loss: 0.49205852514110515\n",
      "Epoch 175, Loss: 0.4917773399684819\n",
      "Epoch 176, Loss: 0.49149629710928233\n",
      "Epoch 177, Loss: 0.4912165647150265\n",
      "Epoch 178, Loss: 0.49094181765497674\n",
      "Epoch 179, Loss: 0.49066901222163783\n",
      "Epoch 180, Loss: 0.490393928005003\n",
      "Epoch 181, Loss: 0.4901079113527018\n",
      "Epoch 182, Loss: 0.48981984154671765\n",
      "Epoch 183, Loss: 0.4895234443341763\n",
      "Epoch 184, Loss: 0.48921459258368105\n",
      "Epoch 185, Loss: 0.4889145977883947\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2640922606938513\n",
      "Valence RMSE: 0.24837830454053964\n",
      "Arousal RMSE: 0.27892232278959883\n",
      "Test R^2 score: tensor([0.2524, 0.2297], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1634, 0.1380], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.24101498743883687\n",
      "Num of epochs: 186\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Epoch 121, Loss: 0.5116998508836337\n",
      "Epoch 122, Loss: 0.5111129902664598\n",
      "Epoch 123, Loss: 0.5105405741436312\n",
      "Epoch 124, Loss: 0.5099849010083686\n",
      "Epoch 125, Loss: 0.5094623759868415\n",
      "Epoch 126, Loss: 0.5089652841754122\n",
      "Epoch 127, Loss: 0.5084717799132713\n",
      "Epoch 128, Loss: 0.5079809643002384\n",
      "Epoch 129, Loss: 0.507507731608986\n",
      "Epoch 130, Loss: 0.5070515727009828\n",
      "Epoch 131, Loss: 0.5065962678643066\n",
      "Epoch 132, Loss: 0.5061448812256618\n",
      "Epoch 133, Loss: 0.5057069703510908\n",
      "Epoch 134, Loss: 0.5052756693952567\n",
      "Epoch 135, Loss: 0.5048445017462813\n",
      "Epoch 136, Loss: 0.504425284231979\n",
      "Epoch 137, Loss: 0.504022717877823\n",
      "Epoch 138, Loss: 0.5036312802042249\n",
      "Epoch 139, Loss: 0.5032368731068207\n",
      "Epoch 140, Loss: 0.5028511060044271\n",
      "Epoch 141, Loss: 0.5024804933282331\n",
      "Epoch 142, Loss: 0.5021182727396313\n",
      "Epoch 143, Loss: 0.5017581664990297\n",
      "Epoch 144, Loss: 0.5014050827892457\n",
      "Epoch 145, Loss: 0.501047675902349\n",
      "Epoch 146, Loss: 0.500690966247362\n",
      "Epoch 147, Loss: 0.5003476722262875\n",
      "Epoch 148, Loss: 0.5000068544871652\n",
      "Epoch 149, Loss: 0.49966338867073357\n",
      "Epoch 150, Loss: 0.4993280873087794\n",
      "Epoch 151, Loss: 0.49900257940428383\n",
      "Epoch 152, Loss: 0.4986865554458311\n",
      "Epoch 153, Loss: 0.49836949389755036\n",
      "Epoch 154, Loss: 0.49805375636713634\n",
      "Epoch 155, Loss: 0.49773873165141763\n",
      "Epoch 156, Loss: 0.49742224924400874\n",
      "Epoch 157, Loss: 0.49711425826812494\n",
      "Epoch 158, Loss: 0.4968010973402242\n",
      "Epoch 159, Loss: 0.4964901399315246\n",
      "Epoch 160, Loss: 0.49617675026847124\n",
      "Epoch 161, Loss: 0.4958670390985117\n",
      "Epoch 162, Loss: 0.4955578109301908\n",
      "Epoch 163, Loss: 0.4952510223984696\n",
      "Epoch 164, Loss: 0.49494577484111674\n",
      "Epoch 165, Loss: 0.49464511374066544\n",
      "Epoch 166, Loss: 0.49434383269975535\n",
      "Epoch 167, Loss: 0.49404631909698943\n",
      "Epoch 168, Loss: 0.49374530645143583\n",
      "Epoch 169, Loss: 0.49345064806438055\n",
      "Epoch 170, Loss: 0.49317001490258333\n",
      "Epoch 171, Loss: 0.4928927591216884\n",
      "Epoch 172, Loss: 0.4926138499628376\n",
      "Epoch 173, Loss: 0.49233528219511824\n",
      "Epoch 174, Loss: 0.49205852514110515\n",
      "Epoch 175, Loss: 0.4917773399684819\n",
      "Epoch 176, Loss: 0.49149629710928233\n",
      "Epoch 177, Loss: 0.4912165647150265\n",
      "Epoch 178, Loss: 0.49094181765497674\n",
      "Epoch 179, Loss: 0.49066901222163783\n",
      "Epoch 180, Loss: 0.490393928005003\n",
      "Epoch 181, Loss: 0.4901079113527018\n",
      "Epoch 182, Loss: 0.48981984154671765\n",
      "Epoch 183, Loss: 0.4895234443341763\n",
      "Epoch 184, Loss: 0.48921459258368105\n",
      "Epoch 185, Loss: 0.4889145977883947\n",
      "Epoch 186, Loss: 0.4886175142174919\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.26437875039725417\n",
      "Valence RMSE: 0.2486218000438466\n",
      "Arousal RMSE: 0.2792480042295465\n",
      "Test R^2 score: tensor([0.2509, 0.2279], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1618, 0.1360], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.23938169140539484\n",
      "Num of epochs: 187\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Epoch 121, Loss: 0.5116998508836337\n",
      "Epoch 122, Loss: 0.5111129902664598\n",
      "Epoch 123, Loss: 0.5105405741436312\n",
      "Epoch 124, Loss: 0.5099849010083686\n",
      "Epoch 125, Loss: 0.5094623759868415\n",
      "Epoch 126, Loss: 0.5089652841754122\n",
      "Epoch 127, Loss: 0.5084717799132713\n",
      "Epoch 128, Loss: 0.5079809643002384\n",
      "Epoch 129, Loss: 0.507507731608986\n",
      "Epoch 130, Loss: 0.5070515727009828\n",
      "Epoch 131, Loss: 0.5065962678643066\n",
      "Epoch 132, Loss: 0.5061448812256618\n",
      "Epoch 133, Loss: 0.5057069703510908\n",
      "Epoch 134, Loss: 0.5052756693952567\n",
      "Epoch 135, Loss: 0.5048445017462813\n",
      "Epoch 136, Loss: 0.504425284231979\n",
      "Epoch 137, Loss: 0.504022717877823\n",
      "Epoch 138, Loss: 0.5036312802042249\n",
      "Epoch 139, Loss: 0.5032368731068207\n",
      "Epoch 140, Loss: 0.5028511060044271\n",
      "Epoch 141, Loss: 0.5024804933282331\n",
      "Epoch 142, Loss: 0.5021182727396313\n",
      "Epoch 143, Loss: 0.5017581664990297\n",
      "Epoch 144, Loss: 0.5014050827892457\n",
      "Epoch 145, Loss: 0.501047675902349\n",
      "Epoch 146, Loss: 0.500690966247362\n",
      "Epoch 147, Loss: 0.5003476722262875\n",
      "Epoch 148, Loss: 0.5000068544871652\n",
      "Epoch 149, Loss: 0.49966338867073357\n",
      "Epoch 150, Loss: 0.4993280873087794\n",
      "Epoch 151, Loss: 0.49900257940428383\n",
      "Epoch 152, Loss: 0.4986865554458311\n",
      "Epoch 153, Loss: 0.49836949389755036\n",
      "Epoch 154, Loss: 0.49805375636713634\n",
      "Epoch 155, Loss: 0.49773873165141763\n",
      "Epoch 156, Loss: 0.49742224924400874\n",
      "Epoch 157, Loss: 0.49711425826812494\n",
      "Epoch 158, Loss: 0.4968010973402242\n",
      "Epoch 159, Loss: 0.4964901399315246\n",
      "Epoch 160, Loss: 0.49617675026847124\n",
      "Epoch 161, Loss: 0.4958670390985117\n",
      "Epoch 162, Loss: 0.4955578109301908\n",
      "Epoch 163, Loss: 0.4952510223984696\n",
      "Epoch 164, Loss: 0.49494577484111674\n",
      "Epoch 165, Loss: 0.49464511374066544\n",
      "Epoch 166, Loss: 0.49434383269975535\n",
      "Epoch 167, Loss: 0.49404631909698943\n",
      "Epoch 168, Loss: 0.49374530645143583\n",
      "Epoch 169, Loss: 0.49345064806438055\n",
      "Epoch 170, Loss: 0.49317001490258333\n",
      "Epoch 171, Loss: 0.4928927591216884\n",
      "Epoch 172, Loss: 0.4926138499628376\n",
      "Epoch 173, Loss: 0.49233528219511824\n",
      "Epoch 174, Loss: 0.49205852514110515\n",
      "Epoch 175, Loss: 0.4917773399684819\n",
      "Epoch 176, Loss: 0.49149629710928233\n",
      "Epoch 177, Loss: 0.4912165647150265\n",
      "Epoch 178, Loss: 0.49094181765497674\n",
      "Epoch 179, Loss: 0.49066901222163783\n",
      "Epoch 180, Loss: 0.490393928005003\n",
      "Epoch 181, Loss: 0.4901079113527018\n",
      "Epoch 182, Loss: 0.48981984154671765\n",
      "Epoch 183, Loss: 0.4895234443341763\n",
      "Epoch 184, Loss: 0.48921459258368105\n",
      "Epoch 185, Loss: 0.4889145977883947\n",
      "Epoch 186, Loss: 0.4886175142174919\n",
      "Epoch 187, Loss: 0.4883282295514753\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.26462290138641015\n",
      "Valence RMSE: 0.2488476677476411\n",
      "Arousal RMSE: 0.2795092093883189\n",
      "Test R^2 score: tensor([0.2495, 0.2264], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1602, 0.1344], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.23797825592941807\n",
      "Num of epochs: 188\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Epoch 121, Loss: 0.5116998508836337\n",
      "Epoch 122, Loss: 0.5111129902664598\n",
      "Epoch 123, Loss: 0.5105405741436312\n",
      "Epoch 124, Loss: 0.5099849010083686\n",
      "Epoch 125, Loss: 0.5094623759868415\n",
      "Epoch 126, Loss: 0.5089652841754122\n",
      "Epoch 127, Loss: 0.5084717799132713\n",
      "Epoch 128, Loss: 0.5079809643002384\n",
      "Epoch 129, Loss: 0.507507731608986\n",
      "Epoch 130, Loss: 0.5070515727009828\n",
      "Epoch 131, Loss: 0.5065962678643066\n",
      "Epoch 132, Loss: 0.5061448812256618\n",
      "Epoch 133, Loss: 0.5057069703510908\n",
      "Epoch 134, Loss: 0.5052756693952567\n",
      "Epoch 135, Loss: 0.5048445017462813\n",
      "Epoch 136, Loss: 0.504425284231979\n",
      "Epoch 137, Loss: 0.504022717877823\n",
      "Epoch 138, Loss: 0.5036312802042249\n",
      "Epoch 139, Loss: 0.5032368731068207\n",
      "Epoch 140, Loss: 0.5028511060044271\n",
      "Epoch 141, Loss: 0.5024804933282331\n",
      "Epoch 142, Loss: 0.5021182727396313\n",
      "Epoch 143, Loss: 0.5017581664990297\n",
      "Epoch 144, Loss: 0.5014050827892457\n",
      "Epoch 145, Loss: 0.501047675902349\n",
      "Epoch 146, Loss: 0.500690966247362\n",
      "Epoch 147, Loss: 0.5003476722262875\n",
      "Epoch 148, Loss: 0.5000068544871652\n",
      "Epoch 149, Loss: 0.49966338867073357\n",
      "Epoch 150, Loss: 0.4993280873087794\n",
      "Epoch 151, Loss: 0.49900257940428383\n",
      "Epoch 152, Loss: 0.4986865554458311\n",
      "Epoch 153, Loss: 0.49836949389755036\n",
      "Epoch 154, Loss: 0.49805375636713634\n",
      "Epoch 155, Loss: 0.49773873165141763\n",
      "Epoch 156, Loss: 0.49742224924400874\n",
      "Epoch 157, Loss: 0.49711425826812494\n",
      "Epoch 158, Loss: 0.4968010973402242\n",
      "Epoch 159, Loss: 0.4964901399315246\n",
      "Epoch 160, Loss: 0.49617675026847124\n",
      "Epoch 161, Loss: 0.4958670390985117\n",
      "Epoch 162, Loss: 0.4955578109301908\n",
      "Epoch 163, Loss: 0.4952510223984696\n",
      "Epoch 164, Loss: 0.49494577484111674\n",
      "Epoch 165, Loss: 0.49464511374066544\n",
      "Epoch 166, Loss: 0.49434383269975535\n",
      "Epoch 167, Loss: 0.49404631909698943\n",
      "Epoch 168, Loss: 0.49374530645143583\n",
      "Epoch 169, Loss: 0.49345064806438055\n",
      "Epoch 170, Loss: 0.49317001490258333\n",
      "Epoch 171, Loss: 0.4928927591216884\n",
      "Epoch 172, Loss: 0.4926138499628376\n",
      "Epoch 173, Loss: 0.49233528219511824\n",
      "Epoch 174, Loss: 0.49205852514110515\n",
      "Epoch 175, Loss: 0.4917773399684819\n",
      "Epoch 176, Loss: 0.49149629710928233\n",
      "Epoch 177, Loss: 0.4912165647150265\n",
      "Epoch 178, Loss: 0.49094181765497674\n",
      "Epoch 179, Loss: 0.49066901222163783\n",
      "Epoch 180, Loss: 0.490393928005003\n",
      "Epoch 181, Loss: 0.4901079113527018\n",
      "Epoch 182, Loss: 0.48981984154671765\n",
      "Epoch 183, Loss: 0.4895234443341763\n",
      "Epoch 184, Loss: 0.48921459258368105\n",
      "Epoch 185, Loss: 0.4889145977883947\n",
      "Epoch 186, Loss: 0.4886175142174919\n",
      "Epoch 187, Loss: 0.4883282295514753\n",
      "Epoch 188, Loss: 0.4880435212302353\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.264825752509016\n",
      "Valence RMSE: 0.24907017554638325\n",
      "Arousal RMSE: 0.27969520202754117\n",
      "Test R^2 score: tensor([0.2482, 0.2254], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1587, 0.1332], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.23679199399140505\n",
      "Num of epochs: 189\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Epoch 121, Loss: 0.5116998508836337\n",
      "Epoch 122, Loss: 0.5111129902664598\n",
      "Epoch 123, Loss: 0.5105405741436312\n",
      "Epoch 124, Loss: 0.5099849010083686\n",
      "Epoch 125, Loss: 0.5094623759868415\n",
      "Epoch 126, Loss: 0.5089652841754122\n",
      "Epoch 127, Loss: 0.5084717799132713\n",
      "Epoch 128, Loss: 0.5079809643002384\n",
      "Epoch 129, Loss: 0.507507731608986\n",
      "Epoch 130, Loss: 0.5070515727009828\n",
      "Epoch 131, Loss: 0.5065962678643066\n",
      "Epoch 132, Loss: 0.5061448812256618\n",
      "Epoch 133, Loss: 0.5057069703510908\n",
      "Epoch 134, Loss: 0.5052756693952567\n",
      "Epoch 135, Loss: 0.5048445017462813\n",
      "Epoch 136, Loss: 0.504425284231979\n",
      "Epoch 137, Loss: 0.504022717877823\n",
      "Epoch 138, Loss: 0.5036312802042249\n",
      "Epoch 139, Loss: 0.5032368731068207\n",
      "Epoch 140, Loss: 0.5028511060044271\n",
      "Epoch 141, Loss: 0.5024804933282331\n",
      "Epoch 142, Loss: 0.5021182727396313\n",
      "Epoch 143, Loss: 0.5017581664990297\n",
      "Epoch 144, Loss: 0.5014050827892457\n",
      "Epoch 145, Loss: 0.501047675902349\n",
      "Epoch 146, Loss: 0.500690966247362\n",
      "Epoch 147, Loss: 0.5003476722262875\n",
      "Epoch 148, Loss: 0.5000068544871652\n",
      "Epoch 149, Loss: 0.49966338867073357\n",
      "Epoch 150, Loss: 0.4993280873087794\n",
      "Epoch 151, Loss: 0.49900257940428383\n",
      "Epoch 152, Loss: 0.4986865554458311\n",
      "Epoch 153, Loss: 0.49836949389755036\n",
      "Epoch 154, Loss: 0.49805375636713634\n",
      "Epoch 155, Loss: 0.49773873165141763\n",
      "Epoch 156, Loss: 0.49742224924400874\n",
      "Epoch 157, Loss: 0.49711425826812494\n",
      "Epoch 158, Loss: 0.4968010973402242\n",
      "Epoch 159, Loss: 0.4964901399315246\n",
      "Epoch 160, Loss: 0.49617675026847124\n",
      "Epoch 161, Loss: 0.4958670390985117\n",
      "Epoch 162, Loss: 0.4955578109301908\n",
      "Epoch 163, Loss: 0.4952510223984696\n",
      "Epoch 164, Loss: 0.49494577484111674\n",
      "Epoch 165, Loss: 0.49464511374066544\n",
      "Epoch 166, Loss: 0.49434383269975535\n",
      "Epoch 167, Loss: 0.49404631909698943\n",
      "Epoch 168, Loss: 0.49374530645143583\n",
      "Epoch 169, Loss: 0.49345064806438055\n",
      "Epoch 170, Loss: 0.49317001490258333\n",
      "Epoch 171, Loss: 0.4928927591216884\n",
      "Epoch 172, Loss: 0.4926138499628376\n",
      "Epoch 173, Loss: 0.49233528219511824\n",
      "Epoch 174, Loss: 0.49205852514110515\n",
      "Epoch 175, Loss: 0.4917773399684819\n",
      "Epoch 176, Loss: 0.49149629710928233\n",
      "Epoch 177, Loss: 0.4912165647150265\n",
      "Epoch 178, Loss: 0.49094181765497674\n",
      "Epoch 179, Loss: 0.49066901222163783\n",
      "Epoch 180, Loss: 0.490393928005003\n",
      "Epoch 181, Loss: 0.4901079113527018\n",
      "Epoch 182, Loss: 0.48981984154671765\n",
      "Epoch 183, Loss: 0.4895234443341763\n",
      "Epoch 184, Loss: 0.48921459258368105\n",
      "Epoch 185, Loss: 0.4889145977883947\n",
      "Epoch 186, Loss: 0.4886175142174919\n",
      "Epoch 187, Loss: 0.4883282295514753\n",
      "Epoch 188, Loss: 0.4880435212302353\n",
      "Epoch 189, Loss: 0.48775699700499114\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2650099174866942\n",
      "Valence RMSE: 0.24928688462331747\n",
      "Arousal RMSE: 0.279850963706416\n",
      "Test R^2 score: tensor([0.2469, 0.2245], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1573, 0.1323], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.23570608263130083\n",
      "Num of epochs: 190\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Epoch 121, Loss: 0.5116998508836337\n",
      "Epoch 122, Loss: 0.5111129902664598\n",
      "Epoch 123, Loss: 0.5105405741436312\n",
      "Epoch 124, Loss: 0.5099849010083686\n",
      "Epoch 125, Loss: 0.5094623759868415\n",
      "Epoch 126, Loss: 0.5089652841754122\n",
      "Epoch 127, Loss: 0.5084717799132713\n",
      "Epoch 128, Loss: 0.5079809643002384\n",
      "Epoch 129, Loss: 0.507507731608986\n",
      "Epoch 130, Loss: 0.5070515727009828\n",
      "Epoch 131, Loss: 0.5065962678643066\n",
      "Epoch 132, Loss: 0.5061448812256618\n",
      "Epoch 133, Loss: 0.5057069703510908\n",
      "Epoch 134, Loss: 0.5052756693952567\n",
      "Epoch 135, Loss: 0.5048445017462813\n",
      "Epoch 136, Loss: 0.504425284231979\n",
      "Epoch 137, Loss: 0.504022717877823\n",
      "Epoch 138, Loss: 0.5036312802042249\n",
      "Epoch 139, Loss: 0.5032368731068207\n",
      "Epoch 140, Loss: 0.5028511060044271\n",
      "Epoch 141, Loss: 0.5024804933282331\n",
      "Epoch 142, Loss: 0.5021182727396313\n",
      "Epoch 143, Loss: 0.5017581664990297\n",
      "Epoch 144, Loss: 0.5014050827892457\n",
      "Epoch 145, Loss: 0.501047675902349\n",
      "Epoch 146, Loss: 0.500690966247362\n",
      "Epoch 147, Loss: 0.5003476722262875\n",
      "Epoch 148, Loss: 0.5000068544871652\n",
      "Epoch 149, Loss: 0.49966338867073357\n",
      "Epoch 150, Loss: 0.4993280873087794\n",
      "Epoch 151, Loss: 0.49900257940428383\n",
      "Epoch 152, Loss: 0.4986865554458311\n",
      "Epoch 153, Loss: 0.49836949389755036\n",
      "Epoch 154, Loss: 0.49805375636713634\n",
      "Epoch 155, Loss: 0.49773873165141763\n",
      "Epoch 156, Loss: 0.49742224924400874\n",
      "Epoch 157, Loss: 0.49711425826812494\n",
      "Epoch 158, Loss: 0.4968010973402242\n",
      "Epoch 159, Loss: 0.4964901399315246\n",
      "Epoch 160, Loss: 0.49617675026847124\n",
      "Epoch 161, Loss: 0.4958670390985117\n",
      "Epoch 162, Loss: 0.4955578109301908\n",
      "Epoch 163, Loss: 0.4952510223984696\n",
      "Epoch 164, Loss: 0.49494577484111674\n",
      "Epoch 165, Loss: 0.49464511374066544\n",
      "Epoch 166, Loss: 0.49434383269975535\n",
      "Epoch 167, Loss: 0.49404631909698943\n",
      "Epoch 168, Loss: 0.49374530645143583\n",
      "Epoch 169, Loss: 0.49345064806438055\n",
      "Epoch 170, Loss: 0.49317001490258333\n",
      "Epoch 171, Loss: 0.4928927591216884\n",
      "Epoch 172, Loss: 0.4926138499628376\n",
      "Epoch 173, Loss: 0.49233528219511824\n",
      "Epoch 174, Loss: 0.49205852514110515\n",
      "Epoch 175, Loss: 0.4917773399684819\n",
      "Epoch 176, Loss: 0.49149629710928233\n",
      "Epoch 177, Loss: 0.4912165647150265\n",
      "Epoch 178, Loss: 0.49094181765497674\n",
      "Epoch 179, Loss: 0.49066901222163783\n",
      "Epoch 180, Loss: 0.490393928005003\n",
      "Epoch 181, Loss: 0.4901079113527018\n",
      "Epoch 182, Loss: 0.48981984154671765\n",
      "Epoch 183, Loss: 0.4895234443341763\n",
      "Epoch 184, Loss: 0.48921459258368105\n",
      "Epoch 185, Loss: 0.4889145977883947\n",
      "Epoch 186, Loss: 0.4886175142174919\n",
      "Epoch 187, Loss: 0.4883282295514753\n",
      "Epoch 188, Loss: 0.4880435212302353\n",
      "Epoch 189, Loss: 0.48775699700499114\n",
      "Epoch 190, Loss: 0.48748008613994775\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.26524553817087737\n",
      "Valence RMSE: 0.24949977379400354\n",
      "Arousal RMSE: 0.2801075756131165\n",
      "Test R^2 score: tensor([0.2456, 0.2231], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1558, 0.1307], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.2343512508419276\n",
      "Num of epochs: 191\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Epoch 121, Loss: 0.5116998508836337\n",
      "Epoch 122, Loss: 0.5111129902664598\n",
      "Epoch 123, Loss: 0.5105405741436312\n",
      "Epoch 124, Loss: 0.5099849010083686\n",
      "Epoch 125, Loss: 0.5094623759868415\n",
      "Epoch 126, Loss: 0.5089652841754122\n",
      "Epoch 127, Loss: 0.5084717799132713\n",
      "Epoch 128, Loss: 0.5079809643002384\n",
      "Epoch 129, Loss: 0.507507731608986\n",
      "Epoch 130, Loss: 0.5070515727009828\n",
      "Epoch 131, Loss: 0.5065962678643066\n",
      "Epoch 132, Loss: 0.5061448812256618\n",
      "Epoch 133, Loss: 0.5057069703510908\n",
      "Epoch 134, Loss: 0.5052756693952567\n",
      "Epoch 135, Loss: 0.5048445017462813\n",
      "Epoch 136, Loss: 0.504425284231979\n",
      "Epoch 137, Loss: 0.504022717877823\n",
      "Epoch 138, Loss: 0.5036312802042249\n",
      "Epoch 139, Loss: 0.5032368731068207\n",
      "Epoch 140, Loss: 0.5028511060044271\n",
      "Epoch 141, Loss: 0.5024804933282331\n",
      "Epoch 142, Loss: 0.5021182727396313\n",
      "Epoch 143, Loss: 0.5017581664990297\n",
      "Epoch 144, Loss: 0.5014050827892457\n",
      "Epoch 145, Loss: 0.501047675902349\n",
      "Epoch 146, Loss: 0.500690966247362\n",
      "Epoch 147, Loss: 0.5003476722262875\n",
      "Epoch 148, Loss: 0.5000068544871652\n",
      "Epoch 149, Loss: 0.49966338867073357\n",
      "Epoch 150, Loss: 0.4993280873087794\n",
      "Epoch 151, Loss: 0.49900257940428383\n",
      "Epoch 152, Loss: 0.4986865554458311\n",
      "Epoch 153, Loss: 0.49836949389755036\n",
      "Epoch 154, Loss: 0.49805375636713634\n",
      "Epoch 155, Loss: 0.49773873165141763\n",
      "Epoch 156, Loss: 0.49742224924400874\n",
      "Epoch 157, Loss: 0.49711425826812494\n",
      "Epoch 158, Loss: 0.4968010973402242\n",
      "Epoch 159, Loss: 0.4964901399315246\n",
      "Epoch 160, Loss: 0.49617675026847124\n",
      "Epoch 161, Loss: 0.4958670390985117\n",
      "Epoch 162, Loss: 0.4955578109301908\n",
      "Epoch 163, Loss: 0.4952510223984696\n",
      "Epoch 164, Loss: 0.49494577484111674\n",
      "Epoch 165, Loss: 0.49464511374066544\n",
      "Epoch 166, Loss: 0.49434383269975535\n",
      "Epoch 167, Loss: 0.49404631909698943\n",
      "Epoch 168, Loss: 0.49374530645143583\n",
      "Epoch 169, Loss: 0.49345064806438055\n",
      "Epoch 170, Loss: 0.49317001490258333\n",
      "Epoch 171, Loss: 0.4928927591216884\n",
      "Epoch 172, Loss: 0.4926138499628376\n",
      "Epoch 173, Loss: 0.49233528219511824\n",
      "Epoch 174, Loss: 0.49205852514110515\n",
      "Epoch 175, Loss: 0.4917773399684819\n",
      "Epoch 176, Loss: 0.49149629710928233\n",
      "Epoch 177, Loss: 0.4912165647150265\n",
      "Epoch 178, Loss: 0.49094181765497674\n",
      "Epoch 179, Loss: 0.49066901222163783\n",
      "Epoch 180, Loss: 0.490393928005003\n",
      "Epoch 181, Loss: 0.4901079113527018\n",
      "Epoch 182, Loss: 0.48981984154671765\n",
      "Epoch 183, Loss: 0.4895234443341763\n",
      "Epoch 184, Loss: 0.48921459258368105\n",
      "Epoch 185, Loss: 0.4889145977883947\n",
      "Epoch 186, Loss: 0.4886175142174919\n",
      "Epoch 187, Loss: 0.4883282295514753\n",
      "Epoch 188, Loss: 0.4880435212302353\n",
      "Epoch 189, Loss: 0.48775699700499114\n",
      "Epoch 190, Loss: 0.48748008613994775\n",
      "Epoch 191, Loss: 0.4872132179171775\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2655311208895671\n",
      "Valence RMSE: 0.24971927168098415\n",
      "Arousal RMSE: 0.280452915251132\n",
      "Test R^2 score: tensor([0.2443, 0.2212], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1544, 0.1285], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.2327288638270315\n",
      "Num of epochs: 192\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Epoch 121, Loss: 0.5116998508836337\n",
      "Epoch 122, Loss: 0.5111129902664598\n",
      "Epoch 123, Loss: 0.5105405741436312\n",
      "Epoch 124, Loss: 0.5099849010083686\n",
      "Epoch 125, Loss: 0.5094623759868415\n",
      "Epoch 126, Loss: 0.5089652841754122\n",
      "Epoch 127, Loss: 0.5084717799132713\n",
      "Epoch 128, Loss: 0.5079809643002384\n",
      "Epoch 129, Loss: 0.507507731608986\n",
      "Epoch 130, Loss: 0.5070515727009828\n",
      "Epoch 131, Loss: 0.5065962678643066\n",
      "Epoch 132, Loss: 0.5061448812256618\n",
      "Epoch 133, Loss: 0.5057069703510908\n",
      "Epoch 134, Loss: 0.5052756693952567\n",
      "Epoch 135, Loss: 0.5048445017462813\n",
      "Epoch 136, Loss: 0.504425284231979\n",
      "Epoch 137, Loss: 0.504022717877823\n",
      "Epoch 138, Loss: 0.5036312802042249\n",
      "Epoch 139, Loss: 0.5032368731068207\n",
      "Epoch 140, Loss: 0.5028511060044271\n",
      "Epoch 141, Loss: 0.5024804933282331\n",
      "Epoch 142, Loss: 0.5021182727396313\n",
      "Epoch 143, Loss: 0.5017581664990297\n",
      "Epoch 144, Loss: 0.5014050827892457\n",
      "Epoch 145, Loss: 0.501047675902349\n",
      "Epoch 146, Loss: 0.500690966247362\n",
      "Epoch 147, Loss: 0.5003476722262875\n",
      "Epoch 148, Loss: 0.5000068544871652\n",
      "Epoch 149, Loss: 0.49966338867073357\n",
      "Epoch 150, Loss: 0.4993280873087794\n",
      "Epoch 151, Loss: 0.49900257940428383\n",
      "Epoch 152, Loss: 0.4986865554458311\n",
      "Epoch 153, Loss: 0.49836949389755036\n",
      "Epoch 154, Loss: 0.49805375636713634\n",
      "Epoch 155, Loss: 0.49773873165141763\n",
      "Epoch 156, Loss: 0.49742224924400874\n",
      "Epoch 157, Loss: 0.49711425826812494\n",
      "Epoch 158, Loss: 0.4968010973402242\n",
      "Epoch 159, Loss: 0.4964901399315246\n",
      "Epoch 160, Loss: 0.49617675026847124\n",
      "Epoch 161, Loss: 0.4958670390985117\n",
      "Epoch 162, Loss: 0.4955578109301908\n",
      "Epoch 163, Loss: 0.4952510223984696\n",
      "Epoch 164, Loss: 0.49494577484111674\n",
      "Epoch 165, Loss: 0.49464511374066544\n",
      "Epoch 166, Loss: 0.49434383269975535\n",
      "Epoch 167, Loss: 0.49404631909698943\n",
      "Epoch 168, Loss: 0.49374530645143583\n",
      "Epoch 169, Loss: 0.49345064806438055\n",
      "Epoch 170, Loss: 0.49317001490258333\n",
      "Epoch 171, Loss: 0.4928927591216884\n",
      "Epoch 172, Loss: 0.4926138499628376\n",
      "Epoch 173, Loss: 0.49233528219511824\n",
      "Epoch 174, Loss: 0.49205852514110515\n",
      "Epoch 175, Loss: 0.4917773399684819\n",
      "Epoch 176, Loss: 0.49149629710928233\n",
      "Epoch 177, Loss: 0.4912165647150265\n",
      "Epoch 178, Loss: 0.49094181765497674\n",
      "Epoch 179, Loss: 0.49066901222163783\n",
      "Epoch 180, Loss: 0.490393928005003\n",
      "Epoch 181, Loss: 0.4901079113527018\n",
      "Epoch 182, Loss: 0.48981984154671765\n",
      "Epoch 183, Loss: 0.4895234443341763\n",
      "Epoch 184, Loss: 0.48921459258368105\n",
      "Epoch 185, Loss: 0.4889145977883947\n",
      "Epoch 186, Loss: 0.4886175142174919\n",
      "Epoch 187, Loss: 0.4883282295514753\n",
      "Epoch 188, Loss: 0.4880435212302353\n",
      "Epoch 189, Loss: 0.48775699700499114\n",
      "Epoch 190, Loss: 0.48748008613994775\n",
      "Epoch 191, Loss: 0.4872132179171775\n",
      "Epoch 192, Loss: 0.48694738158534456\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.26581270826345976\n",
      "Valence RMSE: 0.2499228614465662\n",
      "Arousal RMSE: 0.28080483449376514\n",
      "Test R^2 score: tensor([0.2430, 0.2192], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1530, 0.1264], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.23113460235575822\n",
      "Num of epochs: 193\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Epoch 121, Loss: 0.5116998508836337\n",
      "Epoch 122, Loss: 0.5111129902664598\n",
      "Epoch 123, Loss: 0.5105405741436312\n",
      "Epoch 124, Loss: 0.5099849010083686\n",
      "Epoch 125, Loss: 0.5094623759868415\n",
      "Epoch 126, Loss: 0.5089652841754122\n",
      "Epoch 127, Loss: 0.5084717799132713\n",
      "Epoch 128, Loss: 0.5079809643002384\n",
      "Epoch 129, Loss: 0.507507731608986\n",
      "Epoch 130, Loss: 0.5070515727009828\n",
      "Epoch 131, Loss: 0.5065962678643066\n",
      "Epoch 132, Loss: 0.5061448812256618\n",
      "Epoch 133, Loss: 0.5057069703510908\n",
      "Epoch 134, Loss: 0.5052756693952567\n",
      "Epoch 135, Loss: 0.5048445017462813\n",
      "Epoch 136, Loss: 0.504425284231979\n",
      "Epoch 137, Loss: 0.504022717877823\n",
      "Epoch 138, Loss: 0.5036312802042249\n",
      "Epoch 139, Loss: 0.5032368731068207\n",
      "Epoch 140, Loss: 0.5028511060044271\n",
      "Epoch 141, Loss: 0.5024804933282331\n",
      "Epoch 142, Loss: 0.5021182727396313\n",
      "Epoch 143, Loss: 0.5017581664990297\n",
      "Epoch 144, Loss: 0.5014050827892457\n",
      "Epoch 145, Loss: 0.501047675902349\n",
      "Epoch 146, Loss: 0.500690966247362\n",
      "Epoch 147, Loss: 0.5003476722262875\n",
      "Epoch 148, Loss: 0.5000068544871652\n",
      "Epoch 149, Loss: 0.49966338867073357\n",
      "Epoch 150, Loss: 0.4993280873087794\n",
      "Epoch 151, Loss: 0.49900257940428383\n",
      "Epoch 152, Loss: 0.4986865554458311\n",
      "Epoch 153, Loss: 0.49836949389755036\n",
      "Epoch 154, Loss: 0.49805375636713634\n",
      "Epoch 155, Loss: 0.49773873165141763\n",
      "Epoch 156, Loss: 0.49742224924400874\n",
      "Epoch 157, Loss: 0.49711425826812494\n",
      "Epoch 158, Loss: 0.4968010973402242\n",
      "Epoch 159, Loss: 0.4964901399315246\n",
      "Epoch 160, Loss: 0.49617675026847124\n",
      "Epoch 161, Loss: 0.4958670390985117\n",
      "Epoch 162, Loss: 0.4955578109301908\n",
      "Epoch 163, Loss: 0.4952510223984696\n",
      "Epoch 164, Loss: 0.49494577484111674\n",
      "Epoch 165, Loss: 0.49464511374066544\n",
      "Epoch 166, Loss: 0.49434383269975535\n",
      "Epoch 167, Loss: 0.49404631909698943\n",
      "Epoch 168, Loss: 0.49374530645143583\n",
      "Epoch 169, Loss: 0.49345064806438055\n",
      "Epoch 170, Loss: 0.49317001490258333\n",
      "Epoch 171, Loss: 0.4928927591216884\n",
      "Epoch 172, Loss: 0.4926138499628376\n",
      "Epoch 173, Loss: 0.49233528219511824\n",
      "Epoch 174, Loss: 0.49205852514110515\n",
      "Epoch 175, Loss: 0.4917773399684819\n",
      "Epoch 176, Loss: 0.49149629710928233\n",
      "Epoch 177, Loss: 0.4912165647150265\n",
      "Epoch 178, Loss: 0.49094181765497674\n",
      "Epoch 179, Loss: 0.49066901222163783\n",
      "Epoch 180, Loss: 0.490393928005003\n",
      "Epoch 181, Loss: 0.4901079113527018\n",
      "Epoch 182, Loss: 0.48981984154671765\n",
      "Epoch 183, Loss: 0.4895234443341763\n",
      "Epoch 184, Loss: 0.48921459258368105\n",
      "Epoch 185, Loss: 0.4889145977883947\n",
      "Epoch 186, Loss: 0.4886175142174919\n",
      "Epoch 187, Loss: 0.4883282295514753\n",
      "Epoch 188, Loss: 0.4880435212302353\n",
      "Epoch 189, Loss: 0.48775699700499114\n",
      "Epoch 190, Loss: 0.48748008613994775\n",
      "Epoch 191, Loss: 0.4872132179171775\n",
      "Epoch 192, Loss: 0.48694738158534456\n",
      "Epoch 193, Loss: 0.48668121634032785\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.26606772401921475\n",
      "Valence RMSE: 0.25010175462946\n",
      "Arousal RMSE: 0.281128404578398\n",
      "Test R^2 score: tensor([0.2419, 0.2174], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1518, 0.1243], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.22969238879844972\n",
      "Num of epochs: 194\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Epoch 121, Loss: 0.5116998508836337\n",
      "Epoch 122, Loss: 0.5111129902664598\n",
      "Epoch 123, Loss: 0.5105405741436312\n",
      "Epoch 124, Loss: 0.5099849010083686\n",
      "Epoch 125, Loss: 0.5094623759868415\n",
      "Epoch 126, Loss: 0.5089652841754122\n",
      "Epoch 127, Loss: 0.5084717799132713\n",
      "Epoch 128, Loss: 0.5079809643002384\n",
      "Epoch 129, Loss: 0.507507731608986\n",
      "Epoch 130, Loss: 0.5070515727009828\n",
      "Epoch 131, Loss: 0.5065962678643066\n",
      "Epoch 132, Loss: 0.5061448812256618\n",
      "Epoch 133, Loss: 0.5057069703510908\n",
      "Epoch 134, Loss: 0.5052756693952567\n",
      "Epoch 135, Loss: 0.5048445017462813\n",
      "Epoch 136, Loss: 0.504425284231979\n",
      "Epoch 137, Loss: 0.504022717877823\n",
      "Epoch 138, Loss: 0.5036312802042249\n",
      "Epoch 139, Loss: 0.5032368731068207\n",
      "Epoch 140, Loss: 0.5028511060044271\n",
      "Epoch 141, Loss: 0.5024804933282331\n",
      "Epoch 142, Loss: 0.5021182727396313\n",
      "Epoch 143, Loss: 0.5017581664990297\n",
      "Epoch 144, Loss: 0.5014050827892457\n",
      "Epoch 145, Loss: 0.501047675902349\n",
      "Epoch 146, Loss: 0.500690966247362\n",
      "Epoch 147, Loss: 0.5003476722262875\n",
      "Epoch 148, Loss: 0.5000068544871652\n",
      "Epoch 149, Loss: 0.49966338867073357\n",
      "Epoch 150, Loss: 0.4993280873087794\n",
      "Epoch 151, Loss: 0.49900257940428383\n",
      "Epoch 152, Loss: 0.4986865554458311\n",
      "Epoch 153, Loss: 0.49836949389755036\n",
      "Epoch 154, Loss: 0.49805375636713634\n",
      "Epoch 155, Loss: 0.49773873165141763\n",
      "Epoch 156, Loss: 0.49742224924400874\n",
      "Epoch 157, Loss: 0.49711425826812494\n",
      "Epoch 158, Loss: 0.4968010973402242\n",
      "Epoch 159, Loss: 0.4964901399315246\n",
      "Epoch 160, Loss: 0.49617675026847124\n",
      "Epoch 161, Loss: 0.4958670390985117\n",
      "Epoch 162, Loss: 0.4955578109301908\n",
      "Epoch 163, Loss: 0.4952510223984696\n",
      "Epoch 164, Loss: 0.49494577484111674\n",
      "Epoch 165, Loss: 0.49464511374066544\n",
      "Epoch 166, Loss: 0.49434383269975535\n",
      "Epoch 167, Loss: 0.49404631909698943\n",
      "Epoch 168, Loss: 0.49374530645143583\n",
      "Epoch 169, Loss: 0.49345064806438055\n",
      "Epoch 170, Loss: 0.49317001490258333\n",
      "Epoch 171, Loss: 0.4928927591216884\n",
      "Epoch 172, Loss: 0.4926138499628376\n",
      "Epoch 173, Loss: 0.49233528219511824\n",
      "Epoch 174, Loss: 0.49205852514110515\n",
      "Epoch 175, Loss: 0.4917773399684819\n",
      "Epoch 176, Loss: 0.49149629710928233\n",
      "Epoch 177, Loss: 0.4912165647150265\n",
      "Epoch 178, Loss: 0.49094181765497674\n",
      "Epoch 179, Loss: 0.49066901222163783\n",
      "Epoch 180, Loss: 0.490393928005003\n",
      "Epoch 181, Loss: 0.4901079113527018\n",
      "Epoch 182, Loss: 0.48981984154671765\n",
      "Epoch 183, Loss: 0.4895234443341763\n",
      "Epoch 184, Loss: 0.48921459258368105\n",
      "Epoch 185, Loss: 0.4889145977883947\n",
      "Epoch 186, Loss: 0.4886175142174919\n",
      "Epoch 187, Loss: 0.4883282295514753\n",
      "Epoch 188, Loss: 0.4880435212302353\n",
      "Epoch 189, Loss: 0.48775699700499114\n",
      "Epoch 190, Loss: 0.48748008613994775\n",
      "Epoch 191, Loss: 0.4872132179171775\n",
      "Epoch 192, Loss: 0.48694738158534456\n",
      "Epoch 193, Loss: 0.48668121634032785\n",
      "Epoch 194, Loss: 0.4864280322301768\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2662827559238411\n",
      "Valence RMSE: 0.25024327161754356\n",
      "Arousal RMSE: 0.2814095186999547\n",
      "Test R^2 score: tensor([0.2411, 0.2159], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1508, 0.1226], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.2284804192313718\n",
      "Num of epochs: 195\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Epoch 121, Loss: 0.5116998508836337\n",
      "Epoch 122, Loss: 0.5111129902664598\n",
      "Epoch 123, Loss: 0.5105405741436312\n",
      "Epoch 124, Loss: 0.5099849010083686\n",
      "Epoch 125, Loss: 0.5094623759868415\n",
      "Epoch 126, Loss: 0.5089652841754122\n",
      "Epoch 127, Loss: 0.5084717799132713\n",
      "Epoch 128, Loss: 0.5079809643002384\n",
      "Epoch 129, Loss: 0.507507731608986\n",
      "Epoch 130, Loss: 0.5070515727009828\n",
      "Epoch 131, Loss: 0.5065962678643066\n",
      "Epoch 132, Loss: 0.5061448812256618\n",
      "Epoch 133, Loss: 0.5057069703510908\n",
      "Epoch 134, Loss: 0.5052756693952567\n",
      "Epoch 135, Loss: 0.5048445017462813\n",
      "Epoch 136, Loss: 0.504425284231979\n",
      "Epoch 137, Loss: 0.504022717877823\n",
      "Epoch 138, Loss: 0.5036312802042249\n",
      "Epoch 139, Loss: 0.5032368731068207\n",
      "Epoch 140, Loss: 0.5028511060044271\n",
      "Epoch 141, Loss: 0.5024804933282331\n",
      "Epoch 142, Loss: 0.5021182727396313\n",
      "Epoch 143, Loss: 0.5017581664990297\n",
      "Epoch 144, Loss: 0.5014050827892457\n",
      "Epoch 145, Loss: 0.501047675902349\n",
      "Epoch 146, Loss: 0.500690966247362\n",
      "Epoch 147, Loss: 0.5003476722262875\n",
      "Epoch 148, Loss: 0.5000068544871652\n",
      "Epoch 149, Loss: 0.49966338867073357\n",
      "Epoch 150, Loss: 0.4993280873087794\n",
      "Epoch 151, Loss: 0.49900257940428383\n",
      "Epoch 152, Loss: 0.4986865554458311\n",
      "Epoch 153, Loss: 0.49836949389755036\n",
      "Epoch 154, Loss: 0.49805375636713634\n",
      "Epoch 155, Loss: 0.49773873165141763\n",
      "Epoch 156, Loss: 0.49742224924400874\n",
      "Epoch 157, Loss: 0.49711425826812494\n",
      "Epoch 158, Loss: 0.4968010973402242\n",
      "Epoch 159, Loss: 0.4964901399315246\n",
      "Epoch 160, Loss: 0.49617675026847124\n",
      "Epoch 161, Loss: 0.4958670390985117\n",
      "Epoch 162, Loss: 0.4955578109301908\n",
      "Epoch 163, Loss: 0.4952510223984696\n",
      "Epoch 164, Loss: 0.49494577484111674\n",
      "Epoch 165, Loss: 0.49464511374066544\n",
      "Epoch 166, Loss: 0.49434383269975535\n",
      "Epoch 167, Loss: 0.49404631909698943\n",
      "Epoch 168, Loss: 0.49374530645143583\n",
      "Epoch 169, Loss: 0.49345064806438055\n",
      "Epoch 170, Loss: 0.49317001490258333\n",
      "Epoch 171, Loss: 0.4928927591216884\n",
      "Epoch 172, Loss: 0.4926138499628376\n",
      "Epoch 173, Loss: 0.49233528219511824\n",
      "Epoch 174, Loss: 0.49205852514110515\n",
      "Epoch 175, Loss: 0.4917773399684819\n",
      "Epoch 176, Loss: 0.49149629710928233\n",
      "Epoch 177, Loss: 0.4912165647150265\n",
      "Epoch 178, Loss: 0.49094181765497674\n",
      "Epoch 179, Loss: 0.49066901222163783\n",
      "Epoch 180, Loss: 0.490393928005003\n",
      "Epoch 181, Loss: 0.4901079113527018\n",
      "Epoch 182, Loss: 0.48981984154671765\n",
      "Epoch 183, Loss: 0.4895234443341763\n",
      "Epoch 184, Loss: 0.48921459258368105\n",
      "Epoch 185, Loss: 0.4889145977883947\n",
      "Epoch 186, Loss: 0.4886175142174919\n",
      "Epoch 187, Loss: 0.4883282295514753\n",
      "Epoch 188, Loss: 0.4880435212302353\n",
      "Epoch 189, Loss: 0.48775699700499114\n",
      "Epoch 190, Loss: 0.48748008613994775\n",
      "Epoch 191, Loss: 0.4872132179171775\n",
      "Epoch 192, Loss: 0.48694738158534456\n",
      "Epoch 193, Loss: 0.48668121634032785\n",
      "Epoch 194, Loss: 0.4864280322301768\n",
      "Epoch 195, Loss: 0.486178532155915\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.266438128579756\n",
      "Valence RMSE: 0.25034452824902176\n",
      "Arousal RMSE: 0.2816135115684969\n",
      "Test R^2 score: tensor([0.2405, 0.2147], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1501, 0.1213], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.22760465913746364\n",
      "Num of epochs: 196\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Epoch 121, Loss: 0.5116998508836337\n",
      "Epoch 122, Loss: 0.5111129902664598\n",
      "Epoch 123, Loss: 0.5105405741436312\n",
      "Epoch 124, Loss: 0.5099849010083686\n",
      "Epoch 125, Loss: 0.5094623759868415\n",
      "Epoch 126, Loss: 0.5089652841754122\n",
      "Epoch 127, Loss: 0.5084717799132713\n",
      "Epoch 128, Loss: 0.5079809643002384\n",
      "Epoch 129, Loss: 0.507507731608986\n",
      "Epoch 130, Loss: 0.5070515727009828\n",
      "Epoch 131, Loss: 0.5065962678643066\n",
      "Epoch 132, Loss: 0.5061448812256618\n",
      "Epoch 133, Loss: 0.5057069703510908\n",
      "Epoch 134, Loss: 0.5052756693952567\n",
      "Epoch 135, Loss: 0.5048445017462813\n",
      "Epoch 136, Loss: 0.504425284231979\n",
      "Epoch 137, Loss: 0.504022717877823\n",
      "Epoch 138, Loss: 0.5036312802042249\n",
      "Epoch 139, Loss: 0.5032368731068207\n",
      "Epoch 140, Loss: 0.5028511060044271\n",
      "Epoch 141, Loss: 0.5024804933282331\n",
      "Epoch 142, Loss: 0.5021182727396313\n",
      "Epoch 143, Loss: 0.5017581664990297\n",
      "Epoch 144, Loss: 0.5014050827892457\n",
      "Epoch 145, Loss: 0.501047675902349\n",
      "Epoch 146, Loss: 0.500690966247362\n",
      "Epoch 147, Loss: 0.5003476722262875\n",
      "Epoch 148, Loss: 0.5000068544871652\n",
      "Epoch 149, Loss: 0.49966338867073357\n",
      "Epoch 150, Loss: 0.4993280873087794\n",
      "Epoch 151, Loss: 0.49900257940428383\n",
      "Epoch 152, Loss: 0.4986865554458311\n",
      "Epoch 153, Loss: 0.49836949389755036\n",
      "Epoch 154, Loss: 0.49805375636713634\n",
      "Epoch 155, Loss: 0.49773873165141763\n",
      "Epoch 156, Loss: 0.49742224924400874\n",
      "Epoch 157, Loss: 0.49711425826812494\n",
      "Epoch 158, Loss: 0.4968010973402242\n",
      "Epoch 159, Loss: 0.4964901399315246\n",
      "Epoch 160, Loss: 0.49617675026847124\n",
      "Epoch 161, Loss: 0.4958670390985117\n",
      "Epoch 162, Loss: 0.4955578109301908\n",
      "Epoch 163, Loss: 0.4952510223984696\n",
      "Epoch 164, Loss: 0.49494577484111674\n",
      "Epoch 165, Loss: 0.49464511374066544\n",
      "Epoch 166, Loss: 0.49434383269975535\n",
      "Epoch 167, Loss: 0.49404631909698943\n",
      "Epoch 168, Loss: 0.49374530645143583\n",
      "Epoch 169, Loss: 0.49345064806438055\n",
      "Epoch 170, Loss: 0.49317001490258333\n",
      "Epoch 171, Loss: 0.4928927591216884\n",
      "Epoch 172, Loss: 0.4926138499628376\n",
      "Epoch 173, Loss: 0.49233528219511824\n",
      "Epoch 174, Loss: 0.49205852514110515\n",
      "Epoch 175, Loss: 0.4917773399684819\n",
      "Epoch 176, Loss: 0.49149629710928233\n",
      "Epoch 177, Loss: 0.4912165647150265\n",
      "Epoch 178, Loss: 0.49094181765497674\n",
      "Epoch 179, Loss: 0.49066901222163783\n",
      "Epoch 180, Loss: 0.490393928005003\n",
      "Epoch 181, Loss: 0.4901079113527018\n",
      "Epoch 182, Loss: 0.48981984154671765\n",
      "Epoch 183, Loss: 0.4895234443341763\n",
      "Epoch 184, Loss: 0.48921459258368105\n",
      "Epoch 185, Loss: 0.4889145977883947\n",
      "Epoch 186, Loss: 0.4886175142174919\n",
      "Epoch 187, Loss: 0.4883282295514753\n",
      "Epoch 188, Loss: 0.4880435212302353\n",
      "Epoch 189, Loss: 0.48775699700499114\n",
      "Epoch 190, Loss: 0.48748008613994775\n",
      "Epoch 191, Loss: 0.4872132179171775\n",
      "Epoch 192, Loss: 0.48694738158534456\n",
      "Epoch 193, Loss: 0.48668121634032785\n",
      "Epoch 194, Loss: 0.4864280322301768\n",
      "Epoch 195, Loss: 0.486178532155915\n",
      "Epoch 196, Loss: 0.4859301919172944\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2665534847578992\n",
      "Valence RMSE: 0.25042362871625795\n",
      "Arousal RMSE: 0.28176147120169037\n",
      "Test R^2 score: tensor([0.2400, 0.2139], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1496, 0.1204], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.2269519504261871\n",
      "Num of epochs: 197\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Epoch 121, Loss: 0.5116998508836337\n",
      "Epoch 122, Loss: 0.5111129902664598\n",
      "Epoch 123, Loss: 0.5105405741436312\n",
      "Epoch 124, Loss: 0.5099849010083686\n",
      "Epoch 125, Loss: 0.5094623759868415\n",
      "Epoch 126, Loss: 0.5089652841754122\n",
      "Epoch 127, Loss: 0.5084717799132713\n",
      "Epoch 128, Loss: 0.5079809643002384\n",
      "Epoch 129, Loss: 0.507507731608986\n",
      "Epoch 130, Loss: 0.5070515727009828\n",
      "Epoch 131, Loss: 0.5065962678643066\n",
      "Epoch 132, Loss: 0.5061448812256618\n",
      "Epoch 133, Loss: 0.5057069703510908\n",
      "Epoch 134, Loss: 0.5052756693952567\n",
      "Epoch 135, Loss: 0.5048445017462813\n",
      "Epoch 136, Loss: 0.504425284231979\n",
      "Epoch 137, Loss: 0.504022717877823\n",
      "Epoch 138, Loss: 0.5036312802042249\n",
      "Epoch 139, Loss: 0.5032368731068207\n",
      "Epoch 140, Loss: 0.5028511060044271\n",
      "Epoch 141, Loss: 0.5024804933282331\n",
      "Epoch 142, Loss: 0.5021182727396313\n",
      "Epoch 143, Loss: 0.5017581664990297\n",
      "Epoch 144, Loss: 0.5014050827892457\n",
      "Epoch 145, Loss: 0.501047675902349\n",
      "Epoch 146, Loss: 0.500690966247362\n",
      "Epoch 147, Loss: 0.5003476722262875\n",
      "Epoch 148, Loss: 0.5000068544871652\n",
      "Epoch 149, Loss: 0.49966338867073357\n",
      "Epoch 150, Loss: 0.4993280873087794\n",
      "Epoch 151, Loss: 0.49900257940428383\n",
      "Epoch 152, Loss: 0.4986865554458311\n",
      "Epoch 153, Loss: 0.49836949389755036\n",
      "Epoch 154, Loss: 0.49805375636713634\n",
      "Epoch 155, Loss: 0.49773873165141763\n",
      "Epoch 156, Loss: 0.49742224924400874\n",
      "Epoch 157, Loss: 0.49711425826812494\n",
      "Epoch 158, Loss: 0.4968010973402242\n",
      "Epoch 159, Loss: 0.4964901399315246\n",
      "Epoch 160, Loss: 0.49617675026847124\n",
      "Epoch 161, Loss: 0.4958670390985117\n",
      "Epoch 162, Loss: 0.4955578109301908\n",
      "Epoch 163, Loss: 0.4952510223984696\n",
      "Epoch 164, Loss: 0.49494577484111674\n",
      "Epoch 165, Loss: 0.49464511374066544\n",
      "Epoch 166, Loss: 0.49434383269975535\n",
      "Epoch 167, Loss: 0.49404631909698943\n",
      "Epoch 168, Loss: 0.49374530645143583\n",
      "Epoch 169, Loss: 0.49345064806438055\n",
      "Epoch 170, Loss: 0.49317001490258333\n",
      "Epoch 171, Loss: 0.4928927591216884\n",
      "Epoch 172, Loss: 0.4926138499628376\n",
      "Epoch 173, Loss: 0.49233528219511824\n",
      "Epoch 174, Loss: 0.49205852514110515\n",
      "Epoch 175, Loss: 0.4917773399684819\n",
      "Epoch 176, Loss: 0.49149629710928233\n",
      "Epoch 177, Loss: 0.4912165647150265\n",
      "Epoch 178, Loss: 0.49094181765497674\n",
      "Epoch 179, Loss: 0.49066901222163783\n",
      "Epoch 180, Loss: 0.490393928005003\n",
      "Epoch 181, Loss: 0.4901079113527018\n",
      "Epoch 182, Loss: 0.48981984154671765\n",
      "Epoch 183, Loss: 0.4895234443341763\n",
      "Epoch 184, Loss: 0.48921459258368105\n",
      "Epoch 185, Loss: 0.4889145977883947\n",
      "Epoch 186, Loss: 0.4886175142174919\n",
      "Epoch 187, Loss: 0.4883282295514753\n",
      "Epoch 188, Loss: 0.4880435212302353\n",
      "Epoch 189, Loss: 0.48775699700499114\n",
      "Epoch 190, Loss: 0.48748008613994775\n",
      "Epoch 191, Loss: 0.4872132179171775\n",
      "Epoch 192, Loss: 0.48694738158534456\n",
      "Epoch 193, Loss: 0.48668121634032785\n",
      "Epoch 194, Loss: 0.4864280322301768\n",
      "Epoch 195, Loss: 0.486178532155915\n",
      "Epoch 196, Loss: 0.4859301919172944\n",
      "Epoch 197, Loss: 0.48568442460989747\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2666663373364193\n",
      "Valence RMSE: 0.2505075201202882\n",
      "Arousal RMSE: 0.28190043153572036\n",
      "Test R^2 score: tensor([0.2395, 0.2131], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1490, 0.1195], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.22630952449118458\n",
      "Num of epochs: 198\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Epoch 121, Loss: 0.5116998508836337\n",
      "Epoch 122, Loss: 0.5111129902664598\n",
      "Epoch 123, Loss: 0.5105405741436312\n",
      "Epoch 124, Loss: 0.5099849010083686\n",
      "Epoch 125, Loss: 0.5094623759868415\n",
      "Epoch 126, Loss: 0.5089652841754122\n",
      "Epoch 127, Loss: 0.5084717799132713\n",
      "Epoch 128, Loss: 0.5079809643002384\n",
      "Epoch 129, Loss: 0.507507731608986\n",
      "Epoch 130, Loss: 0.5070515727009828\n",
      "Epoch 131, Loss: 0.5065962678643066\n",
      "Epoch 132, Loss: 0.5061448812256618\n",
      "Epoch 133, Loss: 0.5057069703510908\n",
      "Epoch 134, Loss: 0.5052756693952567\n",
      "Epoch 135, Loss: 0.5048445017462813\n",
      "Epoch 136, Loss: 0.504425284231979\n",
      "Epoch 137, Loss: 0.504022717877823\n",
      "Epoch 138, Loss: 0.5036312802042249\n",
      "Epoch 139, Loss: 0.5032368731068207\n",
      "Epoch 140, Loss: 0.5028511060044271\n",
      "Epoch 141, Loss: 0.5024804933282331\n",
      "Epoch 142, Loss: 0.5021182727396313\n",
      "Epoch 143, Loss: 0.5017581664990297\n",
      "Epoch 144, Loss: 0.5014050827892457\n",
      "Epoch 145, Loss: 0.501047675902349\n",
      "Epoch 146, Loss: 0.500690966247362\n",
      "Epoch 147, Loss: 0.5003476722262875\n",
      "Epoch 148, Loss: 0.5000068544871652\n",
      "Epoch 149, Loss: 0.49966338867073357\n",
      "Epoch 150, Loss: 0.4993280873087794\n",
      "Epoch 151, Loss: 0.49900257940428383\n",
      "Epoch 152, Loss: 0.4986865554458311\n",
      "Epoch 153, Loss: 0.49836949389755036\n",
      "Epoch 154, Loss: 0.49805375636713634\n",
      "Epoch 155, Loss: 0.49773873165141763\n",
      "Epoch 156, Loss: 0.49742224924400874\n",
      "Epoch 157, Loss: 0.49711425826812494\n",
      "Epoch 158, Loss: 0.4968010973402242\n",
      "Epoch 159, Loss: 0.4964901399315246\n",
      "Epoch 160, Loss: 0.49617675026847124\n",
      "Epoch 161, Loss: 0.4958670390985117\n",
      "Epoch 162, Loss: 0.4955578109301908\n",
      "Epoch 163, Loss: 0.4952510223984696\n",
      "Epoch 164, Loss: 0.49494577484111674\n",
      "Epoch 165, Loss: 0.49464511374066544\n",
      "Epoch 166, Loss: 0.49434383269975535\n",
      "Epoch 167, Loss: 0.49404631909698943\n",
      "Epoch 168, Loss: 0.49374530645143583\n",
      "Epoch 169, Loss: 0.49345064806438055\n",
      "Epoch 170, Loss: 0.49317001490258333\n",
      "Epoch 171, Loss: 0.4928927591216884\n",
      "Epoch 172, Loss: 0.4926138499628376\n",
      "Epoch 173, Loss: 0.49233528219511824\n",
      "Epoch 174, Loss: 0.49205852514110515\n",
      "Epoch 175, Loss: 0.4917773399684819\n",
      "Epoch 176, Loss: 0.49149629710928233\n",
      "Epoch 177, Loss: 0.4912165647150265\n",
      "Epoch 178, Loss: 0.49094181765497674\n",
      "Epoch 179, Loss: 0.49066901222163783\n",
      "Epoch 180, Loss: 0.490393928005003\n",
      "Epoch 181, Loss: 0.4901079113527018\n",
      "Epoch 182, Loss: 0.48981984154671765\n",
      "Epoch 183, Loss: 0.4895234443341763\n",
      "Epoch 184, Loss: 0.48921459258368105\n",
      "Epoch 185, Loss: 0.4889145977883947\n",
      "Epoch 186, Loss: 0.4886175142174919\n",
      "Epoch 187, Loss: 0.4883282295514753\n",
      "Epoch 188, Loss: 0.4880435212302353\n",
      "Epoch 189, Loss: 0.48775699700499114\n",
      "Epoch 190, Loss: 0.48748008613994775\n",
      "Epoch 191, Loss: 0.4872132179171775\n",
      "Epoch 192, Loss: 0.48694738158534456\n",
      "Epoch 193, Loss: 0.48668121634032785\n",
      "Epoch 194, Loss: 0.4864280322301768\n",
      "Epoch 195, Loss: 0.486178532155915\n",
      "Epoch 196, Loss: 0.4859301919172944\n",
      "Epoch 197, Loss: 0.48568442460989747\n",
      "Epoch 198, Loss: 0.48544224711238787\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.26678386627997613\n",
      "Valence RMSE: 0.2505850706621542\n",
      "Arousal RMSE: 0.28205386892547246\n",
      "Test R^2 score: tensor([0.2390, 0.2123], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1485, 0.1186], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.22564564790094616\n",
      "Num of epochs: 199\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Epoch 121, Loss: 0.5116998508836337\n",
      "Epoch 122, Loss: 0.5111129902664598\n",
      "Epoch 123, Loss: 0.5105405741436312\n",
      "Epoch 124, Loss: 0.5099849010083686\n",
      "Epoch 125, Loss: 0.5094623759868415\n",
      "Epoch 126, Loss: 0.5089652841754122\n",
      "Epoch 127, Loss: 0.5084717799132713\n",
      "Epoch 128, Loss: 0.5079809643002384\n",
      "Epoch 129, Loss: 0.507507731608986\n",
      "Epoch 130, Loss: 0.5070515727009828\n",
      "Epoch 131, Loss: 0.5065962678643066\n",
      "Epoch 132, Loss: 0.5061448812256618\n",
      "Epoch 133, Loss: 0.5057069703510908\n",
      "Epoch 134, Loss: 0.5052756693952567\n",
      "Epoch 135, Loss: 0.5048445017462813\n",
      "Epoch 136, Loss: 0.504425284231979\n",
      "Epoch 137, Loss: 0.504022717877823\n",
      "Epoch 138, Loss: 0.5036312802042249\n",
      "Epoch 139, Loss: 0.5032368731068207\n",
      "Epoch 140, Loss: 0.5028511060044271\n",
      "Epoch 141, Loss: 0.5024804933282331\n",
      "Epoch 142, Loss: 0.5021182727396313\n",
      "Epoch 143, Loss: 0.5017581664990297\n",
      "Epoch 144, Loss: 0.5014050827892457\n",
      "Epoch 145, Loss: 0.501047675902349\n",
      "Epoch 146, Loss: 0.500690966247362\n",
      "Epoch 147, Loss: 0.5003476722262875\n",
      "Epoch 148, Loss: 0.5000068544871652\n",
      "Epoch 149, Loss: 0.49966338867073357\n",
      "Epoch 150, Loss: 0.4993280873087794\n",
      "Epoch 151, Loss: 0.49900257940428383\n",
      "Epoch 152, Loss: 0.4986865554458311\n",
      "Epoch 153, Loss: 0.49836949389755036\n",
      "Epoch 154, Loss: 0.49805375636713634\n",
      "Epoch 155, Loss: 0.49773873165141763\n",
      "Epoch 156, Loss: 0.49742224924400874\n",
      "Epoch 157, Loss: 0.49711425826812494\n",
      "Epoch 158, Loss: 0.4968010973402242\n",
      "Epoch 159, Loss: 0.4964901399315246\n",
      "Epoch 160, Loss: 0.49617675026847124\n",
      "Epoch 161, Loss: 0.4958670390985117\n",
      "Epoch 162, Loss: 0.4955578109301908\n",
      "Epoch 163, Loss: 0.4952510223984696\n",
      "Epoch 164, Loss: 0.49494577484111674\n",
      "Epoch 165, Loss: 0.49464511374066544\n",
      "Epoch 166, Loss: 0.49434383269975535\n",
      "Epoch 167, Loss: 0.49404631909698943\n",
      "Epoch 168, Loss: 0.49374530645143583\n",
      "Epoch 169, Loss: 0.49345064806438055\n",
      "Epoch 170, Loss: 0.49317001490258333\n",
      "Epoch 171, Loss: 0.4928927591216884\n",
      "Epoch 172, Loss: 0.4926138499628376\n",
      "Epoch 173, Loss: 0.49233528219511824\n",
      "Epoch 174, Loss: 0.49205852514110515\n",
      "Epoch 175, Loss: 0.4917773399684819\n",
      "Epoch 176, Loss: 0.49149629710928233\n",
      "Epoch 177, Loss: 0.4912165647150265\n",
      "Epoch 178, Loss: 0.49094181765497674\n",
      "Epoch 179, Loss: 0.49066901222163783\n",
      "Epoch 180, Loss: 0.490393928005003\n",
      "Epoch 181, Loss: 0.4901079113527018\n",
      "Epoch 182, Loss: 0.48981984154671765\n",
      "Epoch 183, Loss: 0.4895234443341763\n",
      "Epoch 184, Loss: 0.48921459258368105\n",
      "Epoch 185, Loss: 0.4889145977883947\n",
      "Epoch 186, Loss: 0.4886175142174919\n",
      "Epoch 187, Loss: 0.4883282295514753\n",
      "Epoch 188, Loss: 0.4880435212302353\n",
      "Epoch 189, Loss: 0.48775699700499114\n",
      "Epoch 190, Loss: 0.48748008613994775\n",
      "Epoch 191, Loss: 0.4872132179171775\n",
      "Epoch 192, Loss: 0.48694738158534456\n",
      "Epoch 193, Loss: 0.48668121634032785\n",
      "Epoch 194, Loss: 0.4864280322301768\n",
      "Epoch 195, Loss: 0.486178532155915\n",
      "Epoch 196, Loss: 0.4859301919172944\n",
      "Epoch 197, Loss: 0.48568442460989747\n",
      "Epoch 198, Loss: 0.48544224711238787\n",
      "Epoch 199, Loss: 0.48519729219516017\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2669070950285502\n",
      "Valence RMSE: 0.25067895481839453\n",
      "Arousal RMSE: 0.2822035725576775\n",
      "Test R^2 score: tensor([0.2384, 0.2114], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1478, 0.1176], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.22494227904656688\n",
      "Num of epochs: 200\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Epoch 121, Loss: 0.5116998508836337\n",
      "Epoch 122, Loss: 0.5111129902664598\n",
      "Epoch 123, Loss: 0.5105405741436312\n",
      "Epoch 124, Loss: 0.5099849010083686\n",
      "Epoch 125, Loss: 0.5094623759868415\n",
      "Epoch 126, Loss: 0.5089652841754122\n",
      "Epoch 127, Loss: 0.5084717799132713\n",
      "Epoch 128, Loss: 0.5079809643002384\n",
      "Epoch 129, Loss: 0.507507731608986\n",
      "Epoch 130, Loss: 0.5070515727009828\n",
      "Epoch 131, Loss: 0.5065962678643066\n",
      "Epoch 132, Loss: 0.5061448812256618\n",
      "Epoch 133, Loss: 0.5057069703510908\n",
      "Epoch 134, Loss: 0.5052756693952567\n",
      "Epoch 135, Loss: 0.5048445017462813\n",
      "Epoch 136, Loss: 0.504425284231979\n",
      "Epoch 137, Loss: 0.504022717877823\n",
      "Epoch 138, Loss: 0.5036312802042249\n",
      "Epoch 139, Loss: 0.5032368731068207\n",
      "Epoch 140, Loss: 0.5028511060044271\n",
      "Epoch 141, Loss: 0.5024804933282331\n",
      "Epoch 142, Loss: 0.5021182727396313\n",
      "Epoch 143, Loss: 0.5017581664990297\n",
      "Epoch 144, Loss: 0.5014050827892457\n",
      "Epoch 145, Loss: 0.501047675902349\n",
      "Epoch 146, Loss: 0.500690966247362\n",
      "Epoch 147, Loss: 0.5003476722262875\n",
      "Epoch 148, Loss: 0.5000068544871652\n",
      "Epoch 149, Loss: 0.49966338867073357\n",
      "Epoch 150, Loss: 0.4993280873087794\n",
      "Epoch 151, Loss: 0.49900257940428383\n",
      "Epoch 152, Loss: 0.4986865554458311\n",
      "Epoch 153, Loss: 0.49836949389755036\n",
      "Epoch 154, Loss: 0.49805375636713634\n",
      "Epoch 155, Loss: 0.49773873165141763\n",
      "Epoch 156, Loss: 0.49742224924400874\n",
      "Epoch 157, Loss: 0.49711425826812494\n",
      "Epoch 158, Loss: 0.4968010973402242\n",
      "Epoch 159, Loss: 0.4964901399315246\n",
      "Epoch 160, Loss: 0.49617675026847124\n",
      "Epoch 161, Loss: 0.4958670390985117\n",
      "Epoch 162, Loss: 0.4955578109301908\n",
      "Epoch 163, Loss: 0.4952510223984696\n",
      "Epoch 164, Loss: 0.49494577484111674\n",
      "Epoch 165, Loss: 0.49464511374066544\n",
      "Epoch 166, Loss: 0.49434383269975535\n",
      "Epoch 167, Loss: 0.49404631909698943\n",
      "Epoch 168, Loss: 0.49374530645143583\n",
      "Epoch 169, Loss: 0.49345064806438055\n",
      "Epoch 170, Loss: 0.49317001490258333\n",
      "Epoch 171, Loss: 0.4928927591216884\n",
      "Epoch 172, Loss: 0.4926138499628376\n",
      "Epoch 173, Loss: 0.49233528219511824\n",
      "Epoch 174, Loss: 0.49205852514110515\n",
      "Epoch 175, Loss: 0.4917773399684819\n",
      "Epoch 176, Loss: 0.49149629710928233\n",
      "Epoch 177, Loss: 0.4912165647150265\n",
      "Epoch 178, Loss: 0.49094181765497674\n",
      "Epoch 179, Loss: 0.49066901222163783\n",
      "Epoch 180, Loss: 0.490393928005003\n",
      "Epoch 181, Loss: 0.4901079113527018\n",
      "Epoch 182, Loss: 0.48981984154671765\n",
      "Epoch 183, Loss: 0.4895234443341763\n",
      "Epoch 184, Loss: 0.48921459258368105\n",
      "Epoch 185, Loss: 0.4889145977883947\n",
      "Epoch 186, Loss: 0.4886175142174919\n",
      "Epoch 187, Loss: 0.4883282295514753\n",
      "Epoch 188, Loss: 0.4880435212302353\n",
      "Epoch 189, Loss: 0.48775699700499114\n",
      "Epoch 190, Loss: 0.48748008613994775\n",
      "Epoch 191, Loss: 0.4872132179171775\n",
      "Epoch 192, Loss: 0.48694738158534456\n",
      "Epoch 193, Loss: 0.48668121634032785\n",
      "Epoch 194, Loss: 0.4864280322301768\n",
      "Epoch 195, Loss: 0.486178532155915\n",
      "Epoch 196, Loss: 0.4859301919172944\n",
      "Epoch 197, Loss: 0.48568442460989747\n",
      "Epoch 198, Loss: 0.48544224711238787\n",
      "Epoch 199, Loss: 0.48519729219516017\n",
      "Epoch 200, Loss: 0.4849575446659644\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2670736170590838\n",
      "Valence RMSE: 0.25079474429820336\n",
      "Arousal RMSE: 0.28241570439768926\n",
      "Test R^2 score: tensor([0.2377, 0.2103], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1471, 0.1163], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.22399745143975314\n",
      "Num of epochs: 201\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Epoch 121, Loss: 0.5116998508836337\n",
      "Epoch 122, Loss: 0.5111129902664598\n",
      "Epoch 123, Loss: 0.5105405741436312\n",
      "Epoch 124, Loss: 0.5099849010083686\n",
      "Epoch 125, Loss: 0.5094623759868415\n",
      "Epoch 126, Loss: 0.5089652841754122\n",
      "Epoch 127, Loss: 0.5084717799132713\n",
      "Epoch 128, Loss: 0.5079809643002384\n",
      "Epoch 129, Loss: 0.507507731608986\n",
      "Epoch 130, Loss: 0.5070515727009828\n",
      "Epoch 131, Loss: 0.5065962678643066\n",
      "Epoch 132, Loss: 0.5061448812256618\n",
      "Epoch 133, Loss: 0.5057069703510908\n",
      "Epoch 134, Loss: 0.5052756693952567\n",
      "Epoch 135, Loss: 0.5048445017462813\n",
      "Epoch 136, Loss: 0.504425284231979\n",
      "Epoch 137, Loss: 0.504022717877823\n",
      "Epoch 138, Loss: 0.5036312802042249\n",
      "Epoch 139, Loss: 0.5032368731068207\n",
      "Epoch 140, Loss: 0.5028511060044271\n",
      "Epoch 141, Loss: 0.5024804933282331\n",
      "Epoch 142, Loss: 0.5021182727396313\n",
      "Epoch 143, Loss: 0.5017581664990297\n",
      "Epoch 144, Loss: 0.5014050827892457\n",
      "Epoch 145, Loss: 0.501047675902349\n",
      "Epoch 146, Loss: 0.500690966247362\n",
      "Epoch 147, Loss: 0.5003476722262875\n",
      "Epoch 148, Loss: 0.5000068544871652\n",
      "Epoch 149, Loss: 0.49966338867073357\n",
      "Epoch 150, Loss: 0.4993280873087794\n",
      "Epoch 151, Loss: 0.49900257940428383\n",
      "Epoch 152, Loss: 0.4986865554458311\n",
      "Epoch 153, Loss: 0.49836949389755036\n",
      "Epoch 154, Loss: 0.49805375636713634\n",
      "Epoch 155, Loss: 0.49773873165141763\n",
      "Epoch 156, Loss: 0.49742224924400874\n",
      "Epoch 157, Loss: 0.49711425826812494\n",
      "Epoch 158, Loss: 0.4968010973402242\n",
      "Epoch 159, Loss: 0.4964901399315246\n",
      "Epoch 160, Loss: 0.49617675026847124\n",
      "Epoch 161, Loss: 0.4958670390985117\n",
      "Epoch 162, Loss: 0.4955578109301908\n",
      "Epoch 163, Loss: 0.4952510223984696\n",
      "Epoch 164, Loss: 0.49494577484111674\n",
      "Epoch 165, Loss: 0.49464511374066544\n",
      "Epoch 166, Loss: 0.49434383269975535\n",
      "Epoch 167, Loss: 0.49404631909698943\n",
      "Epoch 168, Loss: 0.49374530645143583\n",
      "Epoch 169, Loss: 0.49345064806438055\n",
      "Epoch 170, Loss: 0.49317001490258333\n",
      "Epoch 171, Loss: 0.4928927591216884\n",
      "Epoch 172, Loss: 0.4926138499628376\n",
      "Epoch 173, Loss: 0.49233528219511824\n",
      "Epoch 174, Loss: 0.49205852514110515\n",
      "Epoch 175, Loss: 0.4917773399684819\n",
      "Epoch 176, Loss: 0.49149629710928233\n",
      "Epoch 177, Loss: 0.4912165647150265\n",
      "Epoch 178, Loss: 0.49094181765497674\n",
      "Epoch 179, Loss: 0.49066901222163783\n",
      "Epoch 180, Loss: 0.490393928005003\n",
      "Epoch 181, Loss: 0.4901079113527018\n",
      "Epoch 182, Loss: 0.48981984154671765\n",
      "Epoch 183, Loss: 0.4895234443341763\n",
      "Epoch 184, Loss: 0.48921459258368105\n",
      "Epoch 185, Loss: 0.4889145977883947\n",
      "Epoch 186, Loss: 0.4886175142174919\n",
      "Epoch 187, Loss: 0.4883282295514753\n",
      "Epoch 188, Loss: 0.4880435212302353\n",
      "Epoch 189, Loss: 0.48775699700499114\n",
      "Epoch 190, Loss: 0.48748008613994775\n",
      "Epoch 191, Loss: 0.4872132179171775\n",
      "Epoch 192, Loss: 0.48694738158534456\n",
      "Epoch 193, Loss: 0.48668121634032785\n",
      "Epoch 194, Loss: 0.4864280322301768\n",
      "Epoch 195, Loss: 0.486178532155915\n",
      "Epoch 196, Loss: 0.4859301919172944\n",
      "Epoch 197, Loss: 0.48568442460989747\n",
      "Epoch 198, Loss: 0.48544224711238787\n",
      "Epoch 199, Loss: 0.48519729219516017\n",
      "Epoch 200, Loss: 0.4849575446659644\n",
      "Epoch 201, Loss: 0.4847146197220469\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2672559817248744\n",
      "Valence RMSE: 0.2509205719499036\n",
      "Arousal RMSE: 0.2826488742376061\n",
      "Test R^2 score: tensor([0.2370, 0.2090], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1462, 0.1148], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.22296261442324017\n",
      "Num of epochs: 202\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Epoch 121, Loss: 0.5116998508836337\n",
      "Epoch 122, Loss: 0.5111129902664598\n",
      "Epoch 123, Loss: 0.5105405741436312\n",
      "Epoch 124, Loss: 0.5099849010083686\n",
      "Epoch 125, Loss: 0.5094623759868415\n",
      "Epoch 126, Loss: 0.5089652841754122\n",
      "Epoch 127, Loss: 0.5084717799132713\n",
      "Epoch 128, Loss: 0.5079809643002384\n",
      "Epoch 129, Loss: 0.507507731608986\n",
      "Epoch 130, Loss: 0.5070515727009828\n",
      "Epoch 131, Loss: 0.5065962678643066\n",
      "Epoch 132, Loss: 0.5061448812256618\n",
      "Epoch 133, Loss: 0.5057069703510908\n",
      "Epoch 134, Loss: 0.5052756693952567\n",
      "Epoch 135, Loss: 0.5048445017462813\n",
      "Epoch 136, Loss: 0.504425284231979\n",
      "Epoch 137, Loss: 0.504022717877823\n",
      "Epoch 138, Loss: 0.5036312802042249\n",
      "Epoch 139, Loss: 0.5032368731068207\n",
      "Epoch 140, Loss: 0.5028511060044271\n",
      "Epoch 141, Loss: 0.5024804933282331\n",
      "Epoch 142, Loss: 0.5021182727396313\n",
      "Epoch 143, Loss: 0.5017581664990297\n",
      "Epoch 144, Loss: 0.5014050827892457\n",
      "Epoch 145, Loss: 0.501047675902349\n",
      "Epoch 146, Loss: 0.500690966247362\n",
      "Epoch 147, Loss: 0.5003476722262875\n",
      "Epoch 148, Loss: 0.5000068544871652\n",
      "Epoch 149, Loss: 0.49966338867073357\n",
      "Epoch 150, Loss: 0.4993280873087794\n",
      "Epoch 151, Loss: 0.49900257940428383\n",
      "Epoch 152, Loss: 0.4986865554458311\n",
      "Epoch 153, Loss: 0.49836949389755036\n",
      "Epoch 154, Loss: 0.49805375636713634\n",
      "Epoch 155, Loss: 0.49773873165141763\n",
      "Epoch 156, Loss: 0.49742224924400874\n",
      "Epoch 157, Loss: 0.49711425826812494\n",
      "Epoch 158, Loss: 0.4968010973402242\n",
      "Epoch 159, Loss: 0.4964901399315246\n",
      "Epoch 160, Loss: 0.49617675026847124\n",
      "Epoch 161, Loss: 0.4958670390985117\n",
      "Epoch 162, Loss: 0.4955578109301908\n",
      "Epoch 163, Loss: 0.4952510223984696\n",
      "Epoch 164, Loss: 0.49494577484111674\n",
      "Epoch 165, Loss: 0.49464511374066544\n",
      "Epoch 166, Loss: 0.49434383269975535\n",
      "Epoch 167, Loss: 0.49404631909698943\n",
      "Epoch 168, Loss: 0.49374530645143583\n",
      "Epoch 169, Loss: 0.49345064806438055\n",
      "Epoch 170, Loss: 0.49317001490258333\n",
      "Epoch 171, Loss: 0.4928927591216884\n",
      "Epoch 172, Loss: 0.4926138499628376\n",
      "Epoch 173, Loss: 0.49233528219511824\n",
      "Epoch 174, Loss: 0.49205852514110515\n",
      "Epoch 175, Loss: 0.4917773399684819\n",
      "Epoch 176, Loss: 0.49149629710928233\n",
      "Epoch 177, Loss: 0.4912165647150265\n",
      "Epoch 178, Loss: 0.49094181765497674\n",
      "Epoch 179, Loss: 0.49066901222163783\n",
      "Epoch 180, Loss: 0.490393928005003\n",
      "Epoch 181, Loss: 0.4901079113527018\n",
      "Epoch 182, Loss: 0.48981984154671765\n",
      "Epoch 183, Loss: 0.4895234443341763\n",
      "Epoch 184, Loss: 0.48921459258368105\n",
      "Epoch 185, Loss: 0.4889145977883947\n",
      "Epoch 186, Loss: 0.4886175142174919\n",
      "Epoch 187, Loss: 0.4883282295514753\n",
      "Epoch 188, Loss: 0.4880435212302353\n",
      "Epoch 189, Loss: 0.48775699700499114\n",
      "Epoch 190, Loss: 0.48748008613994775\n",
      "Epoch 191, Loss: 0.4872132179171775\n",
      "Epoch 192, Loss: 0.48694738158534456\n",
      "Epoch 193, Loss: 0.48668121634032785\n",
      "Epoch 194, Loss: 0.4864280322301768\n",
      "Epoch 195, Loss: 0.486178532155915\n",
      "Epoch 196, Loss: 0.4859301919172944\n",
      "Epoch 197, Loss: 0.48568442460989747\n",
      "Epoch 198, Loss: 0.48544224711238787\n",
      "Epoch 199, Loss: 0.48519729219516017\n",
      "Epoch 200, Loss: 0.4849575446659644\n",
      "Epoch 201, Loss: 0.4847146197220469\n",
      "Epoch 202, Loss: 0.48447155759133576\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.26741860936091655\n",
      "Valence RMSE: 0.251045573874269\n",
      "Arousal RMSE: 0.28284544384379245\n",
      "Test R^2 score: tensor([0.2362, 0.2079], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1453, 0.1136], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.22203207030630878\n",
      "Num of epochs: 203\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Epoch 121, Loss: 0.5116998508836337\n",
      "Epoch 122, Loss: 0.5111129902664598\n",
      "Epoch 123, Loss: 0.5105405741436312\n",
      "Epoch 124, Loss: 0.5099849010083686\n",
      "Epoch 125, Loss: 0.5094623759868415\n",
      "Epoch 126, Loss: 0.5089652841754122\n",
      "Epoch 127, Loss: 0.5084717799132713\n",
      "Epoch 128, Loss: 0.5079809643002384\n",
      "Epoch 129, Loss: 0.507507731608986\n",
      "Epoch 130, Loss: 0.5070515727009828\n",
      "Epoch 131, Loss: 0.5065962678643066\n",
      "Epoch 132, Loss: 0.5061448812256618\n",
      "Epoch 133, Loss: 0.5057069703510908\n",
      "Epoch 134, Loss: 0.5052756693952567\n",
      "Epoch 135, Loss: 0.5048445017462813\n",
      "Epoch 136, Loss: 0.504425284231979\n",
      "Epoch 137, Loss: 0.504022717877823\n",
      "Epoch 138, Loss: 0.5036312802042249\n",
      "Epoch 139, Loss: 0.5032368731068207\n",
      "Epoch 140, Loss: 0.5028511060044271\n",
      "Epoch 141, Loss: 0.5024804933282331\n",
      "Epoch 142, Loss: 0.5021182727396313\n",
      "Epoch 143, Loss: 0.5017581664990297\n",
      "Epoch 144, Loss: 0.5014050827892457\n",
      "Epoch 145, Loss: 0.501047675902349\n",
      "Epoch 146, Loss: 0.500690966247362\n",
      "Epoch 147, Loss: 0.5003476722262875\n",
      "Epoch 148, Loss: 0.5000068544871652\n",
      "Epoch 149, Loss: 0.49966338867073357\n",
      "Epoch 150, Loss: 0.4993280873087794\n",
      "Epoch 151, Loss: 0.49900257940428383\n",
      "Epoch 152, Loss: 0.4986865554458311\n",
      "Epoch 153, Loss: 0.49836949389755036\n",
      "Epoch 154, Loss: 0.49805375636713634\n",
      "Epoch 155, Loss: 0.49773873165141763\n",
      "Epoch 156, Loss: 0.49742224924400874\n",
      "Epoch 157, Loss: 0.49711425826812494\n",
      "Epoch 158, Loss: 0.4968010973402242\n",
      "Epoch 159, Loss: 0.4964901399315246\n",
      "Epoch 160, Loss: 0.49617675026847124\n",
      "Epoch 161, Loss: 0.4958670390985117\n",
      "Epoch 162, Loss: 0.4955578109301908\n",
      "Epoch 163, Loss: 0.4952510223984696\n",
      "Epoch 164, Loss: 0.49494577484111674\n",
      "Epoch 165, Loss: 0.49464511374066544\n",
      "Epoch 166, Loss: 0.49434383269975535\n",
      "Epoch 167, Loss: 0.49404631909698943\n",
      "Epoch 168, Loss: 0.49374530645143583\n",
      "Epoch 169, Loss: 0.49345064806438055\n",
      "Epoch 170, Loss: 0.49317001490258333\n",
      "Epoch 171, Loss: 0.4928927591216884\n",
      "Epoch 172, Loss: 0.4926138499628376\n",
      "Epoch 173, Loss: 0.49233528219511824\n",
      "Epoch 174, Loss: 0.49205852514110515\n",
      "Epoch 175, Loss: 0.4917773399684819\n",
      "Epoch 176, Loss: 0.49149629710928233\n",
      "Epoch 177, Loss: 0.4912165647150265\n",
      "Epoch 178, Loss: 0.49094181765497674\n",
      "Epoch 179, Loss: 0.49066901222163783\n",
      "Epoch 180, Loss: 0.490393928005003\n",
      "Epoch 181, Loss: 0.4901079113527018\n",
      "Epoch 182, Loss: 0.48981984154671765\n",
      "Epoch 183, Loss: 0.4895234443341763\n",
      "Epoch 184, Loss: 0.48921459258368105\n",
      "Epoch 185, Loss: 0.4889145977883947\n",
      "Epoch 186, Loss: 0.4886175142174919\n",
      "Epoch 187, Loss: 0.4883282295514753\n",
      "Epoch 188, Loss: 0.4880435212302353\n",
      "Epoch 189, Loss: 0.48775699700499114\n",
      "Epoch 190, Loss: 0.48748008613994775\n",
      "Epoch 191, Loss: 0.4872132179171775\n",
      "Epoch 192, Loss: 0.48694738158534456\n",
      "Epoch 193, Loss: 0.48668121634032785\n",
      "Epoch 194, Loss: 0.4864280322301768\n",
      "Epoch 195, Loss: 0.486178532155915\n",
      "Epoch 196, Loss: 0.4859301919172944\n",
      "Epoch 197, Loss: 0.48568442460989747\n",
      "Epoch 198, Loss: 0.48544224711238787\n",
      "Epoch 199, Loss: 0.48519729219516017\n",
      "Epoch 200, Loss: 0.4849575446659644\n",
      "Epoch 201, Loss: 0.4847146197220469\n",
      "Epoch 202, Loss: 0.48447155759133576\n",
      "Epoch 203, Loss: 0.48422783492589483\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2675525731165358\n",
      "Valence RMSE: 0.2511626283488712\n",
      "Arousal RMSE: 0.2829948637050596\n",
      "Test R^2 score: tensor([0.2355, 0.2070], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1445, 0.1127], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.22125727583329907\n",
      "Num of epochs: 204\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Epoch 121, Loss: 0.5116998508836337\n",
      "Epoch 122, Loss: 0.5111129902664598\n",
      "Epoch 123, Loss: 0.5105405741436312\n",
      "Epoch 124, Loss: 0.5099849010083686\n",
      "Epoch 125, Loss: 0.5094623759868415\n",
      "Epoch 126, Loss: 0.5089652841754122\n",
      "Epoch 127, Loss: 0.5084717799132713\n",
      "Epoch 128, Loss: 0.5079809643002384\n",
      "Epoch 129, Loss: 0.507507731608986\n",
      "Epoch 130, Loss: 0.5070515727009828\n",
      "Epoch 131, Loss: 0.5065962678643066\n",
      "Epoch 132, Loss: 0.5061448812256618\n",
      "Epoch 133, Loss: 0.5057069703510908\n",
      "Epoch 134, Loss: 0.5052756693952567\n",
      "Epoch 135, Loss: 0.5048445017462813\n",
      "Epoch 136, Loss: 0.504425284231979\n",
      "Epoch 137, Loss: 0.504022717877823\n",
      "Epoch 138, Loss: 0.5036312802042249\n",
      "Epoch 139, Loss: 0.5032368731068207\n",
      "Epoch 140, Loss: 0.5028511060044271\n",
      "Epoch 141, Loss: 0.5024804933282331\n",
      "Epoch 142, Loss: 0.5021182727396313\n",
      "Epoch 143, Loss: 0.5017581664990297\n",
      "Epoch 144, Loss: 0.5014050827892457\n",
      "Epoch 145, Loss: 0.501047675902349\n",
      "Epoch 146, Loss: 0.500690966247362\n",
      "Epoch 147, Loss: 0.5003476722262875\n",
      "Epoch 148, Loss: 0.5000068544871652\n",
      "Epoch 149, Loss: 0.49966338867073357\n",
      "Epoch 150, Loss: 0.4993280873087794\n",
      "Epoch 151, Loss: 0.49900257940428383\n",
      "Epoch 152, Loss: 0.4986865554458311\n",
      "Epoch 153, Loss: 0.49836949389755036\n",
      "Epoch 154, Loss: 0.49805375636713634\n",
      "Epoch 155, Loss: 0.49773873165141763\n",
      "Epoch 156, Loss: 0.49742224924400874\n",
      "Epoch 157, Loss: 0.49711425826812494\n",
      "Epoch 158, Loss: 0.4968010973402242\n",
      "Epoch 159, Loss: 0.4964901399315246\n",
      "Epoch 160, Loss: 0.49617675026847124\n",
      "Epoch 161, Loss: 0.4958670390985117\n",
      "Epoch 162, Loss: 0.4955578109301908\n",
      "Epoch 163, Loss: 0.4952510223984696\n",
      "Epoch 164, Loss: 0.49494577484111674\n",
      "Epoch 165, Loss: 0.49464511374066544\n",
      "Epoch 166, Loss: 0.49434383269975535\n",
      "Epoch 167, Loss: 0.49404631909698943\n",
      "Epoch 168, Loss: 0.49374530645143583\n",
      "Epoch 169, Loss: 0.49345064806438055\n",
      "Epoch 170, Loss: 0.49317001490258333\n",
      "Epoch 171, Loss: 0.4928927591216884\n",
      "Epoch 172, Loss: 0.4926138499628376\n",
      "Epoch 173, Loss: 0.49233528219511824\n",
      "Epoch 174, Loss: 0.49205852514110515\n",
      "Epoch 175, Loss: 0.4917773399684819\n",
      "Epoch 176, Loss: 0.49149629710928233\n",
      "Epoch 177, Loss: 0.4912165647150265\n",
      "Epoch 178, Loss: 0.49094181765497674\n",
      "Epoch 179, Loss: 0.49066901222163783\n",
      "Epoch 180, Loss: 0.490393928005003\n",
      "Epoch 181, Loss: 0.4901079113527018\n",
      "Epoch 182, Loss: 0.48981984154671765\n",
      "Epoch 183, Loss: 0.4895234443341763\n",
      "Epoch 184, Loss: 0.48921459258368105\n",
      "Epoch 185, Loss: 0.4889145977883947\n",
      "Epoch 186, Loss: 0.4886175142174919\n",
      "Epoch 187, Loss: 0.4883282295514753\n",
      "Epoch 188, Loss: 0.4880435212302353\n",
      "Epoch 189, Loss: 0.48775699700499114\n",
      "Epoch 190, Loss: 0.48748008613994775\n",
      "Epoch 191, Loss: 0.4872132179171775\n",
      "Epoch 192, Loss: 0.48694738158534456\n",
      "Epoch 193, Loss: 0.48668121634032785\n",
      "Epoch 194, Loss: 0.4864280322301768\n",
      "Epoch 195, Loss: 0.486178532155915\n",
      "Epoch 196, Loss: 0.4859301919172944\n",
      "Epoch 197, Loss: 0.48568442460989747\n",
      "Epoch 198, Loss: 0.48544224711238787\n",
      "Epoch 199, Loss: 0.48519729219516017\n",
      "Epoch 200, Loss: 0.4849575446659644\n",
      "Epoch 201, Loss: 0.4847146197220469\n",
      "Epoch 202, Loss: 0.48447155759133576\n",
      "Epoch 203, Loss: 0.48422783492589483\n",
      "Epoch 204, Loss: 0.48397707745066637\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.26763687773591716\n",
      "Valence RMSE: 0.2512163247042461\n",
      "Arousal RMSE: 0.2831066139293663\n",
      "Test R^2 score: tensor([0.2352, 0.2064], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1442, 0.1120], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.22078061542689814\n",
      "Num of epochs: 205\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Epoch 121, Loss: 0.5116998508836337\n",
      "Epoch 122, Loss: 0.5111129902664598\n",
      "Epoch 123, Loss: 0.5105405741436312\n",
      "Epoch 124, Loss: 0.5099849010083686\n",
      "Epoch 125, Loss: 0.5094623759868415\n",
      "Epoch 126, Loss: 0.5089652841754122\n",
      "Epoch 127, Loss: 0.5084717799132713\n",
      "Epoch 128, Loss: 0.5079809643002384\n",
      "Epoch 129, Loss: 0.507507731608986\n",
      "Epoch 130, Loss: 0.5070515727009828\n",
      "Epoch 131, Loss: 0.5065962678643066\n",
      "Epoch 132, Loss: 0.5061448812256618\n",
      "Epoch 133, Loss: 0.5057069703510908\n",
      "Epoch 134, Loss: 0.5052756693952567\n",
      "Epoch 135, Loss: 0.5048445017462813\n",
      "Epoch 136, Loss: 0.504425284231979\n",
      "Epoch 137, Loss: 0.504022717877823\n",
      "Epoch 138, Loss: 0.5036312802042249\n",
      "Epoch 139, Loss: 0.5032368731068207\n",
      "Epoch 140, Loss: 0.5028511060044271\n",
      "Epoch 141, Loss: 0.5024804933282331\n",
      "Epoch 142, Loss: 0.5021182727396313\n",
      "Epoch 143, Loss: 0.5017581664990297\n",
      "Epoch 144, Loss: 0.5014050827892457\n",
      "Epoch 145, Loss: 0.501047675902349\n",
      "Epoch 146, Loss: 0.500690966247362\n",
      "Epoch 147, Loss: 0.5003476722262875\n",
      "Epoch 148, Loss: 0.5000068544871652\n",
      "Epoch 149, Loss: 0.49966338867073357\n",
      "Epoch 150, Loss: 0.4993280873087794\n",
      "Epoch 151, Loss: 0.49900257940428383\n",
      "Epoch 152, Loss: 0.4986865554458311\n",
      "Epoch 153, Loss: 0.49836949389755036\n",
      "Epoch 154, Loss: 0.49805375636713634\n",
      "Epoch 155, Loss: 0.49773873165141763\n",
      "Epoch 156, Loss: 0.49742224924400874\n",
      "Epoch 157, Loss: 0.49711425826812494\n",
      "Epoch 158, Loss: 0.4968010973402242\n",
      "Epoch 159, Loss: 0.4964901399315246\n",
      "Epoch 160, Loss: 0.49617675026847124\n",
      "Epoch 161, Loss: 0.4958670390985117\n",
      "Epoch 162, Loss: 0.4955578109301908\n",
      "Epoch 163, Loss: 0.4952510223984696\n",
      "Epoch 164, Loss: 0.49494577484111674\n",
      "Epoch 165, Loss: 0.49464511374066544\n",
      "Epoch 166, Loss: 0.49434383269975535\n",
      "Epoch 167, Loss: 0.49404631909698943\n",
      "Epoch 168, Loss: 0.49374530645143583\n",
      "Epoch 169, Loss: 0.49345064806438055\n",
      "Epoch 170, Loss: 0.49317001490258333\n",
      "Epoch 171, Loss: 0.4928927591216884\n",
      "Epoch 172, Loss: 0.4926138499628376\n",
      "Epoch 173, Loss: 0.49233528219511824\n",
      "Epoch 174, Loss: 0.49205852514110515\n",
      "Epoch 175, Loss: 0.4917773399684819\n",
      "Epoch 176, Loss: 0.49149629710928233\n",
      "Epoch 177, Loss: 0.4912165647150265\n",
      "Epoch 178, Loss: 0.49094181765497674\n",
      "Epoch 179, Loss: 0.49066901222163783\n",
      "Epoch 180, Loss: 0.490393928005003\n",
      "Epoch 181, Loss: 0.4901079113527018\n",
      "Epoch 182, Loss: 0.48981984154671765\n",
      "Epoch 183, Loss: 0.4895234443341763\n",
      "Epoch 184, Loss: 0.48921459258368105\n",
      "Epoch 185, Loss: 0.4889145977883947\n",
      "Epoch 186, Loss: 0.4886175142174919\n",
      "Epoch 187, Loss: 0.4883282295514753\n",
      "Epoch 188, Loss: 0.4880435212302353\n",
      "Epoch 189, Loss: 0.48775699700499114\n",
      "Epoch 190, Loss: 0.48748008613994775\n",
      "Epoch 191, Loss: 0.4872132179171775\n",
      "Epoch 192, Loss: 0.48694738158534456\n",
      "Epoch 193, Loss: 0.48668121634032785\n",
      "Epoch 194, Loss: 0.4864280322301768\n",
      "Epoch 195, Loss: 0.486178532155915\n",
      "Epoch 196, Loss: 0.4859301919172944\n",
      "Epoch 197, Loss: 0.48568442460989747\n",
      "Epoch 198, Loss: 0.48544224711238787\n",
      "Epoch 199, Loss: 0.48519729219516017\n",
      "Epoch 200, Loss: 0.4849575446659644\n",
      "Epoch 201, Loss: 0.4847146197220469\n",
      "Epoch 202, Loss: 0.48447155759133576\n",
      "Epoch 203, Loss: 0.48422783492589483\n",
      "Epoch 204, Loss: 0.48397707745066637\n",
      "Epoch 205, Loss: 0.483737926529631\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2677472903380501\n",
      "Valence RMSE: 0.2512535373292364\n",
      "Arousal RMSE: 0.28328233786503926\n",
      "Test R^2 score: tensor([0.2349, 0.2054], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1439, 0.1109], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.22017456590257256\n",
      "Num of epochs: 206\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Epoch 121, Loss: 0.5116998508836337\n",
      "Epoch 122, Loss: 0.5111129902664598\n",
      "Epoch 123, Loss: 0.5105405741436312\n",
      "Epoch 124, Loss: 0.5099849010083686\n",
      "Epoch 125, Loss: 0.5094623759868415\n",
      "Epoch 126, Loss: 0.5089652841754122\n",
      "Epoch 127, Loss: 0.5084717799132713\n",
      "Epoch 128, Loss: 0.5079809643002384\n",
      "Epoch 129, Loss: 0.507507731608986\n",
      "Epoch 130, Loss: 0.5070515727009828\n",
      "Epoch 131, Loss: 0.5065962678643066\n",
      "Epoch 132, Loss: 0.5061448812256618\n",
      "Epoch 133, Loss: 0.5057069703510908\n",
      "Epoch 134, Loss: 0.5052756693952567\n",
      "Epoch 135, Loss: 0.5048445017462813\n",
      "Epoch 136, Loss: 0.504425284231979\n",
      "Epoch 137, Loss: 0.504022717877823\n",
      "Epoch 138, Loss: 0.5036312802042249\n",
      "Epoch 139, Loss: 0.5032368731068207\n",
      "Epoch 140, Loss: 0.5028511060044271\n",
      "Epoch 141, Loss: 0.5024804933282331\n",
      "Epoch 142, Loss: 0.5021182727396313\n",
      "Epoch 143, Loss: 0.5017581664990297\n",
      "Epoch 144, Loss: 0.5014050827892457\n",
      "Epoch 145, Loss: 0.501047675902349\n",
      "Epoch 146, Loss: 0.500690966247362\n",
      "Epoch 147, Loss: 0.5003476722262875\n",
      "Epoch 148, Loss: 0.5000068544871652\n",
      "Epoch 149, Loss: 0.49966338867073357\n",
      "Epoch 150, Loss: 0.4993280873087794\n",
      "Epoch 151, Loss: 0.49900257940428383\n",
      "Epoch 152, Loss: 0.4986865554458311\n",
      "Epoch 153, Loss: 0.49836949389755036\n",
      "Epoch 154, Loss: 0.49805375636713634\n",
      "Epoch 155, Loss: 0.49773873165141763\n",
      "Epoch 156, Loss: 0.49742224924400874\n",
      "Epoch 157, Loss: 0.49711425826812494\n",
      "Epoch 158, Loss: 0.4968010973402242\n",
      "Epoch 159, Loss: 0.4964901399315246\n",
      "Epoch 160, Loss: 0.49617675026847124\n",
      "Epoch 161, Loss: 0.4958670390985117\n",
      "Epoch 162, Loss: 0.4955578109301908\n",
      "Epoch 163, Loss: 0.4952510223984696\n",
      "Epoch 164, Loss: 0.49494577484111674\n",
      "Epoch 165, Loss: 0.49464511374066544\n",
      "Epoch 166, Loss: 0.49434383269975535\n",
      "Epoch 167, Loss: 0.49404631909698943\n",
      "Epoch 168, Loss: 0.49374530645143583\n",
      "Epoch 169, Loss: 0.49345064806438055\n",
      "Epoch 170, Loss: 0.49317001490258333\n",
      "Epoch 171, Loss: 0.4928927591216884\n",
      "Epoch 172, Loss: 0.4926138499628376\n",
      "Epoch 173, Loss: 0.49233528219511824\n",
      "Epoch 174, Loss: 0.49205852514110515\n",
      "Epoch 175, Loss: 0.4917773399684819\n",
      "Epoch 176, Loss: 0.49149629710928233\n",
      "Epoch 177, Loss: 0.4912165647150265\n",
      "Epoch 178, Loss: 0.49094181765497674\n",
      "Epoch 179, Loss: 0.49066901222163783\n",
      "Epoch 180, Loss: 0.490393928005003\n",
      "Epoch 181, Loss: 0.4901079113527018\n",
      "Epoch 182, Loss: 0.48981984154671765\n",
      "Epoch 183, Loss: 0.4895234443341763\n",
      "Epoch 184, Loss: 0.48921459258368105\n",
      "Epoch 185, Loss: 0.4889145977883947\n",
      "Epoch 186, Loss: 0.4886175142174919\n",
      "Epoch 187, Loss: 0.4883282295514753\n",
      "Epoch 188, Loss: 0.4880435212302353\n",
      "Epoch 189, Loss: 0.48775699700499114\n",
      "Epoch 190, Loss: 0.48748008613994775\n",
      "Epoch 191, Loss: 0.4872132179171775\n",
      "Epoch 192, Loss: 0.48694738158534456\n",
      "Epoch 193, Loss: 0.48668121634032785\n",
      "Epoch 194, Loss: 0.4864280322301768\n",
      "Epoch 195, Loss: 0.486178532155915\n",
      "Epoch 196, Loss: 0.4859301919172944\n",
      "Epoch 197, Loss: 0.48568442460989747\n",
      "Epoch 198, Loss: 0.48544224711238787\n",
      "Epoch 199, Loss: 0.48519729219516017\n",
      "Epoch 200, Loss: 0.4849575446659644\n",
      "Epoch 201, Loss: 0.4847146197220469\n",
      "Epoch 202, Loss: 0.48447155759133576\n",
      "Epoch 203, Loss: 0.48422783492589483\n",
      "Epoch 204, Loss: 0.48397707745066637\n",
      "Epoch 205, Loss: 0.483737926529631\n",
      "Epoch 206, Loss: 0.4835020782648923\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2678512560923927\n",
      "Valence RMSE: 0.2512826493836153\n",
      "Arousal RMSE: 0.2834530312049884\n",
      "Test R^2 score: tensor([0.2348, 0.2044], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1437, 0.1098], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.21960698215847485\n",
      "Num of epochs: 207\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Epoch 121, Loss: 0.5116998508836337\n",
      "Epoch 122, Loss: 0.5111129902664598\n",
      "Epoch 123, Loss: 0.5105405741436312\n",
      "Epoch 124, Loss: 0.5099849010083686\n",
      "Epoch 125, Loss: 0.5094623759868415\n",
      "Epoch 126, Loss: 0.5089652841754122\n",
      "Epoch 127, Loss: 0.5084717799132713\n",
      "Epoch 128, Loss: 0.5079809643002384\n",
      "Epoch 129, Loss: 0.507507731608986\n",
      "Epoch 130, Loss: 0.5070515727009828\n",
      "Epoch 131, Loss: 0.5065962678643066\n",
      "Epoch 132, Loss: 0.5061448812256618\n",
      "Epoch 133, Loss: 0.5057069703510908\n",
      "Epoch 134, Loss: 0.5052756693952567\n",
      "Epoch 135, Loss: 0.5048445017462813\n",
      "Epoch 136, Loss: 0.504425284231979\n",
      "Epoch 137, Loss: 0.504022717877823\n",
      "Epoch 138, Loss: 0.5036312802042249\n",
      "Epoch 139, Loss: 0.5032368731068207\n",
      "Epoch 140, Loss: 0.5028511060044271\n",
      "Epoch 141, Loss: 0.5024804933282331\n",
      "Epoch 142, Loss: 0.5021182727396313\n",
      "Epoch 143, Loss: 0.5017581664990297\n",
      "Epoch 144, Loss: 0.5014050827892457\n",
      "Epoch 145, Loss: 0.501047675902349\n",
      "Epoch 146, Loss: 0.500690966247362\n",
      "Epoch 147, Loss: 0.5003476722262875\n",
      "Epoch 148, Loss: 0.5000068544871652\n",
      "Epoch 149, Loss: 0.49966338867073357\n",
      "Epoch 150, Loss: 0.4993280873087794\n",
      "Epoch 151, Loss: 0.49900257940428383\n",
      "Epoch 152, Loss: 0.4986865554458311\n",
      "Epoch 153, Loss: 0.49836949389755036\n",
      "Epoch 154, Loss: 0.49805375636713634\n",
      "Epoch 155, Loss: 0.49773873165141763\n",
      "Epoch 156, Loss: 0.49742224924400874\n",
      "Epoch 157, Loss: 0.49711425826812494\n",
      "Epoch 158, Loss: 0.4968010973402242\n",
      "Epoch 159, Loss: 0.4964901399315246\n",
      "Epoch 160, Loss: 0.49617675026847124\n",
      "Epoch 161, Loss: 0.4958670390985117\n",
      "Epoch 162, Loss: 0.4955578109301908\n",
      "Epoch 163, Loss: 0.4952510223984696\n",
      "Epoch 164, Loss: 0.49494577484111674\n",
      "Epoch 165, Loss: 0.49464511374066544\n",
      "Epoch 166, Loss: 0.49434383269975535\n",
      "Epoch 167, Loss: 0.49404631909698943\n",
      "Epoch 168, Loss: 0.49374530645143583\n",
      "Epoch 169, Loss: 0.49345064806438055\n",
      "Epoch 170, Loss: 0.49317001490258333\n",
      "Epoch 171, Loss: 0.4928927591216884\n",
      "Epoch 172, Loss: 0.4926138499628376\n",
      "Epoch 173, Loss: 0.49233528219511824\n",
      "Epoch 174, Loss: 0.49205852514110515\n",
      "Epoch 175, Loss: 0.4917773399684819\n",
      "Epoch 176, Loss: 0.49149629710928233\n",
      "Epoch 177, Loss: 0.4912165647150265\n",
      "Epoch 178, Loss: 0.49094181765497674\n",
      "Epoch 179, Loss: 0.49066901222163783\n",
      "Epoch 180, Loss: 0.490393928005003\n",
      "Epoch 181, Loss: 0.4901079113527018\n",
      "Epoch 182, Loss: 0.48981984154671765\n",
      "Epoch 183, Loss: 0.4895234443341763\n",
      "Epoch 184, Loss: 0.48921459258368105\n",
      "Epoch 185, Loss: 0.4889145977883947\n",
      "Epoch 186, Loss: 0.4886175142174919\n",
      "Epoch 187, Loss: 0.4883282295514753\n",
      "Epoch 188, Loss: 0.4880435212302353\n",
      "Epoch 189, Loss: 0.48775699700499114\n",
      "Epoch 190, Loss: 0.48748008613994775\n",
      "Epoch 191, Loss: 0.4872132179171775\n",
      "Epoch 192, Loss: 0.48694738158534456\n",
      "Epoch 193, Loss: 0.48668121634032785\n",
      "Epoch 194, Loss: 0.4864280322301768\n",
      "Epoch 195, Loss: 0.486178532155915\n",
      "Epoch 196, Loss: 0.4859301919172944\n",
      "Epoch 197, Loss: 0.48568442460989747\n",
      "Epoch 198, Loss: 0.48544224711238787\n",
      "Epoch 199, Loss: 0.48519729219516017\n",
      "Epoch 200, Loss: 0.4849575446659644\n",
      "Epoch 201, Loss: 0.4847146197220469\n",
      "Epoch 202, Loss: 0.48447155759133576\n",
      "Epoch 203, Loss: 0.48422783492589483\n",
      "Epoch 204, Loss: 0.48397707745066637\n",
      "Epoch 205, Loss: 0.483737926529631\n",
      "Epoch 206, Loss: 0.4835020782648923\n",
      "Epoch 207, Loss: 0.48327083252052233\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.267985604473904\n",
      "Valence RMSE: 0.25132152043678124\n",
      "Arousal RMSE: 0.2836724550883148\n",
      "Test R^2 score: tensor([0.2345, 0.2032], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1435, 0.1084], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.2188725123652045\n",
      "Num of epochs: 208\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Epoch 121, Loss: 0.5116998508836337\n",
      "Epoch 122, Loss: 0.5111129902664598\n",
      "Epoch 123, Loss: 0.5105405741436312\n",
      "Epoch 124, Loss: 0.5099849010083686\n",
      "Epoch 125, Loss: 0.5094623759868415\n",
      "Epoch 126, Loss: 0.5089652841754122\n",
      "Epoch 127, Loss: 0.5084717799132713\n",
      "Epoch 128, Loss: 0.5079809643002384\n",
      "Epoch 129, Loss: 0.507507731608986\n",
      "Epoch 130, Loss: 0.5070515727009828\n",
      "Epoch 131, Loss: 0.5065962678643066\n",
      "Epoch 132, Loss: 0.5061448812256618\n",
      "Epoch 133, Loss: 0.5057069703510908\n",
      "Epoch 134, Loss: 0.5052756693952567\n",
      "Epoch 135, Loss: 0.5048445017462813\n",
      "Epoch 136, Loss: 0.504425284231979\n",
      "Epoch 137, Loss: 0.504022717877823\n",
      "Epoch 138, Loss: 0.5036312802042249\n",
      "Epoch 139, Loss: 0.5032368731068207\n",
      "Epoch 140, Loss: 0.5028511060044271\n",
      "Epoch 141, Loss: 0.5024804933282331\n",
      "Epoch 142, Loss: 0.5021182727396313\n",
      "Epoch 143, Loss: 0.5017581664990297\n",
      "Epoch 144, Loss: 0.5014050827892457\n",
      "Epoch 145, Loss: 0.501047675902349\n",
      "Epoch 146, Loss: 0.500690966247362\n",
      "Epoch 147, Loss: 0.5003476722262875\n",
      "Epoch 148, Loss: 0.5000068544871652\n",
      "Epoch 149, Loss: 0.49966338867073357\n",
      "Epoch 150, Loss: 0.4993280873087794\n",
      "Epoch 151, Loss: 0.49900257940428383\n",
      "Epoch 152, Loss: 0.4986865554458311\n",
      "Epoch 153, Loss: 0.49836949389755036\n",
      "Epoch 154, Loss: 0.49805375636713634\n",
      "Epoch 155, Loss: 0.49773873165141763\n",
      "Epoch 156, Loss: 0.49742224924400874\n",
      "Epoch 157, Loss: 0.49711425826812494\n",
      "Epoch 158, Loss: 0.4968010973402242\n",
      "Epoch 159, Loss: 0.4964901399315246\n",
      "Epoch 160, Loss: 0.49617675026847124\n",
      "Epoch 161, Loss: 0.4958670390985117\n",
      "Epoch 162, Loss: 0.4955578109301908\n",
      "Epoch 163, Loss: 0.4952510223984696\n",
      "Epoch 164, Loss: 0.49494577484111674\n",
      "Epoch 165, Loss: 0.49464511374066544\n",
      "Epoch 166, Loss: 0.49434383269975535\n",
      "Epoch 167, Loss: 0.49404631909698943\n",
      "Epoch 168, Loss: 0.49374530645143583\n",
      "Epoch 169, Loss: 0.49345064806438055\n",
      "Epoch 170, Loss: 0.49317001490258333\n",
      "Epoch 171, Loss: 0.4928927591216884\n",
      "Epoch 172, Loss: 0.4926138499628376\n",
      "Epoch 173, Loss: 0.49233528219511824\n",
      "Epoch 174, Loss: 0.49205852514110515\n",
      "Epoch 175, Loss: 0.4917773399684819\n",
      "Epoch 176, Loss: 0.49149629710928233\n",
      "Epoch 177, Loss: 0.4912165647150265\n",
      "Epoch 178, Loss: 0.49094181765497674\n",
      "Epoch 179, Loss: 0.49066901222163783\n",
      "Epoch 180, Loss: 0.490393928005003\n",
      "Epoch 181, Loss: 0.4901079113527018\n",
      "Epoch 182, Loss: 0.48981984154671765\n",
      "Epoch 183, Loss: 0.4895234443341763\n",
      "Epoch 184, Loss: 0.48921459258368105\n",
      "Epoch 185, Loss: 0.4889145977883947\n",
      "Epoch 186, Loss: 0.4886175142174919\n",
      "Epoch 187, Loss: 0.4883282295514753\n",
      "Epoch 188, Loss: 0.4880435212302353\n",
      "Epoch 189, Loss: 0.48775699700499114\n",
      "Epoch 190, Loss: 0.48748008613994775\n",
      "Epoch 191, Loss: 0.4872132179171775\n",
      "Epoch 192, Loss: 0.48694738158534456\n",
      "Epoch 193, Loss: 0.48668121634032785\n",
      "Epoch 194, Loss: 0.4864280322301768\n",
      "Epoch 195, Loss: 0.486178532155915\n",
      "Epoch 196, Loss: 0.4859301919172944\n",
      "Epoch 197, Loss: 0.48568442460989747\n",
      "Epoch 198, Loss: 0.48544224711238787\n",
      "Epoch 199, Loss: 0.48519729219516017\n",
      "Epoch 200, Loss: 0.4849575446659644\n",
      "Epoch 201, Loss: 0.4847146197220469\n",
      "Epoch 202, Loss: 0.48447155759133576\n",
      "Epoch 203, Loss: 0.48422783492589483\n",
      "Epoch 204, Loss: 0.48397707745066637\n",
      "Epoch 205, Loss: 0.483737926529631\n",
      "Epoch 206, Loss: 0.4835020782648923\n",
      "Epoch 207, Loss: 0.48327083252052233\n",
      "Epoch 208, Loss: 0.4830508282762034\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.26815296254877996\n",
      "Valence RMSE: 0.25136109118375133\n",
      "Arousal RMSE: 0.28395356043953496\n",
      "Test R^2 score: tensor([0.2343, 0.2016], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1432, 0.1066], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.21796201024365042\n",
      "Num of epochs: 209\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Epoch 121, Loss: 0.5116998508836337\n",
      "Epoch 122, Loss: 0.5111129902664598\n",
      "Epoch 123, Loss: 0.5105405741436312\n",
      "Epoch 124, Loss: 0.5099849010083686\n",
      "Epoch 125, Loss: 0.5094623759868415\n",
      "Epoch 126, Loss: 0.5089652841754122\n",
      "Epoch 127, Loss: 0.5084717799132713\n",
      "Epoch 128, Loss: 0.5079809643002384\n",
      "Epoch 129, Loss: 0.507507731608986\n",
      "Epoch 130, Loss: 0.5070515727009828\n",
      "Epoch 131, Loss: 0.5065962678643066\n",
      "Epoch 132, Loss: 0.5061448812256618\n",
      "Epoch 133, Loss: 0.5057069703510908\n",
      "Epoch 134, Loss: 0.5052756693952567\n",
      "Epoch 135, Loss: 0.5048445017462813\n",
      "Epoch 136, Loss: 0.504425284231979\n",
      "Epoch 137, Loss: 0.504022717877823\n",
      "Epoch 138, Loss: 0.5036312802042249\n",
      "Epoch 139, Loss: 0.5032368731068207\n",
      "Epoch 140, Loss: 0.5028511060044271\n",
      "Epoch 141, Loss: 0.5024804933282331\n",
      "Epoch 142, Loss: 0.5021182727396313\n",
      "Epoch 143, Loss: 0.5017581664990297\n",
      "Epoch 144, Loss: 0.5014050827892457\n",
      "Epoch 145, Loss: 0.501047675902349\n",
      "Epoch 146, Loss: 0.500690966247362\n",
      "Epoch 147, Loss: 0.5003476722262875\n",
      "Epoch 148, Loss: 0.5000068544871652\n",
      "Epoch 149, Loss: 0.49966338867073357\n",
      "Epoch 150, Loss: 0.4993280873087794\n",
      "Epoch 151, Loss: 0.49900257940428383\n",
      "Epoch 152, Loss: 0.4986865554458311\n",
      "Epoch 153, Loss: 0.49836949389755036\n",
      "Epoch 154, Loss: 0.49805375636713634\n",
      "Epoch 155, Loss: 0.49773873165141763\n",
      "Epoch 156, Loss: 0.49742224924400874\n",
      "Epoch 157, Loss: 0.49711425826812494\n",
      "Epoch 158, Loss: 0.4968010973402242\n",
      "Epoch 159, Loss: 0.4964901399315246\n",
      "Epoch 160, Loss: 0.49617675026847124\n",
      "Epoch 161, Loss: 0.4958670390985117\n",
      "Epoch 162, Loss: 0.4955578109301908\n",
      "Epoch 163, Loss: 0.4952510223984696\n",
      "Epoch 164, Loss: 0.49494577484111674\n",
      "Epoch 165, Loss: 0.49464511374066544\n",
      "Epoch 166, Loss: 0.49434383269975535\n",
      "Epoch 167, Loss: 0.49404631909698943\n",
      "Epoch 168, Loss: 0.49374530645143583\n",
      "Epoch 169, Loss: 0.49345064806438055\n",
      "Epoch 170, Loss: 0.49317001490258333\n",
      "Epoch 171, Loss: 0.4928927591216884\n",
      "Epoch 172, Loss: 0.4926138499628376\n",
      "Epoch 173, Loss: 0.49233528219511824\n",
      "Epoch 174, Loss: 0.49205852514110515\n",
      "Epoch 175, Loss: 0.4917773399684819\n",
      "Epoch 176, Loss: 0.49149629710928233\n",
      "Epoch 177, Loss: 0.4912165647150265\n",
      "Epoch 178, Loss: 0.49094181765497674\n",
      "Epoch 179, Loss: 0.49066901222163783\n",
      "Epoch 180, Loss: 0.490393928005003\n",
      "Epoch 181, Loss: 0.4901079113527018\n",
      "Epoch 182, Loss: 0.48981984154671765\n",
      "Epoch 183, Loss: 0.4895234443341763\n",
      "Epoch 184, Loss: 0.48921459258368105\n",
      "Epoch 185, Loss: 0.4889145977883947\n",
      "Epoch 186, Loss: 0.4886175142174919\n",
      "Epoch 187, Loss: 0.4883282295514753\n",
      "Epoch 188, Loss: 0.4880435212302353\n",
      "Epoch 189, Loss: 0.48775699700499114\n",
      "Epoch 190, Loss: 0.48748008613994775\n",
      "Epoch 191, Loss: 0.4872132179171775\n",
      "Epoch 192, Loss: 0.48694738158534456\n",
      "Epoch 193, Loss: 0.48668121634032785\n",
      "Epoch 194, Loss: 0.4864280322301768\n",
      "Epoch 195, Loss: 0.486178532155915\n",
      "Epoch 196, Loss: 0.4859301919172944\n",
      "Epoch 197, Loss: 0.48568442460989747\n",
      "Epoch 198, Loss: 0.48544224711238787\n",
      "Epoch 199, Loss: 0.48519729219516017\n",
      "Epoch 200, Loss: 0.4849575446659644\n",
      "Epoch 201, Loss: 0.4847146197220469\n",
      "Epoch 202, Loss: 0.48447155759133576\n",
      "Epoch 203, Loss: 0.48422783492589483\n",
      "Epoch 204, Loss: 0.48397707745066637\n",
      "Epoch 205, Loss: 0.483737926529631\n",
      "Epoch 206, Loss: 0.4835020782648923\n",
      "Epoch 207, Loss: 0.48327083252052233\n",
      "Epoch 208, Loss: 0.4830508282762034\n",
      "Epoch 209, Loss: 0.48283269895503794\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.26819947358712093\n",
      "Valence RMSE: 0.25134332428605183\n",
      "Arousal RMSE: 0.284057122075216\n",
      "Test R^2 score: tensor([0.2344, 0.2010], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1433, 0.1060], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.21772490185155102\n",
      "Num of epochs: 210\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Epoch 121, Loss: 0.5116998508836337\n",
      "Epoch 122, Loss: 0.5111129902664598\n",
      "Epoch 123, Loss: 0.5105405741436312\n",
      "Epoch 124, Loss: 0.5099849010083686\n",
      "Epoch 125, Loss: 0.5094623759868415\n",
      "Epoch 126, Loss: 0.5089652841754122\n",
      "Epoch 127, Loss: 0.5084717799132713\n",
      "Epoch 128, Loss: 0.5079809643002384\n",
      "Epoch 129, Loss: 0.507507731608986\n",
      "Epoch 130, Loss: 0.5070515727009828\n",
      "Epoch 131, Loss: 0.5065962678643066\n",
      "Epoch 132, Loss: 0.5061448812256618\n",
      "Epoch 133, Loss: 0.5057069703510908\n",
      "Epoch 134, Loss: 0.5052756693952567\n",
      "Epoch 135, Loss: 0.5048445017462813\n",
      "Epoch 136, Loss: 0.504425284231979\n",
      "Epoch 137, Loss: 0.504022717877823\n",
      "Epoch 138, Loss: 0.5036312802042249\n",
      "Epoch 139, Loss: 0.5032368731068207\n",
      "Epoch 140, Loss: 0.5028511060044271\n",
      "Epoch 141, Loss: 0.5024804933282331\n",
      "Epoch 142, Loss: 0.5021182727396313\n",
      "Epoch 143, Loss: 0.5017581664990297\n",
      "Epoch 144, Loss: 0.5014050827892457\n",
      "Epoch 145, Loss: 0.501047675902349\n",
      "Epoch 146, Loss: 0.500690966247362\n",
      "Epoch 147, Loss: 0.5003476722262875\n",
      "Epoch 148, Loss: 0.5000068544871652\n",
      "Epoch 149, Loss: 0.49966338867073357\n",
      "Epoch 150, Loss: 0.4993280873087794\n",
      "Epoch 151, Loss: 0.49900257940428383\n",
      "Epoch 152, Loss: 0.4986865554458311\n",
      "Epoch 153, Loss: 0.49836949389755036\n",
      "Epoch 154, Loss: 0.49805375636713634\n",
      "Epoch 155, Loss: 0.49773873165141763\n",
      "Epoch 156, Loss: 0.49742224924400874\n",
      "Epoch 157, Loss: 0.49711425826812494\n",
      "Epoch 158, Loss: 0.4968010973402242\n",
      "Epoch 159, Loss: 0.4964901399315246\n",
      "Epoch 160, Loss: 0.49617675026847124\n",
      "Epoch 161, Loss: 0.4958670390985117\n",
      "Epoch 162, Loss: 0.4955578109301908\n",
      "Epoch 163, Loss: 0.4952510223984696\n",
      "Epoch 164, Loss: 0.49494577484111674\n",
      "Epoch 165, Loss: 0.49464511374066544\n",
      "Epoch 166, Loss: 0.49434383269975535\n",
      "Epoch 167, Loss: 0.49404631909698943\n",
      "Epoch 168, Loss: 0.49374530645143583\n",
      "Epoch 169, Loss: 0.49345064806438055\n",
      "Epoch 170, Loss: 0.49317001490258333\n",
      "Epoch 171, Loss: 0.4928927591216884\n",
      "Epoch 172, Loss: 0.4926138499628376\n",
      "Epoch 173, Loss: 0.49233528219511824\n",
      "Epoch 174, Loss: 0.49205852514110515\n",
      "Epoch 175, Loss: 0.4917773399684819\n",
      "Epoch 176, Loss: 0.49149629710928233\n",
      "Epoch 177, Loss: 0.4912165647150265\n",
      "Epoch 178, Loss: 0.49094181765497674\n",
      "Epoch 179, Loss: 0.49066901222163783\n",
      "Epoch 180, Loss: 0.490393928005003\n",
      "Epoch 181, Loss: 0.4901079113527018\n",
      "Epoch 182, Loss: 0.48981984154671765\n",
      "Epoch 183, Loss: 0.4895234443341763\n",
      "Epoch 184, Loss: 0.48921459258368105\n",
      "Epoch 185, Loss: 0.4889145977883947\n",
      "Epoch 186, Loss: 0.4886175142174919\n",
      "Epoch 187, Loss: 0.4883282295514753\n",
      "Epoch 188, Loss: 0.4880435212302353\n",
      "Epoch 189, Loss: 0.48775699700499114\n",
      "Epoch 190, Loss: 0.48748008613994775\n",
      "Epoch 191, Loss: 0.4872132179171775\n",
      "Epoch 192, Loss: 0.48694738158534456\n",
      "Epoch 193, Loss: 0.48668121634032785\n",
      "Epoch 194, Loss: 0.4864280322301768\n",
      "Epoch 195, Loss: 0.486178532155915\n",
      "Epoch 196, Loss: 0.4859301919172944\n",
      "Epoch 197, Loss: 0.48568442460989747\n",
      "Epoch 198, Loss: 0.48544224711238787\n",
      "Epoch 199, Loss: 0.48519729219516017\n",
      "Epoch 200, Loss: 0.4849575446659644\n",
      "Epoch 201, Loss: 0.4847146197220469\n",
      "Epoch 202, Loss: 0.48447155759133576\n",
      "Epoch 203, Loss: 0.48422783492589483\n",
      "Epoch 204, Loss: 0.48397707745066637\n",
      "Epoch 205, Loss: 0.483737926529631\n",
      "Epoch 206, Loss: 0.4835020782648923\n",
      "Epoch 207, Loss: 0.48327083252052233\n",
      "Epoch 208, Loss: 0.4830508282762034\n",
      "Epoch 209, Loss: 0.48283269895503794\n",
      "Epoch 210, Loss: 0.48261215534620283\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.268282923469302\n",
      "Valence RMSE: 0.2513275274568045\n",
      "Arousal RMSE: 0.2842286544191488\n",
      "Test R^2 score: tensor([0.2345, 0.2001], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1434, 0.1049], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.2172904130615691\n",
      "Num of epochs: 211\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Epoch 121, Loss: 0.5116998508836337\n",
      "Epoch 122, Loss: 0.5111129902664598\n",
      "Epoch 123, Loss: 0.5105405741436312\n",
      "Epoch 124, Loss: 0.5099849010083686\n",
      "Epoch 125, Loss: 0.5094623759868415\n",
      "Epoch 126, Loss: 0.5089652841754122\n",
      "Epoch 127, Loss: 0.5084717799132713\n",
      "Epoch 128, Loss: 0.5079809643002384\n",
      "Epoch 129, Loss: 0.507507731608986\n",
      "Epoch 130, Loss: 0.5070515727009828\n",
      "Epoch 131, Loss: 0.5065962678643066\n",
      "Epoch 132, Loss: 0.5061448812256618\n",
      "Epoch 133, Loss: 0.5057069703510908\n",
      "Epoch 134, Loss: 0.5052756693952567\n",
      "Epoch 135, Loss: 0.5048445017462813\n",
      "Epoch 136, Loss: 0.504425284231979\n",
      "Epoch 137, Loss: 0.504022717877823\n",
      "Epoch 138, Loss: 0.5036312802042249\n",
      "Epoch 139, Loss: 0.5032368731068207\n",
      "Epoch 140, Loss: 0.5028511060044271\n",
      "Epoch 141, Loss: 0.5024804933282331\n",
      "Epoch 142, Loss: 0.5021182727396313\n",
      "Epoch 143, Loss: 0.5017581664990297\n",
      "Epoch 144, Loss: 0.5014050827892457\n",
      "Epoch 145, Loss: 0.501047675902349\n",
      "Epoch 146, Loss: 0.500690966247362\n",
      "Epoch 147, Loss: 0.5003476722262875\n",
      "Epoch 148, Loss: 0.5000068544871652\n",
      "Epoch 149, Loss: 0.49966338867073357\n",
      "Epoch 150, Loss: 0.4993280873087794\n",
      "Epoch 151, Loss: 0.49900257940428383\n",
      "Epoch 152, Loss: 0.4986865554458311\n",
      "Epoch 153, Loss: 0.49836949389755036\n",
      "Epoch 154, Loss: 0.49805375636713634\n",
      "Epoch 155, Loss: 0.49773873165141763\n",
      "Epoch 156, Loss: 0.49742224924400874\n",
      "Epoch 157, Loss: 0.49711425826812494\n",
      "Epoch 158, Loss: 0.4968010973402242\n",
      "Epoch 159, Loss: 0.4964901399315246\n",
      "Epoch 160, Loss: 0.49617675026847124\n",
      "Epoch 161, Loss: 0.4958670390985117\n",
      "Epoch 162, Loss: 0.4955578109301908\n",
      "Epoch 163, Loss: 0.4952510223984696\n",
      "Epoch 164, Loss: 0.49494577484111674\n",
      "Epoch 165, Loss: 0.49464511374066544\n",
      "Epoch 166, Loss: 0.49434383269975535\n",
      "Epoch 167, Loss: 0.49404631909698943\n",
      "Epoch 168, Loss: 0.49374530645143583\n",
      "Epoch 169, Loss: 0.49345064806438055\n",
      "Epoch 170, Loss: 0.49317001490258333\n",
      "Epoch 171, Loss: 0.4928927591216884\n",
      "Epoch 172, Loss: 0.4926138499628376\n",
      "Epoch 173, Loss: 0.49233528219511824\n",
      "Epoch 174, Loss: 0.49205852514110515\n",
      "Epoch 175, Loss: 0.4917773399684819\n",
      "Epoch 176, Loss: 0.49149629710928233\n",
      "Epoch 177, Loss: 0.4912165647150265\n",
      "Epoch 178, Loss: 0.49094181765497674\n",
      "Epoch 179, Loss: 0.49066901222163783\n",
      "Epoch 180, Loss: 0.490393928005003\n",
      "Epoch 181, Loss: 0.4901079113527018\n",
      "Epoch 182, Loss: 0.48981984154671765\n",
      "Epoch 183, Loss: 0.4895234443341763\n",
      "Epoch 184, Loss: 0.48921459258368105\n",
      "Epoch 185, Loss: 0.4889145977883947\n",
      "Epoch 186, Loss: 0.4886175142174919\n",
      "Epoch 187, Loss: 0.4883282295514753\n",
      "Epoch 188, Loss: 0.4880435212302353\n",
      "Epoch 189, Loss: 0.48775699700499114\n",
      "Epoch 190, Loss: 0.48748008613994775\n",
      "Epoch 191, Loss: 0.4872132179171775\n",
      "Epoch 192, Loss: 0.48694738158534456\n",
      "Epoch 193, Loss: 0.48668121634032785\n",
      "Epoch 194, Loss: 0.4864280322301768\n",
      "Epoch 195, Loss: 0.486178532155915\n",
      "Epoch 196, Loss: 0.4859301919172944\n",
      "Epoch 197, Loss: 0.48568442460989747\n",
      "Epoch 198, Loss: 0.48544224711238787\n",
      "Epoch 199, Loss: 0.48519729219516017\n",
      "Epoch 200, Loss: 0.4849575446659644\n",
      "Epoch 201, Loss: 0.4847146197220469\n",
      "Epoch 202, Loss: 0.48447155759133576\n",
      "Epoch 203, Loss: 0.48422783492589483\n",
      "Epoch 204, Loss: 0.48397707745066637\n",
      "Epoch 205, Loss: 0.483737926529631\n",
      "Epoch 206, Loss: 0.4835020782648923\n",
      "Epoch 207, Loss: 0.48327083252052233\n",
      "Epoch 208, Loss: 0.4830508282762034\n",
      "Epoch 209, Loss: 0.48283269895503794\n",
      "Epoch 210, Loss: 0.48261215534620283\n",
      "Epoch 211, Loss: 0.48240746542225255\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.26844778434131317\n",
      "Valence RMSE: 0.2513436296885068\n",
      "Arousal RMSE: 0.2845255799581597\n",
      "Test R^2 score: tensor([0.2344, 0.1984], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1433, 0.1030], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.2164052804423442\n",
      "Num of epochs: 212\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Epoch 121, Loss: 0.5116998508836337\n",
      "Epoch 122, Loss: 0.5111129902664598\n",
      "Epoch 123, Loss: 0.5105405741436312\n",
      "Epoch 124, Loss: 0.5099849010083686\n",
      "Epoch 125, Loss: 0.5094623759868415\n",
      "Epoch 126, Loss: 0.5089652841754122\n",
      "Epoch 127, Loss: 0.5084717799132713\n",
      "Epoch 128, Loss: 0.5079809643002384\n",
      "Epoch 129, Loss: 0.507507731608986\n",
      "Epoch 130, Loss: 0.5070515727009828\n",
      "Epoch 131, Loss: 0.5065962678643066\n",
      "Epoch 132, Loss: 0.5061448812256618\n",
      "Epoch 133, Loss: 0.5057069703510908\n",
      "Epoch 134, Loss: 0.5052756693952567\n",
      "Epoch 135, Loss: 0.5048445017462813\n",
      "Epoch 136, Loss: 0.504425284231979\n",
      "Epoch 137, Loss: 0.504022717877823\n",
      "Epoch 138, Loss: 0.5036312802042249\n",
      "Epoch 139, Loss: 0.5032368731068207\n",
      "Epoch 140, Loss: 0.5028511060044271\n",
      "Epoch 141, Loss: 0.5024804933282331\n",
      "Epoch 142, Loss: 0.5021182727396313\n",
      "Epoch 143, Loss: 0.5017581664990297\n",
      "Epoch 144, Loss: 0.5014050827892457\n",
      "Epoch 145, Loss: 0.501047675902349\n",
      "Epoch 146, Loss: 0.500690966247362\n",
      "Epoch 147, Loss: 0.5003476722262875\n",
      "Epoch 148, Loss: 0.5000068544871652\n",
      "Epoch 149, Loss: 0.49966338867073357\n",
      "Epoch 150, Loss: 0.4993280873087794\n",
      "Epoch 151, Loss: 0.49900257940428383\n",
      "Epoch 152, Loss: 0.4986865554458311\n",
      "Epoch 153, Loss: 0.49836949389755036\n",
      "Epoch 154, Loss: 0.49805375636713634\n",
      "Epoch 155, Loss: 0.49773873165141763\n",
      "Epoch 156, Loss: 0.49742224924400874\n",
      "Epoch 157, Loss: 0.49711425826812494\n",
      "Epoch 158, Loss: 0.4968010973402242\n",
      "Epoch 159, Loss: 0.4964901399315246\n",
      "Epoch 160, Loss: 0.49617675026847124\n",
      "Epoch 161, Loss: 0.4958670390985117\n",
      "Epoch 162, Loss: 0.4955578109301908\n",
      "Epoch 163, Loss: 0.4952510223984696\n",
      "Epoch 164, Loss: 0.49494577484111674\n",
      "Epoch 165, Loss: 0.49464511374066544\n",
      "Epoch 166, Loss: 0.49434383269975535\n",
      "Epoch 167, Loss: 0.49404631909698943\n",
      "Epoch 168, Loss: 0.49374530645143583\n",
      "Epoch 169, Loss: 0.49345064806438055\n",
      "Epoch 170, Loss: 0.49317001490258333\n",
      "Epoch 171, Loss: 0.4928927591216884\n",
      "Epoch 172, Loss: 0.4926138499628376\n",
      "Epoch 173, Loss: 0.49233528219511824\n",
      "Epoch 174, Loss: 0.49205852514110515\n",
      "Epoch 175, Loss: 0.4917773399684819\n",
      "Epoch 176, Loss: 0.49149629710928233\n",
      "Epoch 177, Loss: 0.4912165647150265\n",
      "Epoch 178, Loss: 0.49094181765497674\n",
      "Epoch 179, Loss: 0.49066901222163783\n",
      "Epoch 180, Loss: 0.490393928005003\n",
      "Epoch 181, Loss: 0.4901079113527018\n",
      "Epoch 182, Loss: 0.48981984154671765\n",
      "Epoch 183, Loss: 0.4895234443341763\n",
      "Epoch 184, Loss: 0.48921459258368105\n",
      "Epoch 185, Loss: 0.4889145977883947\n",
      "Epoch 186, Loss: 0.4886175142174919\n",
      "Epoch 187, Loss: 0.4883282295514753\n",
      "Epoch 188, Loss: 0.4880435212302353\n",
      "Epoch 189, Loss: 0.48775699700499114\n",
      "Epoch 190, Loss: 0.48748008613994775\n",
      "Epoch 191, Loss: 0.4872132179171775\n",
      "Epoch 192, Loss: 0.48694738158534456\n",
      "Epoch 193, Loss: 0.48668121634032785\n",
      "Epoch 194, Loss: 0.4864280322301768\n",
      "Epoch 195, Loss: 0.486178532155915\n",
      "Epoch 196, Loss: 0.4859301919172944\n",
      "Epoch 197, Loss: 0.48568442460989747\n",
      "Epoch 198, Loss: 0.48544224711238787\n",
      "Epoch 199, Loss: 0.48519729219516017\n",
      "Epoch 200, Loss: 0.4849575446659644\n",
      "Epoch 201, Loss: 0.4847146197220469\n",
      "Epoch 202, Loss: 0.48447155759133576\n",
      "Epoch 203, Loss: 0.48422783492589483\n",
      "Epoch 204, Loss: 0.48397707745066637\n",
      "Epoch 205, Loss: 0.483737926529631\n",
      "Epoch 206, Loss: 0.4835020782648923\n",
      "Epoch 207, Loss: 0.48327083252052233\n",
      "Epoch 208, Loss: 0.4830508282762034\n",
      "Epoch 209, Loss: 0.48283269895503794\n",
      "Epoch 210, Loss: 0.48261215534620283\n",
      "Epoch 211, Loss: 0.48240746542225255\n",
      "Epoch 212, Loss: 0.4822187420754367\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.268621050533082\n",
      "Valence RMSE: 0.2513905217231951\n",
      "Arousal RMSE: 0.28481106573785536\n",
      "Test R^2 score: tensor([0.2341, 0.1968], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1430, 0.1012], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.21545773521496642\n",
      "Num of epochs: 213\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Epoch 121, Loss: 0.5116998508836337\n",
      "Epoch 122, Loss: 0.5111129902664598\n",
      "Epoch 123, Loss: 0.5105405741436312\n",
      "Epoch 124, Loss: 0.5099849010083686\n",
      "Epoch 125, Loss: 0.5094623759868415\n",
      "Epoch 126, Loss: 0.5089652841754122\n",
      "Epoch 127, Loss: 0.5084717799132713\n",
      "Epoch 128, Loss: 0.5079809643002384\n",
      "Epoch 129, Loss: 0.507507731608986\n",
      "Epoch 130, Loss: 0.5070515727009828\n",
      "Epoch 131, Loss: 0.5065962678643066\n",
      "Epoch 132, Loss: 0.5061448812256618\n",
      "Epoch 133, Loss: 0.5057069703510908\n",
      "Epoch 134, Loss: 0.5052756693952567\n",
      "Epoch 135, Loss: 0.5048445017462813\n",
      "Epoch 136, Loss: 0.504425284231979\n",
      "Epoch 137, Loss: 0.504022717877823\n",
      "Epoch 138, Loss: 0.5036312802042249\n",
      "Epoch 139, Loss: 0.5032368731068207\n",
      "Epoch 140, Loss: 0.5028511060044271\n",
      "Epoch 141, Loss: 0.5024804933282331\n",
      "Epoch 142, Loss: 0.5021182727396313\n",
      "Epoch 143, Loss: 0.5017581664990297\n",
      "Epoch 144, Loss: 0.5014050827892457\n",
      "Epoch 145, Loss: 0.501047675902349\n",
      "Epoch 146, Loss: 0.500690966247362\n",
      "Epoch 147, Loss: 0.5003476722262875\n",
      "Epoch 148, Loss: 0.5000068544871652\n",
      "Epoch 149, Loss: 0.49966338867073357\n",
      "Epoch 150, Loss: 0.4993280873087794\n",
      "Epoch 151, Loss: 0.49900257940428383\n",
      "Epoch 152, Loss: 0.4986865554458311\n",
      "Epoch 153, Loss: 0.49836949389755036\n",
      "Epoch 154, Loss: 0.49805375636713634\n",
      "Epoch 155, Loss: 0.49773873165141763\n",
      "Epoch 156, Loss: 0.49742224924400874\n",
      "Epoch 157, Loss: 0.49711425826812494\n",
      "Epoch 158, Loss: 0.4968010973402242\n",
      "Epoch 159, Loss: 0.4964901399315246\n",
      "Epoch 160, Loss: 0.49617675026847124\n",
      "Epoch 161, Loss: 0.4958670390985117\n",
      "Epoch 162, Loss: 0.4955578109301908\n",
      "Epoch 163, Loss: 0.4952510223984696\n",
      "Epoch 164, Loss: 0.49494577484111674\n",
      "Epoch 165, Loss: 0.49464511374066544\n",
      "Epoch 166, Loss: 0.49434383269975535\n",
      "Epoch 167, Loss: 0.49404631909698943\n",
      "Epoch 168, Loss: 0.49374530645143583\n",
      "Epoch 169, Loss: 0.49345064806438055\n",
      "Epoch 170, Loss: 0.49317001490258333\n",
      "Epoch 171, Loss: 0.4928927591216884\n",
      "Epoch 172, Loss: 0.4926138499628376\n",
      "Epoch 173, Loss: 0.49233528219511824\n",
      "Epoch 174, Loss: 0.49205852514110515\n",
      "Epoch 175, Loss: 0.4917773399684819\n",
      "Epoch 176, Loss: 0.49149629710928233\n",
      "Epoch 177, Loss: 0.4912165647150265\n",
      "Epoch 178, Loss: 0.49094181765497674\n",
      "Epoch 179, Loss: 0.49066901222163783\n",
      "Epoch 180, Loss: 0.490393928005003\n",
      "Epoch 181, Loss: 0.4901079113527018\n",
      "Epoch 182, Loss: 0.48981984154671765\n",
      "Epoch 183, Loss: 0.4895234443341763\n",
      "Epoch 184, Loss: 0.48921459258368105\n",
      "Epoch 185, Loss: 0.4889145977883947\n",
      "Epoch 186, Loss: 0.4886175142174919\n",
      "Epoch 187, Loss: 0.4883282295514753\n",
      "Epoch 188, Loss: 0.4880435212302353\n",
      "Epoch 189, Loss: 0.48775699700499114\n",
      "Epoch 190, Loss: 0.48748008613994775\n",
      "Epoch 191, Loss: 0.4872132179171775\n",
      "Epoch 192, Loss: 0.48694738158534456\n",
      "Epoch 193, Loss: 0.48668121634032785\n",
      "Epoch 194, Loss: 0.4864280322301768\n",
      "Epoch 195, Loss: 0.486178532155915\n",
      "Epoch 196, Loss: 0.4859301919172944\n",
      "Epoch 197, Loss: 0.48568442460989747\n",
      "Epoch 198, Loss: 0.48544224711238787\n",
      "Epoch 199, Loss: 0.48519729219516017\n",
      "Epoch 200, Loss: 0.4849575446659644\n",
      "Epoch 201, Loss: 0.4847146197220469\n",
      "Epoch 202, Loss: 0.48447155759133576\n",
      "Epoch 203, Loss: 0.48422783492589483\n",
      "Epoch 204, Loss: 0.48397707745066637\n",
      "Epoch 205, Loss: 0.483737926529631\n",
      "Epoch 206, Loss: 0.4835020782648923\n",
      "Epoch 207, Loss: 0.48327083252052233\n",
      "Epoch 208, Loss: 0.4830508282762034\n",
      "Epoch 209, Loss: 0.48283269895503794\n",
      "Epoch 210, Loss: 0.48261215534620283\n",
      "Epoch 211, Loss: 0.48240746542225255\n",
      "Epoch 212, Loss: 0.4822187420754367\n",
      "Epoch 213, Loss: 0.48202997575341067\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.26872366951872395\n",
      "Valence RMSE: 0.2513923195629032\n",
      "Arousal RMSE: 0.2850030224120437\n",
      "Test R^2 score: tensor([0.2341, 0.1957], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1430, 0.1000], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.21491073690627277\n",
      "Num of epochs: 214\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Epoch 121, Loss: 0.5116998508836337\n",
      "Epoch 122, Loss: 0.5111129902664598\n",
      "Epoch 123, Loss: 0.5105405741436312\n",
      "Epoch 124, Loss: 0.5099849010083686\n",
      "Epoch 125, Loss: 0.5094623759868415\n",
      "Epoch 126, Loss: 0.5089652841754122\n",
      "Epoch 127, Loss: 0.5084717799132713\n",
      "Epoch 128, Loss: 0.5079809643002384\n",
      "Epoch 129, Loss: 0.507507731608986\n",
      "Epoch 130, Loss: 0.5070515727009828\n",
      "Epoch 131, Loss: 0.5065962678643066\n",
      "Epoch 132, Loss: 0.5061448812256618\n",
      "Epoch 133, Loss: 0.5057069703510908\n",
      "Epoch 134, Loss: 0.5052756693952567\n",
      "Epoch 135, Loss: 0.5048445017462813\n",
      "Epoch 136, Loss: 0.504425284231979\n",
      "Epoch 137, Loss: 0.504022717877823\n",
      "Epoch 138, Loss: 0.5036312802042249\n",
      "Epoch 139, Loss: 0.5032368731068207\n",
      "Epoch 140, Loss: 0.5028511060044271\n",
      "Epoch 141, Loss: 0.5024804933282331\n",
      "Epoch 142, Loss: 0.5021182727396313\n",
      "Epoch 143, Loss: 0.5017581664990297\n",
      "Epoch 144, Loss: 0.5014050827892457\n",
      "Epoch 145, Loss: 0.501047675902349\n",
      "Epoch 146, Loss: 0.500690966247362\n",
      "Epoch 147, Loss: 0.5003476722262875\n",
      "Epoch 148, Loss: 0.5000068544871652\n",
      "Epoch 149, Loss: 0.49966338867073357\n",
      "Epoch 150, Loss: 0.4993280873087794\n",
      "Epoch 151, Loss: 0.49900257940428383\n",
      "Epoch 152, Loss: 0.4986865554458311\n",
      "Epoch 153, Loss: 0.49836949389755036\n",
      "Epoch 154, Loss: 0.49805375636713634\n",
      "Epoch 155, Loss: 0.49773873165141763\n",
      "Epoch 156, Loss: 0.49742224924400874\n",
      "Epoch 157, Loss: 0.49711425826812494\n",
      "Epoch 158, Loss: 0.4968010973402242\n",
      "Epoch 159, Loss: 0.4964901399315246\n",
      "Epoch 160, Loss: 0.49617675026847124\n",
      "Epoch 161, Loss: 0.4958670390985117\n",
      "Epoch 162, Loss: 0.4955578109301908\n",
      "Epoch 163, Loss: 0.4952510223984696\n",
      "Epoch 164, Loss: 0.49494577484111674\n",
      "Epoch 165, Loss: 0.49464511374066544\n",
      "Epoch 166, Loss: 0.49434383269975535\n",
      "Epoch 167, Loss: 0.49404631909698943\n",
      "Epoch 168, Loss: 0.49374530645143583\n",
      "Epoch 169, Loss: 0.49345064806438055\n",
      "Epoch 170, Loss: 0.49317001490258333\n",
      "Epoch 171, Loss: 0.4928927591216884\n",
      "Epoch 172, Loss: 0.4926138499628376\n",
      "Epoch 173, Loss: 0.49233528219511824\n",
      "Epoch 174, Loss: 0.49205852514110515\n",
      "Epoch 175, Loss: 0.4917773399684819\n",
      "Epoch 176, Loss: 0.49149629710928233\n",
      "Epoch 177, Loss: 0.4912165647150265\n",
      "Epoch 178, Loss: 0.49094181765497674\n",
      "Epoch 179, Loss: 0.49066901222163783\n",
      "Epoch 180, Loss: 0.490393928005003\n",
      "Epoch 181, Loss: 0.4901079113527018\n",
      "Epoch 182, Loss: 0.48981984154671765\n",
      "Epoch 183, Loss: 0.4895234443341763\n",
      "Epoch 184, Loss: 0.48921459258368105\n",
      "Epoch 185, Loss: 0.4889145977883947\n",
      "Epoch 186, Loss: 0.4886175142174919\n",
      "Epoch 187, Loss: 0.4883282295514753\n",
      "Epoch 188, Loss: 0.4880435212302353\n",
      "Epoch 189, Loss: 0.48775699700499114\n",
      "Epoch 190, Loss: 0.48748008613994775\n",
      "Epoch 191, Loss: 0.4872132179171775\n",
      "Epoch 192, Loss: 0.48694738158534456\n",
      "Epoch 193, Loss: 0.48668121634032785\n",
      "Epoch 194, Loss: 0.4864280322301768\n",
      "Epoch 195, Loss: 0.486178532155915\n",
      "Epoch 196, Loss: 0.4859301919172944\n",
      "Epoch 197, Loss: 0.48568442460989747\n",
      "Epoch 198, Loss: 0.48544224711238787\n",
      "Epoch 199, Loss: 0.48519729219516017\n",
      "Epoch 200, Loss: 0.4849575446659644\n",
      "Epoch 201, Loss: 0.4847146197220469\n",
      "Epoch 202, Loss: 0.48447155759133576\n",
      "Epoch 203, Loss: 0.48422783492589483\n",
      "Epoch 204, Loss: 0.48397707745066637\n",
      "Epoch 205, Loss: 0.483737926529631\n",
      "Epoch 206, Loss: 0.4835020782648923\n",
      "Epoch 207, Loss: 0.48327083252052233\n",
      "Epoch 208, Loss: 0.4830508282762034\n",
      "Epoch 209, Loss: 0.48283269895503794\n",
      "Epoch 210, Loss: 0.48261215534620283\n",
      "Epoch 211, Loss: 0.48240746542225255\n",
      "Epoch 212, Loss: 0.4822187420754367\n",
      "Epoch 213, Loss: 0.48202997575341067\n",
      "Epoch 214, Loss: 0.4818441197783379\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2688796243513148\n",
      "Valence RMSE: 0.2514416907343949\n",
      "Arousal RMSE: 0.28525353800301445\n",
      "Test R^2 score: tensor([0.2338, 0.1943], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1426, 0.0984], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.21405303891763655\n",
      "Num of epochs: 215\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Epoch 121, Loss: 0.5116998508836337\n",
      "Epoch 122, Loss: 0.5111129902664598\n",
      "Epoch 123, Loss: 0.5105405741436312\n",
      "Epoch 124, Loss: 0.5099849010083686\n",
      "Epoch 125, Loss: 0.5094623759868415\n",
      "Epoch 126, Loss: 0.5089652841754122\n",
      "Epoch 127, Loss: 0.5084717799132713\n",
      "Epoch 128, Loss: 0.5079809643002384\n",
      "Epoch 129, Loss: 0.507507731608986\n",
      "Epoch 130, Loss: 0.5070515727009828\n",
      "Epoch 131, Loss: 0.5065962678643066\n",
      "Epoch 132, Loss: 0.5061448812256618\n",
      "Epoch 133, Loss: 0.5057069703510908\n",
      "Epoch 134, Loss: 0.5052756693952567\n",
      "Epoch 135, Loss: 0.5048445017462813\n",
      "Epoch 136, Loss: 0.504425284231979\n",
      "Epoch 137, Loss: 0.504022717877823\n",
      "Epoch 138, Loss: 0.5036312802042249\n",
      "Epoch 139, Loss: 0.5032368731068207\n",
      "Epoch 140, Loss: 0.5028511060044271\n",
      "Epoch 141, Loss: 0.5024804933282331\n",
      "Epoch 142, Loss: 0.5021182727396313\n",
      "Epoch 143, Loss: 0.5017581664990297\n",
      "Epoch 144, Loss: 0.5014050827892457\n",
      "Epoch 145, Loss: 0.501047675902349\n",
      "Epoch 146, Loss: 0.500690966247362\n",
      "Epoch 147, Loss: 0.5003476722262875\n",
      "Epoch 148, Loss: 0.5000068544871652\n",
      "Epoch 149, Loss: 0.49966338867073357\n",
      "Epoch 150, Loss: 0.4993280873087794\n",
      "Epoch 151, Loss: 0.49900257940428383\n",
      "Epoch 152, Loss: 0.4986865554458311\n",
      "Epoch 153, Loss: 0.49836949389755036\n",
      "Epoch 154, Loss: 0.49805375636713634\n",
      "Epoch 155, Loss: 0.49773873165141763\n",
      "Epoch 156, Loss: 0.49742224924400874\n",
      "Epoch 157, Loss: 0.49711425826812494\n",
      "Epoch 158, Loss: 0.4968010973402242\n",
      "Epoch 159, Loss: 0.4964901399315246\n",
      "Epoch 160, Loss: 0.49617675026847124\n",
      "Epoch 161, Loss: 0.4958670390985117\n",
      "Epoch 162, Loss: 0.4955578109301908\n",
      "Epoch 163, Loss: 0.4952510223984696\n",
      "Epoch 164, Loss: 0.49494577484111674\n",
      "Epoch 165, Loss: 0.49464511374066544\n",
      "Epoch 166, Loss: 0.49434383269975535\n",
      "Epoch 167, Loss: 0.49404631909698943\n",
      "Epoch 168, Loss: 0.49374530645143583\n",
      "Epoch 169, Loss: 0.49345064806438055\n",
      "Epoch 170, Loss: 0.49317001490258333\n",
      "Epoch 171, Loss: 0.4928927591216884\n",
      "Epoch 172, Loss: 0.4926138499628376\n",
      "Epoch 173, Loss: 0.49233528219511824\n",
      "Epoch 174, Loss: 0.49205852514110515\n",
      "Epoch 175, Loss: 0.4917773399684819\n",
      "Epoch 176, Loss: 0.49149629710928233\n",
      "Epoch 177, Loss: 0.4912165647150265\n",
      "Epoch 178, Loss: 0.49094181765497674\n",
      "Epoch 179, Loss: 0.49066901222163783\n",
      "Epoch 180, Loss: 0.490393928005003\n",
      "Epoch 181, Loss: 0.4901079113527018\n",
      "Epoch 182, Loss: 0.48981984154671765\n",
      "Epoch 183, Loss: 0.4895234443341763\n",
      "Epoch 184, Loss: 0.48921459258368105\n",
      "Epoch 185, Loss: 0.4889145977883947\n",
      "Epoch 186, Loss: 0.4886175142174919\n",
      "Epoch 187, Loss: 0.4883282295514753\n",
      "Epoch 188, Loss: 0.4880435212302353\n",
      "Epoch 189, Loss: 0.48775699700499114\n",
      "Epoch 190, Loss: 0.48748008613994775\n",
      "Epoch 191, Loss: 0.4872132179171775\n",
      "Epoch 192, Loss: 0.48694738158534456\n",
      "Epoch 193, Loss: 0.48668121634032785\n",
      "Epoch 194, Loss: 0.4864280322301768\n",
      "Epoch 195, Loss: 0.486178532155915\n",
      "Epoch 196, Loss: 0.4859301919172944\n",
      "Epoch 197, Loss: 0.48568442460989747\n",
      "Epoch 198, Loss: 0.48544224711238787\n",
      "Epoch 199, Loss: 0.48519729219516017\n",
      "Epoch 200, Loss: 0.4849575446659644\n",
      "Epoch 201, Loss: 0.4847146197220469\n",
      "Epoch 202, Loss: 0.48447155759133576\n",
      "Epoch 203, Loss: 0.48422783492589483\n",
      "Epoch 204, Loss: 0.48397707745066637\n",
      "Epoch 205, Loss: 0.483737926529631\n",
      "Epoch 206, Loss: 0.4835020782648923\n",
      "Epoch 207, Loss: 0.48327083252052233\n",
      "Epoch 208, Loss: 0.4830508282762034\n",
      "Epoch 209, Loss: 0.48283269895503794\n",
      "Epoch 210, Loss: 0.48261215534620283\n",
      "Epoch 211, Loss: 0.48240746542225255\n",
      "Epoch 212, Loss: 0.4822187420754367\n",
      "Epoch 213, Loss: 0.48202997575341067\n",
      "Epoch 214, Loss: 0.4818441197783379\n",
      "Epoch 215, Loss: 0.4816502256898402\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2688966261499624\n",
      "Valence RMSE: 0.25140774312867004\n",
      "Arousal RMSE: 0.2853155057206214\n",
      "Test R^2 score: tensor([0.2340, 0.1940], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1429, 0.0981], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.21398143162120425\n",
      "Num of epochs: 216\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Epoch 121, Loss: 0.5116998508836337\n",
      "Epoch 122, Loss: 0.5111129902664598\n",
      "Epoch 123, Loss: 0.5105405741436312\n",
      "Epoch 124, Loss: 0.5099849010083686\n",
      "Epoch 125, Loss: 0.5094623759868415\n",
      "Epoch 126, Loss: 0.5089652841754122\n",
      "Epoch 127, Loss: 0.5084717799132713\n",
      "Epoch 128, Loss: 0.5079809643002384\n",
      "Epoch 129, Loss: 0.507507731608986\n",
      "Epoch 130, Loss: 0.5070515727009828\n",
      "Epoch 131, Loss: 0.5065962678643066\n",
      "Epoch 132, Loss: 0.5061448812256618\n",
      "Epoch 133, Loss: 0.5057069703510908\n",
      "Epoch 134, Loss: 0.5052756693952567\n",
      "Epoch 135, Loss: 0.5048445017462813\n",
      "Epoch 136, Loss: 0.504425284231979\n",
      "Epoch 137, Loss: 0.504022717877823\n",
      "Epoch 138, Loss: 0.5036312802042249\n",
      "Epoch 139, Loss: 0.5032368731068207\n",
      "Epoch 140, Loss: 0.5028511060044271\n",
      "Epoch 141, Loss: 0.5024804933282331\n",
      "Epoch 142, Loss: 0.5021182727396313\n",
      "Epoch 143, Loss: 0.5017581664990297\n",
      "Epoch 144, Loss: 0.5014050827892457\n",
      "Epoch 145, Loss: 0.501047675902349\n",
      "Epoch 146, Loss: 0.500690966247362\n",
      "Epoch 147, Loss: 0.5003476722262875\n",
      "Epoch 148, Loss: 0.5000068544871652\n",
      "Epoch 149, Loss: 0.49966338867073357\n",
      "Epoch 150, Loss: 0.4993280873087794\n",
      "Epoch 151, Loss: 0.49900257940428383\n",
      "Epoch 152, Loss: 0.4986865554458311\n",
      "Epoch 153, Loss: 0.49836949389755036\n",
      "Epoch 154, Loss: 0.49805375636713634\n",
      "Epoch 155, Loss: 0.49773873165141763\n",
      "Epoch 156, Loss: 0.49742224924400874\n",
      "Epoch 157, Loss: 0.49711425826812494\n",
      "Epoch 158, Loss: 0.4968010973402242\n",
      "Epoch 159, Loss: 0.4964901399315246\n",
      "Epoch 160, Loss: 0.49617675026847124\n",
      "Epoch 161, Loss: 0.4958670390985117\n",
      "Epoch 162, Loss: 0.4955578109301908\n",
      "Epoch 163, Loss: 0.4952510223984696\n",
      "Epoch 164, Loss: 0.49494577484111674\n",
      "Epoch 165, Loss: 0.49464511374066544\n",
      "Epoch 166, Loss: 0.49434383269975535\n",
      "Epoch 167, Loss: 0.49404631909698943\n",
      "Epoch 168, Loss: 0.49374530645143583\n",
      "Epoch 169, Loss: 0.49345064806438055\n",
      "Epoch 170, Loss: 0.49317001490258333\n",
      "Epoch 171, Loss: 0.4928927591216884\n",
      "Epoch 172, Loss: 0.4926138499628376\n",
      "Epoch 173, Loss: 0.49233528219511824\n",
      "Epoch 174, Loss: 0.49205852514110515\n",
      "Epoch 175, Loss: 0.4917773399684819\n",
      "Epoch 176, Loss: 0.49149629710928233\n",
      "Epoch 177, Loss: 0.4912165647150265\n",
      "Epoch 178, Loss: 0.49094181765497674\n",
      "Epoch 179, Loss: 0.49066901222163783\n",
      "Epoch 180, Loss: 0.490393928005003\n",
      "Epoch 181, Loss: 0.4901079113527018\n",
      "Epoch 182, Loss: 0.48981984154671765\n",
      "Epoch 183, Loss: 0.4895234443341763\n",
      "Epoch 184, Loss: 0.48921459258368105\n",
      "Epoch 185, Loss: 0.4889145977883947\n",
      "Epoch 186, Loss: 0.4886175142174919\n",
      "Epoch 187, Loss: 0.4883282295514753\n",
      "Epoch 188, Loss: 0.4880435212302353\n",
      "Epoch 189, Loss: 0.48775699700499114\n",
      "Epoch 190, Loss: 0.48748008613994775\n",
      "Epoch 191, Loss: 0.4872132179171775\n",
      "Epoch 192, Loss: 0.48694738158534456\n",
      "Epoch 193, Loss: 0.48668121634032785\n",
      "Epoch 194, Loss: 0.4864280322301768\n",
      "Epoch 195, Loss: 0.486178532155915\n",
      "Epoch 196, Loss: 0.4859301919172944\n",
      "Epoch 197, Loss: 0.48568442460989747\n",
      "Epoch 198, Loss: 0.48544224711238787\n",
      "Epoch 199, Loss: 0.48519729219516017\n",
      "Epoch 200, Loss: 0.4849575446659644\n",
      "Epoch 201, Loss: 0.4847146197220469\n",
      "Epoch 202, Loss: 0.48447155759133576\n",
      "Epoch 203, Loss: 0.48422783492589483\n",
      "Epoch 204, Loss: 0.48397707745066637\n",
      "Epoch 205, Loss: 0.483737926529631\n",
      "Epoch 206, Loss: 0.4835020782648923\n",
      "Epoch 207, Loss: 0.48327083252052233\n",
      "Epoch 208, Loss: 0.4830508282762034\n",
      "Epoch 209, Loss: 0.48283269895503794\n",
      "Epoch 210, Loss: 0.48261215534620283\n",
      "Epoch 211, Loss: 0.48240746542225255\n",
      "Epoch 212, Loss: 0.4822187420754367\n",
      "Epoch 213, Loss: 0.48202997575341067\n",
      "Epoch 214, Loss: 0.4818441197783379\n",
      "Epoch 215, Loss: 0.4816502256898402\n",
      "Epoch 216, Loss: 0.4814589616492725\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2689244005886004\n",
      "Valence RMSE: 0.2513499380339531\n",
      "Arousal RMSE: 0.2854187714818421\n",
      "Test R^2 score: tensor([0.2344, 0.1934], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1433, 0.0974], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.21386574328952407\n",
      "Num of epochs: 217\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Epoch 121, Loss: 0.5116998508836337\n",
      "Epoch 122, Loss: 0.5111129902664598\n",
      "Epoch 123, Loss: 0.5105405741436312\n",
      "Epoch 124, Loss: 0.5099849010083686\n",
      "Epoch 125, Loss: 0.5094623759868415\n",
      "Epoch 126, Loss: 0.5089652841754122\n",
      "Epoch 127, Loss: 0.5084717799132713\n",
      "Epoch 128, Loss: 0.5079809643002384\n",
      "Epoch 129, Loss: 0.507507731608986\n",
      "Epoch 130, Loss: 0.5070515727009828\n",
      "Epoch 131, Loss: 0.5065962678643066\n",
      "Epoch 132, Loss: 0.5061448812256618\n",
      "Epoch 133, Loss: 0.5057069703510908\n",
      "Epoch 134, Loss: 0.5052756693952567\n",
      "Epoch 135, Loss: 0.5048445017462813\n",
      "Epoch 136, Loss: 0.504425284231979\n",
      "Epoch 137, Loss: 0.504022717877823\n",
      "Epoch 138, Loss: 0.5036312802042249\n",
      "Epoch 139, Loss: 0.5032368731068207\n",
      "Epoch 140, Loss: 0.5028511060044271\n",
      "Epoch 141, Loss: 0.5024804933282331\n",
      "Epoch 142, Loss: 0.5021182727396313\n",
      "Epoch 143, Loss: 0.5017581664990297\n",
      "Epoch 144, Loss: 0.5014050827892457\n",
      "Epoch 145, Loss: 0.501047675902349\n",
      "Epoch 146, Loss: 0.500690966247362\n",
      "Epoch 147, Loss: 0.5003476722262875\n",
      "Epoch 148, Loss: 0.5000068544871652\n",
      "Epoch 149, Loss: 0.49966338867073357\n",
      "Epoch 150, Loss: 0.4993280873087794\n",
      "Epoch 151, Loss: 0.49900257940428383\n",
      "Epoch 152, Loss: 0.4986865554458311\n",
      "Epoch 153, Loss: 0.49836949389755036\n",
      "Epoch 154, Loss: 0.49805375636713634\n",
      "Epoch 155, Loss: 0.49773873165141763\n",
      "Epoch 156, Loss: 0.49742224924400874\n",
      "Epoch 157, Loss: 0.49711425826812494\n",
      "Epoch 158, Loss: 0.4968010973402242\n",
      "Epoch 159, Loss: 0.4964901399315246\n",
      "Epoch 160, Loss: 0.49617675026847124\n",
      "Epoch 161, Loss: 0.4958670390985117\n",
      "Epoch 162, Loss: 0.4955578109301908\n",
      "Epoch 163, Loss: 0.4952510223984696\n",
      "Epoch 164, Loss: 0.49494577484111674\n",
      "Epoch 165, Loss: 0.49464511374066544\n",
      "Epoch 166, Loss: 0.49434383269975535\n",
      "Epoch 167, Loss: 0.49404631909698943\n",
      "Epoch 168, Loss: 0.49374530645143583\n",
      "Epoch 169, Loss: 0.49345064806438055\n",
      "Epoch 170, Loss: 0.49317001490258333\n",
      "Epoch 171, Loss: 0.4928927591216884\n",
      "Epoch 172, Loss: 0.4926138499628376\n",
      "Epoch 173, Loss: 0.49233528219511824\n",
      "Epoch 174, Loss: 0.49205852514110515\n",
      "Epoch 175, Loss: 0.4917773399684819\n",
      "Epoch 176, Loss: 0.49149629710928233\n",
      "Epoch 177, Loss: 0.4912165647150265\n",
      "Epoch 178, Loss: 0.49094181765497674\n",
      "Epoch 179, Loss: 0.49066901222163783\n",
      "Epoch 180, Loss: 0.490393928005003\n",
      "Epoch 181, Loss: 0.4901079113527018\n",
      "Epoch 182, Loss: 0.48981984154671765\n",
      "Epoch 183, Loss: 0.4895234443341763\n",
      "Epoch 184, Loss: 0.48921459258368105\n",
      "Epoch 185, Loss: 0.4889145977883947\n",
      "Epoch 186, Loss: 0.4886175142174919\n",
      "Epoch 187, Loss: 0.4883282295514753\n",
      "Epoch 188, Loss: 0.4880435212302353\n",
      "Epoch 189, Loss: 0.48775699700499114\n",
      "Epoch 190, Loss: 0.48748008613994775\n",
      "Epoch 191, Loss: 0.4872132179171775\n",
      "Epoch 192, Loss: 0.48694738158534456\n",
      "Epoch 193, Loss: 0.48668121634032785\n",
      "Epoch 194, Loss: 0.4864280322301768\n",
      "Epoch 195, Loss: 0.486178532155915\n",
      "Epoch 196, Loss: 0.4859301919172944\n",
      "Epoch 197, Loss: 0.48568442460989747\n",
      "Epoch 198, Loss: 0.48544224711238787\n",
      "Epoch 199, Loss: 0.48519729219516017\n",
      "Epoch 200, Loss: 0.4849575446659644\n",
      "Epoch 201, Loss: 0.4847146197220469\n",
      "Epoch 202, Loss: 0.48447155759133576\n",
      "Epoch 203, Loss: 0.48422783492589483\n",
      "Epoch 204, Loss: 0.48397707745066637\n",
      "Epoch 205, Loss: 0.483737926529631\n",
      "Epoch 206, Loss: 0.4835020782648923\n",
      "Epoch 207, Loss: 0.48327083252052233\n",
      "Epoch 208, Loss: 0.4830508282762034\n",
      "Epoch 209, Loss: 0.48283269895503794\n",
      "Epoch 210, Loss: 0.48261215534620283\n",
      "Epoch 211, Loss: 0.48240746542225255\n",
      "Epoch 212, Loss: 0.4822187420754367\n",
      "Epoch 213, Loss: 0.48202997575341067\n",
      "Epoch 214, Loss: 0.4818441197783379\n",
      "Epoch 215, Loss: 0.4816502256898402\n",
      "Epoch 216, Loss: 0.4814589616492725\n",
      "Epoch 217, Loss: 0.48127338055379854\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2693938759218419\n",
      "Valence RMSE: 0.2516354453712253\n",
      "Arousal RMSE: 0.2860519592682605\n",
      "Test R^2 score: tensor([0.2326, 0.1898], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1413, 0.0934], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.21120410992235522\n",
      "Num of epochs: 218\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Epoch 121, Loss: 0.5116998508836337\n",
      "Epoch 122, Loss: 0.5111129902664598\n",
      "Epoch 123, Loss: 0.5105405741436312\n",
      "Epoch 124, Loss: 0.5099849010083686\n",
      "Epoch 125, Loss: 0.5094623759868415\n",
      "Epoch 126, Loss: 0.5089652841754122\n",
      "Epoch 127, Loss: 0.5084717799132713\n",
      "Epoch 128, Loss: 0.5079809643002384\n",
      "Epoch 129, Loss: 0.507507731608986\n",
      "Epoch 130, Loss: 0.5070515727009828\n",
      "Epoch 131, Loss: 0.5065962678643066\n",
      "Epoch 132, Loss: 0.5061448812256618\n",
      "Epoch 133, Loss: 0.5057069703510908\n",
      "Epoch 134, Loss: 0.5052756693952567\n",
      "Epoch 135, Loss: 0.5048445017462813\n",
      "Epoch 136, Loss: 0.504425284231979\n",
      "Epoch 137, Loss: 0.504022717877823\n",
      "Epoch 138, Loss: 0.5036312802042249\n",
      "Epoch 139, Loss: 0.5032368731068207\n",
      "Epoch 140, Loss: 0.5028511060044271\n",
      "Epoch 141, Loss: 0.5024804933282331\n",
      "Epoch 142, Loss: 0.5021182727396313\n",
      "Epoch 143, Loss: 0.5017581664990297\n",
      "Epoch 144, Loss: 0.5014050827892457\n",
      "Epoch 145, Loss: 0.501047675902349\n",
      "Epoch 146, Loss: 0.500690966247362\n",
      "Epoch 147, Loss: 0.5003476722262875\n",
      "Epoch 148, Loss: 0.5000068544871652\n",
      "Epoch 149, Loss: 0.49966338867073357\n",
      "Epoch 150, Loss: 0.4993280873087794\n",
      "Epoch 151, Loss: 0.49900257940428383\n",
      "Epoch 152, Loss: 0.4986865554458311\n",
      "Epoch 153, Loss: 0.49836949389755036\n",
      "Epoch 154, Loss: 0.49805375636713634\n",
      "Epoch 155, Loss: 0.49773873165141763\n",
      "Epoch 156, Loss: 0.49742224924400874\n",
      "Epoch 157, Loss: 0.49711425826812494\n",
      "Epoch 158, Loss: 0.4968010973402242\n",
      "Epoch 159, Loss: 0.4964901399315246\n",
      "Epoch 160, Loss: 0.49617675026847124\n",
      "Epoch 161, Loss: 0.4958670390985117\n",
      "Epoch 162, Loss: 0.4955578109301908\n",
      "Epoch 163, Loss: 0.4952510223984696\n",
      "Epoch 164, Loss: 0.49494577484111674\n",
      "Epoch 165, Loss: 0.49464511374066544\n",
      "Epoch 166, Loss: 0.49434383269975535\n",
      "Epoch 167, Loss: 0.49404631909698943\n",
      "Epoch 168, Loss: 0.49374530645143583\n",
      "Epoch 169, Loss: 0.49345064806438055\n",
      "Epoch 170, Loss: 0.49317001490258333\n",
      "Epoch 171, Loss: 0.4928927591216884\n",
      "Epoch 172, Loss: 0.4926138499628376\n",
      "Epoch 173, Loss: 0.49233528219511824\n",
      "Epoch 174, Loss: 0.49205852514110515\n",
      "Epoch 175, Loss: 0.4917773399684819\n",
      "Epoch 176, Loss: 0.49149629710928233\n",
      "Epoch 177, Loss: 0.4912165647150265\n",
      "Epoch 178, Loss: 0.49094181765497674\n",
      "Epoch 179, Loss: 0.49066901222163783\n",
      "Epoch 180, Loss: 0.490393928005003\n",
      "Epoch 181, Loss: 0.4901079113527018\n",
      "Epoch 182, Loss: 0.48981984154671765\n",
      "Epoch 183, Loss: 0.4895234443341763\n",
      "Epoch 184, Loss: 0.48921459258368105\n",
      "Epoch 185, Loss: 0.4889145977883947\n",
      "Epoch 186, Loss: 0.4886175142174919\n",
      "Epoch 187, Loss: 0.4883282295514753\n",
      "Epoch 188, Loss: 0.4880435212302353\n",
      "Epoch 189, Loss: 0.48775699700499114\n",
      "Epoch 190, Loss: 0.48748008613994775\n",
      "Epoch 191, Loss: 0.4872132179171775\n",
      "Epoch 192, Loss: 0.48694738158534456\n",
      "Epoch 193, Loss: 0.48668121634032785\n",
      "Epoch 194, Loss: 0.4864280322301768\n",
      "Epoch 195, Loss: 0.486178532155915\n",
      "Epoch 196, Loss: 0.4859301919172944\n",
      "Epoch 197, Loss: 0.48568442460989747\n",
      "Epoch 198, Loss: 0.48544224711238787\n",
      "Epoch 199, Loss: 0.48519729219516017\n",
      "Epoch 200, Loss: 0.4849575446659644\n",
      "Epoch 201, Loss: 0.4847146197220469\n",
      "Epoch 202, Loss: 0.48447155759133576\n",
      "Epoch 203, Loss: 0.48422783492589483\n",
      "Epoch 204, Loss: 0.48397707745066637\n",
      "Epoch 205, Loss: 0.483737926529631\n",
      "Epoch 206, Loss: 0.4835020782648923\n",
      "Epoch 207, Loss: 0.48327083252052233\n",
      "Epoch 208, Loss: 0.4830508282762034\n",
      "Epoch 209, Loss: 0.48283269895503794\n",
      "Epoch 210, Loss: 0.48261215534620283\n",
      "Epoch 211, Loss: 0.48240746542225255\n",
      "Epoch 212, Loss: 0.4822187420754367\n",
      "Epoch 213, Loss: 0.48202997575341067\n",
      "Epoch 214, Loss: 0.4818441197783379\n",
      "Epoch 215, Loss: 0.4816502256898402\n",
      "Epoch 216, Loss: 0.4814589616492725\n",
      "Epoch 217, Loss: 0.48127338055379854\n",
      "Epoch 218, Loss: 0.4810898650639108\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.26939242367741745\n",
      "Valence RMSE: 0.2515392451967582\n",
      "Arousal RMSE: 0.2861338218306803\n",
      "Test R^2 score: tensor([0.2332, 0.1893], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1420, 0.0929], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.21126552262140513\n",
      "Num of epochs: 219\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Epoch 121, Loss: 0.5116998508836337\n",
      "Epoch 122, Loss: 0.5111129902664598\n",
      "Epoch 123, Loss: 0.5105405741436312\n",
      "Epoch 124, Loss: 0.5099849010083686\n",
      "Epoch 125, Loss: 0.5094623759868415\n",
      "Epoch 126, Loss: 0.5089652841754122\n",
      "Epoch 127, Loss: 0.5084717799132713\n",
      "Epoch 128, Loss: 0.5079809643002384\n",
      "Epoch 129, Loss: 0.507507731608986\n",
      "Epoch 130, Loss: 0.5070515727009828\n",
      "Epoch 131, Loss: 0.5065962678643066\n",
      "Epoch 132, Loss: 0.5061448812256618\n",
      "Epoch 133, Loss: 0.5057069703510908\n",
      "Epoch 134, Loss: 0.5052756693952567\n",
      "Epoch 135, Loss: 0.5048445017462813\n",
      "Epoch 136, Loss: 0.504425284231979\n",
      "Epoch 137, Loss: 0.504022717877823\n",
      "Epoch 138, Loss: 0.5036312802042249\n",
      "Epoch 139, Loss: 0.5032368731068207\n",
      "Epoch 140, Loss: 0.5028511060044271\n",
      "Epoch 141, Loss: 0.5024804933282331\n",
      "Epoch 142, Loss: 0.5021182727396313\n",
      "Epoch 143, Loss: 0.5017581664990297\n",
      "Epoch 144, Loss: 0.5014050827892457\n",
      "Epoch 145, Loss: 0.501047675902349\n",
      "Epoch 146, Loss: 0.500690966247362\n",
      "Epoch 147, Loss: 0.5003476722262875\n",
      "Epoch 148, Loss: 0.5000068544871652\n",
      "Epoch 149, Loss: 0.49966338867073357\n",
      "Epoch 150, Loss: 0.4993280873087794\n",
      "Epoch 151, Loss: 0.49900257940428383\n",
      "Epoch 152, Loss: 0.4986865554458311\n",
      "Epoch 153, Loss: 0.49836949389755036\n",
      "Epoch 154, Loss: 0.49805375636713634\n",
      "Epoch 155, Loss: 0.49773873165141763\n",
      "Epoch 156, Loss: 0.49742224924400874\n",
      "Epoch 157, Loss: 0.49711425826812494\n",
      "Epoch 158, Loss: 0.4968010973402242\n",
      "Epoch 159, Loss: 0.4964901399315246\n",
      "Epoch 160, Loss: 0.49617675026847124\n",
      "Epoch 161, Loss: 0.4958670390985117\n",
      "Epoch 162, Loss: 0.4955578109301908\n",
      "Epoch 163, Loss: 0.4952510223984696\n",
      "Epoch 164, Loss: 0.49494577484111674\n",
      "Epoch 165, Loss: 0.49464511374066544\n",
      "Epoch 166, Loss: 0.49434383269975535\n",
      "Epoch 167, Loss: 0.49404631909698943\n",
      "Epoch 168, Loss: 0.49374530645143583\n",
      "Epoch 169, Loss: 0.49345064806438055\n",
      "Epoch 170, Loss: 0.49317001490258333\n",
      "Epoch 171, Loss: 0.4928927591216884\n",
      "Epoch 172, Loss: 0.4926138499628376\n",
      "Epoch 173, Loss: 0.49233528219511824\n",
      "Epoch 174, Loss: 0.49205852514110515\n",
      "Epoch 175, Loss: 0.4917773399684819\n",
      "Epoch 176, Loss: 0.49149629710928233\n",
      "Epoch 177, Loss: 0.4912165647150265\n",
      "Epoch 178, Loss: 0.49094181765497674\n",
      "Epoch 179, Loss: 0.49066901222163783\n",
      "Epoch 180, Loss: 0.490393928005003\n",
      "Epoch 181, Loss: 0.4901079113527018\n",
      "Epoch 182, Loss: 0.48981984154671765\n",
      "Epoch 183, Loss: 0.4895234443341763\n",
      "Epoch 184, Loss: 0.48921459258368105\n",
      "Epoch 185, Loss: 0.4889145977883947\n",
      "Epoch 186, Loss: 0.4886175142174919\n",
      "Epoch 187, Loss: 0.4883282295514753\n",
      "Epoch 188, Loss: 0.4880435212302353\n",
      "Epoch 189, Loss: 0.48775699700499114\n",
      "Epoch 190, Loss: 0.48748008613994775\n",
      "Epoch 191, Loss: 0.4872132179171775\n",
      "Epoch 192, Loss: 0.48694738158534456\n",
      "Epoch 193, Loss: 0.48668121634032785\n",
      "Epoch 194, Loss: 0.4864280322301768\n",
      "Epoch 195, Loss: 0.486178532155915\n",
      "Epoch 196, Loss: 0.4859301919172944\n",
      "Epoch 197, Loss: 0.48568442460989747\n",
      "Epoch 198, Loss: 0.48544224711238787\n",
      "Epoch 199, Loss: 0.48519729219516017\n",
      "Epoch 200, Loss: 0.4849575446659644\n",
      "Epoch 201, Loss: 0.4847146197220469\n",
      "Epoch 202, Loss: 0.48447155759133576\n",
      "Epoch 203, Loss: 0.48422783492589483\n",
      "Epoch 204, Loss: 0.48397707745066637\n",
      "Epoch 205, Loss: 0.483737926529631\n",
      "Epoch 206, Loss: 0.4835020782648923\n",
      "Epoch 207, Loss: 0.48327083252052233\n",
      "Epoch 208, Loss: 0.4830508282762034\n",
      "Epoch 209, Loss: 0.48283269895503794\n",
      "Epoch 210, Loss: 0.48261215534620283\n",
      "Epoch 211, Loss: 0.48240746542225255\n",
      "Epoch 212, Loss: 0.4822187420754367\n",
      "Epoch 213, Loss: 0.48202997575341067\n",
      "Epoch 214, Loss: 0.4818441197783379\n",
      "Epoch 215, Loss: 0.4816502256898402\n",
      "Epoch 216, Loss: 0.4814589616492725\n",
      "Epoch 217, Loss: 0.48127338055379854\n",
      "Epoch 218, Loss: 0.4810898650639108\n",
      "Epoch 219, Loss: 0.48091365405597575\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.26952919208415416\n",
      "Valence RMSE: 0.251597261399269\n",
      "Arousal RMSE: 0.28634033740893305\n",
      "Test R^2 score: tensor([0.2329, 0.1882], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1416, 0.0916], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.2105033332207895\n",
      "Num of epochs: 220\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Epoch 121, Loss: 0.5116998508836337\n",
      "Epoch 122, Loss: 0.5111129902664598\n",
      "Epoch 123, Loss: 0.5105405741436312\n",
      "Epoch 124, Loss: 0.5099849010083686\n",
      "Epoch 125, Loss: 0.5094623759868415\n",
      "Epoch 126, Loss: 0.5089652841754122\n",
      "Epoch 127, Loss: 0.5084717799132713\n",
      "Epoch 128, Loss: 0.5079809643002384\n",
      "Epoch 129, Loss: 0.507507731608986\n",
      "Epoch 130, Loss: 0.5070515727009828\n",
      "Epoch 131, Loss: 0.5065962678643066\n",
      "Epoch 132, Loss: 0.5061448812256618\n",
      "Epoch 133, Loss: 0.5057069703510908\n",
      "Epoch 134, Loss: 0.5052756693952567\n",
      "Epoch 135, Loss: 0.5048445017462813\n",
      "Epoch 136, Loss: 0.504425284231979\n",
      "Epoch 137, Loss: 0.504022717877823\n",
      "Epoch 138, Loss: 0.5036312802042249\n",
      "Epoch 139, Loss: 0.5032368731068207\n",
      "Epoch 140, Loss: 0.5028511060044271\n",
      "Epoch 141, Loss: 0.5024804933282331\n",
      "Epoch 142, Loss: 0.5021182727396313\n",
      "Epoch 143, Loss: 0.5017581664990297\n",
      "Epoch 144, Loss: 0.5014050827892457\n",
      "Epoch 145, Loss: 0.501047675902349\n",
      "Epoch 146, Loss: 0.500690966247362\n",
      "Epoch 147, Loss: 0.5003476722262875\n",
      "Epoch 148, Loss: 0.5000068544871652\n",
      "Epoch 149, Loss: 0.49966338867073357\n",
      "Epoch 150, Loss: 0.4993280873087794\n",
      "Epoch 151, Loss: 0.49900257940428383\n",
      "Epoch 152, Loss: 0.4986865554458311\n",
      "Epoch 153, Loss: 0.49836949389755036\n",
      "Epoch 154, Loss: 0.49805375636713634\n",
      "Epoch 155, Loss: 0.49773873165141763\n",
      "Epoch 156, Loss: 0.49742224924400874\n",
      "Epoch 157, Loss: 0.49711425826812494\n",
      "Epoch 158, Loss: 0.4968010973402242\n",
      "Epoch 159, Loss: 0.4964901399315246\n",
      "Epoch 160, Loss: 0.49617675026847124\n",
      "Epoch 161, Loss: 0.4958670390985117\n",
      "Epoch 162, Loss: 0.4955578109301908\n",
      "Epoch 163, Loss: 0.4952510223984696\n",
      "Epoch 164, Loss: 0.49494577484111674\n",
      "Epoch 165, Loss: 0.49464511374066544\n",
      "Epoch 166, Loss: 0.49434383269975535\n",
      "Epoch 167, Loss: 0.49404631909698943\n",
      "Epoch 168, Loss: 0.49374530645143583\n",
      "Epoch 169, Loss: 0.49345064806438055\n",
      "Epoch 170, Loss: 0.49317001490258333\n",
      "Epoch 171, Loss: 0.4928927591216884\n",
      "Epoch 172, Loss: 0.4926138499628376\n",
      "Epoch 173, Loss: 0.49233528219511824\n",
      "Epoch 174, Loss: 0.49205852514110515\n",
      "Epoch 175, Loss: 0.4917773399684819\n",
      "Epoch 176, Loss: 0.49149629710928233\n",
      "Epoch 177, Loss: 0.4912165647150265\n",
      "Epoch 178, Loss: 0.49094181765497674\n",
      "Epoch 179, Loss: 0.49066901222163783\n",
      "Epoch 180, Loss: 0.490393928005003\n",
      "Epoch 181, Loss: 0.4901079113527018\n",
      "Epoch 182, Loss: 0.48981984154671765\n",
      "Epoch 183, Loss: 0.4895234443341763\n",
      "Epoch 184, Loss: 0.48921459258368105\n",
      "Epoch 185, Loss: 0.4889145977883947\n",
      "Epoch 186, Loss: 0.4886175142174919\n",
      "Epoch 187, Loss: 0.4883282295514753\n",
      "Epoch 188, Loss: 0.4880435212302353\n",
      "Epoch 189, Loss: 0.48775699700499114\n",
      "Epoch 190, Loss: 0.48748008613994775\n",
      "Epoch 191, Loss: 0.4872132179171775\n",
      "Epoch 192, Loss: 0.48694738158534456\n",
      "Epoch 193, Loss: 0.48668121634032785\n",
      "Epoch 194, Loss: 0.4864280322301768\n",
      "Epoch 195, Loss: 0.486178532155915\n",
      "Epoch 196, Loss: 0.4859301919172944\n",
      "Epoch 197, Loss: 0.48568442460989747\n",
      "Epoch 198, Loss: 0.48544224711238787\n",
      "Epoch 199, Loss: 0.48519729219516017\n",
      "Epoch 200, Loss: 0.4849575446659644\n",
      "Epoch 201, Loss: 0.4847146197220469\n",
      "Epoch 202, Loss: 0.48447155759133576\n",
      "Epoch 203, Loss: 0.48422783492589483\n",
      "Epoch 204, Loss: 0.48397707745066637\n",
      "Epoch 205, Loss: 0.483737926529631\n",
      "Epoch 206, Loss: 0.4835020782648923\n",
      "Epoch 207, Loss: 0.48327083252052233\n",
      "Epoch 208, Loss: 0.4830508282762034\n",
      "Epoch 209, Loss: 0.48283269895503794\n",
      "Epoch 210, Loss: 0.48261215534620283\n",
      "Epoch 211, Loss: 0.48240746542225255\n",
      "Epoch 212, Loss: 0.4822187420754367\n",
      "Epoch 213, Loss: 0.48202997575341067\n",
      "Epoch 214, Loss: 0.4818441197783379\n",
      "Epoch 215, Loss: 0.4816502256898402\n",
      "Epoch 216, Loss: 0.4814589616492725\n",
      "Epoch 217, Loss: 0.48127338055379854\n",
      "Epoch 218, Loss: 0.4810898650639108\n",
      "Epoch 219, Loss: 0.48091365405597575\n",
      "Epoch 220, Loss: 0.480720996546175\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.26964774332273034\n",
      "Valence RMSE: 0.2516540389759929\n",
      "Arousal RMSE: 0.28651362205862313\n",
      "Test R^2 score: tensor([0.2325, 0.1872], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1412, 0.0905], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.2098387387878457\n",
      "Num of epochs: 221\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Epoch 121, Loss: 0.5116998508836337\n",
      "Epoch 122, Loss: 0.5111129902664598\n",
      "Epoch 123, Loss: 0.5105405741436312\n",
      "Epoch 124, Loss: 0.5099849010083686\n",
      "Epoch 125, Loss: 0.5094623759868415\n",
      "Epoch 126, Loss: 0.5089652841754122\n",
      "Epoch 127, Loss: 0.5084717799132713\n",
      "Epoch 128, Loss: 0.5079809643002384\n",
      "Epoch 129, Loss: 0.507507731608986\n",
      "Epoch 130, Loss: 0.5070515727009828\n",
      "Epoch 131, Loss: 0.5065962678643066\n",
      "Epoch 132, Loss: 0.5061448812256618\n",
      "Epoch 133, Loss: 0.5057069703510908\n",
      "Epoch 134, Loss: 0.5052756693952567\n",
      "Epoch 135, Loss: 0.5048445017462813\n",
      "Epoch 136, Loss: 0.504425284231979\n",
      "Epoch 137, Loss: 0.504022717877823\n",
      "Epoch 138, Loss: 0.5036312802042249\n",
      "Epoch 139, Loss: 0.5032368731068207\n",
      "Epoch 140, Loss: 0.5028511060044271\n",
      "Epoch 141, Loss: 0.5024804933282331\n",
      "Epoch 142, Loss: 0.5021182727396313\n",
      "Epoch 143, Loss: 0.5017581664990297\n",
      "Epoch 144, Loss: 0.5014050827892457\n",
      "Epoch 145, Loss: 0.501047675902349\n",
      "Epoch 146, Loss: 0.500690966247362\n",
      "Epoch 147, Loss: 0.5003476722262875\n",
      "Epoch 148, Loss: 0.5000068544871652\n",
      "Epoch 149, Loss: 0.49966338867073357\n",
      "Epoch 150, Loss: 0.4993280873087794\n",
      "Epoch 151, Loss: 0.49900257940428383\n",
      "Epoch 152, Loss: 0.4986865554458311\n",
      "Epoch 153, Loss: 0.49836949389755036\n",
      "Epoch 154, Loss: 0.49805375636713634\n",
      "Epoch 155, Loss: 0.49773873165141763\n",
      "Epoch 156, Loss: 0.49742224924400874\n",
      "Epoch 157, Loss: 0.49711425826812494\n",
      "Epoch 158, Loss: 0.4968010973402242\n",
      "Epoch 159, Loss: 0.4964901399315246\n",
      "Epoch 160, Loss: 0.49617675026847124\n",
      "Epoch 161, Loss: 0.4958670390985117\n",
      "Epoch 162, Loss: 0.4955578109301908\n",
      "Epoch 163, Loss: 0.4952510223984696\n",
      "Epoch 164, Loss: 0.49494577484111674\n",
      "Epoch 165, Loss: 0.49464511374066544\n",
      "Epoch 166, Loss: 0.49434383269975535\n",
      "Epoch 167, Loss: 0.49404631909698943\n",
      "Epoch 168, Loss: 0.49374530645143583\n",
      "Epoch 169, Loss: 0.49345064806438055\n",
      "Epoch 170, Loss: 0.49317001490258333\n",
      "Epoch 171, Loss: 0.4928927591216884\n",
      "Epoch 172, Loss: 0.4926138499628376\n",
      "Epoch 173, Loss: 0.49233528219511824\n",
      "Epoch 174, Loss: 0.49205852514110515\n",
      "Epoch 175, Loss: 0.4917773399684819\n",
      "Epoch 176, Loss: 0.49149629710928233\n",
      "Epoch 177, Loss: 0.4912165647150265\n",
      "Epoch 178, Loss: 0.49094181765497674\n",
      "Epoch 179, Loss: 0.49066901222163783\n",
      "Epoch 180, Loss: 0.490393928005003\n",
      "Epoch 181, Loss: 0.4901079113527018\n",
      "Epoch 182, Loss: 0.48981984154671765\n",
      "Epoch 183, Loss: 0.4895234443341763\n",
      "Epoch 184, Loss: 0.48921459258368105\n",
      "Epoch 185, Loss: 0.4889145977883947\n",
      "Epoch 186, Loss: 0.4886175142174919\n",
      "Epoch 187, Loss: 0.4883282295514753\n",
      "Epoch 188, Loss: 0.4880435212302353\n",
      "Epoch 189, Loss: 0.48775699700499114\n",
      "Epoch 190, Loss: 0.48748008613994775\n",
      "Epoch 191, Loss: 0.4872132179171775\n",
      "Epoch 192, Loss: 0.48694738158534456\n",
      "Epoch 193, Loss: 0.48668121634032785\n",
      "Epoch 194, Loss: 0.4864280322301768\n",
      "Epoch 195, Loss: 0.486178532155915\n",
      "Epoch 196, Loss: 0.4859301919172944\n",
      "Epoch 197, Loss: 0.48568442460989747\n",
      "Epoch 198, Loss: 0.48544224711238787\n",
      "Epoch 199, Loss: 0.48519729219516017\n",
      "Epoch 200, Loss: 0.4849575446659644\n",
      "Epoch 201, Loss: 0.4847146197220469\n",
      "Epoch 202, Loss: 0.48447155759133576\n",
      "Epoch 203, Loss: 0.48422783492589483\n",
      "Epoch 204, Loss: 0.48397707745066637\n",
      "Epoch 205, Loss: 0.483737926529631\n",
      "Epoch 206, Loss: 0.4835020782648923\n",
      "Epoch 207, Loss: 0.48327083252052233\n",
      "Epoch 208, Loss: 0.4830508282762034\n",
      "Epoch 209, Loss: 0.48283269895503794\n",
      "Epoch 210, Loss: 0.48261215534620283\n",
      "Epoch 211, Loss: 0.48240746542225255\n",
      "Epoch 212, Loss: 0.4822187420754367\n",
      "Epoch 213, Loss: 0.48202997575341067\n",
      "Epoch 214, Loss: 0.4818441197783379\n",
      "Epoch 215, Loss: 0.4816502256898402\n",
      "Epoch 216, Loss: 0.4814589616492725\n",
      "Epoch 217, Loss: 0.48127338055379854\n",
      "Epoch 218, Loss: 0.4810898650639108\n",
      "Epoch 219, Loss: 0.48091365405597575\n",
      "Epoch 220, Loss: 0.480720996546175\n",
      "Epoch 221, Loss: 0.48054865040929456\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.26962677746067587\n",
      "Valence RMSE: 0.25152172504539383\n",
      "Arousal RMSE: 0.28659033493446173\n",
      "Test R^2 score: tensor([0.2333, 0.1867], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1421, 0.0900], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.21002450200236383\n",
      "Num of epochs: 222\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Epoch 121, Loss: 0.5116998508836337\n",
      "Epoch 122, Loss: 0.5111129902664598\n",
      "Epoch 123, Loss: 0.5105405741436312\n",
      "Epoch 124, Loss: 0.5099849010083686\n",
      "Epoch 125, Loss: 0.5094623759868415\n",
      "Epoch 126, Loss: 0.5089652841754122\n",
      "Epoch 127, Loss: 0.5084717799132713\n",
      "Epoch 128, Loss: 0.5079809643002384\n",
      "Epoch 129, Loss: 0.507507731608986\n",
      "Epoch 130, Loss: 0.5070515727009828\n",
      "Epoch 131, Loss: 0.5065962678643066\n",
      "Epoch 132, Loss: 0.5061448812256618\n",
      "Epoch 133, Loss: 0.5057069703510908\n",
      "Epoch 134, Loss: 0.5052756693952567\n",
      "Epoch 135, Loss: 0.5048445017462813\n",
      "Epoch 136, Loss: 0.504425284231979\n",
      "Epoch 137, Loss: 0.504022717877823\n",
      "Epoch 138, Loss: 0.5036312802042249\n",
      "Epoch 139, Loss: 0.5032368731068207\n",
      "Epoch 140, Loss: 0.5028511060044271\n",
      "Epoch 141, Loss: 0.5024804933282331\n",
      "Epoch 142, Loss: 0.5021182727396313\n",
      "Epoch 143, Loss: 0.5017581664990297\n",
      "Epoch 144, Loss: 0.5014050827892457\n",
      "Epoch 145, Loss: 0.501047675902349\n",
      "Epoch 146, Loss: 0.500690966247362\n",
      "Epoch 147, Loss: 0.5003476722262875\n",
      "Epoch 148, Loss: 0.5000068544871652\n",
      "Epoch 149, Loss: 0.49966338867073357\n",
      "Epoch 150, Loss: 0.4993280873087794\n",
      "Epoch 151, Loss: 0.49900257940428383\n",
      "Epoch 152, Loss: 0.4986865554458311\n",
      "Epoch 153, Loss: 0.49836949389755036\n",
      "Epoch 154, Loss: 0.49805375636713634\n",
      "Epoch 155, Loss: 0.49773873165141763\n",
      "Epoch 156, Loss: 0.49742224924400874\n",
      "Epoch 157, Loss: 0.49711425826812494\n",
      "Epoch 158, Loss: 0.4968010973402242\n",
      "Epoch 159, Loss: 0.4964901399315246\n",
      "Epoch 160, Loss: 0.49617675026847124\n",
      "Epoch 161, Loss: 0.4958670390985117\n",
      "Epoch 162, Loss: 0.4955578109301908\n",
      "Epoch 163, Loss: 0.4952510223984696\n",
      "Epoch 164, Loss: 0.49494577484111674\n",
      "Epoch 165, Loss: 0.49464511374066544\n",
      "Epoch 166, Loss: 0.49434383269975535\n",
      "Epoch 167, Loss: 0.49404631909698943\n",
      "Epoch 168, Loss: 0.49374530645143583\n",
      "Epoch 169, Loss: 0.49345064806438055\n",
      "Epoch 170, Loss: 0.49317001490258333\n",
      "Epoch 171, Loss: 0.4928927591216884\n",
      "Epoch 172, Loss: 0.4926138499628376\n",
      "Epoch 173, Loss: 0.49233528219511824\n",
      "Epoch 174, Loss: 0.49205852514110515\n",
      "Epoch 175, Loss: 0.4917773399684819\n",
      "Epoch 176, Loss: 0.49149629710928233\n",
      "Epoch 177, Loss: 0.4912165647150265\n",
      "Epoch 178, Loss: 0.49094181765497674\n",
      "Epoch 179, Loss: 0.49066901222163783\n",
      "Epoch 180, Loss: 0.490393928005003\n",
      "Epoch 181, Loss: 0.4901079113527018\n",
      "Epoch 182, Loss: 0.48981984154671765\n",
      "Epoch 183, Loss: 0.4895234443341763\n",
      "Epoch 184, Loss: 0.48921459258368105\n",
      "Epoch 185, Loss: 0.4889145977883947\n",
      "Epoch 186, Loss: 0.4886175142174919\n",
      "Epoch 187, Loss: 0.4883282295514753\n",
      "Epoch 188, Loss: 0.4880435212302353\n",
      "Epoch 189, Loss: 0.48775699700499114\n",
      "Epoch 190, Loss: 0.48748008613994775\n",
      "Epoch 191, Loss: 0.4872132179171775\n",
      "Epoch 192, Loss: 0.48694738158534456\n",
      "Epoch 193, Loss: 0.48668121634032785\n",
      "Epoch 194, Loss: 0.4864280322301768\n",
      "Epoch 195, Loss: 0.486178532155915\n",
      "Epoch 196, Loss: 0.4859301919172944\n",
      "Epoch 197, Loss: 0.48568442460989747\n",
      "Epoch 198, Loss: 0.48544224711238787\n",
      "Epoch 199, Loss: 0.48519729219516017\n",
      "Epoch 200, Loss: 0.4849575446659644\n",
      "Epoch 201, Loss: 0.4847146197220469\n",
      "Epoch 202, Loss: 0.48447155759133576\n",
      "Epoch 203, Loss: 0.48422783492589483\n",
      "Epoch 204, Loss: 0.48397707745066637\n",
      "Epoch 205, Loss: 0.483737926529631\n",
      "Epoch 206, Loss: 0.4835020782648923\n",
      "Epoch 207, Loss: 0.48327083252052233\n",
      "Epoch 208, Loss: 0.4830508282762034\n",
      "Epoch 209, Loss: 0.48283269895503794\n",
      "Epoch 210, Loss: 0.48261215534620283\n",
      "Epoch 211, Loss: 0.48240746542225255\n",
      "Epoch 212, Loss: 0.4822187420754367\n",
      "Epoch 213, Loss: 0.48202997575341067\n",
      "Epoch 214, Loss: 0.4818441197783379\n",
      "Epoch 215, Loss: 0.4816502256898402\n",
      "Epoch 216, Loss: 0.4814589616492725\n",
      "Epoch 217, Loss: 0.48127338055379854\n",
      "Epoch 218, Loss: 0.4810898650639108\n",
      "Epoch 219, Loss: 0.48091365405597575\n",
      "Epoch 220, Loss: 0.480720996546175\n",
      "Epoch 221, Loss: 0.48054865040929456\n",
      "Epoch 222, Loss: 0.4803774677186426\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2701480538662429\n",
      "Valence RMSE: 0.2519307237402432\n",
      "Arousal RMSE: 0.2872122080468625\n",
      "Test R^2 score: tensor([0.2308, 0.1832], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1393, 0.0860], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.2070101623137814\n",
      "Num of epochs: 223\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Epoch 121, Loss: 0.5116998508836337\n",
      "Epoch 122, Loss: 0.5111129902664598\n",
      "Epoch 123, Loss: 0.5105405741436312\n",
      "Epoch 124, Loss: 0.5099849010083686\n",
      "Epoch 125, Loss: 0.5094623759868415\n",
      "Epoch 126, Loss: 0.5089652841754122\n",
      "Epoch 127, Loss: 0.5084717799132713\n",
      "Epoch 128, Loss: 0.5079809643002384\n",
      "Epoch 129, Loss: 0.507507731608986\n",
      "Epoch 130, Loss: 0.5070515727009828\n",
      "Epoch 131, Loss: 0.5065962678643066\n",
      "Epoch 132, Loss: 0.5061448812256618\n",
      "Epoch 133, Loss: 0.5057069703510908\n",
      "Epoch 134, Loss: 0.5052756693952567\n",
      "Epoch 135, Loss: 0.5048445017462813\n",
      "Epoch 136, Loss: 0.504425284231979\n",
      "Epoch 137, Loss: 0.504022717877823\n",
      "Epoch 138, Loss: 0.5036312802042249\n",
      "Epoch 139, Loss: 0.5032368731068207\n",
      "Epoch 140, Loss: 0.5028511060044271\n",
      "Epoch 141, Loss: 0.5024804933282331\n",
      "Epoch 142, Loss: 0.5021182727396313\n",
      "Epoch 143, Loss: 0.5017581664990297\n",
      "Epoch 144, Loss: 0.5014050827892457\n",
      "Epoch 145, Loss: 0.501047675902349\n",
      "Epoch 146, Loss: 0.500690966247362\n",
      "Epoch 147, Loss: 0.5003476722262875\n",
      "Epoch 148, Loss: 0.5000068544871652\n",
      "Epoch 149, Loss: 0.49966338867073357\n",
      "Epoch 150, Loss: 0.4993280873087794\n",
      "Epoch 151, Loss: 0.49900257940428383\n",
      "Epoch 152, Loss: 0.4986865554458311\n",
      "Epoch 153, Loss: 0.49836949389755036\n",
      "Epoch 154, Loss: 0.49805375636713634\n",
      "Epoch 155, Loss: 0.49773873165141763\n",
      "Epoch 156, Loss: 0.49742224924400874\n",
      "Epoch 157, Loss: 0.49711425826812494\n",
      "Epoch 158, Loss: 0.4968010973402242\n",
      "Epoch 159, Loss: 0.4964901399315246\n",
      "Epoch 160, Loss: 0.49617675026847124\n",
      "Epoch 161, Loss: 0.4958670390985117\n",
      "Epoch 162, Loss: 0.4955578109301908\n",
      "Epoch 163, Loss: 0.4952510223984696\n",
      "Epoch 164, Loss: 0.49494577484111674\n",
      "Epoch 165, Loss: 0.49464511374066544\n",
      "Epoch 166, Loss: 0.49434383269975535\n",
      "Epoch 167, Loss: 0.49404631909698943\n",
      "Epoch 168, Loss: 0.49374530645143583\n",
      "Epoch 169, Loss: 0.49345064806438055\n",
      "Epoch 170, Loss: 0.49317001490258333\n",
      "Epoch 171, Loss: 0.4928927591216884\n",
      "Epoch 172, Loss: 0.4926138499628376\n",
      "Epoch 173, Loss: 0.49233528219511824\n",
      "Epoch 174, Loss: 0.49205852514110515\n",
      "Epoch 175, Loss: 0.4917773399684819\n",
      "Epoch 176, Loss: 0.49149629710928233\n",
      "Epoch 177, Loss: 0.4912165647150265\n",
      "Epoch 178, Loss: 0.49094181765497674\n",
      "Epoch 179, Loss: 0.49066901222163783\n",
      "Epoch 180, Loss: 0.490393928005003\n",
      "Epoch 181, Loss: 0.4901079113527018\n",
      "Epoch 182, Loss: 0.48981984154671765\n",
      "Epoch 183, Loss: 0.4895234443341763\n",
      "Epoch 184, Loss: 0.48921459258368105\n",
      "Epoch 185, Loss: 0.4889145977883947\n",
      "Epoch 186, Loss: 0.4886175142174919\n",
      "Epoch 187, Loss: 0.4883282295514753\n",
      "Epoch 188, Loss: 0.4880435212302353\n",
      "Epoch 189, Loss: 0.48775699700499114\n",
      "Epoch 190, Loss: 0.48748008613994775\n",
      "Epoch 191, Loss: 0.4872132179171775\n",
      "Epoch 192, Loss: 0.48694738158534456\n",
      "Epoch 193, Loss: 0.48668121634032785\n",
      "Epoch 194, Loss: 0.4864280322301768\n",
      "Epoch 195, Loss: 0.486178532155915\n",
      "Epoch 196, Loss: 0.4859301919172944\n",
      "Epoch 197, Loss: 0.48568442460989747\n",
      "Epoch 198, Loss: 0.48544224711238787\n",
      "Epoch 199, Loss: 0.48519729219516017\n",
      "Epoch 200, Loss: 0.4849575446659644\n",
      "Epoch 201, Loss: 0.4847146197220469\n",
      "Epoch 202, Loss: 0.48447155759133576\n",
      "Epoch 203, Loss: 0.48422783492589483\n",
      "Epoch 204, Loss: 0.48397707745066637\n",
      "Epoch 205, Loss: 0.483737926529631\n",
      "Epoch 206, Loss: 0.4835020782648923\n",
      "Epoch 207, Loss: 0.48327083252052233\n",
      "Epoch 208, Loss: 0.4830508282762034\n",
      "Epoch 209, Loss: 0.48283269895503794\n",
      "Epoch 210, Loss: 0.48261215534620283\n",
      "Epoch 211, Loss: 0.48240746542225255\n",
      "Epoch 212, Loss: 0.4822187420754367\n",
      "Epoch 213, Loss: 0.48202997575341067\n",
      "Epoch 214, Loss: 0.4818441197783379\n",
      "Epoch 215, Loss: 0.4816502256898402\n",
      "Epoch 216, Loss: 0.4814589616492725\n",
      "Epoch 217, Loss: 0.48127338055379854\n",
      "Epoch 218, Loss: 0.4810898650639108\n",
      "Epoch 219, Loss: 0.48091365405597575\n",
      "Epoch 220, Loss: 0.480720996546175\n",
      "Epoch 221, Loss: 0.48054865040929456\n",
      "Epoch 222, Loss: 0.4803774677186426\n",
      "Epoch 223, Loss: 0.4801885671816001\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2705262111541496\n",
      "Valence RMSE: 0.2521484554204421\n",
      "Arousal RMSE: 0.2877325464244954\n",
      "Test R^2 score: tensor([0.2295, 0.1802], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1378, 0.0827], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.2048639860031532\n",
      "Num of epochs: 224\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Epoch 121, Loss: 0.5116998508836337\n",
      "Epoch 122, Loss: 0.5111129902664598\n",
      "Epoch 123, Loss: 0.5105405741436312\n",
      "Epoch 124, Loss: 0.5099849010083686\n",
      "Epoch 125, Loss: 0.5094623759868415\n",
      "Epoch 126, Loss: 0.5089652841754122\n",
      "Epoch 127, Loss: 0.5084717799132713\n",
      "Epoch 128, Loss: 0.5079809643002384\n",
      "Epoch 129, Loss: 0.507507731608986\n",
      "Epoch 130, Loss: 0.5070515727009828\n",
      "Epoch 131, Loss: 0.5065962678643066\n",
      "Epoch 132, Loss: 0.5061448812256618\n",
      "Epoch 133, Loss: 0.5057069703510908\n",
      "Epoch 134, Loss: 0.5052756693952567\n",
      "Epoch 135, Loss: 0.5048445017462813\n",
      "Epoch 136, Loss: 0.504425284231979\n",
      "Epoch 137, Loss: 0.504022717877823\n",
      "Epoch 138, Loss: 0.5036312802042249\n",
      "Epoch 139, Loss: 0.5032368731068207\n",
      "Epoch 140, Loss: 0.5028511060044271\n",
      "Epoch 141, Loss: 0.5024804933282331\n",
      "Epoch 142, Loss: 0.5021182727396313\n",
      "Epoch 143, Loss: 0.5017581664990297\n",
      "Epoch 144, Loss: 0.5014050827892457\n",
      "Epoch 145, Loss: 0.501047675902349\n",
      "Epoch 146, Loss: 0.500690966247362\n",
      "Epoch 147, Loss: 0.5003476722262875\n",
      "Epoch 148, Loss: 0.5000068544871652\n",
      "Epoch 149, Loss: 0.49966338867073357\n",
      "Epoch 150, Loss: 0.4993280873087794\n",
      "Epoch 151, Loss: 0.49900257940428383\n",
      "Epoch 152, Loss: 0.4986865554458311\n",
      "Epoch 153, Loss: 0.49836949389755036\n",
      "Epoch 154, Loss: 0.49805375636713634\n",
      "Epoch 155, Loss: 0.49773873165141763\n",
      "Epoch 156, Loss: 0.49742224924400874\n",
      "Epoch 157, Loss: 0.49711425826812494\n",
      "Epoch 158, Loss: 0.4968010973402242\n",
      "Epoch 159, Loss: 0.4964901399315246\n",
      "Epoch 160, Loss: 0.49617675026847124\n",
      "Epoch 161, Loss: 0.4958670390985117\n",
      "Epoch 162, Loss: 0.4955578109301908\n",
      "Epoch 163, Loss: 0.4952510223984696\n",
      "Epoch 164, Loss: 0.49494577484111674\n",
      "Epoch 165, Loss: 0.49464511374066544\n",
      "Epoch 166, Loss: 0.49434383269975535\n",
      "Epoch 167, Loss: 0.49404631909698943\n",
      "Epoch 168, Loss: 0.49374530645143583\n",
      "Epoch 169, Loss: 0.49345064806438055\n",
      "Epoch 170, Loss: 0.49317001490258333\n",
      "Epoch 171, Loss: 0.4928927591216884\n",
      "Epoch 172, Loss: 0.4926138499628376\n",
      "Epoch 173, Loss: 0.49233528219511824\n",
      "Epoch 174, Loss: 0.49205852514110515\n",
      "Epoch 175, Loss: 0.4917773399684819\n",
      "Epoch 176, Loss: 0.49149629710928233\n",
      "Epoch 177, Loss: 0.4912165647150265\n",
      "Epoch 178, Loss: 0.49094181765497674\n",
      "Epoch 179, Loss: 0.49066901222163783\n",
      "Epoch 180, Loss: 0.490393928005003\n",
      "Epoch 181, Loss: 0.4901079113527018\n",
      "Epoch 182, Loss: 0.48981984154671765\n",
      "Epoch 183, Loss: 0.4895234443341763\n",
      "Epoch 184, Loss: 0.48921459258368105\n",
      "Epoch 185, Loss: 0.4889145977883947\n",
      "Epoch 186, Loss: 0.4886175142174919\n",
      "Epoch 187, Loss: 0.4883282295514753\n",
      "Epoch 188, Loss: 0.4880435212302353\n",
      "Epoch 189, Loss: 0.48775699700499114\n",
      "Epoch 190, Loss: 0.48748008613994775\n",
      "Epoch 191, Loss: 0.4872132179171775\n",
      "Epoch 192, Loss: 0.48694738158534456\n",
      "Epoch 193, Loss: 0.48668121634032785\n",
      "Epoch 194, Loss: 0.4864280322301768\n",
      "Epoch 195, Loss: 0.486178532155915\n",
      "Epoch 196, Loss: 0.4859301919172944\n",
      "Epoch 197, Loss: 0.48568442460989747\n",
      "Epoch 198, Loss: 0.48544224711238787\n",
      "Epoch 199, Loss: 0.48519729219516017\n",
      "Epoch 200, Loss: 0.4849575446659644\n",
      "Epoch 201, Loss: 0.4847146197220469\n",
      "Epoch 202, Loss: 0.48447155759133576\n",
      "Epoch 203, Loss: 0.48422783492589483\n",
      "Epoch 204, Loss: 0.48397707745066637\n",
      "Epoch 205, Loss: 0.483737926529631\n",
      "Epoch 206, Loss: 0.4835020782648923\n",
      "Epoch 207, Loss: 0.48327083252052233\n",
      "Epoch 208, Loss: 0.4830508282762034\n",
      "Epoch 209, Loss: 0.48283269895503794\n",
      "Epoch 210, Loss: 0.48261215534620283\n",
      "Epoch 211, Loss: 0.48240746542225255\n",
      "Epoch 212, Loss: 0.4822187420754367\n",
      "Epoch 213, Loss: 0.48202997575341067\n",
      "Epoch 214, Loss: 0.4818441197783379\n",
      "Epoch 215, Loss: 0.4816502256898402\n",
      "Epoch 216, Loss: 0.4814589616492725\n",
      "Epoch 217, Loss: 0.48127338055379854\n",
      "Epoch 218, Loss: 0.4810898650639108\n",
      "Epoch 219, Loss: 0.48091365405597575\n",
      "Epoch 220, Loss: 0.480720996546175\n",
      "Epoch 221, Loss: 0.48054865040929456\n",
      "Epoch 222, Loss: 0.4803774677186426\n",
      "Epoch 223, Loss: 0.4801885671816001\n",
      "Epoch 224, Loss: 0.48000935557804647\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.27044069917394736\n",
      "Valence RMSE: 0.25189982584949894\n",
      "Arousal RMSE: 0.2877895433756743\n",
      "Test R^2 score: tensor([0.2310, 0.1799], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1395, 0.0823], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.20546096779108403\n",
      "Num of epochs: 225\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Epoch 121, Loss: 0.5116998508836337\n",
      "Epoch 122, Loss: 0.5111129902664598\n",
      "Epoch 123, Loss: 0.5105405741436312\n",
      "Epoch 124, Loss: 0.5099849010083686\n",
      "Epoch 125, Loss: 0.5094623759868415\n",
      "Epoch 126, Loss: 0.5089652841754122\n",
      "Epoch 127, Loss: 0.5084717799132713\n",
      "Epoch 128, Loss: 0.5079809643002384\n",
      "Epoch 129, Loss: 0.507507731608986\n",
      "Epoch 130, Loss: 0.5070515727009828\n",
      "Epoch 131, Loss: 0.5065962678643066\n",
      "Epoch 132, Loss: 0.5061448812256618\n",
      "Epoch 133, Loss: 0.5057069703510908\n",
      "Epoch 134, Loss: 0.5052756693952567\n",
      "Epoch 135, Loss: 0.5048445017462813\n",
      "Epoch 136, Loss: 0.504425284231979\n",
      "Epoch 137, Loss: 0.504022717877823\n",
      "Epoch 138, Loss: 0.5036312802042249\n",
      "Epoch 139, Loss: 0.5032368731068207\n",
      "Epoch 140, Loss: 0.5028511060044271\n",
      "Epoch 141, Loss: 0.5024804933282331\n",
      "Epoch 142, Loss: 0.5021182727396313\n",
      "Epoch 143, Loss: 0.5017581664990297\n",
      "Epoch 144, Loss: 0.5014050827892457\n",
      "Epoch 145, Loss: 0.501047675902349\n",
      "Epoch 146, Loss: 0.500690966247362\n",
      "Epoch 147, Loss: 0.5003476722262875\n",
      "Epoch 148, Loss: 0.5000068544871652\n",
      "Epoch 149, Loss: 0.49966338867073357\n",
      "Epoch 150, Loss: 0.4993280873087794\n",
      "Epoch 151, Loss: 0.49900257940428383\n",
      "Epoch 152, Loss: 0.4986865554458311\n",
      "Epoch 153, Loss: 0.49836949389755036\n",
      "Epoch 154, Loss: 0.49805375636713634\n",
      "Epoch 155, Loss: 0.49773873165141763\n",
      "Epoch 156, Loss: 0.49742224924400874\n",
      "Epoch 157, Loss: 0.49711425826812494\n",
      "Epoch 158, Loss: 0.4968010973402242\n",
      "Epoch 159, Loss: 0.4964901399315246\n",
      "Epoch 160, Loss: 0.49617675026847124\n",
      "Epoch 161, Loss: 0.4958670390985117\n",
      "Epoch 162, Loss: 0.4955578109301908\n",
      "Epoch 163, Loss: 0.4952510223984696\n",
      "Epoch 164, Loss: 0.49494577484111674\n",
      "Epoch 165, Loss: 0.49464511374066544\n",
      "Epoch 166, Loss: 0.49434383269975535\n",
      "Epoch 167, Loss: 0.49404631909698943\n",
      "Epoch 168, Loss: 0.49374530645143583\n",
      "Epoch 169, Loss: 0.49345064806438055\n",
      "Epoch 170, Loss: 0.49317001490258333\n",
      "Epoch 171, Loss: 0.4928927591216884\n",
      "Epoch 172, Loss: 0.4926138499628376\n",
      "Epoch 173, Loss: 0.49233528219511824\n",
      "Epoch 174, Loss: 0.49205852514110515\n",
      "Epoch 175, Loss: 0.4917773399684819\n",
      "Epoch 176, Loss: 0.49149629710928233\n",
      "Epoch 177, Loss: 0.4912165647150265\n",
      "Epoch 178, Loss: 0.49094181765497674\n",
      "Epoch 179, Loss: 0.49066901222163783\n",
      "Epoch 180, Loss: 0.490393928005003\n",
      "Epoch 181, Loss: 0.4901079113527018\n",
      "Epoch 182, Loss: 0.48981984154671765\n",
      "Epoch 183, Loss: 0.4895234443341763\n",
      "Epoch 184, Loss: 0.48921459258368105\n",
      "Epoch 185, Loss: 0.4889145977883947\n",
      "Epoch 186, Loss: 0.4886175142174919\n",
      "Epoch 187, Loss: 0.4883282295514753\n",
      "Epoch 188, Loss: 0.4880435212302353\n",
      "Epoch 189, Loss: 0.48775699700499114\n",
      "Epoch 190, Loss: 0.48748008613994775\n",
      "Epoch 191, Loss: 0.4872132179171775\n",
      "Epoch 192, Loss: 0.48694738158534456\n",
      "Epoch 193, Loss: 0.48668121634032785\n",
      "Epoch 194, Loss: 0.4864280322301768\n",
      "Epoch 195, Loss: 0.486178532155915\n",
      "Epoch 196, Loss: 0.4859301919172944\n",
      "Epoch 197, Loss: 0.48568442460989747\n",
      "Epoch 198, Loss: 0.48544224711238787\n",
      "Epoch 199, Loss: 0.48519729219516017\n",
      "Epoch 200, Loss: 0.4849575446659644\n",
      "Epoch 201, Loss: 0.4847146197220469\n",
      "Epoch 202, Loss: 0.48447155759133576\n",
      "Epoch 203, Loss: 0.48422783492589483\n",
      "Epoch 204, Loss: 0.48397707745066637\n",
      "Epoch 205, Loss: 0.483737926529631\n",
      "Epoch 206, Loss: 0.4835020782648923\n",
      "Epoch 207, Loss: 0.48327083252052233\n",
      "Epoch 208, Loss: 0.4830508282762034\n",
      "Epoch 209, Loss: 0.48283269895503794\n",
      "Epoch 210, Loss: 0.48261215534620283\n",
      "Epoch 211, Loss: 0.48240746542225255\n",
      "Epoch 212, Loss: 0.4822187420754367\n",
      "Epoch 213, Loss: 0.48202997575341067\n",
      "Epoch 214, Loss: 0.4818441197783379\n",
      "Epoch 215, Loss: 0.4816502256898402\n",
      "Epoch 216, Loss: 0.4814589616492725\n",
      "Epoch 217, Loss: 0.48127338055379854\n",
      "Epoch 218, Loss: 0.4810898650639108\n",
      "Epoch 219, Loss: 0.48091365405597575\n",
      "Epoch 220, Loss: 0.480720996546175\n",
      "Epoch 221, Loss: 0.48054865040929456\n",
      "Epoch 222, Loss: 0.4803774677186426\n",
      "Epoch 223, Loss: 0.4801885671816001\n",
      "Epoch 224, Loss: 0.48000935557804647\n",
      "Epoch 225, Loss: 0.4798350303004208\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2709460824641784\n",
      "Valence RMSE: 0.2523288054058282\n",
      "Arousal RMSE: 0.28836389019403663\n",
      "Test R^2 score: tensor([0.2284, 0.1766], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1366, 0.0787], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.2025119836557781\n",
      "Num of epochs: 226\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Epoch 121, Loss: 0.5116998508836337\n",
      "Epoch 122, Loss: 0.5111129902664598\n",
      "Epoch 123, Loss: 0.5105405741436312\n",
      "Epoch 124, Loss: 0.5099849010083686\n",
      "Epoch 125, Loss: 0.5094623759868415\n",
      "Epoch 126, Loss: 0.5089652841754122\n",
      "Epoch 127, Loss: 0.5084717799132713\n",
      "Epoch 128, Loss: 0.5079809643002384\n",
      "Epoch 129, Loss: 0.507507731608986\n",
      "Epoch 130, Loss: 0.5070515727009828\n",
      "Epoch 131, Loss: 0.5065962678643066\n",
      "Epoch 132, Loss: 0.5061448812256618\n",
      "Epoch 133, Loss: 0.5057069703510908\n",
      "Epoch 134, Loss: 0.5052756693952567\n",
      "Epoch 135, Loss: 0.5048445017462813\n",
      "Epoch 136, Loss: 0.504425284231979\n",
      "Epoch 137, Loss: 0.504022717877823\n",
      "Epoch 138, Loss: 0.5036312802042249\n",
      "Epoch 139, Loss: 0.5032368731068207\n",
      "Epoch 140, Loss: 0.5028511060044271\n",
      "Epoch 141, Loss: 0.5024804933282331\n",
      "Epoch 142, Loss: 0.5021182727396313\n",
      "Epoch 143, Loss: 0.5017581664990297\n",
      "Epoch 144, Loss: 0.5014050827892457\n",
      "Epoch 145, Loss: 0.501047675902349\n",
      "Epoch 146, Loss: 0.500690966247362\n",
      "Epoch 147, Loss: 0.5003476722262875\n",
      "Epoch 148, Loss: 0.5000068544871652\n",
      "Epoch 149, Loss: 0.49966338867073357\n",
      "Epoch 150, Loss: 0.4993280873087794\n",
      "Epoch 151, Loss: 0.49900257940428383\n",
      "Epoch 152, Loss: 0.4986865554458311\n",
      "Epoch 153, Loss: 0.49836949389755036\n",
      "Epoch 154, Loss: 0.49805375636713634\n",
      "Epoch 155, Loss: 0.49773873165141763\n",
      "Epoch 156, Loss: 0.49742224924400874\n",
      "Epoch 157, Loss: 0.49711425826812494\n",
      "Epoch 158, Loss: 0.4968010973402242\n",
      "Epoch 159, Loss: 0.4964901399315246\n",
      "Epoch 160, Loss: 0.49617675026847124\n",
      "Epoch 161, Loss: 0.4958670390985117\n",
      "Epoch 162, Loss: 0.4955578109301908\n",
      "Epoch 163, Loss: 0.4952510223984696\n",
      "Epoch 164, Loss: 0.49494577484111674\n",
      "Epoch 165, Loss: 0.49464511374066544\n",
      "Epoch 166, Loss: 0.49434383269975535\n",
      "Epoch 167, Loss: 0.49404631909698943\n",
      "Epoch 168, Loss: 0.49374530645143583\n",
      "Epoch 169, Loss: 0.49345064806438055\n",
      "Epoch 170, Loss: 0.49317001490258333\n",
      "Epoch 171, Loss: 0.4928927591216884\n",
      "Epoch 172, Loss: 0.4926138499628376\n",
      "Epoch 173, Loss: 0.49233528219511824\n",
      "Epoch 174, Loss: 0.49205852514110515\n",
      "Epoch 175, Loss: 0.4917773399684819\n",
      "Epoch 176, Loss: 0.49149629710928233\n",
      "Epoch 177, Loss: 0.4912165647150265\n",
      "Epoch 178, Loss: 0.49094181765497674\n",
      "Epoch 179, Loss: 0.49066901222163783\n",
      "Epoch 180, Loss: 0.490393928005003\n",
      "Epoch 181, Loss: 0.4901079113527018\n",
      "Epoch 182, Loss: 0.48981984154671765\n",
      "Epoch 183, Loss: 0.4895234443341763\n",
      "Epoch 184, Loss: 0.48921459258368105\n",
      "Epoch 185, Loss: 0.4889145977883947\n",
      "Epoch 186, Loss: 0.4886175142174919\n",
      "Epoch 187, Loss: 0.4883282295514753\n",
      "Epoch 188, Loss: 0.4880435212302353\n",
      "Epoch 189, Loss: 0.48775699700499114\n",
      "Epoch 190, Loss: 0.48748008613994775\n",
      "Epoch 191, Loss: 0.4872132179171775\n",
      "Epoch 192, Loss: 0.48694738158534456\n",
      "Epoch 193, Loss: 0.48668121634032785\n",
      "Epoch 194, Loss: 0.4864280322301768\n",
      "Epoch 195, Loss: 0.486178532155915\n",
      "Epoch 196, Loss: 0.4859301919172944\n",
      "Epoch 197, Loss: 0.48568442460989747\n",
      "Epoch 198, Loss: 0.48544224711238787\n",
      "Epoch 199, Loss: 0.48519729219516017\n",
      "Epoch 200, Loss: 0.4849575446659644\n",
      "Epoch 201, Loss: 0.4847146197220469\n",
      "Epoch 202, Loss: 0.48447155759133576\n",
      "Epoch 203, Loss: 0.48422783492589483\n",
      "Epoch 204, Loss: 0.48397707745066637\n",
      "Epoch 205, Loss: 0.483737926529631\n",
      "Epoch 206, Loss: 0.4835020782648923\n",
      "Epoch 207, Loss: 0.48327083252052233\n",
      "Epoch 208, Loss: 0.4830508282762034\n",
      "Epoch 209, Loss: 0.48283269895503794\n",
      "Epoch 210, Loss: 0.48261215534620283\n",
      "Epoch 211, Loss: 0.48240746542225255\n",
      "Epoch 212, Loss: 0.4822187420754367\n",
      "Epoch 213, Loss: 0.48202997575341067\n",
      "Epoch 214, Loss: 0.4818441197783379\n",
      "Epoch 215, Loss: 0.4816502256898402\n",
      "Epoch 216, Loss: 0.4814589616492725\n",
      "Epoch 217, Loss: 0.48127338055379854\n",
      "Epoch 218, Loss: 0.4810898650639108\n",
      "Epoch 219, Loss: 0.48091365405597575\n",
      "Epoch 220, Loss: 0.480720996546175\n",
      "Epoch 221, Loss: 0.48054865040929456\n",
      "Epoch 222, Loss: 0.4803774677186426\n",
      "Epoch 223, Loss: 0.4801885671816001\n",
      "Epoch 224, Loss: 0.48000935557804647\n",
      "Epoch 225, Loss: 0.4798350303004208\n",
      "Epoch 226, Loss: 0.47967842665052907\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2707201421052717\n",
      "Valence RMSE: 0.2520050084568383\n",
      "Arousal RMSE: 0.2882225986900851\n",
      "Test R^2 score: tensor([0.2304, 0.1774], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1388, 0.0796], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.2039048392443763\n",
      "Num of epochs: 227\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Epoch 121, Loss: 0.5116998508836337\n",
      "Epoch 122, Loss: 0.5111129902664598\n",
      "Epoch 123, Loss: 0.5105405741436312\n",
      "Epoch 124, Loss: 0.5099849010083686\n",
      "Epoch 125, Loss: 0.5094623759868415\n",
      "Epoch 126, Loss: 0.5089652841754122\n",
      "Epoch 127, Loss: 0.5084717799132713\n",
      "Epoch 128, Loss: 0.5079809643002384\n",
      "Epoch 129, Loss: 0.507507731608986\n",
      "Epoch 130, Loss: 0.5070515727009828\n",
      "Epoch 131, Loss: 0.5065962678643066\n",
      "Epoch 132, Loss: 0.5061448812256618\n",
      "Epoch 133, Loss: 0.5057069703510908\n",
      "Epoch 134, Loss: 0.5052756693952567\n",
      "Epoch 135, Loss: 0.5048445017462813\n",
      "Epoch 136, Loss: 0.504425284231979\n",
      "Epoch 137, Loss: 0.504022717877823\n",
      "Epoch 138, Loss: 0.5036312802042249\n",
      "Epoch 139, Loss: 0.5032368731068207\n",
      "Epoch 140, Loss: 0.5028511060044271\n",
      "Epoch 141, Loss: 0.5024804933282331\n",
      "Epoch 142, Loss: 0.5021182727396313\n",
      "Epoch 143, Loss: 0.5017581664990297\n",
      "Epoch 144, Loss: 0.5014050827892457\n",
      "Epoch 145, Loss: 0.501047675902349\n",
      "Epoch 146, Loss: 0.500690966247362\n",
      "Epoch 147, Loss: 0.5003476722262875\n",
      "Epoch 148, Loss: 0.5000068544871652\n",
      "Epoch 149, Loss: 0.49966338867073357\n",
      "Epoch 150, Loss: 0.4993280873087794\n",
      "Epoch 151, Loss: 0.49900257940428383\n",
      "Epoch 152, Loss: 0.4986865554458311\n",
      "Epoch 153, Loss: 0.49836949389755036\n",
      "Epoch 154, Loss: 0.49805375636713634\n",
      "Epoch 155, Loss: 0.49773873165141763\n",
      "Epoch 156, Loss: 0.49742224924400874\n",
      "Epoch 157, Loss: 0.49711425826812494\n",
      "Epoch 158, Loss: 0.4968010973402242\n",
      "Epoch 159, Loss: 0.4964901399315246\n",
      "Epoch 160, Loss: 0.49617675026847124\n",
      "Epoch 161, Loss: 0.4958670390985117\n",
      "Epoch 162, Loss: 0.4955578109301908\n",
      "Epoch 163, Loss: 0.4952510223984696\n",
      "Epoch 164, Loss: 0.49494577484111674\n",
      "Epoch 165, Loss: 0.49464511374066544\n",
      "Epoch 166, Loss: 0.49434383269975535\n",
      "Epoch 167, Loss: 0.49404631909698943\n",
      "Epoch 168, Loss: 0.49374530645143583\n",
      "Epoch 169, Loss: 0.49345064806438055\n",
      "Epoch 170, Loss: 0.49317001490258333\n",
      "Epoch 171, Loss: 0.4928927591216884\n",
      "Epoch 172, Loss: 0.4926138499628376\n",
      "Epoch 173, Loss: 0.49233528219511824\n",
      "Epoch 174, Loss: 0.49205852514110515\n",
      "Epoch 175, Loss: 0.4917773399684819\n",
      "Epoch 176, Loss: 0.49149629710928233\n",
      "Epoch 177, Loss: 0.4912165647150265\n",
      "Epoch 178, Loss: 0.49094181765497674\n",
      "Epoch 179, Loss: 0.49066901222163783\n",
      "Epoch 180, Loss: 0.490393928005003\n",
      "Epoch 181, Loss: 0.4901079113527018\n",
      "Epoch 182, Loss: 0.48981984154671765\n",
      "Epoch 183, Loss: 0.4895234443341763\n",
      "Epoch 184, Loss: 0.48921459258368105\n",
      "Epoch 185, Loss: 0.4889145977883947\n",
      "Epoch 186, Loss: 0.4886175142174919\n",
      "Epoch 187, Loss: 0.4883282295514753\n",
      "Epoch 188, Loss: 0.4880435212302353\n",
      "Epoch 189, Loss: 0.48775699700499114\n",
      "Epoch 190, Loss: 0.48748008613994775\n",
      "Epoch 191, Loss: 0.4872132179171775\n",
      "Epoch 192, Loss: 0.48694738158534456\n",
      "Epoch 193, Loss: 0.48668121634032785\n",
      "Epoch 194, Loss: 0.4864280322301768\n",
      "Epoch 195, Loss: 0.486178532155915\n",
      "Epoch 196, Loss: 0.4859301919172944\n",
      "Epoch 197, Loss: 0.48568442460989747\n",
      "Epoch 198, Loss: 0.48544224711238787\n",
      "Epoch 199, Loss: 0.48519729219516017\n",
      "Epoch 200, Loss: 0.4849575446659644\n",
      "Epoch 201, Loss: 0.4847146197220469\n",
      "Epoch 202, Loss: 0.48447155759133576\n",
      "Epoch 203, Loss: 0.48422783492589483\n",
      "Epoch 204, Loss: 0.48397707745066637\n",
      "Epoch 205, Loss: 0.483737926529631\n",
      "Epoch 206, Loss: 0.4835020782648923\n",
      "Epoch 207, Loss: 0.48327083252052233\n",
      "Epoch 208, Loss: 0.4830508282762034\n",
      "Epoch 209, Loss: 0.48283269895503794\n",
      "Epoch 210, Loss: 0.48261215534620283\n",
      "Epoch 211, Loss: 0.48240746542225255\n",
      "Epoch 212, Loss: 0.4822187420754367\n",
      "Epoch 213, Loss: 0.48202997575341067\n",
      "Epoch 214, Loss: 0.4818441197783379\n",
      "Epoch 215, Loss: 0.4816502256898402\n",
      "Epoch 216, Loss: 0.4814589616492725\n",
      "Epoch 217, Loss: 0.48127338055379854\n",
      "Epoch 218, Loss: 0.4810898650639108\n",
      "Epoch 219, Loss: 0.48091365405597575\n",
      "Epoch 220, Loss: 0.480720996546175\n",
      "Epoch 221, Loss: 0.48054865040929456\n",
      "Epoch 222, Loss: 0.4803774677186426\n",
      "Epoch 223, Loss: 0.4801885671816001\n",
      "Epoch 224, Loss: 0.48000935557804647\n",
      "Epoch 225, Loss: 0.4798350303004208\n",
      "Epoch 226, Loss: 0.47967842665052907\n",
      "Epoch 227, Loss: 0.47947854451894606\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2708786775346162\n",
      "Valence RMSE: 0.2520524052798818\n",
      "Arousal RMSE: 0.2884789435616253\n",
      "Test R^2 score: tensor([0.2301, 0.1760], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1385, 0.0779], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.20302816859640171\n",
      "Num of epochs: 228\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Epoch 121, Loss: 0.5116998508836337\n",
      "Epoch 122, Loss: 0.5111129902664598\n",
      "Epoch 123, Loss: 0.5105405741436312\n",
      "Epoch 124, Loss: 0.5099849010083686\n",
      "Epoch 125, Loss: 0.5094623759868415\n",
      "Epoch 126, Loss: 0.5089652841754122\n",
      "Epoch 127, Loss: 0.5084717799132713\n",
      "Epoch 128, Loss: 0.5079809643002384\n",
      "Epoch 129, Loss: 0.507507731608986\n",
      "Epoch 130, Loss: 0.5070515727009828\n",
      "Epoch 131, Loss: 0.5065962678643066\n",
      "Epoch 132, Loss: 0.5061448812256618\n",
      "Epoch 133, Loss: 0.5057069703510908\n",
      "Epoch 134, Loss: 0.5052756693952567\n",
      "Epoch 135, Loss: 0.5048445017462813\n",
      "Epoch 136, Loss: 0.504425284231979\n",
      "Epoch 137, Loss: 0.504022717877823\n",
      "Epoch 138, Loss: 0.5036312802042249\n",
      "Epoch 139, Loss: 0.5032368731068207\n",
      "Epoch 140, Loss: 0.5028511060044271\n",
      "Epoch 141, Loss: 0.5024804933282331\n",
      "Epoch 142, Loss: 0.5021182727396313\n",
      "Epoch 143, Loss: 0.5017581664990297\n",
      "Epoch 144, Loss: 0.5014050827892457\n",
      "Epoch 145, Loss: 0.501047675902349\n",
      "Epoch 146, Loss: 0.500690966247362\n",
      "Epoch 147, Loss: 0.5003476722262875\n",
      "Epoch 148, Loss: 0.5000068544871652\n",
      "Epoch 149, Loss: 0.49966338867073357\n",
      "Epoch 150, Loss: 0.4993280873087794\n",
      "Epoch 151, Loss: 0.49900257940428383\n",
      "Epoch 152, Loss: 0.4986865554458311\n",
      "Epoch 153, Loss: 0.49836949389755036\n",
      "Epoch 154, Loss: 0.49805375636713634\n",
      "Epoch 155, Loss: 0.49773873165141763\n",
      "Epoch 156, Loss: 0.49742224924400874\n",
      "Epoch 157, Loss: 0.49711425826812494\n",
      "Epoch 158, Loss: 0.4968010973402242\n",
      "Epoch 159, Loss: 0.4964901399315246\n",
      "Epoch 160, Loss: 0.49617675026847124\n",
      "Epoch 161, Loss: 0.4958670390985117\n",
      "Epoch 162, Loss: 0.4955578109301908\n",
      "Epoch 163, Loss: 0.4952510223984696\n",
      "Epoch 164, Loss: 0.49494577484111674\n",
      "Epoch 165, Loss: 0.49464511374066544\n",
      "Epoch 166, Loss: 0.49434383269975535\n",
      "Epoch 167, Loss: 0.49404631909698943\n",
      "Epoch 168, Loss: 0.49374530645143583\n",
      "Epoch 169, Loss: 0.49345064806438055\n",
      "Epoch 170, Loss: 0.49317001490258333\n",
      "Epoch 171, Loss: 0.4928927591216884\n",
      "Epoch 172, Loss: 0.4926138499628376\n",
      "Epoch 173, Loss: 0.49233528219511824\n",
      "Epoch 174, Loss: 0.49205852514110515\n",
      "Epoch 175, Loss: 0.4917773399684819\n",
      "Epoch 176, Loss: 0.49149629710928233\n",
      "Epoch 177, Loss: 0.4912165647150265\n",
      "Epoch 178, Loss: 0.49094181765497674\n",
      "Epoch 179, Loss: 0.49066901222163783\n",
      "Epoch 180, Loss: 0.490393928005003\n",
      "Epoch 181, Loss: 0.4901079113527018\n",
      "Epoch 182, Loss: 0.48981984154671765\n",
      "Epoch 183, Loss: 0.4895234443341763\n",
      "Epoch 184, Loss: 0.48921459258368105\n",
      "Epoch 185, Loss: 0.4889145977883947\n",
      "Epoch 186, Loss: 0.4886175142174919\n",
      "Epoch 187, Loss: 0.4883282295514753\n",
      "Epoch 188, Loss: 0.4880435212302353\n",
      "Epoch 189, Loss: 0.48775699700499114\n",
      "Epoch 190, Loss: 0.48748008613994775\n",
      "Epoch 191, Loss: 0.4872132179171775\n",
      "Epoch 192, Loss: 0.48694738158534456\n",
      "Epoch 193, Loss: 0.48668121634032785\n",
      "Epoch 194, Loss: 0.4864280322301768\n",
      "Epoch 195, Loss: 0.486178532155915\n",
      "Epoch 196, Loss: 0.4859301919172944\n",
      "Epoch 197, Loss: 0.48568442460989747\n",
      "Epoch 198, Loss: 0.48544224711238787\n",
      "Epoch 199, Loss: 0.48519729219516017\n",
      "Epoch 200, Loss: 0.4849575446659644\n",
      "Epoch 201, Loss: 0.4847146197220469\n",
      "Epoch 202, Loss: 0.48447155759133576\n",
      "Epoch 203, Loss: 0.48422783492589483\n",
      "Epoch 204, Loss: 0.48397707745066637\n",
      "Epoch 205, Loss: 0.483737926529631\n",
      "Epoch 206, Loss: 0.4835020782648923\n",
      "Epoch 207, Loss: 0.48327083252052233\n",
      "Epoch 208, Loss: 0.4830508282762034\n",
      "Epoch 209, Loss: 0.48283269895503794\n",
      "Epoch 210, Loss: 0.48261215534620283\n",
      "Epoch 211, Loss: 0.48240746542225255\n",
      "Epoch 212, Loss: 0.4822187420754367\n",
      "Epoch 213, Loss: 0.48202997575341067\n",
      "Epoch 214, Loss: 0.4818441197783379\n",
      "Epoch 215, Loss: 0.4816502256898402\n",
      "Epoch 216, Loss: 0.4814589616492725\n",
      "Epoch 217, Loss: 0.48127338055379854\n",
      "Epoch 218, Loss: 0.4810898650639108\n",
      "Epoch 219, Loss: 0.48091365405597575\n",
      "Epoch 220, Loss: 0.480720996546175\n",
      "Epoch 221, Loss: 0.48054865040929456\n",
      "Epoch 222, Loss: 0.4803774677186426\n",
      "Epoch 223, Loss: 0.4801885671816001\n",
      "Epoch 224, Loss: 0.48000935557804647\n",
      "Epoch 225, Loss: 0.4798350303004208\n",
      "Epoch 226, Loss: 0.47967842665052907\n",
      "Epoch 227, Loss: 0.47947854451894606\n",
      "Epoch 228, Loss: 0.47928199900429136\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.27135999296667035\n",
      "Valence RMSE: 0.25243796449955375\n",
      "Arousal RMSE: 0.2890459576694765\n",
      "Test R^2 score: tensor([0.2277, 0.1727], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1358, 0.0743], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.20022830442394474\n",
      "Num of epochs: 229\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Epoch 121, Loss: 0.5116998508836337\n",
      "Epoch 122, Loss: 0.5111129902664598\n",
      "Epoch 123, Loss: 0.5105405741436312\n",
      "Epoch 124, Loss: 0.5099849010083686\n",
      "Epoch 125, Loss: 0.5094623759868415\n",
      "Epoch 126, Loss: 0.5089652841754122\n",
      "Epoch 127, Loss: 0.5084717799132713\n",
      "Epoch 128, Loss: 0.5079809643002384\n",
      "Epoch 129, Loss: 0.507507731608986\n",
      "Epoch 130, Loss: 0.5070515727009828\n",
      "Epoch 131, Loss: 0.5065962678643066\n",
      "Epoch 132, Loss: 0.5061448812256618\n",
      "Epoch 133, Loss: 0.5057069703510908\n",
      "Epoch 134, Loss: 0.5052756693952567\n",
      "Epoch 135, Loss: 0.5048445017462813\n",
      "Epoch 136, Loss: 0.504425284231979\n",
      "Epoch 137, Loss: 0.504022717877823\n",
      "Epoch 138, Loss: 0.5036312802042249\n",
      "Epoch 139, Loss: 0.5032368731068207\n",
      "Epoch 140, Loss: 0.5028511060044271\n",
      "Epoch 141, Loss: 0.5024804933282331\n",
      "Epoch 142, Loss: 0.5021182727396313\n",
      "Epoch 143, Loss: 0.5017581664990297\n",
      "Epoch 144, Loss: 0.5014050827892457\n",
      "Epoch 145, Loss: 0.501047675902349\n",
      "Epoch 146, Loss: 0.500690966247362\n",
      "Epoch 147, Loss: 0.5003476722262875\n",
      "Epoch 148, Loss: 0.5000068544871652\n",
      "Epoch 149, Loss: 0.49966338867073357\n",
      "Epoch 150, Loss: 0.4993280873087794\n",
      "Epoch 151, Loss: 0.49900257940428383\n",
      "Epoch 152, Loss: 0.4986865554458311\n",
      "Epoch 153, Loss: 0.49836949389755036\n",
      "Epoch 154, Loss: 0.49805375636713634\n",
      "Epoch 155, Loss: 0.49773873165141763\n",
      "Epoch 156, Loss: 0.49742224924400874\n",
      "Epoch 157, Loss: 0.49711425826812494\n",
      "Epoch 158, Loss: 0.4968010973402242\n",
      "Epoch 159, Loss: 0.4964901399315246\n",
      "Epoch 160, Loss: 0.49617675026847124\n",
      "Epoch 161, Loss: 0.4958670390985117\n",
      "Epoch 162, Loss: 0.4955578109301908\n",
      "Epoch 163, Loss: 0.4952510223984696\n",
      "Epoch 164, Loss: 0.49494577484111674\n",
      "Epoch 165, Loss: 0.49464511374066544\n",
      "Epoch 166, Loss: 0.49434383269975535\n",
      "Epoch 167, Loss: 0.49404631909698943\n",
      "Epoch 168, Loss: 0.49374530645143583\n",
      "Epoch 169, Loss: 0.49345064806438055\n",
      "Epoch 170, Loss: 0.49317001490258333\n",
      "Epoch 171, Loss: 0.4928927591216884\n",
      "Epoch 172, Loss: 0.4926138499628376\n",
      "Epoch 173, Loss: 0.49233528219511824\n",
      "Epoch 174, Loss: 0.49205852514110515\n",
      "Epoch 175, Loss: 0.4917773399684819\n",
      "Epoch 176, Loss: 0.49149629710928233\n",
      "Epoch 177, Loss: 0.4912165647150265\n",
      "Epoch 178, Loss: 0.49094181765497674\n",
      "Epoch 179, Loss: 0.49066901222163783\n",
      "Epoch 180, Loss: 0.490393928005003\n",
      "Epoch 181, Loss: 0.4901079113527018\n",
      "Epoch 182, Loss: 0.48981984154671765\n",
      "Epoch 183, Loss: 0.4895234443341763\n",
      "Epoch 184, Loss: 0.48921459258368105\n",
      "Epoch 185, Loss: 0.4889145977883947\n",
      "Epoch 186, Loss: 0.4886175142174919\n",
      "Epoch 187, Loss: 0.4883282295514753\n",
      "Epoch 188, Loss: 0.4880435212302353\n",
      "Epoch 189, Loss: 0.48775699700499114\n",
      "Epoch 190, Loss: 0.48748008613994775\n",
      "Epoch 191, Loss: 0.4872132179171775\n",
      "Epoch 192, Loss: 0.48694738158534456\n",
      "Epoch 193, Loss: 0.48668121634032785\n",
      "Epoch 194, Loss: 0.4864280322301768\n",
      "Epoch 195, Loss: 0.486178532155915\n",
      "Epoch 196, Loss: 0.4859301919172944\n",
      "Epoch 197, Loss: 0.48568442460989747\n",
      "Epoch 198, Loss: 0.48544224711238787\n",
      "Epoch 199, Loss: 0.48519729219516017\n",
      "Epoch 200, Loss: 0.4849575446659644\n",
      "Epoch 201, Loss: 0.4847146197220469\n",
      "Epoch 202, Loss: 0.48447155759133576\n",
      "Epoch 203, Loss: 0.48422783492589483\n",
      "Epoch 204, Loss: 0.48397707745066637\n",
      "Epoch 205, Loss: 0.483737926529631\n",
      "Epoch 206, Loss: 0.4835020782648923\n",
      "Epoch 207, Loss: 0.48327083252052233\n",
      "Epoch 208, Loss: 0.4830508282762034\n",
      "Epoch 209, Loss: 0.48283269895503794\n",
      "Epoch 210, Loss: 0.48261215534620283\n",
      "Epoch 211, Loss: 0.48240746542225255\n",
      "Epoch 212, Loss: 0.4822187420754367\n",
      "Epoch 213, Loss: 0.48202997575341067\n",
      "Epoch 214, Loss: 0.4818441197783379\n",
      "Epoch 215, Loss: 0.4816502256898402\n",
      "Epoch 216, Loss: 0.4814589616492725\n",
      "Epoch 217, Loss: 0.48127338055379854\n",
      "Epoch 218, Loss: 0.4810898650639108\n",
      "Epoch 219, Loss: 0.48091365405597575\n",
      "Epoch 220, Loss: 0.480720996546175\n",
      "Epoch 221, Loss: 0.48054865040929456\n",
      "Epoch 222, Loss: 0.4803774677186426\n",
      "Epoch 223, Loss: 0.4801885671816001\n",
      "Epoch 224, Loss: 0.48000935557804647\n",
      "Epoch 225, Loss: 0.4798350303004208\n",
      "Epoch 226, Loss: 0.47967842665052907\n",
      "Epoch 227, Loss: 0.47947854451894606\n",
      "Epoch 228, Loss: 0.47928199900429136\n",
      "Epoch 229, Loss: 0.4791424752085709\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2711346349645219\n",
      "Valence RMSE: 0.2520026964220803\n",
      "Arousal RMSE: 0.2890028054374039\n",
      "Test R^2 score: tensor([0.2304, 0.1730], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1388, 0.0746], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.20168226394017974\n",
      "Num of epochs: 230\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Epoch 121, Loss: 0.5116998508836337\n",
      "Epoch 122, Loss: 0.5111129902664598\n",
      "Epoch 123, Loss: 0.5105405741436312\n",
      "Epoch 124, Loss: 0.5099849010083686\n",
      "Epoch 125, Loss: 0.5094623759868415\n",
      "Epoch 126, Loss: 0.5089652841754122\n",
      "Epoch 127, Loss: 0.5084717799132713\n",
      "Epoch 128, Loss: 0.5079809643002384\n",
      "Epoch 129, Loss: 0.507507731608986\n",
      "Epoch 130, Loss: 0.5070515727009828\n",
      "Epoch 131, Loss: 0.5065962678643066\n",
      "Epoch 132, Loss: 0.5061448812256618\n",
      "Epoch 133, Loss: 0.5057069703510908\n",
      "Epoch 134, Loss: 0.5052756693952567\n",
      "Epoch 135, Loss: 0.5048445017462813\n",
      "Epoch 136, Loss: 0.504425284231979\n",
      "Epoch 137, Loss: 0.504022717877823\n",
      "Epoch 138, Loss: 0.5036312802042249\n",
      "Epoch 139, Loss: 0.5032368731068207\n",
      "Epoch 140, Loss: 0.5028511060044271\n",
      "Epoch 141, Loss: 0.5024804933282331\n",
      "Epoch 142, Loss: 0.5021182727396313\n",
      "Epoch 143, Loss: 0.5017581664990297\n",
      "Epoch 144, Loss: 0.5014050827892457\n",
      "Epoch 145, Loss: 0.501047675902349\n",
      "Epoch 146, Loss: 0.500690966247362\n",
      "Epoch 147, Loss: 0.5003476722262875\n",
      "Epoch 148, Loss: 0.5000068544871652\n",
      "Epoch 149, Loss: 0.49966338867073357\n",
      "Epoch 150, Loss: 0.4993280873087794\n",
      "Epoch 151, Loss: 0.49900257940428383\n",
      "Epoch 152, Loss: 0.4986865554458311\n",
      "Epoch 153, Loss: 0.49836949389755036\n",
      "Epoch 154, Loss: 0.49805375636713634\n",
      "Epoch 155, Loss: 0.49773873165141763\n",
      "Epoch 156, Loss: 0.49742224924400874\n",
      "Epoch 157, Loss: 0.49711425826812494\n",
      "Epoch 158, Loss: 0.4968010973402242\n",
      "Epoch 159, Loss: 0.4964901399315246\n",
      "Epoch 160, Loss: 0.49617675026847124\n",
      "Epoch 161, Loss: 0.4958670390985117\n",
      "Epoch 162, Loss: 0.4955578109301908\n",
      "Epoch 163, Loss: 0.4952510223984696\n",
      "Epoch 164, Loss: 0.49494577484111674\n",
      "Epoch 165, Loss: 0.49464511374066544\n",
      "Epoch 166, Loss: 0.49434383269975535\n",
      "Epoch 167, Loss: 0.49404631909698943\n",
      "Epoch 168, Loss: 0.49374530645143583\n",
      "Epoch 169, Loss: 0.49345064806438055\n",
      "Epoch 170, Loss: 0.49317001490258333\n",
      "Epoch 171, Loss: 0.4928927591216884\n",
      "Epoch 172, Loss: 0.4926138499628376\n",
      "Epoch 173, Loss: 0.49233528219511824\n",
      "Epoch 174, Loss: 0.49205852514110515\n",
      "Epoch 175, Loss: 0.4917773399684819\n",
      "Epoch 176, Loss: 0.49149629710928233\n",
      "Epoch 177, Loss: 0.4912165647150265\n",
      "Epoch 178, Loss: 0.49094181765497674\n",
      "Epoch 179, Loss: 0.49066901222163783\n",
      "Epoch 180, Loss: 0.490393928005003\n",
      "Epoch 181, Loss: 0.4901079113527018\n",
      "Epoch 182, Loss: 0.48981984154671765\n",
      "Epoch 183, Loss: 0.4895234443341763\n",
      "Epoch 184, Loss: 0.48921459258368105\n",
      "Epoch 185, Loss: 0.4889145977883947\n",
      "Epoch 186, Loss: 0.4886175142174919\n",
      "Epoch 187, Loss: 0.4883282295514753\n",
      "Epoch 188, Loss: 0.4880435212302353\n",
      "Epoch 189, Loss: 0.48775699700499114\n",
      "Epoch 190, Loss: 0.48748008613994775\n",
      "Epoch 191, Loss: 0.4872132179171775\n",
      "Epoch 192, Loss: 0.48694738158534456\n",
      "Epoch 193, Loss: 0.48668121634032785\n",
      "Epoch 194, Loss: 0.4864280322301768\n",
      "Epoch 195, Loss: 0.486178532155915\n",
      "Epoch 196, Loss: 0.4859301919172944\n",
      "Epoch 197, Loss: 0.48568442460989747\n",
      "Epoch 198, Loss: 0.48544224711238787\n",
      "Epoch 199, Loss: 0.48519729219516017\n",
      "Epoch 200, Loss: 0.4849575446659644\n",
      "Epoch 201, Loss: 0.4847146197220469\n",
      "Epoch 202, Loss: 0.48447155759133576\n",
      "Epoch 203, Loss: 0.48422783492589483\n",
      "Epoch 204, Loss: 0.48397707745066637\n",
      "Epoch 205, Loss: 0.483737926529631\n",
      "Epoch 206, Loss: 0.4835020782648923\n",
      "Epoch 207, Loss: 0.48327083252052233\n",
      "Epoch 208, Loss: 0.4830508282762034\n",
      "Epoch 209, Loss: 0.48283269895503794\n",
      "Epoch 210, Loss: 0.48261215534620283\n",
      "Epoch 211, Loss: 0.48240746542225255\n",
      "Epoch 212, Loss: 0.4822187420754367\n",
      "Epoch 213, Loss: 0.48202997575341067\n",
      "Epoch 214, Loss: 0.4818441197783379\n",
      "Epoch 215, Loss: 0.4816502256898402\n",
      "Epoch 216, Loss: 0.4814589616492725\n",
      "Epoch 217, Loss: 0.48127338055379854\n",
      "Epoch 218, Loss: 0.4810898650639108\n",
      "Epoch 219, Loss: 0.48091365405597575\n",
      "Epoch 220, Loss: 0.480720996546175\n",
      "Epoch 221, Loss: 0.48054865040929456\n",
      "Epoch 222, Loss: 0.4803774677186426\n",
      "Epoch 223, Loss: 0.4801885671816001\n",
      "Epoch 224, Loss: 0.48000935557804647\n",
      "Epoch 225, Loss: 0.4798350303004208\n",
      "Epoch 226, Loss: 0.47967842665052907\n",
      "Epoch 227, Loss: 0.47947854451894606\n",
      "Epoch 228, Loss: 0.47928199900429136\n",
      "Epoch 229, Loss: 0.4791424752085709\n",
      "Epoch 230, Loss: 0.4789254904984497\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2715326102538472\n",
      "Valence RMSE: 0.2523026042674332\n",
      "Arousal RMSE: 0.28948801830543247\n",
      "Test R^2 score: tensor([0.2285, 0.1702], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1368, 0.0715], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.1993761372674201\n",
      "Num of epochs: 231\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Epoch 121, Loss: 0.5116998508836337\n",
      "Epoch 122, Loss: 0.5111129902664598\n",
      "Epoch 123, Loss: 0.5105405741436312\n",
      "Epoch 124, Loss: 0.5099849010083686\n",
      "Epoch 125, Loss: 0.5094623759868415\n",
      "Epoch 126, Loss: 0.5089652841754122\n",
      "Epoch 127, Loss: 0.5084717799132713\n",
      "Epoch 128, Loss: 0.5079809643002384\n",
      "Epoch 129, Loss: 0.507507731608986\n",
      "Epoch 130, Loss: 0.5070515727009828\n",
      "Epoch 131, Loss: 0.5065962678643066\n",
      "Epoch 132, Loss: 0.5061448812256618\n",
      "Epoch 133, Loss: 0.5057069703510908\n",
      "Epoch 134, Loss: 0.5052756693952567\n",
      "Epoch 135, Loss: 0.5048445017462813\n",
      "Epoch 136, Loss: 0.504425284231979\n",
      "Epoch 137, Loss: 0.504022717877823\n",
      "Epoch 138, Loss: 0.5036312802042249\n",
      "Epoch 139, Loss: 0.5032368731068207\n",
      "Epoch 140, Loss: 0.5028511060044271\n",
      "Epoch 141, Loss: 0.5024804933282331\n",
      "Epoch 142, Loss: 0.5021182727396313\n",
      "Epoch 143, Loss: 0.5017581664990297\n",
      "Epoch 144, Loss: 0.5014050827892457\n",
      "Epoch 145, Loss: 0.501047675902349\n",
      "Epoch 146, Loss: 0.500690966247362\n",
      "Epoch 147, Loss: 0.5003476722262875\n",
      "Epoch 148, Loss: 0.5000068544871652\n",
      "Epoch 149, Loss: 0.49966338867073357\n",
      "Epoch 150, Loss: 0.4993280873087794\n",
      "Epoch 151, Loss: 0.49900257940428383\n",
      "Epoch 152, Loss: 0.4986865554458311\n",
      "Epoch 153, Loss: 0.49836949389755036\n",
      "Epoch 154, Loss: 0.49805375636713634\n",
      "Epoch 155, Loss: 0.49773873165141763\n",
      "Epoch 156, Loss: 0.49742224924400874\n",
      "Epoch 157, Loss: 0.49711425826812494\n",
      "Epoch 158, Loss: 0.4968010973402242\n",
      "Epoch 159, Loss: 0.4964901399315246\n",
      "Epoch 160, Loss: 0.49617675026847124\n",
      "Epoch 161, Loss: 0.4958670390985117\n",
      "Epoch 162, Loss: 0.4955578109301908\n",
      "Epoch 163, Loss: 0.4952510223984696\n",
      "Epoch 164, Loss: 0.49494577484111674\n",
      "Epoch 165, Loss: 0.49464511374066544\n",
      "Epoch 166, Loss: 0.49434383269975535\n",
      "Epoch 167, Loss: 0.49404631909698943\n",
      "Epoch 168, Loss: 0.49374530645143583\n",
      "Epoch 169, Loss: 0.49345064806438055\n",
      "Epoch 170, Loss: 0.49317001490258333\n",
      "Epoch 171, Loss: 0.4928927591216884\n",
      "Epoch 172, Loss: 0.4926138499628376\n",
      "Epoch 173, Loss: 0.49233528219511824\n",
      "Epoch 174, Loss: 0.49205852514110515\n",
      "Epoch 175, Loss: 0.4917773399684819\n",
      "Epoch 176, Loss: 0.49149629710928233\n",
      "Epoch 177, Loss: 0.4912165647150265\n",
      "Epoch 178, Loss: 0.49094181765497674\n",
      "Epoch 179, Loss: 0.49066901222163783\n",
      "Epoch 180, Loss: 0.490393928005003\n",
      "Epoch 181, Loss: 0.4901079113527018\n",
      "Epoch 182, Loss: 0.48981984154671765\n",
      "Epoch 183, Loss: 0.4895234443341763\n",
      "Epoch 184, Loss: 0.48921459258368105\n",
      "Epoch 185, Loss: 0.4889145977883947\n",
      "Epoch 186, Loss: 0.4886175142174919\n",
      "Epoch 187, Loss: 0.4883282295514753\n",
      "Epoch 188, Loss: 0.4880435212302353\n",
      "Epoch 189, Loss: 0.48775699700499114\n",
      "Epoch 190, Loss: 0.48748008613994775\n",
      "Epoch 191, Loss: 0.4872132179171775\n",
      "Epoch 192, Loss: 0.48694738158534456\n",
      "Epoch 193, Loss: 0.48668121634032785\n",
      "Epoch 194, Loss: 0.4864280322301768\n",
      "Epoch 195, Loss: 0.486178532155915\n",
      "Epoch 196, Loss: 0.4859301919172944\n",
      "Epoch 197, Loss: 0.48568442460989747\n",
      "Epoch 198, Loss: 0.48544224711238787\n",
      "Epoch 199, Loss: 0.48519729219516017\n",
      "Epoch 200, Loss: 0.4849575446659644\n",
      "Epoch 201, Loss: 0.4847146197220469\n",
      "Epoch 202, Loss: 0.48447155759133576\n",
      "Epoch 203, Loss: 0.48422783492589483\n",
      "Epoch 204, Loss: 0.48397707745066637\n",
      "Epoch 205, Loss: 0.483737926529631\n",
      "Epoch 206, Loss: 0.4835020782648923\n",
      "Epoch 207, Loss: 0.48327083252052233\n",
      "Epoch 208, Loss: 0.4830508282762034\n",
      "Epoch 209, Loss: 0.48283269895503794\n",
      "Epoch 210, Loss: 0.48261215534620283\n",
      "Epoch 211, Loss: 0.48240746542225255\n",
      "Epoch 212, Loss: 0.4822187420754367\n",
      "Epoch 213, Loss: 0.48202997575341067\n",
      "Epoch 214, Loss: 0.4818441197783379\n",
      "Epoch 215, Loss: 0.4816502256898402\n",
      "Epoch 216, Loss: 0.4814589616492725\n",
      "Epoch 217, Loss: 0.48127338055379854\n",
      "Epoch 218, Loss: 0.4810898650639108\n",
      "Epoch 219, Loss: 0.48091365405597575\n",
      "Epoch 220, Loss: 0.480720996546175\n",
      "Epoch 221, Loss: 0.48054865040929456\n",
      "Epoch 222, Loss: 0.4803774677186426\n",
      "Epoch 223, Loss: 0.4801885671816001\n",
      "Epoch 224, Loss: 0.48000935557804647\n",
      "Epoch 225, Loss: 0.4798350303004208\n",
      "Epoch 226, Loss: 0.47967842665052907\n",
      "Epoch 227, Loss: 0.47947854451894606\n",
      "Epoch 228, Loss: 0.47928199900429136\n",
      "Epoch 229, Loss: 0.4791424752085709\n",
      "Epoch 230, Loss: 0.4789254904984497\n",
      "Epoch 231, Loss: 0.4787155823499749\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.27167729595980783\n",
      "Valence RMSE: 0.25230739935795055\n",
      "Arousal RMSE: 0.28975521135830107\n",
      "Test R^2 score: tensor([0.2285, 0.1687], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1367, 0.0698], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.19859523503940774\n",
      "Num of epochs: 232\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Epoch 121, Loss: 0.5116998508836337\n",
      "Epoch 122, Loss: 0.5111129902664598\n",
      "Epoch 123, Loss: 0.5105405741436312\n",
      "Epoch 124, Loss: 0.5099849010083686\n",
      "Epoch 125, Loss: 0.5094623759868415\n",
      "Epoch 126, Loss: 0.5089652841754122\n",
      "Epoch 127, Loss: 0.5084717799132713\n",
      "Epoch 128, Loss: 0.5079809643002384\n",
      "Epoch 129, Loss: 0.507507731608986\n",
      "Epoch 130, Loss: 0.5070515727009828\n",
      "Epoch 131, Loss: 0.5065962678643066\n",
      "Epoch 132, Loss: 0.5061448812256618\n",
      "Epoch 133, Loss: 0.5057069703510908\n",
      "Epoch 134, Loss: 0.5052756693952567\n",
      "Epoch 135, Loss: 0.5048445017462813\n",
      "Epoch 136, Loss: 0.504425284231979\n",
      "Epoch 137, Loss: 0.504022717877823\n",
      "Epoch 138, Loss: 0.5036312802042249\n",
      "Epoch 139, Loss: 0.5032368731068207\n",
      "Epoch 140, Loss: 0.5028511060044271\n",
      "Epoch 141, Loss: 0.5024804933282331\n",
      "Epoch 142, Loss: 0.5021182727396313\n",
      "Epoch 143, Loss: 0.5017581664990297\n",
      "Epoch 144, Loss: 0.5014050827892457\n",
      "Epoch 145, Loss: 0.501047675902349\n",
      "Epoch 146, Loss: 0.500690966247362\n",
      "Epoch 147, Loss: 0.5003476722262875\n",
      "Epoch 148, Loss: 0.5000068544871652\n",
      "Epoch 149, Loss: 0.49966338867073357\n",
      "Epoch 150, Loss: 0.4993280873087794\n",
      "Epoch 151, Loss: 0.49900257940428383\n",
      "Epoch 152, Loss: 0.4986865554458311\n",
      "Epoch 153, Loss: 0.49836949389755036\n",
      "Epoch 154, Loss: 0.49805375636713634\n",
      "Epoch 155, Loss: 0.49773873165141763\n",
      "Epoch 156, Loss: 0.49742224924400874\n",
      "Epoch 157, Loss: 0.49711425826812494\n",
      "Epoch 158, Loss: 0.4968010973402242\n",
      "Epoch 159, Loss: 0.4964901399315246\n",
      "Epoch 160, Loss: 0.49617675026847124\n",
      "Epoch 161, Loss: 0.4958670390985117\n",
      "Epoch 162, Loss: 0.4955578109301908\n",
      "Epoch 163, Loss: 0.4952510223984696\n",
      "Epoch 164, Loss: 0.49494577484111674\n",
      "Epoch 165, Loss: 0.49464511374066544\n",
      "Epoch 166, Loss: 0.49434383269975535\n",
      "Epoch 167, Loss: 0.49404631909698943\n",
      "Epoch 168, Loss: 0.49374530645143583\n",
      "Epoch 169, Loss: 0.49345064806438055\n",
      "Epoch 170, Loss: 0.49317001490258333\n",
      "Epoch 171, Loss: 0.4928927591216884\n",
      "Epoch 172, Loss: 0.4926138499628376\n",
      "Epoch 173, Loss: 0.49233528219511824\n",
      "Epoch 174, Loss: 0.49205852514110515\n",
      "Epoch 175, Loss: 0.4917773399684819\n",
      "Epoch 176, Loss: 0.49149629710928233\n",
      "Epoch 177, Loss: 0.4912165647150265\n",
      "Epoch 178, Loss: 0.49094181765497674\n",
      "Epoch 179, Loss: 0.49066901222163783\n",
      "Epoch 180, Loss: 0.490393928005003\n",
      "Epoch 181, Loss: 0.4901079113527018\n",
      "Epoch 182, Loss: 0.48981984154671765\n",
      "Epoch 183, Loss: 0.4895234443341763\n",
      "Epoch 184, Loss: 0.48921459258368105\n",
      "Epoch 185, Loss: 0.4889145977883947\n",
      "Epoch 186, Loss: 0.4886175142174919\n",
      "Epoch 187, Loss: 0.4883282295514753\n",
      "Epoch 188, Loss: 0.4880435212302353\n",
      "Epoch 189, Loss: 0.48775699700499114\n",
      "Epoch 190, Loss: 0.48748008613994775\n",
      "Epoch 191, Loss: 0.4872132179171775\n",
      "Epoch 192, Loss: 0.48694738158534456\n",
      "Epoch 193, Loss: 0.48668121634032785\n",
      "Epoch 194, Loss: 0.4864280322301768\n",
      "Epoch 195, Loss: 0.486178532155915\n",
      "Epoch 196, Loss: 0.4859301919172944\n",
      "Epoch 197, Loss: 0.48568442460989747\n",
      "Epoch 198, Loss: 0.48544224711238787\n",
      "Epoch 199, Loss: 0.48519729219516017\n",
      "Epoch 200, Loss: 0.4849575446659644\n",
      "Epoch 201, Loss: 0.4847146197220469\n",
      "Epoch 202, Loss: 0.48447155759133576\n",
      "Epoch 203, Loss: 0.48422783492589483\n",
      "Epoch 204, Loss: 0.48397707745066637\n",
      "Epoch 205, Loss: 0.483737926529631\n",
      "Epoch 206, Loss: 0.4835020782648923\n",
      "Epoch 207, Loss: 0.48327083252052233\n",
      "Epoch 208, Loss: 0.4830508282762034\n",
      "Epoch 209, Loss: 0.48283269895503794\n",
      "Epoch 210, Loss: 0.48261215534620283\n",
      "Epoch 211, Loss: 0.48240746542225255\n",
      "Epoch 212, Loss: 0.4822187420754367\n",
      "Epoch 213, Loss: 0.48202997575341067\n",
      "Epoch 214, Loss: 0.4818441197783379\n",
      "Epoch 215, Loss: 0.4816502256898402\n",
      "Epoch 216, Loss: 0.4814589616492725\n",
      "Epoch 217, Loss: 0.48127338055379854\n",
      "Epoch 218, Loss: 0.4810898650639108\n",
      "Epoch 219, Loss: 0.48091365405597575\n",
      "Epoch 220, Loss: 0.480720996546175\n",
      "Epoch 221, Loss: 0.48054865040929456\n",
      "Epoch 222, Loss: 0.4803774677186426\n",
      "Epoch 223, Loss: 0.4801885671816001\n",
      "Epoch 224, Loss: 0.48000935557804647\n",
      "Epoch 225, Loss: 0.4798350303004208\n",
      "Epoch 226, Loss: 0.47967842665052907\n",
      "Epoch 227, Loss: 0.47947854451894606\n",
      "Epoch 228, Loss: 0.47928199900429136\n",
      "Epoch 229, Loss: 0.4791424752085709\n",
      "Epoch 230, Loss: 0.4789254904984497\n",
      "Epoch 231, Loss: 0.4787155823499749\n",
      "Epoch 232, Loss: 0.4785186611779873\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.271639877273156\n",
      "Valence RMSE: 0.2521090058025467\n",
      "Arousal RMSE: 0.2898577151693604\n",
      "Test R^2 score: tensor([0.2297, 0.1681], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1381, 0.0691], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.19890748501378464\n",
      "Num of epochs: 233\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Epoch 121, Loss: 0.5116998508836337\n",
      "Epoch 122, Loss: 0.5111129902664598\n",
      "Epoch 123, Loss: 0.5105405741436312\n",
      "Epoch 124, Loss: 0.5099849010083686\n",
      "Epoch 125, Loss: 0.5094623759868415\n",
      "Epoch 126, Loss: 0.5089652841754122\n",
      "Epoch 127, Loss: 0.5084717799132713\n",
      "Epoch 128, Loss: 0.5079809643002384\n",
      "Epoch 129, Loss: 0.507507731608986\n",
      "Epoch 130, Loss: 0.5070515727009828\n",
      "Epoch 131, Loss: 0.5065962678643066\n",
      "Epoch 132, Loss: 0.5061448812256618\n",
      "Epoch 133, Loss: 0.5057069703510908\n",
      "Epoch 134, Loss: 0.5052756693952567\n",
      "Epoch 135, Loss: 0.5048445017462813\n",
      "Epoch 136, Loss: 0.504425284231979\n",
      "Epoch 137, Loss: 0.504022717877823\n",
      "Epoch 138, Loss: 0.5036312802042249\n",
      "Epoch 139, Loss: 0.5032368731068207\n",
      "Epoch 140, Loss: 0.5028511060044271\n",
      "Epoch 141, Loss: 0.5024804933282331\n",
      "Epoch 142, Loss: 0.5021182727396313\n",
      "Epoch 143, Loss: 0.5017581664990297\n",
      "Epoch 144, Loss: 0.5014050827892457\n",
      "Epoch 145, Loss: 0.501047675902349\n",
      "Epoch 146, Loss: 0.500690966247362\n",
      "Epoch 147, Loss: 0.5003476722262875\n",
      "Epoch 148, Loss: 0.5000068544871652\n",
      "Epoch 149, Loss: 0.49966338867073357\n",
      "Epoch 150, Loss: 0.4993280873087794\n",
      "Epoch 151, Loss: 0.49900257940428383\n",
      "Epoch 152, Loss: 0.4986865554458311\n",
      "Epoch 153, Loss: 0.49836949389755036\n",
      "Epoch 154, Loss: 0.49805375636713634\n",
      "Epoch 155, Loss: 0.49773873165141763\n",
      "Epoch 156, Loss: 0.49742224924400874\n",
      "Epoch 157, Loss: 0.49711425826812494\n",
      "Epoch 158, Loss: 0.4968010973402242\n",
      "Epoch 159, Loss: 0.4964901399315246\n",
      "Epoch 160, Loss: 0.49617675026847124\n",
      "Epoch 161, Loss: 0.4958670390985117\n",
      "Epoch 162, Loss: 0.4955578109301908\n",
      "Epoch 163, Loss: 0.4952510223984696\n",
      "Epoch 164, Loss: 0.49494577484111674\n",
      "Epoch 165, Loss: 0.49464511374066544\n",
      "Epoch 166, Loss: 0.49434383269975535\n",
      "Epoch 167, Loss: 0.49404631909698943\n",
      "Epoch 168, Loss: 0.49374530645143583\n",
      "Epoch 169, Loss: 0.49345064806438055\n",
      "Epoch 170, Loss: 0.49317001490258333\n",
      "Epoch 171, Loss: 0.4928927591216884\n",
      "Epoch 172, Loss: 0.4926138499628376\n",
      "Epoch 173, Loss: 0.49233528219511824\n",
      "Epoch 174, Loss: 0.49205852514110515\n",
      "Epoch 175, Loss: 0.4917773399684819\n",
      "Epoch 176, Loss: 0.49149629710928233\n",
      "Epoch 177, Loss: 0.4912165647150265\n",
      "Epoch 178, Loss: 0.49094181765497674\n",
      "Epoch 179, Loss: 0.49066901222163783\n",
      "Epoch 180, Loss: 0.490393928005003\n",
      "Epoch 181, Loss: 0.4901079113527018\n",
      "Epoch 182, Loss: 0.48981984154671765\n",
      "Epoch 183, Loss: 0.4895234443341763\n",
      "Epoch 184, Loss: 0.48921459258368105\n",
      "Epoch 185, Loss: 0.4889145977883947\n",
      "Epoch 186, Loss: 0.4886175142174919\n",
      "Epoch 187, Loss: 0.4883282295514753\n",
      "Epoch 188, Loss: 0.4880435212302353\n",
      "Epoch 189, Loss: 0.48775699700499114\n",
      "Epoch 190, Loss: 0.48748008613994775\n",
      "Epoch 191, Loss: 0.4872132179171775\n",
      "Epoch 192, Loss: 0.48694738158534456\n",
      "Epoch 193, Loss: 0.48668121634032785\n",
      "Epoch 194, Loss: 0.4864280322301768\n",
      "Epoch 195, Loss: 0.486178532155915\n",
      "Epoch 196, Loss: 0.4859301919172944\n",
      "Epoch 197, Loss: 0.48568442460989747\n",
      "Epoch 198, Loss: 0.48544224711238787\n",
      "Epoch 199, Loss: 0.48519729219516017\n",
      "Epoch 200, Loss: 0.4849575446659644\n",
      "Epoch 201, Loss: 0.4847146197220469\n",
      "Epoch 202, Loss: 0.48447155759133576\n",
      "Epoch 203, Loss: 0.48422783492589483\n",
      "Epoch 204, Loss: 0.48397707745066637\n",
      "Epoch 205, Loss: 0.483737926529631\n",
      "Epoch 206, Loss: 0.4835020782648923\n",
      "Epoch 207, Loss: 0.48327083252052233\n",
      "Epoch 208, Loss: 0.4830508282762034\n",
      "Epoch 209, Loss: 0.48283269895503794\n",
      "Epoch 210, Loss: 0.48261215534620283\n",
      "Epoch 211, Loss: 0.48240746542225255\n",
      "Epoch 212, Loss: 0.4822187420754367\n",
      "Epoch 213, Loss: 0.48202997575341067\n",
      "Epoch 214, Loss: 0.4818441197783379\n",
      "Epoch 215, Loss: 0.4816502256898402\n",
      "Epoch 216, Loss: 0.4814589616492725\n",
      "Epoch 217, Loss: 0.48127338055379854\n",
      "Epoch 218, Loss: 0.4810898650639108\n",
      "Epoch 219, Loss: 0.48091365405597575\n",
      "Epoch 220, Loss: 0.480720996546175\n",
      "Epoch 221, Loss: 0.48054865040929456\n",
      "Epoch 222, Loss: 0.4803774677186426\n",
      "Epoch 223, Loss: 0.4801885671816001\n",
      "Epoch 224, Loss: 0.48000935557804647\n",
      "Epoch 225, Loss: 0.4798350303004208\n",
      "Epoch 226, Loss: 0.47967842665052907\n",
      "Epoch 227, Loss: 0.47947854451894606\n",
      "Epoch 228, Loss: 0.47928199900429136\n",
      "Epoch 229, Loss: 0.4791424752085709\n",
      "Epoch 230, Loss: 0.4789254904984497\n",
      "Epoch 231, Loss: 0.4787155823499749\n",
      "Epoch 232, Loss: 0.4785186611779873\n",
      "Epoch 233, Loss: 0.4783287773458093\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.27194471878595905\n",
      "Valence RMSE: 0.2523435256146273\n",
      "Arousal RMSE: 0.29022509407618186\n",
      "Test R^2 score: tensor([0.2283, 0.1660], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1365, 0.0668], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.19713554702265362\n",
      "Num of epochs: 234\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Epoch 121, Loss: 0.5116998508836337\n",
      "Epoch 122, Loss: 0.5111129902664598\n",
      "Epoch 123, Loss: 0.5105405741436312\n",
      "Epoch 124, Loss: 0.5099849010083686\n",
      "Epoch 125, Loss: 0.5094623759868415\n",
      "Epoch 126, Loss: 0.5089652841754122\n",
      "Epoch 127, Loss: 0.5084717799132713\n",
      "Epoch 128, Loss: 0.5079809643002384\n",
      "Epoch 129, Loss: 0.507507731608986\n",
      "Epoch 130, Loss: 0.5070515727009828\n",
      "Epoch 131, Loss: 0.5065962678643066\n",
      "Epoch 132, Loss: 0.5061448812256618\n",
      "Epoch 133, Loss: 0.5057069703510908\n",
      "Epoch 134, Loss: 0.5052756693952567\n",
      "Epoch 135, Loss: 0.5048445017462813\n",
      "Epoch 136, Loss: 0.504425284231979\n",
      "Epoch 137, Loss: 0.504022717877823\n",
      "Epoch 138, Loss: 0.5036312802042249\n",
      "Epoch 139, Loss: 0.5032368731068207\n",
      "Epoch 140, Loss: 0.5028511060044271\n",
      "Epoch 141, Loss: 0.5024804933282331\n",
      "Epoch 142, Loss: 0.5021182727396313\n",
      "Epoch 143, Loss: 0.5017581664990297\n",
      "Epoch 144, Loss: 0.5014050827892457\n",
      "Epoch 145, Loss: 0.501047675902349\n",
      "Epoch 146, Loss: 0.500690966247362\n",
      "Epoch 147, Loss: 0.5003476722262875\n",
      "Epoch 148, Loss: 0.5000068544871652\n",
      "Epoch 149, Loss: 0.49966338867073357\n",
      "Epoch 150, Loss: 0.4993280873087794\n",
      "Epoch 151, Loss: 0.49900257940428383\n",
      "Epoch 152, Loss: 0.4986865554458311\n",
      "Epoch 153, Loss: 0.49836949389755036\n",
      "Epoch 154, Loss: 0.49805375636713634\n",
      "Epoch 155, Loss: 0.49773873165141763\n",
      "Epoch 156, Loss: 0.49742224924400874\n",
      "Epoch 157, Loss: 0.49711425826812494\n",
      "Epoch 158, Loss: 0.4968010973402242\n",
      "Epoch 159, Loss: 0.4964901399315246\n",
      "Epoch 160, Loss: 0.49617675026847124\n",
      "Epoch 161, Loss: 0.4958670390985117\n",
      "Epoch 162, Loss: 0.4955578109301908\n",
      "Epoch 163, Loss: 0.4952510223984696\n",
      "Epoch 164, Loss: 0.49494577484111674\n",
      "Epoch 165, Loss: 0.49464511374066544\n",
      "Epoch 166, Loss: 0.49434383269975535\n",
      "Epoch 167, Loss: 0.49404631909698943\n",
      "Epoch 168, Loss: 0.49374530645143583\n",
      "Epoch 169, Loss: 0.49345064806438055\n",
      "Epoch 170, Loss: 0.49317001490258333\n",
      "Epoch 171, Loss: 0.4928927591216884\n",
      "Epoch 172, Loss: 0.4926138499628376\n",
      "Epoch 173, Loss: 0.49233528219511824\n",
      "Epoch 174, Loss: 0.49205852514110515\n",
      "Epoch 175, Loss: 0.4917773399684819\n",
      "Epoch 176, Loss: 0.49149629710928233\n",
      "Epoch 177, Loss: 0.4912165647150265\n",
      "Epoch 178, Loss: 0.49094181765497674\n",
      "Epoch 179, Loss: 0.49066901222163783\n",
      "Epoch 180, Loss: 0.490393928005003\n",
      "Epoch 181, Loss: 0.4901079113527018\n",
      "Epoch 182, Loss: 0.48981984154671765\n",
      "Epoch 183, Loss: 0.4895234443341763\n",
      "Epoch 184, Loss: 0.48921459258368105\n",
      "Epoch 185, Loss: 0.4889145977883947\n",
      "Epoch 186, Loss: 0.4886175142174919\n",
      "Epoch 187, Loss: 0.4883282295514753\n",
      "Epoch 188, Loss: 0.4880435212302353\n",
      "Epoch 189, Loss: 0.48775699700499114\n",
      "Epoch 190, Loss: 0.48748008613994775\n",
      "Epoch 191, Loss: 0.4872132179171775\n",
      "Epoch 192, Loss: 0.48694738158534456\n",
      "Epoch 193, Loss: 0.48668121634032785\n",
      "Epoch 194, Loss: 0.4864280322301768\n",
      "Epoch 195, Loss: 0.486178532155915\n",
      "Epoch 196, Loss: 0.4859301919172944\n",
      "Epoch 197, Loss: 0.48568442460989747\n",
      "Epoch 198, Loss: 0.48544224711238787\n",
      "Epoch 199, Loss: 0.48519729219516017\n",
      "Epoch 200, Loss: 0.4849575446659644\n",
      "Epoch 201, Loss: 0.4847146197220469\n",
      "Epoch 202, Loss: 0.48447155759133576\n",
      "Epoch 203, Loss: 0.48422783492589483\n",
      "Epoch 204, Loss: 0.48397707745066637\n",
      "Epoch 205, Loss: 0.483737926529631\n",
      "Epoch 206, Loss: 0.4835020782648923\n",
      "Epoch 207, Loss: 0.48327083252052233\n",
      "Epoch 208, Loss: 0.4830508282762034\n",
      "Epoch 209, Loss: 0.48283269895503794\n",
      "Epoch 210, Loss: 0.48261215534620283\n",
      "Epoch 211, Loss: 0.48240746542225255\n",
      "Epoch 212, Loss: 0.4822187420754367\n",
      "Epoch 213, Loss: 0.48202997575341067\n",
      "Epoch 214, Loss: 0.4818441197783379\n",
      "Epoch 215, Loss: 0.4816502256898402\n",
      "Epoch 216, Loss: 0.4814589616492725\n",
      "Epoch 217, Loss: 0.48127338055379854\n",
      "Epoch 218, Loss: 0.4810898650639108\n",
      "Epoch 219, Loss: 0.48091365405597575\n",
      "Epoch 220, Loss: 0.480720996546175\n",
      "Epoch 221, Loss: 0.48054865040929456\n",
      "Epoch 222, Loss: 0.4803774677186426\n",
      "Epoch 223, Loss: 0.4801885671816001\n",
      "Epoch 224, Loss: 0.48000935557804647\n",
      "Epoch 225, Loss: 0.4798350303004208\n",
      "Epoch 226, Loss: 0.47967842665052907\n",
      "Epoch 227, Loss: 0.47947854451894606\n",
      "Epoch 228, Loss: 0.47928199900429136\n",
      "Epoch 229, Loss: 0.4791424752085709\n",
      "Epoch 230, Loss: 0.4789254904984497\n",
      "Epoch 231, Loss: 0.4787155823499749\n",
      "Epoch 232, Loss: 0.4785186611779873\n",
      "Epoch 233, Loss: 0.4783287773458093\n",
      "Epoch 234, Loss: 0.4781774454711345\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.27158741494768274\n",
      "Valence RMSE: 0.2518749228626577\n",
      "Arousal RMSE: 0.28996287891531874\n",
      "Test R^2 score: tensor([0.2312, 0.1675], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1397, 0.0684], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.19932046552685673\n",
      "Num of epochs: 235\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Epoch 121, Loss: 0.5116998508836337\n",
      "Epoch 122, Loss: 0.5111129902664598\n",
      "Epoch 123, Loss: 0.5105405741436312\n",
      "Epoch 124, Loss: 0.5099849010083686\n",
      "Epoch 125, Loss: 0.5094623759868415\n",
      "Epoch 126, Loss: 0.5089652841754122\n",
      "Epoch 127, Loss: 0.5084717799132713\n",
      "Epoch 128, Loss: 0.5079809643002384\n",
      "Epoch 129, Loss: 0.507507731608986\n",
      "Epoch 130, Loss: 0.5070515727009828\n",
      "Epoch 131, Loss: 0.5065962678643066\n",
      "Epoch 132, Loss: 0.5061448812256618\n",
      "Epoch 133, Loss: 0.5057069703510908\n",
      "Epoch 134, Loss: 0.5052756693952567\n",
      "Epoch 135, Loss: 0.5048445017462813\n",
      "Epoch 136, Loss: 0.504425284231979\n",
      "Epoch 137, Loss: 0.504022717877823\n",
      "Epoch 138, Loss: 0.5036312802042249\n",
      "Epoch 139, Loss: 0.5032368731068207\n",
      "Epoch 140, Loss: 0.5028511060044271\n",
      "Epoch 141, Loss: 0.5024804933282331\n",
      "Epoch 142, Loss: 0.5021182727396313\n",
      "Epoch 143, Loss: 0.5017581664990297\n",
      "Epoch 144, Loss: 0.5014050827892457\n",
      "Epoch 145, Loss: 0.501047675902349\n",
      "Epoch 146, Loss: 0.500690966247362\n",
      "Epoch 147, Loss: 0.5003476722262875\n",
      "Epoch 148, Loss: 0.5000068544871652\n",
      "Epoch 149, Loss: 0.49966338867073357\n",
      "Epoch 150, Loss: 0.4993280873087794\n",
      "Epoch 151, Loss: 0.49900257940428383\n",
      "Epoch 152, Loss: 0.4986865554458311\n",
      "Epoch 153, Loss: 0.49836949389755036\n",
      "Epoch 154, Loss: 0.49805375636713634\n",
      "Epoch 155, Loss: 0.49773873165141763\n",
      "Epoch 156, Loss: 0.49742224924400874\n",
      "Epoch 157, Loss: 0.49711425826812494\n",
      "Epoch 158, Loss: 0.4968010973402242\n",
      "Epoch 159, Loss: 0.4964901399315246\n",
      "Epoch 160, Loss: 0.49617675026847124\n",
      "Epoch 161, Loss: 0.4958670390985117\n",
      "Epoch 162, Loss: 0.4955578109301908\n",
      "Epoch 163, Loss: 0.4952510223984696\n",
      "Epoch 164, Loss: 0.49494577484111674\n",
      "Epoch 165, Loss: 0.49464511374066544\n",
      "Epoch 166, Loss: 0.49434383269975535\n",
      "Epoch 167, Loss: 0.49404631909698943\n",
      "Epoch 168, Loss: 0.49374530645143583\n",
      "Epoch 169, Loss: 0.49345064806438055\n",
      "Epoch 170, Loss: 0.49317001490258333\n",
      "Epoch 171, Loss: 0.4928927591216884\n",
      "Epoch 172, Loss: 0.4926138499628376\n",
      "Epoch 173, Loss: 0.49233528219511824\n",
      "Epoch 174, Loss: 0.49205852514110515\n",
      "Epoch 175, Loss: 0.4917773399684819\n",
      "Epoch 176, Loss: 0.49149629710928233\n",
      "Epoch 177, Loss: 0.4912165647150265\n",
      "Epoch 178, Loss: 0.49094181765497674\n",
      "Epoch 179, Loss: 0.49066901222163783\n",
      "Epoch 180, Loss: 0.490393928005003\n",
      "Epoch 181, Loss: 0.4901079113527018\n",
      "Epoch 182, Loss: 0.48981984154671765\n",
      "Epoch 183, Loss: 0.4895234443341763\n",
      "Epoch 184, Loss: 0.48921459258368105\n",
      "Epoch 185, Loss: 0.4889145977883947\n",
      "Epoch 186, Loss: 0.4886175142174919\n",
      "Epoch 187, Loss: 0.4883282295514753\n",
      "Epoch 188, Loss: 0.4880435212302353\n",
      "Epoch 189, Loss: 0.48775699700499114\n",
      "Epoch 190, Loss: 0.48748008613994775\n",
      "Epoch 191, Loss: 0.4872132179171775\n",
      "Epoch 192, Loss: 0.48694738158534456\n",
      "Epoch 193, Loss: 0.48668121634032785\n",
      "Epoch 194, Loss: 0.4864280322301768\n",
      "Epoch 195, Loss: 0.486178532155915\n",
      "Epoch 196, Loss: 0.4859301919172944\n",
      "Epoch 197, Loss: 0.48568442460989747\n",
      "Epoch 198, Loss: 0.48544224711238787\n",
      "Epoch 199, Loss: 0.48519729219516017\n",
      "Epoch 200, Loss: 0.4849575446659644\n",
      "Epoch 201, Loss: 0.4847146197220469\n",
      "Epoch 202, Loss: 0.48447155759133576\n",
      "Epoch 203, Loss: 0.48422783492589483\n",
      "Epoch 204, Loss: 0.48397707745066637\n",
      "Epoch 205, Loss: 0.483737926529631\n",
      "Epoch 206, Loss: 0.4835020782648923\n",
      "Epoch 207, Loss: 0.48327083252052233\n",
      "Epoch 208, Loss: 0.4830508282762034\n",
      "Epoch 209, Loss: 0.48283269895503794\n",
      "Epoch 210, Loss: 0.48261215534620283\n",
      "Epoch 211, Loss: 0.48240746542225255\n",
      "Epoch 212, Loss: 0.4822187420754367\n",
      "Epoch 213, Loss: 0.48202997575341067\n",
      "Epoch 214, Loss: 0.4818441197783379\n",
      "Epoch 215, Loss: 0.4816502256898402\n",
      "Epoch 216, Loss: 0.4814589616492725\n",
      "Epoch 217, Loss: 0.48127338055379854\n",
      "Epoch 218, Loss: 0.4810898650639108\n",
      "Epoch 219, Loss: 0.48091365405597575\n",
      "Epoch 220, Loss: 0.480720996546175\n",
      "Epoch 221, Loss: 0.48054865040929456\n",
      "Epoch 222, Loss: 0.4803774677186426\n",
      "Epoch 223, Loss: 0.4801885671816001\n",
      "Epoch 224, Loss: 0.48000935557804647\n",
      "Epoch 225, Loss: 0.4798350303004208\n",
      "Epoch 226, Loss: 0.47967842665052907\n",
      "Epoch 227, Loss: 0.47947854451894606\n",
      "Epoch 228, Loss: 0.47928199900429136\n",
      "Epoch 229, Loss: 0.4791424752085709\n",
      "Epoch 230, Loss: 0.4789254904984497\n",
      "Epoch 231, Loss: 0.4787155823499749\n",
      "Epoch 232, Loss: 0.4785186611779873\n",
      "Epoch 233, Loss: 0.4783287773458093\n",
      "Epoch 234, Loss: 0.4781774454711345\n",
      "Epoch 235, Loss: 0.4780410437330356\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.27197133426651293\n",
      "Valence RMSE: 0.25219498928558043\n",
      "Arousal RMSE: 0.2904040301109139\n",
      "Test R^2 score: tensor([0.2292, 0.1649], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1375, 0.0656], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.19707528986463724\n",
      "Num of epochs: 236\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Epoch 121, Loss: 0.5116998508836337\n",
      "Epoch 122, Loss: 0.5111129902664598\n",
      "Epoch 123, Loss: 0.5105405741436312\n",
      "Epoch 124, Loss: 0.5099849010083686\n",
      "Epoch 125, Loss: 0.5094623759868415\n",
      "Epoch 126, Loss: 0.5089652841754122\n",
      "Epoch 127, Loss: 0.5084717799132713\n",
      "Epoch 128, Loss: 0.5079809643002384\n",
      "Epoch 129, Loss: 0.507507731608986\n",
      "Epoch 130, Loss: 0.5070515727009828\n",
      "Epoch 131, Loss: 0.5065962678643066\n",
      "Epoch 132, Loss: 0.5061448812256618\n",
      "Epoch 133, Loss: 0.5057069703510908\n",
      "Epoch 134, Loss: 0.5052756693952567\n",
      "Epoch 135, Loss: 0.5048445017462813\n",
      "Epoch 136, Loss: 0.504425284231979\n",
      "Epoch 137, Loss: 0.504022717877823\n",
      "Epoch 138, Loss: 0.5036312802042249\n",
      "Epoch 139, Loss: 0.5032368731068207\n",
      "Epoch 140, Loss: 0.5028511060044271\n",
      "Epoch 141, Loss: 0.5024804933282331\n",
      "Epoch 142, Loss: 0.5021182727396313\n",
      "Epoch 143, Loss: 0.5017581664990297\n",
      "Epoch 144, Loss: 0.5014050827892457\n",
      "Epoch 145, Loss: 0.501047675902349\n",
      "Epoch 146, Loss: 0.500690966247362\n",
      "Epoch 147, Loss: 0.5003476722262875\n",
      "Epoch 148, Loss: 0.5000068544871652\n",
      "Epoch 149, Loss: 0.49966338867073357\n",
      "Epoch 150, Loss: 0.4993280873087794\n",
      "Epoch 151, Loss: 0.49900257940428383\n",
      "Epoch 152, Loss: 0.4986865554458311\n",
      "Epoch 153, Loss: 0.49836949389755036\n",
      "Epoch 154, Loss: 0.49805375636713634\n",
      "Epoch 155, Loss: 0.49773873165141763\n",
      "Epoch 156, Loss: 0.49742224924400874\n",
      "Epoch 157, Loss: 0.49711425826812494\n",
      "Epoch 158, Loss: 0.4968010973402242\n",
      "Epoch 159, Loss: 0.4964901399315246\n",
      "Epoch 160, Loss: 0.49617675026847124\n",
      "Epoch 161, Loss: 0.4958670390985117\n",
      "Epoch 162, Loss: 0.4955578109301908\n",
      "Epoch 163, Loss: 0.4952510223984696\n",
      "Epoch 164, Loss: 0.49494577484111674\n",
      "Epoch 165, Loss: 0.49464511374066544\n",
      "Epoch 166, Loss: 0.49434383269975535\n",
      "Epoch 167, Loss: 0.49404631909698943\n",
      "Epoch 168, Loss: 0.49374530645143583\n",
      "Epoch 169, Loss: 0.49345064806438055\n",
      "Epoch 170, Loss: 0.49317001490258333\n",
      "Epoch 171, Loss: 0.4928927591216884\n",
      "Epoch 172, Loss: 0.4926138499628376\n",
      "Epoch 173, Loss: 0.49233528219511824\n",
      "Epoch 174, Loss: 0.49205852514110515\n",
      "Epoch 175, Loss: 0.4917773399684819\n",
      "Epoch 176, Loss: 0.49149629710928233\n",
      "Epoch 177, Loss: 0.4912165647150265\n",
      "Epoch 178, Loss: 0.49094181765497674\n",
      "Epoch 179, Loss: 0.49066901222163783\n",
      "Epoch 180, Loss: 0.490393928005003\n",
      "Epoch 181, Loss: 0.4901079113527018\n",
      "Epoch 182, Loss: 0.48981984154671765\n",
      "Epoch 183, Loss: 0.4895234443341763\n",
      "Epoch 184, Loss: 0.48921459258368105\n",
      "Epoch 185, Loss: 0.4889145977883947\n",
      "Epoch 186, Loss: 0.4886175142174919\n",
      "Epoch 187, Loss: 0.4883282295514753\n",
      "Epoch 188, Loss: 0.4880435212302353\n",
      "Epoch 189, Loss: 0.48775699700499114\n",
      "Epoch 190, Loss: 0.48748008613994775\n",
      "Epoch 191, Loss: 0.4872132179171775\n",
      "Epoch 192, Loss: 0.48694738158534456\n",
      "Epoch 193, Loss: 0.48668121634032785\n",
      "Epoch 194, Loss: 0.4864280322301768\n",
      "Epoch 195, Loss: 0.486178532155915\n",
      "Epoch 196, Loss: 0.4859301919172944\n",
      "Epoch 197, Loss: 0.48568442460989747\n",
      "Epoch 198, Loss: 0.48544224711238787\n",
      "Epoch 199, Loss: 0.48519729219516017\n",
      "Epoch 200, Loss: 0.4849575446659644\n",
      "Epoch 201, Loss: 0.4847146197220469\n",
      "Epoch 202, Loss: 0.48447155759133576\n",
      "Epoch 203, Loss: 0.48422783492589483\n",
      "Epoch 204, Loss: 0.48397707745066637\n",
      "Epoch 205, Loss: 0.483737926529631\n",
      "Epoch 206, Loss: 0.4835020782648923\n",
      "Epoch 207, Loss: 0.48327083252052233\n",
      "Epoch 208, Loss: 0.4830508282762034\n",
      "Epoch 209, Loss: 0.48283269895503794\n",
      "Epoch 210, Loss: 0.48261215534620283\n",
      "Epoch 211, Loss: 0.48240746542225255\n",
      "Epoch 212, Loss: 0.4822187420754367\n",
      "Epoch 213, Loss: 0.48202997575341067\n",
      "Epoch 214, Loss: 0.4818441197783379\n",
      "Epoch 215, Loss: 0.4816502256898402\n",
      "Epoch 216, Loss: 0.4814589616492725\n",
      "Epoch 217, Loss: 0.48127338055379854\n",
      "Epoch 218, Loss: 0.4810898650639108\n",
      "Epoch 219, Loss: 0.48091365405597575\n",
      "Epoch 220, Loss: 0.480720996546175\n",
      "Epoch 221, Loss: 0.48054865040929456\n",
      "Epoch 222, Loss: 0.4803774677186426\n",
      "Epoch 223, Loss: 0.4801885671816001\n",
      "Epoch 224, Loss: 0.48000935557804647\n",
      "Epoch 225, Loss: 0.4798350303004208\n",
      "Epoch 226, Loss: 0.47967842665052907\n",
      "Epoch 227, Loss: 0.47947854451894606\n",
      "Epoch 228, Loss: 0.47928199900429136\n",
      "Epoch 229, Loss: 0.4791424752085709\n",
      "Epoch 230, Loss: 0.4789254904984497\n",
      "Epoch 231, Loss: 0.4787155823499749\n",
      "Epoch 232, Loss: 0.4785186611779873\n",
      "Epoch 233, Loss: 0.4783287773458093\n",
      "Epoch 234, Loss: 0.4781774454711345\n",
      "Epoch 235, Loss: 0.4780410437330356\n",
      "Epoch 236, Loss: 0.4778263811299359\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2722378798126422\n",
      "Valence RMSE: 0.25230086773673754\n",
      "Arousal RMSE: 0.29081127651632505\n",
      "Test R^2 score: tensor([0.2286, 0.1626], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1368, 0.0630], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.19557976715763198\n",
      "Num of epochs: 237\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Epoch 121, Loss: 0.5116998508836337\n",
      "Epoch 122, Loss: 0.5111129902664598\n",
      "Epoch 123, Loss: 0.5105405741436312\n",
      "Epoch 124, Loss: 0.5099849010083686\n",
      "Epoch 125, Loss: 0.5094623759868415\n",
      "Epoch 126, Loss: 0.5089652841754122\n",
      "Epoch 127, Loss: 0.5084717799132713\n",
      "Epoch 128, Loss: 0.5079809643002384\n",
      "Epoch 129, Loss: 0.507507731608986\n",
      "Epoch 130, Loss: 0.5070515727009828\n",
      "Epoch 131, Loss: 0.5065962678643066\n",
      "Epoch 132, Loss: 0.5061448812256618\n",
      "Epoch 133, Loss: 0.5057069703510908\n",
      "Epoch 134, Loss: 0.5052756693952567\n",
      "Epoch 135, Loss: 0.5048445017462813\n",
      "Epoch 136, Loss: 0.504425284231979\n",
      "Epoch 137, Loss: 0.504022717877823\n",
      "Epoch 138, Loss: 0.5036312802042249\n",
      "Epoch 139, Loss: 0.5032368731068207\n",
      "Epoch 140, Loss: 0.5028511060044271\n",
      "Epoch 141, Loss: 0.5024804933282331\n",
      "Epoch 142, Loss: 0.5021182727396313\n",
      "Epoch 143, Loss: 0.5017581664990297\n",
      "Epoch 144, Loss: 0.5014050827892457\n",
      "Epoch 145, Loss: 0.501047675902349\n",
      "Epoch 146, Loss: 0.500690966247362\n",
      "Epoch 147, Loss: 0.5003476722262875\n",
      "Epoch 148, Loss: 0.5000068544871652\n",
      "Epoch 149, Loss: 0.49966338867073357\n",
      "Epoch 150, Loss: 0.4993280873087794\n",
      "Epoch 151, Loss: 0.49900257940428383\n",
      "Epoch 152, Loss: 0.4986865554458311\n",
      "Epoch 153, Loss: 0.49836949389755036\n",
      "Epoch 154, Loss: 0.49805375636713634\n",
      "Epoch 155, Loss: 0.49773873165141763\n",
      "Epoch 156, Loss: 0.49742224924400874\n",
      "Epoch 157, Loss: 0.49711425826812494\n",
      "Epoch 158, Loss: 0.4968010973402242\n",
      "Epoch 159, Loss: 0.4964901399315246\n",
      "Epoch 160, Loss: 0.49617675026847124\n",
      "Epoch 161, Loss: 0.4958670390985117\n",
      "Epoch 162, Loss: 0.4955578109301908\n",
      "Epoch 163, Loss: 0.4952510223984696\n",
      "Epoch 164, Loss: 0.49494577484111674\n",
      "Epoch 165, Loss: 0.49464511374066544\n",
      "Epoch 166, Loss: 0.49434383269975535\n",
      "Epoch 167, Loss: 0.49404631909698943\n",
      "Epoch 168, Loss: 0.49374530645143583\n",
      "Epoch 169, Loss: 0.49345064806438055\n",
      "Epoch 170, Loss: 0.49317001490258333\n",
      "Epoch 171, Loss: 0.4928927591216884\n",
      "Epoch 172, Loss: 0.4926138499628376\n",
      "Epoch 173, Loss: 0.49233528219511824\n",
      "Epoch 174, Loss: 0.49205852514110515\n",
      "Epoch 175, Loss: 0.4917773399684819\n",
      "Epoch 176, Loss: 0.49149629710928233\n",
      "Epoch 177, Loss: 0.4912165647150265\n",
      "Epoch 178, Loss: 0.49094181765497674\n",
      "Epoch 179, Loss: 0.49066901222163783\n",
      "Epoch 180, Loss: 0.490393928005003\n",
      "Epoch 181, Loss: 0.4901079113527018\n",
      "Epoch 182, Loss: 0.48981984154671765\n",
      "Epoch 183, Loss: 0.4895234443341763\n",
      "Epoch 184, Loss: 0.48921459258368105\n",
      "Epoch 185, Loss: 0.4889145977883947\n",
      "Epoch 186, Loss: 0.4886175142174919\n",
      "Epoch 187, Loss: 0.4883282295514753\n",
      "Epoch 188, Loss: 0.4880435212302353\n",
      "Epoch 189, Loss: 0.48775699700499114\n",
      "Epoch 190, Loss: 0.48748008613994775\n",
      "Epoch 191, Loss: 0.4872132179171775\n",
      "Epoch 192, Loss: 0.48694738158534456\n",
      "Epoch 193, Loss: 0.48668121634032785\n",
      "Epoch 194, Loss: 0.4864280322301768\n",
      "Epoch 195, Loss: 0.486178532155915\n",
      "Epoch 196, Loss: 0.4859301919172944\n",
      "Epoch 197, Loss: 0.48568442460989747\n",
      "Epoch 198, Loss: 0.48544224711238787\n",
      "Epoch 199, Loss: 0.48519729219516017\n",
      "Epoch 200, Loss: 0.4849575446659644\n",
      "Epoch 201, Loss: 0.4847146197220469\n",
      "Epoch 202, Loss: 0.48447155759133576\n",
      "Epoch 203, Loss: 0.48422783492589483\n",
      "Epoch 204, Loss: 0.48397707745066637\n",
      "Epoch 205, Loss: 0.483737926529631\n",
      "Epoch 206, Loss: 0.4835020782648923\n",
      "Epoch 207, Loss: 0.48327083252052233\n",
      "Epoch 208, Loss: 0.4830508282762034\n",
      "Epoch 209, Loss: 0.48283269895503794\n",
      "Epoch 210, Loss: 0.48261215534620283\n",
      "Epoch 211, Loss: 0.48240746542225255\n",
      "Epoch 212, Loss: 0.4822187420754367\n",
      "Epoch 213, Loss: 0.48202997575341067\n",
      "Epoch 214, Loss: 0.4818441197783379\n",
      "Epoch 215, Loss: 0.4816502256898402\n",
      "Epoch 216, Loss: 0.4814589616492725\n",
      "Epoch 217, Loss: 0.48127338055379854\n",
      "Epoch 218, Loss: 0.4810898650639108\n",
      "Epoch 219, Loss: 0.48091365405597575\n",
      "Epoch 220, Loss: 0.480720996546175\n",
      "Epoch 221, Loss: 0.48054865040929456\n",
      "Epoch 222, Loss: 0.4803774677186426\n",
      "Epoch 223, Loss: 0.4801885671816001\n",
      "Epoch 224, Loss: 0.48000935557804647\n",
      "Epoch 225, Loss: 0.4798350303004208\n",
      "Epoch 226, Loss: 0.47967842665052907\n",
      "Epoch 227, Loss: 0.47947854451894606\n",
      "Epoch 228, Loss: 0.47928199900429136\n",
      "Epoch 229, Loss: 0.4791424752085709\n",
      "Epoch 230, Loss: 0.4789254904984497\n",
      "Epoch 231, Loss: 0.4787155823499749\n",
      "Epoch 232, Loss: 0.4785186611779873\n",
      "Epoch 233, Loss: 0.4783287773458093\n",
      "Epoch 234, Loss: 0.4781774454711345\n",
      "Epoch 235, Loss: 0.4780410437330356\n",
      "Epoch 236, Loss: 0.4778263811299359\n",
      "Epoch 237, Loss: 0.47765285017795805\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.27239897279172826\n",
      "Valence RMSE: 0.25235868951002216\n",
      "Arousal RMSE: 0.2910626952818957\n",
      "Test R^2 score: tensor([0.2282, 0.1612], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1364, 0.0614], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.1946786707537561\n",
      "Num of epochs: 238\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Epoch 121, Loss: 0.5116998508836337\n",
      "Epoch 122, Loss: 0.5111129902664598\n",
      "Epoch 123, Loss: 0.5105405741436312\n",
      "Epoch 124, Loss: 0.5099849010083686\n",
      "Epoch 125, Loss: 0.5094623759868415\n",
      "Epoch 126, Loss: 0.5089652841754122\n",
      "Epoch 127, Loss: 0.5084717799132713\n",
      "Epoch 128, Loss: 0.5079809643002384\n",
      "Epoch 129, Loss: 0.507507731608986\n",
      "Epoch 130, Loss: 0.5070515727009828\n",
      "Epoch 131, Loss: 0.5065962678643066\n",
      "Epoch 132, Loss: 0.5061448812256618\n",
      "Epoch 133, Loss: 0.5057069703510908\n",
      "Epoch 134, Loss: 0.5052756693952567\n",
      "Epoch 135, Loss: 0.5048445017462813\n",
      "Epoch 136, Loss: 0.504425284231979\n",
      "Epoch 137, Loss: 0.504022717877823\n",
      "Epoch 138, Loss: 0.5036312802042249\n",
      "Epoch 139, Loss: 0.5032368731068207\n",
      "Epoch 140, Loss: 0.5028511060044271\n",
      "Epoch 141, Loss: 0.5024804933282331\n",
      "Epoch 142, Loss: 0.5021182727396313\n",
      "Epoch 143, Loss: 0.5017581664990297\n",
      "Epoch 144, Loss: 0.5014050827892457\n",
      "Epoch 145, Loss: 0.501047675902349\n",
      "Epoch 146, Loss: 0.500690966247362\n",
      "Epoch 147, Loss: 0.5003476722262875\n",
      "Epoch 148, Loss: 0.5000068544871652\n",
      "Epoch 149, Loss: 0.49966338867073357\n",
      "Epoch 150, Loss: 0.4993280873087794\n",
      "Epoch 151, Loss: 0.49900257940428383\n",
      "Epoch 152, Loss: 0.4986865554458311\n",
      "Epoch 153, Loss: 0.49836949389755036\n",
      "Epoch 154, Loss: 0.49805375636713634\n",
      "Epoch 155, Loss: 0.49773873165141763\n",
      "Epoch 156, Loss: 0.49742224924400874\n",
      "Epoch 157, Loss: 0.49711425826812494\n",
      "Epoch 158, Loss: 0.4968010973402242\n",
      "Epoch 159, Loss: 0.4964901399315246\n",
      "Epoch 160, Loss: 0.49617675026847124\n",
      "Epoch 161, Loss: 0.4958670390985117\n",
      "Epoch 162, Loss: 0.4955578109301908\n",
      "Epoch 163, Loss: 0.4952510223984696\n",
      "Epoch 164, Loss: 0.49494577484111674\n",
      "Epoch 165, Loss: 0.49464511374066544\n",
      "Epoch 166, Loss: 0.49434383269975535\n",
      "Epoch 167, Loss: 0.49404631909698943\n",
      "Epoch 168, Loss: 0.49374530645143583\n",
      "Epoch 169, Loss: 0.49345064806438055\n",
      "Epoch 170, Loss: 0.49317001490258333\n",
      "Epoch 171, Loss: 0.4928927591216884\n",
      "Epoch 172, Loss: 0.4926138499628376\n",
      "Epoch 173, Loss: 0.49233528219511824\n",
      "Epoch 174, Loss: 0.49205852514110515\n",
      "Epoch 175, Loss: 0.4917773399684819\n",
      "Epoch 176, Loss: 0.49149629710928233\n",
      "Epoch 177, Loss: 0.4912165647150265\n",
      "Epoch 178, Loss: 0.49094181765497674\n",
      "Epoch 179, Loss: 0.49066901222163783\n",
      "Epoch 180, Loss: 0.490393928005003\n",
      "Epoch 181, Loss: 0.4901079113527018\n",
      "Epoch 182, Loss: 0.48981984154671765\n",
      "Epoch 183, Loss: 0.4895234443341763\n",
      "Epoch 184, Loss: 0.48921459258368105\n",
      "Epoch 185, Loss: 0.4889145977883947\n",
      "Epoch 186, Loss: 0.4886175142174919\n",
      "Epoch 187, Loss: 0.4883282295514753\n",
      "Epoch 188, Loss: 0.4880435212302353\n",
      "Epoch 189, Loss: 0.48775699700499114\n",
      "Epoch 190, Loss: 0.48748008613994775\n",
      "Epoch 191, Loss: 0.4872132179171775\n",
      "Epoch 192, Loss: 0.48694738158534456\n",
      "Epoch 193, Loss: 0.48668121634032785\n",
      "Epoch 194, Loss: 0.4864280322301768\n",
      "Epoch 195, Loss: 0.486178532155915\n",
      "Epoch 196, Loss: 0.4859301919172944\n",
      "Epoch 197, Loss: 0.48568442460989747\n",
      "Epoch 198, Loss: 0.48544224711238787\n",
      "Epoch 199, Loss: 0.48519729219516017\n",
      "Epoch 200, Loss: 0.4849575446659644\n",
      "Epoch 201, Loss: 0.4847146197220469\n",
      "Epoch 202, Loss: 0.48447155759133576\n",
      "Epoch 203, Loss: 0.48422783492589483\n",
      "Epoch 204, Loss: 0.48397707745066637\n",
      "Epoch 205, Loss: 0.483737926529631\n",
      "Epoch 206, Loss: 0.4835020782648923\n",
      "Epoch 207, Loss: 0.48327083252052233\n",
      "Epoch 208, Loss: 0.4830508282762034\n",
      "Epoch 209, Loss: 0.48283269895503794\n",
      "Epoch 210, Loss: 0.48261215534620283\n",
      "Epoch 211, Loss: 0.48240746542225255\n",
      "Epoch 212, Loss: 0.4822187420754367\n",
      "Epoch 213, Loss: 0.48202997575341067\n",
      "Epoch 214, Loss: 0.4818441197783379\n",
      "Epoch 215, Loss: 0.4816502256898402\n",
      "Epoch 216, Loss: 0.4814589616492725\n",
      "Epoch 217, Loss: 0.48127338055379854\n",
      "Epoch 218, Loss: 0.4810898650639108\n",
      "Epoch 219, Loss: 0.48091365405597575\n",
      "Epoch 220, Loss: 0.480720996546175\n",
      "Epoch 221, Loss: 0.48054865040929456\n",
      "Epoch 222, Loss: 0.4803774677186426\n",
      "Epoch 223, Loss: 0.4801885671816001\n",
      "Epoch 224, Loss: 0.48000935557804647\n",
      "Epoch 225, Loss: 0.4798350303004208\n",
      "Epoch 226, Loss: 0.47967842665052907\n",
      "Epoch 227, Loss: 0.47947854451894606\n",
      "Epoch 228, Loss: 0.47928199900429136\n",
      "Epoch 229, Loss: 0.4791424752085709\n",
      "Epoch 230, Loss: 0.4789254904984497\n",
      "Epoch 231, Loss: 0.4787155823499749\n",
      "Epoch 232, Loss: 0.4785186611779873\n",
      "Epoch 233, Loss: 0.4783287773458093\n",
      "Epoch 234, Loss: 0.4781774454711345\n",
      "Epoch 235, Loss: 0.4780410437330356\n",
      "Epoch 236, Loss: 0.4778263811299359\n",
      "Epoch 237, Loss: 0.47765285017795805\n",
      "Epoch 238, Loss: 0.4775436338740249\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2730025450096185\n",
      "Valence RMSE: 0.25305269419028376\n",
      "Arousal RMSE: 0.291590660218218\n",
      "Test R^2 score: tensor([0.2240, 0.1581], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1316, 0.0579], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.1910302740670543\n",
      "Num of epochs: 239\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Epoch 121, Loss: 0.5116998508836337\n",
      "Epoch 122, Loss: 0.5111129902664598\n",
      "Epoch 123, Loss: 0.5105405741436312\n",
      "Epoch 124, Loss: 0.5099849010083686\n",
      "Epoch 125, Loss: 0.5094623759868415\n",
      "Epoch 126, Loss: 0.5089652841754122\n",
      "Epoch 127, Loss: 0.5084717799132713\n",
      "Epoch 128, Loss: 0.5079809643002384\n",
      "Epoch 129, Loss: 0.507507731608986\n",
      "Epoch 130, Loss: 0.5070515727009828\n",
      "Epoch 131, Loss: 0.5065962678643066\n",
      "Epoch 132, Loss: 0.5061448812256618\n",
      "Epoch 133, Loss: 0.5057069703510908\n",
      "Epoch 134, Loss: 0.5052756693952567\n",
      "Epoch 135, Loss: 0.5048445017462813\n",
      "Epoch 136, Loss: 0.504425284231979\n",
      "Epoch 137, Loss: 0.504022717877823\n",
      "Epoch 138, Loss: 0.5036312802042249\n",
      "Epoch 139, Loss: 0.5032368731068207\n",
      "Epoch 140, Loss: 0.5028511060044271\n",
      "Epoch 141, Loss: 0.5024804933282331\n",
      "Epoch 142, Loss: 0.5021182727396313\n",
      "Epoch 143, Loss: 0.5017581664990297\n",
      "Epoch 144, Loss: 0.5014050827892457\n",
      "Epoch 145, Loss: 0.501047675902349\n",
      "Epoch 146, Loss: 0.500690966247362\n",
      "Epoch 147, Loss: 0.5003476722262875\n",
      "Epoch 148, Loss: 0.5000068544871652\n",
      "Epoch 149, Loss: 0.49966338867073357\n",
      "Epoch 150, Loss: 0.4993280873087794\n",
      "Epoch 151, Loss: 0.49900257940428383\n",
      "Epoch 152, Loss: 0.4986865554458311\n",
      "Epoch 153, Loss: 0.49836949389755036\n",
      "Epoch 154, Loss: 0.49805375636713634\n",
      "Epoch 155, Loss: 0.49773873165141763\n",
      "Epoch 156, Loss: 0.49742224924400874\n",
      "Epoch 157, Loss: 0.49711425826812494\n",
      "Epoch 158, Loss: 0.4968010973402242\n",
      "Epoch 159, Loss: 0.4964901399315246\n",
      "Epoch 160, Loss: 0.49617675026847124\n",
      "Epoch 161, Loss: 0.4958670390985117\n",
      "Epoch 162, Loss: 0.4955578109301908\n",
      "Epoch 163, Loss: 0.4952510223984696\n",
      "Epoch 164, Loss: 0.49494577484111674\n",
      "Epoch 165, Loss: 0.49464511374066544\n",
      "Epoch 166, Loss: 0.49434383269975535\n",
      "Epoch 167, Loss: 0.49404631909698943\n",
      "Epoch 168, Loss: 0.49374530645143583\n",
      "Epoch 169, Loss: 0.49345064806438055\n",
      "Epoch 170, Loss: 0.49317001490258333\n",
      "Epoch 171, Loss: 0.4928927591216884\n",
      "Epoch 172, Loss: 0.4926138499628376\n",
      "Epoch 173, Loss: 0.49233528219511824\n",
      "Epoch 174, Loss: 0.49205852514110515\n",
      "Epoch 175, Loss: 0.4917773399684819\n",
      "Epoch 176, Loss: 0.49149629710928233\n",
      "Epoch 177, Loss: 0.4912165647150265\n",
      "Epoch 178, Loss: 0.49094181765497674\n",
      "Epoch 179, Loss: 0.49066901222163783\n",
      "Epoch 180, Loss: 0.490393928005003\n",
      "Epoch 181, Loss: 0.4901079113527018\n",
      "Epoch 182, Loss: 0.48981984154671765\n",
      "Epoch 183, Loss: 0.4895234443341763\n",
      "Epoch 184, Loss: 0.48921459258368105\n",
      "Epoch 185, Loss: 0.4889145977883947\n",
      "Epoch 186, Loss: 0.4886175142174919\n",
      "Epoch 187, Loss: 0.4883282295514753\n",
      "Epoch 188, Loss: 0.4880435212302353\n",
      "Epoch 189, Loss: 0.48775699700499114\n",
      "Epoch 190, Loss: 0.48748008613994775\n",
      "Epoch 191, Loss: 0.4872132179171775\n",
      "Epoch 192, Loss: 0.48694738158534456\n",
      "Epoch 193, Loss: 0.48668121634032785\n",
      "Epoch 194, Loss: 0.4864280322301768\n",
      "Epoch 195, Loss: 0.486178532155915\n",
      "Epoch 196, Loss: 0.4859301919172944\n",
      "Epoch 197, Loss: 0.48568442460989747\n",
      "Epoch 198, Loss: 0.48544224711238787\n",
      "Epoch 199, Loss: 0.48519729219516017\n",
      "Epoch 200, Loss: 0.4849575446659644\n",
      "Epoch 201, Loss: 0.4847146197220469\n",
      "Epoch 202, Loss: 0.48447155759133576\n",
      "Epoch 203, Loss: 0.48422783492589483\n",
      "Epoch 204, Loss: 0.48397707745066637\n",
      "Epoch 205, Loss: 0.483737926529631\n",
      "Epoch 206, Loss: 0.4835020782648923\n",
      "Epoch 207, Loss: 0.48327083252052233\n",
      "Epoch 208, Loss: 0.4830508282762034\n",
      "Epoch 209, Loss: 0.48283269895503794\n",
      "Epoch 210, Loss: 0.48261215534620283\n",
      "Epoch 211, Loss: 0.48240746542225255\n",
      "Epoch 212, Loss: 0.4822187420754367\n",
      "Epoch 213, Loss: 0.48202997575341067\n",
      "Epoch 214, Loss: 0.4818441197783379\n",
      "Epoch 215, Loss: 0.4816502256898402\n",
      "Epoch 216, Loss: 0.4814589616492725\n",
      "Epoch 217, Loss: 0.48127338055379854\n",
      "Epoch 218, Loss: 0.4810898650639108\n",
      "Epoch 219, Loss: 0.48091365405597575\n",
      "Epoch 220, Loss: 0.480720996546175\n",
      "Epoch 221, Loss: 0.48054865040929456\n",
      "Epoch 222, Loss: 0.4803774677186426\n",
      "Epoch 223, Loss: 0.4801885671816001\n",
      "Epoch 224, Loss: 0.48000935557804647\n",
      "Epoch 225, Loss: 0.4798350303004208\n",
      "Epoch 226, Loss: 0.47967842665052907\n",
      "Epoch 227, Loss: 0.47947854451894606\n",
      "Epoch 228, Loss: 0.47928199900429136\n",
      "Epoch 229, Loss: 0.4791424752085709\n",
      "Epoch 230, Loss: 0.4789254904984497\n",
      "Epoch 231, Loss: 0.4787155823499749\n",
      "Epoch 232, Loss: 0.4785186611779873\n",
      "Epoch 233, Loss: 0.4783287773458093\n",
      "Epoch 234, Loss: 0.4781774454711345\n",
      "Epoch 235, Loss: 0.4780410437330356\n",
      "Epoch 236, Loss: 0.4778263811299359\n",
      "Epoch 237, Loss: 0.47765285017795805\n",
      "Epoch 238, Loss: 0.4775436338740249\n",
      "Epoch 239, Loss: 0.4774342833479412\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.27277384803444915\n",
      "Valence RMSE: 0.252757145278545\n",
      "Arousal RMSE: 0.29141889069461857\n",
      "Test R^2 score: tensor([0.2258, 0.1591], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1337, 0.0591], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.19243191112775743\n",
      "Num of epochs: 240\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Epoch 121, Loss: 0.5116998508836337\n",
      "Epoch 122, Loss: 0.5111129902664598\n",
      "Epoch 123, Loss: 0.5105405741436312\n",
      "Epoch 124, Loss: 0.5099849010083686\n",
      "Epoch 125, Loss: 0.5094623759868415\n",
      "Epoch 126, Loss: 0.5089652841754122\n",
      "Epoch 127, Loss: 0.5084717799132713\n",
      "Epoch 128, Loss: 0.5079809643002384\n",
      "Epoch 129, Loss: 0.507507731608986\n",
      "Epoch 130, Loss: 0.5070515727009828\n",
      "Epoch 131, Loss: 0.5065962678643066\n",
      "Epoch 132, Loss: 0.5061448812256618\n",
      "Epoch 133, Loss: 0.5057069703510908\n",
      "Epoch 134, Loss: 0.5052756693952567\n",
      "Epoch 135, Loss: 0.5048445017462813\n",
      "Epoch 136, Loss: 0.504425284231979\n",
      "Epoch 137, Loss: 0.504022717877823\n",
      "Epoch 138, Loss: 0.5036312802042249\n",
      "Epoch 139, Loss: 0.5032368731068207\n",
      "Epoch 140, Loss: 0.5028511060044271\n",
      "Epoch 141, Loss: 0.5024804933282331\n",
      "Epoch 142, Loss: 0.5021182727396313\n",
      "Epoch 143, Loss: 0.5017581664990297\n",
      "Epoch 144, Loss: 0.5014050827892457\n",
      "Epoch 145, Loss: 0.501047675902349\n",
      "Epoch 146, Loss: 0.500690966247362\n",
      "Epoch 147, Loss: 0.5003476722262875\n",
      "Epoch 148, Loss: 0.5000068544871652\n",
      "Epoch 149, Loss: 0.49966338867073357\n",
      "Epoch 150, Loss: 0.4993280873087794\n",
      "Epoch 151, Loss: 0.49900257940428383\n",
      "Epoch 152, Loss: 0.4986865554458311\n",
      "Epoch 153, Loss: 0.49836949389755036\n",
      "Epoch 154, Loss: 0.49805375636713634\n",
      "Epoch 155, Loss: 0.49773873165141763\n",
      "Epoch 156, Loss: 0.49742224924400874\n",
      "Epoch 157, Loss: 0.49711425826812494\n",
      "Epoch 158, Loss: 0.4968010973402242\n",
      "Epoch 159, Loss: 0.4964901399315246\n",
      "Epoch 160, Loss: 0.49617675026847124\n",
      "Epoch 161, Loss: 0.4958670390985117\n",
      "Epoch 162, Loss: 0.4955578109301908\n",
      "Epoch 163, Loss: 0.4952510223984696\n",
      "Epoch 164, Loss: 0.49494577484111674\n",
      "Epoch 165, Loss: 0.49464511374066544\n",
      "Epoch 166, Loss: 0.49434383269975535\n",
      "Epoch 167, Loss: 0.49404631909698943\n",
      "Epoch 168, Loss: 0.49374530645143583\n",
      "Epoch 169, Loss: 0.49345064806438055\n",
      "Epoch 170, Loss: 0.49317001490258333\n",
      "Epoch 171, Loss: 0.4928927591216884\n",
      "Epoch 172, Loss: 0.4926138499628376\n",
      "Epoch 173, Loss: 0.49233528219511824\n",
      "Epoch 174, Loss: 0.49205852514110515\n",
      "Epoch 175, Loss: 0.4917773399684819\n",
      "Epoch 176, Loss: 0.49149629710928233\n",
      "Epoch 177, Loss: 0.4912165647150265\n",
      "Epoch 178, Loss: 0.49094181765497674\n",
      "Epoch 179, Loss: 0.49066901222163783\n",
      "Epoch 180, Loss: 0.490393928005003\n",
      "Epoch 181, Loss: 0.4901079113527018\n",
      "Epoch 182, Loss: 0.48981984154671765\n",
      "Epoch 183, Loss: 0.4895234443341763\n",
      "Epoch 184, Loss: 0.48921459258368105\n",
      "Epoch 185, Loss: 0.4889145977883947\n",
      "Epoch 186, Loss: 0.4886175142174919\n",
      "Epoch 187, Loss: 0.4883282295514753\n",
      "Epoch 188, Loss: 0.4880435212302353\n",
      "Epoch 189, Loss: 0.48775699700499114\n",
      "Epoch 190, Loss: 0.48748008613994775\n",
      "Epoch 191, Loss: 0.4872132179171775\n",
      "Epoch 192, Loss: 0.48694738158534456\n",
      "Epoch 193, Loss: 0.48668121634032785\n",
      "Epoch 194, Loss: 0.4864280322301768\n",
      "Epoch 195, Loss: 0.486178532155915\n",
      "Epoch 196, Loss: 0.4859301919172944\n",
      "Epoch 197, Loss: 0.48568442460989747\n",
      "Epoch 198, Loss: 0.48544224711238787\n",
      "Epoch 199, Loss: 0.48519729219516017\n",
      "Epoch 200, Loss: 0.4849575446659644\n",
      "Epoch 201, Loss: 0.4847146197220469\n",
      "Epoch 202, Loss: 0.48447155759133576\n",
      "Epoch 203, Loss: 0.48422783492589483\n",
      "Epoch 204, Loss: 0.48397707745066637\n",
      "Epoch 205, Loss: 0.483737926529631\n",
      "Epoch 206, Loss: 0.4835020782648923\n",
      "Epoch 207, Loss: 0.48327083252052233\n",
      "Epoch 208, Loss: 0.4830508282762034\n",
      "Epoch 209, Loss: 0.48283269895503794\n",
      "Epoch 210, Loss: 0.48261215534620283\n",
      "Epoch 211, Loss: 0.48240746542225255\n",
      "Epoch 212, Loss: 0.4822187420754367\n",
      "Epoch 213, Loss: 0.48202997575341067\n",
      "Epoch 214, Loss: 0.4818441197783379\n",
      "Epoch 215, Loss: 0.4816502256898402\n",
      "Epoch 216, Loss: 0.4814589616492725\n",
      "Epoch 217, Loss: 0.48127338055379854\n",
      "Epoch 218, Loss: 0.4810898650639108\n",
      "Epoch 219, Loss: 0.48091365405597575\n",
      "Epoch 220, Loss: 0.480720996546175\n",
      "Epoch 221, Loss: 0.48054865040929456\n",
      "Epoch 222, Loss: 0.4803774677186426\n",
      "Epoch 223, Loss: 0.4801885671816001\n",
      "Epoch 224, Loss: 0.48000935557804647\n",
      "Epoch 225, Loss: 0.4798350303004208\n",
      "Epoch 226, Loss: 0.47967842665052907\n",
      "Epoch 227, Loss: 0.47947854451894606\n",
      "Epoch 228, Loss: 0.47928199900429136\n",
      "Epoch 229, Loss: 0.4791424752085709\n",
      "Epoch 230, Loss: 0.4789254904984497\n",
      "Epoch 231, Loss: 0.4787155823499749\n",
      "Epoch 232, Loss: 0.4785186611779873\n",
      "Epoch 233, Loss: 0.4783287773458093\n",
      "Epoch 234, Loss: 0.4781774454711345\n",
      "Epoch 235, Loss: 0.4780410437330356\n",
      "Epoch 236, Loss: 0.4778263811299359\n",
      "Epoch 237, Loss: 0.47765285017795805\n",
      "Epoch 238, Loss: 0.4775436338740249\n",
      "Epoch 239, Loss: 0.4774342833479412\n",
      "Epoch 240, Loss: 0.477263591547843\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.27290779418440925\n",
      "Valence RMSE: 0.25276912027232723\n",
      "Arousal RMSE: 0.29165921910674025\n",
      "Test R^2 score: tensor([0.2257, 0.1577], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1336, 0.0575], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.1917014665175512\n",
      "Num of epochs: 241\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Epoch 121, Loss: 0.5116998508836337\n",
      "Epoch 122, Loss: 0.5111129902664598\n",
      "Epoch 123, Loss: 0.5105405741436312\n",
      "Epoch 124, Loss: 0.5099849010083686\n",
      "Epoch 125, Loss: 0.5094623759868415\n",
      "Epoch 126, Loss: 0.5089652841754122\n",
      "Epoch 127, Loss: 0.5084717799132713\n",
      "Epoch 128, Loss: 0.5079809643002384\n",
      "Epoch 129, Loss: 0.507507731608986\n",
      "Epoch 130, Loss: 0.5070515727009828\n",
      "Epoch 131, Loss: 0.5065962678643066\n",
      "Epoch 132, Loss: 0.5061448812256618\n",
      "Epoch 133, Loss: 0.5057069703510908\n",
      "Epoch 134, Loss: 0.5052756693952567\n",
      "Epoch 135, Loss: 0.5048445017462813\n",
      "Epoch 136, Loss: 0.504425284231979\n",
      "Epoch 137, Loss: 0.504022717877823\n",
      "Epoch 138, Loss: 0.5036312802042249\n",
      "Epoch 139, Loss: 0.5032368731068207\n",
      "Epoch 140, Loss: 0.5028511060044271\n",
      "Epoch 141, Loss: 0.5024804933282331\n",
      "Epoch 142, Loss: 0.5021182727396313\n",
      "Epoch 143, Loss: 0.5017581664990297\n",
      "Epoch 144, Loss: 0.5014050827892457\n",
      "Epoch 145, Loss: 0.501047675902349\n",
      "Epoch 146, Loss: 0.500690966247362\n",
      "Epoch 147, Loss: 0.5003476722262875\n",
      "Epoch 148, Loss: 0.5000068544871652\n",
      "Epoch 149, Loss: 0.49966338867073357\n",
      "Epoch 150, Loss: 0.4993280873087794\n",
      "Epoch 151, Loss: 0.49900257940428383\n",
      "Epoch 152, Loss: 0.4986865554458311\n",
      "Epoch 153, Loss: 0.49836949389755036\n",
      "Epoch 154, Loss: 0.49805375636713634\n",
      "Epoch 155, Loss: 0.49773873165141763\n",
      "Epoch 156, Loss: 0.49742224924400874\n",
      "Epoch 157, Loss: 0.49711425826812494\n",
      "Epoch 158, Loss: 0.4968010973402242\n",
      "Epoch 159, Loss: 0.4964901399315246\n",
      "Epoch 160, Loss: 0.49617675026847124\n",
      "Epoch 161, Loss: 0.4958670390985117\n",
      "Epoch 162, Loss: 0.4955578109301908\n",
      "Epoch 163, Loss: 0.4952510223984696\n",
      "Epoch 164, Loss: 0.49494577484111674\n",
      "Epoch 165, Loss: 0.49464511374066544\n",
      "Epoch 166, Loss: 0.49434383269975535\n",
      "Epoch 167, Loss: 0.49404631909698943\n",
      "Epoch 168, Loss: 0.49374530645143583\n",
      "Epoch 169, Loss: 0.49345064806438055\n",
      "Epoch 170, Loss: 0.49317001490258333\n",
      "Epoch 171, Loss: 0.4928927591216884\n",
      "Epoch 172, Loss: 0.4926138499628376\n",
      "Epoch 173, Loss: 0.49233528219511824\n",
      "Epoch 174, Loss: 0.49205852514110515\n",
      "Epoch 175, Loss: 0.4917773399684819\n",
      "Epoch 176, Loss: 0.49149629710928233\n",
      "Epoch 177, Loss: 0.4912165647150265\n",
      "Epoch 178, Loss: 0.49094181765497674\n",
      "Epoch 179, Loss: 0.49066901222163783\n",
      "Epoch 180, Loss: 0.490393928005003\n",
      "Epoch 181, Loss: 0.4901079113527018\n",
      "Epoch 182, Loss: 0.48981984154671765\n",
      "Epoch 183, Loss: 0.4895234443341763\n",
      "Epoch 184, Loss: 0.48921459258368105\n",
      "Epoch 185, Loss: 0.4889145977883947\n",
      "Epoch 186, Loss: 0.4886175142174919\n",
      "Epoch 187, Loss: 0.4883282295514753\n",
      "Epoch 188, Loss: 0.4880435212302353\n",
      "Epoch 189, Loss: 0.48775699700499114\n",
      "Epoch 190, Loss: 0.48748008613994775\n",
      "Epoch 191, Loss: 0.4872132179171775\n",
      "Epoch 192, Loss: 0.48694738158534456\n",
      "Epoch 193, Loss: 0.48668121634032785\n",
      "Epoch 194, Loss: 0.4864280322301768\n",
      "Epoch 195, Loss: 0.486178532155915\n",
      "Epoch 196, Loss: 0.4859301919172944\n",
      "Epoch 197, Loss: 0.48568442460989747\n",
      "Epoch 198, Loss: 0.48544224711238787\n",
      "Epoch 199, Loss: 0.48519729219516017\n",
      "Epoch 200, Loss: 0.4849575446659644\n",
      "Epoch 201, Loss: 0.4847146197220469\n",
      "Epoch 202, Loss: 0.48447155759133576\n",
      "Epoch 203, Loss: 0.48422783492589483\n",
      "Epoch 204, Loss: 0.48397707745066637\n",
      "Epoch 205, Loss: 0.483737926529631\n",
      "Epoch 206, Loss: 0.4835020782648923\n",
      "Epoch 207, Loss: 0.48327083252052233\n",
      "Epoch 208, Loss: 0.4830508282762034\n",
      "Epoch 209, Loss: 0.48283269895503794\n",
      "Epoch 210, Loss: 0.48261215534620283\n",
      "Epoch 211, Loss: 0.48240746542225255\n",
      "Epoch 212, Loss: 0.4822187420754367\n",
      "Epoch 213, Loss: 0.48202997575341067\n",
      "Epoch 214, Loss: 0.4818441197783379\n",
      "Epoch 215, Loss: 0.4816502256898402\n",
      "Epoch 216, Loss: 0.4814589616492725\n",
      "Epoch 217, Loss: 0.48127338055379854\n",
      "Epoch 218, Loss: 0.4810898650639108\n",
      "Epoch 219, Loss: 0.48091365405597575\n",
      "Epoch 220, Loss: 0.480720996546175\n",
      "Epoch 221, Loss: 0.48054865040929456\n",
      "Epoch 222, Loss: 0.4803774677186426\n",
      "Epoch 223, Loss: 0.4801885671816001\n",
      "Epoch 224, Loss: 0.48000935557804647\n",
      "Epoch 225, Loss: 0.4798350303004208\n",
      "Epoch 226, Loss: 0.47967842665052907\n",
      "Epoch 227, Loss: 0.47947854451894606\n",
      "Epoch 228, Loss: 0.47928199900429136\n",
      "Epoch 229, Loss: 0.4791424752085709\n",
      "Epoch 230, Loss: 0.4789254904984497\n",
      "Epoch 231, Loss: 0.4787155823499749\n",
      "Epoch 232, Loss: 0.4785186611779873\n",
      "Epoch 233, Loss: 0.4783287773458093\n",
      "Epoch 234, Loss: 0.4781774454711345\n",
      "Epoch 235, Loss: 0.4780410437330356\n",
      "Epoch 236, Loss: 0.4778263811299359\n",
      "Epoch 237, Loss: 0.47765285017795805\n",
      "Epoch 238, Loss: 0.4775436338740249\n",
      "Epoch 239, Loss: 0.4774342833479412\n",
      "Epoch 240, Loss: 0.477263591547843\n",
      "Epoch 241, Loss: 0.47707859610269837\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.273142300777418\n",
      "Valence RMSE: 0.2528872365771786\n",
      "Arousal RMSE: 0.2919956823727384\n",
      "Test R^2 score: tensor([0.2250, 0.1558], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1328, 0.0553], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.190367317144521\n",
      "Num of epochs: 242\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Epoch 121, Loss: 0.5116998508836337\n",
      "Epoch 122, Loss: 0.5111129902664598\n",
      "Epoch 123, Loss: 0.5105405741436312\n",
      "Epoch 124, Loss: 0.5099849010083686\n",
      "Epoch 125, Loss: 0.5094623759868415\n",
      "Epoch 126, Loss: 0.5089652841754122\n",
      "Epoch 127, Loss: 0.5084717799132713\n",
      "Epoch 128, Loss: 0.5079809643002384\n",
      "Epoch 129, Loss: 0.507507731608986\n",
      "Epoch 130, Loss: 0.5070515727009828\n",
      "Epoch 131, Loss: 0.5065962678643066\n",
      "Epoch 132, Loss: 0.5061448812256618\n",
      "Epoch 133, Loss: 0.5057069703510908\n",
      "Epoch 134, Loss: 0.5052756693952567\n",
      "Epoch 135, Loss: 0.5048445017462813\n",
      "Epoch 136, Loss: 0.504425284231979\n",
      "Epoch 137, Loss: 0.504022717877823\n",
      "Epoch 138, Loss: 0.5036312802042249\n",
      "Epoch 139, Loss: 0.5032368731068207\n",
      "Epoch 140, Loss: 0.5028511060044271\n",
      "Epoch 141, Loss: 0.5024804933282331\n",
      "Epoch 142, Loss: 0.5021182727396313\n",
      "Epoch 143, Loss: 0.5017581664990297\n",
      "Epoch 144, Loss: 0.5014050827892457\n",
      "Epoch 145, Loss: 0.501047675902349\n",
      "Epoch 146, Loss: 0.500690966247362\n",
      "Epoch 147, Loss: 0.5003476722262875\n",
      "Epoch 148, Loss: 0.5000068544871652\n",
      "Epoch 149, Loss: 0.49966338867073357\n",
      "Epoch 150, Loss: 0.4993280873087794\n",
      "Epoch 151, Loss: 0.49900257940428383\n",
      "Epoch 152, Loss: 0.4986865554458311\n",
      "Epoch 153, Loss: 0.49836949389755036\n",
      "Epoch 154, Loss: 0.49805375636713634\n",
      "Epoch 155, Loss: 0.49773873165141763\n",
      "Epoch 156, Loss: 0.49742224924400874\n",
      "Epoch 157, Loss: 0.49711425826812494\n",
      "Epoch 158, Loss: 0.4968010973402242\n",
      "Epoch 159, Loss: 0.4964901399315246\n",
      "Epoch 160, Loss: 0.49617675026847124\n",
      "Epoch 161, Loss: 0.4958670390985117\n",
      "Epoch 162, Loss: 0.4955578109301908\n",
      "Epoch 163, Loss: 0.4952510223984696\n",
      "Epoch 164, Loss: 0.49494577484111674\n",
      "Epoch 165, Loss: 0.49464511374066544\n",
      "Epoch 166, Loss: 0.49434383269975535\n",
      "Epoch 167, Loss: 0.49404631909698943\n",
      "Epoch 168, Loss: 0.49374530645143583\n",
      "Epoch 169, Loss: 0.49345064806438055\n",
      "Epoch 170, Loss: 0.49317001490258333\n",
      "Epoch 171, Loss: 0.4928927591216884\n",
      "Epoch 172, Loss: 0.4926138499628376\n",
      "Epoch 173, Loss: 0.49233528219511824\n",
      "Epoch 174, Loss: 0.49205852514110515\n",
      "Epoch 175, Loss: 0.4917773399684819\n",
      "Epoch 176, Loss: 0.49149629710928233\n",
      "Epoch 177, Loss: 0.4912165647150265\n",
      "Epoch 178, Loss: 0.49094181765497674\n",
      "Epoch 179, Loss: 0.49066901222163783\n",
      "Epoch 180, Loss: 0.490393928005003\n",
      "Epoch 181, Loss: 0.4901079113527018\n",
      "Epoch 182, Loss: 0.48981984154671765\n",
      "Epoch 183, Loss: 0.4895234443341763\n",
      "Epoch 184, Loss: 0.48921459258368105\n",
      "Epoch 185, Loss: 0.4889145977883947\n",
      "Epoch 186, Loss: 0.4886175142174919\n",
      "Epoch 187, Loss: 0.4883282295514753\n",
      "Epoch 188, Loss: 0.4880435212302353\n",
      "Epoch 189, Loss: 0.48775699700499114\n",
      "Epoch 190, Loss: 0.48748008613994775\n",
      "Epoch 191, Loss: 0.4872132179171775\n",
      "Epoch 192, Loss: 0.48694738158534456\n",
      "Epoch 193, Loss: 0.48668121634032785\n",
      "Epoch 194, Loss: 0.4864280322301768\n",
      "Epoch 195, Loss: 0.486178532155915\n",
      "Epoch 196, Loss: 0.4859301919172944\n",
      "Epoch 197, Loss: 0.48568442460989747\n",
      "Epoch 198, Loss: 0.48544224711238787\n",
      "Epoch 199, Loss: 0.48519729219516017\n",
      "Epoch 200, Loss: 0.4849575446659644\n",
      "Epoch 201, Loss: 0.4847146197220469\n",
      "Epoch 202, Loss: 0.48447155759133576\n",
      "Epoch 203, Loss: 0.48422783492589483\n",
      "Epoch 204, Loss: 0.48397707745066637\n",
      "Epoch 205, Loss: 0.483737926529631\n",
      "Epoch 206, Loss: 0.4835020782648923\n",
      "Epoch 207, Loss: 0.48327083252052233\n",
      "Epoch 208, Loss: 0.4830508282762034\n",
      "Epoch 209, Loss: 0.48283269895503794\n",
      "Epoch 210, Loss: 0.48261215534620283\n",
      "Epoch 211, Loss: 0.48240746542225255\n",
      "Epoch 212, Loss: 0.4822187420754367\n",
      "Epoch 213, Loss: 0.48202997575341067\n",
      "Epoch 214, Loss: 0.4818441197783379\n",
      "Epoch 215, Loss: 0.4816502256898402\n",
      "Epoch 216, Loss: 0.4814589616492725\n",
      "Epoch 217, Loss: 0.48127338055379854\n",
      "Epoch 218, Loss: 0.4810898650639108\n",
      "Epoch 219, Loss: 0.48091365405597575\n",
      "Epoch 220, Loss: 0.480720996546175\n",
      "Epoch 221, Loss: 0.48054865040929456\n",
      "Epoch 222, Loss: 0.4803774677186426\n",
      "Epoch 223, Loss: 0.4801885671816001\n",
      "Epoch 224, Loss: 0.48000935557804647\n",
      "Epoch 225, Loss: 0.4798350303004208\n",
      "Epoch 226, Loss: 0.47967842665052907\n",
      "Epoch 227, Loss: 0.47947854451894606\n",
      "Epoch 228, Loss: 0.47928199900429136\n",
      "Epoch 229, Loss: 0.4791424752085709\n",
      "Epoch 230, Loss: 0.4789254904984497\n",
      "Epoch 231, Loss: 0.4787155823499749\n",
      "Epoch 232, Loss: 0.4785186611779873\n",
      "Epoch 233, Loss: 0.4783287773458093\n",
      "Epoch 234, Loss: 0.4781774454711345\n",
      "Epoch 235, Loss: 0.4780410437330356\n",
      "Epoch 236, Loss: 0.4778263811299359\n",
      "Epoch 237, Loss: 0.47765285017795805\n",
      "Epoch 238, Loss: 0.4775436338740249\n",
      "Epoch 239, Loss: 0.4774342833479412\n",
      "Epoch 240, Loss: 0.477263591547843\n",
      "Epoch 241, Loss: 0.47707859610269837\n",
      "Epoch 242, Loss: 0.47697972966255553\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.27333751255099553\n",
      "Valence RMSE: 0.25303734140436007\n",
      "Arousal RMSE: 0.29223089396938273\n",
      "Test R^2 score: tensor([0.2240, 0.1544], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1317, 0.0538], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.1892268195128205\n",
      "Num of epochs: 243\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Epoch 121, Loss: 0.5116998508836337\n",
      "Epoch 122, Loss: 0.5111129902664598\n",
      "Epoch 123, Loss: 0.5105405741436312\n",
      "Epoch 124, Loss: 0.5099849010083686\n",
      "Epoch 125, Loss: 0.5094623759868415\n",
      "Epoch 126, Loss: 0.5089652841754122\n",
      "Epoch 127, Loss: 0.5084717799132713\n",
      "Epoch 128, Loss: 0.5079809643002384\n",
      "Epoch 129, Loss: 0.507507731608986\n",
      "Epoch 130, Loss: 0.5070515727009828\n",
      "Epoch 131, Loss: 0.5065962678643066\n",
      "Epoch 132, Loss: 0.5061448812256618\n",
      "Epoch 133, Loss: 0.5057069703510908\n",
      "Epoch 134, Loss: 0.5052756693952567\n",
      "Epoch 135, Loss: 0.5048445017462813\n",
      "Epoch 136, Loss: 0.504425284231979\n",
      "Epoch 137, Loss: 0.504022717877823\n",
      "Epoch 138, Loss: 0.5036312802042249\n",
      "Epoch 139, Loss: 0.5032368731068207\n",
      "Epoch 140, Loss: 0.5028511060044271\n",
      "Epoch 141, Loss: 0.5024804933282331\n",
      "Epoch 142, Loss: 0.5021182727396313\n",
      "Epoch 143, Loss: 0.5017581664990297\n",
      "Epoch 144, Loss: 0.5014050827892457\n",
      "Epoch 145, Loss: 0.501047675902349\n",
      "Epoch 146, Loss: 0.500690966247362\n",
      "Epoch 147, Loss: 0.5003476722262875\n",
      "Epoch 148, Loss: 0.5000068544871652\n",
      "Epoch 149, Loss: 0.49966338867073357\n",
      "Epoch 150, Loss: 0.4993280873087794\n",
      "Epoch 151, Loss: 0.49900257940428383\n",
      "Epoch 152, Loss: 0.4986865554458311\n",
      "Epoch 153, Loss: 0.49836949389755036\n",
      "Epoch 154, Loss: 0.49805375636713634\n",
      "Epoch 155, Loss: 0.49773873165141763\n",
      "Epoch 156, Loss: 0.49742224924400874\n",
      "Epoch 157, Loss: 0.49711425826812494\n",
      "Epoch 158, Loss: 0.4968010973402242\n",
      "Epoch 159, Loss: 0.4964901399315246\n",
      "Epoch 160, Loss: 0.49617675026847124\n",
      "Epoch 161, Loss: 0.4958670390985117\n",
      "Epoch 162, Loss: 0.4955578109301908\n",
      "Epoch 163, Loss: 0.4952510223984696\n",
      "Epoch 164, Loss: 0.49494577484111674\n",
      "Epoch 165, Loss: 0.49464511374066544\n",
      "Epoch 166, Loss: 0.49434383269975535\n",
      "Epoch 167, Loss: 0.49404631909698943\n",
      "Epoch 168, Loss: 0.49374530645143583\n",
      "Epoch 169, Loss: 0.49345064806438055\n",
      "Epoch 170, Loss: 0.49317001490258333\n",
      "Epoch 171, Loss: 0.4928927591216884\n",
      "Epoch 172, Loss: 0.4926138499628376\n",
      "Epoch 173, Loss: 0.49233528219511824\n",
      "Epoch 174, Loss: 0.49205852514110515\n",
      "Epoch 175, Loss: 0.4917773399684819\n",
      "Epoch 176, Loss: 0.49149629710928233\n",
      "Epoch 177, Loss: 0.4912165647150265\n",
      "Epoch 178, Loss: 0.49094181765497674\n",
      "Epoch 179, Loss: 0.49066901222163783\n",
      "Epoch 180, Loss: 0.490393928005003\n",
      "Epoch 181, Loss: 0.4901079113527018\n",
      "Epoch 182, Loss: 0.48981984154671765\n",
      "Epoch 183, Loss: 0.4895234443341763\n",
      "Epoch 184, Loss: 0.48921459258368105\n",
      "Epoch 185, Loss: 0.4889145977883947\n",
      "Epoch 186, Loss: 0.4886175142174919\n",
      "Epoch 187, Loss: 0.4883282295514753\n",
      "Epoch 188, Loss: 0.4880435212302353\n",
      "Epoch 189, Loss: 0.48775699700499114\n",
      "Epoch 190, Loss: 0.48748008613994775\n",
      "Epoch 191, Loss: 0.4872132179171775\n",
      "Epoch 192, Loss: 0.48694738158534456\n",
      "Epoch 193, Loss: 0.48668121634032785\n",
      "Epoch 194, Loss: 0.4864280322301768\n",
      "Epoch 195, Loss: 0.486178532155915\n",
      "Epoch 196, Loss: 0.4859301919172944\n",
      "Epoch 197, Loss: 0.48568442460989747\n",
      "Epoch 198, Loss: 0.48544224711238787\n",
      "Epoch 199, Loss: 0.48519729219516017\n",
      "Epoch 200, Loss: 0.4849575446659644\n",
      "Epoch 201, Loss: 0.4847146197220469\n",
      "Epoch 202, Loss: 0.48447155759133576\n",
      "Epoch 203, Loss: 0.48422783492589483\n",
      "Epoch 204, Loss: 0.48397707745066637\n",
      "Epoch 205, Loss: 0.483737926529631\n",
      "Epoch 206, Loss: 0.4835020782648923\n",
      "Epoch 207, Loss: 0.48327083252052233\n",
      "Epoch 208, Loss: 0.4830508282762034\n",
      "Epoch 209, Loss: 0.48283269895503794\n",
      "Epoch 210, Loss: 0.48261215534620283\n",
      "Epoch 211, Loss: 0.48240746542225255\n",
      "Epoch 212, Loss: 0.4822187420754367\n",
      "Epoch 213, Loss: 0.48202997575341067\n",
      "Epoch 214, Loss: 0.4818441197783379\n",
      "Epoch 215, Loss: 0.4816502256898402\n",
      "Epoch 216, Loss: 0.4814589616492725\n",
      "Epoch 217, Loss: 0.48127338055379854\n",
      "Epoch 218, Loss: 0.4810898650639108\n",
      "Epoch 219, Loss: 0.48091365405597575\n",
      "Epoch 220, Loss: 0.480720996546175\n",
      "Epoch 221, Loss: 0.48054865040929456\n",
      "Epoch 222, Loss: 0.4803774677186426\n",
      "Epoch 223, Loss: 0.4801885671816001\n",
      "Epoch 224, Loss: 0.48000935557804647\n",
      "Epoch 225, Loss: 0.4798350303004208\n",
      "Epoch 226, Loss: 0.47967842665052907\n",
      "Epoch 227, Loss: 0.47947854451894606\n",
      "Epoch 228, Loss: 0.47928199900429136\n",
      "Epoch 229, Loss: 0.4791424752085709\n",
      "Epoch 230, Loss: 0.4789254904984497\n",
      "Epoch 231, Loss: 0.4787155823499749\n",
      "Epoch 232, Loss: 0.4785186611779873\n",
      "Epoch 233, Loss: 0.4783287773458093\n",
      "Epoch 234, Loss: 0.4781774454711345\n",
      "Epoch 235, Loss: 0.4780410437330356\n",
      "Epoch 236, Loss: 0.4778263811299359\n",
      "Epoch 237, Loss: 0.47765285017795805\n",
      "Epoch 238, Loss: 0.4775436338740249\n",
      "Epoch 239, Loss: 0.4774342833479412\n",
      "Epoch 240, Loss: 0.477263591547843\n",
      "Epoch 241, Loss: 0.47707859610269837\n",
      "Epoch 242, Loss: 0.47697972966255553\n",
      "Epoch 243, Loss: 0.4768431884384324\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2736747753877862\n",
      "Valence RMSE: 0.2534129870511871\n",
      "Arousal RMSE: 0.2925365333781498\n",
      "Test R^2 score: tensor([0.2217, 0.1526], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1292, 0.0518], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.18718917150025244\n",
      "Num of epochs: 244\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Epoch 121, Loss: 0.5116998508836337\n",
      "Epoch 122, Loss: 0.5111129902664598\n",
      "Epoch 123, Loss: 0.5105405741436312\n",
      "Epoch 124, Loss: 0.5099849010083686\n",
      "Epoch 125, Loss: 0.5094623759868415\n",
      "Epoch 126, Loss: 0.5089652841754122\n",
      "Epoch 127, Loss: 0.5084717799132713\n",
      "Epoch 128, Loss: 0.5079809643002384\n",
      "Epoch 129, Loss: 0.507507731608986\n",
      "Epoch 130, Loss: 0.5070515727009828\n",
      "Epoch 131, Loss: 0.5065962678643066\n",
      "Epoch 132, Loss: 0.5061448812256618\n",
      "Epoch 133, Loss: 0.5057069703510908\n",
      "Epoch 134, Loss: 0.5052756693952567\n",
      "Epoch 135, Loss: 0.5048445017462813\n",
      "Epoch 136, Loss: 0.504425284231979\n",
      "Epoch 137, Loss: 0.504022717877823\n",
      "Epoch 138, Loss: 0.5036312802042249\n",
      "Epoch 139, Loss: 0.5032368731068207\n",
      "Epoch 140, Loss: 0.5028511060044271\n",
      "Epoch 141, Loss: 0.5024804933282331\n",
      "Epoch 142, Loss: 0.5021182727396313\n",
      "Epoch 143, Loss: 0.5017581664990297\n",
      "Epoch 144, Loss: 0.5014050827892457\n",
      "Epoch 145, Loss: 0.501047675902349\n",
      "Epoch 146, Loss: 0.500690966247362\n",
      "Epoch 147, Loss: 0.5003476722262875\n",
      "Epoch 148, Loss: 0.5000068544871652\n",
      "Epoch 149, Loss: 0.49966338867073357\n",
      "Epoch 150, Loss: 0.4993280873087794\n",
      "Epoch 151, Loss: 0.49900257940428383\n",
      "Epoch 152, Loss: 0.4986865554458311\n",
      "Epoch 153, Loss: 0.49836949389755036\n",
      "Epoch 154, Loss: 0.49805375636713634\n",
      "Epoch 155, Loss: 0.49773873165141763\n",
      "Epoch 156, Loss: 0.49742224924400874\n",
      "Epoch 157, Loss: 0.49711425826812494\n",
      "Epoch 158, Loss: 0.4968010973402242\n",
      "Epoch 159, Loss: 0.4964901399315246\n",
      "Epoch 160, Loss: 0.49617675026847124\n",
      "Epoch 161, Loss: 0.4958670390985117\n",
      "Epoch 162, Loss: 0.4955578109301908\n",
      "Epoch 163, Loss: 0.4952510223984696\n",
      "Epoch 164, Loss: 0.49494577484111674\n",
      "Epoch 165, Loss: 0.49464511374066544\n",
      "Epoch 166, Loss: 0.49434383269975535\n",
      "Epoch 167, Loss: 0.49404631909698943\n",
      "Epoch 168, Loss: 0.49374530645143583\n",
      "Epoch 169, Loss: 0.49345064806438055\n",
      "Epoch 170, Loss: 0.49317001490258333\n",
      "Epoch 171, Loss: 0.4928927591216884\n",
      "Epoch 172, Loss: 0.4926138499628376\n",
      "Epoch 173, Loss: 0.49233528219511824\n",
      "Epoch 174, Loss: 0.49205852514110515\n",
      "Epoch 175, Loss: 0.4917773399684819\n",
      "Epoch 176, Loss: 0.49149629710928233\n",
      "Epoch 177, Loss: 0.4912165647150265\n",
      "Epoch 178, Loss: 0.49094181765497674\n",
      "Epoch 179, Loss: 0.49066901222163783\n",
      "Epoch 180, Loss: 0.490393928005003\n",
      "Epoch 181, Loss: 0.4901079113527018\n",
      "Epoch 182, Loss: 0.48981984154671765\n",
      "Epoch 183, Loss: 0.4895234443341763\n",
      "Epoch 184, Loss: 0.48921459258368105\n",
      "Epoch 185, Loss: 0.4889145977883947\n",
      "Epoch 186, Loss: 0.4886175142174919\n",
      "Epoch 187, Loss: 0.4883282295514753\n",
      "Epoch 188, Loss: 0.4880435212302353\n",
      "Epoch 189, Loss: 0.48775699700499114\n",
      "Epoch 190, Loss: 0.48748008613994775\n",
      "Epoch 191, Loss: 0.4872132179171775\n",
      "Epoch 192, Loss: 0.48694738158534456\n",
      "Epoch 193, Loss: 0.48668121634032785\n",
      "Epoch 194, Loss: 0.4864280322301768\n",
      "Epoch 195, Loss: 0.486178532155915\n",
      "Epoch 196, Loss: 0.4859301919172944\n",
      "Epoch 197, Loss: 0.48568442460989747\n",
      "Epoch 198, Loss: 0.48544224711238787\n",
      "Epoch 199, Loss: 0.48519729219516017\n",
      "Epoch 200, Loss: 0.4849575446659644\n",
      "Epoch 201, Loss: 0.4847146197220469\n",
      "Epoch 202, Loss: 0.48447155759133576\n",
      "Epoch 203, Loss: 0.48422783492589483\n",
      "Epoch 204, Loss: 0.48397707745066637\n",
      "Epoch 205, Loss: 0.483737926529631\n",
      "Epoch 206, Loss: 0.4835020782648923\n",
      "Epoch 207, Loss: 0.48327083252052233\n",
      "Epoch 208, Loss: 0.4830508282762034\n",
      "Epoch 209, Loss: 0.48283269895503794\n",
      "Epoch 210, Loss: 0.48261215534620283\n",
      "Epoch 211, Loss: 0.48240746542225255\n",
      "Epoch 212, Loss: 0.4822187420754367\n",
      "Epoch 213, Loss: 0.48202997575341067\n",
      "Epoch 214, Loss: 0.4818441197783379\n",
      "Epoch 215, Loss: 0.4816502256898402\n",
      "Epoch 216, Loss: 0.4814589616492725\n",
      "Epoch 217, Loss: 0.48127338055379854\n",
      "Epoch 218, Loss: 0.4810898650639108\n",
      "Epoch 219, Loss: 0.48091365405597575\n",
      "Epoch 220, Loss: 0.480720996546175\n",
      "Epoch 221, Loss: 0.48054865040929456\n",
      "Epoch 222, Loss: 0.4803774677186426\n",
      "Epoch 223, Loss: 0.4801885671816001\n",
      "Epoch 224, Loss: 0.48000935557804647\n",
      "Epoch 225, Loss: 0.4798350303004208\n",
      "Epoch 226, Loss: 0.47967842665052907\n",
      "Epoch 227, Loss: 0.47947854451894606\n",
      "Epoch 228, Loss: 0.47928199900429136\n",
      "Epoch 229, Loss: 0.4791424752085709\n",
      "Epoch 230, Loss: 0.4789254904984497\n",
      "Epoch 231, Loss: 0.4787155823499749\n",
      "Epoch 232, Loss: 0.4785186611779873\n",
      "Epoch 233, Loss: 0.4783287773458093\n",
      "Epoch 234, Loss: 0.4781774454711345\n",
      "Epoch 235, Loss: 0.4780410437330356\n",
      "Epoch 236, Loss: 0.4778263811299359\n",
      "Epoch 237, Loss: 0.47765285017795805\n",
      "Epoch 238, Loss: 0.4775436338740249\n",
      "Epoch 239, Loss: 0.4774342833479412\n",
      "Epoch 240, Loss: 0.477263591547843\n",
      "Epoch 241, Loss: 0.47707859610269837\n",
      "Epoch 242, Loss: 0.47697972966255553\n",
      "Epoch 243, Loss: 0.4768431884384324\n",
      "Epoch 244, Loss: 0.4767408349977067\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2736513703834131\n",
      "Valence RMSE: 0.2533509591434193\n",
      "Arousal RMSE: 0.2925464690037282\n",
      "Test R^2 score: tensor([0.2221, 0.1526], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1296, 0.0518], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.18735086272204304\n",
      "Num of epochs: 245\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Epoch 121, Loss: 0.5116998508836337\n",
      "Epoch 122, Loss: 0.5111129902664598\n",
      "Epoch 123, Loss: 0.5105405741436312\n",
      "Epoch 124, Loss: 0.5099849010083686\n",
      "Epoch 125, Loss: 0.5094623759868415\n",
      "Epoch 126, Loss: 0.5089652841754122\n",
      "Epoch 127, Loss: 0.5084717799132713\n",
      "Epoch 128, Loss: 0.5079809643002384\n",
      "Epoch 129, Loss: 0.507507731608986\n",
      "Epoch 130, Loss: 0.5070515727009828\n",
      "Epoch 131, Loss: 0.5065962678643066\n",
      "Epoch 132, Loss: 0.5061448812256618\n",
      "Epoch 133, Loss: 0.5057069703510908\n",
      "Epoch 134, Loss: 0.5052756693952567\n",
      "Epoch 135, Loss: 0.5048445017462813\n",
      "Epoch 136, Loss: 0.504425284231979\n",
      "Epoch 137, Loss: 0.504022717877823\n",
      "Epoch 138, Loss: 0.5036312802042249\n",
      "Epoch 139, Loss: 0.5032368731068207\n",
      "Epoch 140, Loss: 0.5028511060044271\n",
      "Epoch 141, Loss: 0.5024804933282331\n",
      "Epoch 142, Loss: 0.5021182727396313\n",
      "Epoch 143, Loss: 0.5017581664990297\n",
      "Epoch 144, Loss: 0.5014050827892457\n",
      "Epoch 145, Loss: 0.501047675902349\n",
      "Epoch 146, Loss: 0.500690966247362\n",
      "Epoch 147, Loss: 0.5003476722262875\n",
      "Epoch 148, Loss: 0.5000068544871652\n",
      "Epoch 149, Loss: 0.49966338867073357\n",
      "Epoch 150, Loss: 0.4993280873087794\n",
      "Epoch 151, Loss: 0.49900257940428383\n",
      "Epoch 152, Loss: 0.4986865554458311\n",
      "Epoch 153, Loss: 0.49836949389755036\n",
      "Epoch 154, Loss: 0.49805375636713634\n",
      "Epoch 155, Loss: 0.49773873165141763\n",
      "Epoch 156, Loss: 0.49742224924400874\n",
      "Epoch 157, Loss: 0.49711425826812494\n",
      "Epoch 158, Loss: 0.4968010973402242\n",
      "Epoch 159, Loss: 0.4964901399315246\n",
      "Epoch 160, Loss: 0.49617675026847124\n",
      "Epoch 161, Loss: 0.4958670390985117\n",
      "Epoch 162, Loss: 0.4955578109301908\n",
      "Epoch 163, Loss: 0.4952510223984696\n",
      "Epoch 164, Loss: 0.49494577484111674\n",
      "Epoch 165, Loss: 0.49464511374066544\n",
      "Epoch 166, Loss: 0.49434383269975535\n",
      "Epoch 167, Loss: 0.49404631909698943\n",
      "Epoch 168, Loss: 0.49374530645143583\n",
      "Epoch 169, Loss: 0.49345064806438055\n",
      "Epoch 170, Loss: 0.49317001490258333\n",
      "Epoch 171, Loss: 0.4928927591216884\n",
      "Epoch 172, Loss: 0.4926138499628376\n",
      "Epoch 173, Loss: 0.49233528219511824\n",
      "Epoch 174, Loss: 0.49205852514110515\n",
      "Epoch 175, Loss: 0.4917773399684819\n",
      "Epoch 176, Loss: 0.49149629710928233\n",
      "Epoch 177, Loss: 0.4912165647150265\n",
      "Epoch 178, Loss: 0.49094181765497674\n",
      "Epoch 179, Loss: 0.49066901222163783\n",
      "Epoch 180, Loss: 0.490393928005003\n",
      "Epoch 181, Loss: 0.4901079113527018\n",
      "Epoch 182, Loss: 0.48981984154671765\n",
      "Epoch 183, Loss: 0.4895234443341763\n",
      "Epoch 184, Loss: 0.48921459258368105\n",
      "Epoch 185, Loss: 0.4889145977883947\n",
      "Epoch 186, Loss: 0.4886175142174919\n",
      "Epoch 187, Loss: 0.4883282295514753\n",
      "Epoch 188, Loss: 0.4880435212302353\n",
      "Epoch 189, Loss: 0.48775699700499114\n",
      "Epoch 190, Loss: 0.48748008613994775\n",
      "Epoch 191, Loss: 0.4872132179171775\n",
      "Epoch 192, Loss: 0.48694738158534456\n",
      "Epoch 193, Loss: 0.48668121634032785\n",
      "Epoch 194, Loss: 0.4864280322301768\n",
      "Epoch 195, Loss: 0.486178532155915\n",
      "Epoch 196, Loss: 0.4859301919172944\n",
      "Epoch 197, Loss: 0.48568442460989747\n",
      "Epoch 198, Loss: 0.48544224711238787\n",
      "Epoch 199, Loss: 0.48519729219516017\n",
      "Epoch 200, Loss: 0.4849575446659644\n",
      "Epoch 201, Loss: 0.4847146197220469\n",
      "Epoch 202, Loss: 0.48447155759133576\n",
      "Epoch 203, Loss: 0.48422783492589483\n",
      "Epoch 204, Loss: 0.48397707745066637\n",
      "Epoch 205, Loss: 0.483737926529631\n",
      "Epoch 206, Loss: 0.4835020782648923\n",
      "Epoch 207, Loss: 0.48327083252052233\n",
      "Epoch 208, Loss: 0.4830508282762034\n",
      "Epoch 209, Loss: 0.48283269895503794\n",
      "Epoch 210, Loss: 0.48261215534620283\n",
      "Epoch 211, Loss: 0.48240746542225255\n",
      "Epoch 212, Loss: 0.4822187420754367\n",
      "Epoch 213, Loss: 0.48202997575341067\n",
      "Epoch 214, Loss: 0.4818441197783379\n",
      "Epoch 215, Loss: 0.4816502256898402\n",
      "Epoch 216, Loss: 0.4814589616492725\n",
      "Epoch 217, Loss: 0.48127338055379854\n",
      "Epoch 218, Loss: 0.4810898650639108\n",
      "Epoch 219, Loss: 0.48091365405597575\n",
      "Epoch 220, Loss: 0.480720996546175\n",
      "Epoch 221, Loss: 0.48054865040929456\n",
      "Epoch 222, Loss: 0.4803774677186426\n",
      "Epoch 223, Loss: 0.4801885671816001\n",
      "Epoch 224, Loss: 0.48000935557804647\n",
      "Epoch 225, Loss: 0.4798350303004208\n",
      "Epoch 226, Loss: 0.47967842665052907\n",
      "Epoch 227, Loss: 0.47947854451894606\n",
      "Epoch 228, Loss: 0.47928199900429136\n",
      "Epoch 229, Loss: 0.4791424752085709\n",
      "Epoch 230, Loss: 0.4789254904984497\n",
      "Epoch 231, Loss: 0.4787155823499749\n",
      "Epoch 232, Loss: 0.4785186611779873\n",
      "Epoch 233, Loss: 0.4783287773458093\n",
      "Epoch 234, Loss: 0.4781774454711345\n",
      "Epoch 235, Loss: 0.4780410437330356\n",
      "Epoch 236, Loss: 0.4778263811299359\n",
      "Epoch 237, Loss: 0.47765285017795805\n",
      "Epoch 238, Loss: 0.4775436338740249\n",
      "Epoch 239, Loss: 0.4774342833479412\n",
      "Epoch 240, Loss: 0.477263591547843\n",
      "Epoch 241, Loss: 0.47707859610269837\n",
      "Epoch 242, Loss: 0.47697972966255553\n",
      "Epoch 243, Loss: 0.4768431884384324\n",
      "Epoch 244, Loss: 0.4767408349977067\n",
      "Epoch 245, Loss: 0.4765819952057135\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2738667880497357\n",
      "Valence RMSE: 0.25349288125735975\n",
      "Arousal RMSE: 0.2928265601771803\n",
      "Test R^2 score: tensor([0.2212, 0.1510], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1286, 0.0499], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.18610325986036713\n",
      "Num of epochs: 246\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Epoch 121, Loss: 0.5116998508836337\n",
      "Epoch 122, Loss: 0.5111129902664598\n",
      "Epoch 123, Loss: 0.5105405741436312\n",
      "Epoch 124, Loss: 0.5099849010083686\n",
      "Epoch 125, Loss: 0.5094623759868415\n",
      "Epoch 126, Loss: 0.5089652841754122\n",
      "Epoch 127, Loss: 0.5084717799132713\n",
      "Epoch 128, Loss: 0.5079809643002384\n",
      "Epoch 129, Loss: 0.507507731608986\n",
      "Epoch 130, Loss: 0.5070515727009828\n",
      "Epoch 131, Loss: 0.5065962678643066\n",
      "Epoch 132, Loss: 0.5061448812256618\n",
      "Epoch 133, Loss: 0.5057069703510908\n",
      "Epoch 134, Loss: 0.5052756693952567\n",
      "Epoch 135, Loss: 0.5048445017462813\n",
      "Epoch 136, Loss: 0.504425284231979\n",
      "Epoch 137, Loss: 0.504022717877823\n",
      "Epoch 138, Loss: 0.5036312802042249\n",
      "Epoch 139, Loss: 0.5032368731068207\n",
      "Epoch 140, Loss: 0.5028511060044271\n",
      "Epoch 141, Loss: 0.5024804933282331\n",
      "Epoch 142, Loss: 0.5021182727396313\n",
      "Epoch 143, Loss: 0.5017581664990297\n",
      "Epoch 144, Loss: 0.5014050827892457\n",
      "Epoch 145, Loss: 0.501047675902349\n",
      "Epoch 146, Loss: 0.500690966247362\n",
      "Epoch 147, Loss: 0.5003476722262875\n",
      "Epoch 148, Loss: 0.5000068544871652\n",
      "Epoch 149, Loss: 0.49966338867073357\n",
      "Epoch 150, Loss: 0.4993280873087794\n",
      "Epoch 151, Loss: 0.49900257940428383\n",
      "Epoch 152, Loss: 0.4986865554458311\n",
      "Epoch 153, Loss: 0.49836949389755036\n",
      "Epoch 154, Loss: 0.49805375636713634\n",
      "Epoch 155, Loss: 0.49773873165141763\n",
      "Epoch 156, Loss: 0.49742224924400874\n",
      "Epoch 157, Loss: 0.49711425826812494\n",
      "Epoch 158, Loss: 0.4968010973402242\n",
      "Epoch 159, Loss: 0.4964901399315246\n",
      "Epoch 160, Loss: 0.49617675026847124\n",
      "Epoch 161, Loss: 0.4958670390985117\n",
      "Epoch 162, Loss: 0.4955578109301908\n",
      "Epoch 163, Loss: 0.4952510223984696\n",
      "Epoch 164, Loss: 0.49494577484111674\n",
      "Epoch 165, Loss: 0.49464511374066544\n",
      "Epoch 166, Loss: 0.49434383269975535\n",
      "Epoch 167, Loss: 0.49404631909698943\n",
      "Epoch 168, Loss: 0.49374530645143583\n",
      "Epoch 169, Loss: 0.49345064806438055\n",
      "Epoch 170, Loss: 0.49317001490258333\n",
      "Epoch 171, Loss: 0.4928927591216884\n",
      "Epoch 172, Loss: 0.4926138499628376\n",
      "Epoch 173, Loss: 0.49233528219511824\n",
      "Epoch 174, Loss: 0.49205852514110515\n",
      "Epoch 175, Loss: 0.4917773399684819\n",
      "Epoch 176, Loss: 0.49149629710928233\n",
      "Epoch 177, Loss: 0.4912165647150265\n",
      "Epoch 178, Loss: 0.49094181765497674\n",
      "Epoch 179, Loss: 0.49066901222163783\n",
      "Epoch 180, Loss: 0.490393928005003\n",
      "Epoch 181, Loss: 0.4901079113527018\n",
      "Epoch 182, Loss: 0.48981984154671765\n",
      "Epoch 183, Loss: 0.4895234443341763\n",
      "Epoch 184, Loss: 0.48921459258368105\n",
      "Epoch 185, Loss: 0.4889145977883947\n",
      "Epoch 186, Loss: 0.4886175142174919\n",
      "Epoch 187, Loss: 0.4883282295514753\n",
      "Epoch 188, Loss: 0.4880435212302353\n",
      "Epoch 189, Loss: 0.48775699700499114\n",
      "Epoch 190, Loss: 0.48748008613994775\n",
      "Epoch 191, Loss: 0.4872132179171775\n",
      "Epoch 192, Loss: 0.48694738158534456\n",
      "Epoch 193, Loss: 0.48668121634032785\n",
      "Epoch 194, Loss: 0.4864280322301768\n",
      "Epoch 195, Loss: 0.486178532155915\n",
      "Epoch 196, Loss: 0.4859301919172944\n",
      "Epoch 197, Loss: 0.48568442460989747\n",
      "Epoch 198, Loss: 0.48544224711238787\n",
      "Epoch 199, Loss: 0.48519729219516017\n",
      "Epoch 200, Loss: 0.4849575446659644\n",
      "Epoch 201, Loss: 0.4847146197220469\n",
      "Epoch 202, Loss: 0.48447155759133576\n",
      "Epoch 203, Loss: 0.48422783492589483\n",
      "Epoch 204, Loss: 0.48397707745066637\n",
      "Epoch 205, Loss: 0.483737926529631\n",
      "Epoch 206, Loss: 0.4835020782648923\n",
      "Epoch 207, Loss: 0.48327083252052233\n",
      "Epoch 208, Loss: 0.4830508282762034\n",
      "Epoch 209, Loss: 0.48283269895503794\n",
      "Epoch 210, Loss: 0.48261215534620283\n",
      "Epoch 211, Loss: 0.48240746542225255\n",
      "Epoch 212, Loss: 0.4822187420754367\n",
      "Epoch 213, Loss: 0.48202997575341067\n",
      "Epoch 214, Loss: 0.4818441197783379\n",
      "Epoch 215, Loss: 0.4816502256898402\n",
      "Epoch 216, Loss: 0.4814589616492725\n",
      "Epoch 217, Loss: 0.48127338055379854\n",
      "Epoch 218, Loss: 0.4810898650639108\n",
      "Epoch 219, Loss: 0.48091365405597575\n",
      "Epoch 220, Loss: 0.480720996546175\n",
      "Epoch 221, Loss: 0.48054865040929456\n",
      "Epoch 222, Loss: 0.4803774677186426\n",
      "Epoch 223, Loss: 0.4801885671816001\n",
      "Epoch 224, Loss: 0.48000935557804647\n",
      "Epoch 225, Loss: 0.4798350303004208\n",
      "Epoch 226, Loss: 0.47967842665052907\n",
      "Epoch 227, Loss: 0.47947854451894606\n",
      "Epoch 228, Loss: 0.47928199900429136\n",
      "Epoch 229, Loss: 0.4791424752085709\n",
      "Epoch 230, Loss: 0.4789254904984497\n",
      "Epoch 231, Loss: 0.4787155823499749\n",
      "Epoch 232, Loss: 0.4785186611779873\n",
      "Epoch 233, Loss: 0.4783287773458093\n",
      "Epoch 234, Loss: 0.4781774454711345\n",
      "Epoch 235, Loss: 0.4780410437330356\n",
      "Epoch 236, Loss: 0.4778263811299359\n",
      "Epoch 237, Loss: 0.47765285017795805\n",
      "Epoch 238, Loss: 0.4775436338740249\n",
      "Epoch 239, Loss: 0.4774342833479412\n",
      "Epoch 240, Loss: 0.477263591547843\n",
      "Epoch 241, Loss: 0.47707859610269837\n",
      "Epoch 242, Loss: 0.47697972966255553\n",
      "Epoch 243, Loss: 0.4768431884384324\n",
      "Epoch 244, Loss: 0.4767408349977067\n",
      "Epoch 245, Loss: 0.4765819952057135\n",
      "Epoch 246, Loss: 0.4764706413665339\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.27403944907594596\n",
      "Valence RMSE: 0.25358947526493186\n",
      "Arousal RMSE: 0.29306589247906767\n",
      "Test R^2 score: tensor([0.2207, 0.1496], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1279, 0.0484], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.18511223728841608\n",
      "Num of epochs: 247\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Epoch 121, Loss: 0.5116998508836337\n",
      "Epoch 122, Loss: 0.5111129902664598\n",
      "Epoch 123, Loss: 0.5105405741436312\n",
      "Epoch 124, Loss: 0.5099849010083686\n",
      "Epoch 125, Loss: 0.5094623759868415\n",
      "Epoch 126, Loss: 0.5089652841754122\n",
      "Epoch 127, Loss: 0.5084717799132713\n",
      "Epoch 128, Loss: 0.5079809643002384\n",
      "Epoch 129, Loss: 0.507507731608986\n",
      "Epoch 130, Loss: 0.5070515727009828\n",
      "Epoch 131, Loss: 0.5065962678643066\n",
      "Epoch 132, Loss: 0.5061448812256618\n",
      "Epoch 133, Loss: 0.5057069703510908\n",
      "Epoch 134, Loss: 0.5052756693952567\n",
      "Epoch 135, Loss: 0.5048445017462813\n",
      "Epoch 136, Loss: 0.504425284231979\n",
      "Epoch 137, Loss: 0.504022717877823\n",
      "Epoch 138, Loss: 0.5036312802042249\n",
      "Epoch 139, Loss: 0.5032368731068207\n",
      "Epoch 140, Loss: 0.5028511060044271\n",
      "Epoch 141, Loss: 0.5024804933282331\n",
      "Epoch 142, Loss: 0.5021182727396313\n",
      "Epoch 143, Loss: 0.5017581664990297\n",
      "Epoch 144, Loss: 0.5014050827892457\n",
      "Epoch 145, Loss: 0.501047675902349\n",
      "Epoch 146, Loss: 0.500690966247362\n",
      "Epoch 147, Loss: 0.5003476722262875\n",
      "Epoch 148, Loss: 0.5000068544871652\n",
      "Epoch 149, Loss: 0.49966338867073357\n",
      "Epoch 150, Loss: 0.4993280873087794\n",
      "Epoch 151, Loss: 0.49900257940428383\n",
      "Epoch 152, Loss: 0.4986865554458311\n",
      "Epoch 153, Loss: 0.49836949389755036\n",
      "Epoch 154, Loss: 0.49805375636713634\n",
      "Epoch 155, Loss: 0.49773873165141763\n",
      "Epoch 156, Loss: 0.49742224924400874\n",
      "Epoch 157, Loss: 0.49711425826812494\n",
      "Epoch 158, Loss: 0.4968010973402242\n",
      "Epoch 159, Loss: 0.4964901399315246\n",
      "Epoch 160, Loss: 0.49617675026847124\n",
      "Epoch 161, Loss: 0.4958670390985117\n",
      "Epoch 162, Loss: 0.4955578109301908\n",
      "Epoch 163, Loss: 0.4952510223984696\n",
      "Epoch 164, Loss: 0.49494577484111674\n",
      "Epoch 165, Loss: 0.49464511374066544\n",
      "Epoch 166, Loss: 0.49434383269975535\n",
      "Epoch 167, Loss: 0.49404631909698943\n",
      "Epoch 168, Loss: 0.49374530645143583\n",
      "Epoch 169, Loss: 0.49345064806438055\n",
      "Epoch 170, Loss: 0.49317001490258333\n",
      "Epoch 171, Loss: 0.4928927591216884\n",
      "Epoch 172, Loss: 0.4926138499628376\n",
      "Epoch 173, Loss: 0.49233528219511824\n",
      "Epoch 174, Loss: 0.49205852514110515\n",
      "Epoch 175, Loss: 0.4917773399684819\n",
      "Epoch 176, Loss: 0.49149629710928233\n",
      "Epoch 177, Loss: 0.4912165647150265\n",
      "Epoch 178, Loss: 0.49094181765497674\n",
      "Epoch 179, Loss: 0.49066901222163783\n",
      "Epoch 180, Loss: 0.490393928005003\n",
      "Epoch 181, Loss: 0.4901079113527018\n",
      "Epoch 182, Loss: 0.48981984154671765\n",
      "Epoch 183, Loss: 0.4895234443341763\n",
      "Epoch 184, Loss: 0.48921459258368105\n",
      "Epoch 185, Loss: 0.4889145977883947\n",
      "Epoch 186, Loss: 0.4886175142174919\n",
      "Epoch 187, Loss: 0.4883282295514753\n",
      "Epoch 188, Loss: 0.4880435212302353\n",
      "Epoch 189, Loss: 0.48775699700499114\n",
      "Epoch 190, Loss: 0.48748008613994775\n",
      "Epoch 191, Loss: 0.4872132179171775\n",
      "Epoch 192, Loss: 0.48694738158534456\n",
      "Epoch 193, Loss: 0.48668121634032785\n",
      "Epoch 194, Loss: 0.4864280322301768\n",
      "Epoch 195, Loss: 0.486178532155915\n",
      "Epoch 196, Loss: 0.4859301919172944\n",
      "Epoch 197, Loss: 0.48568442460989747\n",
      "Epoch 198, Loss: 0.48544224711238787\n",
      "Epoch 199, Loss: 0.48519729219516017\n",
      "Epoch 200, Loss: 0.4849575446659644\n",
      "Epoch 201, Loss: 0.4847146197220469\n",
      "Epoch 202, Loss: 0.48447155759133576\n",
      "Epoch 203, Loss: 0.48422783492589483\n",
      "Epoch 204, Loss: 0.48397707745066637\n",
      "Epoch 205, Loss: 0.483737926529631\n",
      "Epoch 206, Loss: 0.4835020782648923\n",
      "Epoch 207, Loss: 0.48327083252052233\n",
      "Epoch 208, Loss: 0.4830508282762034\n",
      "Epoch 209, Loss: 0.48283269895503794\n",
      "Epoch 210, Loss: 0.48261215534620283\n",
      "Epoch 211, Loss: 0.48240746542225255\n",
      "Epoch 212, Loss: 0.4822187420754367\n",
      "Epoch 213, Loss: 0.48202997575341067\n",
      "Epoch 214, Loss: 0.4818441197783379\n",
      "Epoch 215, Loss: 0.4816502256898402\n",
      "Epoch 216, Loss: 0.4814589616492725\n",
      "Epoch 217, Loss: 0.48127338055379854\n",
      "Epoch 218, Loss: 0.4810898650639108\n",
      "Epoch 219, Loss: 0.48091365405597575\n",
      "Epoch 220, Loss: 0.480720996546175\n",
      "Epoch 221, Loss: 0.48054865040929456\n",
      "Epoch 222, Loss: 0.4803774677186426\n",
      "Epoch 223, Loss: 0.4801885671816001\n",
      "Epoch 224, Loss: 0.48000935557804647\n",
      "Epoch 225, Loss: 0.4798350303004208\n",
      "Epoch 226, Loss: 0.47967842665052907\n",
      "Epoch 227, Loss: 0.47947854451894606\n",
      "Epoch 228, Loss: 0.47928199900429136\n",
      "Epoch 229, Loss: 0.4791424752085709\n",
      "Epoch 230, Loss: 0.4789254904984497\n",
      "Epoch 231, Loss: 0.4787155823499749\n",
      "Epoch 232, Loss: 0.4785186611779873\n",
      "Epoch 233, Loss: 0.4783287773458093\n",
      "Epoch 234, Loss: 0.4781774454711345\n",
      "Epoch 235, Loss: 0.4780410437330356\n",
      "Epoch 236, Loss: 0.4778263811299359\n",
      "Epoch 237, Loss: 0.47765285017795805\n",
      "Epoch 238, Loss: 0.4775436338740249\n",
      "Epoch 239, Loss: 0.4774342833479412\n",
      "Epoch 240, Loss: 0.477263591547843\n",
      "Epoch 241, Loss: 0.47707859610269837\n",
      "Epoch 242, Loss: 0.47697972966255553\n",
      "Epoch 243, Loss: 0.4768431884384324\n",
      "Epoch 244, Loss: 0.4767408349977067\n",
      "Epoch 245, Loss: 0.4765819952057135\n",
      "Epoch 246, Loss: 0.4764706413665339\n",
      "Epoch 247, Loss: 0.47634462160009333\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2741279383666382\n",
      "Valence RMSE: 0.2535659891532165\n",
      "Arousal RMSE: 0.2932516706363966\n",
      "Test R^2 score: tensor([0.2208, 0.1485], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1281, 0.0472], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.18464514240056612\n",
      "Num of epochs: 248\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Epoch 121, Loss: 0.5116998508836337\n",
      "Epoch 122, Loss: 0.5111129902664598\n",
      "Epoch 123, Loss: 0.5105405741436312\n",
      "Epoch 124, Loss: 0.5099849010083686\n",
      "Epoch 125, Loss: 0.5094623759868415\n",
      "Epoch 126, Loss: 0.5089652841754122\n",
      "Epoch 127, Loss: 0.5084717799132713\n",
      "Epoch 128, Loss: 0.5079809643002384\n",
      "Epoch 129, Loss: 0.507507731608986\n",
      "Epoch 130, Loss: 0.5070515727009828\n",
      "Epoch 131, Loss: 0.5065962678643066\n",
      "Epoch 132, Loss: 0.5061448812256618\n",
      "Epoch 133, Loss: 0.5057069703510908\n",
      "Epoch 134, Loss: 0.5052756693952567\n",
      "Epoch 135, Loss: 0.5048445017462813\n",
      "Epoch 136, Loss: 0.504425284231979\n",
      "Epoch 137, Loss: 0.504022717877823\n",
      "Epoch 138, Loss: 0.5036312802042249\n",
      "Epoch 139, Loss: 0.5032368731068207\n",
      "Epoch 140, Loss: 0.5028511060044271\n",
      "Epoch 141, Loss: 0.5024804933282331\n",
      "Epoch 142, Loss: 0.5021182727396313\n",
      "Epoch 143, Loss: 0.5017581664990297\n",
      "Epoch 144, Loss: 0.5014050827892457\n",
      "Epoch 145, Loss: 0.501047675902349\n",
      "Epoch 146, Loss: 0.500690966247362\n",
      "Epoch 147, Loss: 0.5003476722262875\n",
      "Epoch 148, Loss: 0.5000068544871652\n",
      "Epoch 149, Loss: 0.49966338867073357\n",
      "Epoch 150, Loss: 0.4993280873087794\n",
      "Epoch 151, Loss: 0.49900257940428383\n",
      "Epoch 152, Loss: 0.4986865554458311\n",
      "Epoch 153, Loss: 0.49836949389755036\n",
      "Epoch 154, Loss: 0.49805375636713634\n",
      "Epoch 155, Loss: 0.49773873165141763\n",
      "Epoch 156, Loss: 0.49742224924400874\n",
      "Epoch 157, Loss: 0.49711425826812494\n",
      "Epoch 158, Loss: 0.4968010973402242\n",
      "Epoch 159, Loss: 0.4964901399315246\n",
      "Epoch 160, Loss: 0.49617675026847124\n",
      "Epoch 161, Loss: 0.4958670390985117\n",
      "Epoch 162, Loss: 0.4955578109301908\n",
      "Epoch 163, Loss: 0.4952510223984696\n",
      "Epoch 164, Loss: 0.49494577484111674\n",
      "Epoch 165, Loss: 0.49464511374066544\n",
      "Epoch 166, Loss: 0.49434383269975535\n",
      "Epoch 167, Loss: 0.49404631909698943\n",
      "Epoch 168, Loss: 0.49374530645143583\n",
      "Epoch 169, Loss: 0.49345064806438055\n",
      "Epoch 170, Loss: 0.49317001490258333\n",
      "Epoch 171, Loss: 0.4928927591216884\n",
      "Epoch 172, Loss: 0.4926138499628376\n",
      "Epoch 173, Loss: 0.49233528219511824\n",
      "Epoch 174, Loss: 0.49205852514110515\n",
      "Epoch 175, Loss: 0.4917773399684819\n",
      "Epoch 176, Loss: 0.49149629710928233\n",
      "Epoch 177, Loss: 0.4912165647150265\n",
      "Epoch 178, Loss: 0.49094181765497674\n",
      "Epoch 179, Loss: 0.49066901222163783\n",
      "Epoch 180, Loss: 0.490393928005003\n",
      "Epoch 181, Loss: 0.4901079113527018\n",
      "Epoch 182, Loss: 0.48981984154671765\n",
      "Epoch 183, Loss: 0.4895234443341763\n",
      "Epoch 184, Loss: 0.48921459258368105\n",
      "Epoch 185, Loss: 0.4889145977883947\n",
      "Epoch 186, Loss: 0.4886175142174919\n",
      "Epoch 187, Loss: 0.4883282295514753\n",
      "Epoch 188, Loss: 0.4880435212302353\n",
      "Epoch 189, Loss: 0.48775699700499114\n",
      "Epoch 190, Loss: 0.48748008613994775\n",
      "Epoch 191, Loss: 0.4872132179171775\n",
      "Epoch 192, Loss: 0.48694738158534456\n",
      "Epoch 193, Loss: 0.48668121634032785\n",
      "Epoch 194, Loss: 0.4864280322301768\n",
      "Epoch 195, Loss: 0.486178532155915\n",
      "Epoch 196, Loss: 0.4859301919172944\n",
      "Epoch 197, Loss: 0.48568442460989747\n",
      "Epoch 198, Loss: 0.48544224711238787\n",
      "Epoch 199, Loss: 0.48519729219516017\n",
      "Epoch 200, Loss: 0.4849575446659644\n",
      "Epoch 201, Loss: 0.4847146197220469\n",
      "Epoch 202, Loss: 0.48447155759133576\n",
      "Epoch 203, Loss: 0.48422783492589483\n",
      "Epoch 204, Loss: 0.48397707745066637\n",
      "Epoch 205, Loss: 0.483737926529631\n",
      "Epoch 206, Loss: 0.4835020782648923\n",
      "Epoch 207, Loss: 0.48327083252052233\n",
      "Epoch 208, Loss: 0.4830508282762034\n",
      "Epoch 209, Loss: 0.48283269895503794\n",
      "Epoch 210, Loss: 0.48261215534620283\n",
      "Epoch 211, Loss: 0.48240746542225255\n",
      "Epoch 212, Loss: 0.4822187420754367\n",
      "Epoch 213, Loss: 0.48202997575341067\n",
      "Epoch 214, Loss: 0.4818441197783379\n",
      "Epoch 215, Loss: 0.4816502256898402\n",
      "Epoch 216, Loss: 0.4814589616492725\n",
      "Epoch 217, Loss: 0.48127338055379854\n",
      "Epoch 218, Loss: 0.4810898650639108\n",
      "Epoch 219, Loss: 0.48091365405597575\n",
      "Epoch 220, Loss: 0.480720996546175\n",
      "Epoch 221, Loss: 0.48054865040929456\n",
      "Epoch 222, Loss: 0.4803774677186426\n",
      "Epoch 223, Loss: 0.4801885671816001\n",
      "Epoch 224, Loss: 0.48000935557804647\n",
      "Epoch 225, Loss: 0.4798350303004208\n",
      "Epoch 226, Loss: 0.47967842665052907\n",
      "Epoch 227, Loss: 0.47947854451894606\n",
      "Epoch 228, Loss: 0.47928199900429136\n",
      "Epoch 229, Loss: 0.4791424752085709\n",
      "Epoch 230, Loss: 0.4789254904984497\n",
      "Epoch 231, Loss: 0.4787155823499749\n",
      "Epoch 232, Loss: 0.4785186611779873\n",
      "Epoch 233, Loss: 0.4783287773458093\n",
      "Epoch 234, Loss: 0.4781774454711345\n",
      "Epoch 235, Loss: 0.4780410437330356\n",
      "Epoch 236, Loss: 0.4778263811299359\n",
      "Epoch 237, Loss: 0.47765285017795805\n",
      "Epoch 238, Loss: 0.4775436338740249\n",
      "Epoch 239, Loss: 0.4774342833479412\n",
      "Epoch 240, Loss: 0.477263591547843\n",
      "Epoch 241, Loss: 0.47707859610269837\n",
      "Epoch 242, Loss: 0.47697972966255553\n",
      "Epoch 243, Loss: 0.4768431884384324\n",
      "Epoch 244, Loss: 0.4767408349977067\n",
      "Epoch 245, Loss: 0.4765819952057135\n",
      "Epoch 246, Loss: 0.4764706413665339\n",
      "Epoch 247, Loss: 0.47634462160009333\n",
      "Epoch 248, Loss: 0.4762527991635621\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2743258945657761\n",
      "Valence RMSE: 0.25378413447909653\n",
      "Arousal RMSE: 0.2934331370948456\n",
      "Test R^2 score: tensor([0.2195, 0.1474], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1266, 0.0460], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.18344741672191406\n",
      "Num of epochs: 249\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Epoch 121, Loss: 0.5116998508836337\n",
      "Epoch 122, Loss: 0.5111129902664598\n",
      "Epoch 123, Loss: 0.5105405741436312\n",
      "Epoch 124, Loss: 0.5099849010083686\n",
      "Epoch 125, Loss: 0.5094623759868415\n",
      "Epoch 126, Loss: 0.5089652841754122\n",
      "Epoch 127, Loss: 0.5084717799132713\n",
      "Epoch 128, Loss: 0.5079809643002384\n",
      "Epoch 129, Loss: 0.507507731608986\n",
      "Epoch 130, Loss: 0.5070515727009828\n",
      "Epoch 131, Loss: 0.5065962678643066\n",
      "Epoch 132, Loss: 0.5061448812256618\n",
      "Epoch 133, Loss: 0.5057069703510908\n",
      "Epoch 134, Loss: 0.5052756693952567\n",
      "Epoch 135, Loss: 0.5048445017462813\n",
      "Epoch 136, Loss: 0.504425284231979\n",
      "Epoch 137, Loss: 0.504022717877823\n",
      "Epoch 138, Loss: 0.5036312802042249\n",
      "Epoch 139, Loss: 0.5032368731068207\n",
      "Epoch 140, Loss: 0.5028511060044271\n",
      "Epoch 141, Loss: 0.5024804933282331\n",
      "Epoch 142, Loss: 0.5021182727396313\n",
      "Epoch 143, Loss: 0.5017581664990297\n",
      "Epoch 144, Loss: 0.5014050827892457\n",
      "Epoch 145, Loss: 0.501047675902349\n",
      "Epoch 146, Loss: 0.500690966247362\n",
      "Epoch 147, Loss: 0.5003476722262875\n",
      "Epoch 148, Loss: 0.5000068544871652\n",
      "Epoch 149, Loss: 0.49966338867073357\n",
      "Epoch 150, Loss: 0.4993280873087794\n",
      "Epoch 151, Loss: 0.49900257940428383\n",
      "Epoch 152, Loss: 0.4986865554458311\n",
      "Epoch 153, Loss: 0.49836949389755036\n",
      "Epoch 154, Loss: 0.49805375636713634\n",
      "Epoch 155, Loss: 0.49773873165141763\n",
      "Epoch 156, Loss: 0.49742224924400874\n",
      "Epoch 157, Loss: 0.49711425826812494\n",
      "Epoch 158, Loss: 0.4968010973402242\n",
      "Epoch 159, Loss: 0.4964901399315246\n",
      "Epoch 160, Loss: 0.49617675026847124\n",
      "Epoch 161, Loss: 0.4958670390985117\n",
      "Epoch 162, Loss: 0.4955578109301908\n",
      "Epoch 163, Loss: 0.4952510223984696\n",
      "Epoch 164, Loss: 0.49494577484111674\n",
      "Epoch 165, Loss: 0.49464511374066544\n",
      "Epoch 166, Loss: 0.49434383269975535\n",
      "Epoch 167, Loss: 0.49404631909698943\n",
      "Epoch 168, Loss: 0.49374530645143583\n",
      "Epoch 169, Loss: 0.49345064806438055\n",
      "Epoch 170, Loss: 0.49317001490258333\n",
      "Epoch 171, Loss: 0.4928927591216884\n",
      "Epoch 172, Loss: 0.4926138499628376\n",
      "Epoch 173, Loss: 0.49233528219511824\n",
      "Epoch 174, Loss: 0.49205852514110515\n",
      "Epoch 175, Loss: 0.4917773399684819\n",
      "Epoch 176, Loss: 0.49149629710928233\n",
      "Epoch 177, Loss: 0.4912165647150265\n",
      "Epoch 178, Loss: 0.49094181765497674\n",
      "Epoch 179, Loss: 0.49066901222163783\n",
      "Epoch 180, Loss: 0.490393928005003\n",
      "Epoch 181, Loss: 0.4901079113527018\n",
      "Epoch 182, Loss: 0.48981984154671765\n",
      "Epoch 183, Loss: 0.4895234443341763\n",
      "Epoch 184, Loss: 0.48921459258368105\n",
      "Epoch 185, Loss: 0.4889145977883947\n",
      "Epoch 186, Loss: 0.4886175142174919\n",
      "Epoch 187, Loss: 0.4883282295514753\n",
      "Epoch 188, Loss: 0.4880435212302353\n",
      "Epoch 189, Loss: 0.48775699700499114\n",
      "Epoch 190, Loss: 0.48748008613994775\n",
      "Epoch 191, Loss: 0.4872132179171775\n",
      "Epoch 192, Loss: 0.48694738158534456\n",
      "Epoch 193, Loss: 0.48668121634032785\n",
      "Epoch 194, Loss: 0.4864280322301768\n",
      "Epoch 195, Loss: 0.486178532155915\n",
      "Epoch 196, Loss: 0.4859301919172944\n",
      "Epoch 197, Loss: 0.48568442460989747\n",
      "Epoch 198, Loss: 0.48544224711238787\n",
      "Epoch 199, Loss: 0.48519729219516017\n",
      "Epoch 200, Loss: 0.4849575446659644\n",
      "Epoch 201, Loss: 0.4847146197220469\n",
      "Epoch 202, Loss: 0.48447155759133576\n",
      "Epoch 203, Loss: 0.48422783492589483\n",
      "Epoch 204, Loss: 0.48397707745066637\n",
      "Epoch 205, Loss: 0.483737926529631\n",
      "Epoch 206, Loss: 0.4835020782648923\n",
      "Epoch 207, Loss: 0.48327083252052233\n",
      "Epoch 208, Loss: 0.4830508282762034\n",
      "Epoch 209, Loss: 0.48283269895503794\n",
      "Epoch 210, Loss: 0.48261215534620283\n",
      "Epoch 211, Loss: 0.48240746542225255\n",
      "Epoch 212, Loss: 0.4822187420754367\n",
      "Epoch 213, Loss: 0.48202997575341067\n",
      "Epoch 214, Loss: 0.4818441197783379\n",
      "Epoch 215, Loss: 0.4816502256898402\n",
      "Epoch 216, Loss: 0.4814589616492725\n",
      "Epoch 217, Loss: 0.48127338055379854\n",
      "Epoch 218, Loss: 0.4810898650639108\n",
      "Epoch 219, Loss: 0.48091365405597575\n",
      "Epoch 220, Loss: 0.480720996546175\n",
      "Epoch 221, Loss: 0.48054865040929456\n",
      "Epoch 222, Loss: 0.4803774677186426\n",
      "Epoch 223, Loss: 0.4801885671816001\n",
      "Epoch 224, Loss: 0.48000935557804647\n",
      "Epoch 225, Loss: 0.4798350303004208\n",
      "Epoch 226, Loss: 0.47967842665052907\n",
      "Epoch 227, Loss: 0.47947854451894606\n",
      "Epoch 228, Loss: 0.47928199900429136\n",
      "Epoch 229, Loss: 0.4791424752085709\n",
      "Epoch 230, Loss: 0.4789254904984497\n",
      "Epoch 231, Loss: 0.4787155823499749\n",
      "Epoch 232, Loss: 0.4785186611779873\n",
      "Epoch 233, Loss: 0.4783287773458093\n",
      "Epoch 234, Loss: 0.4781774454711345\n",
      "Epoch 235, Loss: 0.4780410437330356\n",
      "Epoch 236, Loss: 0.4778263811299359\n",
      "Epoch 237, Loss: 0.47765285017795805\n",
      "Epoch 238, Loss: 0.4775436338740249\n",
      "Epoch 239, Loss: 0.4774342833479412\n",
      "Epoch 240, Loss: 0.477263591547843\n",
      "Epoch 241, Loss: 0.47707859610269837\n",
      "Epoch 242, Loss: 0.47697972966255553\n",
      "Epoch 243, Loss: 0.4768431884384324\n",
      "Epoch 244, Loss: 0.4767408349977067\n",
      "Epoch 245, Loss: 0.4765819952057135\n",
      "Epoch 246, Loss: 0.4764706413665339\n",
      "Epoch 247, Loss: 0.47634462160009333\n",
      "Epoch 248, Loss: 0.4762527991635621\n",
      "Epoch 249, Loss: 0.4762014052862575\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.27418869805763935\n",
      "Valence RMSE: 0.25353057979293575\n",
      "Arousal RMSE: 0.2933958578353561\n",
      "Test R^2 score: tensor([0.2210, 0.1477], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1283, 0.0462], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.18433516951208934\n",
      "Num of epochs: 250\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Epoch 121, Loss: 0.5116998508836337\n",
      "Epoch 122, Loss: 0.5111129902664598\n",
      "Epoch 123, Loss: 0.5105405741436312\n",
      "Epoch 124, Loss: 0.5099849010083686\n",
      "Epoch 125, Loss: 0.5094623759868415\n",
      "Epoch 126, Loss: 0.5089652841754122\n",
      "Epoch 127, Loss: 0.5084717799132713\n",
      "Epoch 128, Loss: 0.5079809643002384\n",
      "Epoch 129, Loss: 0.507507731608986\n",
      "Epoch 130, Loss: 0.5070515727009828\n",
      "Epoch 131, Loss: 0.5065962678643066\n",
      "Epoch 132, Loss: 0.5061448812256618\n",
      "Epoch 133, Loss: 0.5057069703510908\n",
      "Epoch 134, Loss: 0.5052756693952567\n",
      "Epoch 135, Loss: 0.5048445017462813\n",
      "Epoch 136, Loss: 0.504425284231979\n",
      "Epoch 137, Loss: 0.504022717877823\n",
      "Epoch 138, Loss: 0.5036312802042249\n",
      "Epoch 139, Loss: 0.5032368731068207\n",
      "Epoch 140, Loss: 0.5028511060044271\n",
      "Epoch 141, Loss: 0.5024804933282331\n",
      "Epoch 142, Loss: 0.5021182727396313\n",
      "Epoch 143, Loss: 0.5017581664990297\n",
      "Epoch 144, Loss: 0.5014050827892457\n",
      "Epoch 145, Loss: 0.501047675902349\n",
      "Epoch 146, Loss: 0.500690966247362\n",
      "Epoch 147, Loss: 0.5003476722262875\n",
      "Epoch 148, Loss: 0.5000068544871652\n",
      "Epoch 149, Loss: 0.49966338867073357\n",
      "Epoch 150, Loss: 0.4993280873087794\n",
      "Epoch 151, Loss: 0.49900257940428383\n",
      "Epoch 152, Loss: 0.4986865554458311\n",
      "Epoch 153, Loss: 0.49836949389755036\n",
      "Epoch 154, Loss: 0.49805375636713634\n",
      "Epoch 155, Loss: 0.49773873165141763\n",
      "Epoch 156, Loss: 0.49742224924400874\n",
      "Epoch 157, Loss: 0.49711425826812494\n",
      "Epoch 158, Loss: 0.4968010973402242\n",
      "Epoch 159, Loss: 0.4964901399315246\n",
      "Epoch 160, Loss: 0.49617675026847124\n",
      "Epoch 161, Loss: 0.4958670390985117\n",
      "Epoch 162, Loss: 0.4955578109301908\n",
      "Epoch 163, Loss: 0.4952510223984696\n",
      "Epoch 164, Loss: 0.49494577484111674\n",
      "Epoch 165, Loss: 0.49464511374066544\n",
      "Epoch 166, Loss: 0.49434383269975535\n",
      "Epoch 167, Loss: 0.49404631909698943\n",
      "Epoch 168, Loss: 0.49374530645143583\n",
      "Epoch 169, Loss: 0.49345064806438055\n",
      "Epoch 170, Loss: 0.49317001490258333\n",
      "Epoch 171, Loss: 0.4928927591216884\n",
      "Epoch 172, Loss: 0.4926138499628376\n",
      "Epoch 173, Loss: 0.49233528219511824\n",
      "Epoch 174, Loss: 0.49205852514110515\n",
      "Epoch 175, Loss: 0.4917773399684819\n",
      "Epoch 176, Loss: 0.49149629710928233\n",
      "Epoch 177, Loss: 0.4912165647150265\n",
      "Epoch 178, Loss: 0.49094181765497674\n",
      "Epoch 179, Loss: 0.49066901222163783\n",
      "Epoch 180, Loss: 0.490393928005003\n",
      "Epoch 181, Loss: 0.4901079113527018\n",
      "Epoch 182, Loss: 0.48981984154671765\n",
      "Epoch 183, Loss: 0.4895234443341763\n",
      "Epoch 184, Loss: 0.48921459258368105\n",
      "Epoch 185, Loss: 0.4889145977883947\n",
      "Epoch 186, Loss: 0.4886175142174919\n",
      "Epoch 187, Loss: 0.4883282295514753\n",
      "Epoch 188, Loss: 0.4880435212302353\n",
      "Epoch 189, Loss: 0.48775699700499114\n",
      "Epoch 190, Loss: 0.48748008613994775\n",
      "Epoch 191, Loss: 0.4872132179171775\n",
      "Epoch 192, Loss: 0.48694738158534456\n",
      "Epoch 193, Loss: 0.48668121634032785\n",
      "Epoch 194, Loss: 0.4864280322301768\n",
      "Epoch 195, Loss: 0.486178532155915\n",
      "Epoch 196, Loss: 0.4859301919172944\n",
      "Epoch 197, Loss: 0.48568442460989747\n",
      "Epoch 198, Loss: 0.48544224711238787\n",
      "Epoch 199, Loss: 0.48519729219516017\n",
      "Epoch 200, Loss: 0.4849575446659644\n",
      "Epoch 201, Loss: 0.4847146197220469\n",
      "Epoch 202, Loss: 0.48447155759133576\n",
      "Epoch 203, Loss: 0.48422783492589483\n",
      "Epoch 204, Loss: 0.48397707745066637\n",
      "Epoch 205, Loss: 0.483737926529631\n",
      "Epoch 206, Loss: 0.4835020782648923\n",
      "Epoch 207, Loss: 0.48327083252052233\n",
      "Epoch 208, Loss: 0.4830508282762034\n",
      "Epoch 209, Loss: 0.48283269895503794\n",
      "Epoch 210, Loss: 0.48261215534620283\n",
      "Epoch 211, Loss: 0.48240746542225255\n",
      "Epoch 212, Loss: 0.4822187420754367\n",
      "Epoch 213, Loss: 0.48202997575341067\n",
      "Epoch 214, Loss: 0.4818441197783379\n",
      "Epoch 215, Loss: 0.4816502256898402\n",
      "Epoch 216, Loss: 0.4814589616492725\n",
      "Epoch 217, Loss: 0.48127338055379854\n",
      "Epoch 218, Loss: 0.4810898650639108\n",
      "Epoch 219, Loss: 0.48091365405597575\n",
      "Epoch 220, Loss: 0.480720996546175\n",
      "Epoch 221, Loss: 0.48054865040929456\n",
      "Epoch 222, Loss: 0.4803774677186426\n",
      "Epoch 223, Loss: 0.4801885671816001\n",
      "Epoch 224, Loss: 0.48000935557804647\n",
      "Epoch 225, Loss: 0.4798350303004208\n",
      "Epoch 226, Loss: 0.47967842665052907\n",
      "Epoch 227, Loss: 0.47947854451894606\n",
      "Epoch 228, Loss: 0.47928199900429136\n",
      "Epoch 229, Loss: 0.4791424752085709\n",
      "Epoch 230, Loss: 0.4789254904984497\n",
      "Epoch 231, Loss: 0.4787155823499749\n",
      "Epoch 232, Loss: 0.4785186611779873\n",
      "Epoch 233, Loss: 0.4783287773458093\n",
      "Epoch 234, Loss: 0.4781774454711345\n",
      "Epoch 235, Loss: 0.4780410437330356\n",
      "Epoch 236, Loss: 0.4778263811299359\n",
      "Epoch 237, Loss: 0.47765285017795805\n",
      "Epoch 238, Loss: 0.4775436338740249\n",
      "Epoch 239, Loss: 0.4774342833479412\n",
      "Epoch 240, Loss: 0.477263591547843\n",
      "Epoch 241, Loss: 0.47707859610269837\n",
      "Epoch 242, Loss: 0.47697972966255553\n",
      "Epoch 243, Loss: 0.4768431884384324\n",
      "Epoch 244, Loss: 0.4767408349977067\n",
      "Epoch 245, Loss: 0.4765819952057135\n",
      "Epoch 246, Loss: 0.4764706413665339\n",
      "Epoch 247, Loss: 0.47634462160009333\n",
      "Epoch 248, Loss: 0.4762527991635621\n",
      "Epoch 249, Loss: 0.4762014052862575\n",
      "Epoch 250, Loss: 0.4760283306209857\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2741135016135553\n",
      "Valence RMSE: 0.2534529400011984\n",
      "Arousal RMSE: 0.2933224006761811\n",
      "Test R^2 score: tensor([0.2215, 0.1481], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1289, 0.0467], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.18478705860233424\n",
      "Num of epochs: 251\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Epoch 121, Loss: 0.5116998508836337\n",
      "Epoch 122, Loss: 0.5111129902664598\n",
      "Epoch 123, Loss: 0.5105405741436312\n",
      "Epoch 124, Loss: 0.5099849010083686\n",
      "Epoch 125, Loss: 0.5094623759868415\n",
      "Epoch 126, Loss: 0.5089652841754122\n",
      "Epoch 127, Loss: 0.5084717799132713\n",
      "Epoch 128, Loss: 0.5079809643002384\n",
      "Epoch 129, Loss: 0.507507731608986\n",
      "Epoch 130, Loss: 0.5070515727009828\n",
      "Epoch 131, Loss: 0.5065962678643066\n",
      "Epoch 132, Loss: 0.5061448812256618\n",
      "Epoch 133, Loss: 0.5057069703510908\n",
      "Epoch 134, Loss: 0.5052756693952567\n",
      "Epoch 135, Loss: 0.5048445017462813\n",
      "Epoch 136, Loss: 0.504425284231979\n",
      "Epoch 137, Loss: 0.504022717877823\n",
      "Epoch 138, Loss: 0.5036312802042249\n",
      "Epoch 139, Loss: 0.5032368731068207\n",
      "Epoch 140, Loss: 0.5028511060044271\n",
      "Epoch 141, Loss: 0.5024804933282331\n",
      "Epoch 142, Loss: 0.5021182727396313\n",
      "Epoch 143, Loss: 0.5017581664990297\n",
      "Epoch 144, Loss: 0.5014050827892457\n",
      "Epoch 145, Loss: 0.501047675902349\n",
      "Epoch 146, Loss: 0.500690966247362\n",
      "Epoch 147, Loss: 0.5003476722262875\n",
      "Epoch 148, Loss: 0.5000068544871652\n",
      "Epoch 149, Loss: 0.49966338867073357\n",
      "Epoch 150, Loss: 0.4993280873087794\n",
      "Epoch 151, Loss: 0.49900257940428383\n",
      "Epoch 152, Loss: 0.4986865554458311\n",
      "Epoch 153, Loss: 0.49836949389755036\n",
      "Epoch 154, Loss: 0.49805375636713634\n",
      "Epoch 155, Loss: 0.49773873165141763\n",
      "Epoch 156, Loss: 0.49742224924400874\n",
      "Epoch 157, Loss: 0.49711425826812494\n",
      "Epoch 158, Loss: 0.4968010973402242\n",
      "Epoch 159, Loss: 0.4964901399315246\n",
      "Epoch 160, Loss: 0.49617675026847124\n",
      "Epoch 161, Loss: 0.4958670390985117\n",
      "Epoch 162, Loss: 0.4955578109301908\n",
      "Epoch 163, Loss: 0.4952510223984696\n",
      "Epoch 164, Loss: 0.49494577484111674\n",
      "Epoch 165, Loss: 0.49464511374066544\n",
      "Epoch 166, Loss: 0.49434383269975535\n",
      "Epoch 167, Loss: 0.49404631909698943\n",
      "Epoch 168, Loss: 0.49374530645143583\n",
      "Epoch 169, Loss: 0.49345064806438055\n",
      "Epoch 170, Loss: 0.49317001490258333\n",
      "Epoch 171, Loss: 0.4928927591216884\n",
      "Epoch 172, Loss: 0.4926138499628376\n",
      "Epoch 173, Loss: 0.49233528219511824\n",
      "Epoch 174, Loss: 0.49205852514110515\n",
      "Epoch 175, Loss: 0.4917773399684819\n",
      "Epoch 176, Loss: 0.49149629710928233\n",
      "Epoch 177, Loss: 0.4912165647150265\n",
      "Epoch 178, Loss: 0.49094181765497674\n",
      "Epoch 179, Loss: 0.49066901222163783\n",
      "Epoch 180, Loss: 0.490393928005003\n",
      "Epoch 181, Loss: 0.4901079113527018\n",
      "Epoch 182, Loss: 0.48981984154671765\n",
      "Epoch 183, Loss: 0.4895234443341763\n",
      "Epoch 184, Loss: 0.48921459258368105\n",
      "Epoch 185, Loss: 0.4889145977883947\n",
      "Epoch 186, Loss: 0.4886175142174919\n",
      "Epoch 187, Loss: 0.4883282295514753\n",
      "Epoch 188, Loss: 0.4880435212302353\n",
      "Epoch 189, Loss: 0.48775699700499114\n",
      "Epoch 190, Loss: 0.48748008613994775\n",
      "Epoch 191, Loss: 0.4872132179171775\n",
      "Epoch 192, Loss: 0.48694738158534456\n",
      "Epoch 193, Loss: 0.48668121634032785\n",
      "Epoch 194, Loss: 0.4864280322301768\n",
      "Epoch 195, Loss: 0.486178532155915\n",
      "Epoch 196, Loss: 0.4859301919172944\n",
      "Epoch 197, Loss: 0.48568442460989747\n",
      "Epoch 198, Loss: 0.48544224711238787\n",
      "Epoch 199, Loss: 0.48519729219516017\n",
      "Epoch 200, Loss: 0.4849575446659644\n",
      "Epoch 201, Loss: 0.4847146197220469\n",
      "Epoch 202, Loss: 0.48447155759133576\n",
      "Epoch 203, Loss: 0.48422783492589483\n",
      "Epoch 204, Loss: 0.48397707745066637\n",
      "Epoch 205, Loss: 0.483737926529631\n",
      "Epoch 206, Loss: 0.4835020782648923\n",
      "Epoch 207, Loss: 0.48327083252052233\n",
      "Epoch 208, Loss: 0.4830508282762034\n",
      "Epoch 209, Loss: 0.48283269895503794\n",
      "Epoch 210, Loss: 0.48261215534620283\n",
      "Epoch 211, Loss: 0.48240746542225255\n",
      "Epoch 212, Loss: 0.4822187420754367\n",
      "Epoch 213, Loss: 0.48202997575341067\n",
      "Epoch 214, Loss: 0.4818441197783379\n",
      "Epoch 215, Loss: 0.4816502256898402\n",
      "Epoch 216, Loss: 0.4814589616492725\n",
      "Epoch 217, Loss: 0.48127338055379854\n",
      "Epoch 218, Loss: 0.4810898650639108\n",
      "Epoch 219, Loss: 0.48091365405597575\n",
      "Epoch 220, Loss: 0.480720996546175\n",
      "Epoch 221, Loss: 0.48054865040929456\n",
      "Epoch 222, Loss: 0.4803774677186426\n",
      "Epoch 223, Loss: 0.4801885671816001\n",
      "Epoch 224, Loss: 0.48000935557804647\n",
      "Epoch 225, Loss: 0.4798350303004208\n",
      "Epoch 226, Loss: 0.47967842665052907\n",
      "Epoch 227, Loss: 0.47947854451894606\n",
      "Epoch 228, Loss: 0.47928199900429136\n",
      "Epoch 229, Loss: 0.4791424752085709\n",
      "Epoch 230, Loss: 0.4789254904984497\n",
      "Epoch 231, Loss: 0.4787155823499749\n",
      "Epoch 232, Loss: 0.4785186611779873\n",
      "Epoch 233, Loss: 0.4783287773458093\n",
      "Epoch 234, Loss: 0.4781774454711345\n",
      "Epoch 235, Loss: 0.4780410437330356\n",
      "Epoch 236, Loss: 0.4778263811299359\n",
      "Epoch 237, Loss: 0.47765285017795805\n",
      "Epoch 238, Loss: 0.4775436338740249\n",
      "Epoch 239, Loss: 0.4774342833479412\n",
      "Epoch 240, Loss: 0.477263591547843\n",
      "Epoch 241, Loss: 0.47707859610269837\n",
      "Epoch 242, Loss: 0.47697972966255553\n",
      "Epoch 243, Loss: 0.4768431884384324\n",
      "Epoch 244, Loss: 0.4767408349977067\n",
      "Epoch 245, Loss: 0.4765819952057135\n",
      "Epoch 246, Loss: 0.4764706413665339\n",
      "Epoch 247, Loss: 0.47634462160009333\n",
      "Epoch 248, Loss: 0.4762527991635621\n",
      "Epoch 249, Loss: 0.4762014052862575\n",
      "Epoch 250, Loss: 0.4760283306209857\n",
      "Epoch 251, Loss: 0.47592260832103706\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2744362615522826\n",
      "Valence RMSE: 0.2537908818436228\n",
      "Arousal RMSE: 0.2936336349988231\n",
      "Test R^2 score: tensor([0.2194, 0.1463], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1266, 0.0447], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.18284392230979507\n",
      "Num of epochs: 252\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Epoch 121, Loss: 0.5116998508836337\n",
      "Epoch 122, Loss: 0.5111129902664598\n",
      "Epoch 123, Loss: 0.5105405741436312\n",
      "Epoch 124, Loss: 0.5099849010083686\n",
      "Epoch 125, Loss: 0.5094623759868415\n",
      "Epoch 126, Loss: 0.5089652841754122\n",
      "Epoch 127, Loss: 0.5084717799132713\n",
      "Epoch 128, Loss: 0.5079809643002384\n",
      "Epoch 129, Loss: 0.507507731608986\n",
      "Epoch 130, Loss: 0.5070515727009828\n",
      "Epoch 131, Loss: 0.5065962678643066\n",
      "Epoch 132, Loss: 0.5061448812256618\n",
      "Epoch 133, Loss: 0.5057069703510908\n",
      "Epoch 134, Loss: 0.5052756693952567\n",
      "Epoch 135, Loss: 0.5048445017462813\n",
      "Epoch 136, Loss: 0.504425284231979\n",
      "Epoch 137, Loss: 0.504022717877823\n",
      "Epoch 138, Loss: 0.5036312802042249\n",
      "Epoch 139, Loss: 0.5032368731068207\n",
      "Epoch 140, Loss: 0.5028511060044271\n",
      "Epoch 141, Loss: 0.5024804933282331\n",
      "Epoch 142, Loss: 0.5021182727396313\n",
      "Epoch 143, Loss: 0.5017581664990297\n",
      "Epoch 144, Loss: 0.5014050827892457\n",
      "Epoch 145, Loss: 0.501047675902349\n",
      "Epoch 146, Loss: 0.500690966247362\n",
      "Epoch 147, Loss: 0.5003476722262875\n",
      "Epoch 148, Loss: 0.5000068544871652\n",
      "Epoch 149, Loss: 0.49966338867073357\n",
      "Epoch 150, Loss: 0.4993280873087794\n",
      "Epoch 151, Loss: 0.49900257940428383\n",
      "Epoch 152, Loss: 0.4986865554458311\n",
      "Epoch 153, Loss: 0.49836949389755036\n",
      "Epoch 154, Loss: 0.49805375636713634\n",
      "Epoch 155, Loss: 0.49773873165141763\n",
      "Epoch 156, Loss: 0.49742224924400874\n",
      "Epoch 157, Loss: 0.49711425826812494\n",
      "Epoch 158, Loss: 0.4968010973402242\n",
      "Epoch 159, Loss: 0.4964901399315246\n",
      "Epoch 160, Loss: 0.49617675026847124\n",
      "Epoch 161, Loss: 0.4958670390985117\n",
      "Epoch 162, Loss: 0.4955578109301908\n",
      "Epoch 163, Loss: 0.4952510223984696\n",
      "Epoch 164, Loss: 0.49494577484111674\n",
      "Epoch 165, Loss: 0.49464511374066544\n",
      "Epoch 166, Loss: 0.49434383269975535\n",
      "Epoch 167, Loss: 0.49404631909698943\n",
      "Epoch 168, Loss: 0.49374530645143583\n",
      "Epoch 169, Loss: 0.49345064806438055\n",
      "Epoch 170, Loss: 0.49317001490258333\n",
      "Epoch 171, Loss: 0.4928927591216884\n",
      "Epoch 172, Loss: 0.4926138499628376\n",
      "Epoch 173, Loss: 0.49233528219511824\n",
      "Epoch 174, Loss: 0.49205852514110515\n",
      "Epoch 175, Loss: 0.4917773399684819\n",
      "Epoch 176, Loss: 0.49149629710928233\n",
      "Epoch 177, Loss: 0.4912165647150265\n",
      "Epoch 178, Loss: 0.49094181765497674\n",
      "Epoch 179, Loss: 0.49066901222163783\n",
      "Epoch 180, Loss: 0.490393928005003\n",
      "Epoch 181, Loss: 0.4901079113527018\n",
      "Epoch 182, Loss: 0.48981984154671765\n",
      "Epoch 183, Loss: 0.4895234443341763\n",
      "Epoch 184, Loss: 0.48921459258368105\n",
      "Epoch 185, Loss: 0.4889145977883947\n",
      "Epoch 186, Loss: 0.4886175142174919\n",
      "Epoch 187, Loss: 0.4883282295514753\n",
      "Epoch 188, Loss: 0.4880435212302353\n",
      "Epoch 189, Loss: 0.48775699700499114\n",
      "Epoch 190, Loss: 0.48748008613994775\n",
      "Epoch 191, Loss: 0.4872132179171775\n",
      "Epoch 192, Loss: 0.48694738158534456\n",
      "Epoch 193, Loss: 0.48668121634032785\n",
      "Epoch 194, Loss: 0.4864280322301768\n",
      "Epoch 195, Loss: 0.486178532155915\n",
      "Epoch 196, Loss: 0.4859301919172944\n",
      "Epoch 197, Loss: 0.48568442460989747\n",
      "Epoch 198, Loss: 0.48544224711238787\n",
      "Epoch 199, Loss: 0.48519729219516017\n",
      "Epoch 200, Loss: 0.4849575446659644\n",
      "Epoch 201, Loss: 0.4847146197220469\n",
      "Epoch 202, Loss: 0.48447155759133576\n",
      "Epoch 203, Loss: 0.48422783492589483\n",
      "Epoch 204, Loss: 0.48397707745066637\n",
      "Epoch 205, Loss: 0.483737926529631\n",
      "Epoch 206, Loss: 0.4835020782648923\n",
      "Epoch 207, Loss: 0.48327083252052233\n",
      "Epoch 208, Loss: 0.4830508282762034\n",
      "Epoch 209, Loss: 0.48283269895503794\n",
      "Epoch 210, Loss: 0.48261215534620283\n",
      "Epoch 211, Loss: 0.48240746542225255\n",
      "Epoch 212, Loss: 0.4822187420754367\n",
      "Epoch 213, Loss: 0.48202997575341067\n",
      "Epoch 214, Loss: 0.4818441197783379\n",
      "Epoch 215, Loss: 0.4816502256898402\n",
      "Epoch 216, Loss: 0.4814589616492725\n",
      "Epoch 217, Loss: 0.48127338055379854\n",
      "Epoch 218, Loss: 0.4810898650639108\n",
      "Epoch 219, Loss: 0.48091365405597575\n",
      "Epoch 220, Loss: 0.480720996546175\n",
      "Epoch 221, Loss: 0.48054865040929456\n",
      "Epoch 222, Loss: 0.4803774677186426\n",
      "Epoch 223, Loss: 0.4801885671816001\n",
      "Epoch 224, Loss: 0.48000935557804647\n",
      "Epoch 225, Loss: 0.4798350303004208\n",
      "Epoch 226, Loss: 0.47967842665052907\n",
      "Epoch 227, Loss: 0.47947854451894606\n",
      "Epoch 228, Loss: 0.47928199900429136\n",
      "Epoch 229, Loss: 0.4791424752085709\n",
      "Epoch 230, Loss: 0.4789254904984497\n",
      "Epoch 231, Loss: 0.4787155823499749\n",
      "Epoch 232, Loss: 0.4785186611779873\n",
      "Epoch 233, Loss: 0.4783287773458093\n",
      "Epoch 234, Loss: 0.4781774454711345\n",
      "Epoch 235, Loss: 0.4780410437330356\n",
      "Epoch 236, Loss: 0.4778263811299359\n",
      "Epoch 237, Loss: 0.47765285017795805\n",
      "Epoch 238, Loss: 0.4775436338740249\n",
      "Epoch 239, Loss: 0.4774342833479412\n",
      "Epoch 240, Loss: 0.477263591547843\n",
      "Epoch 241, Loss: 0.47707859610269837\n",
      "Epoch 242, Loss: 0.47697972966255553\n",
      "Epoch 243, Loss: 0.4768431884384324\n",
      "Epoch 244, Loss: 0.4767408349977067\n",
      "Epoch 245, Loss: 0.4765819952057135\n",
      "Epoch 246, Loss: 0.4764706413665339\n",
      "Epoch 247, Loss: 0.47634462160009333\n",
      "Epoch 248, Loss: 0.4762527991635621\n",
      "Epoch 249, Loss: 0.4762014052862575\n",
      "Epoch 250, Loss: 0.4760283306209857\n",
      "Epoch 251, Loss: 0.47592260832103706\n",
      "Epoch 252, Loss: 0.475834869470249\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.274635608651342\n",
      "Valence RMSE: 0.25379477697330366\n",
      "Arousal RMSE: 0.2940027997479909\n",
      "Test R^2 score: tensor([0.2194, 0.1441], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1265, 0.0423], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.1817579332164489\n",
      "Num of epochs: 253\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Epoch 121, Loss: 0.5116998508836337\n",
      "Epoch 122, Loss: 0.5111129902664598\n",
      "Epoch 123, Loss: 0.5105405741436312\n",
      "Epoch 124, Loss: 0.5099849010083686\n",
      "Epoch 125, Loss: 0.5094623759868415\n",
      "Epoch 126, Loss: 0.5089652841754122\n",
      "Epoch 127, Loss: 0.5084717799132713\n",
      "Epoch 128, Loss: 0.5079809643002384\n",
      "Epoch 129, Loss: 0.507507731608986\n",
      "Epoch 130, Loss: 0.5070515727009828\n",
      "Epoch 131, Loss: 0.5065962678643066\n",
      "Epoch 132, Loss: 0.5061448812256618\n",
      "Epoch 133, Loss: 0.5057069703510908\n",
      "Epoch 134, Loss: 0.5052756693952567\n",
      "Epoch 135, Loss: 0.5048445017462813\n",
      "Epoch 136, Loss: 0.504425284231979\n",
      "Epoch 137, Loss: 0.504022717877823\n",
      "Epoch 138, Loss: 0.5036312802042249\n",
      "Epoch 139, Loss: 0.5032368731068207\n",
      "Epoch 140, Loss: 0.5028511060044271\n",
      "Epoch 141, Loss: 0.5024804933282331\n",
      "Epoch 142, Loss: 0.5021182727396313\n",
      "Epoch 143, Loss: 0.5017581664990297\n",
      "Epoch 144, Loss: 0.5014050827892457\n",
      "Epoch 145, Loss: 0.501047675902349\n",
      "Epoch 146, Loss: 0.500690966247362\n",
      "Epoch 147, Loss: 0.5003476722262875\n",
      "Epoch 148, Loss: 0.5000068544871652\n",
      "Epoch 149, Loss: 0.49966338867073357\n",
      "Epoch 150, Loss: 0.4993280873087794\n",
      "Epoch 151, Loss: 0.49900257940428383\n",
      "Epoch 152, Loss: 0.4986865554458311\n",
      "Epoch 153, Loss: 0.49836949389755036\n",
      "Epoch 154, Loss: 0.49805375636713634\n",
      "Epoch 155, Loss: 0.49773873165141763\n",
      "Epoch 156, Loss: 0.49742224924400874\n",
      "Epoch 157, Loss: 0.49711425826812494\n",
      "Epoch 158, Loss: 0.4968010973402242\n",
      "Epoch 159, Loss: 0.4964901399315246\n",
      "Epoch 160, Loss: 0.49617675026847124\n",
      "Epoch 161, Loss: 0.4958670390985117\n",
      "Epoch 162, Loss: 0.4955578109301908\n",
      "Epoch 163, Loss: 0.4952510223984696\n",
      "Epoch 164, Loss: 0.49494577484111674\n",
      "Epoch 165, Loss: 0.49464511374066544\n",
      "Epoch 166, Loss: 0.49434383269975535\n",
      "Epoch 167, Loss: 0.49404631909698943\n",
      "Epoch 168, Loss: 0.49374530645143583\n",
      "Epoch 169, Loss: 0.49345064806438055\n",
      "Epoch 170, Loss: 0.49317001490258333\n",
      "Epoch 171, Loss: 0.4928927591216884\n",
      "Epoch 172, Loss: 0.4926138499628376\n",
      "Epoch 173, Loss: 0.49233528219511824\n",
      "Epoch 174, Loss: 0.49205852514110515\n",
      "Epoch 175, Loss: 0.4917773399684819\n",
      "Epoch 176, Loss: 0.49149629710928233\n",
      "Epoch 177, Loss: 0.4912165647150265\n",
      "Epoch 178, Loss: 0.49094181765497674\n",
      "Epoch 179, Loss: 0.49066901222163783\n",
      "Epoch 180, Loss: 0.490393928005003\n",
      "Epoch 181, Loss: 0.4901079113527018\n",
      "Epoch 182, Loss: 0.48981984154671765\n",
      "Epoch 183, Loss: 0.4895234443341763\n",
      "Epoch 184, Loss: 0.48921459258368105\n",
      "Epoch 185, Loss: 0.4889145977883947\n",
      "Epoch 186, Loss: 0.4886175142174919\n",
      "Epoch 187, Loss: 0.4883282295514753\n",
      "Epoch 188, Loss: 0.4880435212302353\n",
      "Epoch 189, Loss: 0.48775699700499114\n",
      "Epoch 190, Loss: 0.48748008613994775\n",
      "Epoch 191, Loss: 0.4872132179171775\n",
      "Epoch 192, Loss: 0.48694738158534456\n",
      "Epoch 193, Loss: 0.48668121634032785\n",
      "Epoch 194, Loss: 0.4864280322301768\n",
      "Epoch 195, Loss: 0.486178532155915\n",
      "Epoch 196, Loss: 0.4859301919172944\n",
      "Epoch 197, Loss: 0.48568442460989747\n",
      "Epoch 198, Loss: 0.48544224711238787\n",
      "Epoch 199, Loss: 0.48519729219516017\n",
      "Epoch 200, Loss: 0.4849575446659644\n",
      "Epoch 201, Loss: 0.4847146197220469\n",
      "Epoch 202, Loss: 0.48447155759133576\n",
      "Epoch 203, Loss: 0.48422783492589483\n",
      "Epoch 204, Loss: 0.48397707745066637\n",
      "Epoch 205, Loss: 0.483737926529631\n",
      "Epoch 206, Loss: 0.4835020782648923\n",
      "Epoch 207, Loss: 0.48327083252052233\n",
      "Epoch 208, Loss: 0.4830508282762034\n",
      "Epoch 209, Loss: 0.48283269895503794\n",
      "Epoch 210, Loss: 0.48261215534620283\n",
      "Epoch 211, Loss: 0.48240746542225255\n",
      "Epoch 212, Loss: 0.4822187420754367\n",
      "Epoch 213, Loss: 0.48202997575341067\n",
      "Epoch 214, Loss: 0.4818441197783379\n",
      "Epoch 215, Loss: 0.4816502256898402\n",
      "Epoch 216, Loss: 0.4814589616492725\n",
      "Epoch 217, Loss: 0.48127338055379854\n",
      "Epoch 218, Loss: 0.4810898650639108\n",
      "Epoch 219, Loss: 0.48091365405597575\n",
      "Epoch 220, Loss: 0.480720996546175\n",
      "Epoch 221, Loss: 0.48054865040929456\n",
      "Epoch 222, Loss: 0.4803774677186426\n",
      "Epoch 223, Loss: 0.4801885671816001\n",
      "Epoch 224, Loss: 0.48000935557804647\n",
      "Epoch 225, Loss: 0.4798350303004208\n",
      "Epoch 226, Loss: 0.47967842665052907\n",
      "Epoch 227, Loss: 0.47947854451894606\n",
      "Epoch 228, Loss: 0.47928199900429136\n",
      "Epoch 229, Loss: 0.4791424752085709\n",
      "Epoch 230, Loss: 0.4789254904984497\n",
      "Epoch 231, Loss: 0.4787155823499749\n",
      "Epoch 232, Loss: 0.4785186611779873\n",
      "Epoch 233, Loss: 0.4783287773458093\n",
      "Epoch 234, Loss: 0.4781774454711345\n",
      "Epoch 235, Loss: 0.4780410437330356\n",
      "Epoch 236, Loss: 0.4778263811299359\n",
      "Epoch 237, Loss: 0.47765285017795805\n",
      "Epoch 238, Loss: 0.4775436338740249\n",
      "Epoch 239, Loss: 0.4774342833479412\n",
      "Epoch 240, Loss: 0.477263591547843\n",
      "Epoch 241, Loss: 0.47707859610269837\n",
      "Epoch 242, Loss: 0.47697972966255553\n",
      "Epoch 243, Loss: 0.4768431884384324\n",
      "Epoch 244, Loss: 0.4767408349977067\n",
      "Epoch 245, Loss: 0.4765819952057135\n",
      "Epoch 246, Loss: 0.4764706413665339\n",
      "Epoch 247, Loss: 0.47634462160009333\n",
      "Epoch 248, Loss: 0.4762527991635621\n",
      "Epoch 249, Loss: 0.4762014052862575\n",
      "Epoch 250, Loss: 0.4760283306209857\n",
      "Epoch 251, Loss: 0.47592260832103706\n",
      "Epoch 252, Loss: 0.475834869470249\n",
      "Epoch 253, Loss: 0.47570980894919\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.27474983258590474\n",
      "Valence RMSE: 0.25381729780670426\n",
      "Arousal RMSE: 0.29419673748372566\n",
      "Test R^2 score: tensor([0.2193, 0.1430], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1264, 0.0410], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.18112389940731566\n",
      "Num of epochs: 254\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Epoch 121, Loss: 0.5116998508836337\n",
      "Epoch 122, Loss: 0.5111129902664598\n",
      "Epoch 123, Loss: 0.5105405741436312\n",
      "Epoch 124, Loss: 0.5099849010083686\n",
      "Epoch 125, Loss: 0.5094623759868415\n",
      "Epoch 126, Loss: 0.5089652841754122\n",
      "Epoch 127, Loss: 0.5084717799132713\n",
      "Epoch 128, Loss: 0.5079809643002384\n",
      "Epoch 129, Loss: 0.507507731608986\n",
      "Epoch 130, Loss: 0.5070515727009828\n",
      "Epoch 131, Loss: 0.5065962678643066\n",
      "Epoch 132, Loss: 0.5061448812256618\n",
      "Epoch 133, Loss: 0.5057069703510908\n",
      "Epoch 134, Loss: 0.5052756693952567\n",
      "Epoch 135, Loss: 0.5048445017462813\n",
      "Epoch 136, Loss: 0.504425284231979\n",
      "Epoch 137, Loss: 0.504022717877823\n",
      "Epoch 138, Loss: 0.5036312802042249\n",
      "Epoch 139, Loss: 0.5032368731068207\n",
      "Epoch 140, Loss: 0.5028511060044271\n",
      "Epoch 141, Loss: 0.5024804933282331\n",
      "Epoch 142, Loss: 0.5021182727396313\n",
      "Epoch 143, Loss: 0.5017581664990297\n",
      "Epoch 144, Loss: 0.5014050827892457\n",
      "Epoch 145, Loss: 0.501047675902349\n",
      "Epoch 146, Loss: 0.500690966247362\n",
      "Epoch 147, Loss: 0.5003476722262875\n",
      "Epoch 148, Loss: 0.5000068544871652\n",
      "Epoch 149, Loss: 0.49966338867073357\n",
      "Epoch 150, Loss: 0.4993280873087794\n",
      "Epoch 151, Loss: 0.49900257940428383\n",
      "Epoch 152, Loss: 0.4986865554458311\n",
      "Epoch 153, Loss: 0.49836949389755036\n",
      "Epoch 154, Loss: 0.49805375636713634\n",
      "Epoch 155, Loss: 0.49773873165141763\n",
      "Epoch 156, Loss: 0.49742224924400874\n",
      "Epoch 157, Loss: 0.49711425826812494\n",
      "Epoch 158, Loss: 0.4968010973402242\n",
      "Epoch 159, Loss: 0.4964901399315246\n",
      "Epoch 160, Loss: 0.49617675026847124\n",
      "Epoch 161, Loss: 0.4958670390985117\n",
      "Epoch 162, Loss: 0.4955578109301908\n",
      "Epoch 163, Loss: 0.4952510223984696\n",
      "Epoch 164, Loss: 0.49494577484111674\n",
      "Epoch 165, Loss: 0.49464511374066544\n",
      "Epoch 166, Loss: 0.49434383269975535\n",
      "Epoch 167, Loss: 0.49404631909698943\n",
      "Epoch 168, Loss: 0.49374530645143583\n",
      "Epoch 169, Loss: 0.49345064806438055\n",
      "Epoch 170, Loss: 0.49317001490258333\n",
      "Epoch 171, Loss: 0.4928927591216884\n",
      "Epoch 172, Loss: 0.4926138499628376\n",
      "Epoch 173, Loss: 0.49233528219511824\n",
      "Epoch 174, Loss: 0.49205852514110515\n",
      "Epoch 175, Loss: 0.4917773399684819\n",
      "Epoch 176, Loss: 0.49149629710928233\n",
      "Epoch 177, Loss: 0.4912165647150265\n",
      "Epoch 178, Loss: 0.49094181765497674\n",
      "Epoch 179, Loss: 0.49066901222163783\n",
      "Epoch 180, Loss: 0.490393928005003\n",
      "Epoch 181, Loss: 0.4901079113527018\n",
      "Epoch 182, Loss: 0.48981984154671765\n",
      "Epoch 183, Loss: 0.4895234443341763\n",
      "Epoch 184, Loss: 0.48921459258368105\n",
      "Epoch 185, Loss: 0.4889145977883947\n",
      "Epoch 186, Loss: 0.4886175142174919\n",
      "Epoch 187, Loss: 0.4883282295514753\n",
      "Epoch 188, Loss: 0.4880435212302353\n",
      "Epoch 189, Loss: 0.48775699700499114\n",
      "Epoch 190, Loss: 0.48748008613994775\n",
      "Epoch 191, Loss: 0.4872132179171775\n",
      "Epoch 192, Loss: 0.48694738158534456\n",
      "Epoch 193, Loss: 0.48668121634032785\n",
      "Epoch 194, Loss: 0.4864280322301768\n",
      "Epoch 195, Loss: 0.486178532155915\n",
      "Epoch 196, Loss: 0.4859301919172944\n",
      "Epoch 197, Loss: 0.48568442460989747\n",
      "Epoch 198, Loss: 0.48544224711238787\n",
      "Epoch 199, Loss: 0.48519729219516017\n",
      "Epoch 200, Loss: 0.4849575446659644\n",
      "Epoch 201, Loss: 0.4847146197220469\n",
      "Epoch 202, Loss: 0.48447155759133576\n",
      "Epoch 203, Loss: 0.48422783492589483\n",
      "Epoch 204, Loss: 0.48397707745066637\n",
      "Epoch 205, Loss: 0.483737926529631\n",
      "Epoch 206, Loss: 0.4835020782648923\n",
      "Epoch 207, Loss: 0.48327083252052233\n",
      "Epoch 208, Loss: 0.4830508282762034\n",
      "Epoch 209, Loss: 0.48283269895503794\n",
      "Epoch 210, Loss: 0.48261215534620283\n",
      "Epoch 211, Loss: 0.48240746542225255\n",
      "Epoch 212, Loss: 0.4822187420754367\n",
      "Epoch 213, Loss: 0.48202997575341067\n",
      "Epoch 214, Loss: 0.4818441197783379\n",
      "Epoch 215, Loss: 0.4816502256898402\n",
      "Epoch 216, Loss: 0.4814589616492725\n",
      "Epoch 217, Loss: 0.48127338055379854\n",
      "Epoch 218, Loss: 0.4810898650639108\n",
      "Epoch 219, Loss: 0.48091365405597575\n",
      "Epoch 220, Loss: 0.480720996546175\n",
      "Epoch 221, Loss: 0.48054865040929456\n",
      "Epoch 222, Loss: 0.4803774677186426\n",
      "Epoch 223, Loss: 0.4801885671816001\n",
      "Epoch 224, Loss: 0.48000935557804647\n",
      "Epoch 225, Loss: 0.4798350303004208\n",
      "Epoch 226, Loss: 0.47967842665052907\n",
      "Epoch 227, Loss: 0.47947854451894606\n",
      "Epoch 228, Loss: 0.47928199900429136\n",
      "Epoch 229, Loss: 0.4791424752085709\n",
      "Epoch 230, Loss: 0.4789254904984497\n",
      "Epoch 231, Loss: 0.4787155823499749\n",
      "Epoch 232, Loss: 0.4785186611779873\n",
      "Epoch 233, Loss: 0.4783287773458093\n",
      "Epoch 234, Loss: 0.4781774454711345\n",
      "Epoch 235, Loss: 0.4780410437330356\n",
      "Epoch 236, Loss: 0.4778263811299359\n",
      "Epoch 237, Loss: 0.47765285017795805\n",
      "Epoch 238, Loss: 0.4775436338740249\n",
      "Epoch 239, Loss: 0.4774342833479412\n",
      "Epoch 240, Loss: 0.477263591547843\n",
      "Epoch 241, Loss: 0.47707859610269837\n",
      "Epoch 242, Loss: 0.47697972966255553\n",
      "Epoch 243, Loss: 0.4768431884384324\n",
      "Epoch 244, Loss: 0.4767408349977067\n",
      "Epoch 245, Loss: 0.4765819952057135\n",
      "Epoch 246, Loss: 0.4764706413665339\n",
      "Epoch 247, Loss: 0.47634462160009333\n",
      "Epoch 248, Loss: 0.4762527991635621\n",
      "Epoch 249, Loss: 0.4762014052862575\n",
      "Epoch 250, Loss: 0.4760283306209857\n",
      "Epoch 251, Loss: 0.47592260832103706\n",
      "Epoch 252, Loss: 0.475834869470249\n",
      "Epoch 253, Loss: 0.47570980894919\n",
      "Epoch 254, Loss: 0.47559397414543253\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2747728838093409\n",
      "Valence RMSE: 0.25381100310355115\n",
      "Arousal RMSE: 0.29424522095927447\n",
      "Test R^2 score: tensor([0.2193, 0.1427], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1264, 0.0407], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.18100201570502816\n",
      "Num of epochs: 255\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Epoch 121, Loss: 0.5116998508836337\n",
      "Epoch 122, Loss: 0.5111129902664598\n",
      "Epoch 123, Loss: 0.5105405741436312\n",
      "Epoch 124, Loss: 0.5099849010083686\n",
      "Epoch 125, Loss: 0.5094623759868415\n",
      "Epoch 126, Loss: 0.5089652841754122\n",
      "Epoch 127, Loss: 0.5084717799132713\n",
      "Epoch 128, Loss: 0.5079809643002384\n",
      "Epoch 129, Loss: 0.507507731608986\n",
      "Epoch 130, Loss: 0.5070515727009828\n",
      "Epoch 131, Loss: 0.5065962678643066\n",
      "Epoch 132, Loss: 0.5061448812256618\n",
      "Epoch 133, Loss: 0.5057069703510908\n",
      "Epoch 134, Loss: 0.5052756693952567\n",
      "Epoch 135, Loss: 0.5048445017462813\n",
      "Epoch 136, Loss: 0.504425284231979\n",
      "Epoch 137, Loss: 0.504022717877823\n",
      "Epoch 138, Loss: 0.5036312802042249\n",
      "Epoch 139, Loss: 0.5032368731068207\n",
      "Epoch 140, Loss: 0.5028511060044271\n",
      "Epoch 141, Loss: 0.5024804933282331\n",
      "Epoch 142, Loss: 0.5021182727396313\n",
      "Epoch 143, Loss: 0.5017581664990297\n",
      "Epoch 144, Loss: 0.5014050827892457\n",
      "Epoch 145, Loss: 0.501047675902349\n",
      "Epoch 146, Loss: 0.500690966247362\n",
      "Epoch 147, Loss: 0.5003476722262875\n",
      "Epoch 148, Loss: 0.5000068544871652\n",
      "Epoch 149, Loss: 0.49966338867073357\n",
      "Epoch 150, Loss: 0.4993280873087794\n",
      "Epoch 151, Loss: 0.49900257940428383\n",
      "Epoch 152, Loss: 0.4986865554458311\n",
      "Epoch 153, Loss: 0.49836949389755036\n",
      "Epoch 154, Loss: 0.49805375636713634\n",
      "Epoch 155, Loss: 0.49773873165141763\n",
      "Epoch 156, Loss: 0.49742224924400874\n",
      "Epoch 157, Loss: 0.49711425826812494\n",
      "Epoch 158, Loss: 0.4968010973402242\n",
      "Epoch 159, Loss: 0.4964901399315246\n",
      "Epoch 160, Loss: 0.49617675026847124\n",
      "Epoch 161, Loss: 0.4958670390985117\n",
      "Epoch 162, Loss: 0.4955578109301908\n",
      "Epoch 163, Loss: 0.4952510223984696\n",
      "Epoch 164, Loss: 0.49494577484111674\n",
      "Epoch 165, Loss: 0.49464511374066544\n",
      "Epoch 166, Loss: 0.49434383269975535\n",
      "Epoch 167, Loss: 0.49404631909698943\n",
      "Epoch 168, Loss: 0.49374530645143583\n",
      "Epoch 169, Loss: 0.49345064806438055\n",
      "Epoch 170, Loss: 0.49317001490258333\n",
      "Epoch 171, Loss: 0.4928927591216884\n",
      "Epoch 172, Loss: 0.4926138499628376\n",
      "Epoch 173, Loss: 0.49233528219511824\n",
      "Epoch 174, Loss: 0.49205852514110515\n",
      "Epoch 175, Loss: 0.4917773399684819\n",
      "Epoch 176, Loss: 0.49149629710928233\n",
      "Epoch 177, Loss: 0.4912165647150265\n",
      "Epoch 178, Loss: 0.49094181765497674\n",
      "Epoch 179, Loss: 0.49066901222163783\n",
      "Epoch 180, Loss: 0.490393928005003\n",
      "Epoch 181, Loss: 0.4901079113527018\n",
      "Epoch 182, Loss: 0.48981984154671765\n",
      "Epoch 183, Loss: 0.4895234443341763\n",
      "Epoch 184, Loss: 0.48921459258368105\n",
      "Epoch 185, Loss: 0.4889145977883947\n",
      "Epoch 186, Loss: 0.4886175142174919\n",
      "Epoch 187, Loss: 0.4883282295514753\n",
      "Epoch 188, Loss: 0.4880435212302353\n",
      "Epoch 189, Loss: 0.48775699700499114\n",
      "Epoch 190, Loss: 0.48748008613994775\n",
      "Epoch 191, Loss: 0.4872132179171775\n",
      "Epoch 192, Loss: 0.48694738158534456\n",
      "Epoch 193, Loss: 0.48668121634032785\n",
      "Epoch 194, Loss: 0.4864280322301768\n",
      "Epoch 195, Loss: 0.486178532155915\n",
      "Epoch 196, Loss: 0.4859301919172944\n",
      "Epoch 197, Loss: 0.48568442460989747\n",
      "Epoch 198, Loss: 0.48544224711238787\n",
      "Epoch 199, Loss: 0.48519729219516017\n",
      "Epoch 200, Loss: 0.4849575446659644\n",
      "Epoch 201, Loss: 0.4847146197220469\n",
      "Epoch 202, Loss: 0.48447155759133576\n",
      "Epoch 203, Loss: 0.48422783492589483\n",
      "Epoch 204, Loss: 0.48397707745066637\n",
      "Epoch 205, Loss: 0.483737926529631\n",
      "Epoch 206, Loss: 0.4835020782648923\n",
      "Epoch 207, Loss: 0.48327083252052233\n",
      "Epoch 208, Loss: 0.4830508282762034\n",
      "Epoch 209, Loss: 0.48283269895503794\n",
      "Epoch 210, Loss: 0.48261215534620283\n",
      "Epoch 211, Loss: 0.48240746542225255\n",
      "Epoch 212, Loss: 0.4822187420754367\n",
      "Epoch 213, Loss: 0.48202997575341067\n",
      "Epoch 214, Loss: 0.4818441197783379\n",
      "Epoch 215, Loss: 0.4816502256898402\n",
      "Epoch 216, Loss: 0.4814589616492725\n",
      "Epoch 217, Loss: 0.48127338055379854\n",
      "Epoch 218, Loss: 0.4810898650639108\n",
      "Epoch 219, Loss: 0.48091365405597575\n",
      "Epoch 220, Loss: 0.480720996546175\n",
      "Epoch 221, Loss: 0.48054865040929456\n",
      "Epoch 222, Loss: 0.4803774677186426\n",
      "Epoch 223, Loss: 0.4801885671816001\n",
      "Epoch 224, Loss: 0.48000935557804647\n",
      "Epoch 225, Loss: 0.4798350303004208\n",
      "Epoch 226, Loss: 0.47967842665052907\n",
      "Epoch 227, Loss: 0.47947854451894606\n",
      "Epoch 228, Loss: 0.47928199900429136\n",
      "Epoch 229, Loss: 0.4791424752085709\n",
      "Epoch 230, Loss: 0.4789254904984497\n",
      "Epoch 231, Loss: 0.4787155823499749\n",
      "Epoch 232, Loss: 0.4785186611779873\n",
      "Epoch 233, Loss: 0.4783287773458093\n",
      "Epoch 234, Loss: 0.4781774454711345\n",
      "Epoch 235, Loss: 0.4780410437330356\n",
      "Epoch 236, Loss: 0.4778263811299359\n",
      "Epoch 237, Loss: 0.47765285017795805\n",
      "Epoch 238, Loss: 0.4775436338740249\n",
      "Epoch 239, Loss: 0.4774342833479412\n",
      "Epoch 240, Loss: 0.477263591547843\n",
      "Epoch 241, Loss: 0.47707859610269837\n",
      "Epoch 242, Loss: 0.47697972966255553\n",
      "Epoch 243, Loss: 0.4768431884384324\n",
      "Epoch 244, Loss: 0.4767408349977067\n",
      "Epoch 245, Loss: 0.4765819952057135\n",
      "Epoch 246, Loss: 0.4764706413665339\n",
      "Epoch 247, Loss: 0.47634462160009333\n",
      "Epoch 248, Loss: 0.4762527991635621\n",
      "Epoch 249, Loss: 0.4762014052862575\n",
      "Epoch 250, Loss: 0.4760283306209857\n",
      "Epoch 251, Loss: 0.47592260832103706\n",
      "Epoch 252, Loss: 0.475834869470249\n",
      "Epoch 253, Loss: 0.47570980894919\n",
      "Epoch 254, Loss: 0.47559397414543253\n",
      "Epoch 255, Loss: 0.4755005182088922\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.27471686254544075\n",
      "Valence RMSE: 0.25364115037405177\n",
      "Arousal RMSE: 0.29428706388584064\n",
      "Test R^2 score: tensor([0.2203, 0.1425], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1276, 0.0404], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.18140237780856672\n",
      "Num of epochs: 256\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Epoch 121, Loss: 0.5116998508836337\n",
      "Epoch 122, Loss: 0.5111129902664598\n",
      "Epoch 123, Loss: 0.5105405741436312\n",
      "Epoch 124, Loss: 0.5099849010083686\n",
      "Epoch 125, Loss: 0.5094623759868415\n",
      "Epoch 126, Loss: 0.5089652841754122\n",
      "Epoch 127, Loss: 0.5084717799132713\n",
      "Epoch 128, Loss: 0.5079809643002384\n",
      "Epoch 129, Loss: 0.507507731608986\n",
      "Epoch 130, Loss: 0.5070515727009828\n",
      "Epoch 131, Loss: 0.5065962678643066\n",
      "Epoch 132, Loss: 0.5061448812256618\n",
      "Epoch 133, Loss: 0.5057069703510908\n",
      "Epoch 134, Loss: 0.5052756693952567\n",
      "Epoch 135, Loss: 0.5048445017462813\n",
      "Epoch 136, Loss: 0.504425284231979\n",
      "Epoch 137, Loss: 0.504022717877823\n",
      "Epoch 138, Loss: 0.5036312802042249\n",
      "Epoch 139, Loss: 0.5032368731068207\n",
      "Epoch 140, Loss: 0.5028511060044271\n",
      "Epoch 141, Loss: 0.5024804933282331\n",
      "Epoch 142, Loss: 0.5021182727396313\n",
      "Epoch 143, Loss: 0.5017581664990297\n",
      "Epoch 144, Loss: 0.5014050827892457\n",
      "Epoch 145, Loss: 0.501047675902349\n",
      "Epoch 146, Loss: 0.500690966247362\n",
      "Epoch 147, Loss: 0.5003476722262875\n",
      "Epoch 148, Loss: 0.5000068544871652\n",
      "Epoch 149, Loss: 0.49966338867073357\n",
      "Epoch 150, Loss: 0.4993280873087794\n",
      "Epoch 151, Loss: 0.49900257940428383\n",
      "Epoch 152, Loss: 0.4986865554458311\n",
      "Epoch 153, Loss: 0.49836949389755036\n",
      "Epoch 154, Loss: 0.49805375636713634\n",
      "Epoch 155, Loss: 0.49773873165141763\n",
      "Epoch 156, Loss: 0.49742224924400874\n",
      "Epoch 157, Loss: 0.49711425826812494\n",
      "Epoch 158, Loss: 0.4968010973402242\n",
      "Epoch 159, Loss: 0.4964901399315246\n",
      "Epoch 160, Loss: 0.49617675026847124\n",
      "Epoch 161, Loss: 0.4958670390985117\n",
      "Epoch 162, Loss: 0.4955578109301908\n",
      "Epoch 163, Loss: 0.4952510223984696\n",
      "Epoch 164, Loss: 0.49494577484111674\n",
      "Epoch 165, Loss: 0.49464511374066544\n",
      "Epoch 166, Loss: 0.49434383269975535\n",
      "Epoch 167, Loss: 0.49404631909698943\n",
      "Epoch 168, Loss: 0.49374530645143583\n",
      "Epoch 169, Loss: 0.49345064806438055\n",
      "Epoch 170, Loss: 0.49317001490258333\n",
      "Epoch 171, Loss: 0.4928927591216884\n",
      "Epoch 172, Loss: 0.4926138499628376\n",
      "Epoch 173, Loss: 0.49233528219511824\n",
      "Epoch 174, Loss: 0.49205852514110515\n",
      "Epoch 175, Loss: 0.4917773399684819\n",
      "Epoch 176, Loss: 0.49149629710928233\n",
      "Epoch 177, Loss: 0.4912165647150265\n",
      "Epoch 178, Loss: 0.49094181765497674\n",
      "Epoch 179, Loss: 0.49066901222163783\n",
      "Epoch 180, Loss: 0.490393928005003\n",
      "Epoch 181, Loss: 0.4901079113527018\n",
      "Epoch 182, Loss: 0.48981984154671765\n",
      "Epoch 183, Loss: 0.4895234443341763\n",
      "Epoch 184, Loss: 0.48921459258368105\n",
      "Epoch 185, Loss: 0.4889145977883947\n",
      "Epoch 186, Loss: 0.4886175142174919\n",
      "Epoch 187, Loss: 0.4883282295514753\n",
      "Epoch 188, Loss: 0.4880435212302353\n",
      "Epoch 189, Loss: 0.48775699700499114\n",
      "Epoch 190, Loss: 0.48748008613994775\n",
      "Epoch 191, Loss: 0.4872132179171775\n",
      "Epoch 192, Loss: 0.48694738158534456\n",
      "Epoch 193, Loss: 0.48668121634032785\n",
      "Epoch 194, Loss: 0.4864280322301768\n",
      "Epoch 195, Loss: 0.486178532155915\n",
      "Epoch 196, Loss: 0.4859301919172944\n",
      "Epoch 197, Loss: 0.48568442460989747\n",
      "Epoch 198, Loss: 0.48544224711238787\n",
      "Epoch 199, Loss: 0.48519729219516017\n",
      "Epoch 200, Loss: 0.4849575446659644\n",
      "Epoch 201, Loss: 0.4847146197220469\n",
      "Epoch 202, Loss: 0.48447155759133576\n",
      "Epoch 203, Loss: 0.48422783492589483\n",
      "Epoch 204, Loss: 0.48397707745066637\n",
      "Epoch 205, Loss: 0.483737926529631\n",
      "Epoch 206, Loss: 0.4835020782648923\n",
      "Epoch 207, Loss: 0.48327083252052233\n",
      "Epoch 208, Loss: 0.4830508282762034\n",
      "Epoch 209, Loss: 0.48283269895503794\n",
      "Epoch 210, Loss: 0.48261215534620283\n",
      "Epoch 211, Loss: 0.48240746542225255\n",
      "Epoch 212, Loss: 0.4822187420754367\n",
      "Epoch 213, Loss: 0.48202997575341067\n",
      "Epoch 214, Loss: 0.4818441197783379\n",
      "Epoch 215, Loss: 0.4816502256898402\n",
      "Epoch 216, Loss: 0.4814589616492725\n",
      "Epoch 217, Loss: 0.48127338055379854\n",
      "Epoch 218, Loss: 0.4810898650639108\n",
      "Epoch 219, Loss: 0.48091365405597575\n",
      "Epoch 220, Loss: 0.480720996546175\n",
      "Epoch 221, Loss: 0.48054865040929456\n",
      "Epoch 222, Loss: 0.4803774677186426\n",
      "Epoch 223, Loss: 0.4801885671816001\n",
      "Epoch 224, Loss: 0.48000935557804647\n",
      "Epoch 225, Loss: 0.4798350303004208\n",
      "Epoch 226, Loss: 0.47967842665052907\n",
      "Epoch 227, Loss: 0.47947854451894606\n",
      "Epoch 228, Loss: 0.47928199900429136\n",
      "Epoch 229, Loss: 0.4791424752085709\n",
      "Epoch 230, Loss: 0.4789254904984497\n",
      "Epoch 231, Loss: 0.4787155823499749\n",
      "Epoch 232, Loss: 0.4785186611779873\n",
      "Epoch 233, Loss: 0.4783287773458093\n",
      "Epoch 234, Loss: 0.4781774454711345\n",
      "Epoch 235, Loss: 0.4780410437330356\n",
      "Epoch 236, Loss: 0.4778263811299359\n",
      "Epoch 237, Loss: 0.47765285017795805\n",
      "Epoch 238, Loss: 0.4775436338740249\n",
      "Epoch 239, Loss: 0.4774342833479412\n",
      "Epoch 240, Loss: 0.477263591547843\n",
      "Epoch 241, Loss: 0.47707859610269837\n",
      "Epoch 242, Loss: 0.47697972966255553\n",
      "Epoch 243, Loss: 0.4768431884384324\n",
      "Epoch 244, Loss: 0.4767408349977067\n",
      "Epoch 245, Loss: 0.4765819952057135\n",
      "Epoch 246, Loss: 0.4764706413665339\n",
      "Epoch 247, Loss: 0.47634462160009333\n",
      "Epoch 248, Loss: 0.4762527991635621\n",
      "Epoch 249, Loss: 0.4762014052862575\n",
      "Epoch 250, Loss: 0.4760283306209857\n",
      "Epoch 251, Loss: 0.47592260832103706\n",
      "Epoch 252, Loss: 0.475834869470249\n",
      "Epoch 253, Loss: 0.47570980894919\n",
      "Epoch 254, Loss: 0.47559397414543253\n",
      "Epoch 255, Loss: 0.4755005182088922\n",
      "Epoch 256, Loss: 0.4753894909333468\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2747951384496833\n",
      "Valence RMSE: 0.2536514433799077\n",
      "Arousal RMSE: 0.294424322199187\n",
      "Test R^2 score: tensor([0.2203, 0.1417], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1275, 0.0395], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.1809706824787231\n",
      "Num of epochs: 257\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Epoch 121, Loss: 0.5116998508836337\n",
      "Epoch 122, Loss: 0.5111129902664598\n",
      "Epoch 123, Loss: 0.5105405741436312\n",
      "Epoch 124, Loss: 0.5099849010083686\n",
      "Epoch 125, Loss: 0.5094623759868415\n",
      "Epoch 126, Loss: 0.5089652841754122\n",
      "Epoch 127, Loss: 0.5084717799132713\n",
      "Epoch 128, Loss: 0.5079809643002384\n",
      "Epoch 129, Loss: 0.507507731608986\n",
      "Epoch 130, Loss: 0.5070515727009828\n",
      "Epoch 131, Loss: 0.5065962678643066\n",
      "Epoch 132, Loss: 0.5061448812256618\n",
      "Epoch 133, Loss: 0.5057069703510908\n",
      "Epoch 134, Loss: 0.5052756693952567\n",
      "Epoch 135, Loss: 0.5048445017462813\n",
      "Epoch 136, Loss: 0.504425284231979\n",
      "Epoch 137, Loss: 0.504022717877823\n",
      "Epoch 138, Loss: 0.5036312802042249\n",
      "Epoch 139, Loss: 0.5032368731068207\n",
      "Epoch 140, Loss: 0.5028511060044271\n",
      "Epoch 141, Loss: 0.5024804933282331\n",
      "Epoch 142, Loss: 0.5021182727396313\n",
      "Epoch 143, Loss: 0.5017581664990297\n",
      "Epoch 144, Loss: 0.5014050827892457\n",
      "Epoch 145, Loss: 0.501047675902349\n",
      "Epoch 146, Loss: 0.500690966247362\n",
      "Epoch 147, Loss: 0.5003476722262875\n",
      "Epoch 148, Loss: 0.5000068544871652\n",
      "Epoch 149, Loss: 0.49966338867073357\n",
      "Epoch 150, Loss: 0.4993280873087794\n",
      "Epoch 151, Loss: 0.49900257940428383\n",
      "Epoch 152, Loss: 0.4986865554458311\n",
      "Epoch 153, Loss: 0.49836949389755036\n",
      "Epoch 154, Loss: 0.49805375636713634\n",
      "Epoch 155, Loss: 0.49773873165141763\n",
      "Epoch 156, Loss: 0.49742224924400874\n",
      "Epoch 157, Loss: 0.49711425826812494\n",
      "Epoch 158, Loss: 0.4968010973402242\n",
      "Epoch 159, Loss: 0.4964901399315246\n",
      "Epoch 160, Loss: 0.49617675026847124\n",
      "Epoch 161, Loss: 0.4958670390985117\n",
      "Epoch 162, Loss: 0.4955578109301908\n",
      "Epoch 163, Loss: 0.4952510223984696\n",
      "Epoch 164, Loss: 0.49494577484111674\n",
      "Epoch 165, Loss: 0.49464511374066544\n",
      "Epoch 166, Loss: 0.49434383269975535\n",
      "Epoch 167, Loss: 0.49404631909698943\n",
      "Epoch 168, Loss: 0.49374530645143583\n",
      "Epoch 169, Loss: 0.49345064806438055\n",
      "Epoch 170, Loss: 0.49317001490258333\n",
      "Epoch 171, Loss: 0.4928927591216884\n",
      "Epoch 172, Loss: 0.4926138499628376\n",
      "Epoch 173, Loss: 0.49233528219511824\n",
      "Epoch 174, Loss: 0.49205852514110515\n",
      "Epoch 175, Loss: 0.4917773399684819\n",
      "Epoch 176, Loss: 0.49149629710928233\n",
      "Epoch 177, Loss: 0.4912165647150265\n",
      "Epoch 178, Loss: 0.49094181765497674\n",
      "Epoch 179, Loss: 0.49066901222163783\n",
      "Epoch 180, Loss: 0.490393928005003\n",
      "Epoch 181, Loss: 0.4901079113527018\n",
      "Epoch 182, Loss: 0.48981984154671765\n",
      "Epoch 183, Loss: 0.4895234443341763\n",
      "Epoch 184, Loss: 0.48921459258368105\n",
      "Epoch 185, Loss: 0.4889145977883947\n",
      "Epoch 186, Loss: 0.4886175142174919\n",
      "Epoch 187, Loss: 0.4883282295514753\n",
      "Epoch 188, Loss: 0.4880435212302353\n",
      "Epoch 189, Loss: 0.48775699700499114\n",
      "Epoch 190, Loss: 0.48748008613994775\n",
      "Epoch 191, Loss: 0.4872132179171775\n",
      "Epoch 192, Loss: 0.48694738158534456\n",
      "Epoch 193, Loss: 0.48668121634032785\n",
      "Epoch 194, Loss: 0.4864280322301768\n",
      "Epoch 195, Loss: 0.486178532155915\n",
      "Epoch 196, Loss: 0.4859301919172944\n",
      "Epoch 197, Loss: 0.48568442460989747\n",
      "Epoch 198, Loss: 0.48544224711238787\n",
      "Epoch 199, Loss: 0.48519729219516017\n",
      "Epoch 200, Loss: 0.4849575446659644\n",
      "Epoch 201, Loss: 0.4847146197220469\n",
      "Epoch 202, Loss: 0.48447155759133576\n",
      "Epoch 203, Loss: 0.48422783492589483\n",
      "Epoch 204, Loss: 0.48397707745066637\n",
      "Epoch 205, Loss: 0.483737926529631\n",
      "Epoch 206, Loss: 0.4835020782648923\n",
      "Epoch 207, Loss: 0.48327083252052233\n",
      "Epoch 208, Loss: 0.4830508282762034\n",
      "Epoch 209, Loss: 0.48283269895503794\n",
      "Epoch 210, Loss: 0.48261215534620283\n",
      "Epoch 211, Loss: 0.48240746542225255\n",
      "Epoch 212, Loss: 0.4822187420754367\n",
      "Epoch 213, Loss: 0.48202997575341067\n",
      "Epoch 214, Loss: 0.4818441197783379\n",
      "Epoch 215, Loss: 0.4816502256898402\n",
      "Epoch 216, Loss: 0.4814589616492725\n",
      "Epoch 217, Loss: 0.48127338055379854\n",
      "Epoch 218, Loss: 0.4810898650639108\n",
      "Epoch 219, Loss: 0.48091365405597575\n",
      "Epoch 220, Loss: 0.480720996546175\n",
      "Epoch 221, Loss: 0.48054865040929456\n",
      "Epoch 222, Loss: 0.4803774677186426\n",
      "Epoch 223, Loss: 0.4801885671816001\n",
      "Epoch 224, Loss: 0.48000935557804647\n",
      "Epoch 225, Loss: 0.4798350303004208\n",
      "Epoch 226, Loss: 0.47967842665052907\n",
      "Epoch 227, Loss: 0.47947854451894606\n",
      "Epoch 228, Loss: 0.47928199900429136\n",
      "Epoch 229, Loss: 0.4791424752085709\n",
      "Epoch 230, Loss: 0.4789254904984497\n",
      "Epoch 231, Loss: 0.4787155823499749\n",
      "Epoch 232, Loss: 0.4785186611779873\n",
      "Epoch 233, Loss: 0.4783287773458093\n",
      "Epoch 234, Loss: 0.4781774454711345\n",
      "Epoch 235, Loss: 0.4780410437330356\n",
      "Epoch 236, Loss: 0.4778263811299359\n",
      "Epoch 237, Loss: 0.47765285017795805\n",
      "Epoch 238, Loss: 0.4775436338740249\n",
      "Epoch 239, Loss: 0.4774342833479412\n",
      "Epoch 240, Loss: 0.477263591547843\n",
      "Epoch 241, Loss: 0.47707859610269837\n",
      "Epoch 242, Loss: 0.47697972966255553\n",
      "Epoch 243, Loss: 0.4768431884384324\n",
      "Epoch 244, Loss: 0.4767408349977067\n",
      "Epoch 245, Loss: 0.4765819952057135\n",
      "Epoch 246, Loss: 0.4764706413665339\n",
      "Epoch 247, Loss: 0.47634462160009333\n",
      "Epoch 248, Loss: 0.4762527991635621\n",
      "Epoch 249, Loss: 0.4762014052862575\n",
      "Epoch 250, Loss: 0.4760283306209857\n",
      "Epoch 251, Loss: 0.47592260832103706\n",
      "Epoch 252, Loss: 0.475834869470249\n",
      "Epoch 253, Loss: 0.47570980894919\n",
      "Epoch 254, Loss: 0.47559397414543253\n",
      "Epoch 255, Loss: 0.4755005182088922\n",
      "Epoch 256, Loss: 0.4753894909333468\n",
      "Epoch 257, Loss: 0.47527958208571863\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2749452385469702\n",
      "Valence RMSE: 0.25376439971371095\n",
      "Arousal RMSE: 0.2946071924397649\n",
      "Test R^2 score: tensor([0.2196, 0.1406], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1267, 0.0384], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.18009009095669926\n",
      "Num of epochs: 258\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Epoch 121, Loss: 0.5116998508836337\n",
      "Epoch 122, Loss: 0.5111129902664598\n",
      "Epoch 123, Loss: 0.5105405741436312\n",
      "Epoch 124, Loss: 0.5099849010083686\n",
      "Epoch 125, Loss: 0.5094623759868415\n",
      "Epoch 126, Loss: 0.5089652841754122\n",
      "Epoch 127, Loss: 0.5084717799132713\n",
      "Epoch 128, Loss: 0.5079809643002384\n",
      "Epoch 129, Loss: 0.507507731608986\n",
      "Epoch 130, Loss: 0.5070515727009828\n",
      "Epoch 131, Loss: 0.5065962678643066\n",
      "Epoch 132, Loss: 0.5061448812256618\n",
      "Epoch 133, Loss: 0.5057069703510908\n",
      "Epoch 134, Loss: 0.5052756693952567\n",
      "Epoch 135, Loss: 0.5048445017462813\n",
      "Epoch 136, Loss: 0.504425284231979\n",
      "Epoch 137, Loss: 0.504022717877823\n",
      "Epoch 138, Loss: 0.5036312802042249\n",
      "Epoch 139, Loss: 0.5032368731068207\n",
      "Epoch 140, Loss: 0.5028511060044271\n",
      "Epoch 141, Loss: 0.5024804933282331\n",
      "Epoch 142, Loss: 0.5021182727396313\n",
      "Epoch 143, Loss: 0.5017581664990297\n",
      "Epoch 144, Loss: 0.5014050827892457\n",
      "Epoch 145, Loss: 0.501047675902349\n",
      "Epoch 146, Loss: 0.500690966247362\n",
      "Epoch 147, Loss: 0.5003476722262875\n",
      "Epoch 148, Loss: 0.5000068544871652\n",
      "Epoch 149, Loss: 0.49966338867073357\n",
      "Epoch 150, Loss: 0.4993280873087794\n",
      "Epoch 151, Loss: 0.49900257940428383\n",
      "Epoch 152, Loss: 0.4986865554458311\n",
      "Epoch 153, Loss: 0.49836949389755036\n",
      "Epoch 154, Loss: 0.49805375636713634\n",
      "Epoch 155, Loss: 0.49773873165141763\n",
      "Epoch 156, Loss: 0.49742224924400874\n",
      "Epoch 157, Loss: 0.49711425826812494\n",
      "Epoch 158, Loss: 0.4968010973402242\n",
      "Epoch 159, Loss: 0.4964901399315246\n",
      "Epoch 160, Loss: 0.49617675026847124\n",
      "Epoch 161, Loss: 0.4958670390985117\n",
      "Epoch 162, Loss: 0.4955578109301908\n",
      "Epoch 163, Loss: 0.4952510223984696\n",
      "Epoch 164, Loss: 0.49494577484111674\n",
      "Epoch 165, Loss: 0.49464511374066544\n",
      "Epoch 166, Loss: 0.49434383269975535\n",
      "Epoch 167, Loss: 0.49404631909698943\n",
      "Epoch 168, Loss: 0.49374530645143583\n",
      "Epoch 169, Loss: 0.49345064806438055\n",
      "Epoch 170, Loss: 0.49317001490258333\n",
      "Epoch 171, Loss: 0.4928927591216884\n",
      "Epoch 172, Loss: 0.4926138499628376\n",
      "Epoch 173, Loss: 0.49233528219511824\n",
      "Epoch 174, Loss: 0.49205852514110515\n",
      "Epoch 175, Loss: 0.4917773399684819\n",
      "Epoch 176, Loss: 0.49149629710928233\n",
      "Epoch 177, Loss: 0.4912165647150265\n",
      "Epoch 178, Loss: 0.49094181765497674\n",
      "Epoch 179, Loss: 0.49066901222163783\n",
      "Epoch 180, Loss: 0.490393928005003\n",
      "Epoch 181, Loss: 0.4901079113527018\n",
      "Epoch 182, Loss: 0.48981984154671765\n",
      "Epoch 183, Loss: 0.4895234443341763\n",
      "Epoch 184, Loss: 0.48921459258368105\n",
      "Epoch 185, Loss: 0.4889145977883947\n",
      "Epoch 186, Loss: 0.4886175142174919\n",
      "Epoch 187, Loss: 0.4883282295514753\n",
      "Epoch 188, Loss: 0.4880435212302353\n",
      "Epoch 189, Loss: 0.48775699700499114\n",
      "Epoch 190, Loss: 0.48748008613994775\n",
      "Epoch 191, Loss: 0.4872132179171775\n",
      "Epoch 192, Loss: 0.48694738158534456\n",
      "Epoch 193, Loss: 0.48668121634032785\n",
      "Epoch 194, Loss: 0.4864280322301768\n",
      "Epoch 195, Loss: 0.486178532155915\n",
      "Epoch 196, Loss: 0.4859301919172944\n",
      "Epoch 197, Loss: 0.48568442460989747\n",
      "Epoch 198, Loss: 0.48544224711238787\n",
      "Epoch 199, Loss: 0.48519729219516017\n",
      "Epoch 200, Loss: 0.4849575446659644\n",
      "Epoch 201, Loss: 0.4847146197220469\n",
      "Epoch 202, Loss: 0.48447155759133576\n",
      "Epoch 203, Loss: 0.48422783492589483\n",
      "Epoch 204, Loss: 0.48397707745066637\n",
      "Epoch 205, Loss: 0.483737926529631\n",
      "Epoch 206, Loss: 0.4835020782648923\n",
      "Epoch 207, Loss: 0.48327083252052233\n",
      "Epoch 208, Loss: 0.4830508282762034\n",
      "Epoch 209, Loss: 0.48283269895503794\n",
      "Epoch 210, Loss: 0.48261215534620283\n",
      "Epoch 211, Loss: 0.48240746542225255\n",
      "Epoch 212, Loss: 0.4822187420754367\n",
      "Epoch 213, Loss: 0.48202997575341067\n",
      "Epoch 214, Loss: 0.4818441197783379\n",
      "Epoch 215, Loss: 0.4816502256898402\n",
      "Epoch 216, Loss: 0.4814589616492725\n",
      "Epoch 217, Loss: 0.48127338055379854\n",
      "Epoch 218, Loss: 0.4810898650639108\n",
      "Epoch 219, Loss: 0.48091365405597575\n",
      "Epoch 220, Loss: 0.480720996546175\n",
      "Epoch 221, Loss: 0.48054865040929456\n",
      "Epoch 222, Loss: 0.4803774677186426\n",
      "Epoch 223, Loss: 0.4801885671816001\n",
      "Epoch 224, Loss: 0.48000935557804647\n",
      "Epoch 225, Loss: 0.4798350303004208\n",
      "Epoch 226, Loss: 0.47967842665052907\n",
      "Epoch 227, Loss: 0.47947854451894606\n",
      "Epoch 228, Loss: 0.47928199900429136\n",
      "Epoch 229, Loss: 0.4791424752085709\n",
      "Epoch 230, Loss: 0.4789254904984497\n",
      "Epoch 231, Loss: 0.4787155823499749\n",
      "Epoch 232, Loss: 0.4785186611779873\n",
      "Epoch 233, Loss: 0.4783287773458093\n",
      "Epoch 234, Loss: 0.4781774454711345\n",
      "Epoch 235, Loss: 0.4780410437330356\n",
      "Epoch 236, Loss: 0.4778263811299359\n",
      "Epoch 237, Loss: 0.47765285017795805\n",
      "Epoch 238, Loss: 0.4775436338740249\n",
      "Epoch 239, Loss: 0.4774342833479412\n",
      "Epoch 240, Loss: 0.477263591547843\n",
      "Epoch 241, Loss: 0.47707859610269837\n",
      "Epoch 242, Loss: 0.47697972966255553\n",
      "Epoch 243, Loss: 0.4768431884384324\n",
      "Epoch 244, Loss: 0.4767408349977067\n",
      "Epoch 245, Loss: 0.4765819952057135\n",
      "Epoch 246, Loss: 0.4764706413665339\n",
      "Epoch 247, Loss: 0.47634462160009333\n",
      "Epoch 248, Loss: 0.4762527991635621\n",
      "Epoch 249, Loss: 0.4762014052862575\n",
      "Epoch 250, Loss: 0.4760283306209857\n",
      "Epoch 251, Loss: 0.47592260832103706\n",
      "Epoch 252, Loss: 0.475834869470249\n",
      "Epoch 253, Loss: 0.47570980894919\n",
      "Epoch 254, Loss: 0.47559397414543253\n",
      "Epoch 255, Loss: 0.4755005182088922\n",
      "Epoch 256, Loss: 0.4753894909333468\n",
      "Epoch 257, Loss: 0.47527958208571863\n",
      "Epoch 258, Loss: 0.47517947896910767\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.27515987455732444\n",
      "Valence RMSE: 0.2538881979218862\n",
      "Arousal RMSE: 0.2949011632543727\n",
      "Test R^2 score: tensor([0.2188, 0.1389], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1259, 0.0364], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.17885130090068369\n",
      "Num of epochs: 259\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Epoch 121, Loss: 0.5116998508836337\n",
      "Epoch 122, Loss: 0.5111129902664598\n",
      "Epoch 123, Loss: 0.5105405741436312\n",
      "Epoch 124, Loss: 0.5099849010083686\n",
      "Epoch 125, Loss: 0.5094623759868415\n",
      "Epoch 126, Loss: 0.5089652841754122\n",
      "Epoch 127, Loss: 0.5084717799132713\n",
      "Epoch 128, Loss: 0.5079809643002384\n",
      "Epoch 129, Loss: 0.507507731608986\n",
      "Epoch 130, Loss: 0.5070515727009828\n",
      "Epoch 131, Loss: 0.5065962678643066\n",
      "Epoch 132, Loss: 0.5061448812256618\n",
      "Epoch 133, Loss: 0.5057069703510908\n",
      "Epoch 134, Loss: 0.5052756693952567\n",
      "Epoch 135, Loss: 0.5048445017462813\n",
      "Epoch 136, Loss: 0.504425284231979\n",
      "Epoch 137, Loss: 0.504022717877823\n",
      "Epoch 138, Loss: 0.5036312802042249\n",
      "Epoch 139, Loss: 0.5032368731068207\n",
      "Epoch 140, Loss: 0.5028511060044271\n",
      "Epoch 141, Loss: 0.5024804933282331\n",
      "Epoch 142, Loss: 0.5021182727396313\n",
      "Epoch 143, Loss: 0.5017581664990297\n",
      "Epoch 144, Loss: 0.5014050827892457\n",
      "Epoch 145, Loss: 0.501047675902349\n",
      "Epoch 146, Loss: 0.500690966247362\n",
      "Epoch 147, Loss: 0.5003476722262875\n",
      "Epoch 148, Loss: 0.5000068544871652\n",
      "Epoch 149, Loss: 0.49966338867073357\n",
      "Epoch 150, Loss: 0.4993280873087794\n",
      "Epoch 151, Loss: 0.49900257940428383\n",
      "Epoch 152, Loss: 0.4986865554458311\n",
      "Epoch 153, Loss: 0.49836949389755036\n",
      "Epoch 154, Loss: 0.49805375636713634\n",
      "Epoch 155, Loss: 0.49773873165141763\n",
      "Epoch 156, Loss: 0.49742224924400874\n",
      "Epoch 157, Loss: 0.49711425826812494\n",
      "Epoch 158, Loss: 0.4968010973402242\n",
      "Epoch 159, Loss: 0.4964901399315246\n",
      "Epoch 160, Loss: 0.49617675026847124\n",
      "Epoch 161, Loss: 0.4958670390985117\n",
      "Epoch 162, Loss: 0.4955578109301908\n",
      "Epoch 163, Loss: 0.4952510223984696\n",
      "Epoch 164, Loss: 0.49494577484111674\n",
      "Epoch 165, Loss: 0.49464511374066544\n",
      "Epoch 166, Loss: 0.49434383269975535\n",
      "Epoch 167, Loss: 0.49404631909698943\n",
      "Epoch 168, Loss: 0.49374530645143583\n",
      "Epoch 169, Loss: 0.49345064806438055\n",
      "Epoch 170, Loss: 0.49317001490258333\n",
      "Epoch 171, Loss: 0.4928927591216884\n",
      "Epoch 172, Loss: 0.4926138499628376\n",
      "Epoch 173, Loss: 0.49233528219511824\n",
      "Epoch 174, Loss: 0.49205852514110515\n",
      "Epoch 175, Loss: 0.4917773399684819\n",
      "Epoch 176, Loss: 0.49149629710928233\n",
      "Epoch 177, Loss: 0.4912165647150265\n",
      "Epoch 178, Loss: 0.49094181765497674\n",
      "Epoch 179, Loss: 0.49066901222163783\n",
      "Epoch 180, Loss: 0.490393928005003\n",
      "Epoch 181, Loss: 0.4901079113527018\n",
      "Epoch 182, Loss: 0.48981984154671765\n",
      "Epoch 183, Loss: 0.4895234443341763\n",
      "Epoch 184, Loss: 0.48921459258368105\n",
      "Epoch 185, Loss: 0.4889145977883947\n",
      "Epoch 186, Loss: 0.4886175142174919\n",
      "Epoch 187, Loss: 0.4883282295514753\n",
      "Epoch 188, Loss: 0.4880435212302353\n",
      "Epoch 189, Loss: 0.48775699700499114\n",
      "Epoch 190, Loss: 0.48748008613994775\n",
      "Epoch 191, Loss: 0.4872132179171775\n",
      "Epoch 192, Loss: 0.48694738158534456\n",
      "Epoch 193, Loss: 0.48668121634032785\n",
      "Epoch 194, Loss: 0.4864280322301768\n",
      "Epoch 195, Loss: 0.486178532155915\n",
      "Epoch 196, Loss: 0.4859301919172944\n",
      "Epoch 197, Loss: 0.48568442460989747\n",
      "Epoch 198, Loss: 0.48544224711238787\n",
      "Epoch 199, Loss: 0.48519729219516017\n",
      "Epoch 200, Loss: 0.4849575446659644\n",
      "Epoch 201, Loss: 0.4847146197220469\n",
      "Epoch 202, Loss: 0.48447155759133576\n",
      "Epoch 203, Loss: 0.48422783492589483\n",
      "Epoch 204, Loss: 0.48397707745066637\n",
      "Epoch 205, Loss: 0.483737926529631\n",
      "Epoch 206, Loss: 0.4835020782648923\n",
      "Epoch 207, Loss: 0.48327083252052233\n",
      "Epoch 208, Loss: 0.4830508282762034\n",
      "Epoch 209, Loss: 0.48283269895503794\n",
      "Epoch 210, Loss: 0.48261215534620283\n",
      "Epoch 211, Loss: 0.48240746542225255\n",
      "Epoch 212, Loss: 0.4822187420754367\n",
      "Epoch 213, Loss: 0.48202997575341067\n",
      "Epoch 214, Loss: 0.4818441197783379\n",
      "Epoch 215, Loss: 0.4816502256898402\n",
      "Epoch 216, Loss: 0.4814589616492725\n",
      "Epoch 217, Loss: 0.48127338055379854\n",
      "Epoch 218, Loss: 0.4810898650639108\n",
      "Epoch 219, Loss: 0.48091365405597575\n",
      "Epoch 220, Loss: 0.480720996546175\n",
      "Epoch 221, Loss: 0.48054865040929456\n",
      "Epoch 222, Loss: 0.4803774677186426\n",
      "Epoch 223, Loss: 0.4801885671816001\n",
      "Epoch 224, Loss: 0.48000935557804647\n",
      "Epoch 225, Loss: 0.4798350303004208\n",
      "Epoch 226, Loss: 0.47967842665052907\n",
      "Epoch 227, Loss: 0.47947854451894606\n",
      "Epoch 228, Loss: 0.47928199900429136\n",
      "Epoch 229, Loss: 0.4791424752085709\n",
      "Epoch 230, Loss: 0.4789254904984497\n",
      "Epoch 231, Loss: 0.4787155823499749\n",
      "Epoch 232, Loss: 0.4785186611779873\n",
      "Epoch 233, Loss: 0.4783287773458093\n",
      "Epoch 234, Loss: 0.4781774454711345\n",
      "Epoch 235, Loss: 0.4780410437330356\n",
      "Epoch 236, Loss: 0.4778263811299359\n",
      "Epoch 237, Loss: 0.47765285017795805\n",
      "Epoch 238, Loss: 0.4775436338740249\n",
      "Epoch 239, Loss: 0.4774342833479412\n",
      "Epoch 240, Loss: 0.477263591547843\n",
      "Epoch 241, Loss: 0.47707859610269837\n",
      "Epoch 242, Loss: 0.47697972966255553\n",
      "Epoch 243, Loss: 0.4768431884384324\n",
      "Epoch 244, Loss: 0.4767408349977067\n",
      "Epoch 245, Loss: 0.4765819952057135\n",
      "Epoch 246, Loss: 0.4764706413665339\n",
      "Epoch 247, Loss: 0.47634462160009333\n",
      "Epoch 248, Loss: 0.4762527991635621\n",
      "Epoch 249, Loss: 0.4762014052862575\n",
      "Epoch 250, Loss: 0.4760283306209857\n",
      "Epoch 251, Loss: 0.47592260832103706\n",
      "Epoch 252, Loss: 0.475834869470249\n",
      "Epoch 253, Loss: 0.47570980894919\n",
      "Epoch 254, Loss: 0.47559397414543253\n",
      "Epoch 255, Loss: 0.4755005182088922\n",
      "Epoch 256, Loss: 0.4753894909333468\n",
      "Epoch 257, Loss: 0.47527958208571863\n",
      "Epoch 258, Loss: 0.47517947896910767\n",
      "Epoch 259, Loss: 0.4750690667233416\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2753761080434503\n",
      "Valence RMSE: 0.25400932239432517\n",
      "Arousal RMSE: 0.295200382620165\n",
      "Test R^2 score: tensor([0.2181, 0.1371], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1250, 0.0345], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.17760435885036985\n",
      "Num of epochs: 260\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Epoch 121, Loss: 0.5116998508836337\n",
      "Epoch 122, Loss: 0.5111129902664598\n",
      "Epoch 123, Loss: 0.5105405741436312\n",
      "Epoch 124, Loss: 0.5099849010083686\n",
      "Epoch 125, Loss: 0.5094623759868415\n",
      "Epoch 126, Loss: 0.5089652841754122\n",
      "Epoch 127, Loss: 0.5084717799132713\n",
      "Epoch 128, Loss: 0.5079809643002384\n",
      "Epoch 129, Loss: 0.507507731608986\n",
      "Epoch 130, Loss: 0.5070515727009828\n",
      "Epoch 131, Loss: 0.5065962678643066\n",
      "Epoch 132, Loss: 0.5061448812256618\n",
      "Epoch 133, Loss: 0.5057069703510908\n",
      "Epoch 134, Loss: 0.5052756693952567\n",
      "Epoch 135, Loss: 0.5048445017462813\n",
      "Epoch 136, Loss: 0.504425284231979\n",
      "Epoch 137, Loss: 0.504022717877823\n",
      "Epoch 138, Loss: 0.5036312802042249\n",
      "Epoch 139, Loss: 0.5032368731068207\n",
      "Epoch 140, Loss: 0.5028511060044271\n",
      "Epoch 141, Loss: 0.5024804933282331\n",
      "Epoch 142, Loss: 0.5021182727396313\n",
      "Epoch 143, Loss: 0.5017581664990297\n",
      "Epoch 144, Loss: 0.5014050827892457\n",
      "Epoch 145, Loss: 0.501047675902349\n",
      "Epoch 146, Loss: 0.500690966247362\n",
      "Epoch 147, Loss: 0.5003476722262875\n",
      "Epoch 148, Loss: 0.5000068544871652\n",
      "Epoch 149, Loss: 0.49966338867073357\n",
      "Epoch 150, Loss: 0.4993280873087794\n",
      "Epoch 151, Loss: 0.49900257940428383\n",
      "Epoch 152, Loss: 0.4986865554458311\n",
      "Epoch 153, Loss: 0.49836949389755036\n",
      "Epoch 154, Loss: 0.49805375636713634\n",
      "Epoch 155, Loss: 0.49773873165141763\n",
      "Epoch 156, Loss: 0.49742224924400874\n",
      "Epoch 157, Loss: 0.49711425826812494\n",
      "Epoch 158, Loss: 0.4968010973402242\n",
      "Epoch 159, Loss: 0.4964901399315246\n",
      "Epoch 160, Loss: 0.49617675026847124\n",
      "Epoch 161, Loss: 0.4958670390985117\n",
      "Epoch 162, Loss: 0.4955578109301908\n",
      "Epoch 163, Loss: 0.4952510223984696\n",
      "Epoch 164, Loss: 0.49494577484111674\n",
      "Epoch 165, Loss: 0.49464511374066544\n",
      "Epoch 166, Loss: 0.49434383269975535\n",
      "Epoch 167, Loss: 0.49404631909698943\n",
      "Epoch 168, Loss: 0.49374530645143583\n",
      "Epoch 169, Loss: 0.49345064806438055\n",
      "Epoch 170, Loss: 0.49317001490258333\n",
      "Epoch 171, Loss: 0.4928927591216884\n",
      "Epoch 172, Loss: 0.4926138499628376\n",
      "Epoch 173, Loss: 0.49233528219511824\n",
      "Epoch 174, Loss: 0.49205852514110515\n",
      "Epoch 175, Loss: 0.4917773399684819\n",
      "Epoch 176, Loss: 0.49149629710928233\n",
      "Epoch 177, Loss: 0.4912165647150265\n",
      "Epoch 178, Loss: 0.49094181765497674\n",
      "Epoch 179, Loss: 0.49066901222163783\n",
      "Epoch 180, Loss: 0.490393928005003\n",
      "Epoch 181, Loss: 0.4901079113527018\n",
      "Epoch 182, Loss: 0.48981984154671765\n",
      "Epoch 183, Loss: 0.4895234443341763\n",
      "Epoch 184, Loss: 0.48921459258368105\n",
      "Epoch 185, Loss: 0.4889145977883947\n",
      "Epoch 186, Loss: 0.4886175142174919\n",
      "Epoch 187, Loss: 0.4883282295514753\n",
      "Epoch 188, Loss: 0.4880435212302353\n",
      "Epoch 189, Loss: 0.48775699700499114\n",
      "Epoch 190, Loss: 0.48748008613994775\n",
      "Epoch 191, Loss: 0.4872132179171775\n",
      "Epoch 192, Loss: 0.48694738158534456\n",
      "Epoch 193, Loss: 0.48668121634032785\n",
      "Epoch 194, Loss: 0.4864280322301768\n",
      "Epoch 195, Loss: 0.486178532155915\n",
      "Epoch 196, Loss: 0.4859301919172944\n",
      "Epoch 197, Loss: 0.48568442460989747\n",
      "Epoch 198, Loss: 0.48544224711238787\n",
      "Epoch 199, Loss: 0.48519729219516017\n",
      "Epoch 200, Loss: 0.4849575446659644\n",
      "Epoch 201, Loss: 0.4847146197220469\n",
      "Epoch 202, Loss: 0.48447155759133576\n",
      "Epoch 203, Loss: 0.48422783492589483\n",
      "Epoch 204, Loss: 0.48397707745066637\n",
      "Epoch 205, Loss: 0.483737926529631\n",
      "Epoch 206, Loss: 0.4835020782648923\n",
      "Epoch 207, Loss: 0.48327083252052233\n",
      "Epoch 208, Loss: 0.4830508282762034\n",
      "Epoch 209, Loss: 0.48283269895503794\n",
      "Epoch 210, Loss: 0.48261215534620283\n",
      "Epoch 211, Loss: 0.48240746542225255\n",
      "Epoch 212, Loss: 0.4822187420754367\n",
      "Epoch 213, Loss: 0.48202997575341067\n",
      "Epoch 214, Loss: 0.4818441197783379\n",
      "Epoch 215, Loss: 0.4816502256898402\n",
      "Epoch 216, Loss: 0.4814589616492725\n",
      "Epoch 217, Loss: 0.48127338055379854\n",
      "Epoch 218, Loss: 0.4810898650639108\n",
      "Epoch 219, Loss: 0.48091365405597575\n",
      "Epoch 220, Loss: 0.480720996546175\n",
      "Epoch 221, Loss: 0.48054865040929456\n",
      "Epoch 222, Loss: 0.4803774677186426\n",
      "Epoch 223, Loss: 0.4801885671816001\n",
      "Epoch 224, Loss: 0.48000935557804647\n",
      "Epoch 225, Loss: 0.4798350303004208\n",
      "Epoch 226, Loss: 0.47967842665052907\n",
      "Epoch 227, Loss: 0.47947854451894606\n",
      "Epoch 228, Loss: 0.47928199900429136\n",
      "Epoch 229, Loss: 0.4791424752085709\n",
      "Epoch 230, Loss: 0.4789254904984497\n",
      "Epoch 231, Loss: 0.4787155823499749\n",
      "Epoch 232, Loss: 0.4785186611779873\n",
      "Epoch 233, Loss: 0.4783287773458093\n",
      "Epoch 234, Loss: 0.4781774454711345\n",
      "Epoch 235, Loss: 0.4780410437330356\n",
      "Epoch 236, Loss: 0.4778263811299359\n",
      "Epoch 237, Loss: 0.47765285017795805\n",
      "Epoch 238, Loss: 0.4775436338740249\n",
      "Epoch 239, Loss: 0.4774342833479412\n",
      "Epoch 240, Loss: 0.477263591547843\n",
      "Epoch 241, Loss: 0.47707859610269837\n",
      "Epoch 242, Loss: 0.47697972966255553\n",
      "Epoch 243, Loss: 0.4768431884384324\n",
      "Epoch 244, Loss: 0.4767408349977067\n",
      "Epoch 245, Loss: 0.4765819952057135\n",
      "Epoch 246, Loss: 0.4764706413665339\n",
      "Epoch 247, Loss: 0.47634462160009333\n",
      "Epoch 248, Loss: 0.4762527991635621\n",
      "Epoch 249, Loss: 0.4762014052862575\n",
      "Epoch 250, Loss: 0.4760283306209857\n",
      "Epoch 251, Loss: 0.47592260832103706\n",
      "Epoch 252, Loss: 0.475834869470249\n",
      "Epoch 253, Loss: 0.47570980894919\n",
      "Epoch 254, Loss: 0.47559397414543253\n",
      "Epoch 255, Loss: 0.4755005182088922\n",
      "Epoch 256, Loss: 0.4753894909333468\n",
      "Epoch 257, Loss: 0.47527958208571863\n",
      "Epoch 258, Loss: 0.47517947896910767\n",
      "Epoch 259, Loss: 0.4750690667233416\n",
      "Epoch 260, Loss: 0.47496625253354696\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2756359340577174\n",
      "Valence RMSE: 0.25424043286122733\n",
      "Arousal RMSE: 0.2954862747849323\n",
      "Test R^2 score: tensor([0.2166, 0.1355], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1235, 0.0326], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.17605653741286342\n",
      "Num of epochs: 261\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Epoch 121, Loss: 0.5116998508836337\n",
      "Epoch 122, Loss: 0.5111129902664598\n",
      "Epoch 123, Loss: 0.5105405741436312\n",
      "Epoch 124, Loss: 0.5099849010083686\n",
      "Epoch 125, Loss: 0.5094623759868415\n",
      "Epoch 126, Loss: 0.5089652841754122\n",
      "Epoch 127, Loss: 0.5084717799132713\n",
      "Epoch 128, Loss: 0.5079809643002384\n",
      "Epoch 129, Loss: 0.507507731608986\n",
      "Epoch 130, Loss: 0.5070515727009828\n",
      "Epoch 131, Loss: 0.5065962678643066\n",
      "Epoch 132, Loss: 0.5061448812256618\n",
      "Epoch 133, Loss: 0.5057069703510908\n",
      "Epoch 134, Loss: 0.5052756693952567\n",
      "Epoch 135, Loss: 0.5048445017462813\n",
      "Epoch 136, Loss: 0.504425284231979\n",
      "Epoch 137, Loss: 0.504022717877823\n",
      "Epoch 138, Loss: 0.5036312802042249\n",
      "Epoch 139, Loss: 0.5032368731068207\n",
      "Epoch 140, Loss: 0.5028511060044271\n",
      "Epoch 141, Loss: 0.5024804933282331\n",
      "Epoch 142, Loss: 0.5021182727396313\n",
      "Epoch 143, Loss: 0.5017581664990297\n",
      "Epoch 144, Loss: 0.5014050827892457\n",
      "Epoch 145, Loss: 0.501047675902349\n",
      "Epoch 146, Loss: 0.500690966247362\n",
      "Epoch 147, Loss: 0.5003476722262875\n",
      "Epoch 148, Loss: 0.5000068544871652\n",
      "Epoch 149, Loss: 0.49966338867073357\n",
      "Epoch 150, Loss: 0.4993280873087794\n",
      "Epoch 151, Loss: 0.49900257940428383\n",
      "Epoch 152, Loss: 0.4986865554458311\n",
      "Epoch 153, Loss: 0.49836949389755036\n",
      "Epoch 154, Loss: 0.49805375636713634\n",
      "Epoch 155, Loss: 0.49773873165141763\n",
      "Epoch 156, Loss: 0.49742224924400874\n",
      "Epoch 157, Loss: 0.49711425826812494\n",
      "Epoch 158, Loss: 0.4968010973402242\n",
      "Epoch 159, Loss: 0.4964901399315246\n",
      "Epoch 160, Loss: 0.49617675026847124\n",
      "Epoch 161, Loss: 0.4958670390985117\n",
      "Epoch 162, Loss: 0.4955578109301908\n",
      "Epoch 163, Loss: 0.4952510223984696\n",
      "Epoch 164, Loss: 0.49494577484111674\n",
      "Epoch 165, Loss: 0.49464511374066544\n",
      "Epoch 166, Loss: 0.49434383269975535\n",
      "Epoch 167, Loss: 0.49404631909698943\n",
      "Epoch 168, Loss: 0.49374530645143583\n",
      "Epoch 169, Loss: 0.49345064806438055\n",
      "Epoch 170, Loss: 0.49317001490258333\n",
      "Epoch 171, Loss: 0.4928927591216884\n",
      "Epoch 172, Loss: 0.4926138499628376\n",
      "Epoch 173, Loss: 0.49233528219511824\n",
      "Epoch 174, Loss: 0.49205852514110515\n",
      "Epoch 175, Loss: 0.4917773399684819\n",
      "Epoch 176, Loss: 0.49149629710928233\n",
      "Epoch 177, Loss: 0.4912165647150265\n",
      "Epoch 178, Loss: 0.49094181765497674\n",
      "Epoch 179, Loss: 0.49066901222163783\n",
      "Epoch 180, Loss: 0.490393928005003\n",
      "Epoch 181, Loss: 0.4901079113527018\n",
      "Epoch 182, Loss: 0.48981984154671765\n",
      "Epoch 183, Loss: 0.4895234443341763\n",
      "Epoch 184, Loss: 0.48921459258368105\n",
      "Epoch 185, Loss: 0.4889145977883947\n",
      "Epoch 186, Loss: 0.4886175142174919\n",
      "Epoch 187, Loss: 0.4883282295514753\n",
      "Epoch 188, Loss: 0.4880435212302353\n",
      "Epoch 189, Loss: 0.48775699700499114\n",
      "Epoch 190, Loss: 0.48748008613994775\n",
      "Epoch 191, Loss: 0.4872132179171775\n",
      "Epoch 192, Loss: 0.48694738158534456\n",
      "Epoch 193, Loss: 0.48668121634032785\n",
      "Epoch 194, Loss: 0.4864280322301768\n",
      "Epoch 195, Loss: 0.486178532155915\n",
      "Epoch 196, Loss: 0.4859301919172944\n",
      "Epoch 197, Loss: 0.48568442460989747\n",
      "Epoch 198, Loss: 0.48544224711238787\n",
      "Epoch 199, Loss: 0.48519729219516017\n",
      "Epoch 200, Loss: 0.4849575446659644\n",
      "Epoch 201, Loss: 0.4847146197220469\n",
      "Epoch 202, Loss: 0.48447155759133576\n",
      "Epoch 203, Loss: 0.48422783492589483\n",
      "Epoch 204, Loss: 0.48397707745066637\n",
      "Epoch 205, Loss: 0.483737926529631\n",
      "Epoch 206, Loss: 0.4835020782648923\n",
      "Epoch 207, Loss: 0.48327083252052233\n",
      "Epoch 208, Loss: 0.4830508282762034\n",
      "Epoch 209, Loss: 0.48283269895503794\n",
      "Epoch 210, Loss: 0.48261215534620283\n",
      "Epoch 211, Loss: 0.48240746542225255\n",
      "Epoch 212, Loss: 0.4822187420754367\n",
      "Epoch 213, Loss: 0.48202997575341067\n",
      "Epoch 214, Loss: 0.4818441197783379\n",
      "Epoch 215, Loss: 0.4816502256898402\n",
      "Epoch 216, Loss: 0.4814589616492725\n",
      "Epoch 217, Loss: 0.48127338055379854\n",
      "Epoch 218, Loss: 0.4810898650639108\n",
      "Epoch 219, Loss: 0.48091365405597575\n",
      "Epoch 220, Loss: 0.480720996546175\n",
      "Epoch 221, Loss: 0.48054865040929456\n",
      "Epoch 222, Loss: 0.4803774677186426\n",
      "Epoch 223, Loss: 0.4801885671816001\n",
      "Epoch 224, Loss: 0.48000935557804647\n",
      "Epoch 225, Loss: 0.4798350303004208\n",
      "Epoch 226, Loss: 0.47967842665052907\n",
      "Epoch 227, Loss: 0.47947854451894606\n",
      "Epoch 228, Loss: 0.47928199900429136\n",
      "Epoch 229, Loss: 0.4791424752085709\n",
      "Epoch 230, Loss: 0.4789254904984497\n",
      "Epoch 231, Loss: 0.4787155823499749\n",
      "Epoch 232, Loss: 0.4785186611779873\n",
      "Epoch 233, Loss: 0.4783287773458093\n",
      "Epoch 234, Loss: 0.4781774454711345\n",
      "Epoch 235, Loss: 0.4780410437330356\n",
      "Epoch 236, Loss: 0.4778263811299359\n",
      "Epoch 237, Loss: 0.47765285017795805\n",
      "Epoch 238, Loss: 0.4775436338740249\n",
      "Epoch 239, Loss: 0.4774342833479412\n",
      "Epoch 240, Loss: 0.477263591547843\n",
      "Epoch 241, Loss: 0.47707859610269837\n",
      "Epoch 242, Loss: 0.47697972966255553\n",
      "Epoch 243, Loss: 0.4768431884384324\n",
      "Epoch 244, Loss: 0.4767408349977067\n",
      "Epoch 245, Loss: 0.4765819952057135\n",
      "Epoch 246, Loss: 0.4764706413665339\n",
      "Epoch 247, Loss: 0.47634462160009333\n",
      "Epoch 248, Loss: 0.4762527991635621\n",
      "Epoch 249, Loss: 0.4762014052862575\n",
      "Epoch 250, Loss: 0.4760283306209857\n",
      "Epoch 251, Loss: 0.47592260832103706\n",
      "Epoch 252, Loss: 0.475834869470249\n",
      "Epoch 253, Loss: 0.47570980894919\n",
      "Epoch 254, Loss: 0.47559397414543253\n",
      "Epoch 255, Loss: 0.4755005182088922\n",
      "Epoch 256, Loss: 0.4753894909333468\n",
      "Epoch 257, Loss: 0.47527958208571863\n",
      "Epoch 258, Loss: 0.47517947896910767\n",
      "Epoch 259, Loss: 0.4750690667233416\n",
      "Epoch 260, Loss: 0.47496625253354696\n",
      "Epoch 261, Loss: 0.47486506352442986\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2756118642045852\n",
      "Valence RMSE: 0.2540789230669924\n",
      "Arousal RMSE: 0.295580277139343\n",
      "Test R^2 score: tensor([0.2176, 0.1349], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1246, 0.0320], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.17627893661164507\n",
      "Num of epochs: 262\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Epoch 121, Loss: 0.5116998508836337\n",
      "Epoch 122, Loss: 0.5111129902664598\n",
      "Epoch 123, Loss: 0.5105405741436312\n",
      "Epoch 124, Loss: 0.5099849010083686\n",
      "Epoch 125, Loss: 0.5094623759868415\n",
      "Epoch 126, Loss: 0.5089652841754122\n",
      "Epoch 127, Loss: 0.5084717799132713\n",
      "Epoch 128, Loss: 0.5079809643002384\n",
      "Epoch 129, Loss: 0.507507731608986\n",
      "Epoch 130, Loss: 0.5070515727009828\n",
      "Epoch 131, Loss: 0.5065962678643066\n",
      "Epoch 132, Loss: 0.5061448812256618\n",
      "Epoch 133, Loss: 0.5057069703510908\n",
      "Epoch 134, Loss: 0.5052756693952567\n",
      "Epoch 135, Loss: 0.5048445017462813\n",
      "Epoch 136, Loss: 0.504425284231979\n",
      "Epoch 137, Loss: 0.504022717877823\n",
      "Epoch 138, Loss: 0.5036312802042249\n",
      "Epoch 139, Loss: 0.5032368731068207\n",
      "Epoch 140, Loss: 0.5028511060044271\n",
      "Epoch 141, Loss: 0.5024804933282331\n",
      "Epoch 142, Loss: 0.5021182727396313\n",
      "Epoch 143, Loss: 0.5017581664990297\n",
      "Epoch 144, Loss: 0.5014050827892457\n",
      "Epoch 145, Loss: 0.501047675902349\n",
      "Epoch 146, Loss: 0.500690966247362\n",
      "Epoch 147, Loss: 0.5003476722262875\n",
      "Epoch 148, Loss: 0.5000068544871652\n",
      "Epoch 149, Loss: 0.49966338867073357\n",
      "Epoch 150, Loss: 0.4993280873087794\n",
      "Epoch 151, Loss: 0.49900257940428383\n",
      "Epoch 152, Loss: 0.4986865554458311\n",
      "Epoch 153, Loss: 0.49836949389755036\n",
      "Epoch 154, Loss: 0.49805375636713634\n",
      "Epoch 155, Loss: 0.49773873165141763\n",
      "Epoch 156, Loss: 0.49742224924400874\n",
      "Epoch 157, Loss: 0.49711425826812494\n",
      "Epoch 158, Loss: 0.4968010973402242\n",
      "Epoch 159, Loss: 0.4964901399315246\n",
      "Epoch 160, Loss: 0.49617675026847124\n",
      "Epoch 161, Loss: 0.4958670390985117\n",
      "Epoch 162, Loss: 0.4955578109301908\n",
      "Epoch 163, Loss: 0.4952510223984696\n",
      "Epoch 164, Loss: 0.49494577484111674\n",
      "Epoch 165, Loss: 0.49464511374066544\n",
      "Epoch 166, Loss: 0.49434383269975535\n",
      "Epoch 167, Loss: 0.49404631909698943\n",
      "Epoch 168, Loss: 0.49374530645143583\n",
      "Epoch 169, Loss: 0.49345064806438055\n",
      "Epoch 170, Loss: 0.49317001490258333\n",
      "Epoch 171, Loss: 0.4928927591216884\n",
      "Epoch 172, Loss: 0.4926138499628376\n",
      "Epoch 173, Loss: 0.49233528219511824\n",
      "Epoch 174, Loss: 0.49205852514110515\n",
      "Epoch 175, Loss: 0.4917773399684819\n",
      "Epoch 176, Loss: 0.49149629710928233\n",
      "Epoch 177, Loss: 0.4912165647150265\n",
      "Epoch 178, Loss: 0.49094181765497674\n",
      "Epoch 179, Loss: 0.49066901222163783\n",
      "Epoch 180, Loss: 0.490393928005003\n",
      "Epoch 181, Loss: 0.4901079113527018\n",
      "Epoch 182, Loss: 0.48981984154671765\n",
      "Epoch 183, Loss: 0.4895234443341763\n",
      "Epoch 184, Loss: 0.48921459258368105\n",
      "Epoch 185, Loss: 0.4889145977883947\n",
      "Epoch 186, Loss: 0.4886175142174919\n",
      "Epoch 187, Loss: 0.4883282295514753\n",
      "Epoch 188, Loss: 0.4880435212302353\n",
      "Epoch 189, Loss: 0.48775699700499114\n",
      "Epoch 190, Loss: 0.48748008613994775\n",
      "Epoch 191, Loss: 0.4872132179171775\n",
      "Epoch 192, Loss: 0.48694738158534456\n",
      "Epoch 193, Loss: 0.48668121634032785\n",
      "Epoch 194, Loss: 0.4864280322301768\n",
      "Epoch 195, Loss: 0.486178532155915\n",
      "Epoch 196, Loss: 0.4859301919172944\n",
      "Epoch 197, Loss: 0.48568442460989747\n",
      "Epoch 198, Loss: 0.48544224711238787\n",
      "Epoch 199, Loss: 0.48519729219516017\n",
      "Epoch 200, Loss: 0.4849575446659644\n",
      "Epoch 201, Loss: 0.4847146197220469\n",
      "Epoch 202, Loss: 0.48447155759133576\n",
      "Epoch 203, Loss: 0.48422783492589483\n",
      "Epoch 204, Loss: 0.48397707745066637\n",
      "Epoch 205, Loss: 0.483737926529631\n",
      "Epoch 206, Loss: 0.4835020782648923\n",
      "Epoch 207, Loss: 0.48327083252052233\n",
      "Epoch 208, Loss: 0.4830508282762034\n",
      "Epoch 209, Loss: 0.48283269895503794\n",
      "Epoch 210, Loss: 0.48261215534620283\n",
      "Epoch 211, Loss: 0.48240746542225255\n",
      "Epoch 212, Loss: 0.4822187420754367\n",
      "Epoch 213, Loss: 0.48202997575341067\n",
      "Epoch 214, Loss: 0.4818441197783379\n",
      "Epoch 215, Loss: 0.4816502256898402\n",
      "Epoch 216, Loss: 0.4814589616492725\n",
      "Epoch 217, Loss: 0.48127338055379854\n",
      "Epoch 218, Loss: 0.4810898650639108\n",
      "Epoch 219, Loss: 0.48091365405597575\n",
      "Epoch 220, Loss: 0.480720996546175\n",
      "Epoch 221, Loss: 0.48054865040929456\n",
      "Epoch 222, Loss: 0.4803774677186426\n",
      "Epoch 223, Loss: 0.4801885671816001\n",
      "Epoch 224, Loss: 0.48000935557804647\n",
      "Epoch 225, Loss: 0.4798350303004208\n",
      "Epoch 226, Loss: 0.47967842665052907\n",
      "Epoch 227, Loss: 0.47947854451894606\n",
      "Epoch 228, Loss: 0.47928199900429136\n",
      "Epoch 229, Loss: 0.4791424752085709\n",
      "Epoch 230, Loss: 0.4789254904984497\n",
      "Epoch 231, Loss: 0.4787155823499749\n",
      "Epoch 232, Loss: 0.4785186611779873\n",
      "Epoch 233, Loss: 0.4783287773458093\n",
      "Epoch 234, Loss: 0.4781774454711345\n",
      "Epoch 235, Loss: 0.4780410437330356\n",
      "Epoch 236, Loss: 0.4778263811299359\n",
      "Epoch 237, Loss: 0.47765285017795805\n",
      "Epoch 238, Loss: 0.4775436338740249\n",
      "Epoch 239, Loss: 0.4774342833479412\n",
      "Epoch 240, Loss: 0.477263591547843\n",
      "Epoch 241, Loss: 0.47707859610269837\n",
      "Epoch 242, Loss: 0.47697972966255553\n",
      "Epoch 243, Loss: 0.4768431884384324\n",
      "Epoch 244, Loss: 0.4767408349977067\n",
      "Epoch 245, Loss: 0.4765819952057135\n",
      "Epoch 246, Loss: 0.4764706413665339\n",
      "Epoch 247, Loss: 0.47634462160009333\n",
      "Epoch 248, Loss: 0.4762527991635621\n",
      "Epoch 249, Loss: 0.4762014052862575\n",
      "Epoch 250, Loss: 0.4760283306209857\n",
      "Epoch 251, Loss: 0.47592260832103706\n",
      "Epoch 252, Loss: 0.475834869470249\n",
      "Epoch 253, Loss: 0.47570980894919\n",
      "Epoch 254, Loss: 0.47559397414543253\n",
      "Epoch 255, Loss: 0.4755005182088922\n",
      "Epoch 256, Loss: 0.4753894909333468\n",
      "Epoch 257, Loss: 0.47527958208571863\n",
      "Epoch 258, Loss: 0.47517947896910767\n",
      "Epoch 259, Loss: 0.4750690667233416\n",
      "Epoch 260, Loss: 0.47496625253354696\n",
      "Epoch 261, Loss: 0.47486506352442986\n",
      "Epoch 262, Loss: 0.47477116593942054\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2760212405273043\n",
      "Valence RMSE: 0.25460910689611294\n",
      "Arousal RMSE: 0.2958879063598708\n",
      "Test R^2 score: tensor([0.2144, 0.1331], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1209, 0.0300], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.17374388141657726\n",
      "Num of epochs: 263\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Epoch 121, Loss: 0.5116998508836337\n",
      "Epoch 122, Loss: 0.5111129902664598\n",
      "Epoch 123, Loss: 0.5105405741436312\n",
      "Epoch 124, Loss: 0.5099849010083686\n",
      "Epoch 125, Loss: 0.5094623759868415\n",
      "Epoch 126, Loss: 0.5089652841754122\n",
      "Epoch 127, Loss: 0.5084717799132713\n",
      "Epoch 128, Loss: 0.5079809643002384\n",
      "Epoch 129, Loss: 0.507507731608986\n",
      "Epoch 130, Loss: 0.5070515727009828\n",
      "Epoch 131, Loss: 0.5065962678643066\n",
      "Epoch 132, Loss: 0.5061448812256618\n",
      "Epoch 133, Loss: 0.5057069703510908\n",
      "Epoch 134, Loss: 0.5052756693952567\n",
      "Epoch 135, Loss: 0.5048445017462813\n",
      "Epoch 136, Loss: 0.504425284231979\n",
      "Epoch 137, Loss: 0.504022717877823\n",
      "Epoch 138, Loss: 0.5036312802042249\n",
      "Epoch 139, Loss: 0.5032368731068207\n",
      "Epoch 140, Loss: 0.5028511060044271\n",
      "Epoch 141, Loss: 0.5024804933282331\n",
      "Epoch 142, Loss: 0.5021182727396313\n",
      "Epoch 143, Loss: 0.5017581664990297\n",
      "Epoch 144, Loss: 0.5014050827892457\n",
      "Epoch 145, Loss: 0.501047675902349\n",
      "Epoch 146, Loss: 0.500690966247362\n",
      "Epoch 147, Loss: 0.5003476722262875\n",
      "Epoch 148, Loss: 0.5000068544871652\n",
      "Epoch 149, Loss: 0.49966338867073357\n",
      "Epoch 150, Loss: 0.4993280873087794\n",
      "Epoch 151, Loss: 0.49900257940428383\n",
      "Epoch 152, Loss: 0.4986865554458311\n",
      "Epoch 153, Loss: 0.49836949389755036\n",
      "Epoch 154, Loss: 0.49805375636713634\n",
      "Epoch 155, Loss: 0.49773873165141763\n",
      "Epoch 156, Loss: 0.49742224924400874\n",
      "Epoch 157, Loss: 0.49711425826812494\n",
      "Epoch 158, Loss: 0.4968010973402242\n",
      "Epoch 159, Loss: 0.4964901399315246\n",
      "Epoch 160, Loss: 0.49617675026847124\n",
      "Epoch 161, Loss: 0.4958670390985117\n",
      "Epoch 162, Loss: 0.4955578109301908\n",
      "Epoch 163, Loss: 0.4952510223984696\n",
      "Epoch 164, Loss: 0.49494577484111674\n",
      "Epoch 165, Loss: 0.49464511374066544\n",
      "Epoch 166, Loss: 0.49434383269975535\n",
      "Epoch 167, Loss: 0.49404631909698943\n",
      "Epoch 168, Loss: 0.49374530645143583\n",
      "Epoch 169, Loss: 0.49345064806438055\n",
      "Epoch 170, Loss: 0.49317001490258333\n",
      "Epoch 171, Loss: 0.4928927591216884\n",
      "Epoch 172, Loss: 0.4926138499628376\n",
      "Epoch 173, Loss: 0.49233528219511824\n",
      "Epoch 174, Loss: 0.49205852514110515\n",
      "Epoch 175, Loss: 0.4917773399684819\n",
      "Epoch 176, Loss: 0.49149629710928233\n",
      "Epoch 177, Loss: 0.4912165647150265\n",
      "Epoch 178, Loss: 0.49094181765497674\n",
      "Epoch 179, Loss: 0.49066901222163783\n",
      "Epoch 180, Loss: 0.490393928005003\n",
      "Epoch 181, Loss: 0.4901079113527018\n",
      "Epoch 182, Loss: 0.48981984154671765\n",
      "Epoch 183, Loss: 0.4895234443341763\n",
      "Epoch 184, Loss: 0.48921459258368105\n",
      "Epoch 185, Loss: 0.4889145977883947\n",
      "Epoch 186, Loss: 0.4886175142174919\n",
      "Epoch 187, Loss: 0.4883282295514753\n",
      "Epoch 188, Loss: 0.4880435212302353\n",
      "Epoch 189, Loss: 0.48775699700499114\n",
      "Epoch 190, Loss: 0.48748008613994775\n",
      "Epoch 191, Loss: 0.4872132179171775\n",
      "Epoch 192, Loss: 0.48694738158534456\n",
      "Epoch 193, Loss: 0.48668121634032785\n",
      "Epoch 194, Loss: 0.4864280322301768\n",
      "Epoch 195, Loss: 0.486178532155915\n",
      "Epoch 196, Loss: 0.4859301919172944\n",
      "Epoch 197, Loss: 0.48568442460989747\n",
      "Epoch 198, Loss: 0.48544224711238787\n",
      "Epoch 199, Loss: 0.48519729219516017\n",
      "Epoch 200, Loss: 0.4849575446659644\n",
      "Epoch 201, Loss: 0.4847146197220469\n",
      "Epoch 202, Loss: 0.48447155759133576\n",
      "Epoch 203, Loss: 0.48422783492589483\n",
      "Epoch 204, Loss: 0.48397707745066637\n",
      "Epoch 205, Loss: 0.483737926529631\n",
      "Epoch 206, Loss: 0.4835020782648923\n",
      "Epoch 207, Loss: 0.48327083252052233\n",
      "Epoch 208, Loss: 0.4830508282762034\n",
      "Epoch 209, Loss: 0.48283269895503794\n",
      "Epoch 210, Loss: 0.48261215534620283\n",
      "Epoch 211, Loss: 0.48240746542225255\n",
      "Epoch 212, Loss: 0.4822187420754367\n",
      "Epoch 213, Loss: 0.48202997575341067\n",
      "Epoch 214, Loss: 0.4818441197783379\n",
      "Epoch 215, Loss: 0.4816502256898402\n",
      "Epoch 216, Loss: 0.4814589616492725\n",
      "Epoch 217, Loss: 0.48127338055379854\n",
      "Epoch 218, Loss: 0.4810898650639108\n",
      "Epoch 219, Loss: 0.48091365405597575\n",
      "Epoch 220, Loss: 0.480720996546175\n",
      "Epoch 221, Loss: 0.48054865040929456\n",
      "Epoch 222, Loss: 0.4803774677186426\n",
      "Epoch 223, Loss: 0.4801885671816001\n",
      "Epoch 224, Loss: 0.48000935557804647\n",
      "Epoch 225, Loss: 0.4798350303004208\n",
      "Epoch 226, Loss: 0.47967842665052907\n",
      "Epoch 227, Loss: 0.47947854451894606\n",
      "Epoch 228, Loss: 0.47928199900429136\n",
      "Epoch 229, Loss: 0.4791424752085709\n",
      "Epoch 230, Loss: 0.4789254904984497\n",
      "Epoch 231, Loss: 0.4787155823499749\n",
      "Epoch 232, Loss: 0.4785186611779873\n",
      "Epoch 233, Loss: 0.4783287773458093\n",
      "Epoch 234, Loss: 0.4781774454711345\n",
      "Epoch 235, Loss: 0.4780410437330356\n",
      "Epoch 236, Loss: 0.4778263811299359\n",
      "Epoch 237, Loss: 0.47765285017795805\n",
      "Epoch 238, Loss: 0.4775436338740249\n",
      "Epoch 239, Loss: 0.4774342833479412\n",
      "Epoch 240, Loss: 0.477263591547843\n",
      "Epoch 241, Loss: 0.47707859610269837\n",
      "Epoch 242, Loss: 0.47697972966255553\n",
      "Epoch 243, Loss: 0.4768431884384324\n",
      "Epoch 244, Loss: 0.4767408349977067\n",
      "Epoch 245, Loss: 0.4765819952057135\n",
      "Epoch 246, Loss: 0.4764706413665339\n",
      "Epoch 247, Loss: 0.47634462160009333\n",
      "Epoch 248, Loss: 0.4762527991635621\n",
      "Epoch 249, Loss: 0.4762014052862575\n",
      "Epoch 250, Loss: 0.4760283306209857\n",
      "Epoch 251, Loss: 0.47592260832103706\n",
      "Epoch 252, Loss: 0.475834869470249\n",
      "Epoch 253, Loss: 0.47570980894919\n",
      "Epoch 254, Loss: 0.47559397414543253\n",
      "Epoch 255, Loss: 0.4755005182088922\n",
      "Epoch 256, Loss: 0.4753894909333468\n",
      "Epoch 257, Loss: 0.47527958208571863\n",
      "Epoch 258, Loss: 0.47517947896910767\n",
      "Epoch 259, Loss: 0.4750690667233416\n",
      "Epoch 260, Loss: 0.47496625253354696\n",
      "Epoch 261, Loss: 0.47486506352442986\n",
      "Epoch 262, Loss: 0.47477116593942054\n",
      "Epoch 263, Loss: 0.474752914637718\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.27604114544288344\n",
      "Valence RMSE: 0.2542338751899923\n",
      "Arousal RMSE: 0.29624747199042506\n",
      "Test R^2 score: tensor([0.2167, 0.1310], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1235, 0.0276], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.1738467548006108\n",
      "Num of epochs: 264\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Epoch 121, Loss: 0.5116998508836337\n",
      "Epoch 122, Loss: 0.5111129902664598\n",
      "Epoch 123, Loss: 0.5105405741436312\n",
      "Epoch 124, Loss: 0.5099849010083686\n",
      "Epoch 125, Loss: 0.5094623759868415\n",
      "Epoch 126, Loss: 0.5089652841754122\n",
      "Epoch 127, Loss: 0.5084717799132713\n",
      "Epoch 128, Loss: 0.5079809643002384\n",
      "Epoch 129, Loss: 0.507507731608986\n",
      "Epoch 130, Loss: 0.5070515727009828\n",
      "Epoch 131, Loss: 0.5065962678643066\n",
      "Epoch 132, Loss: 0.5061448812256618\n",
      "Epoch 133, Loss: 0.5057069703510908\n",
      "Epoch 134, Loss: 0.5052756693952567\n",
      "Epoch 135, Loss: 0.5048445017462813\n",
      "Epoch 136, Loss: 0.504425284231979\n",
      "Epoch 137, Loss: 0.504022717877823\n",
      "Epoch 138, Loss: 0.5036312802042249\n",
      "Epoch 139, Loss: 0.5032368731068207\n",
      "Epoch 140, Loss: 0.5028511060044271\n",
      "Epoch 141, Loss: 0.5024804933282331\n",
      "Epoch 142, Loss: 0.5021182727396313\n",
      "Epoch 143, Loss: 0.5017581664990297\n",
      "Epoch 144, Loss: 0.5014050827892457\n",
      "Epoch 145, Loss: 0.501047675902349\n",
      "Epoch 146, Loss: 0.500690966247362\n",
      "Epoch 147, Loss: 0.5003476722262875\n",
      "Epoch 148, Loss: 0.5000068544871652\n",
      "Epoch 149, Loss: 0.49966338867073357\n",
      "Epoch 150, Loss: 0.4993280873087794\n",
      "Epoch 151, Loss: 0.49900257940428383\n",
      "Epoch 152, Loss: 0.4986865554458311\n",
      "Epoch 153, Loss: 0.49836949389755036\n",
      "Epoch 154, Loss: 0.49805375636713634\n",
      "Epoch 155, Loss: 0.49773873165141763\n",
      "Epoch 156, Loss: 0.49742224924400874\n",
      "Epoch 157, Loss: 0.49711425826812494\n",
      "Epoch 158, Loss: 0.4968010973402242\n",
      "Epoch 159, Loss: 0.4964901399315246\n",
      "Epoch 160, Loss: 0.49617675026847124\n",
      "Epoch 161, Loss: 0.4958670390985117\n",
      "Epoch 162, Loss: 0.4955578109301908\n",
      "Epoch 163, Loss: 0.4952510223984696\n",
      "Epoch 164, Loss: 0.49494577484111674\n",
      "Epoch 165, Loss: 0.49464511374066544\n",
      "Epoch 166, Loss: 0.49434383269975535\n",
      "Epoch 167, Loss: 0.49404631909698943\n",
      "Epoch 168, Loss: 0.49374530645143583\n",
      "Epoch 169, Loss: 0.49345064806438055\n",
      "Epoch 170, Loss: 0.49317001490258333\n",
      "Epoch 171, Loss: 0.4928927591216884\n",
      "Epoch 172, Loss: 0.4926138499628376\n",
      "Epoch 173, Loss: 0.49233528219511824\n",
      "Epoch 174, Loss: 0.49205852514110515\n",
      "Epoch 175, Loss: 0.4917773399684819\n",
      "Epoch 176, Loss: 0.49149629710928233\n",
      "Epoch 177, Loss: 0.4912165647150265\n",
      "Epoch 178, Loss: 0.49094181765497674\n",
      "Epoch 179, Loss: 0.49066901222163783\n",
      "Epoch 180, Loss: 0.490393928005003\n",
      "Epoch 181, Loss: 0.4901079113527018\n",
      "Epoch 182, Loss: 0.48981984154671765\n",
      "Epoch 183, Loss: 0.4895234443341763\n",
      "Epoch 184, Loss: 0.48921459258368105\n",
      "Epoch 185, Loss: 0.4889145977883947\n",
      "Epoch 186, Loss: 0.4886175142174919\n",
      "Epoch 187, Loss: 0.4883282295514753\n",
      "Epoch 188, Loss: 0.4880435212302353\n",
      "Epoch 189, Loss: 0.48775699700499114\n",
      "Epoch 190, Loss: 0.48748008613994775\n",
      "Epoch 191, Loss: 0.4872132179171775\n",
      "Epoch 192, Loss: 0.48694738158534456\n",
      "Epoch 193, Loss: 0.48668121634032785\n",
      "Epoch 194, Loss: 0.4864280322301768\n",
      "Epoch 195, Loss: 0.486178532155915\n",
      "Epoch 196, Loss: 0.4859301919172944\n",
      "Epoch 197, Loss: 0.48568442460989747\n",
      "Epoch 198, Loss: 0.48544224711238787\n",
      "Epoch 199, Loss: 0.48519729219516017\n",
      "Epoch 200, Loss: 0.4849575446659644\n",
      "Epoch 201, Loss: 0.4847146197220469\n",
      "Epoch 202, Loss: 0.48447155759133576\n",
      "Epoch 203, Loss: 0.48422783492589483\n",
      "Epoch 204, Loss: 0.48397707745066637\n",
      "Epoch 205, Loss: 0.483737926529631\n",
      "Epoch 206, Loss: 0.4835020782648923\n",
      "Epoch 207, Loss: 0.48327083252052233\n",
      "Epoch 208, Loss: 0.4830508282762034\n",
      "Epoch 209, Loss: 0.48283269895503794\n",
      "Epoch 210, Loss: 0.48261215534620283\n",
      "Epoch 211, Loss: 0.48240746542225255\n",
      "Epoch 212, Loss: 0.4822187420754367\n",
      "Epoch 213, Loss: 0.48202997575341067\n",
      "Epoch 214, Loss: 0.4818441197783379\n",
      "Epoch 215, Loss: 0.4816502256898402\n",
      "Epoch 216, Loss: 0.4814589616492725\n",
      "Epoch 217, Loss: 0.48127338055379854\n",
      "Epoch 218, Loss: 0.4810898650639108\n",
      "Epoch 219, Loss: 0.48091365405597575\n",
      "Epoch 220, Loss: 0.480720996546175\n",
      "Epoch 221, Loss: 0.48054865040929456\n",
      "Epoch 222, Loss: 0.4803774677186426\n",
      "Epoch 223, Loss: 0.4801885671816001\n",
      "Epoch 224, Loss: 0.48000935557804647\n",
      "Epoch 225, Loss: 0.4798350303004208\n",
      "Epoch 226, Loss: 0.47967842665052907\n",
      "Epoch 227, Loss: 0.47947854451894606\n",
      "Epoch 228, Loss: 0.47928199900429136\n",
      "Epoch 229, Loss: 0.4791424752085709\n",
      "Epoch 230, Loss: 0.4789254904984497\n",
      "Epoch 231, Loss: 0.4787155823499749\n",
      "Epoch 232, Loss: 0.4785186611779873\n",
      "Epoch 233, Loss: 0.4783287773458093\n",
      "Epoch 234, Loss: 0.4781774454711345\n",
      "Epoch 235, Loss: 0.4780410437330356\n",
      "Epoch 236, Loss: 0.4778263811299359\n",
      "Epoch 237, Loss: 0.47765285017795805\n",
      "Epoch 238, Loss: 0.4775436338740249\n",
      "Epoch 239, Loss: 0.4774342833479412\n",
      "Epoch 240, Loss: 0.477263591547843\n",
      "Epoch 241, Loss: 0.47707859610269837\n",
      "Epoch 242, Loss: 0.47697972966255553\n",
      "Epoch 243, Loss: 0.4768431884384324\n",
      "Epoch 244, Loss: 0.4767408349977067\n",
      "Epoch 245, Loss: 0.4765819952057135\n",
      "Epoch 246, Loss: 0.4764706413665339\n",
      "Epoch 247, Loss: 0.47634462160009333\n",
      "Epoch 248, Loss: 0.4762527991635621\n",
      "Epoch 249, Loss: 0.4762014052862575\n",
      "Epoch 250, Loss: 0.4760283306209857\n",
      "Epoch 251, Loss: 0.47592260832103706\n",
      "Epoch 252, Loss: 0.475834869470249\n",
      "Epoch 253, Loss: 0.47570980894919\n",
      "Epoch 254, Loss: 0.47559397414543253\n",
      "Epoch 255, Loss: 0.4755005182088922\n",
      "Epoch 256, Loss: 0.4753894909333468\n",
      "Epoch 257, Loss: 0.47527958208571863\n",
      "Epoch 258, Loss: 0.47517947896910767\n",
      "Epoch 259, Loss: 0.4750690667233416\n",
      "Epoch 260, Loss: 0.47496625253354696\n",
      "Epoch 261, Loss: 0.47486506352442986\n",
      "Epoch 262, Loss: 0.47477116593942054\n",
      "Epoch 263, Loss: 0.474752914637718\n",
      "Epoch 264, Loss: 0.47457458619766507\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.27601269506851195\n",
      "Valence RMSE: 0.2542562158920145\n",
      "Arousal RMSE: 0.29617527303649105\n",
      "Test R^2 score: tensor([0.2166, 0.1314], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1233, 0.0281], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.17398967757599354\n",
      "Num of epochs: 265\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Epoch 121, Loss: 0.5116998508836337\n",
      "Epoch 122, Loss: 0.5111129902664598\n",
      "Epoch 123, Loss: 0.5105405741436312\n",
      "Epoch 124, Loss: 0.5099849010083686\n",
      "Epoch 125, Loss: 0.5094623759868415\n",
      "Epoch 126, Loss: 0.5089652841754122\n",
      "Epoch 127, Loss: 0.5084717799132713\n",
      "Epoch 128, Loss: 0.5079809643002384\n",
      "Epoch 129, Loss: 0.507507731608986\n",
      "Epoch 130, Loss: 0.5070515727009828\n",
      "Epoch 131, Loss: 0.5065962678643066\n",
      "Epoch 132, Loss: 0.5061448812256618\n",
      "Epoch 133, Loss: 0.5057069703510908\n",
      "Epoch 134, Loss: 0.5052756693952567\n",
      "Epoch 135, Loss: 0.5048445017462813\n",
      "Epoch 136, Loss: 0.504425284231979\n",
      "Epoch 137, Loss: 0.504022717877823\n",
      "Epoch 138, Loss: 0.5036312802042249\n",
      "Epoch 139, Loss: 0.5032368731068207\n",
      "Epoch 140, Loss: 0.5028511060044271\n",
      "Epoch 141, Loss: 0.5024804933282331\n",
      "Epoch 142, Loss: 0.5021182727396313\n",
      "Epoch 143, Loss: 0.5017581664990297\n",
      "Epoch 144, Loss: 0.5014050827892457\n",
      "Epoch 145, Loss: 0.501047675902349\n",
      "Epoch 146, Loss: 0.500690966247362\n",
      "Epoch 147, Loss: 0.5003476722262875\n",
      "Epoch 148, Loss: 0.5000068544871652\n",
      "Epoch 149, Loss: 0.49966338867073357\n",
      "Epoch 150, Loss: 0.4993280873087794\n",
      "Epoch 151, Loss: 0.49900257940428383\n",
      "Epoch 152, Loss: 0.4986865554458311\n",
      "Epoch 153, Loss: 0.49836949389755036\n",
      "Epoch 154, Loss: 0.49805375636713634\n",
      "Epoch 155, Loss: 0.49773873165141763\n",
      "Epoch 156, Loss: 0.49742224924400874\n",
      "Epoch 157, Loss: 0.49711425826812494\n",
      "Epoch 158, Loss: 0.4968010973402242\n",
      "Epoch 159, Loss: 0.4964901399315246\n",
      "Epoch 160, Loss: 0.49617675026847124\n",
      "Epoch 161, Loss: 0.4958670390985117\n",
      "Epoch 162, Loss: 0.4955578109301908\n",
      "Epoch 163, Loss: 0.4952510223984696\n",
      "Epoch 164, Loss: 0.49494577484111674\n",
      "Epoch 165, Loss: 0.49464511374066544\n",
      "Epoch 166, Loss: 0.49434383269975535\n",
      "Epoch 167, Loss: 0.49404631909698943\n",
      "Epoch 168, Loss: 0.49374530645143583\n",
      "Epoch 169, Loss: 0.49345064806438055\n",
      "Epoch 170, Loss: 0.49317001490258333\n",
      "Epoch 171, Loss: 0.4928927591216884\n",
      "Epoch 172, Loss: 0.4926138499628376\n",
      "Epoch 173, Loss: 0.49233528219511824\n",
      "Epoch 174, Loss: 0.49205852514110515\n",
      "Epoch 175, Loss: 0.4917773399684819\n",
      "Epoch 176, Loss: 0.49149629710928233\n",
      "Epoch 177, Loss: 0.4912165647150265\n",
      "Epoch 178, Loss: 0.49094181765497674\n",
      "Epoch 179, Loss: 0.49066901222163783\n",
      "Epoch 180, Loss: 0.490393928005003\n",
      "Epoch 181, Loss: 0.4901079113527018\n",
      "Epoch 182, Loss: 0.48981984154671765\n",
      "Epoch 183, Loss: 0.4895234443341763\n",
      "Epoch 184, Loss: 0.48921459258368105\n",
      "Epoch 185, Loss: 0.4889145977883947\n",
      "Epoch 186, Loss: 0.4886175142174919\n",
      "Epoch 187, Loss: 0.4883282295514753\n",
      "Epoch 188, Loss: 0.4880435212302353\n",
      "Epoch 189, Loss: 0.48775699700499114\n",
      "Epoch 190, Loss: 0.48748008613994775\n",
      "Epoch 191, Loss: 0.4872132179171775\n",
      "Epoch 192, Loss: 0.48694738158534456\n",
      "Epoch 193, Loss: 0.48668121634032785\n",
      "Epoch 194, Loss: 0.4864280322301768\n",
      "Epoch 195, Loss: 0.486178532155915\n",
      "Epoch 196, Loss: 0.4859301919172944\n",
      "Epoch 197, Loss: 0.48568442460989747\n",
      "Epoch 198, Loss: 0.48544224711238787\n",
      "Epoch 199, Loss: 0.48519729219516017\n",
      "Epoch 200, Loss: 0.4849575446659644\n",
      "Epoch 201, Loss: 0.4847146197220469\n",
      "Epoch 202, Loss: 0.48447155759133576\n",
      "Epoch 203, Loss: 0.48422783492589483\n",
      "Epoch 204, Loss: 0.48397707745066637\n",
      "Epoch 205, Loss: 0.483737926529631\n",
      "Epoch 206, Loss: 0.4835020782648923\n",
      "Epoch 207, Loss: 0.48327083252052233\n",
      "Epoch 208, Loss: 0.4830508282762034\n",
      "Epoch 209, Loss: 0.48283269895503794\n",
      "Epoch 210, Loss: 0.48261215534620283\n",
      "Epoch 211, Loss: 0.48240746542225255\n",
      "Epoch 212, Loss: 0.4822187420754367\n",
      "Epoch 213, Loss: 0.48202997575341067\n",
      "Epoch 214, Loss: 0.4818441197783379\n",
      "Epoch 215, Loss: 0.4816502256898402\n",
      "Epoch 216, Loss: 0.4814589616492725\n",
      "Epoch 217, Loss: 0.48127338055379854\n",
      "Epoch 218, Loss: 0.4810898650639108\n",
      "Epoch 219, Loss: 0.48091365405597575\n",
      "Epoch 220, Loss: 0.480720996546175\n",
      "Epoch 221, Loss: 0.48054865040929456\n",
      "Epoch 222, Loss: 0.4803774677186426\n",
      "Epoch 223, Loss: 0.4801885671816001\n",
      "Epoch 224, Loss: 0.48000935557804647\n",
      "Epoch 225, Loss: 0.4798350303004208\n",
      "Epoch 226, Loss: 0.47967842665052907\n",
      "Epoch 227, Loss: 0.47947854451894606\n",
      "Epoch 228, Loss: 0.47928199900429136\n",
      "Epoch 229, Loss: 0.4791424752085709\n",
      "Epoch 230, Loss: 0.4789254904984497\n",
      "Epoch 231, Loss: 0.4787155823499749\n",
      "Epoch 232, Loss: 0.4785186611779873\n",
      "Epoch 233, Loss: 0.4783287773458093\n",
      "Epoch 234, Loss: 0.4781774454711345\n",
      "Epoch 235, Loss: 0.4780410437330356\n",
      "Epoch 236, Loss: 0.4778263811299359\n",
      "Epoch 237, Loss: 0.47765285017795805\n",
      "Epoch 238, Loss: 0.4775436338740249\n",
      "Epoch 239, Loss: 0.4774342833479412\n",
      "Epoch 240, Loss: 0.477263591547843\n",
      "Epoch 241, Loss: 0.47707859610269837\n",
      "Epoch 242, Loss: 0.47697972966255553\n",
      "Epoch 243, Loss: 0.4768431884384324\n",
      "Epoch 244, Loss: 0.4767408349977067\n",
      "Epoch 245, Loss: 0.4765819952057135\n",
      "Epoch 246, Loss: 0.4764706413665339\n",
      "Epoch 247, Loss: 0.47634462160009333\n",
      "Epoch 248, Loss: 0.4762527991635621\n",
      "Epoch 249, Loss: 0.4762014052862575\n",
      "Epoch 250, Loss: 0.4760283306209857\n",
      "Epoch 251, Loss: 0.47592260832103706\n",
      "Epoch 252, Loss: 0.475834869470249\n",
      "Epoch 253, Loss: 0.47570980894919\n",
      "Epoch 254, Loss: 0.47559397414543253\n",
      "Epoch 255, Loss: 0.4755005182088922\n",
      "Epoch 256, Loss: 0.4753894909333468\n",
      "Epoch 257, Loss: 0.47527958208571863\n",
      "Epoch 258, Loss: 0.47517947896910767\n",
      "Epoch 259, Loss: 0.4750690667233416\n",
      "Epoch 260, Loss: 0.47496625253354696\n",
      "Epoch 261, Loss: 0.47486506352442986\n",
      "Epoch 262, Loss: 0.47477116593942054\n",
      "Epoch 263, Loss: 0.474752914637718\n",
      "Epoch 264, Loss: 0.47457458619766507\n",
      "Epoch 265, Loss: 0.47441594770111134\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2760114123075356\n",
      "Valence RMSE: 0.25438429778005117\n",
      "Arousal RMSE: 0.2960628792857205\n",
      "Test R^2 score: tensor([0.2158, 0.1321], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1225, 0.0288], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.17392446265697437\n",
      "Num of epochs: 266\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Epoch 121, Loss: 0.5116998508836337\n",
      "Epoch 122, Loss: 0.5111129902664598\n",
      "Epoch 123, Loss: 0.5105405741436312\n",
      "Epoch 124, Loss: 0.5099849010083686\n",
      "Epoch 125, Loss: 0.5094623759868415\n",
      "Epoch 126, Loss: 0.5089652841754122\n",
      "Epoch 127, Loss: 0.5084717799132713\n",
      "Epoch 128, Loss: 0.5079809643002384\n",
      "Epoch 129, Loss: 0.507507731608986\n",
      "Epoch 130, Loss: 0.5070515727009828\n",
      "Epoch 131, Loss: 0.5065962678643066\n",
      "Epoch 132, Loss: 0.5061448812256618\n",
      "Epoch 133, Loss: 0.5057069703510908\n",
      "Epoch 134, Loss: 0.5052756693952567\n",
      "Epoch 135, Loss: 0.5048445017462813\n",
      "Epoch 136, Loss: 0.504425284231979\n",
      "Epoch 137, Loss: 0.504022717877823\n",
      "Epoch 138, Loss: 0.5036312802042249\n",
      "Epoch 139, Loss: 0.5032368731068207\n",
      "Epoch 140, Loss: 0.5028511060044271\n",
      "Epoch 141, Loss: 0.5024804933282331\n",
      "Epoch 142, Loss: 0.5021182727396313\n",
      "Epoch 143, Loss: 0.5017581664990297\n",
      "Epoch 144, Loss: 0.5014050827892457\n",
      "Epoch 145, Loss: 0.501047675902349\n",
      "Epoch 146, Loss: 0.500690966247362\n",
      "Epoch 147, Loss: 0.5003476722262875\n",
      "Epoch 148, Loss: 0.5000068544871652\n",
      "Epoch 149, Loss: 0.49966338867073357\n",
      "Epoch 150, Loss: 0.4993280873087794\n",
      "Epoch 151, Loss: 0.49900257940428383\n",
      "Epoch 152, Loss: 0.4986865554458311\n",
      "Epoch 153, Loss: 0.49836949389755036\n",
      "Epoch 154, Loss: 0.49805375636713634\n",
      "Epoch 155, Loss: 0.49773873165141763\n",
      "Epoch 156, Loss: 0.49742224924400874\n",
      "Epoch 157, Loss: 0.49711425826812494\n",
      "Epoch 158, Loss: 0.4968010973402242\n",
      "Epoch 159, Loss: 0.4964901399315246\n",
      "Epoch 160, Loss: 0.49617675026847124\n",
      "Epoch 161, Loss: 0.4958670390985117\n",
      "Epoch 162, Loss: 0.4955578109301908\n",
      "Epoch 163, Loss: 0.4952510223984696\n",
      "Epoch 164, Loss: 0.49494577484111674\n",
      "Epoch 165, Loss: 0.49464511374066544\n",
      "Epoch 166, Loss: 0.49434383269975535\n",
      "Epoch 167, Loss: 0.49404631909698943\n",
      "Epoch 168, Loss: 0.49374530645143583\n",
      "Epoch 169, Loss: 0.49345064806438055\n",
      "Epoch 170, Loss: 0.49317001490258333\n",
      "Epoch 171, Loss: 0.4928927591216884\n",
      "Epoch 172, Loss: 0.4926138499628376\n",
      "Epoch 173, Loss: 0.49233528219511824\n",
      "Epoch 174, Loss: 0.49205852514110515\n",
      "Epoch 175, Loss: 0.4917773399684819\n",
      "Epoch 176, Loss: 0.49149629710928233\n",
      "Epoch 177, Loss: 0.4912165647150265\n",
      "Epoch 178, Loss: 0.49094181765497674\n",
      "Epoch 179, Loss: 0.49066901222163783\n",
      "Epoch 180, Loss: 0.490393928005003\n",
      "Epoch 181, Loss: 0.4901079113527018\n",
      "Epoch 182, Loss: 0.48981984154671765\n",
      "Epoch 183, Loss: 0.4895234443341763\n",
      "Epoch 184, Loss: 0.48921459258368105\n",
      "Epoch 185, Loss: 0.4889145977883947\n",
      "Epoch 186, Loss: 0.4886175142174919\n",
      "Epoch 187, Loss: 0.4883282295514753\n",
      "Epoch 188, Loss: 0.4880435212302353\n",
      "Epoch 189, Loss: 0.48775699700499114\n",
      "Epoch 190, Loss: 0.48748008613994775\n",
      "Epoch 191, Loss: 0.4872132179171775\n",
      "Epoch 192, Loss: 0.48694738158534456\n",
      "Epoch 193, Loss: 0.48668121634032785\n",
      "Epoch 194, Loss: 0.4864280322301768\n",
      "Epoch 195, Loss: 0.486178532155915\n",
      "Epoch 196, Loss: 0.4859301919172944\n",
      "Epoch 197, Loss: 0.48568442460989747\n",
      "Epoch 198, Loss: 0.48544224711238787\n",
      "Epoch 199, Loss: 0.48519729219516017\n",
      "Epoch 200, Loss: 0.4849575446659644\n",
      "Epoch 201, Loss: 0.4847146197220469\n",
      "Epoch 202, Loss: 0.48447155759133576\n",
      "Epoch 203, Loss: 0.48422783492589483\n",
      "Epoch 204, Loss: 0.48397707745066637\n",
      "Epoch 205, Loss: 0.483737926529631\n",
      "Epoch 206, Loss: 0.4835020782648923\n",
      "Epoch 207, Loss: 0.48327083252052233\n",
      "Epoch 208, Loss: 0.4830508282762034\n",
      "Epoch 209, Loss: 0.48283269895503794\n",
      "Epoch 210, Loss: 0.48261215534620283\n",
      "Epoch 211, Loss: 0.48240746542225255\n",
      "Epoch 212, Loss: 0.4822187420754367\n",
      "Epoch 213, Loss: 0.48202997575341067\n",
      "Epoch 214, Loss: 0.4818441197783379\n",
      "Epoch 215, Loss: 0.4816502256898402\n",
      "Epoch 216, Loss: 0.4814589616492725\n",
      "Epoch 217, Loss: 0.48127338055379854\n",
      "Epoch 218, Loss: 0.4810898650639108\n",
      "Epoch 219, Loss: 0.48091365405597575\n",
      "Epoch 220, Loss: 0.480720996546175\n",
      "Epoch 221, Loss: 0.48054865040929456\n",
      "Epoch 222, Loss: 0.4803774677186426\n",
      "Epoch 223, Loss: 0.4801885671816001\n",
      "Epoch 224, Loss: 0.48000935557804647\n",
      "Epoch 225, Loss: 0.4798350303004208\n",
      "Epoch 226, Loss: 0.47967842665052907\n",
      "Epoch 227, Loss: 0.47947854451894606\n",
      "Epoch 228, Loss: 0.47928199900429136\n",
      "Epoch 229, Loss: 0.4791424752085709\n",
      "Epoch 230, Loss: 0.4789254904984497\n",
      "Epoch 231, Loss: 0.4787155823499749\n",
      "Epoch 232, Loss: 0.4785186611779873\n",
      "Epoch 233, Loss: 0.4783287773458093\n",
      "Epoch 234, Loss: 0.4781774454711345\n",
      "Epoch 235, Loss: 0.4780410437330356\n",
      "Epoch 236, Loss: 0.4778263811299359\n",
      "Epoch 237, Loss: 0.47765285017795805\n",
      "Epoch 238, Loss: 0.4775436338740249\n",
      "Epoch 239, Loss: 0.4774342833479412\n",
      "Epoch 240, Loss: 0.477263591547843\n",
      "Epoch 241, Loss: 0.47707859610269837\n",
      "Epoch 242, Loss: 0.47697972966255553\n",
      "Epoch 243, Loss: 0.4768431884384324\n",
      "Epoch 244, Loss: 0.4767408349977067\n",
      "Epoch 245, Loss: 0.4765819952057135\n",
      "Epoch 246, Loss: 0.4764706413665339\n",
      "Epoch 247, Loss: 0.47634462160009333\n",
      "Epoch 248, Loss: 0.4762527991635621\n",
      "Epoch 249, Loss: 0.4762014052862575\n",
      "Epoch 250, Loss: 0.4760283306209857\n",
      "Epoch 251, Loss: 0.47592260832103706\n",
      "Epoch 252, Loss: 0.475834869470249\n",
      "Epoch 253, Loss: 0.47570980894919\n",
      "Epoch 254, Loss: 0.47559397414543253\n",
      "Epoch 255, Loss: 0.4755005182088922\n",
      "Epoch 256, Loss: 0.4753894909333468\n",
      "Epoch 257, Loss: 0.47527958208571863\n",
      "Epoch 258, Loss: 0.47517947896910767\n",
      "Epoch 259, Loss: 0.4750690667233416\n",
      "Epoch 260, Loss: 0.47496625253354696\n",
      "Epoch 261, Loss: 0.47486506352442986\n",
      "Epoch 262, Loss: 0.47477116593942054\n",
      "Epoch 263, Loss: 0.474752914637718\n",
      "Epoch 264, Loss: 0.47457458619766507\n",
      "Epoch 265, Loss: 0.47441594770111134\n",
      "Epoch 266, Loss: 0.4743326895520222\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2761126899765426\n",
      "Valence RMSE: 0.2542436761693814\n",
      "Arousal RMSE: 0.2963723810682152\n",
      "Test R^2 score: tensor([0.2166, 0.1303], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1234, 0.0268], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.1734500778197679\n",
      "Num of epochs: 267\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Epoch 121, Loss: 0.5116998508836337\n",
      "Epoch 122, Loss: 0.5111129902664598\n",
      "Epoch 123, Loss: 0.5105405741436312\n",
      "Epoch 124, Loss: 0.5099849010083686\n",
      "Epoch 125, Loss: 0.5094623759868415\n",
      "Epoch 126, Loss: 0.5089652841754122\n",
      "Epoch 127, Loss: 0.5084717799132713\n",
      "Epoch 128, Loss: 0.5079809643002384\n",
      "Epoch 129, Loss: 0.507507731608986\n",
      "Epoch 130, Loss: 0.5070515727009828\n",
      "Epoch 131, Loss: 0.5065962678643066\n",
      "Epoch 132, Loss: 0.5061448812256618\n",
      "Epoch 133, Loss: 0.5057069703510908\n",
      "Epoch 134, Loss: 0.5052756693952567\n",
      "Epoch 135, Loss: 0.5048445017462813\n",
      "Epoch 136, Loss: 0.504425284231979\n",
      "Epoch 137, Loss: 0.504022717877823\n",
      "Epoch 138, Loss: 0.5036312802042249\n",
      "Epoch 139, Loss: 0.5032368731068207\n",
      "Epoch 140, Loss: 0.5028511060044271\n",
      "Epoch 141, Loss: 0.5024804933282331\n",
      "Epoch 142, Loss: 0.5021182727396313\n",
      "Epoch 143, Loss: 0.5017581664990297\n",
      "Epoch 144, Loss: 0.5014050827892457\n",
      "Epoch 145, Loss: 0.501047675902349\n",
      "Epoch 146, Loss: 0.500690966247362\n",
      "Epoch 147, Loss: 0.5003476722262875\n",
      "Epoch 148, Loss: 0.5000068544871652\n",
      "Epoch 149, Loss: 0.49966338867073357\n",
      "Epoch 150, Loss: 0.4993280873087794\n",
      "Epoch 151, Loss: 0.49900257940428383\n",
      "Epoch 152, Loss: 0.4986865554458311\n",
      "Epoch 153, Loss: 0.49836949389755036\n",
      "Epoch 154, Loss: 0.49805375636713634\n",
      "Epoch 155, Loss: 0.49773873165141763\n",
      "Epoch 156, Loss: 0.49742224924400874\n",
      "Epoch 157, Loss: 0.49711425826812494\n",
      "Epoch 158, Loss: 0.4968010973402242\n",
      "Epoch 159, Loss: 0.4964901399315246\n",
      "Epoch 160, Loss: 0.49617675026847124\n",
      "Epoch 161, Loss: 0.4958670390985117\n",
      "Epoch 162, Loss: 0.4955578109301908\n",
      "Epoch 163, Loss: 0.4952510223984696\n",
      "Epoch 164, Loss: 0.49494577484111674\n",
      "Epoch 165, Loss: 0.49464511374066544\n",
      "Epoch 166, Loss: 0.49434383269975535\n",
      "Epoch 167, Loss: 0.49404631909698943\n",
      "Epoch 168, Loss: 0.49374530645143583\n",
      "Epoch 169, Loss: 0.49345064806438055\n",
      "Epoch 170, Loss: 0.49317001490258333\n",
      "Epoch 171, Loss: 0.4928927591216884\n",
      "Epoch 172, Loss: 0.4926138499628376\n",
      "Epoch 173, Loss: 0.49233528219511824\n",
      "Epoch 174, Loss: 0.49205852514110515\n",
      "Epoch 175, Loss: 0.4917773399684819\n",
      "Epoch 176, Loss: 0.49149629710928233\n",
      "Epoch 177, Loss: 0.4912165647150265\n",
      "Epoch 178, Loss: 0.49094181765497674\n",
      "Epoch 179, Loss: 0.49066901222163783\n",
      "Epoch 180, Loss: 0.490393928005003\n",
      "Epoch 181, Loss: 0.4901079113527018\n",
      "Epoch 182, Loss: 0.48981984154671765\n",
      "Epoch 183, Loss: 0.4895234443341763\n",
      "Epoch 184, Loss: 0.48921459258368105\n",
      "Epoch 185, Loss: 0.4889145977883947\n",
      "Epoch 186, Loss: 0.4886175142174919\n",
      "Epoch 187, Loss: 0.4883282295514753\n",
      "Epoch 188, Loss: 0.4880435212302353\n",
      "Epoch 189, Loss: 0.48775699700499114\n",
      "Epoch 190, Loss: 0.48748008613994775\n",
      "Epoch 191, Loss: 0.4872132179171775\n",
      "Epoch 192, Loss: 0.48694738158534456\n",
      "Epoch 193, Loss: 0.48668121634032785\n",
      "Epoch 194, Loss: 0.4864280322301768\n",
      "Epoch 195, Loss: 0.486178532155915\n",
      "Epoch 196, Loss: 0.4859301919172944\n",
      "Epoch 197, Loss: 0.48568442460989747\n",
      "Epoch 198, Loss: 0.48544224711238787\n",
      "Epoch 199, Loss: 0.48519729219516017\n",
      "Epoch 200, Loss: 0.4849575446659644\n",
      "Epoch 201, Loss: 0.4847146197220469\n",
      "Epoch 202, Loss: 0.48447155759133576\n",
      "Epoch 203, Loss: 0.48422783492589483\n",
      "Epoch 204, Loss: 0.48397707745066637\n",
      "Epoch 205, Loss: 0.483737926529631\n",
      "Epoch 206, Loss: 0.4835020782648923\n",
      "Epoch 207, Loss: 0.48327083252052233\n",
      "Epoch 208, Loss: 0.4830508282762034\n",
      "Epoch 209, Loss: 0.48283269895503794\n",
      "Epoch 210, Loss: 0.48261215534620283\n",
      "Epoch 211, Loss: 0.48240746542225255\n",
      "Epoch 212, Loss: 0.4822187420754367\n",
      "Epoch 213, Loss: 0.48202997575341067\n",
      "Epoch 214, Loss: 0.4818441197783379\n",
      "Epoch 215, Loss: 0.4816502256898402\n",
      "Epoch 216, Loss: 0.4814589616492725\n",
      "Epoch 217, Loss: 0.48127338055379854\n",
      "Epoch 218, Loss: 0.4810898650639108\n",
      "Epoch 219, Loss: 0.48091365405597575\n",
      "Epoch 220, Loss: 0.480720996546175\n",
      "Epoch 221, Loss: 0.48054865040929456\n",
      "Epoch 222, Loss: 0.4803774677186426\n",
      "Epoch 223, Loss: 0.4801885671816001\n",
      "Epoch 224, Loss: 0.48000935557804647\n",
      "Epoch 225, Loss: 0.4798350303004208\n",
      "Epoch 226, Loss: 0.47967842665052907\n",
      "Epoch 227, Loss: 0.47947854451894606\n",
      "Epoch 228, Loss: 0.47928199900429136\n",
      "Epoch 229, Loss: 0.4791424752085709\n",
      "Epoch 230, Loss: 0.4789254904984497\n",
      "Epoch 231, Loss: 0.4787155823499749\n",
      "Epoch 232, Loss: 0.4785186611779873\n",
      "Epoch 233, Loss: 0.4783287773458093\n",
      "Epoch 234, Loss: 0.4781774454711345\n",
      "Epoch 235, Loss: 0.4780410437330356\n",
      "Epoch 236, Loss: 0.4778263811299359\n",
      "Epoch 237, Loss: 0.47765285017795805\n",
      "Epoch 238, Loss: 0.4775436338740249\n",
      "Epoch 239, Loss: 0.4774342833479412\n",
      "Epoch 240, Loss: 0.477263591547843\n",
      "Epoch 241, Loss: 0.47707859610269837\n",
      "Epoch 242, Loss: 0.47697972966255553\n",
      "Epoch 243, Loss: 0.4768431884384324\n",
      "Epoch 244, Loss: 0.4767408349977067\n",
      "Epoch 245, Loss: 0.4765819952057135\n",
      "Epoch 246, Loss: 0.4764706413665339\n",
      "Epoch 247, Loss: 0.47634462160009333\n",
      "Epoch 248, Loss: 0.4762527991635621\n",
      "Epoch 249, Loss: 0.4762014052862575\n",
      "Epoch 250, Loss: 0.4760283306209857\n",
      "Epoch 251, Loss: 0.47592260832103706\n",
      "Epoch 252, Loss: 0.475834869470249\n",
      "Epoch 253, Loss: 0.47570980894919\n",
      "Epoch 254, Loss: 0.47559397414543253\n",
      "Epoch 255, Loss: 0.4755005182088922\n",
      "Epoch 256, Loss: 0.4753894909333468\n",
      "Epoch 257, Loss: 0.47527958208571863\n",
      "Epoch 258, Loss: 0.47517947896910767\n",
      "Epoch 259, Loss: 0.4750690667233416\n",
      "Epoch 260, Loss: 0.47496625253354696\n",
      "Epoch 261, Loss: 0.47486506352442986\n",
      "Epoch 262, Loss: 0.47477116593942054\n",
      "Epoch 263, Loss: 0.474752914637718\n",
      "Epoch 264, Loss: 0.47457458619766507\n",
      "Epoch 265, Loss: 0.47441594770111134\n",
      "Epoch 266, Loss: 0.4743326895520222\n",
      "Epoch 267, Loss: 0.47424442089802893\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.27652312601855644\n",
      "Valence RMSE: 0.25473422911727867\n",
      "Arousal RMSE: 0.29671628024457086\n",
      "Test R^2 score: tensor([0.2136, 0.1283], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1200, 0.0245], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.17092735193533481\n",
      "Num of epochs: 268\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Epoch 121, Loss: 0.5116998508836337\n",
      "Epoch 122, Loss: 0.5111129902664598\n",
      "Epoch 123, Loss: 0.5105405741436312\n",
      "Epoch 124, Loss: 0.5099849010083686\n",
      "Epoch 125, Loss: 0.5094623759868415\n",
      "Epoch 126, Loss: 0.5089652841754122\n",
      "Epoch 127, Loss: 0.5084717799132713\n",
      "Epoch 128, Loss: 0.5079809643002384\n",
      "Epoch 129, Loss: 0.507507731608986\n",
      "Epoch 130, Loss: 0.5070515727009828\n",
      "Epoch 131, Loss: 0.5065962678643066\n",
      "Epoch 132, Loss: 0.5061448812256618\n",
      "Epoch 133, Loss: 0.5057069703510908\n",
      "Epoch 134, Loss: 0.5052756693952567\n",
      "Epoch 135, Loss: 0.5048445017462813\n",
      "Epoch 136, Loss: 0.504425284231979\n",
      "Epoch 137, Loss: 0.504022717877823\n",
      "Epoch 138, Loss: 0.5036312802042249\n",
      "Epoch 139, Loss: 0.5032368731068207\n",
      "Epoch 140, Loss: 0.5028511060044271\n",
      "Epoch 141, Loss: 0.5024804933282331\n",
      "Epoch 142, Loss: 0.5021182727396313\n",
      "Epoch 143, Loss: 0.5017581664990297\n",
      "Epoch 144, Loss: 0.5014050827892457\n",
      "Epoch 145, Loss: 0.501047675902349\n",
      "Epoch 146, Loss: 0.500690966247362\n",
      "Epoch 147, Loss: 0.5003476722262875\n",
      "Epoch 148, Loss: 0.5000068544871652\n",
      "Epoch 149, Loss: 0.49966338867073357\n",
      "Epoch 150, Loss: 0.4993280873087794\n",
      "Epoch 151, Loss: 0.49900257940428383\n",
      "Epoch 152, Loss: 0.4986865554458311\n",
      "Epoch 153, Loss: 0.49836949389755036\n",
      "Epoch 154, Loss: 0.49805375636713634\n",
      "Epoch 155, Loss: 0.49773873165141763\n",
      "Epoch 156, Loss: 0.49742224924400874\n",
      "Epoch 157, Loss: 0.49711425826812494\n",
      "Epoch 158, Loss: 0.4968010973402242\n",
      "Epoch 159, Loss: 0.4964901399315246\n",
      "Epoch 160, Loss: 0.49617675026847124\n",
      "Epoch 161, Loss: 0.4958670390985117\n",
      "Epoch 162, Loss: 0.4955578109301908\n",
      "Epoch 163, Loss: 0.4952510223984696\n",
      "Epoch 164, Loss: 0.49494577484111674\n",
      "Epoch 165, Loss: 0.49464511374066544\n",
      "Epoch 166, Loss: 0.49434383269975535\n",
      "Epoch 167, Loss: 0.49404631909698943\n",
      "Epoch 168, Loss: 0.49374530645143583\n",
      "Epoch 169, Loss: 0.49345064806438055\n",
      "Epoch 170, Loss: 0.49317001490258333\n",
      "Epoch 171, Loss: 0.4928927591216884\n",
      "Epoch 172, Loss: 0.4926138499628376\n",
      "Epoch 173, Loss: 0.49233528219511824\n",
      "Epoch 174, Loss: 0.49205852514110515\n",
      "Epoch 175, Loss: 0.4917773399684819\n",
      "Epoch 176, Loss: 0.49149629710928233\n",
      "Epoch 177, Loss: 0.4912165647150265\n",
      "Epoch 178, Loss: 0.49094181765497674\n",
      "Epoch 179, Loss: 0.49066901222163783\n",
      "Epoch 180, Loss: 0.490393928005003\n",
      "Epoch 181, Loss: 0.4901079113527018\n",
      "Epoch 182, Loss: 0.48981984154671765\n",
      "Epoch 183, Loss: 0.4895234443341763\n",
      "Epoch 184, Loss: 0.48921459258368105\n",
      "Epoch 185, Loss: 0.4889145977883947\n",
      "Epoch 186, Loss: 0.4886175142174919\n",
      "Epoch 187, Loss: 0.4883282295514753\n",
      "Epoch 188, Loss: 0.4880435212302353\n",
      "Epoch 189, Loss: 0.48775699700499114\n",
      "Epoch 190, Loss: 0.48748008613994775\n",
      "Epoch 191, Loss: 0.4872132179171775\n",
      "Epoch 192, Loss: 0.48694738158534456\n",
      "Epoch 193, Loss: 0.48668121634032785\n",
      "Epoch 194, Loss: 0.4864280322301768\n",
      "Epoch 195, Loss: 0.486178532155915\n",
      "Epoch 196, Loss: 0.4859301919172944\n",
      "Epoch 197, Loss: 0.48568442460989747\n",
      "Epoch 198, Loss: 0.48544224711238787\n",
      "Epoch 199, Loss: 0.48519729219516017\n",
      "Epoch 200, Loss: 0.4849575446659644\n",
      "Epoch 201, Loss: 0.4847146197220469\n",
      "Epoch 202, Loss: 0.48447155759133576\n",
      "Epoch 203, Loss: 0.48422783492589483\n",
      "Epoch 204, Loss: 0.48397707745066637\n",
      "Epoch 205, Loss: 0.483737926529631\n",
      "Epoch 206, Loss: 0.4835020782648923\n",
      "Epoch 207, Loss: 0.48327083252052233\n",
      "Epoch 208, Loss: 0.4830508282762034\n",
      "Epoch 209, Loss: 0.48283269895503794\n",
      "Epoch 210, Loss: 0.48261215534620283\n",
      "Epoch 211, Loss: 0.48240746542225255\n",
      "Epoch 212, Loss: 0.4822187420754367\n",
      "Epoch 213, Loss: 0.48202997575341067\n",
      "Epoch 214, Loss: 0.4818441197783379\n",
      "Epoch 215, Loss: 0.4816502256898402\n",
      "Epoch 216, Loss: 0.4814589616492725\n",
      "Epoch 217, Loss: 0.48127338055379854\n",
      "Epoch 218, Loss: 0.4810898650639108\n",
      "Epoch 219, Loss: 0.48091365405597575\n",
      "Epoch 220, Loss: 0.480720996546175\n",
      "Epoch 221, Loss: 0.48054865040929456\n",
      "Epoch 222, Loss: 0.4803774677186426\n",
      "Epoch 223, Loss: 0.4801885671816001\n",
      "Epoch 224, Loss: 0.48000935557804647\n",
      "Epoch 225, Loss: 0.4798350303004208\n",
      "Epoch 226, Loss: 0.47967842665052907\n",
      "Epoch 227, Loss: 0.47947854451894606\n",
      "Epoch 228, Loss: 0.47928199900429136\n",
      "Epoch 229, Loss: 0.4791424752085709\n",
      "Epoch 230, Loss: 0.4789254904984497\n",
      "Epoch 231, Loss: 0.4787155823499749\n",
      "Epoch 232, Loss: 0.4785186611779873\n",
      "Epoch 233, Loss: 0.4783287773458093\n",
      "Epoch 234, Loss: 0.4781774454711345\n",
      "Epoch 235, Loss: 0.4780410437330356\n",
      "Epoch 236, Loss: 0.4778263811299359\n",
      "Epoch 237, Loss: 0.47765285017795805\n",
      "Epoch 238, Loss: 0.4775436338740249\n",
      "Epoch 239, Loss: 0.4774342833479412\n",
      "Epoch 240, Loss: 0.477263591547843\n",
      "Epoch 241, Loss: 0.47707859610269837\n",
      "Epoch 242, Loss: 0.47697972966255553\n",
      "Epoch 243, Loss: 0.4768431884384324\n",
      "Epoch 244, Loss: 0.4767408349977067\n",
      "Epoch 245, Loss: 0.4765819952057135\n",
      "Epoch 246, Loss: 0.4764706413665339\n",
      "Epoch 247, Loss: 0.47634462160009333\n",
      "Epoch 248, Loss: 0.4762527991635621\n",
      "Epoch 249, Loss: 0.4762014052862575\n",
      "Epoch 250, Loss: 0.4760283306209857\n",
      "Epoch 251, Loss: 0.47592260832103706\n",
      "Epoch 252, Loss: 0.475834869470249\n",
      "Epoch 253, Loss: 0.47570980894919\n",
      "Epoch 254, Loss: 0.47559397414543253\n",
      "Epoch 255, Loss: 0.4755005182088922\n",
      "Epoch 256, Loss: 0.4753894909333468\n",
      "Epoch 257, Loss: 0.47527958208571863\n",
      "Epoch 258, Loss: 0.47517947896910767\n",
      "Epoch 259, Loss: 0.4750690667233416\n",
      "Epoch 260, Loss: 0.47496625253354696\n",
      "Epoch 261, Loss: 0.47486506352442986\n",
      "Epoch 262, Loss: 0.47477116593942054\n",
      "Epoch 263, Loss: 0.474752914637718\n",
      "Epoch 264, Loss: 0.47457458619766507\n",
      "Epoch 265, Loss: 0.47441594770111134\n",
      "Epoch 266, Loss: 0.4743326895520222\n",
      "Epoch 267, Loss: 0.47424442089802893\n",
      "Epoch 268, Loss: 0.47409961152836255\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2766609832078837\n",
      "Valence RMSE: 0.254718487313353\n",
      "Arousal RMSE: 0.2969866856946672\n",
      "Test R^2 score: tensor([0.2137, 0.1267], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1202, 0.0228], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.17018113689633807\n",
      "Num of epochs: 269\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Epoch 121, Loss: 0.5116998508836337\n",
      "Epoch 122, Loss: 0.5111129902664598\n",
      "Epoch 123, Loss: 0.5105405741436312\n",
      "Epoch 124, Loss: 0.5099849010083686\n",
      "Epoch 125, Loss: 0.5094623759868415\n",
      "Epoch 126, Loss: 0.5089652841754122\n",
      "Epoch 127, Loss: 0.5084717799132713\n",
      "Epoch 128, Loss: 0.5079809643002384\n",
      "Epoch 129, Loss: 0.507507731608986\n",
      "Epoch 130, Loss: 0.5070515727009828\n",
      "Epoch 131, Loss: 0.5065962678643066\n",
      "Epoch 132, Loss: 0.5061448812256618\n",
      "Epoch 133, Loss: 0.5057069703510908\n",
      "Epoch 134, Loss: 0.5052756693952567\n",
      "Epoch 135, Loss: 0.5048445017462813\n",
      "Epoch 136, Loss: 0.504425284231979\n",
      "Epoch 137, Loss: 0.504022717877823\n",
      "Epoch 138, Loss: 0.5036312802042249\n",
      "Epoch 139, Loss: 0.5032368731068207\n",
      "Epoch 140, Loss: 0.5028511060044271\n",
      "Epoch 141, Loss: 0.5024804933282331\n",
      "Epoch 142, Loss: 0.5021182727396313\n",
      "Epoch 143, Loss: 0.5017581664990297\n",
      "Epoch 144, Loss: 0.5014050827892457\n",
      "Epoch 145, Loss: 0.501047675902349\n",
      "Epoch 146, Loss: 0.500690966247362\n",
      "Epoch 147, Loss: 0.5003476722262875\n",
      "Epoch 148, Loss: 0.5000068544871652\n",
      "Epoch 149, Loss: 0.49966338867073357\n",
      "Epoch 150, Loss: 0.4993280873087794\n",
      "Epoch 151, Loss: 0.49900257940428383\n",
      "Epoch 152, Loss: 0.4986865554458311\n",
      "Epoch 153, Loss: 0.49836949389755036\n",
      "Epoch 154, Loss: 0.49805375636713634\n",
      "Epoch 155, Loss: 0.49773873165141763\n",
      "Epoch 156, Loss: 0.49742224924400874\n",
      "Epoch 157, Loss: 0.49711425826812494\n",
      "Epoch 158, Loss: 0.4968010973402242\n",
      "Epoch 159, Loss: 0.4964901399315246\n",
      "Epoch 160, Loss: 0.49617675026847124\n",
      "Epoch 161, Loss: 0.4958670390985117\n",
      "Epoch 162, Loss: 0.4955578109301908\n",
      "Epoch 163, Loss: 0.4952510223984696\n",
      "Epoch 164, Loss: 0.49494577484111674\n",
      "Epoch 165, Loss: 0.49464511374066544\n",
      "Epoch 166, Loss: 0.49434383269975535\n",
      "Epoch 167, Loss: 0.49404631909698943\n",
      "Epoch 168, Loss: 0.49374530645143583\n",
      "Epoch 169, Loss: 0.49345064806438055\n",
      "Epoch 170, Loss: 0.49317001490258333\n",
      "Epoch 171, Loss: 0.4928927591216884\n",
      "Epoch 172, Loss: 0.4926138499628376\n",
      "Epoch 173, Loss: 0.49233528219511824\n",
      "Epoch 174, Loss: 0.49205852514110515\n",
      "Epoch 175, Loss: 0.4917773399684819\n",
      "Epoch 176, Loss: 0.49149629710928233\n",
      "Epoch 177, Loss: 0.4912165647150265\n",
      "Epoch 178, Loss: 0.49094181765497674\n",
      "Epoch 179, Loss: 0.49066901222163783\n",
      "Epoch 180, Loss: 0.490393928005003\n",
      "Epoch 181, Loss: 0.4901079113527018\n",
      "Epoch 182, Loss: 0.48981984154671765\n",
      "Epoch 183, Loss: 0.4895234443341763\n",
      "Epoch 184, Loss: 0.48921459258368105\n",
      "Epoch 185, Loss: 0.4889145977883947\n",
      "Epoch 186, Loss: 0.4886175142174919\n",
      "Epoch 187, Loss: 0.4883282295514753\n",
      "Epoch 188, Loss: 0.4880435212302353\n",
      "Epoch 189, Loss: 0.48775699700499114\n",
      "Epoch 190, Loss: 0.48748008613994775\n",
      "Epoch 191, Loss: 0.4872132179171775\n",
      "Epoch 192, Loss: 0.48694738158534456\n",
      "Epoch 193, Loss: 0.48668121634032785\n",
      "Epoch 194, Loss: 0.4864280322301768\n",
      "Epoch 195, Loss: 0.486178532155915\n",
      "Epoch 196, Loss: 0.4859301919172944\n",
      "Epoch 197, Loss: 0.48568442460989747\n",
      "Epoch 198, Loss: 0.48544224711238787\n",
      "Epoch 199, Loss: 0.48519729219516017\n",
      "Epoch 200, Loss: 0.4849575446659644\n",
      "Epoch 201, Loss: 0.4847146197220469\n",
      "Epoch 202, Loss: 0.48447155759133576\n",
      "Epoch 203, Loss: 0.48422783492589483\n",
      "Epoch 204, Loss: 0.48397707745066637\n",
      "Epoch 205, Loss: 0.483737926529631\n",
      "Epoch 206, Loss: 0.4835020782648923\n",
      "Epoch 207, Loss: 0.48327083252052233\n",
      "Epoch 208, Loss: 0.4830508282762034\n",
      "Epoch 209, Loss: 0.48283269895503794\n",
      "Epoch 210, Loss: 0.48261215534620283\n",
      "Epoch 211, Loss: 0.48240746542225255\n",
      "Epoch 212, Loss: 0.4822187420754367\n",
      "Epoch 213, Loss: 0.48202997575341067\n",
      "Epoch 214, Loss: 0.4818441197783379\n",
      "Epoch 215, Loss: 0.4816502256898402\n",
      "Epoch 216, Loss: 0.4814589616492725\n",
      "Epoch 217, Loss: 0.48127338055379854\n",
      "Epoch 218, Loss: 0.4810898650639108\n",
      "Epoch 219, Loss: 0.48091365405597575\n",
      "Epoch 220, Loss: 0.480720996546175\n",
      "Epoch 221, Loss: 0.48054865040929456\n",
      "Epoch 222, Loss: 0.4803774677186426\n",
      "Epoch 223, Loss: 0.4801885671816001\n",
      "Epoch 224, Loss: 0.48000935557804647\n",
      "Epoch 225, Loss: 0.4798350303004208\n",
      "Epoch 226, Loss: 0.47967842665052907\n",
      "Epoch 227, Loss: 0.47947854451894606\n",
      "Epoch 228, Loss: 0.47928199900429136\n",
      "Epoch 229, Loss: 0.4791424752085709\n",
      "Epoch 230, Loss: 0.4789254904984497\n",
      "Epoch 231, Loss: 0.4787155823499749\n",
      "Epoch 232, Loss: 0.4785186611779873\n",
      "Epoch 233, Loss: 0.4783287773458093\n",
      "Epoch 234, Loss: 0.4781774454711345\n",
      "Epoch 235, Loss: 0.4780410437330356\n",
      "Epoch 236, Loss: 0.4778263811299359\n",
      "Epoch 237, Loss: 0.47765285017795805\n",
      "Epoch 238, Loss: 0.4775436338740249\n",
      "Epoch 239, Loss: 0.4774342833479412\n",
      "Epoch 240, Loss: 0.477263591547843\n",
      "Epoch 241, Loss: 0.47707859610269837\n",
      "Epoch 242, Loss: 0.47697972966255553\n",
      "Epoch 243, Loss: 0.4768431884384324\n",
      "Epoch 244, Loss: 0.4767408349977067\n",
      "Epoch 245, Loss: 0.4765819952057135\n",
      "Epoch 246, Loss: 0.4764706413665339\n",
      "Epoch 247, Loss: 0.47634462160009333\n",
      "Epoch 248, Loss: 0.4762527991635621\n",
      "Epoch 249, Loss: 0.4762014052862575\n",
      "Epoch 250, Loss: 0.4760283306209857\n",
      "Epoch 251, Loss: 0.47592260832103706\n",
      "Epoch 252, Loss: 0.475834869470249\n",
      "Epoch 253, Loss: 0.47570980894919\n",
      "Epoch 254, Loss: 0.47559397414543253\n",
      "Epoch 255, Loss: 0.4755005182088922\n",
      "Epoch 256, Loss: 0.4753894909333468\n",
      "Epoch 257, Loss: 0.47527958208571863\n",
      "Epoch 258, Loss: 0.47517947896910767\n",
      "Epoch 259, Loss: 0.4750690667233416\n",
      "Epoch 260, Loss: 0.47496625253354696\n",
      "Epoch 261, Loss: 0.47486506352442986\n",
      "Epoch 262, Loss: 0.47477116593942054\n",
      "Epoch 263, Loss: 0.474752914637718\n",
      "Epoch 264, Loss: 0.47457458619766507\n",
      "Epoch 265, Loss: 0.47441594770111134\n",
      "Epoch 266, Loss: 0.4743326895520222\n",
      "Epoch 267, Loss: 0.47424442089802893\n",
      "Epoch 268, Loss: 0.47409961152836255\n",
      "Epoch 269, Loss: 0.47397674971911014\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2765713043473756\n",
      "Valence RMSE: 0.2544393822955444\n",
      "Arousal RMSE: 0.29705887213459736\n",
      "Test R^2 score: tensor([0.2154, 0.1262], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1221, 0.0223], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.17082994107694877\n",
      "Num of epochs: 270\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Epoch 121, Loss: 0.5116998508836337\n",
      "Epoch 122, Loss: 0.5111129902664598\n",
      "Epoch 123, Loss: 0.5105405741436312\n",
      "Epoch 124, Loss: 0.5099849010083686\n",
      "Epoch 125, Loss: 0.5094623759868415\n",
      "Epoch 126, Loss: 0.5089652841754122\n",
      "Epoch 127, Loss: 0.5084717799132713\n",
      "Epoch 128, Loss: 0.5079809643002384\n",
      "Epoch 129, Loss: 0.507507731608986\n",
      "Epoch 130, Loss: 0.5070515727009828\n",
      "Epoch 131, Loss: 0.5065962678643066\n",
      "Epoch 132, Loss: 0.5061448812256618\n",
      "Epoch 133, Loss: 0.5057069703510908\n",
      "Epoch 134, Loss: 0.5052756693952567\n",
      "Epoch 135, Loss: 0.5048445017462813\n",
      "Epoch 136, Loss: 0.504425284231979\n",
      "Epoch 137, Loss: 0.504022717877823\n",
      "Epoch 138, Loss: 0.5036312802042249\n",
      "Epoch 139, Loss: 0.5032368731068207\n",
      "Epoch 140, Loss: 0.5028511060044271\n",
      "Epoch 141, Loss: 0.5024804933282331\n",
      "Epoch 142, Loss: 0.5021182727396313\n",
      "Epoch 143, Loss: 0.5017581664990297\n",
      "Epoch 144, Loss: 0.5014050827892457\n",
      "Epoch 145, Loss: 0.501047675902349\n",
      "Epoch 146, Loss: 0.500690966247362\n",
      "Epoch 147, Loss: 0.5003476722262875\n",
      "Epoch 148, Loss: 0.5000068544871652\n",
      "Epoch 149, Loss: 0.49966338867073357\n",
      "Epoch 150, Loss: 0.4993280873087794\n",
      "Epoch 151, Loss: 0.49900257940428383\n",
      "Epoch 152, Loss: 0.4986865554458311\n",
      "Epoch 153, Loss: 0.49836949389755036\n",
      "Epoch 154, Loss: 0.49805375636713634\n",
      "Epoch 155, Loss: 0.49773873165141763\n",
      "Epoch 156, Loss: 0.49742224924400874\n",
      "Epoch 157, Loss: 0.49711425826812494\n",
      "Epoch 158, Loss: 0.4968010973402242\n",
      "Epoch 159, Loss: 0.4964901399315246\n",
      "Epoch 160, Loss: 0.49617675026847124\n",
      "Epoch 161, Loss: 0.4958670390985117\n",
      "Epoch 162, Loss: 0.4955578109301908\n",
      "Epoch 163, Loss: 0.4952510223984696\n",
      "Epoch 164, Loss: 0.49494577484111674\n",
      "Epoch 165, Loss: 0.49464511374066544\n",
      "Epoch 166, Loss: 0.49434383269975535\n",
      "Epoch 167, Loss: 0.49404631909698943\n",
      "Epoch 168, Loss: 0.49374530645143583\n",
      "Epoch 169, Loss: 0.49345064806438055\n",
      "Epoch 170, Loss: 0.49317001490258333\n",
      "Epoch 171, Loss: 0.4928927591216884\n",
      "Epoch 172, Loss: 0.4926138499628376\n",
      "Epoch 173, Loss: 0.49233528219511824\n",
      "Epoch 174, Loss: 0.49205852514110515\n",
      "Epoch 175, Loss: 0.4917773399684819\n",
      "Epoch 176, Loss: 0.49149629710928233\n",
      "Epoch 177, Loss: 0.4912165647150265\n",
      "Epoch 178, Loss: 0.49094181765497674\n",
      "Epoch 179, Loss: 0.49066901222163783\n",
      "Epoch 180, Loss: 0.490393928005003\n",
      "Epoch 181, Loss: 0.4901079113527018\n",
      "Epoch 182, Loss: 0.48981984154671765\n",
      "Epoch 183, Loss: 0.4895234443341763\n",
      "Epoch 184, Loss: 0.48921459258368105\n",
      "Epoch 185, Loss: 0.4889145977883947\n",
      "Epoch 186, Loss: 0.4886175142174919\n",
      "Epoch 187, Loss: 0.4883282295514753\n",
      "Epoch 188, Loss: 0.4880435212302353\n",
      "Epoch 189, Loss: 0.48775699700499114\n",
      "Epoch 190, Loss: 0.48748008613994775\n",
      "Epoch 191, Loss: 0.4872132179171775\n",
      "Epoch 192, Loss: 0.48694738158534456\n",
      "Epoch 193, Loss: 0.48668121634032785\n",
      "Epoch 194, Loss: 0.4864280322301768\n",
      "Epoch 195, Loss: 0.486178532155915\n",
      "Epoch 196, Loss: 0.4859301919172944\n",
      "Epoch 197, Loss: 0.48568442460989747\n",
      "Epoch 198, Loss: 0.48544224711238787\n",
      "Epoch 199, Loss: 0.48519729219516017\n",
      "Epoch 200, Loss: 0.4849575446659644\n",
      "Epoch 201, Loss: 0.4847146197220469\n",
      "Epoch 202, Loss: 0.48447155759133576\n",
      "Epoch 203, Loss: 0.48422783492589483\n",
      "Epoch 204, Loss: 0.48397707745066637\n",
      "Epoch 205, Loss: 0.483737926529631\n",
      "Epoch 206, Loss: 0.4835020782648923\n",
      "Epoch 207, Loss: 0.48327083252052233\n",
      "Epoch 208, Loss: 0.4830508282762034\n",
      "Epoch 209, Loss: 0.48283269895503794\n",
      "Epoch 210, Loss: 0.48261215534620283\n",
      "Epoch 211, Loss: 0.48240746542225255\n",
      "Epoch 212, Loss: 0.4822187420754367\n",
      "Epoch 213, Loss: 0.48202997575341067\n",
      "Epoch 214, Loss: 0.4818441197783379\n",
      "Epoch 215, Loss: 0.4816502256898402\n",
      "Epoch 216, Loss: 0.4814589616492725\n",
      "Epoch 217, Loss: 0.48127338055379854\n",
      "Epoch 218, Loss: 0.4810898650639108\n",
      "Epoch 219, Loss: 0.48091365405597575\n",
      "Epoch 220, Loss: 0.480720996546175\n",
      "Epoch 221, Loss: 0.48054865040929456\n",
      "Epoch 222, Loss: 0.4803774677186426\n",
      "Epoch 223, Loss: 0.4801885671816001\n",
      "Epoch 224, Loss: 0.48000935557804647\n",
      "Epoch 225, Loss: 0.4798350303004208\n",
      "Epoch 226, Loss: 0.47967842665052907\n",
      "Epoch 227, Loss: 0.47947854451894606\n",
      "Epoch 228, Loss: 0.47928199900429136\n",
      "Epoch 229, Loss: 0.4791424752085709\n",
      "Epoch 230, Loss: 0.4789254904984497\n",
      "Epoch 231, Loss: 0.4787155823499749\n",
      "Epoch 232, Loss: 0.4785186611779873\n",
      "Epoch 233, Loss: 0.4783287773458093\n",
      "Epoch 234, Loss: 0.4781774454711345\n",
      "Epoch 235, Loss: 0.4780410437330356\n",
      "Epoch 236, Loss: 0.4778263811299359\n",
      "Epoch 237, Loss: 0.47765285017795805\n",
      "Epoch 238, Loss: 0.4775436338740249\n",
      "Epoch 239, Loss: 0.4774342833479412\n",
      "Epoch 240, Loss: 0.477263591547843\n",
      "Epoch 241, Loss: 0.47707859610269837\n",
      "Epoch 242, Loss: 0.47697972966255553\n",
      "Epoch 243, Loss: 0.4768431884384324\n",
      "Epoch 244, Loss: 0.4767408349977067\n",
      "Epoch 245, Loss: 0.4765819952057135\n",
      "Epoch 246, Loss: 0.4764706413665339\n",
      "Epoch 247, Loss: 0.47634462160009333\n",
      "Epoch 248, Loss: 0.4762527991635621\n",
      "Epoch 249, Loss: 0.4762014052862575\n",
      "Epoch 250, Loss: 0.4760283306209857\n",
      "Epoch 251, Loss: 0.47592260832103706\n",
      "Epoch 252, Loss: 0.475834869470249\n",
      "Epoch 253, Loss: 0.47570980894919\n",
      "Epoch 254, Loss: 0.47559397414543253\n",
      "Epoch 255, Loss: 0.4755005182088922\n",
      "Epoch 256, Loss: 0.4753894909333468\n",
      "Epoch 257, Loss: 0.47527958208571863\n",
      "Epoch 258, Loss: 0.47517947896910767\n",
      "Epoch 259, Loss: 0.4750690667233416\n",
      "Epoch 260, Loss: 0.47496625253354696\n",
      "Epoch 261, Loss: 0.47486506352442986\n",
      "Epoch 262, Loss: 0.47477116593942054\n",
      "Epoch 263, Loss: 0.474752914637718\n",
      "Epoch 264, Loss: 0.47457458619766507\n",
      "Epoch 265, Loss: 0.47441594770111134\n",
      "Epoch 266, Loss: 0.4743326895520222\n",
      "Epoch 267, Loss: 0.47424442089802893\n",
      "Epoch 268, Loss: 0.47409961152836255\n",
      "Epoch 269, Loss: 0.47397674971911014\n",
      "Epoch 270, Loss: 0.4739006464998255\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2767440867289121\n",
      "Valence RMSE: 0.2547762763791199\n",
      "Arousal RMSE: 0.2970919522188871\n",
      "Test R^2 score: tensor([0.2133, 0.1260], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1198, 0.0221], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.16969311705393164\n",
      "Num of epochs: 271\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Epoch 121, Loss: 0.5116998508836337\n",
      "Epoch 122, Loss: 0.5111129902664598\n",
      "Epoch 123, Loss: 0.5105405741436312\n",
      "Epoch 124, Loss: 0.5099849010083686\n",
      "Epoch 125, Loss: 0.5094623759868415\n",
      "Epoch 126, Loss: 0.5089652841754122\n",
      "Epoch 127, Loss: 0.5084717799132713\n",
      "Epoch 128, Loss: 0.5079809643002384\n",
      "Epoch 129, Loss: 0.507507731608986\n",
      "Epoch 130, Loss: 0.5070515727009828\n",
      "Epoch 131, Loss: 0.5065962678643066\n",
      "Epoch 132, Loss: 0.5061448812256618\n",
      "Epoch 133, Loss: 0.5057069703510908\n",
      "Epoch 134, Loss: 0.5052756693952567\n",
      "Epoch 135, Loss: 0.5048445017462813\n",
      "Epoch 136, Loss: 0.504425284231979\n",
      "Epoch 137, Loss: 0.504022717877823\n",
      "Epoch 138, Loss: 0.5036312802042249\n",
      "Epoch 139, Loss: 0.5032368731068207\n",
      "Epoch 140, Loss: 0.5028511060044271\n",
      "Epoch 141, Loss: 0.5024804933282331\n",
      "Epoch 142, Loss: 0.5021182727396313\n",
      "Epoch 143, Loss: 0.5017581664990297\n",
      "Epoch 144, Loss: 0.5014050827892457\n",
      "Epoch 145, Loss: 0.501047675902349\n",
      "Epoch 146, Loss: 0.500690966247362\n",
      "Epoch 147, Loss: 0.5003476722262875\n",
      "Epoch 148, Loss: 0.5000068544871652\n",
      "Epoch 149, Loss: 0.49966338867073357\n",
      "Epoch 150, Loss: 0.4993280873087794\n",
      "Epoch 151, Loss: 0.49900257940428383\n",
      "Epoch 152, Loss: 0.4986865554458311\n",
      "Epoch 153, Loss: 0.49836949389755036\n",
      "Epoch 154, Loss: 0.49805375636713634\n",
      "Epoch 155, Loss: 0.49773873165141763\n",
      "Epoch 156, Loss: 0.49742224924400874\n",
      "Epoch 157, Loss: 0.49711425826812494\n",
      "Epoch 158, Loss: 0.4968010973402242\n",
      "Epoch 159, Loss: 0.4964901399315246\n",
      "Epoch 160, Loss: 0.49617675026847124\n",
      "Epoch 161, Loss: 0.4958670390985117\n",
      "Epoch 162, Loss: 0.4955578109301908\n",
      "Epoch 163, Loss: 0.4952510223984696\n",
      "Epoch 164, Loss: 0.49494577484111674\n",
      "Epoch 165, Loss: 0.49464511374066544\n",
      "Epoch 166, Loss: 0.49434383269975535\n",
      "Epoch 167, Loss: 0.49404631909698943\n",
      "Epoch 168, Loss: 0.49374530645143583\n",
      "Epoch 169, Loss: 0.49345064806438055\n",
      "Epoch 170, Loss: 0.49317001490258333\n",
      "Epoch 171, Loss: 0.4928927591216884\n",
      "Epoch 172, Loss: 0.4926138499628376\n",
      "Epoch 173, Loss: 0.49233528219511824\n",
      "Epoch 174, Loss: 0.49205852514110515\n",
      "Epoch 175, Loss: 0.4917773399684819\n",
      "Epoch 176, Loss: 0.49149629710928233\n",
      "Epoch 177, Loss: 0.4912165647150265\n",
      "Epoch 178, Loss: 0.49094181765497674\n",
      "Epoch 179, Loss: 0.49066901222163783\n",
      "Epoch 180, Loss: 0.490393928005003\n",
      "Epoch 181, Loss: 0.4901079113527018\n",
      "Epoch 182, Loss: 0.48981984154671765\n",
      "Epoch 183, Loss: 0.4895234443341763\n",
      "Epoch 184, Loss: 0.48921459258368105\n",
      "Epoch 185, Loss: 0.4889145977883947\n",
      "Epoch 186, Loss: 0.4886175142174919\n",
      "Epoch 187, Loss: 0.4883282295514753\n",
      "Epoch 188, Loss: 0.4880435212302353\n",
      "Epoch 189, Loss: 0.48775699700499114\n",
      "Epoch 190, Loss: 0.48748008613994775\n",
      "Epoch 191, Loss: 0.4872132179171775\n",
      "Epoch 192, Loss: 0.48694738158534456\n",
      "Epoch 193, Loss: 0.48668121634032785\n",
      "Epoch 194, Loss: 0.4864280322301768\n",
      "Epoch 195, Loss: 0.486178532155915\n",
      "Epoch 196, Loss: 0.4859301919172944\n",
      "Epoch 197, Loss: 0.48568442460989747\n",
      "Epoch 198, Loss: 0.48544224711238787\n",
      "Epoch 199, Loss: 0.48519729219516017\n",
      "Epoch 200, Loss: 0.4849575446659644\n",
      "Epoch 201, Loss: 0.4847146197220469\n",
      "Epoch 202, Loss: 0.48447155759133576\n",
      "Epoch 203, Loss: 0.48422783492589483\n",
      "Epoch 204, Loss: 0.48397707745066637\n",
      "Epoch 205, Loss: 0.483737926529631\n",
      "Epoch 206, Loss: 0.4835020782648923\n",
      "Epoch 207, Loss: 0.48327083252052233\n",
      "Epoch 208, Loss: 0.4830508282762034\n",
      "Epoch 209, Loss: 0.48283269895503794\n",
      "Epoch 210, Loss: 0.48261215534620283\n",
      "Epoch 211, Loss: 0.48240746542225255\n",
      "Epoch 212, Loss: 0.4822187420754367\n",
      "Epoch 213, Loss: 0.48202997575341067\n",
      "Epoch 214, Loss: 0.4818441197783379\n",
      "Epoch 215, Loss: 0.4816502256898402\n",
      "Epoch 216, Loss: 0.4814589616492725\n",
      "Epoch 217, Loss: 0.48127338055379854\n",
      "Epoch 218, Loss: 0.4810898650639108\n",
      "Epoch 219, Loss: 0.48091365405597575\n",
      "Epoch 220, Loss: 0.480720996546175\n",
      "Epoch 221, Loss: 0.48054865040929456\n",
      "Epoch 222, Loss: 0.4803774677186426\n",
      "Epoch 223, Loss: 0.4801885671816001\n",
      "Epoch 224, Loss: 0.48000935557804647\n",
      "Epoch 225, Loss: 0.4798350303004208\n",
      "Epoch 226, Loss: 0.47967842665052907\n",
      "Epoch 227, Loss: 0.47947854451894606\n",
      "Epoch 228, Loss: 0.47928199900429136\n",
      "Epoch 229, Loss: 0.4791424752085709\n",
      "Epoch 230, Loss: 0.4789254904984497\n",
      "Epoch 231, Loss: 0.4787155823499749\n",
      "Epoch 232, Loss: 0.4785186611779873\n",
      "Epoch 233, Loss: 0.4783287773458093\n",
      "Epoch 234, Loss: 0.4781774454711345\n",
      "Epoch 235, Loss: 0.4780410437330356\n",
      "Epoch 236, Loss: 0.4778263811299359\n",
      "Epoch 237, Loss: 0.47765285017795805\n",
      "Epoch 238, Loss: 0.4775436338740249\n",
      "Epoch 239, Loss: 0.4774342833479412\n",
      "Epoch 240, Loss: 0.477263591547843\n",
      "Epoch 241, Loss: 0.47707859610269837\n",
      "Epoch 242, Loss: 0.47697972966255553\n",
      "Epoch 243, Loss: 0.4768431884384324\n",
      "Epoch 244, Loss: 0.4767408349977067\n",
      "Epoch 245, Loss: 0.4765819952057135\n",
      "Epoch 246, Loss: 0.4764706413665339\n",
      "Epoch 247, Loss: 0.47634462160009333\n",
      "Epoch 248, Loss: 0.4762527991635621\n",
      "Epoch 249, Loss: 0.4762014052862575\n",
      "Epoch 250, Loss: 0.4760283306209857\n",
      "Epoch 251, Loss: 0.47592260832103706\n",
      "Epoch 252, Loss: 0.475834869470249\n",
      "Epoch 253, Loss: 0.47570980894919\n",
      "Epoch 254, Loss: 0.47559397414543253\n",
      "Epoch 255, Loss: 0.4755005182088922\n",
      "Epoch 256, Loss: 0.4753894909333468\n",
      "Epoch 257, Loss: 0.47527958208571863\n",
      "Epoch 258, Loss: 0.47517947896910767\n",
      "Epoch 259, Loss: 0.4750690667233416\n",
      "Epoch 260, Loss: 0.47496625253354696\n",
      "Epoch 261, Loss: 0.47486506352442986\n",
      "Epoch 262, Loss: 0.47477116593942054\n",
      "Epoch 263, Loss: 0.474752914637718\n",
      "Epoch 264, Loss: 0.47457458619766507\n",
      "Epoch 265, Loss: 0.47441594770111134\n",
      "Epoch 266, Loss: 0.4743326895520222\n",
      "Epoch 267, Loss: 0.47424442089802893\n",
      "Epoch 268, Loss: 0.47409961152836255\n",
      "Epoch 269, Loss: 0.47397674971911014\n",
      "Epoch 270, Loss: 0.4739006464998255\n",
      "Epoch 271, Loss: 0.4738391701966867\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.27664104403517\n",
      "Valence RMSE: 0.25441287513926913\n",
      "Arousal RMSE: 0.29721141205059465\n",
      "Test R^2 score: tensor([0.2156, 0.1253], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1223, 0.0213], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.17046287960817302\n",
      "Num of epochs: 272\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Epoch 121, Loss: 0.5116998508836337\n",
      "Epoch 122, Loss: 0.5111129902664598\n",
      "Epoch 123, Loss: 0.5105405741436312\n",
      "Epoch 124, Loss: 0.5099849010083686\n",
      "Epoch 125, Loss: 0.5094623759868415\n",
      "Epoch 126, Loss: 0.5089652841754122\n",
      "Epoch 127, Loss: 0.5084717799132713\n",
      "Epoch 128, Loss: 0.5079809643002384\n",
      "Epoch 129, Loss: 0.507507731608986\n",
      "Epoch 130, Loss: 0.5070515727009828\n",
      "Epoch 131, Loss: 0.5065962678643066\n",
      "Epoch 132, Loss: 0.5061448812256618\n",
      "Epoch 133, Loss: 0.5057069703510908\n",
      "Epoch 134, Loss: 0.5052756693952567\n",
      "Epoch 135, Loss: 0.5048445017462813\n",
      "Epoch 136, Loss: 0.504425284231979\n",
      "Epoch 137, Loss: 0.504022717877823\n",
      "Epoch 138, Loss: 0.5036312802042249\n",
      "Epoch 139, Loss: 0.5032368731068207\n",
      "Epoch 140, Loss: 0.5028511060044271\n",
      "Epoch 141, Loss: 0.5024804933282331\n",
      "Epoch 142, Loss: 0.5021182727396313\n",
      "Epoch 143, Loss: 0.5017581664990297\n",
      "Epoch 144, Loss: 0.5014050827892457\n",
      "Epoch 145, Loss: 0.501047675902349\n",
      "Epoch 146, Loss: 0.500690966247362\n",
      "Epoch 147, Loss: 0.5003476722262875\n",
      "Epoch 148, Loss: 0.5000068544871652\n",
      "Epoch 149, Loss: 0.49966338867073357\n",
      "Epoch 150, Loss: 0.4993280873087794\n",
      "Epoch 151, Loss: 0.49900257940428383\n",
      "Epoch 152, Loss: 0.4986865554458311\n",
      "Epoch 153, Loss: 0.49836949389755036\n",
      "Epoch 154, Loss: 0.49805375636713634\n",
      "Epoch 155, Loss: 0.49773873165141763\n",
      "Epoch 156, Loss: 0.49742224924400874\n",
      "Epoch 157, Loss: 0.49711425826812494\n",
      "Epoch 158, Loss: 0.4968010973402242\n",
      "Epoch 159, Loss: 0.4964901399315246\n",
      "Epoch 160, Loss: 0.49617675026847124\n",
      "Epoch 161, Loss: 0.4958670390985117\n",
      "Epoch 162, Loss: 0.4955578109301908\n",
      "Epoch 163, Loss: 0.4952510223984696\n",
      "Epoch 164, Loss: 0.49494577484111674\n",
      "Epoch 165, Loss: 0.49464511374066544\n",
      "Epoch 166, Loss: 0.49434383269975535\n",
      "Epoch 167, Loss: 0.49404631909698943\n",
      "Epoch 168, Loss: 0.49374530645143583\n",
      "Epoch 169, Loss: 0.49345064806438055\n",
      "Epoch 170, Loss: 0.49317001490258333\n",
      "Epoch 171, Loss: 0.4928927591216884\n",
      "Epoch 172, Loss: 0.4926138499628376\n",
      "Epoch 173, Loss: 0.49233528219511824\n",
      "Epoch 174, Loss: 0.49205852514110515\n",
      "Epoch 175, Loss: 0.4917773399684819\n",
      "Epoch 176, Loss: 0.49149629710928233\n",
      "Epoch 177, Loss: 0.4912165647150265\n",
      "Epoch 178, Loss: 0.49094181765497674\n",
      "Epoch 179, Loss: 0.49066901222163783\n",
      "Epoch 180, Loss: 0.490393928005003\n",
      "Epoch 181, Loss: 0.4901079113527018\n",
      "Epoch 182, Loss: 0.48981984154671765\n",
      "Epoch 183, Loss: 0.4895234443341763\n",
      "Epoch 184, Loss: 0.48921459258368105\n",
      "Epoch 185, Loss: 0.4889145977883947\n",
      "Epoch 186, Loss: 0.4886175142174919\n",
      "Epoch 187, Loss: 0.4883282295514753\n",
      "Epoch 188, Loss: 0.4880435212302353\n",
      "Epoch 189, Loss: 0.48775699700499114\n",
      "Epoch 190, Loss: 0.48748008613994775\n",
      "Epoch 191, Loss: 0.4872132179171775\n",
      "Epoch 192, Loss: 0.48694738158534456\n",
      "Epoch 193, Loss: 0.48668121634032785\n",
      "Epoch 194, Loss: 0.4864280322301768\n",
      "Epoch 195, Loss: 0.486178532155915\n",
      "Epoch 196, Loss: 0.4859301919172944\n",
      "Epoch 197, Loss: 0.48568442460989747\n",
      "Epoch 198, Loss: 0.48544224711238787\n",
      "Epoch 199, Loss: 0.48519729219516017\n",
      "Epoch 200, Loss: 0.4849575446659644\n",
      "Epoch 201, Loss: 0.4847146197220469\n",
      "Epoch 202, Loss: 0.48447155759133576\n",
      "Epoch 203, Loss: 0.48422783492589483\n",
      "Epoch 204, Loss: 0.48397707745066637\n",
      "Epoch 205, Loss: 0.483737926529631\n",
      "Epoch 206, Loss: 0.4835020782648923\n",
      "Epoch 207, Loss: 0.48327083252052233\n",
      "Epoch 208, Loss: 0.4830508282762034\n",
      "Epoch 209, Loss: 0.48283269895503794\n",
      "Epoch 210, Loss: 0.48261215534620283\n",
      "Epoch 211, Loss: 0.48240746542225255\n",
      "Epoch 212, Loss: 0.4822187420754367\n",
      "Epoch 213, Loss: 0.48202997575341067\n",
      "Epoch 214, Loss: 0.4818441197783379\n",
      "Epoch 215, Loss: 0.4816502256898402\n",
      "Epoch 216, Loss: 0.4814589616492725\n",
      "Epoch 217, Loss: 0.48127338055379854\n",
      "Epoch 218, Loss: 0.4810898650639108\n",
      "Epoch 219, Loss: 0.48091365405597575\n",
      "Epoch 220, Loss: 0.480720996546175\n",
      "Epoch 221, Loss: 0.48054865040929456\n",
      "Epoch 222, Loss: 0.4803774677186426\n",
      "Epoch 223, Loss: 0.4801885671816001\n",
      "Epoch 224, Loss: 0.48000935557804647\n",
      "Epoch 225, Loss: 0.4798350303004208\n",
      "Epoch 226, Loss: 0.47967842665052907\n",
      "Epoch 227, Loss: 0.47947854451894606\n",
      "Epoch 228, Loss: 0.47928199900429136\n",
      "Epoch 229, Loss: 0.4791424752085709\n",
      "Epoch 230, Loss: 0.4789254904984497\n",
      "Epoch 231, Loss: 0.4787155823499749\n",
      "Epoch 232, Loss: 0.4785186611779873\n",
      "Epoch 233, Loss: 0.4783287773458093\n",
      "Epoch 234, Loss: 0.4781774454711345\n",
      "Epoch 235, Loss: 0.4780410437330356\n",
      "Epoch 236, Loss: 0.4778263811299359\n",
      "Epoch 237, Loss: 0.47765285017795805\n",
      "Epoch 238, Loss: 0.4775436338740249\n",
      "Epoch 239, Loss: 0.4774342833479412\n",
      "Epoch 240, Loss: 0.477263591547843\n",
      "Epoch 241, Loss: 0.47707859610269837\n",
      "Epoch 242, Loss: 0.47697972966255553\n",
      "Epoch 243, Loss: 0.4768431884384324\n",
      "Epoch 244, Loss: 0.4767408349977067\n",
      "Epoch 245, Loss: 0.4765819952057135\n",
      "Epoch 246, Loss: 0.4764706413665339\n",
      "Epoch 247, Loss: 0.47634462160009333\n",
      "Epoch 248, Loss: 0.4762527991635621\n",
      "Epoch 249, Loss: 0.4762014052862575\n",
      "Epoch 250, Loss: 0.4760283306209857\n",
      "Epoch 251, Loss: 0.47592260832103706\n",
      "Epoch 252, Loss: 0.475834869470249\n",
      "Epoch 253, Loss: 0.47570980894919\n",
      "Epoch 254, Loss: 0.47559397414543253\n",
      "Epoch 255, Loss: 0.4755005182088922\n",
      "Epoch 256, Loss: 0.4753894909333468\n",
      "Epoch 257, Loss: 0.47527958208571863\n",
      "Epoch 258, Loss: 0.47517947896910767\n",
      "Epoch 259, Loss: 0.4750690667233416\n",
      "Epoch 260, Loss: 0.47496625253354696\n",
      "Epoch 261, Loss: 0.47486506352442986\n",
      "Epoch 262, Loss: 0.47477116593942054\n",
      "Epoch 263, Loss: 0.474752914637718\n",
      "Epoch 264, Loss: 0.47457458619766507\n",
      "Epoch 265, Loss: 0.47441594770111134\n",
      "Epoch 266, Loss: 0.4743326895520222\n",
      "Epoch 267, Loss: 0.47424442089802893\n",
      "Epoch 268, Loss: 0.47409961152836255\n",
      "Epoch 269, Loss: 0.47397674971911014\n",
      "Epoch 270, Loss: 0.4739006464998255\n",
      "Epoch 271, Loss: 0.4738391701966867\n",
      "Epoch 272, Loss: 0.4737234441817917\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.27666351644255777\n",
      "Valence RMSE: 0.2546592399747482\n",
      "Arousal RMSE: 0.29704220938480524\n",
      "Test R^2 score: tensor([0.2141, 0.1263], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1206, 0.0224], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.1702007161966207\n",
      "Num of epochs: 273\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Epoch 121, Loss: 0.5116998508836337\n",
      "Epoch 122, Loss: 0.5111129902664598\n",
      "Epoch 123, Loss: 0.5105405741436312\n",
      "Epoch 124, Loss: 0.5099849010083686\n",
      "Epoch 125, Loss: 0.5094623759868415\n",
      "Epoch 126, Loss: 0.5089652841754122\n",
      "Epoch 127, Loss: 0.5084717799132713\n",
      "Epoch 128, Loss: 0.5079809643002384\n",
      "Epoch 129, Loss: 0.507507731608986\n",
      "Epoch 130, Loss: 0.5070515727009828\n",
      "Epoch 131, Loss: 0.5065962678643066\n",
      "Epoch 132, Loss: 0.5061448812256618\n",
      "Epoch 133, Loss: 0.5057069703510908\n",
      "Epoch 134, Loss: 0.5052756693952567\n",
      "Epoch 135, Loss: 0.5048445017462813\n",
      "Epoch 136, Loss: 0.504425284231979\n",
      "Epoch 137, Loss: 0.504022717877823\n",
      "Epoch 138, Loss: 0.5036312802042249\n",
      "Epoch 139, Loss: 0.5032368731068207\n",
      "Epoch 140, Loss: 0.5028511060044271\n",
      "Epoch 141, Loss: 0.5024804933282331\n",
      "Epoch 142, Loss: 0.5021182727396313\n",
      "Epoch 143, Loss: 0.5017581664990297\n",
      "Epoch 144, Loss: 0.5014050827892457\n",
      "Epoch 145, Loss: 0.501047675902349\n",
      "Epoch 146, Loss: 0.500690966247362\n",
      "Epoch 147, Loss: 0.5003476722262875\n",
      "Epoch 148, Loss: 0.5000068544871652\n",
      "Epoch 149, Loss: 0.49966338867073357\n",
      "Epoch 150, Loss: 0.4993280873087794\n",
      "Epoch 151, Loss: 0.49900257940428383\n",
      "Epoch 152, Loss: 0.4986865554458311\n",
      "Epoch 153, Loss: 0.49836949389755036\n",
      "Epoch 154, Loss: 0.49805375636713634\n",
      "Epoch 155, Loss: 0.49773873165141763\n",
      "Epoch 156, Loss: 0.49742224924400874\n",
      "Epoch 157, Loss: 0.49711425826812494\n",
      "Epoch 158, Loss: 0.4968010973402242\n",
      "Epoch 159, Loss: 0.4964901399315246\n",
      "Epoch 160, Loss: 0.49617675026847124\n",
      "Epoch 161, Loss: 0.4958670390985117\n",
      "Epoch 162, Loss: 0.4955578109301908\n",
      "Epoch 163, Loss: 0.4952510223984696\n",
      "Epoch 164, Loss: 0.49494577484111674\n",
      "Epoch 165, Loss: 0.49464511374066544\n",
      "Epoch 166, Loss: 0.49434383269975535\n",
      "Epoch 167, Loss: 0.49404631909698943\n",
      "Epoch 168, Loss: 0.49374530645143583\n",
      "Epoch 169, Loss: 0.49345064806438055\n",
      "Epoch 170, Loss: 0.49317001490258333\n",
      "Epoch 171, Loss: 0.4928927591216884\n",
      "Epoch 172, Loss: 0.4926138499628376\n",
      "Epoch 173, Loss: 0.49233528219511824\n",
      "Epoch 174, Loss: 0.49205852514110515\n",
      "Epoch 175, Loss: 0.4917773399684819\n",
      "Epoch 176, Loss: 0.49149629710928233\n",
      "Epoch 177, Loss: 0.4912165647150265\n",
      "Epoch 178, Loss: 0.49094181765497674\n",
      "Epoch 179, Loss: 0.49066901222163783\n",
      "Epoch 180, Loss: 0.490393928005003\n",
      "Epoch 181, Loss: 0.4901079113527018\n",
      "Epoch 182, Loss: 0.48981984154671765\n",
      "Epoch 183, Loss: 0.4895234443341763\n",
      "Epoch 184, Loss: 0.48921459258368105\n",
      "Epoch 185, Loss: 0.4889145977883947\n",
      "Epoch 186, Loss: 0.4886175142174919\n",
      "Epoch 187, Loss: 0.4883282295514753\n",
      "Epoch 188, Loss: 0.4880435212302353\n",
      "Epoch 189, Loss: 0.48775699700499114\n",
      "Epoch 190, Loss: 0.48748008613994775\n",
      "Epoch 191, Loss: 0.4872132179171775\n",
      "Epoch 192, Loss: 0.48694738158534456\n",
      "Epoch 193, Loss: 0.48668121634032785\n",
      "Epoch 194, Loss: 0.4864280322301768\n",
      "Epoch 195, Loss: 0.486178532155915\n",
      "Epoch 196, Loss: 0.4859301919172944\n",
      "Epoch 197, Loss: 0.48568442460989747\n",
      "Epoch 198, Loss: 0.48544224711238787\n",
      "Epoch 199, Loss: 0.48519729219516017\n",
      "Epoch 200, Loss: 0.4849575446659644\n",
      "Epoch 201, Loss: 0.4847146197220469\n",
      "Epoch 202, Loss: 0.48447155759133576\n",
      "Epoch 203, Loss: 0.48422783492589483\n",
      "Epoch 204, Loss: 0.48397707745066637\n",
      "Epoch 205, Loss: 0.483737926529631\n",
      "Epoch 206, Loss: 0.4835020782648923\n",
      "Epoch 207, Loss: 0.48327083252052233\n",
      "Epoch 208, Loss: 0.4830508282762034\n",
      "Epoch 209, Loss: 0.48283269895503794\n",
      "Epoch 210, Loss: 0.48261215534620283\n",
      "Epoch 211, Loss: 0.48240746542225255\n",
      "Epoch 212, Loss: 0.4822187420754367\n",
      "Epoch 213, Loss: 0.48202997575341067\n",
      "Epoch 214, Loss: 0.4818441197783379\n",
      "Epoch 215, Loss: 0.4816502256898402\n",
      "Epoch 216, Loss: 0.4814589616492725\n",
      "Epoch 217, Loss: 0.48127338055379854\n",
      "Epoch 218, Loss: 0.4810898650639108\n",
      "Epoch 219, Loss: 0.48091365405597575\n",
      "Epoch 220, Loss: 0.480720996546175\n",
      "Epoch 221, Loss: 0.48054865040929456\n",
      "Epoch 222, Loss: 0.4803774677186426\n",
      "Epoch 223, Loss: 0.4801885671816001\n",
      "Epoch 224, Loss: 0.48000935557804647\n",
      "Epoch 225, Loss: 0.4798350303004208\n",
      "Epoch 226, Loss: 0.47967842665052907\n",
      "Epoch 227, Loss: 0.47947854451894606\n",
      "Epoch 228, Loss: 0.47928199900429136\n",
      "Epoch 229, Loss: 0.4791424752085709\n",
      "Epoch 230, Loss: 0.4789254904984497\n",
      "Epoch 231, Loss: 0.4787155823499749\n",
      "Epoch 232, Loss: 0.4785186611779873\n",
      "Epoch 233, Loss: 0.4783287773458093\n",
      "Epoch 234, Loss: 0.4781774454711345\n",
      "Epoch 235, Loss: 0.4780410437330356\n",
      "Epoch 236, Loss: 0.4778263811299359\n",
      "Epoch 237, Loss: 0.47765285017795805\n",
      "Epoch 238, Loss: 0.4775436338740249\n",
      "Epoch 239, Loss: 0.4774342833479412\n",
      "Epoch 240, Loss: 0.477263591547843\n",
      "Epoch 241, Loss: 0.47707859610269837\n",
      "Epoch 242, Loss: 0.47697972966255553\n",
      "Epoch 243, Loss: 0.4768431884384324\n",
      "Epoch 244, Loss: 0.4767408349977067\n",
      "Epoch 245, Loss: 0.4765819952057135\n",
      "Epoch 246, Loss: 0.4764706413665339\n",
      "Epoch 247, Loss: 0.47634462160009333\n",
      "Epoch 248, Loss: 0.4762527991635621\n",
      "Epoch 249, Loss: 0.4762014052862575\n",
      "Epoch 250, Loss: 0.4760283306209857\n",
      "Epoch 251, Loss: 0.47592260832103706\n",
      "Epoch 252, Loss: 0.475834869470249\n",
      "Epoch 253, Loss: 0.47570980894919\n",
      "Epoch 254, Loss: 0.47559397414543253\n",
      "Epoch 255, Loss: 0.4755005182088922\n",
      "Epoch 256, Loss: 0.4753894909333468\n",
      "Epoch 257, Loss: 0.47527958208571863\n",
      "Epoch 258, Loss: 0.47517947896910767\n",
      "Epoch 259, Loss: 0.4750690667233416\n",
      "Epoch 260, Loss: 0.47496625253354696\n",
      "Epoch 261, Loss: 0.47486506352442986\n",
      "Epoch 262, Loss: 0.47477116593942054\n",
      "Epoch 263, Loss: 0.474752914637718\n",
      "Epoch 264, Loss: 0.47457458619766507\n",
      "Epoch 265, Loss: 0.47441594770111134\n",
      "Epoch 266, Loss: 0.4743326895520222\n",
      "Epoch 267, Loss: 0.47424442089802893\n",
      "Epoch 268, Loss: 0.47409961152836255\n",
      "Epoch 269, Loss: 0.47397674971911014\n",
      "Epoch 270, Loss: 0.4739006464998255\n",
      "Epoch 271, Loss: 0.4738391701966867\n",
      "Epoch 272, Loss: 0.4737234441817917\n",
      "Epoch 273, Loss: 0.47358564948211956\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2766220708957417\n",
      "Valence RMSE: 0.2545377510283435\n",
      "Arousal RMSE: 0.2970691392836531\n",
      "Test R^2 score: tensor([0.2148, 0.1262], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1214, 0.0222], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.17049635742184538\n",
      "Num of epochs: 274\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Epoch 121, Loss: 0.5116998508836337\n",
      "Epoch 122, Loss: 0.5111129902664598\n",
      "Epoch 123, Loss: 0.5105405741436312\n",
      "Epoch 124, Loss: 0.5099849010083686\n",
      "Epoch 125, Loss: 0.5094623759868415\n",
      "Epoch 126, Loss: 0.5089652841754122\n",
      "Epoch 127, Loss: 0.5084717799132713\n",
      "Epoch 128, Loss: 0.5079809643002384\n",
      "Epoch 129, Loss: 0.507507731608986\n",
      "Epoch 130, Loss: 0.5070515727009828\n",
      "Epoch 131, Loss: 0.5065962678643066\n",
      "Epoch 132, Loss: 0.5061448812256618\n",
      "Epoch 133, Loss: 0.5057069703510908\n",
      "Epoch 134, Loss: 0.5052756693952567\n",
      "Epoch 135, Loss: 0.5048445017462813\n",
      "Epoch 136, Loss: 0.504425284231979\n",
      "Epoch 137, Loss: 0.504022717877823\n",
      "Epoch 138, Loss: 0.5036312802042249\n",
      "Epoch 139, Loss: 0.5032368731068207\n",
      "Epoch 140, Loss: 0.5028511060044271\n",
      "Epoch 141, Loss: 0.5024804933282331\n",
      "Epoch 142, Loss: 0.5021182727396313\n",
      "Epoch 143, Loss: 0.5017581664990297\n",
      "Epoch 144, Loss: 0.5014050827892457\n",
      "Epoch 145, Loss: 0.501047675902349\n",
      "Epoch 146, Loss: 0.500690966247362\n",
      "Epoch 147, Loss: 0.5003476722262875\n",
      "Epoch 148, Loss: 0.5000068544871652\n",
      "Epoch 149, Loss: 0.49966338867073357\n",
      "Epoch 150, Loss: 0.4993280873087794\n",
      "Epoch 151, Loss: 0.49900257940428383\n",
      "Epoch 152, Loss: 0.4986865554458311\n",
      "Epoch 153, Loss: 0.49836949389755036\n",
      "Epoch 154, Loss: 0.49805375636713634\n",
      "Epoch 155, Loss: 0.49773873165141763\n",
      "Epoch 156, Loss: 0.49742224924400874\n",
      "Epoch 157, Loss: 0.49711425826812494\n",
      "Epoch 158, Loss: 0.4968010973402242\n",
      "Epoch 159, Loss: 0.4964901399315246\n",
      "Epoch 160, Loss: 0.49617675026847124\n",
      "Epoch 161, Loss: 0.4958670390985117\n",
      "Epoch 162, Loss: 0.4955578109301908\n",
      "Epoch 163, Loss: 0.4952510223984696\n",
      "Epoch 164, Loss: 0.49494577484111674\n",
      "Epoch 165, Loss: 0.49464511374066544\n",
      "Epoch 166, Loss: 0.49434383269975535\n",
      "Epoch 167, Loss: 0.49404631909698943\n",
      "Epoch 168, Loss: 0.49374530645143583\n",
      "Epoch 169, Loss: 0.49345064806438055\n",
      "Epoch 170, Loss: 0.49317001490258333\n",
      "Epoch 171, Loss: 0.4928927591216884\n",
      "Epoch 172, Loss: 0.4926138499628376\n",
      "Epoch 173, Loss: 0.49233528219511824\n",
      "Epoch 174, Loss: 0.49205852514110515\n",
      "Epoch 175, Loss: 0.4917773399684819\n",
      "Epoch 176, Loss: 0.49149629710928233\n",
      "Epoch 177, Loss: 0.4912165647150265\n",
      "Epoch 178, Loss: 0.49094181765497674\n",
      "Epoch 179, Loss: 0.49066901222163783\n",
      "Epoch 180, Loss: 0.490393928005003\n",
      "Epoch 181, Loss: 0.4901079113527018\n",
      "Epoch 182, Loss: 0.48981984154671765\n",
      "Epoch 183, Loss: 0.4895234443341763\n",
      "Epoch 184, Loss: 0.48921459258368105\n",
      "Epoch 185, Loss: 0.4889145977883947\n",
      "Epoch 186, Loss: 0.4886175142174919\n",
      "Epoch 187, Loss: 0.4883282295514753\n",
      "Epoch 188, Loss: 0.4880435212302353\n",
      "Epoch 189, Loss: 0.48775699700499114\n",
      "Epoch 190, Loss: 0.48748008613994775\n",
      "Epoch 191, Loss: 0.4872132179171775\n",
      "Epoch 192, Loss: 0.48694738158534456\n",
      "Epoch 193, Loss: 0.48668121634032785\n",
      "Epoch 194, Loss: 0.4864280322301768\n",
      "Epoch 195, Loss: 0.486178532155915\n",
      "Epoch 196, Loss: 0.4859301919172944\n",
      "Epoch 197, Loss: 0.48568442460989747\n",
      "Epoch 198, Loss: 0.48544224711238787\n",
      "Epoch 199, Loss: 0.48519729219516017\n",
      "Epoch 200, Loss: 0.4849575446659644\n",
      "Epoch 201, Loss: 0.4847146197220469\n",
      "Epoch 202, Loss: 0.48447155759133576\n",
      "Epoch 203, Loss: 0.48422783492589483\n",
      "Epoch 204, Loss: 0.48397707745066637\n",
      "Epoch 205, Loss: 0.483737926529631\n",
      "Epoch 206, Loss: 0.4835020782648923\n",
      "Epoch 207, Loss: 0.48327083252052233\n",
      "Epoch 208, Loss: 0.4830508282762034\n",
      "Epoch 209, Loss: 0.48283269895503794\n",
      "Epoch 210, Loss: 0.48261215534620283\n",
      "Epoch 211, Loss: 0.48240746542225255\n",
      "Epoch 212, Loss: 0.4822187420754367\n",
      "Epoch 213, Loss: 0.48202997575341067\n",
      "Epoch 214, Loss: 0.4818441197783379\n",
      "Epoch 215, Loss: 0.4816502256898402\n",
      "Epoch 216, Loss: 0.4814589616492725\n",
      "Epoch 217, Loss: 0.48127338055379854\n",
      "Epoch 218, Loss: 0.4810898650639108\n",
      "Epoch 219, Loss: 0.48091365405597575\n",
      "Epoch 220, Loss: 0.480720996546175\n",
      "Epoch 221, Loss: 0.48054865040929456\n",
      "Epoch 222, Loss: 0.4803774677186426\n",
      "Epoch 223, Loss: 0.4801885671816001\n",
      "Epoch 224, Loss: 0.48000935557804647\n",
      "Epoch 225, Loss: 0.4798350303004208\n",
      "Epoch 226, Loss: 0.47967842665052907\n",
      "Epoch 227, Loss: 0.47947854451894606\n",
      "Epoch 228, Loss: 0.47928199900429136\n",
      "Epoch 229, Loss: 0.4791424752085709\n",
      "Epoch 230, Loss: 0.4789254904984497\n",
      "Epoch 231, Loss: 0.4787155823499749\n",
      "Epoch 232, Loss: 0.4785186611779873\n",
      "Epoch 233, Loss: 0.4783287773458093\n",
      "Epoch 234, Loss: 0.4781774454711345\n",
      "Epoch 235, Loss: 0.4780410437330356\n",
      "Epoch 236, Loss: 0.4778263811299359\n",
      "Epoch 237, Loss: 0.47765285017795805\n",
      "Epoch 238, Loss: 0.4775436338740249\n",
      "Epoch 239, Loss: 0.4774342833479412\n",
      "Epoch 240, Loss: 0.477263591547843\n",
      "Epoch 241, Loss: 0.47707859610269837\n",
      "Epoch 242, Loss: 0.47697972966255553\n",
      "Epoch 243, Loss: 0.4768431884384324\n",
      "Epoch 244, Loss: 0.4767408349977067\n",
      "Epoch 245, Loss: 0.4765819952057135\n",
      "Epoch 246, Loss: 0.4764706413665339\n",
      "Epoch 247, Loss: 0.47634462160009333\n",
      "Epoch 248, Loss: 0.4762527991635621\n",
      "Epoch 249, Loss: 0.4762014052862575\n",
      "Epoch 250, Loss: 0.4760283306209857\n",
      "Epoch 251, Loss: 0.47592260832103706\n",
      "Epoch 252, Loss: 0.475834869470249\n",
      "Epoch 253, Loss: 0.47570980894919\n",
      "Epoch 254, Loss: 0.47559397414543253\n",
      "Epoch 255, Loss: 0.4755005182088922\n",
      "Epoch 256, Loss: 0.4753894909333468\n",
      "Epoch 257, Loss: 0.47527958208571863\n",
      "Epoch 258, Loss: 0.47517947896910767\n",
      "Epoch 259, Loss: 0.4750690667233416\n",
      "Epoch 260, Loss: 0.47496625253354696\n",
      "Epoch 261, Loss: 0.47486506352442986\n",
      "Epoch 262, Loss: 0.47477116593942054\n",
      "Epoch 263, Loss: 0.474752914637718\n",
      "Epoch 264, Loss: 0.47457458619766507\n",
      "Epoch 265, Loss: 0.47441594770111134\n",
      "Epoch 266, Loss: 0.4743326895520222\n",
      "Epoch 267, Loss: 0.47424442089802893\n",
      "Epoch 268, Loss: 0.47409961152836255\n",
      "Epoch 269, Loss: 0.47397674971911014\n",
      "Epoch 270, Loss: 0.4739006464998255\n",
      "Epoch 271, Loss: 0.4738391701966867\n",
      "Epoch 272, Loss: 0.4737234441817917\n",
      "Epoch 273, Loss: 0.47358564948211956\n",
      "Epoch 274, Loss: 0.47351060057458727\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2768955487378001\n",
      "Valence RMSE: 0.25470729474603143\n",
      "Arousal RMSE: 0.2974331585831901\n",
      "Test R^2 score: tensor([0.2138, 0.1240], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1202, 0.0198], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.16890177178643934\n",
      "Num of epochs: 275\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Epoch 121, Loss: 0.5116998508836337\n",
      "Epoch 122, Loss: 0.5111129902664598\n",
      "Epoch 123, Loss: 0.5105405741436312\n",
      "Epoch 124, Loss: 0.5099849010083686\n",
      "Epoch 125, Loss: 0.5094623759868415\n",
      "Epoch 126, Loss: 0.5089652841754122\n",
      "Epoch 127, Loss: 0.5084717799132713\n",
      "Epoch 128, Loss: 0.5079809643002384\n",
      "Epoch 129, Loss: 0.507507731608986\n",
      "Epoch 130, Loss: 0.5070515727009828\n",
      "Epoch 131, Loss: 0.5065962678643066\n",
      "Epoch 132, Loss: 0.5061448812256618\n",
      "Epoch 133, Loss: 0.5057069703510908\n",
      "Epoch 134, Loss: 0.5052756693952567\n",
      "Epoch 135, Loss: 0.5048445017462813\n",
      "Epoch 136, Loss: 0.504425284231979\n",
      "Epoch 137, Loss: 0.504022717877823\n",
      "Epoch 138, Loss: 0.5036312802042249\n",
      "Epoch 139, Loss: 0.5032368731068207\n",
      "Epoch 140, Loss: 0.5028511060044271\n",
      "Epoch 141, Loss: 0.5024804933282331\n",
      "Epoch 142, Loss: 0.5021182727396313\n",
      "Epoch 143, Loss: 0.5017581664990297\n",
      "Epoch 144, Loss: 0.5014050827892457\n",
      "Epoch 145, Loss: 0.501047675902349\n",
      "Epoch 146, Loss: 0.500690966247362\n",
      "Epoch 147, Loss: 0.5003476722262875\n",
      "Epoch 148, Loss: 0.5000068544871652\n",
      "Epoch 149, Loss: 0.49966338867073357\n",
      "Epoch 150, Loss: 0.4993280873087794\n",
      "Epoch 151, Loss: 0.49900257940428383\n",
      "Epoch 152, Loss: 0.4986865554458311\n",
      "Epoch 153, Loss: 0.49836949389755036\n",
      "Epoch 154, Loss: 0.49805375636713634\n",
      "Epoch 155, Loss: 0.49773873165141763\n",
      "Epoch 156, Loss: 0.49742224924400874\n",
      "Epoch 157, Loss: 0.49711425826812494\n",
      "Epoch 158, Loss: 0.4968010973402242\n",
      "Epoch 159, Loss: 0.4964901399315246\n",
      "Epoch 160, Loss: 0.49617675026847124\n",
      "Epoch 161, Loss: 0.4958670390985117\n",
      "Epoch 162, Loss: 0.4955578109301908\n",
      "Epoch 163, Loss: 0.4952510223984696\n",
      "Epoch 164, Loss: 0.49494577484111674\n",
      "Epoch 165, Loss: 0.49464511374066544\n",
      "Epoch 166, Loss: 0.49434383269975535\n",
      "Epoch 167, Loss: 0.49404631909698943\n",
      "Epoch 168, Loss: 0.49374530645143583\n",
      "Epoch 169, Loss: 0.49345064806438055\n",
      "Epoch 170, Loss: 0.49317001490258333\n",
      "Epoch 171, Loss: 0.4928927591216884\n",
      "Epoch 172, Loss: 0.4926138499628376\n",
      "Epoch 173, Loss: 0.49233528219511824\n",
      "Epoch 174, Loss: 0.49205852514110515\n",
      "Epoch 175, Loss: 0.4917773399684819\n",
      "Epoch 176, Loss: 0.49149629710928233\n",
      "Epoch 177, Loss: 0.4912165647150265\n",
      "Epoch 178, Loss: 0.49094181765497674\n",
      "Epoch 179, Loss: 0.49066901222163783\n",
      "Epoch 180, Loss: 0.490393928005003\n",
      "Epoch 181, Loss: 0.4901079113527018\n",
      "Epoch 182, Loss: 0.48981984154671765\n",
      "Epoch 183, Loss: 0.4895234443341763\n",
      "Epoch 184, Loss: 0.48921459258368105\n",
      "Epoch 185, Loss: 0.4889145977883947\n",
      "Epoch 186, Loss: 0.4886175142174919\n",
      "Epoch 187, Loss: 0.4883282295514753\n",
      "Epoch 188, Loss: 0.4880435212302353\n",
      "Epoch 189, Loss: 0.48775699700499114\n",
      "Epoch 190, Loss: 0.48748008613994775\n",
      "Epoch 191, Loss: 0.4872132179171775\n",
      "Epoch 192, Loss: 0.48694738158534456\n",
      "Epoch 193, Loss: 0.48668121634032785\n",
      "Epoch 194, Loss: 0.4864280322301768\n",
      "Epoch 195, Loss: 0.486178532155915\n",
      "Epoch 196, Loss: 0.4859301919172944\n",
      "Epoch 197, Loss: 0.48568442460989747\n",
      "Epoch 198, Loss: 0.48544224711238787\n",
      "Epoch 199, Loss: 0.48519729219516017\n",
      "Epoch 200, Loss: 0.4849575446659644\n",
      "Epoch 201, Loss: 0.4847146197220469\n",
      "Epoch 202, Loss: 0.48447155759133576\n",
      "Epoch 203, Loss: 0.48422783492589483\n",
      "Epoch 204, Loss: 0.48397707745066637\n",
      "Epoch 205, Loss: 0.483737926529631\n",
      "Epoch 206, Loss: 0.4835020782648923\n",
      "Epoch 207, Loss: 0.48327083252052233\n",
      "Epoch 208, Loss: 0.4830508282762034\n",
      "Epoch 209, Loss: 0.48283269895503794\n",
      "Epoch 210, Loss: 0.48261215534620283\n",
      "Epoch 211, Loss: 0.48240746542225255\n",
      "Epoch 212, Loss: 0.4822187420754367\n",
      "Epoch 213, Loss: 0.48202997575341067\n",
      "Epoch 214, Loss: 0.4818441197783379\n",
      "Epoch 215, Loss: 0.4816502256898402\n",
      "Epoch 216, Loss: 0.4814589616492725\n",
      "Epoch 217, Loss: 0.48127338055379854\n",
      "Epoch 218, Loss: 0.4810898650639108\n",
      "Epoch 219, Loss: 0.48091365405597575\n",
      "Epoch 220, Loss: 0.480720996546175\n",
      "Epoch 221, Loss: 0.48054865040929456\n",
      "Epoch 222, Loss: 0.4803774677186426\n",
      "Epoch 223, Loss: 0.4801885671816001\n",
      "Epoch 224, Loss: 0.48000935557804647\n",
      "Epoch 225, Loss: 0.4798350303004208\n",
      "Epoch 226, Loss: 0.47967842665052907\n",
      "Epoch 227, Loss: 0.47947854451894606\n",
      "Epoch 228, Loss: 0.47928199900429136\n",
      "Epoch 229, Loss: 0.4791424752085709\n",
      "Epoch 230, Loss: 0.4789254904984497\n",
      "Epoch 231, Loss: 0.4787155823499749\n",
      "Epoch 232, Loss: 0.4785186611779873\n",
      "Epoch 233, Loss: 0.4783287773458093\n",
      "Epoch 234, Loss: 0.4781774454711345\n",
      "Epoch 235, Loss: 0.4780410437330356\n",
      "Epoch 236, Loss: 0.4778263811299359\n",
      "Epoch 237, Loss: 0.47765285017795805\n",
      "Epoch 238, Loss: 0.4775436338740249\n",
      "Epoch 239, Loss: 0.4774342833479412\n",
      "Epoch 240, Loss: 0.477263591547843\n",
      "Epoch 241, Loss: 0.47707859610269837\n",
      "Epoch 242, Loss: 0.47697972966255553\n",
      "Epoch 243, Loss: 0.4768431884384324\n",
      "Epoch 244, Loss: 0.4767408349977067\n",
      "Epoch 245, Loss: 0.4765819952057135\n",
      "Epoch 246, Loss: 0.4764706413665339\n",
      "Epoch 247, Loss: 0.47634462160009333\n",
      "Epoch 248, Loss: 0.4762527991635621\n",
      "Epoch 249, Loss: 0.4762014052862575\n",
      "Epoch 250, Loss: 0.4760283306209857\n",
      "Epoch 251, Loss: 0.47592260832103706\n",
      "Epoch 252, Loss: 0.475834869470249\n",
      "Epoch 253, Loss: 0.47570980894919\n",
      "Epoch 254, Loss: 0.47559397414543253\n",
      "Epoch 255, Loss: 0.4755005182088922\n",
      "Epoch 256, Loss: 0.4753894909333468\n",
      "Epoch 257, Loss: 0.47527958208571863\n",
      "Epoch 258, Loss: 0.47517947896910767\n",
      "Epoch 259, Loss: 0.4750690667233416\n",
      "Epoch 260, Loss: 0.47496625253354696\n",
      "Epoch 261, Loss: 0.47486506352442986\n",
      "Epoch 262, Loss: 0.47477116593942054\n",
      "Epoch 263, Loss: 0.474752914637718\n",
      "Epoch 264, Loss: 0.47457458619766507\n",
      "Epoch 265, Loss: 0.47441594770111134\n",
      "Epoch 266, Loss: 0.4743326895520222\n",
      "Epoch 267, Loss: 0.47424442089802893\n",
      "Epoch 268, Loss: 0.47409961152836255\n",
      "Epoch 269, Loss: 0.47397674971911014\n",
      "Epoch 270, Loss: 0.4739006464998255\n",
      "Epoch 271, Loss: 0.4738391701966867\n",
      "Epoch 272, Loss: 0.4737234441817917\n",
      "Epoch 273, Loss: 0.47358564948211956\n",
      "Epoch 274, Loss: 0.47351060057458727\n",
      "Epoch 275, Loss: 0.4734065036258047\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2768664871024171\n",
      "Valence RMSE: 0.2545317955032499\n",
      "Arousal RMSE: 0.29752927324678086\n",
      "Test R^2 score: tensor([0.2149, 0.1235], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1214, 0.0192], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.16916020375912488\n",
      "Num of epochs: 276\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Epoch 121, Loss: 0.5116998508836337\n",
      "Epoch 122, Loss: 0.5111129902664598\n",
      "Epoch 123, Loss: 0.5105405741436312\n",
      "Epoch 124, Loss: 0.5099849010083686\n",
      "Epoch 125, Loss: 0.5094623759868415\n",
      "Epoch 126, Loss: 0.5089652841754122\n",
      "Epoch 127, Loss: 0.5084717799132713\n",
      "Epoch 128, Loss: 0.5079809643002384\n",
      "Epoch 129, Loss: 0.507507731608986\n",
      "Epoch 130, Loss: 0.5070515727009828\n",
      "Epoch 131, Loss: 0.5065962678643066\n",
      "Epoch 132, Loss: 0.5061448812256618\n",
      "Epoch 133, Loss: 0.5057069703510908\n",
      "Epoch 134, Loss: 0.5052756693952567\n",
      "Epoch 135, Loss: 0.5048445017462813\n",
      "Epoch 136, Loss: 0.504425284231979\n",
      "Epoch 137, Loss: 0.504022717877823\n",
      "Epoch 138, Loss: 0.5036312802042249\n",
      "Epoch 139, Loss: 0.5032368731068207\n",
      "Epoch 140, Loss: 0.5028511060044271\n",
      "Epoch 141, Loss: 0.5024804933282331\n",
      "Epoch 142, Loss: 0.5021182727396313\n",
      "Epoch 143, Loss: 0.5017581664990297\n",
      "Epoch 144, Loss: 0.5014050827892457\n",
      "Epoch 145, Loss: 0.501047675902349\n",
      "Epoch 146, Loss: 0.500690966247362\n",
      "Epoch 147, Loss: 0.5003476722262875\n",
      "Epoch 148, Loss: 0.5000068544871652\n",
      "Epoch 149, Loss: 0.49966338867073357\n",
      "Epoch 150, Loss: 0.4993280873087794\n",
      "Epoch 151, Loss: 0.49900257940428383\n",
      "Epoch 152, Loss: 0.4986865554458311\n",
      "Epoch 153, Loss: 0.49836949389755036\n",
      "Epoch 154, Loss: 0.49805375636713634\n",
      "Epoch 155, Loss: 0.49773873165141763\n",
      "Epoch 156, Loss: 0.49742224924400874\n",
      "Epoch 157, Loss: 0.49711425826812494\n",
      "Epoch 158, Loss: 0.4968010973402242\n",
      "Epoch 159, Loss: 0.4964901399315246\n",
      "Epoch 160, Loss: 0.49617675026847124\n",
      "Epoch 161, Loss: 0.4958670390985117\n",
      "Epoch 162, Loss: 0.4955578109301908\n",
      "Epoch 163, Loss: 0.4952510223984696\n",
      "Epoch 164, Loss: 0.49494577484111674\n",
      "Epoch 165, Loss: 0.49464511374066544\n",
      "Epoch 166, Loss: 0.49434383269975535\n",
      "Epoch 167, Loss: 0.49404631909698943\n",
      "Epoch 168, Loss: 0.49374530645143583\n",
      "Epoch 169, Loss: 0.49345064806438055\n",
      "Epoch 170, Loss: 0.49317001490258333\n",
      "Epoch 171, Loss: 0.4928927591216884\n",
      "Epoch 172, Loss: 0.4926138499628376\n",
      "Epoch 173, Loss: 0.49233528219511824\n",
      "Epoch 174, Loss: 0.49205852514110515\n",
      "Epoch 175, Loss: 0.4917773399684819\n",
      "Epoch 176, Loss: 0.49149629710928233\n",
      "Epoch 177, Loss: 0.4912165647150265\n",
      "Epoch 178, Loss: 0.49094181765497674\n",
      "Epoch 179, Loss: 0.49066901222163783\n",
      "Epoch 180, Loss: 0.490393928005003\n",
      "Epoch 181, Loss: 0.4901079113527018\n",
      "Epoch 182, Loss: 0.48981984154671765\n",
      "Epoch 183, Loss: 0.4895234443341763\n",
      "Epoch 184, Loss: 0.48921459258368105\n",
      "Epoch 185, Loss: 0.4889145977883947\n",
      "Epoch 186, Loss: 0.4886175142174919\n",
      "Epoch 187, Loss: 0.4883282295514753\n",
      "Epoch 188, Loss: 0.4880435212302353\n",
      "Epoch 189, Loss: 0.48775699700499114\n",
      "Epoch 190, Loss: 0.48748008613994775\n",
      "Epoch 191, Loss: 0.4872132179171775\n",
      "Epoch 192, Loss: 0.48694738158534456\n",
      "Epoch 193, Loss: 0.48668121634032785\n",
      "Epoch 194, Loss: 0.4864280322301768\n",
      "Epoch 195, Loss: 0.486178532155915\n",
      "Epoch 196, Loss: 0.4859301919172944\n",
      "Epoch 197, Loss: 0.48568442460989747\n",
      "Epoch 198, Loss: 0.48544224711238787\n",
      "Epoch 199, Loss: 0.48519729219516017\n",
      "Epoch 200, Loss: 0.4849575446659644\n",
      "Epoch 201, Loss: 0.4847146197220469\n",
      "Epoch 202, Loss: 0.48447155759133576\n",
      "Epoch 203, Loss: 0.48422783492589483\n",
      "Epoch 204, Loss: 0.48397707745066637\n",
      "Epoch 205, Loss: 0.483737926529631\n",
      "Epoch 206, Loss: 0.4835020782648923\n",
      "Epoch 207, Loss: 0.48327083252052233\n",
      "Epoch 208, Loss: 0.4830508282762034\n",
      "Epoch 209, Loss: 0.48283269895503794\n",
      "Epoch 210, Loss: 0.48261215534620283\n",
      "Epoch 211, Loss: 0.48240746542225255\n",
      "Epoch 212, Loss: 0.4822187420754367\n",
      "Epoch 213, Loss: 0.48202997575341067\n",
      "Epoch 214, Loss: 0.4818441197783379\n",
      "Epoch 215, Loss: 0.4816502256898402\n",
      "Epoch 216, Loss: 0.4814589616492725\n",
      "Epoch 217, Loss: 0.48127338055379854\n",
      "Epoch 218, Loss: 0.4810898650639108\n",
      "Epoch 219, Loss: 0.48091365405597575\n",
      "Epoch 220, Loss: 0.480720996546175\n",
      "Epoch 221, Loss: 0.48054865040929456\n",
      "Epoch 222, Loss: 0.4803774677186426\n",
      "Epoch 223, Loss: 0.4801885671816001\n",
      "Epoch 224, Loss: 0.48000935557804647\n",
      "Epoch 225, Loss: 0.4798350303004208\n",
      "Epoch 226, Loss: 0.47967842665052907\n",
      "Epoch 227, Loss: 0.47947854451894606\n",
      "Epoch 228, Loss: 0.47928199900429136\n",
      "Epoch 229, Loss: 0.4791424752085709\n",
      "Epoch 230, Loss: 0.4789254904984497\n",
      "Epoch 231, Loss: 0.4787155823499749\n",
      "Epoch 232, Loss: 0.4785186611779873\n",
      "Epoch 233, Loss: 0.4783287773458093\n",
      "Epoch 234, Loss: 0.4781774454711345\n",
      "Epoch 235, Loss: 0.4780410437330356\n",
      "Epoch 236, Loss: 0.4778263811299359\n",
      "Epoch 237, Loss: 0.47765285017795805\n",
      "Epoch 238, Loss: 0.4775436338740249\n",
      "Epoch 239, Loss: 0.4774342833479412\n",
      "Epoch 240, Loss: 0.477263591547843\n",
      "Epoch 241, Loss: 0.47707859610269837\n",
      "Epoch 242, Loss: 0.47697972966255553\n",
      "Epoch 243, Loss: 0.4768431884384324\n",
      "Epoch 244, Loss: 0.4767408349977067\n",
      "Epoch 245, Loss: 0.4765819952057135\n",
      "Epoch 246, Loss: 0.4764706413665339\n",
      "Epoch 247, Loss: 0.47634462160009333\n",
      "Epoch 248, Loss: 0.4762527991635621\n",
      "Epoch 249, Loss: 0.4762014052862575\n",
      "Epoch 250, Loss: 0.4760283306209857\n",
      "Epoch 251, Loss: 0.47592260832103706\n",
      "Epoch 252, Loss: 0.475834869470249\n",
      "Epoch 253, Loss: 0.47570980894919\n",
      "Epoch 254, Loss: 0.47559397414543253\n",
      "Epoch 255, Loss: 0.4755005182088922\n",
      "Epoch 256, Loss: 0.4753894909333468\n",
      "Epoch 257, Loss: 0.47527958208571863\n",
      "Epoch 258, Loss: 0.47517947896910767\n",
      "Epoch 259, Loss: 0.4750690667233416\n",
      "Epoch 260, Loss: 0.47496625253354696\n",
      "Epoch 261, Loss: 0.47486506352442986\n",
      "Epoch 262, Loss: 0.47477116593942054\n",
      "Epoch 263, Loss: 0.474752914637718\n",
      "Epoch 264, Loss: 0.47457458619766507\n",
      "Epoch 265, Loss: 0.47441594770111134\n",
      "Epoch 266, Loss: 0.4743326895520222\n",
      "Epoch 267, Loss: 0.47424442089802893\n",
      "Epoch 268, Loss: 0.47409961152836255\n",
      "Epoch 269, Loss: 0.47397674971911014\n",
      "Epoch 270, Loss: 0.4739006464998255\n",
      "Epoch 271, Loss: 0.4738391701966867\n",
      "Epoch 272, Loss: 0.4737234441817917\n",
      "Epoch 273, Loss: 0.47358564948211956\n",
      "Epoch 274, Loss: 0.47351060057458727\n",
      "Epoch 275, Loss: 0.4734065036258047\n",
      "Epoch 276, Loss: 0.4733269559263822\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2767532691017015\n",
      "Valence RMSE: 0.2544701288293749\n",
      "Arousal RMSE: 0.2973713124202113\n",
      "Test R^2 score: tensor([0.2152, 0.1244], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1219, 0.0202], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.16981563737551897\n",
      "Num of epochs: 277\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Epoch 121, Loss: 0.5116998508836337\n",
      "Epoch 122, Loss: 0.5111129902664598\n",
      "Epoch 123, Loss: 0.5105405741436312\n",
      "Epoch 124, Loss: 0.5099849010083686\n",
      "Epoch 125, Loss: 0.5094623759868415\n",
      "Epoch 126, Loss: 0.5089652841754122\n",
      "Epoch 127, Loss: 0.5084717799132713\n",
      "Epoch 128, Loss: 0.5079809643002384\n",
      "Epoch 129, Loss: 0.507507731608986\n",
      "Epoch 130, Loss: 0.5070515727009828\n",
      "Epoch 131, Loss: 0.5065962678643066\n",
      "Epoch 132, Loss: 0.5061448812256618\n",
      "Epoch 133, Loss: 0.5057069703510908\n",
      "Epoch 134, Loss: 0.5052756693952567\n",
      "Epoch 135, Loss: 0.5048445017462813\n",
      "Epoch 136, Loss: 0.504425284231979\n",
      "Epoch 137, Loss: 0.504022717877823\n",
      "Epoch 138, Loss: 0.5036312802042249\n",
      "Epoch 139, Loss: 0.5032368731068207\n",
      "Epoch 140, Loss: 0.5028511060044271\n",
      "Epoch 141, Loss: 0.5024804933282331\n",
      "Epoch 142, Loss: 0.5021182727396313\n",
      "Epoch 143, Loss: 0.5017581664990297\n",
      "Epoch 144, Loss: 0.5014050827892457\n",
      "Epoch 145, Loss: 0.501047675902349\n",
      "Epoch 146, Loss: 0.500690966247362\n",
      "Epoch 147, Loss: 0.5003476722262875\n",
      "Epoch 148, Loss: 0.5000068544871652\n",
      "Epoch 149, Loss: 0.49966338867073357\n",
      "Epoch 150, Loss: 0.4993280873087794\n",
      "Epoch 151, Loss: 0.49900257940428383\n",
      "Epoch 152, Loss: 0.4986865554458311\n",
      "Epoch 153, Loss: 0.49836949389755036\n",
      "Epoch 154, Loss: 0.49805375636713634\n",
      "Epoch 155, Loss: 0.49773873165141763\n",
      "Epoch 156, Loss: 0.49742224924400874\n",
      "Epoch 157, Loss: 0.49711425826812494\n",
      "Epoch 158, Loss: 0.4968010973402242\n",
      "Epoch 159, Loss: 0.4964901399315246\n",
      "Epoch 160, Loss: 0.49617675026847124\n",
      "Epoch 161, Loss: 0.4958670390985117\n",
      "Epoch 162, Loss: 0.4955578109301908\n",
      "Epoch 163, Loss: 0.4952510223984696\n",
      "Epoch 164, Loss: 0.49494577484111674\n",
      "Epoch 165, Loss: 0.49464511374066544\n",
      "Epoch 166, Loss: 0.49434383269975535\n",
      "Epoch 167, Loss: 0.49404631909698943\n",
      "Epoch 168, Loss: 0.49374530645143583\n",
      "Epoch 169, Loss: 0.49345064806438055\n",
      "Epoch 170, Loss: 0.49317001490258333\n",
      "Epoch 171, Loss: 0.4928927591216884\n",
      "Epoch 172, Loss: 0.4926138499628376\n",
      "Epoch 173, Loss: 0.49233528219511824\n",
      "Epoch 174, Loss: 0.49205852514110515\n",
      "Epoch 175, Loss: 0.4917773399684819\n",
      "Epoch 176, Loss: 0.49149629710928233\n",
      "Epoch 177, Loss: 0.4912165647150265\n",
      "Epoch 178, Loss: 0.49094181765497674\n",
      "Epoch 179, Loss: 0.49066901222163783\n",
      "Epoch 180, Loss: 0.490393928005003\n",
      "Epoch 181, Loss: 0.4901079113527018\n",
      "Epoch 182, Loss: 0.48981984154671765\n",
      "Epoch 183, Loss: 0.4895234443341763\n",
      "Epoch 184, Loss: 0.48921459258368105\n",
      "Epoch 185, Loss: 0.4889145977883947\n",
      "Epoch 186, Loss: 0.4886175142174919\n",
      "Epoch 187, Loss: 0.4883282295514753\n",
      "Epoch 188, Loss: 0.4880435212302353\n",
      "Epoch 189, Loss: 0.48775699700499114\n",
      "Epoch 190, Loss: 0.48748008613994775\n",
      "Epoch 191, Loss: 0.4872132179171775\n",
      "Epoch 192, Loss: 0.48694738158534456\n",
      "Epoch 193, Loss: 0.48668121634032785\n",
      "Epoch 194, Loss: 0.4864280322301768\n",
      "Epoch 195, Loss: 0.486178532155915\n",
      "Epoch 196, Loss: 0.4859301919172944\n",
      "Epoch 197, Loss: 0.48568442460989747\n",
      "Epoch 198, Loss: 0.48544224711238787\n",
      "Epoch 199, Loss: 0.48519729219516017\n",
      "Epoch 200, Loss: 0.4849575446659644\n",
      "Epoch 201, Loss: 0.4847146197220469\n",
      "Epoch 202, Loss: 0.48447155759133576\n",
      "Epoch 203, Loss: 0.48422783492589483\n",
      "Epoch 204, Loss: 0.48397707745066637\n",
      "Epoch 205, Loss: 0.483737926529631\n",
      "Epoch 206, Loss: 0.4835020782648923\n",
      "Epoch 207, Loss: 0.48327083252052233\n",
      "Epoch 208, Loss: 0.4830508282762034\n",
      "Epoch 209, Loss: 0.48283269895503794\n",
      "Epoch 210, Loss: 0.48261215534620283\n",
      "Epoch 211, Loss: 0.48240746542225255\n",
      "Epoch 212, Loss: 0.4822187420754367\n",
      "Epoch 213, Loss: 0.48202997575341067\n",
      "Epoch 214, Loss: 0.4818441197783379\n",
      "Epoch 215, Loss: 0.4816502256898402\n",
      "Epoch 216, Loss: 0.4814589616492725\n",
      "Epoch 217, Loss: 0.48127338055379854\n",
      "Epoch 218, Loss: 0.4810898650639108\n",
      "Epoch 219, Loss: 0.48091365405597575\n",
      "Epoch 220, Loss: 0.480720996546175\n",
      "Epoch 221, Loss: 0.48054865040929456\n",
      "Epoch 222, Loss: 0.4803774677186426\n",
      "Epoch 223, Loss: 0.4801885671816001\n",
      "Epoch 224, Loss: 0.48000935557804647\n",
      "Epoch 225, Loss: 0.4798350303004208\n",
      "Epoch 226, Loss: 0.47967842665052907\n",
      "Epoch 227, Loss: 0.47947854451894606\n",
      "Epoch 228, Loss: 0.47928199900429136\n",
      "Epoch 229, Loss: 0.4791424752085709\n",
      "Epoch 230, Loss: 0.4789254904984497\n",
      "Epoch 231, Loss: 0.4787155823499749\n",
      "Epoch 232, Loss: 0.4785186611779873\n",
      "Epoch 233, Loss: 0.4783287773458093\n",
      "Epoch 234, Loss: 0.4781774454711345\n",
      "Epoch 235, Loss: 0.4780410437330356\n",
      "Epoch 236, Loss: 0.4778263811299359\n",
      "Epoch 237, Loss: 0.47765285017795805\n",
      "Epoch 238, Loss: 0.4775436338740249\n",
      "Epoch 239, Loss: 0.4774342833479412\n",
      "Epoch 240, Loss: 0.477263591547843\n",
      "Epoch 241, Loss: 0.47707859610269837\n",
      "Epoch 242, Loss: 0.47697972966255553\n",
      "Epoch 243, Loss: 0.4768431884384324\n",
      "Epoch 244, Loss: 0.4767408349977067\n",
      "Epoch 245, Loss: 0.4765819952057135\n",
      "Epoch 246, Loss: 0.4764706413665339\n",
      "Epoch 247, Loss: 0.47634462160009333\n",
      "Epoch 248, Loss: 0.4762527991635621\n",
      "Epoch 249, Loss: 0.4762014052862575\n",
      "Epoch 250, Loss: 0.4760283306209857\n",
      "Epoch 251, Loss: 0.47592260832103706\n",
      "Epoch 252, Loss: 0.475834869470249\n",
      "Epoch 253, Loss: 0.47570980894919\n",
      "Epoch 254, Loss: 0.47559397414543253\n",
      "Epoch 255, Loss: 0.4755005182088922\n",
      "Epoch 256, Loss: 0.4753894909333468\n",
      "Epoch 257, Loss: 0.47527958208571863\n",
      "Epoch 258, Loss: 0.47517947896910767\n",
      "Epoch 259, Loss: 0.4750690667233416\n",
      "Epoch 260, Loss: 0.47496625253354696\n",
      "Epoch 261, Loss: 0.47486506352442986\n",
      "Epoch 262, Loss: 0.47477116593942054\n",
      "Epoch 263, Loss: 0.474752914637718\n",
      "Epoch 264, Loss: 0.47457458619766507\n",
      "Epoch 265, Loss: 0.47441594770111134\n",
      "Epoch 266, Loss: 0.4743326895520222\n",
      "Epoch 267, Loss: 0.47424442089802893\n",
      "Epoch 268, Loss: 0.47409961152836255\n",
      "Epoch 269, Loss: 0.47397674971911014\n",
      "Epoch 270, Loss: 0.4739006464998255\n",
      "Epoch 271, Loss: 0.4738391701966867\n",
      "Epoch 272, Loss: 0.4737234441817917\n",
      "Epoch 273, Loss: 0.47358564948211956\n",
      "Epoch 274, Loss: 0.47351060057458727\n",
      "Epoch 275, Loss: 0.4734065036258047\n",
      "Epoch 276, Loss: 0.4733269559263822\n",
      "Epoch 277, Loss: 0.4732341071370639\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2768970422924543\n",
      "Valence RMSE: 0.25460494371417186\n",
      "Arousal RMSE: 0.2975235565411954\n",
      "Test R^2 score: tensor([0.2144, 0.1235], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1209, 0.0192], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.16895137473809924\n",
      "Num of epochs: 278\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Epoch 121, Loss: 0.5116998508836337\n",
      "Epoch 122, Loss: 0.5111129902664598\n",
      "Epoch 123, Loss: 0.5105405741436312\n",
      "Epoch 124, Loss: 0.5099849010083686\n",
      "Epoch 125, Loss: 0.5094623759868415\n",
      "Epoch 126, Loss: 0.5089652841754122\n",
      "Epoch 127, Loss: 0.5084717799132713\n",
      "Epoch 128, Loss: 0.5079809643002384\n",
      "Epoch 129, Loss: 0.507507731608986\n",
      "Epoch 130, Loss: 0.5070515727009828\n",
      "Epoch 131, Loss: 0.5065962678643066\n",
      "Epoch 132, Loss: 0.5061448812256618\n",
      "Epoch 133, Loss: 0.5057069703510908\n",
      "Epoch 134, Loss: 0.5052756693952567\n",
      "Epoch 135, Loss: 0.5048445017462813\n",
      "Epoch 136, Loss: 0.504425284231979\n",
      "Epoch 137, Loss: 0.504022717877823\n",
      "Epoch 138, Loss: 0.5036312802042249\n",
      "Epoch 139, Loss: 0.5032368731068207\n",
      "Epoch 140, Loss: 0.5028511060044271\n",
      "Epoch 141, Loss: 0.5024804933282331\n",
      "Epoch 142, Loss: 0.5021182727396313\n",
      "Epoch 143, Loss: 0.5017581664990297\n",
      "Epoch 144, Loss: 0.5014050827892457\n",
      "Epoch 145, Loss: 0.501047675902349\n",
      "Epoch 146, Loss: 0.500690966247362\n",
      "Epoch 147, Loss: 0.5003476722262875\n",
      "Epoch 148, Loss: 0.5000068544871652\n",
      "Epoch 149, Loss: 0.49966338867073357\n",
      "Epoch 150, Loss: 0.4993280873087794\n",
      "Epoch 151, Loss: 0.49900257940428383\n",
      "Epoch 152, Loss: 0.4986865554458311\n",
      "Epoch 153, Loss: 0.49836949389755036\n",
      "Epoch 154, Loss: 0.49805375636713634\n",
      "Epoch 155, Loss: 0.49773873165141763\n",
      "Epoch 156, Loss: 0.49742224924400874\n",
      "Epoch 157, Loss: 0.49711425826812494\n",
      "Epoch 158, Loss: 0.4968010973402242\n",
      "Epoch 159, Loss: 0.4964901399315246\n",
      "Epoch 160, Loss: 0.49617675026847124\n",
      "Epoch 161, Loss: 0.4958670390985117\n",
      "Epoch 162, Loss: 0.4955578109301908\n",
      "Epoch 163, Loss: 0.4952510223984696\n",
      "Epoch 164, Loss: 0.49494577484111674\n",
      "Epoch 165, Loss: 0.49464511374066544\n",
      "Epoch 166, Loss: 0.49434383269975535\n",
      "Epoch 167, Loss: 0.49404631909698943\n",
      "Epoch 168, Loss: 0.49374530645143583\n",
      "Epoch 169, Loss: 0.49345064806438055\n",
      "Epoch 170, Loss: 0.49317001490258333\n",
      "Epoch 171, Loss: 0.4928927591216884\n",
      "Epoch 172, Loss: 0.4926138499628376\n",
      "Epoch 173, Loss: 0.49233528219511824\n",
      "Epoch 174, Loss: 0.49205852514110515\n",
      "Epoch 175, Loss: 0.4917773399684819\n",
      "Epoch 176, Loss: 0.49149629710928233\n",
      "Epoch 177, Loss: 0.4912165647150265\n",
      "Epoch 178, Loss: 0.49094181765497674\n",
      "Epoch 179, Loss: 0.49066901222163783\n",
      "Epoch 180, Loss: 0.490393928005003\n",
      "Epoch 181, Loss: 0.4901079113527018\n",
      "Epoch 182, Loss: 0.48981984154671765\n",
      "Epoch 183, Loss: 0.4895234443341763\n",
      "Epoch 184, Loss: 0.48921459258368105\n",
      "Epoch 185, Loss: 0.4889145977883947\n",
      "Epoch 186, Loss: 0.4886175142174919\n",
      "Epoch 187, Loss: 0.4883282295514753\n",
      "Epoch 188, Loss: 0.4880435212302353\n",
      "Epoch 189, Loss: 0.48775699700499114\n",
      "Epoch 190, Loss: 0.48748008613994775\n",
      "Epoch 191, Loss: 0.4872132179171775\n",
      "Epoch 192, Loss: 0.48694738158534456\n",
      "Epoch 193, Loss: 0.48668121634032785\n",
      "Epoch 194, Loss: 0.4864280322301768\n",
      "Epoch 195, Loss: 0.486178532155915\n",
      "Epoch 196, Loss: 0.4859301919172944\n",
      "Epoch 197, Loss: 0.48568442460989747\n",
      "Epoch 198, Loss: 0.48544224711238787\n",
      "Epoch 199, Loss: 0.48519729219516017\n",
      "Epoch 200, Loss: 0.4849575446659644\n",
      "Epoch 201, Loss: 0.4847146197220469\n",
      "Epoch 202, Loss: 0.48447155759133576\n",
      "Epoch 203, Loss: 0.48422783492589483\n",
      "Epoch 204, Loss: 0.48397707745066637\n",
      "Epoch 205, Loss: 0.483737926529631\n",
      "Epoch 206, Loss: 0.4835020782648923\n",
      "Epoch 207, Loss: 0.48327083252052233\n",
      "Epoch 208, Loss: 0.4830508282762034\n",
      "Epoch 209, Loss: 0.48283269895503794\n",
      "Epoch 210, Loss: 0.48261215534620283\n",
      "Epoch 211, Loss: 0.48240746542225255\n",
      "Epoch 212, Loss: 0.4822187420754367\n",
      "Epoch 213, Loss: 0.48202997575341067\n",
      "Epoch 214, Loss: 0.4818441197783379\n",
      "Epoch 215, Loss: 0.4816502256898402\n",
      "Epoch 216, Loss: 0.4814589616492725\n",
      "Epoch 217, Loss: 0.48127338055379854\n",
      "Epoch 218, Loss: 0.4810898650639108\n",
      "Epoch 219, Loss: 0.48091365405597575\n",
      "Epoch 220, Loss: 0.480720996546175\n",
      "Epoch 221, Loss: 0.48054865040929456\n",
      "Epoch 222, Loss: 0.4803774677186426\n",
      "Epoch 223, Loss: 0.4801885671816001\n",
      "Epoch 224, Loss: 0.48000935557804647\n",
      "Epoch 225, Loss: 0.4798350303004208\n",
      "Epoch 226, Loss: 0.47967842665052907\n",
      "Epoch 227, Loss: 0.47947854451894606\n",
      "Epoch 228, Loss: 0.47928199900429136\n",
      "Epoch 229, Loss: 0.4791424752085709\n",
      "Epoch 230, Loss: 0.4789254904984497\n",
      "Epoch 231, Loss: 0.4787155823499749\n",
      "Epoch 232, Loss: 0.4785186611779873\n",
      "Epoch 233, Loss: 0.4783287773458093\n",
      "Epoch 234, Loss: 0.4781774454711345\n",
      "Epoch 235, Loss: 0.4780410437330356\n",
      "Epoch 236, Loss: 0.4778263811299359\n",
      "Epoch 237, Loss: 0.47765285017795805\n",
      "Epoch 238, Loss: 0.4775436338740249\n",
      "Epoch 239, Loss: 0.4774342833479412\n",
      "Epoch 240, Loss: 0.477263591547843\n",
      "Epoch 241, Loss: 0.47707859610269837\n",
      "Epoch 242, Loss: 0.47697972966255553\n",
      "Epoch 243, Loss: 0.4768431884384324\n",
      "Epoch 244, Loss: 0.4767408349977067\n",
      "Epoch 245, Loss: 0.4765819952057135\n",
      "Epoch 246, Loss: 0.4764706413665339\n",
      "Epoch 247, Loss: 0.47634462160009333\n",
      "Epoch 248, Loss: 0.4762527991635621\n",
      "Epoch 249, Loss: 0.4762014052862575\n",
      "Epoch 250, Loss: 0.4760283306209857\n",
      "Epoch 251, Loss: 0.47592260832103706\n",
      "Epoch 252, Loss: 0.475834869470249\n",
      "Epoch 253, Loss: 0.47570980894919\n",
      "Epoch 254, Loss: 0.47559397414543253\n",
      "Epoch 255, Loss: 0.4755005182088922\n",
      "Epoch 256, Loss: 0.4753894909333468\n",
      "Epoch 257, Loss: 0.47527958208571863\n",
      "Epoch 258, Loss: 0.47517947896910767\n",
      "Epoch 259, Loss: 0.4750690667233416\n",
      "Epoch 260, Loss: 0.47496625253354696\n",
      "Epoch 261, Loss: 0.47486506352442986\n",
      "Epoch 262, Loss: 0.47477116593942054\n",
      "Epoch 263, Loss: 0.474752914637718\n",
      "Epoch 264, Loss: 0.47457458619766507\n",
      "Epoch 265, Loss: 0.47441594770111134\n",
      "Epoch 266, Loss: 0.4743326895520222\n",
      "Epoch 267, Loss: 0.47424442089802893\n",
      "Epoch 268, Loss: 0.47409961152836255\n",
      "Epoch 269, Loss: 0.47397674971911014\n",
      "Epoch 270, Loss: 0.4739006464998255\n",
      "Epoch 271, Loss: 0.4738391701966867\n",
      "Epoch 272, Loss: 0.4737234441817917\n",
      "Epoch 273, Loss: 0.47358564948211956\n",
      "Epoch 274, Loss: 0.47351060057458727\n",
      "Epoch 275, Loss: 0.4734065036258047\n",
      "Epoch 276, Loss: 0.4733269559263822\n",
      "Epoch 277, Loss: 0.4732341071370639\n",
      "Epoch 278, Loss: 0.4731407519682655\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2770640415269241\n",
      "Valence RMSE: 0.25453402816926163\n",
      "Arousal RMSE: 0.2978949390614009\n",
      "Test R^2 score: tensor([0.2148, 0.1213], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1214, 0.0168], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.16807538870725214\n",
      "Num of epochs: 279\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Epoch 121, Loss: 0.5116998508836337\n",
      "Epoch 122, Loss: 0.5111129902664598\n",
      "Epoch 123, Loss: 0.5105405741436312\n",
      "Epoch 124, Loss: 0.5099849010083686\n",
      "Epoch 125, Loss: 0.5094623759868415\n",
      "Epoch 126, Loss: 0.5089652841754122\n",
      "Epoch 127, Loss: 0.5084717799132713\n",
      "Epoch 128, Loss: 0.5079809643002384\n",
      "Epoch 129, Loss: 0.507507731608986\n",
      "Epoch 130, Loss: 0.5070515727009828\n",
      "Epoch 131, Loss: 0.5065962678643066\n",
      "Epoch 132, Loss: 0.5061448812256618\n",
      "Epoch 133, Loss: 0.5057069703510908\n",
      "Epoch 134, Loss: 0.5052756693952567\n",
      "Epoch 135, Loss: 0.5048445017462813\n",
      "Epoch 136, Loss: 0.504425284231979\n",
      "Epoch 137, Loss: 0.504022717877823\n",
      "Epoch 138, Loss: 0.5036312802042249\n",
      "Epoch 139, Loss: 0.5032368731068207\n",
      "Epoch 140, Loss: 0.5028511060044271\n",
      "Epoch 141, Loss: 0.5024804933282331\n",
      "Epoch 142, Loss: 0.5021182727396313\n",
      "Epoch 143, Loss: 0.5017581664990297\n",
      "Epoch 144, Loss: 0.5014050827892457\n",
      "Epoch 145, Loss: 0.501047675902349\n",
      "Epoch 146, Loss: 0.500690966247362\n",
      "Epoch 147, Loss: 0.5003476722262875\n",
      "Epoch 148, Loss: 0.5000068544871652\n",
      "Epoch 149, Loss: 0.49966338867073357\n",
      "Epoch 150, Loss: 0.4993280873087794\n",
      "Epoch 151, Loss: 0.49900257940428383\n",
      "Epoch 152, Loss: 0.4986865554458311\n",
      "Epoch 153, Loss: 0.49836949389755036\n",
      "Epoch 154, Loss: 0.49805375636713634\n",
      "Epoch 155, Loss: 0.49773873165141763\n",
      "Epoch 156, Loss: 0.49742224924400874\n",
      "Epoch 157, Loss: 0.49711425826812494\n",
      "Epoch 158, Loss: 0.4968010973402242\n",
      "Epoch 159, Loss: 0.4964901399315246\n",
      "Epoch 160, Loss: 0.49617675026847124\n",
      "Epoch 161, Loss: 0.4958670390985117\n",
      "Epoch 162, Loss: 0.4955578109301908\n",
      "Epoch 163, Loss: 0.4952510223984696\n",
      "Epoch 164, Loss: 0.49494577484111674\n",
      "Epoch 165, Loss: 0.49464511374066544\n",
      "Epoch 166, Loss: 0.49434383269975535\n",
      "Epoch 167, Loss: 0.49404631909698943\n",
      "Epoch 168, Loss: 0.49374530645143583\n",
      "Epoch 169, Loss: 0.49345064806438055\n",
      "Epoch 170, Loss: 0.49317001490258333\n",
      "Epoch 171, Loss: 0.4928927591216884\n",
      "Epoch 172, Loss: 0.4926138499628376\n",
      "Epoch 173, Loss: 0.49233528219511824\n",
      "Epoch 174, Loss: 0.49205852514110515\n",
      "Epoch 175, Loss: 0.4917773399684819\n",
      "Epoch 176, Loss: 0.49149629710928233\n",
      "Epoch 177, Loss: 0.4912165647150265\n",
      "Epoch 178, Loss: 0.49094181765497674\n",
      "Epoch 179, Loss: 0.49066901222163783\n",
      "Epoch 180, Loss: 0.490393928005003\n",
      "Epoch 181, Loss: 0.4901079113527018\n",
      "Epoch 182, Loss: 0.48981984154671765\n",
      "Epoch 183, Loss: 0.4895234443341763\n",
      "Epoch 184, Loss: 0.48921459258368105\n",
      "Epoch 185, Loss: 0.4889145977883947\n",
      "Epoch 186, Loss: 0.4886175142174919\n",
      "Epoch 187, Loss: 0.4883282295514753\n",
      "Epoch 188, Loss: 0.4880435212302353\n",
      "Epoch 189, Loss: 0.48775699700499114\n",
      "Epoch 190, Loss: 0.48748008613994775\n",
      "Epoch 191, Loss: 0.4872132179171775\n",
      "Epoch 192, Loss: 0.48694738158534456\n",
      "Epoch 193, Loss: 0.48668121634032785\n",
      "Epoch 194, Loss: 0.4864280322301768\n",
      "Epoch 195, Loss: 0.486178532155915\n",
      "Epoch 196, Loss: 0.4859301919172944\n",
      "Epoch 197, Loss: 0.48568442460989747\n",
      "Epoch 198, Loss: 0.48544224711238787\n",
      "Epoch 199, Loss: 0.48519729219516017\n",
      "Epoch 200, Loss: 0.4849575446659644\n",
      "Epoch 201, Loss: 0.4847146197220469\n",
      "Epoch 202, Loss: 0.48447155759133576\n",
      "Epoch 203, Loss: 0.48422783492589483\n",
      "Epoch 204, Loss: 0.48397707745066637\n",
      "Epoch 205, Loss: 0.483737926529631\n",
      "Epoch 206, Loss: 0.4835020782648923\n",
      "Epoch 207, Loss: 0.48327083252052233\n",
      "Epoch 208, Loss: 0.4830508282762034\n",
      "Epoch 209, Loss: 0.48283269895503794\n",
      "Epoch 210, Loss: 0.48261215534620283\n",
      "Epoch 211, Loss: 0.48240746542225255\n",
      "Epoch 212, Loss: 0.4822187420754367\n",
      "Epoch 213, Loss: 0.48202997575341067\n",
      "Epoch 214, Loss: 0.4818441197783379\n",
      "Epoch 215, Loss: 0.4816502256898402\n",
      "Epoch 216, Loss: 0.4814589616492725\n",
      "Epoch 217, Loss: 0.48127338055379854\n",
      "Epoch 218, Loss: 0.4810898650639108\n",
      "Epoch 219, Loss: 0.48091365405597575\n",
      "Epoch 220, Loss: 0.480720996546175\n",
      "Epoch 221, Loss: 0.48054865040929456\n",
      "Epoch 222, Loss: 0.4803774677186426\n",
      "Epoch 223, Loss: 0.4801885671816001\n",
      "Epoch 224, Loss: 0.48000935557804647\n",
      "Epoch 225, Loss: 0.4798350303004208\n",
      "Epoch 226, Loss: 0.47967842665052907\n",
      "Epoch 227, Loss: 0.47947854451894606\n",
      "Epoch 228, Loss: 0.47928199900429136\n",
      "Epoch 229, Loss: 0.4791424752085709\n",
      "Epoch 230, Loss: 0.4789254904984497\n",
      "Epoch 231, Loss: 0.4787155823499749\n",
      "Epoch 232, Loss: 0.4785186611779873\n",
      "Epoch 233, Loss: 0.4783287773458093\n",
      "Epoch 234, Loss: 0.4781774454711345\n",
      "Epoch 235, Loss: 0.4780410437330356\n",
      "Epoch 236, Loss: 0.4778263811299359\n",
      "Epoch 237, Loss: 0.47765285017795805\n",
      "Epoch 238, Loss: 0.4775436338740249\n",
      "Epoch 239, Loss: 0.4774342833479412\n",
      "Epoch 240, Loss: 0.477263591547843\n",
      "Epoch 241, Loss: 0.47707859610269837\n",
      "Epoch 242, Loss: 0.47697972966255553\n",
      "Epoch 243, Loss: 0.4768431884384324\n",
      "Epoch 244, Loss: 0.4767408349977067\n",
      "Epoch 245, Loss: 0.4765819952057135\n",
      "Epoch 246, Loss: 0.4764706413665339\n",
      "Epoch 247, Loss: 0.47634462160009333\n",
      "Epoch 248, Loss: 0.4762527991635621\n",
      "Epoch 249, Loss: 0.4762014052862575\n",
      "Epoch 250, Loss: 0.4760283306209857\n",
      "Epoch 251, Loss: 0.47592260832103706\n",
      "Epoch 252, Loss: 0.475834869470249\n",
      "Epoch 253, Loss: 0.47570980894919\n",
      "Epoch 254, Loss: 0.47559397414543253\n",
      "Epoch 255, Loss: 0.4755005182088922\n",
      "Epoch 256, Loss: 0.4753894909333468\n",
      "Epoch 257, Loss: 0.47527958208571863\n",
      "Epoch 258, Loss: 0.47517947896910767\n",
      "Epoch 259, Loss: 0.4750690667233416\n",
      "Epoch 260, Loss: 0.47496625253354696\n",
      "Epoch 261, Loss: 0.47486506352442986\n",
      "Epoch 262, Loss: 0.47477116593942054\n",
      "Epoch 263, Loss: 0.474752914637718\n",
      "Epoch 264, Loss: 0.47457458619766507\n",
      "Epoch 265, Loss: 0.47441594770111134\n",
      "Epoch 266, Loss: 0.4743326895520222\n",
      "Epoch 267, Loss: 0.47424442089802893\n",
      "Epoch 268, Loss: 0.47409961152836255\n",
      "Epoch 269, Loss: 0.47397674971911014\n",
      "Epoch 270, Loss: 0.4739006464998255\n",
      "Epoch 271, Loss: 0.4738391701966867\n",
      "Epoch 272, Loss: 0.4737234441817917\n",
      "Epoch 273, Loss: 0.47358564948211956\n",
      "Epoch 274, Loss: 0.47351060057458727\n",
      "Epoch 275, Loss: 0.4734065036258047\n",
      "Epoch 276, Loss: 0.4733269559263822\n",
      "Epoch 277, Loss: 0.4732341071370639\n",
      "Epoch 278, Loss: 0.4731407519682655\n",
      "Epoch 279, Loss: 0.4730939966069229\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.27739582775693206\n",
      "Valence RMSE: 0.25512098208153877\n",
      "Arousal RMSE: 0.29801036058442365\n",
      "Test R^2 score: tensor([0.2112, 0.1206], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1174, 0.0160], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.1659222059710071\n",
      "Num of epochs: 280\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Epoch 121, Loss: 0.5116998508836337\n",
      "Epoch 122, Loss: 0.5111129902664598\n",
      "Epoch 123, Loss: 0.5105405741436312\n",
      "Epoch 124, Loss: 0.5099849010083686\n",
      "Epoch 125, Loss: 0.5094623759868415\n",
      "Epoch 126, Loss: 0.5089652841754122\n",
      "Epoch 127, Loss: 0.5084717799132713\n",
      "Epoch 128, Loss: 0.5079809643002384\n",
      "Epoch 129, Loss: 0.507507731608986\n",
      "Epoch 130, Loss: 0.5070515727009828\n",
      "Epoch 131, Loss: 0.5065962678643066\n",
      "Epoch 132, Loss: 0.5061448812256618\n",
      "Epoch 133, Loss: 0.5057069703510908\n",
      "Epoch 134, Loss: 0.5052756693952567\n",
      "Epoch 135, Loss: 0.5048445017462813\n",
      "Epoch 136, Loss: 0.504425284231979\n",
      "Epoch 137, Loss: 0.504022717877823\n",
      "Epoch 138, Loss: 0.5036312802042249\n",
      "Epoch 139, Loss: 0.5032368731068207\n",
      "Epoch 140, Loss: 0.5028511060044271\n",
      "Epoch 141, Loss: 0.5024804933282331\n",
      "Epoch 142, Loss: 0.5021182727396313\n",
      "Epoch 143, Loss: 0.5017581664990297\n",
      "Epoch 144, Loss: 0.5014050827892457\n",
      "Epoch 145, Loss: 0.501047675902349\n",
      "Epoch 146, Loss: 0.500690966247362\n",
      "Epoch 147, Loss: 0.5003476722262875\n",
      "Epoch 148, Loss: 0.5000068544871652\n",
      "Epoch 149, Loss: 0.49966338867073357\n",
      "Epoch 150, Loss: 0.4993280873087794\n",
      "Epoch 151, Loss: 0.49900257940428383\n",
      "Epoch 152, Loss: 0.4986865554458311\n",
      "Epoch 153, Loss: 0.49836949389755036\n",
      "Epoch 154, Loss: 0.49805375636713634\n",
      "Epoch 155, Loss: 0.49773873165141763\n",
      "Epoch 156, Loss: 0.49742224924400874\n",
      "Epoch 157, Loss: 0.49711425826812494\n",
      "Epoch 158, Loss: 0.4968010973402242\n",
      "Epoch 159, Loss: 0.4964901399315246\n",
      "Epoch 160, Loss: 0.49617675026847124\n",
      "Epoch 161, Loss: 0.4958670390985117\n",
      "Epoch 162, Loss: 0.4955578109301908\n",
      "Epoch 163, Loss: 0.4952510223984696\n",
      "Epoch 164, Loss: 0.49494577484111674\n",
      "Epoch 165, Loss: 0.49464511374066544\n",
      "Epoch 166, Loss: 0.49434383269975535\n",
      "Epoch 167, Loss: 0.49404631909698943\n",
      "Epoch 168, Loss: 0.49374530645143583\n",
      "Epoch 169, Loss: 0.49345064806438055\n",
      "Epoch 170, Loss: 0.49317001490258333\n",
      "Epoch 171, Loss: 0.4928927591216884\n",
      "Epoch 172, Loss: 0.4926138499628376\n",
      "Epoch 173, Loss: 0.49233528219511824\n",
      "Epoch 174, Loss: 0.49205852514110515\n",
      "Epoch 175, Loss: 0.4917773399684819\n",
      "Epoch 176, Loss: 0.49149629710928233\n",
      "Epoch 177, Loss: 0.4912165647150265\n",
      "Epoch 178, Loss: 0.49094181765497674\n",
      "Epoch 179, Loss: 0.49066901222163783\n",
      "Epoch 180, Loss: 0.490393928005003\n",
      "Epoch 181, Loss: 0.4901079113527018\n",
      "Epoch 182, Loss: 0.48981984154671765\n",
      "Epoch 183, Loss: 0.4895234443341763\n",
      "Epoch 184, Loss: 0.48921459258368105\n",
      "Epoch 185, Loss: 0.4889145977883947\n",
      "Epoch 186, Loss: 0.4886175142174919\n",
      "Epoch 187, Loss: 0.4883282295514753\n",
      "Epoch 188, Loss: 0.4880435212302353\n",
      "Epoch 189, Loss: 0.48775699700499114\n",
      "Epoch 190, Loss: 0.48748008613994775\n",
      "Epoch 191, Loss: 0.4872132179171775\n",
      "Epoch 192, Loss: 0.48694738158534456\n",
      "Epoch 193, Loss: 0.48668121634032785\n",
      "Epoch 194, Loss: 0.4864280322301768\n",
      "Epoch 195, Loss: 0.486178532155915\n",
      "Epoch 196, Loss: 0.4859301919172944\n",
      "Epoch 197, Loss: 0.48568442460989747\n",
      "Epoch 198, Loss: 0.48544224711238787\n",
      "Epoch 199, Loss: 0.48519729219516017\n",
      "Epoch 200, Loss: 0.4849575446659644\n",
      "Epoch 201, Loss: 0.4847146197220469\n",
      "Epoch 202, Loss: 0.48447155759133576\n",
      "Epoch 203, Loss: 0.48422783492589483\n",
      "Epoch 204, Loss: 0.48397707745066637\n",
      "Epoch 205, Loss: 0.483737926529631\n",
      "Epoch 206, Loss: 0.4835020782648923\n",
      "Epoch 207, Loss: 0.48327083252052233\n",
      "Epoch 208, Loss: 0.4830508282762034\n",
      "Epoch 209, Loss: 0.48283269895503794\n",
      "Epoch 210, Loss: 0.48261215534620283\n",
      "Epoch 211, Loss: 0.48240746542225255\n",
      "Epoch 212, Loss: 0.4822187420754367\n",
      "Epoch 213, Loss: 0.48202997575341067\n",
      "Epoch 214, Loss: 0.4818441197783379\n",
      "Epoch 215, Loss: 0.4816502256898402\n",
      "Epoch 216, Loss: 0.4814589616492725\n",
      "Epoch 217, Loss: 0.48127338055379854\n",
      "Epoch 218, Loss: 0.4810898650639108\n",
      "Epoch 219, Loss: 0.48091365405597575\n",
      "Epoch 220, Loss: 0.480720996546175\n",
      "Epoch 221, Loss: 0.48054865040929456\n",
      "Epoch 222, Loss: 0.4803774677186426\n",
      "Epoch 223, Loss: 0.4801885671816001\n",
      "Epoch 224, Loss: 0.48000935557804647\n",
      "Epoch 225, Loss: 0.4798350303004208\n",
      "Epoch 226, Loss: 0.47967842665052907\n",
      "Epoch 227, Loss: 0.47947854451894606\n",
      "Epoch 228, Loss: 0.47928199900429136\n",
      "Epoch 229, Loss: 0.4791424752085709\n",
      "Epoch 230, Loss: 0.4789254904984497\n",
      "Epoch 231, Loss: 0.4787155823499749\n",
      "Epoch 232, Loss: 0.4785186611779873\n",
      "Epoch 233, Loss: 0.4783287773458093\n",
      "Epoch 234, Loss: 0.4781774454711345\n",
      "Epoch 235, Loss: 0.4780410437330356\n",
      "Epoch 236, Loss: 0.4778263811299359\n",
      "Epoch 237, Loss: 0.47765285017795805\n",
      "Epoch 238, Loss: 0.4775436338740249\n",
      "Epoch 239, Loss: 0.4774342833479412\n",
      "Epoch 240, Loss: 0.477263591547843\n",
      "Epoch 241, Loss: 0.47707859610269837\n",
      "Epoch 242, Loss: 0.47697972966255553\n",
      "Epoch 243, Loss: 0.4768431884384324\n",
      "Epoch 244, Loss: 0.4767408349977067\n",
      "Epoch 245, Loss: 0.4765819952057135\n",
      "Epoch 246, Loss: 0.4764706413665339\n",
      "Epoch 247, Loss: 0.47634462160009333\n",
      "Epoch 248, Loss: 0.4762527991635621\n",
      "Epoch 249, Loss: 0.4762014052862575\n",
      "Epoch 250, Loss: 0.4760283306209857\n",
      "Epoch 251, Loss: 0.47592260832103706\n",
      "Epoch 252, Loss: 0.475834869470249\n",
      "Epoch 253, Loss: 0.47570980894919\n",
      "Epoch 254, Loss: 0.47559397414543253\n",
      "Epoch 255, Loss: 0.4755005182088922\n",
      "Epoch 256, Loss: 0.4753894909333468\n",
      "Epoch 257, Loss: 0.47527958208571863\n",
      "Epoch 258, Loss: 0.47517947896910767\n",
      "Epoch 259, Loss: 0.4750690667233416\n",
      "Epoch 260, Loss: 0.47496625253354696\n",
      "Epoch 261, Loss: 0.47486506352442986\n",
      "Epoch 262, Loss: 0.47477116593942054\n",
      "Epoch 263, Loss: 0.474752914637718\n",
      "Epoch 264, Loss: 0.47457458619766507\n",
      "Epoch 265, Loss: 0.47441594770111134\n",
      "Epoch 266, Loss: 0.4743326895520222\n",
      "Epoch 267, Loss: 0.47424442089802893\n",
      "Epoch 268, Loss: 0.47409961152836255\n",
      "Epoch 269, Loss: 0.47397674971911014\n",
      "Epoch 270, Loss: 0.4739006464998255\n",
      "Epoch 271, Loss: 0.4738391701966867\n",
      "Epoch 272, Loss: 0.4737234441817917\n",
      "Epoch 273, Loss: 0.47358564948211956\n",
      "Epoch 274, Loss: 0.47351060057458727\n",
      "Epoch 275, Loss: 0.4734065036258047\n",
      "Epoch 276, Loss: 0.4733269559263822\n",
      "Epoch 277, Loss: 0.4732341071370639\n",
      "Epoch 278, Loss: 0.4731407519682655\n",
      "Epoch 279, Loss: 0.4730939966069229\n",
      "Epoch 280, Loss: 0.47298024635218033\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2773610786019847\n",
      "Valence RMSE: 0.2548866195082846\n",
      "Arousal RMSE: 0.29814618401406995\n",
      "Test R^2 score: tensor([0.2127, 0.1198], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1190, 0.0151], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.16624559818354923\n",
      "Num of epochs: 281\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Epoch 121, Loss: 0.5116998508836337\n",
      "Epoch 122, Loss: 0.5111129902664598\n",
      "Epoch 123, Loss: 0.5105405741436312\n",
      "Epoch 124, Loss: 0.5099849010083686\n",
      "Epoch 125, Loss: 0.5094623759868415\n",
      "Epoch 126, Loss: 0.5089652841754122\n",
      "Epoch 127, Loss: 0.5084717799132713\n",
      "Epoch 128, Loss: 0.5079809643002384\n",
      "Epoch 129, Loss: 0.507507731608986\n",
      "Epoch 130, Loss: 0.5070515727009828\n",
      "Epoch 131, Loss: 0.5065962678643066\n",
      "Epoch 132, Loss: 0.5061448812256618\n",
      "Epoch 133, Loss: 0.5057069703510908\n",
      "Epoch 134, Loss: 0.5052756693952567\n",
      "Epoch 135, Loss: 0.5048445017462813\n",
      "Epoch 136, Loss: 0.504425284231979\n",
      "Epoch 137, Loss: 0.504022717877823\n",
      "Epoch 138, Loss: 0.5036312802042249\n",
      "Epoch 139, Loss: 0.5032368731068207\n",
      "Epoch 140, Loss: 0.5028511060044271\n",
      "Epoch 141, Loss: 0.5024804933282331\n",
      "Epoch 142, Loss: 0.5021182727396313\n",
      "Epoch 143, Loss: 0.5017581664990297\n",
      "Epoch 144, Loss: 0.5014050827892457\n",
      "Epoch 145, Loss: 0.501047675902349\n",
      "Epoch 146, Loss: 0.500690966247362\n",
      "Epoch 147, Loss: 0.5003476722262875\n",
      "Epoch 148, Loss: 0.5000068544871652\n",
      "Epoch 149, Loss: 0.49966338867073357\n",
      "Epoch 150, Loss: 0.4993280873087794\n",
      "Epoch 151, Loss: 0.49900257940428383\n",
      "Epoch 152, Loss: 0.4986865554458311\n",
      "Epoch 153, Loss: 0.49836949389755036\n",
      "Epoch 154, Loss: 0.49805375636713634\n",
      "Epoch 155, Loss: 0.49773873165141763\n",
      "Epoch 156, Loss: 0.49742224924400874\n",
      "Epoch 157, Loss: 0.49711425826812494\n",
      "Epoch 158, Loss: 0.4968010973402242\n",
      "Epoch 159, Loss: 0.4964901399315246\n",
      "Epoch 160, Loss: 0.49617675026847124\n",
      "Epoch 161, Loss: 0.4958670390985117\n",
      "Epoch 162, Loss: 0.4955578109301908\n",
      "Epoch 163, Loss: 0.4952510223984696\n",
      "Epoch 164, Loss: 0.49494577484111674\n",
      "Epoch 165, Loss: 0.49464511374066544\n",
      "Epoch 166, Loss: 0.49434383269975535\n",
      "Epoch 167, Loss: 0.49404631909698943\n",
      "Epoch 168, Loss: 0.49374530645143583\n",
      "Epoch 169, Loss: 0.49345064806438055\n",
      "Epoch 170, Loss: 0.49317001490258333\n",
      "Epoch 171, Loss: 0.4928927591216884\n",
      "Epoch 172, Loss: 0.4926138499628376\n",
      "Epoch 173, Loss: 0.49233528219511824\n",
      "Epoch 174, Loss: 0.49205852514110515\n",
      "Epoch 175, Loss: 0.4917773399684819\n",
      "Epoch 176, Loss: 0.49149629710928233\n",
      "Epoch 177, Loss: 0.4912165647150265\n",
      "Epoch 178, Loss: 0.49094181765497674\n",
      "Epoch 179, Loss: 0.49066901222163783\n",
      "Epoch 180, Loss: 0.490393928005003\n",
      "Epoch 181, Loss: 0.4901079113527018\n",
      "Epoch 182, Loss: 0.48981984154671765\n",
      "Epoch 183, Loss: 0.4895234443341763\n",
      "Epoch 184, Loss: 0.48921459258368105\n",
      "Epoch 185, Loss: 0.4889145977883947\n",
      "Epoch 186, Loss: 0.4886175142174919\n",
      "Epoch 187, Loss: 0.4883282295514753\n",
      "Epoch 188, Loss: 0.4880435212302353\n",
      "Epoch 189, Loss: 0.48775699700499114\n",
      "Epoch 190, Loss: 0.48748008613994775\n",
      "Epoch 191, Loss: 0.4872132179171775\n",
      "Epoch 192, Loss: 0.48694738158534456\n",
      "Epoch 193, Loss: 0.48668121634032785\n",
      "Epoch 194, Loss: 0.4864280322301768\n",
      "Epoch 195, Loss: 0.486178532155915\n",
      "Epoch 196, Loss: 0.4859301919172944\n",
      "Epoch 197, Loss: 0.48568442460989747\n",
      "Epoch 198, Loss: 0.48544224711238787\n",
      "Epoch 199, Loss: 0.48519729219516017\n",
      "Epoch 200, Loss: 0.4849575446659644\n",
      "Epoch 201, Loss: 0.4847146197220469\n",
      "Epoch 202, Loss: 0.48447155759133576\n",
      "Epoch 203, Loss: 0.48422783492589483\n",
      "Epoch 204, Loss: 0.48397707745066637\n",
      "Epoch 205, Loss: 0.483737926529631\n",
      "Epoch 206, Loss: 0.4835020782648923\n",
      "Epoch 207, Loss: 0.48327083252052233\n",
      "Epoch 208, Loss: 0.4830508282762034\n",
      "Epoch 209, Loss: 0.48283269895503794\n",
      "Epoch 210, Loss: 0.48261215534620283\n",
      "Epoch 211, Loss: 0.48240746542225255\n",
      "Epoch 212, Loss: 0.4822187420754367\n",
      "Epoch 213, Loss: 0.48202997575341067\n",
      "Epoch 214, Loss: 0.4818441197783379\n",
      "Epoch 215, Loss: 0.4816502256898402\n",
      "Epoch 216, Loss: 0.4814589616492725\n",
      "Epoch 217, Loss: 0.48127338055379854\n",
      "Epoch 218, Loss: 0.4810898650639108\n",
      "Epoch 219, Loss: 0.48091365405597575\n",
      "Epoch 220, Loss: 0.480720996546175\n",
      "Epoch 221, Loss: 0.48054865040929456\n",
      "Epoch 222, Loss: 0.4803774677186426\n",
      "Epoch 223, Loss: 0.4801885671816001\n",
      "Epoch 224, Loss: 0.48000935557804647\n",
      "Epoch 225, Loss: 0.4798350303004208\n",
      "Epoch 226, Loss: 0.47967842665052907\n",
      "Epoch 227, Loss: 0.47947854451894606\n",
      "Epoch 228, Loss: 0.47928199900429136\n",
      "Epoch 229, Loss: 0.4791424752085709\n",
      "Epoch 230, Loss: 0.4789254904984497\n",
      "Epoch 231, Loss: 0.4787155823499749\n",
      "Epoch 232, Loss: 0.4785186611779873\n",
      "Epoch 233, Loss: 0.4783287773458093\n",
      "Epoch 234, Loss: 0.4781774454711345\n",
      "Epoch 235, Loss: 0.4780410437330356\n",
      "Epoch 236, Loss: 0.4778263811299359\n",
      "Epoch 237, Loss: 0.47765285017795805\n",
      "Epoch 238, Loss: 0.4775436338740249\n",
      "Epoch 239, Loss: 0.4774342833479412\n",
      "Epoch 240, Loss: 0.477263591547843\n",
      "Epoch 241, Loss: 0.47707859610269837\n",
      "Epoch 242, Loss: 0.47697972966255553\n",
      "Epoch 243, Loss: 0.4768431884384324\n",
      "Epoch 244, Loss: 0.4767408349977067\n",
      "Epoch 245, Loss: 0.4765819952057135\n",
      "Epoch 246, Loss: 0.4764706413665339\n",
      "Epoch 247, Loss: 0.47634462160009333\n",
      "Epoch 248, Loss: 0.4762527991635621\n",
      "Epoch 249, Loss: 0.4762014052862575\n",
      "Epoch 250, Loss: 0.4760283306209857\n",
      "Epoch 251, Loss: 0.47592260832103706\n",
      "Epoch 252, Loss: 0.475834869470249\n",
      "Epoch 253, Loss: 0.47570980894919\n",
      "Epoch 254, Loss: 0.47559397414543253\n",
      "Epoch 255, Loss: 0.4755005182088922\n",
      "Epoch 256, Loss: 0.4753894909333468\n",
      "Epoch 257, Loss: 0.47527958208571863\n",
      "Epoch 258, Loss: 0.47517947896910767\n",
      "Epoch 259, Loss: 0.4750690667233416\n",
      "Epoch 260, Loss: 0.47496625253354696\n",
      "Epoch 261, Loss: 0.47486506352442986\n",
      "Epoch 262, Loss: 0.47477116593942054\n",
      "Epoch 263, Loss: 0.474752914637718\n",
      "Epoch 264, Loss: 0.47457458619766507\n",
      "Epoch 265, Loss: 0.47441594770111134\n",
      "Epoch 266, Loss: 0.4743326895520222\n",
      "Epoch 267, Loss: 0.47424442089802893\n",
      "Epoch 268, Loss: 0.47409961152836255\n",
      "Epoch 269, Loss: 0.47397674971911014\n",
      "Epoch 270, Loss: 0.4739006464998255\n",
      "Epoch 271, Loss: 0.4738391701966867\n",
      "Epoch 272, Loss: 0.4737234441817917\n",
      "Epoch 273, Loss: 0.47358564948211956\n",
      "Epoch 274, Loss: 0.47351060057458727\n",
      "Epoch 275, Loss: 0.4734065036258047\n",
      "Epoch 276, Loss: 0.4733269559263822\n",
      "Epoch 277, Loss: 0.4732341071370639\n",
      "Epoch 278, Loss: 0.4731407519682655\n",
      "Epoch 279, Loss: 0.4730939966069229\n",
      "Epoch 280, Loss: 0.47298024635218033\n",
      "Epoch 281, Loss: 0.47280886054292864\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.27749128471839435\n",
      "Valence RMSE: 0.2549508948893723\n",
      "Arousal RMSE: 0.29833348351205613\n",
      "Test R^2 score: tensor([0.2123, 0.1187], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1185, 0.0139], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.16549391888601278\n",
      "Num of epochs: 282\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Epoch 121, Loss: 0.5116998508836337\n",
      "Epoch 122, Loss: 0.5111129902664598\n",
      "Epoch 123, Loss: 0.5105405741436312\n",
      "Epoch 124, Loss: 0.5099849010083686\n",
      "Epoch 125, Loss: 0.5094623759868415\n",
      "Epoch 126, Loss: 0.5089652841754122\n",
      "Epoch 127, Loss: 0.5084717799132713\n",
      "Epoch 128, Loss: 0.5079809643002384\n",
      "Epoch 129, Loss: 0.507507731608986\n",
      "Epoch 130, Loss: 0.5070515727009828\n",
      "Epoch 131, Loss: 0.5065962678643066\n",
      "Epoch 132, Loss: 0.5061448812256618\n",
      "Epoch 133, Loss: 0.5057069703510908\n",
      "Epoch 134, Loss: 0.5052756693952567\n",
      "Epoch 135, Loss: 0.5048445017462813\n",
      "Epoch 136, Loss: 0.504425284231979\n",
      "Epoch 137, Loss: 0.504022717877823\n",
      "Epoch 138, Loss: 0.5036312802042249\n",
      "Epoch 139, Loss: 0.5032368731068207\n",
      "Epoch 140, Loss: 0.5028511060044271\n",
      "Epoch 141, Loss: 0.5024804933282331\n",
      "Epoch 142, Loss: 0.5021182727396313\n",
      "Epoch 143, Loss: 0.5017581664990297\n",
      "Epoch 144, Loss: 0.5014050827892457\n",
      "Epoch 145, Loss: 0.501047675902349\n",
      "Epoch 146, Loss: 0.500690966247362\n",
      "Epoch 147, Loss: 0.5003476722262875\n",
      "Epoch 148, Loss: 0.5000068544871652\n",
      "Epoch 149, Loss: 0.49966338867073357\n",
      "Epoch 150, Loss: 0.4993280873087794\n",
      "Epoch 151, Loss: 0.49900257940428383\n",
      "Epoch 152, Loss: 0.4986865554458311\n",
      "Epoch 153, Loss: 0.49836949389755036\n",
      "Epoch 154, Loss: 0.49805375636713634\n",
      "Epoch 155, Loss: 0.49773873165141763\n",
      "Epoch 156, Loss: 0.49742224924400874\n",
      "Epoch 157, Loss: 0.49711425826812494\n",
      "Epoch 158, Loss: 0.4968010973402242\n",
      "Epoch 159, Loss: 0.4964901399315246\n",
      "Epoch 160, Loss: 0.49617675026847124\n",
      "Epoch 161, Loss: 0.4958670390985117\n",
      "Epoch 162, Loss: 0.4955578109301908\n",
      "Epoch 163, Loss: 0.4952510223984696\n",
      "Epoch 164, Loss: 0.49494577484111674\n",
      "Epoch 165, Loss: 0.49464511374066544\n",
      "Epoch 166, Loss: 0.49434383269975535\n",
      "Epoch 167, Loss: 0.49404631909698943\n",
      "Epoch 168, Loss: 0.49374530645143583\n",
      "Epoch 169, Loss: 0.49345064806438055\n",
      "Epoch 170, Loss: 0.49317001490258333\n",
      "Epoch 171, Loss: 0.4928927591216884\n",
      "Epoch 172, Loss: 0.4926138499628376\n",
      "Epoch 173, Loss: 0.49233528219511824\n",
      "Epoch 174, Loss: 0.49205852514110515\n",
      "Epoch 175, Loss: 0.4917773399684819\n",
      "Epoch 176, Loss: 0.49149629710928233\n",
      "Epoch 177, Loss: 0.4912165647150265\n",
      "Epoch 178, Loss: 0.49094181765497674\n",
      "Epoch 179, Loss: 0.49066901222163783\n",
      "Epoch 180, Loss: 0.490393928005003\n",
      "Epoch 181, Loss: 0.4901079113527018\n",
      "Epoch 182, Loss: 0.48981984154671765\n",
      "Epoch 183, Loss: 0.4895234443341763\n",
      "Epoch 184, Loss: 0.48921459258368105\n",
      "Epoch 185, Loss: 0.4889145977883947\n",
      "Epoch 186, Loss: 0.4886175142174919\n",
      "Epoch 187, Loss: 0.4883282295514753\n",
      "Epoch 188, Loss: 0.4880435212302353\n",
      "Epoch 189, Loss: 0.48775699700499114\n",
      "Epoch 190, Loss: 0.48748008613994775\n",
      "Epoch 191, Loss: 0.4872132179171775\n",
      "Epoch 192, Loss: 0.48694738158534456\n",
      "Epoch 193, Loss: 0.48668121634032785\n",
      "Epoch 194, Loss: 0.4864280322301768\n",
      "Epoch 195, Loss: 0.486178532155915\n",
      "Epoch 196, Loss: 0.4859301919172944\n",
      "Epoch 197, Loss: 0.48568442460989747\n",
      "Epoch 198, Loss: 0.48544224711238787\n",
      "Epoch 199, Loss: 0.48519729219516017\n",
      "Epoch 200, Loss: 0.4849575446659644\n",
      "Epoch 201, Loss: 0.4847146197220469\n",
      "Epoch 202, Loss: 0.48447155759133576\n",
      "Epoch 203, Loss: 0.48422783492589483\n",
      "Epoch 204, Loss: 0.48397707745066637\n",
      "Epoch 205, Loss: 0.483737926529631\n",
      "Epoch 206, Loss: 0.4835020782648923\n",
      "Epoch 207, Loss: 0.48327083252052233\n",
      "Epoch 208, Loss: 0.4830508282762034\n",
      "Epoch 209, Loss: 0.48283269895503794\n",
      "Epoch 210, Loss: 0.48261215534620283\n",
      "Epoch 211, Loss: 0.48240746542225255\n",
      "Epoch 212, Loss: 0.4822187420754367\n",
      "Epoch 213, Loss: 0.48202997575341067\n",
      "Epoch 214, Loss: 0.4818441197783379\n",
      "Epoch 215, Loss: 0.4816502256898402\n",
      "Epoch 216, Loss: 0.4814589616492725\n",
      "Epoch 217, Loss: 0.48127338055379854\n",
      "Epoch 218, Loss: 0.4810898650639108\n",
      "Epoch 219, Loss: 0.48091365405597575\n",
      "Epoch 220, Loss: 0.480720996546175\n",
      "Epoch 221, Loss: 0.48054865040929456\n",
      "Epoch 222, Loss: 0.4803774677186426\n",
      "Epoch 223, Loss: 0.4801885671816001\n",
      "Epoch 224, Loss: 0.48000935557804647\n",
      "Epoch 225, Loss: 0.4798350303004208\n",
      "Epoch 226, Loss: 0.47967842665052907\n",
      "Epoch 227, Loss: 0.47947854451894606\n",
      "Epoch 228, Loss: 0.47928199900429136\n",
      "Epoch 229, Loss: 0.4791424752085709\n",
      "Epoch 230, Loss: 0.4789254904984497\n",
      "Epoch 231, Loss: 0.4787155823499749\n",
      "Epoch 232, Loss: 0.4785186611779873\n",
      "Epoch 233, Loss: 0.4783287773458093\n",
      "Epoch 234, Loss: 0.4781774454711345\n",
      "Epoch 235, Loss: 0.4780410437330356\n",
      "Epoch 236, Loss: 0.4778263811299359\n",
      "Epoch 237, Loss: 0.47765285017795805\n",
      "Epoch 238, Loss: 0.4775436338740249\n",
      "Epoch 239, Loss: 0.4774342833479412\n",
      "Epoch 240, Loss: 0.477263591547843\n",
      "Epoch 241, Loss: 0.47707859610269837\n",
      "Epoch 242, Loss: 0.47697972966255553\n",
      "Epoch 243, Loss: 0.4768431884384324\n",
      "Epoch 244, Loss: 0.4767408349977067\n",
      "Epoch 245, Loss: 0.4765819952057135\n",
      "Epoch 246, Loss: 0.4764706413665339\n",
      "Epoch 247, Loss: 0.47634462160009333\n",
      "Epoch 248, Loss: 0.4762527991635621\n",
      "Epoch 249, Loss: 0.4762014052862575\n",
      "Epoch 250, Loss: 0.4760283306209857\n",
      "Epoch 251, Loss: 0.47592260832103706\n",
      "Epoch 252, Loss: 0.475834869470249\n",
      "Epoch 253, Loss: 0.47570980894919\n",
      "Epoch 254, Loss: 0.47559397414543253\n",
      "Epoch 255, Loss: 0.4755005182088922\n",
      "Epoch 256, Loss: 0.4753894909333468\n",
      "Epoch 257, Loss: 0.47527958208571863\n",
      "Epoch 258, Loss: 0.47517947896910767\n",
      "Epoch 259, Loss: 0.4750690667233416\n",
      "Epoch 260, Loss: 0.47496625253354696\n",
      "Epoch 261, Loss: 0.47486506352442986\n",
      "Epoch 262, Loss: 0.47477116593942054\n",
      "Epoch 263, Loss: 0.474752914637718\n",
      "Epoch 264, Loss: 0.47457458619766507\n",
      "Epoch 265, Loss: 0.47441594770111134\n",
      "Epoch 266, Loss: 0.4743326895520222\n",
      "Epoch 267, Loss: 0.47424442089802893\n",
      "Epoch 268, Loss: 0.47409961152836255\n",
      "Epoch 269, Loss: 0.47397674971911014\n",
      "Epoch 270, Loss: 0.4739006464998255\n",
      "Epoch 271, Loss: 0.4738391701966867\n",
      "Epoch 272, Loss: 0.4737234441817917\n",
      "Epoch 273, Loss: 0.47358564948211956\n",
      "Epoch 274, Loss: 0.47351060057458727\n",
      "Epoch 275, Loss: 0.4734065036258047\n",
      "Epoch 276, Loss: 0.4733269559263822\n",
      "Epoch 277, Loss: 0.4732341071370639\n",
      "Epoch 278, Loss: 0.4731407519682655\n",
      "Epoch 279, Loss: 0.4730939966069229\n",
      "Epoch 280, Loss: 0.47298024635218033\n",
      "Epoch 281, Loss: 0.47280886054292864\n",
      "Epoch 282, Loss: 0.47271222185116163\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2775670112202594\n",
      "Valence RMSE: 0.25510607537512453\n",
      "Arousal RMSE: 0.2983417197480039\n",
      "Test R^2 score: tensor([0.2113, 0.1187], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1175, 0.0138], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.16498997403595178\n",
      "Num of epochs: 283\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Epoch 121, Loss: 0.5116998508836337\n",
      "Epoch 122, Loss: 0.5111129902664598\n",
      "Epoch 123, Loss: 0.5105405741436312\n",
      "Epoch 124, Loss: 0.5099849010083686\n",
      "Epoch 125, Loss: 0.5094623759868415\n",
      "Epoch 126, Loss: 0.5089652841754122\n",
      "Epoch 127, Loss: 0.5084717799132713\n",
      "Epoch 128, Loss: 0.5079809643002384\n",
      "Epoch 129, Loss: 0.507507731608986\n",
      "Epoch 130, Loss: 0.5070515727009828\n",
      "Epoch 131, Loss: 0.5065962678643066\n",
      "Epoch 132, Loss: 0.5061448812256618\n",
      "Epoch 133, Loss: 0.5057069703510908\n",
      "Epoch 134, Loss: 0.5052756693952567\n",
      "Epoch 135, Loss: 0.5048445017462813\n",
      "Epoch 136, Loss: 0.504425284231979\n",
      "Epoch 137, Loss: 0.504022717877823\n",
      "Epoch 138, Loss: 0.5036312802042249\n",
      "Epoch 139, Loss: 0.5032368731068207\n",
      "Epoch 140, Loss: 0.5028511060044271\n",
      "Epoch 141, Loss: 0.5024804933282331\n",
      "Epoch 142, Loss: 0.5021182727396313\n",
      "Epoch 143, Loss: 0.5017581664990297\n",
      "Epoch 144, Loss: 0.5014050827892457\n",
      "Epoch 145, Loss: 0.501047675902349\n",
      "Epoch 146, Loss: 0.500690966247362\n",
      "Epoch 147, Loss: 0.5003476722262875\n",
      "Epoch 148, Loss: 0.5000068544871652\n",
      "Epoch 149, Loss: 0.49966338867073357\n",
      "Epoch 150, Loss: 0.4993280873087794\n",
      "Epoch 151, Loss: 0.49900257940428383\n",
      "Epoch 152, Loss: 0.4986865554458311\n",
      "Epoch 153, Loss: 0.49836949389755036\n",
      "Epoch 154, Loss: 0.49805375636713634\n",
      "Epoch 155, Loss: 0.49773873165141763\n",
      "Epoch 156, Loss: 0.49742224924400874\n",
      "Epoch 157, Loss: 0.49711425826812494\n",
      "Epoch 158, Loss: 0.4968010973402242\n",
      "Epoch 159, Loss: 0.4964901399315246\n",
      "Epoch 160, Loss: 0.49617675026847124\n",
      "Epoch 161, Loss: 0.4958670390985117\n",
      "Epoch 162, Loss: 0.4955578109301908\n",
      "Epoch 163, Loss: 0.4952510223984696\n",
      "Epoch 164, Loss: 0.49494577484111674\n",
      "Epoch 165, Loss: 0.49464511374066544\n",
      "Epoch 166, Loss: 0.49434383269975535\n",
      "Epoch 167, Loss: 0.49404631909698943\n",
      "Epoch 168, Loss: 0.49374530645143583\n",
      "Epoch 169, Loss: 0.49345064806438055\n",
      "Epoch 170, Loss: 0.49317001490258333\n",
      "Epoch 171, Loss: 0.4928927591216884\n",
      "Epoch 172, Loss: 0.4926138499628376\n",
      "Epoch 173, Loss: 0.49233528219511824\n",
      "Epoch 174, Loss: 0.49205852514110515\n",
      "Epoch 175, Loss: 0.4917773399684819\n",
      "Epoch 176, Loss: 0.49149629710928233\n",
      "Epoch 177, Loss: 0.4912165647150265\n",
      "Epoch 178, Loss: 0.49094181765497674\n",
      "Epoch 179, Loss: 0.49066901222163783\n",
      "Epoch 180, Loss: 0.490393928005003\n",
      "Epoch 181, Loss: 0.4901079113527018\n",
      "Epoch 182, Loss: 0.48981984154671765\n",
      "Epoch 183, Loss: 0.4895234443341763\n",
      "Epoch 184, Loss: 0.48921459258368105\n",
      "Epoch 185, Loss: 0.4889145977883947\n",
      "Epoch 186, Loss: 0.4886175142174919\n",
      "Epoch 187, Loss: 0.4883282295514753\n",
      "Epoch 188, Loss: 0.4880435212302353\n",
      "Epoch 189, Loss: 0.48775699700499114\n",
      "Epoch 190, Loss: 0.48748008613994775\n",
      "Epoch 191, Loss: 0.4872132179171775\n",
      "Epoch 192, Loss: 0.48694738158534456\n",
      "Epoch 193, Loss: 0.48668121634032785\n",
      "Epoch 194, Loss: 0.4864280322301768\n",
      "Epoch 195, Loss: 0.486178532155915\n",
      "Epoch 196, Loss: 0.4859301919172944\n",
      "Epoch 197, Loss: 0.48568442460989747\n",
      "Epoch 198, Loss: 0.48544224711238787\n",
      "Epoch 199, Loss: 0.48519729219516017\n",
      "Epoch 200, Loss: 0.4849575446659644\n",
      "Epoch 201, Loss: 0.4847146197220469\n",
      "Epoch 202, Loss: 0.48447155759133576\n",
      "Epoch 203, Loss: 0.48422783492589483\n",
      "Epoch 204, Loss: 0.48397707745066637\n",
      "Epoch 205, Loss: 0.483737926529631\n",
      "Epoch 206, Loss: 0.4835020782648923\n",
      "Epoch 207, Loss: 0.48327083252052233\n",
      "Epoch 208, Loss: 0.4830508282762034\n",
      "Epoch 209, Loss: 0.48283269895503794\n",
      "Epoch 210, Loss: 0.48261215534620283\n",
      "Epoch 211, Loss: 0.48240746542225255\n",
      "Epoch 212, Loss: 0.4822187420754367\n",
      "Epoch 213, Loss: 0.48202997575341067\n",
      "Epoch 214, Loss: 0.4818441197783379\n",
      "Epoch 215, Loss: 0.4816502256898402\n",
      "Epoch 216, Loss: 0.4814589616492725\n",
      "Epoch 217, Loss: 0.48127338055379854\n",
      "Epoch 218, Loss: 0.4810898650639108\n",
      "Epoch 219, Loss: 0.48091365405597575\n",
      "Epoch 220, Loss: 0.480720996546175\n",
      "Epoch 221, Loss: 0.48054865040929456\n",
      "Epoch 222, Loss: 0.4803774677186426\n",
      "Epoch 223, Loss: 0.4801885671816001\n",
      "Epoch 224, Loss: 0.48000935557804647\n",
      "Epoch 225, Loss: 0.4798350303004208\n",
      "Epoch 226, Loss: 0.47967842665052907\n",
      "Epoch 227, Loss: 0.47947854451894606\n",
      "Epoch 228, Loss: 0.47928199900429136\n",
      "Epoch 229, Loss: 0.4791424752085709\n",
      "Epoch 230, Loss: 0.4789254904984497\n",
      "Epoch 231, Loss: 0.4787155823499749\n",
      "Epoch 232, Loss: 0.4785186611779873\n",
      "Epoch 233, Loss: 0.4783287773458093\n",
      "Epoch 234, Loss: 0.4781774454711345\n",
      "Epoch 235, Loss: 0.4780410437330356\n",
      "Epoch 236, Loss: 0.4778263811299359\n",
      "Epoch 237, Loss: 0.47765285017795805\n",
      "Epoch 238, Loss: 0.4775436338740249\n",
      "Epoch 239, Loss: 0.4774342833479412\n",
      "Epoch 240, Loss: 0.477263591547843\n",
      "Epoch 241, Loss: 0.47707859610269837\n",
      "Epoch 242, Loss: 0.47697972966255553\n",
      "Epoch 243, Loss: 0.4768431884384324\n",
      "Epoch 244, Loss: 0.4767408349977067\n",
      "Epoch 245, Loss: 0.4765819952057135\n",
      "Epoch 246, Loss: 0.4764706413665339\n",
      "Epoch 247, Loss: 0.47634462160009333\n",
      "Epoch 248, Loss: 0.4762527991635621\n",
      "Epoch 249, Loss: 0.4762014052862575\n",
      "Epoch 250, Loss: 0.4760283306209857\n",
      "Epoch 251, Loss: 0.47592260832103706\n",
      "Epoch 252, Loss: 0.475834869470249\n",
      "Epoch 253, Loss: 0.47570980894919\n",
      "Epoch 254, Loss: 0.47559397414543253\n",
      "Epoch 255, Loss: 0.4755005182088922\n",
      "Epoch 256, Loss: 0.4753894909333468\n",
      "Epoch 257, Loss: 0.47527958208571863\n",
      "Epoch 258, Loss: 0.47517947896910767\n",
      "Epoch 259, Loss: 0.4750690667233416\n",
      "Epoch 260, Loss: 0.47496625253354696\n",
      "Epoch 261, Loss: 0.47486506352442986\n",
      "Epoch 262, Loss: 0.47477116593942054\n",
      "Epoch 263, Loss: 0.474752914637718\n",
      "Epoch 264, Loss: 0.47457458619766507\n",
      "Epoch 265, Loss: 0.47441594770111134\n",
      "Epoch 266, Loss: 0.4743326895520222\n",
      "Epoch 267, Loss: 0.47424442089802893\n",
      "Epoch 268, Loss: 0.47409961152836255\n",
      "Epoch 269, Loss: 0.47397674971911014\n",
      "Epoch 270, Loss: 0.4739006464998255\n",
      "Epoch 271, Loss: 0.4738391701966867\n",
      "Epoch 272, Loss: 0.4737234441817917\n",
      "Epoch 273, Loss: 0.47358564948211956\n",
      "Epoch 274, Loss: 0.47351060057458727\n",
      "Epoch 275, Loss: 0.4734065036258047\n",
      "Epoch 276, Loss: 0.4733269559263822\n",
      "Epoch 277, Loss: 0.4732341071370639\n",
      "Epoch 278, Loss: 0.4731407519682655\n",
      "Epoch 279, Loss: 0.4730939966069229\n",
      "Epoch 280, Loss: 0.47298024635218033\n",
      "Epoch 281, Loss: 0.47280886054292864\n",
      "Epoch 282, Loss: 0.47271222185116163\n",
      "Epoch 283, Loss: 0.47268730250728586\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2772236380546212\n",
      "Valence RMSE: 0.2546159048131452\n",
      "Arousal RMSE: 0.2981218408782936\n",
      "Test R^2 score: tensor([0.2143, 0.1200], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1209, 0.0153], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.16715324526586012\n",
      "Num of epochs: 284\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Epoch 121, Loss: 0.5116998508836337\n",
      "Epoch 122, Loss: 0.5111129902664598\n",
      "Epoch 123, Loss: 0.5105405741436312\n",
      "Epoch 124, Loss: 0.5099849010083686\n",
      "Epoch 125, Loss: 0.5094623759868415\n",
      "Epoch 126, Loss: 0.5089652841754122\n",
      "Epoch 127, Loss: 0.5084717799132713\n",
      "Epoch 128, Loss: 0.5079809643002384\n",
      "Epoch 129, Loss: 0.507507731608986\n",
      "Epoch 130, Loss: 0.5070515727009828\n",
      "Epoch 131, Loss: 0.5065962678643066\n",
      "Epoch 132, Loss: 0.5061448812256618\n",
      "Epoch 133, Loss: 0.5057069703510908\n",
      "Epoch 134, Loss: 0.5052756693952567\n",
      "Epoch 135, Loss: 0.5048445017462813\n",
      "Epoch 136, Loss: 0.504425284231979\n",
      "Epoch 137, Loss: 0.504022717877823\n",
      "Epoch 138, Loss: 0.5036312802042249\n",
      "Epoch 139, Loss: 0.5032368731068207\n",
      "Epoch 140, Loss: 0.5028511060044271\n",
      "Epoch 141, Loss: 0.5024804933282331\n",
      "Epoch 142, Loss: 0.5021182727396313\n",
      "Epoch 143, Loss: 0.5017581664990297\n",
      "Epoch 144, Loss: 0.5014050827892457\n",
      "Epoch 145, Loss: 0.501047675902349\n",
      "Epoch 146, Loss: 0.500690966247362\n",
      "Epoch 147, Loss: 0.5003476722262875\n",
      "Epoch 148, Loss: 0.5000068544871652\n",
      "Epoch 149, Loss: 0.49966338867073357\n",
      "Epoch 150, Loss: 0.4993280873087794\n",
      "Epoch 151, Loss: 0.49900257940428383\n",
      "Epoch 152, Loss: 0.4986865554458311\n",
      "Epoch 153, Loss: 0.49836949389755036\n",
      "Epoch 154, Loss: 0.49805375636713634\n",
      "Epoch 155, Loss: 0.49773873165141763\n",
      "Epoch 156, Loss: 0.49742224924400874\n",
      "Epoch 157, Loss: 0.49711425826812494\n",
      "Epoch 158, Loss: 0.4968010973402242\n",
      "Epoch 159, Loss: 0.4964901399315246\n",
      "Epoch 160, Loss: 0.49617675026847124\n",
      "Epoch 161, Loss: 0.4958670390985117\n",
      "Epoch 162, Loss: 0.4955578109301908\n",
      "Epoch 163, Loss: 0.4952510223984696\n",
      "Epoch 164, Loss: 0.49494577484111674\n",
      "Epoch 165, Loss: 0.49464511374066544\n",
      "Epoch 166, Loss: 0.49434383269975535\n",
      "Epoch 167, Loss: 0.49404631909698943\n",
      "Epoch 168, Loss: 0.49374530645143583\n",
      "Epoch 169, Loss: 0.49345064806438055\n",
      "Epoch 170, Loss: 0.49317001490258333\n",
      "Epoch 171, Loss: 0.4928927591216884\n",
      "Epoch 172, Loss: 0.4926138499628376\n",
      "Epoch 173, Loss: 0.49233528219511824\n",
      "Epoch 174, Loss: 0.49205852514110515\n",
      "Epoch 175, Loss: 0.4917773399684819\n",
      "Epoch 176, Loss: 0.49149629710928233\n",
      "Epoch 177, Loss: 0.4912165647150265\n",
      "Epoch 178, Loss: 0.49094181765497674\n",
      "Epoch 179, Loss: 0.49066901222163783\n",
      "Epoch 180, Loss: 0.490393928005003\n",
      "Epoch 181, Loss: 0.4901079113527018\n",
      "Epoch 182, Loss: 0.48981984154671765\n",
      "Epoch 183, Loss: 0.4895234443341763\n",
      "Epoch 184, Loss: 0.48921459258368105\n",
      "Epoch 185, Loss: 0.4889145977883947\n",
      "Epoch 186, Loss: 0.4886175142174919\n",
      "Epoch 187, Loss: 0.4883282295514753\n",
      "Epoch 188, Loss: 0.4880435212302353\n",
      "Epoch 189, Loss: 0.48775699700499114\n",
      "Epoch 190, Loss: 0.48748008613994775\n",
      "Epoch 191, Loss: 0.4872132179171775\n",
      "Epoch 192, Loss: 0.48694738158534456\n",
      "Epoch 193, Loss: 0.48668121634032785\n",
      "Epoch 194, Loss: 0.4864280322301768\n",
      "Epoch 195, Loss: 0.486178532155915\n",
      "Epoch 196, Loss: 0.4859301919172944\n",
      "Epoch 197, Loss: 0.48568442460989747\n",
      "Epoch 198, Loss: 0.48544224711238787\n",
      "Epoch 199, Loss: 0.48519729219516017\n",
      "Epoch 200, Loss: 0.4849575446659644\n",
      "Epoch 201, Loss: 0.4847146197220469\n",
      "Epoch 202, Loss: 0.48447155759133576\n",
      "Epoch 203, Loss: 0.48422783492589483\n",
      "Epoch 204, Loss: 0.48397707745066637\n",
      "Epoch 205, Loss: 0.483737926529631\n",
      "Epoch 206, Loss: 0.4835020782648923\n",
      "Epoch 207, Loss: 0.48327083252052233\n",
      "Epoch 208, Loss: 0.4830508282762034\n",
      "Epoch 209, Loss: 0.48283269895503794\n",
      "Epoch 210, Loss: 0.48261215534620283\n",
      "Epoch 211, Loss: 0.48240746542225255\n",
      "Epoch 212, Loss: 0.4822187420754367\n",
      "Epoch 213, Loss: 0.48202997575341067\n",
      "Epoch 214, Loss: 0.4818441197783379\n",
      "Epoch 215, Loss: 0.4816502256898402\n",
      "Epoch 216, Loss: 0.4814589616492725\n",
      "Epoch 217, Loss: 0.48127338055379854\n",
      "Epoch 218, Loss: 0.4810898650639108\n",
      "Epoch 219, Loss: 0.48091365405597575\n",
      "Epoch 220, Loss: 0.480720996546175\n",
      "Epoch 221, Loss: 0.48054865040929456\n",
      "Epoch 222, Loss: 0.4803774677186426\n",
      "Epoch 223, Loss: 0.4801885671816001\n",
      "Epoch 224, Loss: 0.48000935557804647\n",
      "Epoch 225, Loss: 0.4798350303004208\n",
      "Epoch 226, Loss: 0.47967842665052907\n",
      "Epoch 227, Loss: 0.47947854451894606\n",
      "Epoch 228, Loss: 0.47928199900429136\n",
      "Epoch 229, Loss: 0.4791424752085709\n",
      "Epoch 230, Loss: 0.4789254904984497\n",
      "Epoch 231, Loss: 0.4787155823499749\n",
      "Epoch 232, Loss: 0.4785186611779873\n",
      "Epoch 233, Loss: 0.4783287773458093\n",
      "Epoch 234, Loss: 0.4781774454711345\n",
      "Epoch 235, Loss: 0.4780410437330356\n",
      "Epoch 236, Loss: 0.4778263811299359\n",
      "Epoch 237, Loss: 0.47765285017795805\n",
      "Epoch 238, Loss: 0.4775436338740249\n",
      "Epoch 239, Loss: 0.4774342833479412\n",
      "Epoch 240, Loss: 0.477263591547843\n",
      "Epoch 241, Loss: 0.47707859610269837\n",
      "Epoch 242, Loss: 0.47697972966255553\n",
      "Epoch 243, Loss: 0.4768431884384324\n",
      "Epoch 244, Loss: 0.4767408349977067\n",
      "Epoch 245, Loss: 0.4765819952057135\n",
      "Epoch 246, Loss: 0.4764706413665339\n",
      "Epoch 247, Loss: 0.47634462160009333\n",
      "Epoch 248, Loss: 0.4762527991635621\n",
      "Epoch 249, Loss: 0.4762014052862575\n",
      "Epoch 250, Loss: 0.4760283306209857\n",
      "Epoch 251, Loss: 0.47592260832103706\n",
      "Epoch 252, Loss: 0.475834869470249\n",
      "Epoch 253, Loss: 0.47570980894919\n",
      "Epoch 254, Loss: 0.47559397414543253\n",
      "Epoch 255, Loss: 0.4755005182088922\n",
      "Epoch 256, Loss: 0.4753894909333468\n",
      "Epoch 257, Loss: 0.47527958208571863\n",
      "Epoch 258, Loss: 0.47517947896910767\n",
      "Epoch 259, Loss: 0.4750690667233416\n",
      "Epoch 260, Loss: 0.47496625253354696\n",
      "Epoch 261, Loss: 0.47486506352442986\n",
      "Epoch 262, Loss: 0.47477116593942054\n",
      "Epoch 263, Loss: 0.474752914637718\n",
      "Epoch 264, Loss: 0.47457458619766507\n",
      "Epoch 265, Loss: 0.47441594770111134\n",
      "Epoch 266, Loss: 0.4743326895520222\n",
      "Epoch 267, Loss: 0.47424442089802893\n",
      "Epoch 268, Loss: 0.47409961152836255\n",
      "Epoch 269, Loss: 0.47397674971911014\n",
      "Epoch 270, Loss: 0.4739006464998255\n",
      "Epoch 271, Loss: 0.4738391701966867\n",
      "Epoch 272, Loss: 0.4737234441817917\n",
      "Epoch 273, Loss: 0.47358564948211956\n",
      "Epoch 274, Loss: 0.47351060057458727\n",
      "Epoch 275, Loss: 0.4734065036258047\n",
      "Epoch 276, Loss: 0.4733269559263822\n",
      "Epoch 277, Loss: 0.4732341071370639\n",
      "Epoch 278, Loss: 0.4731407519682655\n",
      "Epoch 279, Loss: 0.4730939966069229\n",
      "Epoch 280, Loss: 0.47298024635218033\n",
      "Epoch 281, Loss: 0.47280886054292864\n",
      "Epoch 282, Loss: 0.47271222185116163\n",
      "Epoch 283, Loss: 0.47268730250728586\n",
      "Epoch 284, Loss: 0.4725869656059248\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2773262287446399\n",
      "Valence RMSE: 0.25492312696162533\n",
      "Arousal RMSE: 0.2980501193416231\n",
      "Test R^2 score: tensor([0.2124, 0.1204], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1187, 0.0157], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.16641637113709074\n",
      "Num of epochs: 285\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Epoch 121, Loss: 0.5116998508836337\n",
      "Epoch 122, Loss: 0.5111129902664598\n",
      "Epoch 123, Loss: 0.5105405741436312\n",
      "Epoch 124, Loss: 0.5099849010083686\n",
      "Epoch 125, Loss: 0.5094623759868415\n",
      "Epoch 126, Loss: 0.5089652841754122\n",
      "Epoch 127, Loss: 0.5084717799132713\n",
      "Epoch 128, Loss: 0.5079809643002384\n",
      "Epoch 129, Loss: 0.507507731608986\n",
      "Epoch 130, Loss: 0.5070515727009828\n",
      "Epoch 131, Loss: 0.5065962678643066\n",
      "Epoch 132, Loss: 0.5061448812256618\n",
      "Epoch 133, Loss: 0.5057069703510908\n",
      "Epoch 134, Loss: 0.5052756693952567\n",
      "Epoch 135, Loss: 0.5048445017462813\n",
      "Epoch 136, Loss: 0.504425284231979\n",
      "Epoch 137, Loss: 0.504022717877823\n",
      "Epoch 138, Loss: 0.5036312802042249\n",
      "Epoch 139, Loss: 0.5032368731068207\n",
      "Epoch 140, Loss: 0.5028511060044271\n",
      "Epoch 141, Loss: 0.5024804933282331\n",
      "Epoch 142, Loss: 0.5021182727396313\n",
      "Epoch 143, Loss: 0.5017581664990297\n",
      "Epoch 144, Loss: 0.5014050827892457\n",
      "Epoch 145, Loss: 0.501047675902349\n",
      "Epoch 146, Loss: 0.500690966247362\n",
      "Epoch 147, Loss: 0.5003476722262875\n",
      "Epoch 148, Loss: 0.5000068544871652\n",
      "Epoch 149, Loss: 0.49966338867073357\n",
      "Epoch 150, Loss: 0.4993280873087794\n",
      "Epoch 151, Loss: 0.49900257940428383\n",
      "Epoch 152, Loss: 0.4986865554458311\n",
      "Epoch 153, Loss: 0.49836949389755036\n",
      "Epoch 154, Loss: 0.49805375636713634\n",
      "Epoch 155, Loss: 0.49773873165141763\n",
      "Epoch 156, Loss: 0.49742224924400874\n",
      "Epoch 157, Loss: 0.49711425826812494\n",
      "Epoch 158, Loss: 0.4968010973402242\n",
      "Epoch 159, Loss: 0.4964901399315246\n",
      "Epoch 160, Loss: 0.49617675026847124\n",
      "Epoch 161, Loss: 0.4958670390985117\n",
      "Epoch 162, Loss: 0.4955578109301908\n",
      "Epoch 163, Loss: 0.4952510223984696\n",
      "Epoch 164, Loss: 0.49494577484111674\n",
      "Epoch 165, Loss: 0.49464511374066544\n",
      "Epoch 166, Loss: 0.49434383269975535\n",
      "Epoch 167, Loss: 0.49404631909698943\n",
      "Epoch 168, Loss: 0.49374530645143583\n",
      "Epoch 169, Loss: 0.49345064806438055\n",
      "Epoch 170, Loss: 0.49317001490258333\n",
      "Epoch 171, Loss: 0.4928927591216884\n",
      "Epoch 172, Loss: 0.4926138499628376\n",
      "Epoch 173, Loss: 0.49233528219511824\n",
      "Epoch 174, Loss: 0.49205852514110515\n",
      "Epoch 175, Loss: 0.4917773399684819\n",
      "Epoch 176, Loss: 0.49149629710928233\n",
      "Epoch 177, Loss: 0.4912165647150265\n",
      "Epoch 178, Loss: 0.49094181765497674\n",
      "Epoch 179, Loss: 0.49066901222163783\n",
      "Epoch 180, Loss: 0.490393928005003\n",
      "Epoch 181, Loss: 0.4901079113527018\n",
      "Epoch 182, Loss: 0.48981984154671765\n",
      "Epoch 183, Loss: 0.4895234443341763\n",
      "Epoch 184, Loss: 0.48921459258368105\n",
      "Epoch 185, Loss: 0.4889145977883947\n",
      "Epoch 186, Loss: 0.4886175142174919\n",
      "Epoch 187, Loss: 0.4883282295514753\n",
      "Epoch 188, Loss: 0.4880435212302353\n",
      "Epoch 189, Loss: 0.48775699700499114\n",
      "Epoch 190, Loss: 0.48748008613994775\n",
      "Epoch 191, Loss: 0.4872132179171775\n",
      "Epoch 192, Loss: 0.48694738158534456\n",
      "Epoch 193, Loss: 0.48668121634032785\n",
      "Epoch 194, Loss: 0.4864280322301768\n",
      "Epoch 195, Loss: 0.486178532155915\n",
      "Epoch 196, Loss: 0.4859301919172944\n",
      "Epoch 197, Loss: 0.48568442460989747\n",
      "Epoch 198, Loss: 0.48544224711238787\n",
      "Epoch 199, Loss: 0.48519729219516017\n",
      "Epoch 200, Loss: 0.4849575446659644\n",
      "Epoch 201, Loss: 0.4847146197220469\n",
      "Epoch 202, Loss: 0.48447155759133576\n",
      "Epoch 203, Loss: 0.48422783492589483\n",
      "Epoch 204, Loss: 0.48397707745066637\n",
      "Epoch 205, Loss: 0.483737926529631\n",
      "Epoch 206, Loss: 0.4835020782648923\n",
      "Epoch 207, Loss: 0.48327083252052233\n",
      "Epoch 208, Loss: 0.4830508282762034\n",
      "Epoch 209, Loss: 0.48283269895503794\n",
      "Epoch 210, Loss: 0.48261215534620283\n",
      "Epoch 211, Loss: 0.48240746542225255\n",
      "Epoch 212, Loss: 0.4822187420754367\n",
      "Epoch 213, Loss: 0.48202997575341067\n",
      "Epoch 214, Loss: 0.4818441197783379\n",
      "Epoch 215, Loss: 0.4816502256898402\n",
      "Epoch 216, Loss: 0.4814589616492725\n",
      "Epoch 217, Loss: 0.48127338055379854\n",
      "Epoch 218, Loss: 0.4810898650639108\n",
      "Epoch 219, Loss: 0.48091365405597575\n",
      "Epoch 220, Loss: 0.480720996546175\n",
      "Epoch 221, Loss: 0.48054865040929456\n",
      "Epoch 222, Loss: 0.4803774677186426\n",
      "Epoch 223, Loss: 0.4801885671816001\n",
      "Epoch 224, Loss: 0.48000935557804647\n",
      "Epoch 225, Loss: 0.4798350303004208\n",
      "Epoch 226, Loss: 0.47967842665052907\n",
      "Epoch 227, Loss: 0.47947854451894606\n",
      "Epoch 228, Loss: 0.47928199900429136\n",
      "Epoch 229, Loss: 0.4791424752085709\n",
      "Epoch 230, Loss: 0.4789254904984497\n",
      "Epoch 231, Loss: 0.4787155823499749\n",
      "Epoch 232, Loss: 0.4785186611779873\n",
      "Epoch 233, Loss: 0.4783287773458093\n",
      "Epoch 234, Loss: 0.4781774454711345\n",
      "Epoch 235, Loss: 0.4780410437330356\n",
      "Epoch 236, Loss: 0.4778263811299359\n",
      "Epoch 237, Loss: 0.47765285017795805\n",
      "Epoch 238, Loss: 0.4775436338740249\n",
      "Epoch 239, Loss: 0.4774342833479412\n",
      "Epoch 240, Loss: 0.477263591547843\n",
      "Epoch 241, Loss: 0.47707859610269837\n",
      "Epoch 242, Loss: 0.47697972966255553\n",
      "Epoch 243, Loss: 0.4768431884384324\n",
      "Epoch 244, Loss: 0.4767408349977067\n",
      "Epoch 245, Loss: 0.4765819952057135\n",
      "Epoch 246, Loss: 0.4764706413665339\n",
      "Epoch 247, Loss: 0.47634462160009333\n",
      "Epoch 248, Loss: 0.4762527991635621\n",
      "Epoch 249, Loss: 0.4762014052862575\n",
      "Epoch 250, Loss: 0.4760283306209857\n",
      "Epoch 251, Loss: 0.47592260832103706\n",
      "Epoch 252, Loss: 0.475834869470249\n",
      "Epoch 253, Loss: 0.47570980894919\n",
      "Epoch 254, Loss: 0.47559397414543253\n",
      "Epoch 255, Loss: 0.4755005182088922\n",
      "Epoch 256, Loss: 0.4753894909333468\n",
      "Epoch 257, Loss: 0.47527958208571863\n",
      "Epoch 258, Loss: 0.47517947896910767\n",
      "Epoch 259, Loss: 0.4750690667233416\n",
      "Epoch 260, Loss: 0.47496625253354696\n",
      "Epoch 261, Loss: 0.47486506352442986\n",
      "Epoch 262, Loss: 0.47477116593942054\n",
      "Epoch 263, Loss: 0.474752914637718\n",
      "Epoch 264, Loss: 0.47457458619766507\n",
      "Epoch 265, Loss: 0.47441594770111134\n",
      "Epoch 266, Loss: 0.4743326895520222\n",
      "Epoch 267, Loss: 0.47424442089802893\n",
      "Epoch 268, Loss: 0.47409961152836255\n",
      "Epoch 269, Loss: 0.47397674971911014\n",
      "Epoch 270, Loss: 0.4739006464998255\n",
      "Epoch 271, Loss: 0.4738391701966867\n",
      "Epoch 272, Loss: 0.4737234441817917\n",
      "Epoch 273, Loss: 0.47358564948211956\n",
      "Epoch 274, Loss: 0.47351060057458727\n",
      "Epoch 275, Loss: 0.4734065036258047\n",
      "Epoch 276, Loss: 0.4733269559263822\n",
      "Epoch 277, Loss: 0.4732341071370639\n",
      "Epoch 278, Loss: 0.4731407519682655\n",
      "Epoch 279, Loss: 0.4730939966069229\n",
      "Epoch 280, Loss: 0.47298024635218033\n",
      "Epoch 281, Loss: 0.47280886054292864\n",
      "Epoch 282, Loss: 0.47271222185116163\n",
      "Epoch 283, Loss: 0.47268730250728586\n",
      "Epoch 284, Loss: 0.4725869656059248\n",
      "Epoch 285, Loss: 0.4724867493169173\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2773779360688165\n",
      "Valence RMSE: 0.2548806195202512\n",
      "Arousal RMSE: 0.2981826766070857\n",
      "Test R^2 score: tensor([0.2127, 0.1196], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1190, 0.0149], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.16615639361453616\n",
      "Num of epochs: 286\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Epoch 121, Loss: 0.5116998508836337\n",
      "Epoch 122, Loss: 0.5111129902664598\n",
      "Epoch 123, Loss: 0.5105405741436312\n",
      "Epoch 124, Loss: 0.5099849010083686\n",
      "Epoch 125, Loss: 0.5094623759868415\n",
      "Epoch 126, Loss: 0.5089652841754122\n",
      "Epoch 127, Loss: 0.5084717799132713\n",
      "Epoch 128, Loss: 0.5079809643002384\n",
      "Epoch 129, Loss: 0.507507731608986\n",
      "Epoch 130, Loss: 0.5070515727009828\n",
      "Epoch 131, Loss: 0.5065962678643066\n",
      "Epoch 132, Loss: 0.5061448812256618\n",
      "Epoch 133, Loss: 0.5057069703510908\n",
      "Epoch 134, Loss: 0.5052756693952567\n",
      "Epoch 135, Loss: 0.5048445017462813\n",
      "Epoch 136, Loss: 0.504425284231979\n",
      "Epoch 137, Loss: 0.504022717877823\n",
      "Epoch 138, Loss: 0.5036312802042249\n",
      "Epoch 139, Loss: 0.5032368731068207\n",
      "Epoch 140, Loss: 0.5028511060044271\n",
      "Epoch 141, Loss: 0.5024804933282331\n",
      "Epoch 142, Loss: 0.5021182727396313\n",
      "Epoch 143, Loss: 0.5017581664990297\n",
      "Epoch 144, Loss: 0.5014050827892457\n",
      "Epoch 145, Loss: 0.501047675902349\n",
      "Epoch 146, Loss: 0.500690966247362\n",
      "Epoch 147, Loss: 0.5003476722262875\n",
      "Epoch 148, Loss: 0.5000068544871652\n",
      "Epoch 149, Loss: 0.49966338867073357\n",
      "Epoch 150, Loss: 0.4993280873087794\n",
      "Epoch 151, Loss: 0.49900257940428383\n",
      "Epoch 152, Loss: 0.4986865554458311\n",
      "Epoch 153, Loss: 0.49836949389755036\n",
      "Epoch 154, Loss: 0.49805375636713634\n",
      "Epoch 155, Loss: 0.49773873165141763\n",
      "Epoch 156, Loss: 0.49742224924400874\n",
      "Epoch 157, Loss: 0.49711425826812494\n",
      "Epoch 158, Loss: 0.4968010973402242\n",
      "Epoch 159, Loss: 0.4964901399315246\n",
      "Epoch 160, Loss: 0.49617675026847124\n",
      "Epoch 161, Loss: 0.4958670390985117\n",
      "Epoch 162, Loss: 0.4955578109301908\n",
      "Epoch 163, Loss: 0.4952510223984696\n",
      "Epoch 164, Loss: 0.49494577484111674\n",
      "Epoch 165, Loss: 0.49464511374066544\n",
      "Epoch 166, Loss: 0.49434383269975535\n",
      "Epoch 167, Loss: 0.49404631909698943\n",
      "Epoch 168, Loss: 0.49374530645143583\n",
      "Epoch 169, Loss: 0.49345064806438055\n",
      "Epoch 170, Loss: 0.49317001490258333\n",
      "Epoch 171, Loss: 0.4928927591216884\n",
      "Epoch 172, Loss: 0.4926138499628376\n",
      "Epoch 173, Loss: 0.49233528219511824\n",
      "Epoch 174, Loss: 0.49205852514110515\n",
      "Epoch 175, Loss: 0.4917773399684819\n",
      "Epoch 176, Loss: 0.49149629710928233\n",
      "Epoch 177, Loss: 0.4912165647150265\n",
      "Epoch 178, Loss: 0.49094181765497674\n",
      "Epoch 179, Loss: 0.49066901222163783\n",
      "Epoch 180, Loss: 0.490393928005003\n",
      "Epoch 181, Loss: 0.4901079113527018\n",
      "Epoch 182, Loss: 0.48981984154671765\n",
      "Epoch 183, Loss: 0.4895234443341763\n",
      "Epoch 184, Loss: 0.48921459258368105\n",
      "Epoch 185, Loss: 0.4889145977883947\n",
      "Epoch 186, Loss: 0.4886175142174919\n",
      "Epoch 187, Loss: 0.4883282295514753\n",
      "Epoch 188, Loss: 0.4880435212302353\n",
      "Epoch 189, Loss: 0.48775699700499114\n",
      "Epoch 190, Loss: 0.48748008613994775\n",
      "Epoch 191, Loss: 0.4872132179171775\n",
      "Epoch 192, Loss: 0.48694738158534456\n",
      "Epoch 193, Loss: 0.48668121634032785\n",
      "Epoch 194, Loss: 0.4864280322301768\n",
      "Epoch 195, Loss: 0.486178532155915\n",
      "Epoch 196, Loss: 0.4859301919172944\n",
      "Epoch 197, Loss: 0.48568442460989747\n",
      "Epoch 198, Loss: 0.48544224711238787\n",
      "Epoch 199, Loss: 0.48519729219516017\n",
      "Epoch 200, Loss: 0.4849575446659644\n",
      "Epoch 201, Loss: 0.4847146197220469\n",
      "Epoch 202, Loss: 0.48447155759133576\n",
      "Epoch 203, Loss: 0.48422783492589483\n",
      "Epoch 204, Loss: 0.48397707745066637\n",
      "Epoch 205, Loss: 0.483737926529631\n",
      "Epoch 206, Loss: 0.4835020782648923\n",
      "Epoch 207, Loss: 0.48327083252052233\n",
      "Epoch 208, Loss: 0.4830508282762034\n",
      "Epoch 209, Loss: 0.48283269895503794\n",
      "Epoch 210, Loss: 0.48261215534620283\n",
      "Epoch 211, Loss: 0.48240746542225255\n",
      "Epoch 212, Loss: 0.4822187420754367\n",
      "Epoch 213, Loss: 0.48202997575341067\n",
      "Epoch 214, Loss: 0.4818441197783379\n",
      "Epoch 215, Loss: 0.4816502256898402\n",
      "Epoch 216, Loss: 0.4814589616492725\n",
      "Epoch 217, Loss: 0.48127338055379854\n",
      "Epoch 218, Loss: 0.4810898650639108\n",
      "Epoch 219, Loss: 0.48091365405597575\n",
      "Epoch 220, Loss: 0.480720996546175\n",
      "Epoch 221, Loss: 0.48054865040929456\n",
      "Epoch 222, Loss: 0.4803774677186426\n",
      "Epoch 223, Loss: 0.4801885671816001\n",
      "Epoch 224, Loss: 0.48000935557804647\n",
      "Epoch 225, Loss: 0.4798350303004208\n",
      "Epoch 226, Loss: 0.47967842665052907\n",
      "Epoch 227, Loss: 0.47947854451894606\n",
      "Epoch 228, Loss: 0.47928199900429136\n",
      "Epoch 229, Loss: 0.4791424752085709\n",
      "Epoch 230, Loss: 0.4789254904984497\n",
      "Epoch 231, Loss: 0.4787155823499749\n",
      "Epoch 232, Loss: 0.4785186611779873\n",
      "Epoch 233, Loss: 0.4783287773458093\n",
      "Epoch 234, Loss: 0.4781774454711345\n",
      "Epoch 235, Loss: 0.4780410437330356\n",
      "Epoch 236, Loss: 0.4778263811299359\n",
      "Epoch 237, Loss: 0.47765285017795805\n",
      "Epoch 238, Loss: 0.4775436338740249\n",
      "Epoch 239, Loss: 0.4774342833479412\n",
      "Epoch 240, Loss: 0.477263591547843\n",
      "Epoch 241, Loss: 0.47707859610269837\n",
      "Epoch 242, Loss: 0.47697972966255553\n",
      "Epoch 243, Loss: 0.4768431884384324\n",
      "Epoch 244, Loss: 0.4767408349977067\n",
      "Epoch 245, Loss: 0.4765819952057135\n",
      "Epoch 246, Loss: 0.4764706413665339\n",
      "Epoch 247, Loss: 0.47634462160009333\n",
      "Epoch 248, Loss: 0.4762527991635621\n",
      "Epoch 249, Loss: 0.4762014052862575\n",
      "Epoch 250, Loss: 0.4760283306209857\n",
      "Epoch 251, Loss: 0.47592260832103706\n",
      "Epoch 252, Loss: 0.475834869470249\n",
      "Epoch 253, Loss: 0.47570980894919\n",
      "Epoch 254, Loss: 0.47559397414543253\n",
      "Epoch 255, Loss: 0.4755005182088922\n",
      "Epoch 256, Loss: 0.4753894909333468\n",
      "Epoch 257, Loss: 0.47527958208571863\n",
      "Epoch 258, Loss: 0.47517947896910767\n",
      "Epoch 259, Loss: 0.4750690667233416\n",
      "Epoch 260, Loss: 0.47496625253354696\n",
      "Epoch 261, Loss: 0.47486506352442986\n",
      "Epoch 262, Loss: 0.47477116593942054\n",
      "Epoch 263, Loss: 0.474752914637718\n",
      "Epoch 264, Loss: 0.47457458619766507\n",
      "Epoch 265, Loss: 0.47441594770111134\n",
      "Epoch 266, Loss: 0.4743326895520222\n",
      "Epoch 267, Loss: 0.47424442089802893\n",
      "Epoch 268, Loss: 0.47409961152836255\n",
      "Epoch 269, Loss: 0.47397674971911014\n",
      "Epoch 270, Loss: 0.4739006464998255\n",
      "Epoch 271, Loss: 0.4738391701966867\n",
      "Epoch 272, Loss: 0.4737234441817917\n",
      "Epoch 273, Loss: 0.47358564948211956\n",
      "Epoch 274, Loss: 0.47351060057458727\n",
      "Epoch 275, Loss: 0.4734065036258047\n",
      "Epoch 276, Loss: 0.4733269559263822\n",
      "Epoch 277, Loss: 0.4732341071370639\n",
      "Epoch 278, Loss: 0.4731407519682655\n",
      "Epoch 279, Loss: 0.4730939966069229\n",
      "Epoch 280, Loss: 0.47298024635218033\n",
      "Epoch 281, Loss: 0.47280886054292864\n",
      "Epoch 282, Loss: 0.47271222185116163\n",
      "Epoch 283, Loss: 0.47268730250728586\n",
      "Epoch 284, Loss: 0.4725869656059248\n",
      "Epoch 285, Loss: 0.4724867493169173\n",
      "Epoch 286, Loss: 0.4723455337808729\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.27749886553197833\n",
      "Valence RMSE: 0.25495302061841085\n",
      "Arousal RMSE: 0.2983457692353256\n",
      "Test R^2 score: tensor([0.2123, 0.1186], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1185, 0.0138], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.16545105808255445\n",
      "Num of epochs: 287\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Epoch 121, Loss: 0.5116998508836337\n",
      "Epoch 122, Loss: 0.5111129902664598\n",
      "Epoch 123, Loss: 0.5105405741436312\n",
      "Epoch 124, Loss: 0.5099849010083686\n",
      "Epoch 125, Loss: 0.5094623759868415\n",
      "Epoch 126, Loss: 0.5089652841754122\n",
      "Epoch 127, Loss: 0.5084717799132713\n",
      "Epoch 128, Loss: 0.5079809643002384\n",
      "Epoch 129, Loss: 0.507507731608986\n",
      "Epoch 130, Loss: 0.5070515727009828\n",
      "Epoch 131, Loss: 0.5065962678643066\n",
      "Epoch 132, Loss: 0.5061448812256618\n",
      "Epoch 133, Loss: 0.5057069703510908\n",
      "Epoch 134, Loss: 0.5052756693952567\n",
      "Epoch 135, Loss: 0.5048445017462813\n",
      "Epoch 136, Loss: 0.504425284231979\n",
      "Epoch 137, Loss: 0.504022717877823\n",
      "Epoch 138, Loss: 0.5036312802042249\n",
      "Epoch 139, Loss: 0.5032368731068207\n",
      "Epoch 140, Loss: 0.5028511060044271\n",
      "Epoch 141, Loss: 0.5024804933282331\n",
      "Epoch 142, Loss: 0.5021182727396313\n",
      "Epoch 143, Loss: 0.5017581664990297\n",
      "Epoch 144, Loss: 0.5014050827892457\n",
      "Epoch 145, Loss: 0.501047675902349\n",
      "Epoch 146, Loss: 0.500690966247362\n",
      "Epoch 147, Loss: 0.5003476722262875\n",
      "Epoch 148, Loss: 0.5000068544871652\n",
      "Epoch 149, Loss: 0.49966338867073357\n",
      "Epoch 150, Loss: 0.4993280873087794\n",
      "Epoch 151, Loss: 0.49900257940428383\n",
      "Epoch 152, Loss: 0.4986865554458311\n",
      "Epoch 153, Loss: 0.49836949389755036\n",
      "Epoch 154, Loss: 0.49805375636713634\n",
      "Epoch 155, Loss: 0.49773873165141763\n",
      "Epoch 156, Loss: 0.49742224924400874\n",
      "Epoch 157, Loss: 0.49711425826812494\n",
      "Epoch 158, Loss: 0.4968010973402242\n",
      "Epoch 159, Loss: 0.4964901399315246\n",
      "Epoch 160, Loss: 0.49617675026847124\n",
      "Epoch 161, Loss: 0.4958670390985117\n",
      "Epoch 162, Loss: 0.4955578109301908\n",
      "Epoch 163, Loss: 0.4952510223984696\n",
      "Epoch 164, Loss: 0.49494577484111674\n",
      "Epoch 165, Loss: 0.49464511374066544\n",
      "Epoch 166, Loss: 0.49434383269975535\n",
      "Epoch 167, Loss: 0.49404631909698943\n",
      "Epoch 168, Loss: 0.49374530645143583\n",
      "Epoch 169, Loss: 0.49345064806438055\n",
      "Epoch 170, Loss: 0.49317001490258333\n",
      "Epoch 171, Loss: 0.4928927591216884\n",
      "Epoch 172, Loss: 0.4926138499628376\n",
      "Epoch 173, Loss: 0.49233528219511824\n",
      "Epoch 174, Loss: 0.49205852514110515\n",
      "Epoch 175, Loss: 0.4917773399684819\n",
      "Epoch 176, Loss: 0.49149629710928233\n",
      "Epoch 177, Loss: 0.4912165647150265\n",
      "Epoch 178, Loss: 0.49094181765497674\n",
      "Epoch 179, Loss: 0.49066901222163783\n",
      "Epoch 180, Loss: 0.490393928005003\n",
      "Epoch 181, Loss: 0.4901079113527018\n",
      "Epoch 182, Loss: 0.48981984154671765\n",
      "Epoch 183, Loss: 0.4895234443341763\n",
      "Epoch 184, Loss: 0.48921459258368105\n",
      "Epoch 185, Loss: 0.4889145977883947\n",
      "Epoch 186, Loss: 0.4886175142174919\n",
      "Epoch 187, Loss: 0.4883282295514753\n",
      "Epoch 188, Loss: 0.4880435212302353\n",
      "Epoch 189, Loss: 0.48775699700499114\n",
      "Epoch 190, Loss: 0.48748008613994775\n",
      "Epoch 191, Loss: 0.4872132179171775\n",
      "Epoch 192, Loss: 0.48694738158534456\n",
      "Epoch 193, Loss: 0.48668121634032785\n",
      "Epoch 194, Loss: 0.4864280322301768\n",
      "Epoch 195, Loss: 0.486178532155915\n",
      "Epoch 196, Loss: 0.4859301919172944\n",
      "Epoch 197, Loss: 0.48568442460989747\n",
      "Epoch 198, Loss: 0.48544224711238787\n",
      "Epoch 199, Loss: 0.48519729219516017\n",
      "Epoch 200, Loss: 0.4849575446659644\n",
      "Epoch 201, Loss: 0.4847146197220469\n",
      "Epoch 202, Loss: 0.48447155759133576\n",
      "Epoch 203, Loss: 0.48422783492589483\n",
      "Epoch 204, Loss: 0.48397707745066637\n",
      "Epoch 205, Loss: 0.483737926529631\n",
      "Epoch 206, Loss: 0.4835020782648923\n",
      "Epoch 207, Loss: 0.48327083252052233\n",
      "Epoch 208, Loss: 0.4830508282762034\n",
      "Epoch 209, Loss: 0.48283269895503794\n",
      "Epoch 210, Loss: 0.48261215534620283\n",
      "Epoch 211, Loss: 0.48240746542225255\n",
      "Epoch 212, Loss: 0.4822187420754367\n",
      "Epoch 213, Loss: 0.48202997575341067\n",
      "Epoch 214, Loss: 0.4818441197783379\n",
      "Epoch 215, Loss: 0.4816502256898402\n",
      "Epoch 216, Loss: 0.4814589616492725\n",
      "Epoch 217, Loss: 0.48127338055379854\n",
      "Epoch 218, Loss: 0.4810898650639108\n",
      "Epoch 219, Loss: 0.48091365405597575\n",
      "Epoch 220, Loss: 0.480720996546175\n",
      "Epoch 221, Loss: 0.48054865040929456\n",
      "Epoch 222, Loss: 0.4803774677186426\n",
      "Epoch 223, Loss: 0.4801885671816001\n",
      "Epoch 224, Loss: 0.48000935557804647\n",
      "Epoch 225, Loss: 0.4798350303004208\n",
      "Epoch 226, Loss: 0.47967842665052907\n",
      "Epoch 227, Loss: 0.47947854451894606\n",
      "Epoch 228, Loss: 0.47928199900429136\n",
      "Epoch 229, Loss: 0.4791424752085709\n",
      "Epoch 230, Loss: 0.4789254904984497\n",
      "Epoch 231, Loss: 0.4787155823499749\n",
      "Epoch 232, Loss: 0.4785186611779873\n",
      "Epoch 233, Loss: 0.4783287773458093\n",
      "Epoch 234, Loss: 0.4781774454711345\n",
      "Epoch 235, Loss: 0.4780410437330356\n",
      "Epoch 236, Loss: 0.4778263811299359\n",
      "Epoch 237, Loss: 0.47765285017795805\n",
      "Epoch 238, Loss: 0.4775436338740249\n",
      "Epoch 239, Loss: 0.4774342833479412\n",
      "Epoch 240, Loss: 0.477263591547843\n",
      "Epoch 241, Loss: 0.47707859610269837\n",
      "Epoch 242, Loss: 0.47697972966255553\n",
      "Epoch 243, Loss: 0.4768431884384324\n",
      "Epoch 244, Loss: 0.4767408349977067\n",
      "Epoch 245, Loss: 0.4765819952057135\n",
      "Epoch 246, Loss: 0.4764706413665339\n",
      "Epoch 247, Loss: 0.47634462160009333\n",
      "Epoch 248, Loss: 0.4762527991635621\n",
      "Epoch 249, Loss: 0.4762014052862575\n",
      "Epoch 250, Loss: 0.4760283306209857\n",
      "Epoch 251, Loss: 0.47592260832103706\n",
      "Epoch 252, Loss: 0.475834869470249\n",
      "Epoch 253, Loss: 0.47570980894919\n",
      "Epoch 254, Loss: 0.47559397414543253\n",
      "Epoch 255, Loss: 0.4755005182088922\n",
      "Epoch 256, Loss: 0.4753894909333468\n",
      "Epoch 257, Loss: 0.47527958208571863\n",
      "Epoch 258, Loss: 0.47517947896910767\n",
      "Epoch 259, Loss: 0.4750690667233416\n",
      "Epoch 260, Loss: 0.47496625253354696\n",
      "Epoch 261, Loss: 0.47486506352442986\n",
      "Epoch 262, Loss: 0.47477116593942054\n",
      "Epoch 263, Loss: 0.474752914637718\n",
      "Epoch 264, Loss: 0.47457458619766507\n",
      "Epoch 265, Loss: 0.47441594770111134\n",
      "Epoch 266, Loss: 0.4743326895520222\n",
      "Epoch 267, Loss: 0.47424442089802893\n",
      "Epoch 268, Loss: 0.47409961152836255\n",
      "Epoch 269, Loss: 0.47397674971911014\n",
      "Epoch 270, Loss: 0.4739006464998255\n",
      "Epoch 271, Loss: 0.4738391701966867\n",
      "Epoch 272, Loss: 0.4737234441817917\n",
      "Epoch 273, Loss: 0.47358564948211956\n",
      "Epoch 274, Loss: 0.47351060057458727\n",
      "Epoch 275, Loss: 0.4734065036258047\n",
      "Epoch 276, Loss: 0.4733269559263822\n",
      "Epoch 277, Loss: 0.4732341071370639\n",
      "Epoch 278, Loss: 0.4731407519682655\n",
      "Epoch 279, Loss: 0.4730939966069229\n",
      "Epoch 280, Loss: 0.47298024635218033\n",
      "Epoch 281, Loss: 0.47280886054292864\n",
      "Epoch 282, Loss: 0.47271222185116163\n",
      "Epoch 283, Loss: 0.47268730250728586\n",
      "Epoch 284, Loss: 0.4725869656059248\n",
      "Epoch 285, Loss: 0.4724867493169173\n",
      "Epoch 286, Loss: 0.4723455337808729\n",
      "Epoch 287, Loss: 0.4722632042859861\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2775243240452795\n",
      "Valence RMSE: 0.25506817751381555\n",
      "Arousal RMSE: 0.2982946960530128\n",
      "Test R^2 score: tensor([0.2115, 0.1190], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1177, 0.0141], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.16524603248960484\n",
      "Num of epochs: 288\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Epoch 121, Loss: 0.5116998508836337\n",
      "Epoch 122, Loss: 0.5111129902664598\n",
      "Epoch 123, Loss: 0.5105405741436312\n",
      "Epoch 124, Loss: 0.5099849010083686\n",
      "Epoch 125, Loss: 0.5094623759868415\n",
      "Epoch 126, Loss: 0.5089652841754122\n",
      "Epoch 127, Loss: 0.5084717799132713\n",
      "Epoch 128, Loss: 0.5079809643002384\n",
      "Epoch 129, Loss: 0.507507731608986\n",
      "Epoch 130, Loss: 0.5070515727009828\n",
      "Epoch 131, Loss: 0.5065962678643066\n",
      "Epoch 132, Loss: 0.5061448812256618\n",
      "Epoch 133, Loss: 0.5057069703510908\n",
      "Epoch 134, Loss: 0.5052756693952567\n",
      "Epoch 135, Loss: 0.5048445017462813\n",
      "Epoch 136, Loss: 0.504425284231979\n",
      "Epoch 137, Loss: 0.504022717877823\n",
      "Epoch 138, Loss: 0.5036312802042249\n",
      "Epoch 139, Loss: 0.5032368731068207\n",
      "Epoch 140, Loss: 0.5028511060044271\n",
      "Epoch 141, Loss: 0.5024804933282331\n",
      "Epoch 142, Loss: 0.5021182727396313\n",
      "Epoch 143, Loss: 0.5017581664990297\n",
      "Epoch 144, Loss: 0.5014050827892457\n",
      "Epoch 145, Loss: 0.501047675902349\n",
      "Epoch 146, Loss: 0.500690966247362\n",
      "Epoch 147, Loss: 0.5003476722262875\n",
      "Epoch 148, Loss: 0.5000068544871652\n",
      "Epoch 149, Loss: 0.49966338867073357\n",
      "Epoch 150, Loss: 0.4993280873087794\n",
      "Epoch 151, Loss: 0.49900257940428383\n",
      "Epoch 152, Loss: 0.4986865554458311\n",
      "Epoch 153, Loss: 0.49836949389755036\n",
      "Epoch 154, Loss: 0.49805375636713634\n",
      "Epoch 155, Loss: 0.49773873165141763\n",
      "Epoch 156, Loss: 0.49742224924400874\n",
      "Epoch 157, Loss: 0.49711425826812494\n",
      "Epoch 158, Loss: 0.4968010973402242\n",
      "Epoch 159, Loss: 0.4964901399315246\n",
      "Epoch 160, Loss: 0.49617675026847124\n",
      "Epoch 161, Loss: 0.4958670390985117\n",
      "Epoch 162, Loss: 0.4955578109301908\n",
      "Epoch 163, Loss: 0.4952510223984696\n",
      "Epoch 164, Loss: 0.49494577484111674\n",
      "Epoch 165, Loss: 0.49464511374066544\n",
      "Epoch 166, Loss: 0.49434383269975535\n",
      "Epoch 167, Loss: 0.49404631909698943\n",
      "Epoch 168, Loss: 0.49374530645143583\n",
      "Epoch 169, Loss: 0.49345064806438055\n",
      "Epoch 170, Loss: 0.49317001490258333\n",
      "Epoch 171, Loss: 0.4928927591216884\n",
      "Epoch 172, Loss: 0.4926138499628376\n",
      "Epoch 173, Loss: 0.49233528219511824\n",
      "Epoch 174, Loss: 0.49205852514110515\n",
      "Epoch 175, Loss: 0.4917773399684819\n",
      "Epoch 176, Loss: 0.49149629710928233\n",
      "Epoch 177, Loss: 0.4912165647150265\n",
      "Epoch 178, Loss: 0.49094181765497674\n",
      "Epoch 179, Loss: 0.49066901222163783\n",
      "Epoch 180, Loss: 0.490393928005003\n",
      "Epoch 181, Loss: 0.4901079113527018\n",
      "Epoch 182, Loss: 0.48981984154671765\n",
      "Epoch 183, Loss: 0.4895234443341763\n",
      "Epoch 184, Loss: 0.48921459258368105\n",
      "Epoch 185, Loss: 0.4889145977883947\n",
      "Epoch 186, Loss: 0.4886175142174919\n",
      "Epoch 187, Loss: 0.4883282295514753\n",
      "Epoch 188, Loss: 0.4880435212302353\n",
      "Epoch 189, Loss: 0.48775699700499114\n",
      "Epoch 190, Loss: 0.48748008613994775\n",
      "Epoch 191, Loss: 0.4872132179171775\n",
      "Epoch 192, Loss: 0.48694738158534456\n",
      "Epoch 193, Loss: 0.48668121634032785\n",
      "Epoch 194, Loss: 0.4864280322301768\n",
      "Epoch 195, Loss: 0.486178532155915\n",
      "Epoch 196, Loss: 0.4859301919172944\n",
      "Epoch 197, Loss: 0.48568442460989747\n",
      "Epoch 198, Loss: 0.48544224711238787\n",
      "Epoch 199, Loss: 0.48519729219516017\n",
      "Epoch 200, Loss: 0.4849575446659644\n",
      "Epoch 201, Loss: 0.4847146197220469\n",
      "Epoch 202, Loss: 0.48447155759133576\n",
      "Epoch 203, Loss: 0.48422783492589483\n",
      "Epoch 204, Loss: 0.48397707745066637\n",
      "Epoch 205, Loss: 0.483737926529631\n",
      "Epoch 206, Loss: 0.4835020782648923\n",
      "Epoch 207, Loss: 0.48327083252052233\n",
      "Epoch 208, Loss: 0.4830508282762034\n",
      "Epoch 209, Loss: 0.48283269895503794\n",
      "Epoch 210, Loss: 0.48261215534620283\n",
      "Epoch 211, Loss: 0.48240746542225255\n",
      "Epoch 212, Loss: 0.4822187420754367\n",
      "Epoch 213, Loss: 0.48202997575341067\n",
      "Epoch 214, Loss: 0.4818441197783379\n",
      "Epoch 215, Loss: 0.4816502256898402\n",
      "Epoch 216, Loss: 0.4814589616492725\n",
      "Epoch 217, Loss: 0.48127338055379854\n",
      "Epoch 218, Loss: 0.4810898650639108\n",
      "Epoch 219, Loss: 0.48091365405597575\n",
      "Epoch 220, Loss: 0.480720996546175\n",
      "Epoch 221, Loss: 0.48054865040929456\n",
      "Epoch 222, Loss: 0.4803774677186426\n",
      "Epoch 223, Loss: 0.4801885671816001\n",
      "Epoch 224, Loss: 0.48000935557804647\n",
      "Epoch 225, Loss: 0.4798350303004208\n",
      "Epoch 226, Loss: 0.47967842665052907\n",
      "Epoch 227, Loss: 0.47947854451894606\n",
      "Epoch 228, Loss: 0.47928199900429136\n",
      "Epoch 229, Loss: 0.4791424752085709\n",
      "Epoch 230, Loss: 0.4789254904984497\n",
      "Epoch 231, Loss: 0.4787155823499749\n",
      "Epoch 232, Loss: 0.4785186611779873\n",
      "Epoch 233, Loss: 0.4783287773458093\n",
      "Epoch 234, Loss: 0.4781774454711345\n",
      "Epoch 235, Loss: 0.4780410437330356\n",
      "Epoch 236, Loss: 0.4778263811299359\n",
      "Epoch 237, Loss: 0.47765285017795805\n",
      "Epoch 238, Loss: 0.4775436338740249\n",
      "Epoch 239, Loss: 0.4774342833479412\n",
      "Epoch 240, Loss: 0.477263591547843\n",
      "Epoch 241, Loss: 0.47707859610269837\n",
      "Epoch 242, Loss: 0.47697972966255553\n",
      "Epoch 243, Loss: 0.4768431884384324\n",
      "Epoch 244, Loss: 0.4767408349977067\n",
      "Epoch 245, Loss: 0.4765819952057135\n",
      "Epoch 246, Loss: 0.4764706413665339\n",
      "Epoch 247, Loss: 0.47634462160009333\n",
      "Epoch 248, Loss: 0.4762527991635621\n",
      "Epoch 249, Loss: 0.4762014052862575\n",
      "Epoch 250, Loss: 0.4760283306209857\n",
      "Epoch 251, Loss: 0.47592260832103706\n",
      "Epoch 252, Loss: 0.475834869470249\n",
      "Epoch 253, Loss: 0.47570980894919\n",
      "Epoch 254, Loss: 0.47559397414543253\n",
      "Epoch 255, Loss: 0.4755005182088922\n",
      "Epoch 256, Loss: 0.4753894909333468\n",
      "Epoch 257, Loss: 0.47527958208571863\n",
      "Epoch 258, Loss: 0.47517947896910767\n",
      "Epoch 259, Loss: 0.4750690667233416\n",
      "Epoch 260, Loss: 0.47496625253354696\n",
      "Epoch 261, Loss: 0.47486506352442986\n",
      "Epoch 262, Loss: 0.47477116593942054\n",
      "Epoch 263, Loss: 0.474752914637718\n",
      "Epoch 264, Loss: 0.47457458619766507\n",
      "Epoch 265, Loss: 0.47441594770111134\n",
      "Epoch 266, Loss: 0.4743326895520222\n",
      "Epoch 267, Loss: 0.47424442089802893\n",
      "Epoch 268, Loss: 0.47409961152836255\n",
      "Epoch 269, Loss: 0.47397674971911014\n",
      "Epoch 270, Loss: 0.4739006464998255\n",
      "Epoch 271, Loss: 0.4738391701966867\n",
      "Epoch 272, Loss: 0.4737234441817917\n",
      "Epoch 273, Loss: 0.47358564948211956\n",
      "Epoch 274, Loss: 0.47351060057458727\n",
      "Epoch 275, Loss: 0.4734065036258047\n",
      "Epoch 276, Loss: 0.4733269559263822\n",
      "Epoch 277, Loss: 0.4732341071370639\n",
      "Epoch 278, Loss: 0.4731407519682655\n",
      "Epoch 279, Loss: 0.4730939966069229\n",
      "Epoch 280, Loss: 0.47298024635218033\n",
      "Epoch 281, Loss: 0.47280886054292864\n",
      "Epoch 282, Loss: 0.47271222185116163\n",
      "Epoch 283, Loss: 0.47268730250728586\n",
      "Epoch 284, Loss: 0.4725869656059248\n",
      "Epoch 285, Loss: 0.4724867493169173\n",
      "Epoch 286, Loss: 0.4723455337808729\n",
      "Epoch 287, Loss: 0.4722632042859861\n",
      "Epoch 288, Loss: 0.4722048755884816\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.277332677161048\n",
      "Valence RMSE: 0.2547550480575301\n",
      "Arousal RMSE: 0.29820578990327085\n",
      "Test R^2 score: tensor([0.2135, 0.1195], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1199, 0.0147], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.16647593053778487\n",
      "Num of epochs: 289\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Epoch 121, Loss: 0.5116998508836337\n",
      "Epoch 122, Loss: 0.5111129902664598\n",
      "Epoch 123, Loss: 0.5105405741436312\n",
      "Epoch 124, Loss: 0.5099849010083686\n",
      "Epoch 125, Loss: 0.5094623759868415\n",
      "Epoch 126, Loss: 0.5089652841754122\n",
      "Epoch 127, Loss: 0.5084717799132713\n",
      "Epoch 128, Loss: 0.5079809643002384\n",
      "Epoch 129, Loss: 0.507507731608986\n",
      "Epoch 130, Loss: 0.5070515727009828\n",
      "Epoch 131, Loss: 0.5065962678643066\n",
      "Epoch 132, Loss: 0.5061448812256618\n",
      "Epoch 133, Loss: 0.5057069703510908\n",
      "Epoch 134, Loss: 0.5052756693952567\n",
      "Epoch 135, Loss: 0.5048445017462813\n",
      "Epoch 136, Loss: 0.504425284231979\n",
      "Epoch 137, Loss: 0.504022717877823\n",
      "Epoch 138, Loss: 0.5036312802042249\n",
      "Epoch 139, Loss: 0.5032368731068207\n",
      "Epoch 140, Loss: 0.5028511060044271\n",
      "Epoch 141, Loss: 0.5024804933282331\n",
      "Epoch 142, Loss: 0.5021182727396313\n",
      "Epoch 143, Loss: 0.5017581664990297\n",
      "Epoch 144, Loss: 0.5014050827892457\n",
      "Epoch 145, Loss: 0.501047675902349\n",
      "Epoch 146, Loss: 0.500690966247362\n",
      "Epoch 147, Loss: 0.5003476722262875\n",
      "Epoch 148, Loss: 0.5000068544871652\n",
      "Epoch 149, Loss: 0.49966338867073357\n",
      "Epoch 150, Loss: 0.4993280873087794\n",
      "Epoch 151, Loss: 0.49900257940428383\n",
      "Epoch 152, Loss: 0.4986865554458311\n",
      "Epoch 153, Loss: 0.49836949389755036\n",
      "Epoch 154, Loss: 0.49805375636713634\n",
      "Epoch 155, Loss: 0.49773873165141763\n",
      "Epoch 156, Loss: 0.49742224924400874\n",
      "Epoch 157, Loss: 0.49711425826812494\n",
      "Epoch 158, Loss: 0.4968010973402242\n",
      "Epoch 159, Loss: 0.4964901399315246\n",
      "Epoch 160, Loss: 0.49617675026847124\n",
      "Epoch 161, Loss: 0.4958670390985117\n",
      "Epoch 162, Loss: 0.4955578109301908\n",
      "Epoch 163, Loss: 0.4952510223984696\n",
      "Epoch 164, Loss: 0.49494577484111674\n",
      "Epoch 165, Loss: 0.49464511374066544\n",
      "Epoch 166, Loss: 0.49434383269975535\n",
      "Epoch 167, Loss: 0.49404631909698943\n",
      "Epoch 168, Loss: 0.49374530645143583\n",
      "Epoch 169, Loss: 0.49345064806438055\n",
      "Epoch 170, Loss: 0.49317001490258333\n",
      "Epoch 171, Loss: 0.4928927591216884\n",
      "Epoch 172, Loss: 0.4926138499628376\n",
      "Epoch 173, Loss: 0.49233528219511824\n",
      "Epoch 174, Loss: 0.49205852514110515\n",
      "Epoch 175, Loss: 0.4917773399684819\n",
      "Epoch 176, Loss: 0.49149629710928233\n",
      "Epoch 177, Loss: 0.4912165647150265\n",
      "Epoch 178, Loss: 0.49094181765497674\n",
      "Epoch 179, Loss: 0.49066901222163783\n",
      "Epoch 180, Loss: 0.490393928005003\n",
      "Epoch 181, Loss: 0.4901079113527018\n",
      "Epoch 182, Loss: 0.48981984154671765\n",
      "Epoch 183, Loss: 0.4895234443341763\n",
      "Epoch 184, Loss: 0.48921459258368105\n",
      "Epoch 185, Loss: 0.4889145977883947\n",
      "Epoch 186, Loss: 0.4886175142174919\n",
      "Epoch 187, Loss: 0.4883282295514753\n",
      "Epoch 188, Loss: 0.4880435212302353\n",
      "Epoch 189, Loss: 0.48775699700499114\n",
      "Epoch 190, Loss: 0.48748008613994775\n",
      "Epoch 191, Loss: 0.4872132179171775\n",
      "Epoch 192, Loss: 0.48694738158534456\n",
      "Epoch 193, Loss: 0.48668121634032785\n",
      "Epoch 194, Loss: 0.4864280322301768\n",
      "Epoch 195, Loss: 0.486178532155915\n",
      "Epoch 196, Loss: 0.4859301919172944\n",
      "Epoch 197, Loss: 0.48568442460989747\n",
      "Epoch 198, Loss: 0.48544224711238787\n",
      "Epoch 199, Loss: 0.48519729219516017\n",
      "Epoch 200, Loss: 0.4849575446659644\n",
      "Epoch 201, Loss: 0.4847146197220469\n",
      "Epoch 202, Loss: 0.48447155759133576\n",
      "Epoch 203, Loss: 0.48422783492589483\n",
      "Epoch 204, Loss: 0.48397707745066637\n",
      "Epoch 205, Loss: 0.483737926529631\n",
      "Epoch 206, Loss: 0.4835020782648923\n",
      "Epoch 207, Loss: 0.48327083252052233\n",
      "Epoch 208, Loss: 0.4830508282762034\n",
      "Epoch 209, Loss: 0.48283269895503794\n",
      "Epoch 210, Loss: 0.48261215534620283\n",
      "Epoch 211, Loss: 0.48240746542225255\n",
      "Epoch 212, Loss: 0.4822187420754367\n",
      "Epoch 213, Loss: 0.48202997575341067\n",
      "Epoch 214, Loss: 0.4818441197783379\n",
      "Epoch 215, Loss: 0.4816502256898402\n",
      "Epoch 216, Loss: 0.4814589616492725\n",
      "Epoch 217, Loss: 0.48127338055379854\n",
      "Epoch 218, Loss: 0.4810898650639108\n",
      "Epoch 219, Loss: 0.48091365405597575\n",
      "Epoch 220, Loss: 0.480720996546175\n",
      "Epoch 221, Loss: 0.48054865040929456\n",
      "Epoch 222, Loss: 0.4803774677186426\n",
      "Epoch 223, Loss: 0.4801885671816001\n",
      "Epoch 224, Loss: 0.48000935557804647\n",
      "Epoch 225, Loss: 0.4798350303004208\n",
      "Epoch 226, Loss: 0.47967842665052907\n",
      "Epoch 227, Loss: 0.47947854451894606\n",
      "Epoch 228, Loss: 0.47928199900429136\n",
      "Epoch 229, Loss: 0.4791424752085709\n",
      "Epoch 230, Loss: 0.4789254904984497\n",
      "Epoch 231, Loss: 0.4787155823499749\n",
      "Epoch 232, Loss: 0.4785186611779873\n",
      "Epoch 233, Loss: 0.4783287773458093\n",
      "Epoch 234, Loss: 0.4781774454711345\n",
      "Epoch 235, Loss: 0.4780410437330356\n",
      "Epoch 236, Loss: 0.4778263811299359\n",
      "Epoch 237, Loss: 0.47765285017795805\n",
      "Epoch 238, Loss: 0.4775436338740249\n",
      "Epoch 239, Loss: 0.4774342833479412\n",
      "Epoch 240, Loss: 0.477263591547843\n",
      "Epoch 241, Loss: 0.47707859610269837\n",
      "Epoch 242, Loss: 0.47697972966255553\n",
      "Epoch 243, Loss: 0.4768431884384324\n",
      "Epoch 244, Loss: 0.4767408349977067\n",
      "Epoch 245, Loss: 0.4765819952057135\n",
      "Epoch 246, Loss: 0.4764706413665339\n",
      "Epoch 247, Loss: 0.47634462160009333\n",
      "Epoch 248, Loss: 0.4762527991635621\n",
      "Epoch 249, Loss: 0.4762014052862575\n",
      "Epoch 250, Loss: 0.4760283306209857\n",
      "Epoch 251, Loss: 0.47592260832103706\n",
      "Epoch 252, Loss: 0.475834869470249\n",
      "Epoch 253, Loss: 0.47570980894919\n",
      "Epoch 254, Loss: 0.47559397414543253\n",
      "Epoch 255, Loss: 0.4755005182088922\n",
      "Epoch 256, Loss: 0.4753894909333468\n",
      "Epoch 257, Loss: 0.47527958208571863\n",
      "Epoch 258, Loss: 0.47517947896910767\n",
      "Epoch 259, Loss: 0.4750690667233416\n",
      "Epoch 260, Loss: 0.47496625253354696\n",
      "Epoch 261, Loss: 0.47486506352442986\n",
      "Epoch 262, Loss: 0.47477116593942054\n",
      "Epoch 263, Loss: 0.474752914637718\n",
      "Epoch 264, Loss: 0.47457458619766507\n",
      "Epoch 265, Loss: 0.47441594770111134\n",
      "Epoch 266, Loss: 0.4743326895520222\n",
      "Epoch 267, Loss: 0.47424442089802893\n",
      "Epoch 268, Loss: 0.47409961152836255\n",
      "Epoch 269, Loss: 0.47397674971911014\n",
      "Epoch 270, Loss: 0.4739006464998255\n",
      "Epoch 271, Loss: 0.4738391701966867\n",
      "Epoch 272, Loss: 0.4737234441817917\n",
      "Epoch 273, Loss: 0.47358564948211956\n",
      "Epoch 274, Loss: 0.47351060057458727\n",
      "Epoch 275, Loss: 0.4734065036258047\n",
      "Epoch 276, Loss: 0.4733269559263822\n",
      "Epoch 277, Loss: 0.4732341071370639\n",
      "Epoch 278, Loss: 0.4731407519682655\n",
      "Epoch 279, Loss: 0.4730939966069229\n",
      "Epoch 280, Loss: 0.47298024635218033\n",
      "Epoch 281, Loss: 0.47280886054292864\n",
      "Epoch 282, Loss: 0.47271222185116163\n",
      "Epoch 283, Loss: 0.47268730250728586\n",
      "Epoch 284, Loss: 0.4725869656059248\n",
      "Epoch 285, Loss: 0.4724867493169173\n",
      "Epoch 286, Loss: 0.4723455337808729\n",
      "Epoch 287, Loss: 0.4722632042859861\n",
      "Epoch 288, Loss: 0.4722048755884816\n",
      "Epoch 289, Loss: 0.472146413443235\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2775423588198244\n",
      "Valence RMSE: 0.2550916475892761\n",
      "Arousal RMSE: 0.29830818495095457\n",
      "Test R^2 score: tensor([0.2114, 0.1189], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1176, 0.0140], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.16513363719185703\n",
      "Num of epochs: 290\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Epoch 121, Loss: 0.5116998508836337\n",
      "Epoch 122, Loss: 0.5111129902664598\n",
      "Epoch 123, Loss: 0.5105405741436312\n",
      "Epoch 124, Loss: 0.5099849010083686\n",
      "Epoch 125, Loss: 0.5094623759868415\n",
      "Epoch 126, Loss: 0.5089652841754122\n",
      "Epoch 127, Loss: 0.5084717799132713\n",
      "Epoch 128, Loss: 0.5079809643002384\n",
      "Epoch 129, Loss: 0.507507731608986\n",
      "Epoch 130, Loss: 0.5070515727009828\n",
      "Epoch 131, Loss: 0.5065962678643066\n",
      "Epoch 132, Loss: 0.5061448812256618\n",
      "Epoch 133, Loss: 0.5057069703510908\n",
      "Epoch 134, Loss: 0.5052756693952567\n",
      "Epoch 135, Loss: 0.5048445017462813\n",
      "Epoch 136, Loss: 0.504425284231979\n",
      "Epoch 137, Loss: 0.504022717877823\n",
      "Epoch 138, Loss: 0.5036312802042249\n",
      "Epoch 139, Loss: 0.5032368731068207\n",
      "Epoch 140, Loss: 0.5028511060044271\n",
      "Epoch 141, Loss: 0.5024804933282331\n",
      "Epoch 142, Loss: 0.5021182727396313\n",
      "Epoch 143, Loss: 0.5017581664990297\n",
      "Epoch 144, Loss: 0.5014050827892457\n",
      "Epoch 145, Loss: 0.501047675902349\n",
      "Epoch 146, Loss: 0.500690966247362\n",
      "Epoch 147, Loss: 0.5003476722262875\n",
      "Epoch 148, Loss: 0.5000068544871652\n",
      "Epoch 149, Loss: 0.49966338867073357\n",
      "Epoch 150, Loss: 0.4993280873087794\n",
      "Epoch 151, Loss: 0.49900257940428383\n",
      "Epoch 152, Loss: 0.4986865554458311\n",
      "Epoch 153, Loss: 0.49836949389755036\n",
      "Epoch 154, Loss: 0.49805375636713634\n",
      "Epoch 155, Loss: 0.49773873165141763\n",
      "Epoch 156, Loss: 0.49742224924400874\n",
      "Epoch 157, Loss: 0.49711425826812494\n",
      "Epoch 158, Loss: 0.4968010973402242\n",
      "Epoch 159, Loss: 0.4964901399315246\n",
      "Epoch 160, Loss: 0.49617675026847124\n",
      "Epoch 161, Loss: 0.4958670390985117\n",
      "Epoch 162, Loss: 0.4955578109301908\n",
      "Epoch 163, Loss: 0.4952510223984696\n",
      "Epoch 164, Loss: 0.49494577484111674\n",
      "Epoch 165, Loss: 0.49464511374066544\n",
      "Epoch 166, Loss: 0.49434383269975535\n",
      "Epoch 167, Loss: 0.49404631909698943\n",
      "Epoch 168, Loss: 0.49374530645143583\n",
      "Epoch 169, Loss: 0.49345064806438055\n",
      "Epoch 170, Loss: 0.49317001490258333\n",
      "Epoch 171, Loss: 0.4928927591216884\n",
      "Epoch 172, Loss: 0.4926138499628376\n",
      "Epoch 173, Loss: 0.49233528219511824\n",
      "Epoch 174, Loss: 0.49205852514110515\n",
      "Epoch 175, Loss: 0.4917773399684819\n",
      "Epoch 176, Loss: 0.49149629710928233\n",
      "Epoch 177, Loss: 0.4912165647150265\n",
      "Epoch 178, Loss: 0.49094181765497674\n",
      "Epoch 179, Loss: 0.49066901222163783\n",
      "Epoch 180, Loss: 0.490393928005003\n",
      "Epoch 181, Loss: 0.4901079113527018\n",
      "Epoch 182, Loss: 0.48981984154671765\n",
      "Epoch 183, Loss: 0.4895234443341763\n",
      "Epoch 184, Loss: 0.48921459258368105\n",
      "Epoch 185, Loss: 0.4889145977883947\n",
      "Epoch 186, Loss: 0.4886175142174919\n",
      "Epoch 187, Loss: 0.4883282295514753\n",
      "Epoch 188, Loss: 0.4880435212302353\n",
      "Epoch 189, Loss: 0.48775699700499114\n",
      "Epoch 190, Loss: 0.48748008613994775\n",
      "Epoch 191, Loss: 0.4872132179171775\n",
      "Epoch 192, Loss: 0.48694738158534456\n",
      "Epoch 193, Loss: 0.48668121634032785\n",
      "Epoch 194, Loss: 0.4864280322301768\n",
      "Epoch 195, Loss: 0.486178532155915\n",
      "Epoch 196, Loss: 0.4859301919172944\n",
      "Epoch 197, Loss: 0.48568442460989747\n",
      "Epoch 198, Loss: 0.48544224711238787\n",
      "Epoch 199, Loss: 0.48519729219516017\n",
      "Epoch 200, Loss: 0.4849575446659644\n",
      "Epoch 201, Loss: 0.4847146197220469\n",
      "Epoch 202, Loss: 0.48447155759133576\n",
      "Epoch 203, Loss: 0.48422783492589483\n",
      "Epoch 204, Loss: 0.48397707745066637\n",
      "Epoch 205, Loss: 0.483737926529631\n",
      "Epoch 206, Loss: 0.4835020782648923\n",
      "Epoch 207, Loss: 0.48327083252052233\n",
      "Epoch 208, Loss: 0.4830508282762034\n",
      "Epoch 209, Loss: 0.48283269895503794\n",
      "Epoch 210, Loss: 0.48261215534620283\n",
      "Epoch 211, Loss: 0.48240746542225255\n",
      "Epoch 212, Loss: 0.4822187420754367\n",
      "Epoch 213, Loss: 0.48202997575341067\n",
      "Epoch 214, Loss: 0.4818441197783379\n",
      "Epoch 215, Loss: 0.4816502256898402\n",
      "Epoch 216, Loss: 0.4814589616492725\n",
      "Epoch 217, Loss: 0.48127338055379854\n",
      "Epoch 218, Loss: 0.4810898650639108\n",
      "Epoch 219, Loss: 0.48091365405597575\n",
      "Epoch 220, Loss: 0.480720996546175\n",
      "Epoch 221, Loss: 0.48054865040929456\n",
      "Epoch 222, Loss: 0.4803774677186426\n",
      "Epoch 223, Loss: 0.4801885671816001\n",
      "Epoch 224, Loss: 0.48000935557804647\n",
      "Epoch 225, Loss: 0.4798350303004208\n",
      "Epoch 226, Loss: 0.47967842665052907\n",
      "Epoch 227, Loss: 0.47947854451894606\n",
      "Epoch 228, Loss: 0.47928199900429136\n",
      "Epoch 229, Loss: 0.4791424752085709\n",
      "Epoch 230, Loss: 0.4789254904984497\n",
      "Epoch 231, Loss: 0.4787155823499749\n",
      "Epoch 232, Loss: 0.4785186611779873\n",
      "Epoch 233, Loss: 0.4783287773458093\n",
      "Epoch 234, Loss: 0.4781774454711345\n",
      "Epoch 235, Loss: 0.4780410437330356\n",
      "Epoch 236, Loss: 0.4778263811299359\n",
      "Epoch 237, Loss: 0.47765285017795805\n",
      "Epoch 238, Loss: 0.4775436338740249\n",
      "Epoch 239, Loss: 0.4774342833479412\n",
      "Epoch 240, Loss: 0.477263591547843\n",
      "Epoch 241, Loss: 0.47707859610269837\n",
      "Epoch 242, Loss: 0.47697972966255553\n",
      "Epoch 243, Loss: 0.4768431884384324\n",
      "Epoch 244, Loss: 0.4767408349977067\n",
      "Epoch 245, Loss: 0.4765819952057135\n",
      "Epoch 246, Loss: 0.4764706413665339\n",
      "Epoch 247, Loss: 0.47634462160009333\n",
      "Epoch 248, Loss: 0.4762527991635621\n",
      "Epoch 249, Loss: 0.4762014052862575\n",
      "Epoch 250, Loss: 0.4760283306209857\n",
      "Epoch 251, Loss: 0.47592260832103706\n",
      "Epoch 252, Loss: 0.475834869470249\n",
      "Epoch 253, Loss: 0.47570980894919\n",
      "Epoch 254, Loss: 0.47559397414543253\n",
      "Epoch 255, Loss: 0.4755005182088922\n",
      "Epoch 256, Loss: 0.4753894909333468\n",
      "Epoch 257, Loss: 0.47527958208571863\n",
      "Epoch 258, Loss: 0.47517947896910767\n",
      "Epoch 259, Loss: 0.4750690667233416\n",
      "Epoch 260, Loss: 0.47496625253354696\n",
      "Epoch 261, Loss: 0.47486506352442986\n",
      "Epoch 262, Loss: 0.47477116593942054\n",
      "Epoch 263, Loss: 0.474752914637718\n",
      "Epoch 264, Loss: 0.47457458619766507\n",
      "Epoch 265, Loss: 0.47441594770111134\n",
      "Epoch 266, Loss: 0.4743326895520222\n",
      "Epoch 267, Loss: 0.47424442089802893\n",
      "Epoch 268, Loss: 0.47409961152836255\n",
      "Epoch 269, Loss: 0.47397674971911014\n",
      "Epoch 270, Loss: 0.4739006464998255\n",
      "Epoch 271, Loss: 0.4738391701966867\n",
      "Epoch 272, Loss: 0.4737234441817917\n",
      "Epoch 273, Loss: 0.47358564948211956\n",
      "Epoch 274, Loss: 0.47351060057458727\n",
      "Epoch 275, Loss: 0.4734065036258047\n",
      "Epoch 276, Loss: 0.4733269559263822\n",
      "Epoch 277, Loss: 0.4732341071370639\n",
      "Epoch 278, Loss: 0.4731407519682655\n",
      "Epoch 279, Loss: 0.4730939966069229\n",
      "Epoch 280, Loss: 0.47298024635218033\n",
      "Epoch 281, Loss: 0.47280886054292864\n",
      "Epoch 282, Loss: 0.47271222185116163\n",
      "Epoch 283, Loss: 0.47268730250728586\n",
      "Epoch 284, Loss: 0.4725869656059248\n",
      "Epoch 285, Loss: 0.4724867493169173\n",
      "Epoch 286, Loss: 0.4723455337808729\n",
      "Epoch 287, Loss: 0.4722632042859861\n",
      "Epoch 288, Loss: 0.4722048755884816\n",
      "Epoch 289, Loss: 0.472146413443235\n",
      "Epoch 290, Loss: 0.4720312036831835\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.27767716596786035\n",
      "Valence RMSE: 0.255106119799934\n",
      "Arousal RMSE: 0.29854662054779163\n",
      "Test R^2 score: tensor([0.2113, 0.1175], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1175, 0.0125], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.16438433460424062\n",
      "Num of epochs: 291\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Epoch 121, Loss: 0.5116998508836337\n",
      "Epoch 122, Loss: 0.5111129902664598\n",
      "Epoch 123, Loss: 0.5105405741436312\n",
      "Epoch 124, Loss: 0.5099849010083686\n",
      "Epoch 125, Loss: 0.5094623759868415\n",
      "Epoch 126, Loss: 0.5089652841754122\n",
      "Epoch 127, Loss: 0.5084717799132713\n",
      "Epoch 128, Loss: 0.5079809643002384\n",
      "Epoch 129, Loss: 0.507507731608986\n",
      "Epoch 130, Loss: 0.5070515727009828\n",
      "Epoch 131, Loss: 0.5065962678643066\n",
      "Epoch 132, Loss: 0.5061448812256618\n",
      "Epoch 133, Loss: 0.5057069703510908\n",
      "Epoch 134, Loss: 0.5052756693952567\n",
      "Epoch 135, Loss: 0.5048445017462813\n",
      "Epoch 136, Loss: 0.504425284231979\n",
      "Epoch 137, Loss: 0.504022717877823\n",
      "Epoch 138, Loss: 0.5036312802042249\n",
      "Epoch 139, Loss: 0.5032368731068207\n",
      "Epoch 140, Loss: 0.5028511060044271\n",
      "Epoch 141, Loss: 0.5024804933282331\n",
      "Epoch 142, Loss: 0.5021182727396313\n",
      "Epoch 143, Loss: 0.5017581664990297\n",
      "Epoch 144, Loss: 0.5014050827892457\n",
      "Epoch 145, Loss: 0.501047675902349\n",
      "Epoch 146, Loss: 0.500690966247362\n",
      "Epoch 147, Loss: 0.5003476722262875\n",
      "Epoch 148, Loss: 0.5000068544871652\n",
      "Epoch 149, Loss: 0.49966338867073357\n",
      "Epoch 150, Loss: 0.4993280873087794\n",
      "Epoch 151, Loss: 0.49900257940428383\n",
      "Epoch 152, Loss: 0.4986865554458311\n",
      "Epoch 153, Loss: 0.49836949389755036\n",
      "Epoch 154, Loss: 0.49805375636713634\n",
      "Epoch 155, Loss: 0.49773873165141763\n",
      "Epoch 156, Loss: 0.49742224924400874\n",
      "Epoch 157, Loss: 0.49711425826812494\n",
      "Epoch 158, Loss: 0.4968010973402242\n",
      "Epoch 159, Loss: 0.4964901399315246\n",
      "Epoch 160, Loss: 0.49617675026847124\n",
      "Epoch 161, Loss: 0.4958670390985117\n",
      "Epoch 162, Loss: 0.4955578109301908\n",
      "Epoch 163, Loss: 0.4952510223984696\n",
      "Epoch 164, Loss: 0.49494577484111674\n",
      "Epoch 165, Loss: 0.49464511374066544\n",
      "Epoch 166, Loss: 0.49434383269975535\n",
      "Epoch 167, Loss: 0.49404631909698943\n",
      "Epoch 168, Loss: 0.49374530645143583\n",
      "Epoch 169, Loss: 0.49345064806438055\n",
      "Epoch 170, Loss: 0.49317001490258333\n",
      "Epoch 171, Loss: 0.4928927591216884\n",
      "Epoch 172, Loss: 0.4926138499628376\n",
      "Epoch 173, Loss: 0.49233528219511824\n",
      "Epoch 174, Loss: 0.49205852514110515\n",
      "Epoch 175, Loss: 0.4917773399684819\n",
      "Epoch 176, Loss: 0.49149629710928233\n",
      "Epoch 177, Loss: 0.4912165647150265\n",
      "Epoch 178, Loss: 0.49094181765497674\n",
      "Epoch 179, Loss: 0.49066901222163783\n",
      "Epoch 180, Loss: 0.490393928005003\n",
      "Epoch 181, Loss: 0.4901079113527018\n",
      "Epoch 182, Loss: 0.48981984154671765\n",
      "Epoch 183, Loss: 0.4895234443341763\n",
      "Epoch 184, Loss: 0.48921459258368105\n",
      "Epoch 185, Loss: 0.4889145977883947\n",
      "Epoch 186, Loss: 0.4886175142174919\n",
      "Epoch 187, Loss: 0.4883282295514753\n",
      "Epoch 188, Loss: 0.4880435212302353\n",
      "Epoch 189, Loss: 0.48775699700499114\n",
      "Epoch 190, Loss: 0.48748008613994775\n",
      "Epoch 191, Loss: 0.4872132179171775\n",
      "Epoch 192, Loss: 0.48694738158534456\n",
      "Epoch 193, Loss: 0.48668121634032785\n",
      "Epoch 194, Loss: 0.4864280322301768\n",
      "Epoch 195, Loss: 0.486178532155915\n",
      "Epoch 196, Loss: 0.4859301919172944\n",
      "Epoch 197, Loss: 0.48568442460989747\n",
      "Epoch 198, Loss: 0.48544224711238787\n",
      "Epoch 199, Loss: 0.48519729219516017\n",
      "Epoch 200, Loss: 0.4849575446659644\n",
      "Epoch 201, Loss: 0.4847146197220469\n",
      "Epoch 202, Loss: 0.48447155759133576\n",
      "Epoch 203, Loss: 0.48422783492589483\n",
      "Epoch 204, Loss: 0.48397707745066637\n",
      "Epoch 205, Loss: 0.483737926529631\n",
      "Epoch 206, Loss: 0.4835020782648923\n",
      "Epoch 207, Loss: 0.48327083252052233\n",
      "Epoch 208, Loss: 0.4830508282762034\n",
      "Epoch 209, Loss: 0.48283269895503794\n",
      "Epoch 210, Loss: 0.48261215534620283\n",
      "Epoch 211, Loss: 0.48240746542225255\n",
      "Epoch 212, Loss: 0.4822187420754367\n",
      "Epoch 213, Loss: 0.48202997575341067\n",
      "Epoch 214, Loss: 0.4818441197783379\n",
      "Epoch 215, Loss: 0.4816502256898402\n",
      "Epoch 216, Loss: 0.4814589616492725\n",
      "Epoch 217, Loss: 0.48127338055379854\n",
      "Epoch 218, Loss: 0.4810898650639108\n",
      "Epoch 219, Loss: 0.48091365405597575\n",
      "Epoch 220, Loss: 0.480720996546175\n",
      "Epoch 221, Loss: 0.48054865040929456\n",
      "Epoch 222, Loss: 0.4803774677186426\n",
      "Epoch 223, Loss: 0.4801885671816001\n",
      "Epoch 224, Loss: 0.48000935557804647\n",
      "Epoch 225, Loss: 0.4798350303004208\n",
      "Epoch 226, Loss: 0.47967842665052907\n",
      "Epoch 227, Loss: 0.47947854451894606\n",
      "Epoch 228, Loss: 0.47928199900429136\n",
      "Epoch 229, Loss: 0.4791424752085709\n",
      "Epoch 230, Loss: 0.4789254904984497\n",
      "Epoch 231, Loss: 0.4787155823499749\n",
      "Epoch 232, Loss: 0.4785186611779873\n",
      "Epoch 233, Loss: 0.4783287773458093\n",
      "Epoch 234, Loss: 0.4781774454711345\n",
      "Epoch 235, Loss: 0.4780410437330356\n",
      "Epoch 236, Loss: 0.4778263811299359\n",
      "Epoch 237, Loss: 0.47765285017795805\n",
      "Epoch 238, Loss: 0.4775436338740249\n",
      "Epoch 239, Loss: 0.4774342833479412\n",
      "Epoch 240, Loss: 0.477263591547843\n",
      "Epoch 241, Loss: 0.47707859610269837\n",
      "Epoch 242, Loss: 0.47697972966255553\n",
      "Epoch 243, Loss: 0.4768431884384324\n",
      "Epoch 244, Loss: 0.4767408349977067\n",
      "Epoch 245, Loss: 0.4765819952057135\n",
      "Epoch 246, Loss: 0.4764706413665339\n",
      "Epoch 247, Loss: 0.47634462160009333\n",
      "Epoch 248, Loss: 0.4762527991635621\n",
      "Epoch 249, Loss: 0.4762014052862575\n",
      "Epoch 250, Loss: 0.4760283306209857\n",
      "Epoch 251, Loss: 0.47592260832103706\n",
      "Epoch 252, Loss: 0.475834869470249\n",
      "Epoch 253, Loss: 0.47570980894919\n",
      "Epoch 254, Loss: 0.47559397414543253\n",
      "Epoch 255, Loss: 0.4755005182088922\n",
      "Epoch 256, Loss: 0.4753894909333468\n",
      "Epoch 257, Loss: 0.47527958208571863\n",
      "Epoch 258, Loss: 0.47517947896910767\n",
      "Epoch 259, Loss: 0.4750690667233416\n",
      "Epoch 260, Loss: 0.47496625253354696\n",
      "Epoch 261, Loss: 0.47486506352442986\n",
      "Epoch 262, Loss: 0.47477116593942054\n",
      "Epoch 263, Loss: 0.474752914637718\n",
      "Epoch 264, Loss: 0.47457458619766507\n",
      "Epoch 265, Loss: 0.47441594770111134\n",
      "Epoch 266, Loss: 0.4743326895520222\n",
      "Epoch 267, Loss: 0.47424442089802893\n",
      "Epoch 268, Loss: 0.47409961152836255\n",
      "Epoch 269, Loss: 0.47397674971911014\n",
      "Epoch 270, Loss: 0.4739006464998255\n",
      "Epoch 271, Loss: 0.4738391701966867\n",
      "Epoch 272, Loss: 0.4737234441817917\n",
      "Epoch 273, Loss: 0.47358564948211956\n",
      "Epoch 274, Loss: 0.47351060057458727\n",
      "Epoch 275, Loss: 0.4734065036258047\n",
      "Epoch 276, Loss: 0.4733269559263822\n",
      "Epoch 277, Loss: 0.4732341071370639\n",
      "Epoch 278, Loss: 0.4731407519682655\n",
      "Epoch 279, Loss: 0.4730939966069229\n",
      "Epoch 280, Loss: 0.47298024635218033\n",
      "Epoch 281, Loss: 0.47280886054292864\n",
      "Epoch 282, Loss: 0.47271222185116163\n",
      "Epoch 283, Loss: 0.47268730250728586\n",
      "Epoch 284, Loss: 0.4725869656059248\n",
      "Epoch 285, Loss: 0.4724867493169173\n",
      "Epoch 286, Loss: 0.4723455337808729\n",
      "Epoch 287, Loss: 0.4722632042859861\n",
      "Epoch 288, Loss: 0.4722048755884816\n",
      "Epoch 289, Loss: 0.472146413443235\n",
      "Epoch 290, Loss: 0.4720312036831835\n",
      "Epoch 291, Loss: 0.47193135878647013\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2777037650971675\n",
      "Valence RMSE: 0.2552319871983144\n",
      "Arousal RMSE: 0.29848851738230237\n",
      "Test R^2 score: tensor([0.2105, 0.1178], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1166, 0.0129], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.164166845991155\n",
      "Num of epochs: 292\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Epoch 121, Loss: 0.5116998508836337\n",
      "Epoch 122, Loss: 0.5111129902664598\n",
      "Epoch 123, Loss: 0.5105405741436312\n",
      "Epoch 124, Loss: 0.5099849010083686\n",
      "Epoch 125, Loss: 0.5094623759868415\n",
      "Epoch 126, Loss: 0.5089652841754122\n",
      "Epoch 127, Loss: 0.5084717799132713\n",
      "Epoch 128, Loss: 0.5079809643002384\n",
      "Epoch 129, Loss: 0.507507731608986\n",
      "Epoch 130, Loss: 0.5070515727009828\n",
      "Epoch 131, Loss: 0.5065962678643066\n",
      "Epoch 132, Loss: 0.5061448812256618\n",
      "Epoch 133, Loss: 0.5057069703510908\n",
      "Epoch 134, Loss: 0.5052756693952567\n",
      "Epoch 135, Loss: 0.5048445017462813\n",
      "Epoch 136, Loss: 0.504425284231979\n",
      "Epoch 137, Loss: 0.504022717877823\n",
      "Epoch 138, Loss: 0.5036312802042249\n",
      "Epoch 139, Loss: 0.5032368731068207\n",
      "Epoch 140, Loss: 0.5028511060044271\n",
      "Epoch 141, Loss: 0.5024804933282331\n",
      "Epoch 142, Loss: 0.5021182727396313\n",
      "Epoch 143, Loss: 0.5017581664990297\n",
      "Epoch 144, Loss: 0.5014050827892457\n",
      "Epoch 145, Loss: 0.501047675902349\n",
      "Epoch 146, Loss: 0.500690966247362\n",
      "Epoch 147, Loss: 0.5003476722262875\n",
      "Epoch 148, Loss: 0.5000068544871652\n",
      "Epoch 149, Loss: 0.49966338867073357\n",
      "Epoch 150, Loss: 0.4993280873087794\n",
      "Epoch 151, Loss: 0.49900257940428383\n",
      "Epoch 152, Loss: 0.4986865554458311\n",
      "Epoch 153, Loss: 0.49836949389755036\n",
      "Epoch 154, Loss: 0.49805375636713634\n",
      "Epoch 155, Loss: 0.49773873165141763\n",
      "Epoch 156, Loss: 0.49742224924400874\n",
      "Epoch 157, Loss: 0.49711425826812494\n",
      "Epoch 158, Loss: 0.4968010973402242\n",
      "Epoch 159, Loss: 0.4964901399315246\n",
      "Epoch 160, Loss: 0.49617675026847124\n",
      "Epoch 161, Loss: 0.4958670390985117\n",
      "Epoch 162, Loss: 0.4955578109301908\n",
      "Epoch 163, Loss: 0.4952510223984696\n",
      "Epoch 164, Loss: 0.49494577484111674\n",
      "Epoch 165, Loss: 0.49464511374066544\n",
      "Epoch 166, Loss: 0.49434383269975535\n",
      "Epoch 167, Loss: 0.49404631909698943\n",
      "Epoch 168, Loss: 0.49374530645143583\n",
      "Epoch 169, Loss: 0.49345064806438055\n",
      "Epoch 170, Loss: 0.49317001490258333\n",
      "Epoch 171, Loss: 0.4928927591216884\n",
      "Epoch 172, Loss: 0.4926138499628376\n",
      "Epoch 173, Loss: 0.49233528219511824\n",
      "Epoch 174, Loss: 0.49205852514110515\n",
      "Epoch 175, Loss: 0.4917773399684819\n",
      "Epoch 176, Loss: 0.49149629710928233\n",
      "Epoch 177, Loss: 0.4912165647150265\n",
      "Epoch 178, Loss: 0.49094181765497674\n",
      "Epoch 179, Loss: 0.49066901222163783\n",
      "Epoch 180, Loss: 0.490393928005003\n",
      "Epoch 181, Loss: 0.4901079113527018\n",
      "Epoch 182, Loss: 0.48981984154671765\n",
      "Epoch 183, Loss: 0.4895234443341763\n",
      "Epoch 184, Loss: 0.48921459258368105\n",
      "Epoch 185, Loss: 0.4889145977883947\n",
      "Epoch 186, Loss: 0.4886175142174919\n",
      "Epoch 187, Loss: 0.4883282295514753\n",
      "Epoch 188, Loss: 0.4880435212302353\n",
      "Epoch 189, Loss: 0.48775699700499114\n",
      "Epoch 190, Loss: 0.48748008613994775\n",
      "Epoch 191, Loss: 0.4872132179171775\n",
      "Epoch 192, Loss: 0.48694738158534456\n",
      "Epoch 193, Loss: 0.48668121634032785\n",
      "Epoch 194, Loss: 0.4864280322301768\n",
      "Epoch 195, Loss: 0.486178532155915\n",
      "Epoch 196, Loss: 0.4859301919172944\n",
      "Epoch 197, Loss: 0.48568442460989747\n",
      "Epoch 198, Loss: 0.48544224711238787\n",
      "Epoch 199, Loss: 0.48519729219516017\n",
      "Epoch 200, Loss: 0.4849575446659644\n",
      "Epoch 201, Loss: 0.4847146197220469\n",
      "Epoch 202, Loss: 0.48447155759133576\n",
      "Epoch 203, Loss: 0.48422783492589483\n",
      "Epoch 204, Loss: 0.48397707745066637\n",
      "Epoch 205, Loss: 0.483737926529631\n",
      "Epoch 206, Loss: 0.4835020782648923\n",
      "Epoch 207, Loss: 0.48327083252052233\n",
      "Epoch 208, Loss: 0.4830508282762034\n",
      "Epoch 209, Loss: 0.48283269895503794\n",
      "Epoch 210, Loss: 0.48261215534620283\n",
      "Epoch 211, Loss: 0.48240746542225255\n",
      "Epoch 212, Loss: 0.4822187420754367\n",
      "Epoch 213, Loss: 0.48202997575341067\n",
      "Epoch 214, Loss: 0.4818441197783379\n",
      "Epoch 215, Loss: 0.4816502256898402\n",
      "Epoch 216, Loss: 0.4814589616492725\n",
      "Epoch 217, Loss: 0.48127338055379854\n",
      "Epoch 218, Loss: 0.4810898650639108\n",
      "Epoch 219, Loss: 0.48091365405597575\n",
      "Epoch 220, Loss: 0.480720996546175\n",
      "Epoch 221, Loss: 0.48054865040929456\n",
      "Epoch 222, Loss: 0.4803774677186426\n",
      "Epoch 223, Loss: 0.4801885671816001\n",
      "Epoch 224, Loss: 0.48000935557804647\n",
      "Epoch 225, Loss: 0.4798350303004208\n",
      "Epoch 226, Loss: 0.47967842665052907\n",
      "Epoch 227, Loss: 0.47947854451894606\n",
      "Epoch 228, Loss: 0.47928199900429136\n",
      "Epoch 229, Loss: 0.4791424752085709\n",
      "Epoch 230, Loss: 0.4789254904984497\n",
      "Epoch 231, Loss: 0.4787155823499749\n",
      "Epoch 232, Loss: 0.4785186611779873\n",
      "Epoch 233, Loss: 0.4783287773458093\n",
      "Epoch 234, Loss: 0.4781774454711345\n",
      "Epoch 235, Loss: 0.4780410437330356\n",
      "Epoch 236, Loss: 0.4778263811299359\n",
      "Epoch 237, Loss: 0.47765285017795805\n",
      "Epoch 238, Loss: 0.4775436338740249\n",
      "Epoch 239, Loss: 0.4774342833479412\n",
      "Epoch 240, Loss: 0.477263591547843\n",
      "Epoch 241, Loss: 0.47707859610269837\n",
      "Epoch 242, Loss: 0.47697972966255553\n",
      "Epoch 243, Loss: 0.4768431884384324\n",
      "Epoch 244, Loss: 0.4767408349977067\n",
      "Epoch 245, Loss: 0.4765819952057135\n",
      "Epoch 246, Loss: 0.4764706413665339\n",
      "Epoch 247, Loss: 0.47634462160009333\n",
      "Epoch 248, Loss: 0.4762527991635621\n",
      "Epoch 249, Loss: 0.4762014052862575\n",
      "Epoch 250, Loss: 0.4760283306209857\n",
      "Epoch 251, Loss: 0.47592260832103706\n",
      "Epoch 252, Loss: 0.475834869470249\n",
      "Epoch 253, Loss: 0.47570980894919\n",
      "Epoch 254, Loss: 0.47559397414543253\n",
      "Epoch 255, Loss: 0.4755005182088922\n",
      "Epoch 256, Loss: 0.4753894909333468\n",
      "Epoch 257, Loss: 0.47527958208571863\n",
      "Epoch 258, Loss: 0.47517947896910767\n",
      "Epoch 259, Loss: 0.4750690667233416\n",
      "Epoch 260, Loss: 0.47496625253354696\n",
      "Epoch 261, Loss: 0.47486506352442986\n",
      "Epoch 262, Loss: 0.47477116593942054\n",
      "Epoch 263, Loss: 0.474752914637718\n",
      "Epoch 264, Loss: 0.47457458619766507\n",
      "Epoch 265, Loss: 0.47441594770111134\n",
      "Epoch 266, Loss: 0.4743326895520222\n",
      "Epoch 267, Loss: 0.47424442089802893\n",
      "Epoch 268, Loss: 0.47409961152836255\n",
      "Epoch 269, Loss: 0.47397674971911014\n",
      "Epoch 270, Loss: 0.4739006464998255\n",
      "Epoch 271, Loss: 0.4738391701966867\n",
      "Epoch 272, Loss: 0.4737234441817917\n",
      "Epoch 273, Loss: 0.47358564948211956\n",
      "Epoch 274, Loss: 0.47351060057458727\n",
      "Epoch 275, Loss: 0.4734065036258047\n",
      "Epoch 276, Loss: 0.4733269559263822\n",
      "Epoch 277, Loss: 0.4732341071370639\n",
      "Epoch 278, Loss: 0.4731407519682655\n",
      "Epoch 279, Loss: 0.4730939966069229\n",
      "Epoch 280, Loss: 0.47298024635218033\n",
      "Epoch 281, Loss: 0.47280886054292864\n",
      "Epoch 282, Loss: 0.47271222185116163\n",
      "Epoch 283, Loss: 0.47268730250728586\n",
      "Epoch 284, Loss: 0.4725869656059248\n",
      "Epoch 285, Loss: 0.4724867493169173\n",
      "Epoch 286, Loss: 0.4723455337808729\n",
      "Epoch 287, Loss: 0.4722632042859861\n",
      "Epoch 288, Loss: 0.4722048755884816\n",
      "Epoch 289, Loss: 0.472146413443235\n",
      "Epoch 290, Loss: 0.4720312036831835\n",
      "Epoch 291, Loss: 0.47193135878647013\n",
      "Epoch 292, Loss: 0.4718300873811956\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2776168817983711\n",
      "Valence RMSE: 0.25533377603094976\n",
      "Arousal RMSE: 0.2982397172354451\n",
      "Test R^2 score: tensor([0.2099, 0.1193], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1159, 0.0145], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.16458696630443292\n",
      "Num of epochs: 293\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Epoch 121, Loss: 0.5116998508836337\n",
      "Epoch 122, Loss: 0.5111129902664598\n",
      "Epoch 123, Loss: 0.5105405741436312\n",
      "Epoch 124, Loss: 0.5099849010083686\n",
      "Epoch 125, Loss: 0.5094623759868415\n",
      "Epoch 126, Loss: 0.5089652841754122\n",
      "Epoch 127, Loss: 0.5084717799132713\n",
      "Epoch 128, Loss: 0.5079809643002384\n",
      "Epoch 129, Loss: 0.507507731608986\n",
      "Epoch 130, Loss: 0.5070515727009828\n",
      "Epoch 131, Loss: 0.5065962678643066\n",
      "Epoch 132, Loss: 0.5061448812256618\n",
      "Epoch 133, Loss: 0.5057069703510908\n",
      "Epoch 134, Loss: 0.5052756693952567\n",
      "Epoch 135, Loss: 0.5048445017462813\n",
      "Epoch 136, Loss: 0.504425284231979\n",
      "Epoch 137, Loss: 0.504022717877823\n",
      "Epoch 138, Loss: 0.5036312802042249\n",
      "Epoch 139, Loss: 0.5032368731068207\n",
      "Epoch 140, Loss: 0.5028511060044271\n",
      "Epoch 141, Loss: 0.5024804933282331\n",
      "Epoch 142, Loss: 0.5021182727396313\n",
      "Epoch 143, Loss: 0.5017581664990297\n",
      "Epoch 144, Loss: 0.5014050827892457\n",
      "Epoch 145, Loss: 0.501047675902349\n",
      "Epoch 146, Loss: 0.500690966247362\n",
      "Epoch 147, Loss: 0.5003476722262875\n",
      "Epoch 148, Loss: 0.5000068544871652\n",
      "Epoch 149, Loss: 0.49966338867073357\n",
      "Epoch 150, Loss: 0.4993280873087794\n",
      "Epoch 151, Loss: 0.49900257940428383\n",
      "Epoch 152, Loss: 0.4986865554458311\n",
      "Epoch 153, Loss: 0.49836949389755036\n",
      "Epoch 154, Loss: 0.49805375636713634\n",
      "Epoch 155, Loss: 0.49773873165141763\n",
      "Epoch 156, Loss: 0.49742224924400874\n",
      "Epoch 157, Loss: 0.49711425826812494\n",
      "Epoch 158, Loss: 0.4968010973402242\n",
      "Epoch 159, Loss: 0.4964901399315246\n",
      "Epoch 160, Loss: 0.49617675026847124\n",
      "Epoch 161, Loss: 0.4958670390985117\n",
      "Epoch 162, Loss: 0.4955578109301908\n",
      "Epoch 163, Loss: 0.4952510223984696\n",
      "Epoch 164, Loss: 0.49494577484111674\n",
      "Epoch 165, Loss: 0.49464511374066544\n",
      "Epoch 166, Loss: 0.49434383269975535\n",
      "Epoch 167, Loss: 0.49404631909698943\n",
      "Epoch 168, Loss: 0.49374530645143583\n",
      "Epoch 169, Loss: 0.49345064806438055\n",
      "Epoch 170, Loss: 0.49317001490258333\n",
      "Epoch 171, Loss: 0.4928927591216884\n",
      "Epoch 172, Loss: 0.4926138499628376\n",
      "Epoch 173, Loss: 0.49233528219511824\n",
      "Epoch 174, Loss: 0.49205852514110515\n",
      "Epoch 175, Loss: 0.4917773399684819\n",
      "Epoch 176, Loss: 0.49149629710928233\n",
      "Epoch 177, Loss: 0.4912165647150265\n",
      "Epoch 178, Loss: 0.49094181765497674\n",
      "Epoch 179, Loss: 0.49066901222163783\n",
      "Epoch 180, Loss: 0.490393928005003\n",
      "Epoch 181, Loss: 0.4901079113527018\n",
      "Epoch 182, Loss: 0.48981984154671765\n",
      "Epoch 183, Loss: 0.4895234443341763\n",
      "Epoch 184, Loss: 0.48921459258368105\n",
      "Epoch 185, Loss: 0.4889145977883947\n",
      "Epoch 186, Loss: 0.4886175142174919\n",
      "Epoch 187, Loss: 0.4883282295514753\n",
      "Epoch 188, Loss: 0.4880435212302353\n",
      "Epoch 189, Loss: 0.48775699700499114\n",
      "Epoch 190, Loss: 0.48748008613994775\n",
      "Epoch 191, Loss: 0.4872132179171775\n",
      "Epoch 192, Loss: 0.48694738158534456\n",
      "Epoch 193, Loss: 0.48668121634032785\n",
      "Epoch 194, Loss: 0.4864280322301768\n",
      "Epoch 195, Loss: 0.486178532155915\n",
      "Epoch 196, Loss: 0.4859301919172944\n",
      "Epoch 197, Loss: 0.48568442460989747\n",
      "Epoch 198, Loss: 0.48544224711238787\n",
      "Epoch 199, Loss: 0.48519729219516017\n",
      "Epoch 200, Loss: 0.4849575446659644\n",
      "Epoch 201, Loss: 0.4847146197220469\n",
      "Epoch 202, Loss: 0.48447155759133576\n",
      "Epoch 203, Loss: 0.48422783492589483\n",
      "Epoch 204, Loss: 0.48397707745066637\n",
      "Epoch 205, Loss: 0.483737926529631\n",
      "Epoch 206, Loss: 0.4835020782648923\n",
      "Epoch 207, Loss: 0.48327083252052233\n",
      "Epoch 208, Loss: 0.4830508282762034\n",
      "Epoch 209, Loss: 0.48283269895503794\n",
      "Epoch 210, Loss: 0.48261215534620283\n",
      "Epoch 211, Loss: 0.48240746542225255\n",
      "Epoch 212, Loss: 0.4822187420754367\n",
      "Epoch 213, Loss: 0.48202997575341067\n",
      "Epoch 214, Loss: 0.4818441197783379\n",
      "Epoch 215, Loss: 0.4816502256898402\n",
      "Epoch 216, Loss: 0.4814589616492725\n",
      "Epoch 217, Loss: 0.48127338055379854\n",
      "Epoch 218, Loss: 0.4810898650639108\n",
      "Epoch 219, Loss: 0.48091365405597575\n",
      "Epoch 220, Loss: 0.480720996546175\n",
      "Epoch 221, Loss: 0.48054865040929456\n",
      "Epoch 222, Loss: 0.4803774677186426\n",
      "Epoch 223, Loss: 0.4801885671816001\n",
      "Epoch 224, Loss: 0.48000935557804647\n",
      "Epoch 225, Loss: 0.4798350303004208\n",
      "Epoch 226, Loss: 0.47967842665052907\n",
      "Epoch 227, Loss: 0.47947854451894606\n",
      "Epoch 228, Loss: 0.47928199900429136\n",
      "Epoch 229, Loss: 0.4791424752085709\n",
      "Epoch 230, Loss: 0.4789254904984497\n",
      "Epoch 231, Loss: 0.4787155823499749\n",
      "Epoch 232, Loss: 0.4785186611779873\n",
      "Epoch 233, Loss: 0.4783287773458093\n",
      "Epoch 234, Loss: 0.4781774454711345\n",
      "Epoch 235, Loss: 0.4780410437330356\n",
      "Epoch 236, Loss: 0.4778263811299359\n",
      "Epoch 237, Loss: 0.47765285017795805\n",
      "Epoch 238, Loss: 0.4775436338740249\n",
      "Epoch 239, Loss: 0.4774342833479412\n",
      "Epoch 240, Loss: 0.477263591547843\n",
      "Epoch 241, Loss: 0.47707859610269837\n",
      "Epoch 242, Loss: 0.47697972966255553\n",
      "Epoch 243, Loss: 0.4768431884384324\n",
      "Epoch 244, Loss: 0.4767408349977067\n",
      "Epoch 245, Loss: 0.4765819952057135\n",
      "Epoch 246, Loss: 0.4764706413665339\n",
      "Epoch 247, Loss: 0.47634462160009333\n",
      "Epoch 248, Loss: 0.4762527991635621\n",
      "Epoch 249, Loss: 0.4762014052862575\n",
      "Epoch 250, Loss: 0.4760283306209857\n",
      "Epoch 251, Loss: 0.47592260832103706\n",
      "Epoch 252, Loss: 0.475834869470249\n",
      "Epoch 253, Loss: 0.47570980894919\n",
      "Epoch 254, Loss: 0.47559397414543253\n",
      "Epoch 255, Loss: 0.4755005182088922\n",
      "Epoch 256, Loss: 0.4753894909333468\n",
      "Epoch 257, Loss: 0.47527958208571863\n",
      "Epoch 258, Loss: 0.47517947896910767\n",
      "Epoch 259, Loss: 0.4750690667233416\n",
      "Epoch 260, Loss: 0.47496625253354696\n",
      "Epoch 261, Loss: 0.47486506352442986\n",
      "Epoch 262, Loss: 0.47477116593942054\n",
      "Epoch 263, Loss: 0.474752914637718\n",
      "Epoch 264, Loss: 0.47457458619766507\n",
      "Epoch 265, Loss: 0.47441594770111134\n",
      "Epoch 266, Loss: 0.4743326895520222\n",
      "Epoch 267, Loss: 0.47424442089802893\n",
      "Epoch 268, Loss: 0.47409961152836255\n",
      "Epoch 269, Loss: 0.47397674971911014\n",
      "Epoch 270, Loss: 0.4739006464998255\n",
      "Epoch 271, Loss: 0.4738391701966867\n",
      "Epoch 272, Loss: 0.4737234441817917\n",
      "Epoch 273, Loss: 0.47358564948211956\n",
      "Epoch 274, Loss: 0.47351060057458727\n",
      "Epoch 275, Loss: 0.4734065036258047\n",
      "Epoch 276, Loss: 0.4733269559263822\n",
      "Epoch 277, Loss: 0.4732341071370639\n",
      "Epoch 278, Loss: 0.4731407519682655\n",
      "Epoch 279, Loss: 0.4730939966069229\n",
      "Epoch 280, Loss: 0.47298024635218033\n",
      "Epoch 281, Loss: 0.47280886054292864\n",
      "Epoch 282, Loss: 0.47271222185116163\n",
      "Epoch 283, Loss: 0.47268730250728586\n",
      "Epoch 284, Loss: 0.4725869656059248\n",
      "Epoch 285, Loss: 0.4724867493169173\n",
      "Epoch 286, Loss: 0.4723455337808729\n",
      "Epoch 287, Loss: 0.4722632042859861\n",
      "Epoch 288, Loss: 0.4722048755884816\n",
      "Epoch 289, Loss: 0.472146413443235\n",
      "Epoch 290, Loss: 0.4720312036831835\n",
      "Epoch 291, Loss: 0.47193135878647013\n",
      "Epoch 292, Loss: 0.4718300873811956\n",
      "Epoch 293, Loss: 0.4717440511887379\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.27756487859214474\n",
      "Valence RMSE: 0.2552258626566362\n",
      "Arousal RMSE: 0.29823528075484285\n",
      "Test R^2 score: tensor([0.2106, 0.1193], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1166, 0.0145], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.16493392285921404\n",
      "Num of epochs: 294\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Epoch 121, Loss: 0.5116998508836337\n",
      "Epoch 122, Loss: 0.5111129902664598\n",
      "Epoch 123, Loss: 0.5105405741436312\n",
      "Epoch 124, Loss: 0.5099849010083686\n",
      "Epoch 125, Loss: 0.5094623759868415\n",
      "Epoch 126, Loss: 0.5089652841754122\n",
      "Epoch 127, Loss: 0.5084717799132713\n",
      "Epoch 128, Loss: 0.5079809643002384\n",
      "Epoch 129, Loss: 0.507507731608986\n",
      "Epoch 130, Loss: 0.5070515727009828\n",
      "Epoch 131, Loss: 0.5065962678643066\n",
      "Epoch 132, Loss: 0.5061448812256618\n",
      "Epoch 133, Loss: 0.5057069703510908\n",
      "Epoch 134, Loss: 0.5052756693952567\n",
      "Epoch 135, Loss: 0.5048445017462813\n",
      "Epoch 136, Loss: 0.504425284231979\n",
      "Epoch 137, Loss: 0.504022717877823\n",
      "Epoch 138, Loss: 0.5036312802042249\n",
      "Epoch 139, Loss: 0.5032368731068207\n",
      "Epoch 140, Loss: 0.5028511060044271\n",
      "Epoch 141, Loss: 0.5024804933282331\n",
      "Epoch 142, Loss: 0.5021182727396313\n",
      "Epoch 143, Loss: 0.5017581664990297\n",
      "Epoch 144, Loss: 0.5014050827892457\n",
      "Epoch 145, Loss: 0.501047675902349\n",
      "Epoch 146, Loss: 0.500690966247362\n",
      "Epoch 147, Loss: 0.5003476722262875\n",
      "Epoch 148, Loss: 0.5000068544871652\n",
      "Epoch 149, Loss: 0.49966338867073357\n",
      "Epoch 150, Loss: 0.4993280873087794\n",
      "Epoch 151, Loss: 0.49900257940428383\n",
      "Epoch 152, Loss: 0.4986865554458311\n",
      "Epoch 153, Loss: 0.49836949389755036\n",
      "Epoch 154, Loss: 0.49805375636713634\n",
      "Epoch 155, Loss: 0.49773873165141763\n",
      "Epoch 156, Loss: 0.49742224924400874\n",
      "Epoch 157, Loss: 0.49711425826812494\n",
      "Epoch 158, Loss: 0.4968010973402242\n",
      "Epoch 159, Loss: 0.4964901399315246\n",
      "Epoch 160, Loss: 0.49617675026847124\n",
      "Epoch 161, Loss: 0.4958670390985117\n",
      "Epoch 162, Loss: 0.4955578109301908\n",
      "Epoch 163, Loss: 0.4952510223984696\n",
      "Epoch 164, Loss: 0.49494577484111674\n",
      "Epoch 165, Loss: 0.49464511374066544\n",
      "Epoch 166, Loss: 0.49434383269975535\n",
      "Epoch 167, Loss: 0.49404631909698943\n",
      "Epoch 168, Loss: 0.49374530645143583\n",
      "Epoch 169, Loss: 0.49345064806438055\n",
      "Epoch 170, Loss: 0.49317001490258333\n",
      "Epoch 171, Loss: 0.4928927591216884\n",
      "Epoch 172, Loss: 0.4926138499628376\n",
      "Epoch 173, Loss: 0.49233528219511824\n",
      "Epoch 174, Loss: 0.49205852514110515\n",
      "Epoch 175, Loss: 0.4917773399684819\n",
      "Epoch 176, Loss: 0.49149629710928233\n",
      "Epoch 177, Loss: 0.4912165647150265\n",
      "Epoch 178, Loss: 0.49094181765497674\n",
      "Epoch 179, Loss: 0.49066901222163783\n",
      "Epoch 180, Loss: 0.490393928005003\n",
      "Epoch 181, Loss: 0.4901079113527018\n",
      "Epoch 182, Loss: 0.48981984154671765\n",
      "Epoch 183, Loss: 0.4895234443341763\n",
      "Epoch 184, Loss: 0.48921459258368105\n",
      "Epoch 185, Loss: 0.4889145977883947\n",
      "Epoch 186, Loss: 0.4886175142174919\n",
      "Epoch 187, Loss: 0.4883282295514753\n",
      "Epoch 188, Loss: 0.4880435212302353\n",
      "Epoch 189, Loss: 0.48775699700499114\n",
      "Epoch 190, Loss: 0.48748008613994775\n",
      "Epoch 191, Loss: 0.4872132179171775\n",
      "Epoch 192, Loss: 0.48694738158534456\n",
      "Epoch 193, Loss: 0.48668121634032785\n",
      "Epoch 194, Loss: 0.4864280322301768\n",
      "Epoch 195, Loss: 0.486178532155915\n",
      "Epoch 196, Loss: 0.4859301919172944\n",
      "Epoch 197, Loss: 0.48568442460989747\n",
      "Epoch 198, Loss: 0.48544224711238787\n",
      "Epoch 199, Loss: 0.48519729219516017\n",
      "Epoch 200, Loss: 0.4849575446659644\n",
      "Epoch 201, Loss: 0.4847146197220469\n",
      "Epoch 202, Loss: 0.48447155759133576\n",
      "Epoch 203, Loss: 0.48422783492589483\n",
      "Epoch 204, Loss: 0.48397707745066637\n",
      "Epoch 205, Loss: 0.483737926529631\n",
      "Epoch 206, Loss: 0.4835020782648923\n",
      "Epoch 207, Loss: 0.48327083252052233\n",
      "Epoch 208, Loss: 0.4830508282762034\n",
      "Epoch 209, Loss: 0.48283269895503794\n",
      "Epoch 210, Loss: 0.48261215534620283\n",
      "Epoch 211, Loss: 0.48240746542225255\n",
      "Epoch 212, Loss: 0.4822187420754367\n",
      "Epoch 213, Loss: 0.48202997575341067\n",
      "Epoch 214, Loss: 0.4818441197783379\n",
      "Epoch 215, Loss: 0.4816502256898402\n",
      "Epoch 216, Loss: 0.4814589616492725\n",
      "Epoch 217, Loss: 0.48127338055379854\n",
      "Epoch 218, Loss: 0.4810898650639108\n",
      "Epoch 219, Loss: 0.48091365405597575\n",
      "Epoch 220, Loss: 0.480720996546175\n",
      "Epoch 221, Loss: 0.48054865040929456\n",
      "Epoch 222, Loss: 0.4803774677186426\n",
      "Epoch 223, Loss: 0.4801885671816001\n",
      "Epoch 224, Loss: 0.48000935557804647\n",
      "Epoch 225, Loss: 0.4798350303004208\n",
      "Epoch 226, Loss: 0.47967842665052907\n",
      "Epoch 227, Loss: 0.47947854451894606\n",
      "Epoch 228, Loss: 0.47928199900429136\n",
      "Epoch 229, Loss: 0.4791424752085709\n",
      "Epoch 230, Loss: 0.4789254904984497\n",
      "Epoch 231, Loss: 0.4787155823499749\n",
      "Epoch 232, Loss: 0.4785186611779873\n",
      "Epoch 233, Loss: 0.4783287773458093\n",
      "Epoch 234, Loss: 0.4781774454711345\n",
      "Epoch 235, Loss: 0.4780410437330356\n",
      "Epoch 236, Loss: 0.4778263811299359\n",
      "Epoch 237, Loss: 0.47765285017795805\n",
      "Epoch 238, Loss: 0.4775436338740249\n",
      "Epoch 239, Loss: 0.4774342833479412\n",
      "Epoch 240, Loss: 0.477263591547843\n",
      "Epoch 241, Loss: 0.47707859610269837\n",
      "Epoch 242, Loss: 0.47697972966255553\n",
      "Epoch 243, Loss: 0.4768431884384324\n",
      "Epoch 244, Loss: 0.4767408349977067\n",
      "Epoch 245, Loss: 0.4765819952057135\n",
      "Epoch 246, Loss: 0.4764706413665339\n",
      "Epoch 247, Loss: 0.47634462160009333\n",
      "Epoch 248, Loss: 0.4762527991635621\n",
      "Epoch 249, Loss: 0.4762014052862575\n",
      "Epoch 250, Loss: 0.4760283306209857\n",
      "Epoch 251, Loss: 0.47592260832103706\n",
      "Epoch 252, Loss: 0.475834869470249\n",
      "Epoch 253, Loss: 0.47570980894919\n",
      "Epoch 254, Loss: 0.47559397414543253\n",
      "Epoch 255, Loss: 0.4755005182088922\n",
      "Epoch 256, Loss: 0.4753894909333468\n",
      "Epoch 257, Loss: 0.47527958208571863\n",
      "Epoch 258, Loss: 0.47517947896910767\n",
      "Epoch 259, Loss: 0.4750690667233416\n",
      "Epoch 260, Loss: 0.47496625253354696\n",
      "Epoch 261, Loss: 0.47486506352442986\n",
      "Epoch 262, Loss: 0.47477116593942054\n",
      "Epoch 263, Loss: 0.474752914637718\n",
      "Epoch 264, Loss: 0.47457458619766507\n",
      "Epoch 265, Loss: 0.47441594770111134\n",
      "Epoch 266, Loss: 0.4743326895520222\n",
      "Epoch 267, Loss: 0.47424442089802893\n",
      "Epoch 268, Loss: 0.47409961152836255\n",
      "Epoch 269, Loss: 0.47397674971911014\n",
      "Epoch 270, Loss: 0.4739006464998255\n",
      "Epoch 271, Loss: 0.4738391701966867\n",
      "Epoch 272, Loss: 0.4737234441817917\n",
      "Epoch 273, Loss: 0.47358564948211956\n",
      "Epoch 274, Loss: 0.47351060057458727\n",
      "Epoch 275, Loss: 0.4734065036258047\n",
      "Epoch 276, Loss: 0.4733269559263822\n",
      "Epoch 277, Loss: 0.4732341071370639\n",
      "Epoch 278, Loss: 0.4731407519682655\n",
      "Epoch 279, Loss: 0.4730939966069229\n",
      "Epoch 280, Loss: 0.47298024635218033\n",
      "Epoch 281, Loss: 0.47280886054292864\n",
      "Epoch 282, Loss: 0.47271222185116163\n",
      "Epoch 283, Loss: 0.47268730250728586\n",
      "Epoch 284, Loss: 0.4725869656059248\n",
      "Epoch 285, Loss: 0.4724867493169173\n",
      "Epoch 286, Loss: 0.4723455337808729\n",
      "Epoch 287, Loss: 0.4722632042859861\n",
      "Epoch 288, Loss: 0.4722048755884816\n",
      "Epoch 289, Loss: 0.472146413443235\n",
      "Epoch 290, Loss: 0.4720312036831835\n",
      "Epoch 291, Loss: 0.47193135878647013\n",
      "Epoch 292, Loss: 0.4718300873811956\n",
      "Epoch 293, Loss: 0.4717440511887379\n",
      "Epoch 294, Loss: 0.47169887906475694\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.27790372602429836\n",
      "Valence RMSE: 0.2556921334618495\n",
      "Arousal RMSE: 0.2984669073148719\n",
      "Test R^2 score: tensor([0.2077, 0.1179], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1134, 0.0130], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.16280612612808976\n",
      "Num of epochs: 295\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Epoch 121, Loss: 0.5116998508836337\n",
      "Epoch 122, Loss: 0.5111129902664598\n",
      "Epoch 123, Loss: 0.5105405741436312\n",
      "Epoch 124, Loss: 0.5099849010083686\n",
      "Epoch 125, Loss: 0.5094623759868415\n",
      "Epoch 126, Loss: 0.5089652841754122\n",
      "Epoch 127, Loss: 0.5084717799132713\n",
      "Epoch 128, Loss: 0.5079809643002384\n",
      "Epoch 129, Loss: 0.507507731608986\n",
      "Epoch 130, Loss: 0.5070515727009828\n",
      "Epoch 131, Loss: 0.5065962678643066\n",
      "Epoch 132, Loss: 0.5061448812256618\n",
      "Epoch 133, Loss: 0.5057069703510908\n",
      "Epoch 134, Loss: 0.5052756693952567\n",
      "Epoch 135, Loss: 0.5048445017462813\n",
      "Epoch 136, Loss: 0.504425284231979\n",
      "Epoch 137, Loss: 0.504022717877823\n",
      "Epoch 138, Loss: 0.5036312802042249\n",
      "Epoch 139, Loss: 0.5032368731068207\n",
      "Epoch 140, Loss: 0.5028511060044271\n",
      "Epoch 141, Loss: 0.5024804933282331\n",
      "Epoch 142, Loss: 0.5021182727396313\n",
      "Epoch 143, Loss: 0.5017581664990297\n",
      "Epoch 144, Loss: 0.5014050827892457\n",
      "Epoch 145, Loss: 0.501047675902349\n",
      "Epoch 146, Loss: 0.500690966247362\n",
      "Epoch 147, Loss: 0.5003476722262875\n",
      "Epoch 148, Loss: 0.5000068544871652\n",
      "Epoch 149, Loss: 0.49966338867073357\n",
      "Epoch 150, Loss: 0.4993280873087794\n",
      "Epoch 151, Loss: 0.49900257940428383\n",
      "Epoch 152, Loss: 0.4986865554458311\n",
      "Epoch 153, Loss: 0.49836949389755036\n",
      "Epoch 154, Loss: 0.49805375636713634\n",
      "Epoch 155, Loss: 0.49773873165141763\n",
      "Epoch 156, Loss: 0.49742224924400874\n",
      "Epoch 157, Loss: 0.49711425826812494\n",
      "Epoch 158, Loss: 0.4968010973402242\n",
      "Epoch 159, Loss: 0.4964901399315246\n",
      "Epoch 160, Loss: 0.49617675026847124\n",
      "Epoch 161, Loss: 0.4958670390985117\n",
      "Epoch 162, Loss: 0.4955578109301908\n",
      "Epoch 163, Loss: 0.4952510223984696\n",
      "Epoch 164, Loss: 0.49494577484111674\n",
      "Epoch 165, Loss: 0.49464511374066544\n",
      "Epoch 166, Loss: 0.49434383269975535\n",
      "Epoch 167, Loss: 0.49404631909698943\n",
      "Epoch 168, Loss: 0.49374530645143583\n",
      "Epoch 169, Loss: 0.49345064806438055\n",
      "Epoch 170, Loss: 0.49317001490258333\n",
      "Epoch 171, Loss: 0.4928927591216884\n",
      "Epoch 172, Loss: 0.4926138499628376\n",
      "Epoch 173, Loss: 0.49233528219511824\n",
      "Epoch 174, Loss: 0.49205852514110515\n",
      "Epoch 175, Loss: 0.4917773399684819\n",
      "Epoch 176, Loss: 0.49149629710928233\n",
      "Epoch 177, Loss: 0.4912165647150265\n",
      "Epoch 178, Loss: 0.49094181765497674\n",
      "Epoch 179, Loss: 0.49066901222163783\n",
      "Epoch 180, Loss: 0.490393928005003\n",
      "Epoch 181, Loss: 0.4901079113527018\n",
      "Epoch 182, Loss: 0.48981984154671765\n",
      "Epoch 183, Loss: 0.4895234443341763\n",
      "Epoch 184, Loss: 0.48921459258368105\n",
      "Epoch 185, Loss: 0.4889145977883947\n",
      "Epoch 186, Loss: 0.4886175142174919\n",
      "Epoch 187, Loss: 0.4883282295514753\n",
      "Epoch 188, Loss: 0.4880435212302353\n",
      "Epoch 189, Loss: 0.48775699700499114\n",
      "Epoch 190, Loss: 0.48748008613994775\n",
      "Epoch 191, Loss: 0.4872132179171775\n",
      "Epoch 192, Loss: 0.48694738158534456\n",
      "Epoch 193, Loss: 0.48668121634032785\n",
      "Epoch 194, Loss: 0.4864280322301768\n",
      "Epoch 195, Loss: 0.486178532155915\n",
      "Epoch 196, Loss: 0.4859301919172944\n",
      "Epoch 197, Loss: 0.48568442460989747\n",
      "Epoch 198, Loss: 0.48544224711238787\n",
      "Epoch 199, Loss: 0.48519729219516017\n",
      "Epoch 200, Loss: 0.4849575446659644\n",
      "Epoch 201, Loss: 0.4847146197220469\n",
      "Epoch 202, Loss: 0.48447155759133576\n",
      "Epoch 203, Loss: 0.48422783492589483\n",
      "Epoch 204, Loss: 0.48397707745066637\n",
      "Epoch 205, Loss: 0.483737926529631\n",
      "Epoch 206, Loss: 0.4835020782648923\n",
      "Epoch 207, Loss: 0.48327083252052233\n",
      "Epoch 208, Loss: 0.4830508282762034\n",
      "Epoch 209, Loss: 0.48283269895503794\n",
      "Epoch 210, Loss: 0.48261215534620283\n",
      "Epoch 211, Loss: 0.48240746542225255\n",
      "Epoch 212, Loss: 0.4822187420754367\n",
      "Epoch 213, Loss: 0.48202997575341067\n",
      "Epoch 214, Loss: 0.4818441197783379\n",
      "Epoch 215, Loss: 0.4816502256898402\n",
      "Epoch 216, Loss: 0.4814589616492725\n",
      "Epoch 217, Loss: 0.48127338055379854\n",
      "Epoch 218, Loss: 0.4810898650639108\n",
      "Epoch 219, Loss: 0.48091365405597575\n",
      "Epoch 220, Loss: 0.480720996546175\n",
      "Epoch 221, Loss: 0.48054865040929456\n",
      "Epoch 222, Loss: 0.4803774677186426\n",
      "Epoch 223, Loss: 0.4801885671816001\n",
      "Epoch 224, Loss: 0.48000935557804647\n",
      "Epoch 225, Loss: 0.4798350303004208\n",
      "Epoch 226, Loss: 0.47967842665052907\n",
      "Epoch 227, Loss: 0.47947854451894606\n",
      "Epoch 228, Loss: 0.47928199900429136\n",
      "Epoch 229, Loss: 0.4791424752085709\n",
      "Epoch 230, Loss: 0.4789254904984497\n",
      "Epoch 231, Loss: 0.4787155823499749\n",
      "Epoch 232, Loss: 0.4785186611779873\n",
      "Epoch 233, Loss: 0.4783287773458093\n",
      "Epoch 234, Loss: 0.4781774454711345\n",
      "Epoch 235, Loss: 0.4780410437330356\n",
      "Epoch 236, Loss: 0.4778263811299359\n",
      "Epoch 237, Loss: 0.47765285017795805\n",
      "Epoch 238, Loss: 0.4775436338740249\n",
      "Epoch 239, Loss: 0.4774342833479412\n",
      "Epoch 240, Loss: 0.477263591547843\n",
      "Epoch 241, Loss: 0.47707859610269837\n",
      "Epoch 242, Loss: 0.47697972966255553\n",
      "Epoch 243, Loss: 0.4768431884384324\n",
      "Epoch 244, Loss: 0.4767408349977067\n",
      "Epoch 245, Loss: 0.4765819952057135\n",
      "Epoch 246, Loss: 0.4764706413665339\n",
      "Epoch 247, Loss: 0.47634462160009333\n",
      "Epoch 248, Loss: 0.4762527991635621\n",
      "Epoch 249, Loss: 0.4762014052862575\n",
      "Epoch 250, Loss: 0.4760283306209857\n",
      "Epoch 251, Loss: 0.47592260832103706\n",
      "Epoch 252, Loss: 0.475834869470249\n",
      "Epoch 253, Loss: 0.47570980894919\n",
      "Epoch 254, Loss: 0.47559397414543253\n",
      "Epoch 255, Loss: 0.4755005182088922\n",
      "Epoch 256, Loss: 0.4753894909333468\n",
      "Epoch 257, Loss: 0.47527958208571863\n",
      "Epoch 258, Loss: 0.47517947896910767\n",
      "Epoch 259, Loss: 0.4750690667233416\n",
      "Epoch 260, Loss: 0.47496625253354696\n",
      "Epoch 261, Loss: 0.47486506352442986\n",
      "Epoch 262, Loss: 0.47477116593942054\n",
      "Epoch 263, Loss: 0.474752914637718\n",
      "Epoch 264, Loss: 0.47457458619766507\n",
      "Epoch 265, Loss: 0.47441594770111134\n",
      "Epoch 266, Loss: 0.4743326895520222\n",
      "Epoch 267, Loss: 0.47424442089802893\n",
      "Epoch 268, Loss: 0.47409961152836255\n",
      "Epoch 269, Loss: 0.47397674971911014\n",
      "Epoch 270, Loss: 0.4739006464998255\n",
      "Epoch 271, Loss: 0.4738391701966867\n",
      "Epoch 272, Loss: 0.4737234441817917\n",
      "Epoch 273, Loss: 0.47358564948211956\n",
      "Epoch 274, Loss: 0.47351060057458727\n",
      "Epoch 275, Loss: 0.4734065036258047\n",
      "Epoch 276, Loss: 0.4733269559263822\n",
      "Epoch 277, Loss: 0.4732341071370639\n",
      "Epoch 278, Loss: 0.4731407519682655\n",
      "Epoch 279, Loss: 0.4730939966069229\n",
      "Epoch 280, Loss: 0.47298024635218033\n",
      "Epoch 281, Loss: 0.47280886054292864\n",
      "Epoch 282, Loss: 0.47271222185116163\n",
      "Epoch 283, Loss: 0.47268730250728586\n",
      "Epoch 284, Loss: 0.4725869656059248\n",
      "Epoch 285, Loss: 0.4724867493169173\n",
      "Epoch 286, Loss: 0.4723455337808729\n",
      "Epoch 287, Loss: 0.4722632042859861\n",
      "Epoch 288, Loss: 0.4722048755884816\n",
      "Epoch 289, Loss: 0.472146413443235\n",
      "Epoch 290, Loss: 0.4720312036831835\n",
      "Epoch 291, Loss: 0.47193135878647013\n",
      "Epoch 292, Loss: 0.4718300873811956\n",
      "Epoch 293, Loss: 0.4717440511887379\n",
      "Epoch 294, Loss: 0.47169887906475694\n",
      "Epoch 295, Loss: 0.4716194856818237\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2778758367231792\n",
      "Valence RMSE: 0.25550244363899\n",
      "Arousal RMSE: 0.2985773979451872\n",
      "Test R^2 score: tensor([0.2089, 0.1173], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1147, 0.0123], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.1630671099742551\n",
      "Num of epochs: 296\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Epoch 121, Loss: 0.5116998508836337\n",
      "Epoch 122, Loss: 0.5111129902664598\n",
      "Epoch 123, Loss: 0.5105405741436312\n",
      "Epoch 124, Loss: 0.5099849010083686\n",
      "Epoch 125, Loss: 0.5094623759868415\n",
      "Epoch 126, Loss: 0.5089652841754122\n",
      "Epoch 127, Loss: 0.5084717799132713\n",
      "Epoch 128, Loss: 0.5079809643002384\n",
      "Epoch 129, Loss: 0.507507731608986\n",
      "Epoch 130, Loss: 0.5070515727009828\n",
      "Epoch 131, Loss: 0.5065962678643066\n",
      "Epoch 132, Loss: 0.5061448812256618\n",
      "Epoch 133, Loss: 0.5057069703510908\n",
      "Epoch 134, Loss: 0.5052756693952567\n",
      "Epoch 135, Loss: 0.5048445017462813\n",
      "Epoch 136, Loss: 0.504425284231979\n",
      "Epoch 137, Loss: 0.504022717877823\n",
      "Epoch 138, Loss: 0.5036312802042249\n",
      "Epoch 139, Loss: 0.5032368731068207\n",
      "Epoch 140, Loss: 0.5028511060044271\n",
      "Epoch 141, Loss: 0.5024804933282331\n",
      "Epoch 142, Loss: 0.5021182727396313\n",
      "Epoch 143, Loss: 0.5017581664990297\n",
      "Epoch 144, Loss: 0.5014050827892457\n",
      "Epoch 145, Loss: 0.501047675902349\n",
      "Epoch 146, Loss: 0.500690966247362\n",
      "Epoch 147, Loss: 0.5003476722262875\n",
      "Epoch 148, Loss: 0.5000068544871652\n",
      "Epoch 149, Loss: 0.49966338867073357\n",
      "Epoch 150, Loss: 0.4993280873087794\n",
      "Epoch 151, Loss: 0.49900257940428383\n",
      "Epoch 152, Loss: 0.4986865554458311\n",
      "Epoch 153, Loss: 0.49836949389755036\n",
      "Epoch 154, Loss: 0.49805375636713634\n",
      "Epoch 155, Loss: 0.49773873165141763\n",
      "Epoch 156, Loss: 0.49742224924400874\n",
      "Epoch 157, Loss: 0.49711425826812494\n",
      "Epoch 158, Loss: 0.4968010973402242\n",
      "Epoch 159, Loss: 0.4964901399315246\n",
      "Epoch 160, Loss: 0.49617675026847124\n",
      "Epoch 161, Loss: 0.4958670390985117\n",
      "Epoch 162, Loss: 0.4955578109301908\n",
      "Epoch 163, Loss: 0.4952510223984696\n",
      "Epoch 164, Loss: 0.49494577484111674\n",
      "Epoch 165, Loss: 0.49464511374066544\n",
      "Epoch 166, Loss: 0.49434383269975535\n",
      "Epoch 167, Loss: 0.49404631909698943\n",
      "Epoch 168, Loss: 0.49374530645143583\n",
      "Epoch 169, Loss: 0.49345064806438055\n",
      "Epoch 170, Loss: 0.49317001490258333\n",
      "Epoch 171, Loss: 0.4928927591216884\n",
      "Epoch 172, Loss: 0.4926138499628376\n",
      "Epoch 173, Loss: 0.49233528219511824\n",
      "Epoch 174, Loss: 0.49205852514110515\n",
      "Epoch 175, Loss: 0.4917773399684819\n",
      "Epoch 176, Loss: 0.49149629710928233\n",
      "Epoch 177, Loss: 0.4912165647150265\n",
      "Epoch 178, Loss: 0.49094181765497674\n",
      "Epoch 179, Loss: 0.49066901222163783\n",
      "Epoch 180, Loss: 0.490393928005003\n",
      "Epoch 181, Loss: 0.4901079113527018\n",
      "Epoch 182, Loss: 0.48981984154671765\n",
      "Epoch 183, Loss: 0.4895234443341763\n",
      "Epoch 184, Loss: 0.48921459258368105\n",
      "Epoch 185, Loss: 0.4889145977883947\n",
      "Epoch 186, Loss: 0.4886175142174919\n",
      "Epoch 187, Loss: 0.4883282295514753\n",
      "Epoch 188, Loss: 0.4880435212302353\n",
      "Epoch 189, Loss: 0.48775699700499114\n",
      "Epoch 190, Loss: 0.48748008613994775\n",
      "Epoch 191, Loss: 0.4872132179171775\n",
      "Epoch 192, Loss: 0.48694738158534456\n",
      "Epoch 193, Loss: 0.48668121634032785\n",
      "Epoch 194, Loss: 0.4864280322301768\n",
      "Epoch 195, Loss: 0.486178532155915\n",
      "Epoch 196, Loss: 0.4859301919172944\n",
      "Epoch 197, Loss: 0.48568442460989747\n",
      "Epoch 198, Loss: 0.48544224711238787\n",
      "Epoch 199, Loss: 0.48519729219516017\n",
      "Epoch 200, Loss: 0.4849575446659644\n",
      "Epoch 201, Loss: 0.4847146197220469\n",
      "Epoch 202, Loss: 0.48447155759133576\n",
      "Epoch 203, Loss: 0.48422783492589483\n",
      "Epoch 204, Loss: 0.48397707745066637\n",
      "Epoch 205, Loss: 0.483737926529631\n",
      "Epoch 206, Loss: 0.4835020782648923\n",
      "Epoch 207, Loss: 0.48327083252052233\n",
      "Epoch 208, Loss: 0.4830508282762034\n",
      "Epoch 209, Loss: 0.48283269895503794\n",
      "Epoch 210, Loss: 0.48261215534620283\n",
      "Epoch 211, Loss: 0.48240746542225255\n",
      "Epoch 212, Loss: 0.4822187420754367\n",
      "Epoch 213, Loss: 0.48202997575341067\n",
      "Epoch 214, Loss: 0.4818441197783379\n",
      "Epoch 215, Loss: 0.4816502256898402\n",
      "Epoch 216, Loss: 0.4814589616492725\n",
      "Epoch 217, Loss: 0.48127338055379854\n",
      "Epoch 218, Loss: 0.4810898650639108\n",
      "Epoch 219, Loss: 0.48091365405597575\n",
      "Epoch 220, Loss: 0.480720996546175\n",
      "Epoch 221, Loss: 0.48054865040929456\n",
      "Epoch 222, Loss: 0.4803774677186426\n",
      "Epoch 223, Loss: 0.4801885671816001\n",
      "Epoch 224, Loss: 0.48000935557804647\n",
      "Epoch 225, Loss: 0.4798350303004208\n",
      "Epoch 226, Loss: 0.47967842665052907\n",
      "Epoch 227, Loss: 0.47947854451894606\n",
      "Epoch 228, Loss: 0.47928199900429136\n",
      "Epoch 229, Loss: 0.4791424752085709\n",
      "Epoch 230, Loss: 0.4789254904984497\n",
      "Epoch 231, Loss: 0.4787155823499749\n",
      "Epoch 232, Loss: 0.4785186611779873\n",
      "Epoch 233, Loss: 0.4783287773458093\n",
      "Epoch 234, Loss: 0.4781774454711345\n",
      "Epoch 235, Loss: 0.4780410437330356\n",
      "Epoch 236, Loss: 0.4778263811299359\n",
      "Epoch 237, Loss: 0.47765285017795805\n",
      "Epoch 238, Loss: 0.4775436338740249\n",
      "Epoch 239, Loss: 0.4774342833479412\n",
      "Epoch 240, Loss: 0.477263591547843\n",
      "Epoch 241, Loss: 0.47707859610269837\n",
      "Epoch 242, Loss: 0.47697972966255553\n",
      "Epoch 243, Loss: 0.4768431884384324\n",
      "Epoch 244, Loss: 0.4767408349977067\n",
      "Epoch 245, Loss: 0.4765819952057135\n",
      "Epoch 246, Loss: 0.4764706413665339\n",
      "Epoch 247, Loss: 0.47634462160009333\n",
      "Epoch 248, Loss: 0.4762527991635621\n",
      "Epoch 249, Loss: 0.4762014052862575\n",
      "Epoch 250, Loss: 0.4760283306209857\n",
      "Epoch 251, Loss: 0.47592260832103706\n",
      "Epoch 252, Loss: 0.475834869470249\n",
      "Epoch 253, Loss: 0.47570980894919\n",
      "Epoch 254, Loss: 0.47559397414543253\n",
      "Epoch 255, Loss: 0.4755005182088922\n",
      "Epoch 256, Loss: 0.4753894909333468\n",
      "Epoch 257, Loss: 0.47527958208571863\n",
      "Epoch 258, Loss: 0.47517947896910767\n",
      "Epoch 259, Loss: 0.4750690667233416\n",
      "Epoch 260, Loss: 0.47496625253354696\n",
      "Epoch 261, Loss: 0.47486506352442986\n",
      "Epoch 262, Loss: 0.47477116593942054\n",
      "Epoch 263, Loss: 0.474752914637718\n",
      "Epoch 264, Loss: 0.47457458619766507\n",
      "Epoch 265, Loss: 0.47441594770111134\n",
      "Epoch 266, Loss: 0.4743326895520222\n",
      "Epoch 267, Loss: 0.47424442089802893\n",
      "Epoch 268, Loss: 0.47409961152836255\n",
      "Epoch 269, Loss: 0.47397674971911014\n",
      "Epoch 270, Loss: 0.4739006464998255\n",
      "Epoch 271, Loss: 0.4738391701966867\n",
      "Epoch 272, Loss: 0.4737234441817917\n",
      "Epoch 273, Loss: 0.47358564948211956\n",
      "Epoch 274, Loss: 0.47351060057458727\n",
      "Epoch 275, Loss: 0.4734065036258047\n",
      "Epoch 276, Loss: 0.4733269559263822\n",
      "Epoch 277, Loss: 0.4732341071370639\n",
      "Epoch 278, Loss: 0.4731407519682655\n",
      "Epoch 279, Loss: 0.4730939966069229\n",
      "Epoch 280, Loss: 0.47298024635218033\n",
      "Epoch 281, Loss: 0.47280886054292864\n",
      "Epoch 282, Loss: 0.47271222185116163\n",
      "Epoch 283, Loss: 0.47268730250728586\n",
      "Epoch 284, Loss: 0.4725869656059248\n",
      "Epoch 285, Loss: 0.4724867493169173\n",
      "Epoch 286, Loss: 0.4723455337808729\n",
      "Epoch 287, Loss: 0.4722632042859861\n",
      "Epoch 288, Loss: 0.4722048755884816\n",
      "Epoch 289, Loss: 0.472146413443235\n",
      "Epoch 290, Loss: 0.4720312036831835\n",
      "Epoch 291, Loss: 0.47193135878647013\n",
      "Epoch 292, Loss: 0.4718300873811956\n",
      "Epoch 293, Loss: 0.4717440511887379\n",
      "Epoch 294, Loss: 0.47169887906475694\n",
      "Epoch 295, Loss: 0.4716194856818237\n",
      "Epoch 296, Loss: 0.4715081133867502\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.27791164062525386\n",
      "Valence RMSE: 0.2556914314117623\n",
      "Arousal RMSE: 0.2984822471984633\n",
      "Test R^2 score: tensor([0.2077, 0.1178], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1134, 0.0129], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.16276296610155666\n",
      "Num of epochs: 297\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Epoch 121, Loss: 0.5116998508836337\n",
      "Epoch 122, Loss: 0.5111129902664598\n",
      "Epoch 123, Loss: 0.5105405741436312\n",
      "Epoch 124, Loss: 0.5099849010083686\n",
      "Epoch 125, Loss: 0.5094623759868415\n",
      "Epoch 126, Loss: 0.5089652841754122\n",
      "Epoch 127, Loss: 0.5084717799132713\n",
      "Epoch 128, Loss: 0.5079809643002384\n",
      "Epoch 129, Loss: 0.507507731608986\n",
      "Epoch 130, Loss: 0.5070515727009828\n",
      "Epoch 131, Loss: 0.5065962678643066\n",
      "Epoch 132, Loss: 0.5061448812256618\n",
      "Epoch 133, Loss: 0.5057069703510908\n",
      "Epoch 134, Loss: 0.5052756693952567\n",
      "Epoch 135, Loss: 0.5048445017462813\n",
      "Epoch 136, Loss: 0.504425284231979\n",
      "Epoch 137, Loss: 0.504022717877823\n",
      "Epoch 138, Loss: 0.5036312802042249\n",
      "Epoch 139, Loss: 0.5032368731068207\n",
      "Epoch 140, Loss: 0.5028511060044271\n",
      "Epoch 141, Loss: 0.5024804933282331\n",
      "Epoch 142, Loss: 0.5021182727396313\n",
      "Epoch 143, Loss: 0.5017581664990297\n",
      "Epoch 144, Loss: 0.5014050827892457\n",
      "Epoch 145, Loss: 0.501047675902349\n",
      "Epoch 146, Loss: 0.500690966247362\n",
      "Epoch 147, Loss: 0.5003476722262875\n",
      "Epoch 148, Loss: 0.5000068544871652\n",
      "Epoch 149, Loss: 0.49966338867073357\n",
      "Epoch 150, Loss: 0.4993280873087794\n",
      "Epoch 151, Loss: 0.49900257940428383\n",
      "Epoch 152, Loss: 0.4986865554458311\n",
      "Epoch 153, Loss: 0.49836949389755036\n",
      "Epoch 154, Loss: 0.49805375636713634\n",
      "Epoch 155, Loss: 0.49773873165141763\n",
      "Epoch 156, Loss: 0.49742224924400874\n",
      "Epoch 157, Loss: 0.49711425826812494\n",
      "Epoch 158, Loss: 0.4968010973402242\n",
      "Epoch 159, Loss: 0.4964901399315246\n",
      "Epoch 160, Loss: 0.49617675026847124\n",
      "Epoch 161, Loss: 0.4958670390985117\n",
      "Epoch 162, Loss: 0.4955578109301908\n",
      "Epoch 163, Loss: 0.4952510223984696\n",
      "Epoch 164, Loss: 0.49494577484111674\n",
      "Epoch 165, Loss: 0.49464511374066544\n",
      "Epoch 166, Loss: 0.49434383269975535\n",
      "Epoch 167, Loss: 0.49404631909698943\n",
      "Epoch 168, Loss: 0.49374530645143583\n",
      "Epoch 169, Loss: 0.49345064806438055\n",
      "Epoch 170, Loss: 0.49317001490258333\n",
      "Epoch 171, Loss: 0.4928927591216884\n",
      "Epoch 172, Loss: 0.4926138499628376\n",
      "Epoch 173, Loss: 0.49233528219511824\n",
      "Epoch 174, Loss: 0.49205852514110515\n",
      "Epoch 175, Loss: 0.4917773399684819\n",
      "Epoch 176, Loss: 0.49149629710928233\n",
      "Epoch 177, Loss: 0.4912165647150265\n",
      "Epoch 178, Loss: 0.49094181765497674\n",
      "Epoch 179, Loss: 0.49066901222163783\n",
      "Epoch 180, Loss: 0.490393928005003\n",
      "Epoch 181, Loss: 0.4901079113527018\n",
      "Epoch 182, Loss: 0.48981984154671765\n",
      "Epoch 183, Loss: 0.4895234443341763\n",
      "Epoch 184, Loss: 0.48921459258368105\n",
      "Epoch 185, Loss: 0.4889145977883947\n",
      "Epoch 186, Loss: 0.4886175142174919\n",
      "Epoch 187, Loss: 0.4883282295514753\n",
      "Epoch 188, Loss: 0.4880435212302353\n",
      "Epoch 189, Loss: 0.48775699700499114\n",
      "Epoch 190, Loss: 0.48748008613994775\n",
      "Epoch 191, Loss: 0.4872132179171775\n",
      "Epoch 192, Loss: 0.48694738158534456\n",
      "Epoch 193, Loss: 0.48668121634032785\n",
      "Epoch 194, Loss: 0.4864280322301768\n",
      "Epoch 195, Loss: 0.486178532155915\n",
      "Epoch 196, Loss: 0.4859301919172944\n",
      "Epoch 197, Loss: 0.48568442460989747\n",
      "Epoch 198, Loss: 0.48544224711238787\n",
      "Epoch 199, Loss: 0.48519729219516017\n",
      "Epoch 200, Loss: 0.4849575446659644\n",
      "Epoch 201, Loss: 0.4847146197220469\n",
      "Epoch 202, Loss: 0.48447155759133576\n",
      "Epoch 203, Loss: 0.48422783492589483\n",
      "Epoch 204, Loss: 0.48397707745066637\n",
      "Epoch 205, Loss: 0.483737926529631\n",
      "Epoch 206, Loss: 0.4835020782648923\n",
      "Epoch 207, Loss: 0.48327083252052233\n",
      "Epoch 208, Loss: 0.4830508282762034\n",
      "Epoch 209, Loss: 0.48283269895503794\n",
      "Epoch 210, Loss: 0.48261215534620283\n",
      "Epoch 211, Loss: 0.48240746542225255\n",
      "Epoch 212, Loss: 0.4822187420754367\n",
      "Epoch 213, Loss: 0.48202997575341067\n",
      "Epoch 214, Loss: 0.4818441197783379\n",
      "Epoch 215, Loss: 0.4816502256898402\n",
      "Epoch 216, Loss: 0.4814589616492725\n",
      "Epoch 217, Loss: 0.48127338055379854\n",
      "Epoch 218, Loss: 0.4810898650639108\n",
      "Epoch 219, Loss: 0.48091365405597575\n",
      "Epoch 220, Loss: 0.480720996546175\n",
      "Epoch 221, Loss: 0.48054865040929456\n",
      "Epoch 222, Loss: 0.4803774677186426\n",
      "Epoch 223, Loss: 0.4801885671816001\n",
      "Epoch 224, Loss: 0.48000935557804647\n",
      "Epoch 225, Loss: 0.4798350303004208\n",
      "Epoch 226, Loss: 0.47967842665052907\n",
      "Epoch 227, Loss: 0.47947854451894606\n",
      "Epoch 228, Loss: 0.47928199900429136\n",
      "Epoch 229, Loss: 0.4791424752085709\n",
      "Epoch 230, Loss: 0.4789254904984497\n",
      "Epoch 231, Loss: 0.4787155823499749\n",
      "Epoch 232, Loss: 0.4785186611779873\n",
      "Epoch 233, Loss: 0.4783287773458093\n",
      "Epoch 234, Loss: 0.4781774454711345\n",
      "Epoch 235, Loss: 0.4780410437330356\n",
      "Epoch 236, Loss: 0.4778263811299359\n",
      "Epoch 237, Loss: 0.47765285017795805\n",
      "Epoch 238, Loss: 0.4775436338740249\n",
      "Epoch 239, Loss: 0.4774342833479412\n",
      "Epoch 240, Loss: 0.477263591547843\n",
      "Epoch 241, Loss: 0.47707859610269837\n",
      "Epoch 242, Loss: 0.47697972966255553\n",
      "Epoch 243, Loss: 0.4768431884384324\n",
      "Epoch 244, Loss: 0.4767408349977067\n",
      "Epoch 245, Loss: 0.4765819952057135\n",
      "Epoch 246, Loss: 0.4764706413665339\n",
      "Epoch 247, Loss: 0.47634462160009333\n",
      "Epoch 248, Loss: 0.4762527991635621\n",
      "Epoch 249, Loss: 0.4762014052862575\n",
      "Epoch 250, Loss: 0.4760283306209857\n",
      "Epoch 251, Loss: 0.47592260832103706\n",
      "Epoch 252, Loss: 0.475834869470249\n",
      "Epoch 253, Loss: 0.47570980894919\n",
      "Epoch 254, Loss: 0.47559397414543253\n",
      "Epoch 255, Loss: 0.4755005182088922\n",
      "Epoch 256, Loss: 0.4753894909333468\n",
      "Epoch 257, Loss: 0.47527958208571863\n",
      "Epoch 258, Loss: 0.47517947896910767\n",
      "Epoch 259, Loss: 0.4750690667233416\n",
      "Epoch 260, Loss: 0.47496625253354696\n",
      "Epoch 261, Loss: 0.47486506352442986\n",
      "Epoch 262, Loss: 0.47477116593942054\n",
      "Epoch 263, Loss: 0.474752914637718\n",
      "Epoch 264, Loss: 0.47457458619766507\n",
      "Epoch 265, Loss: 0.47441594770111134\n",
      "Epoch 266, Loss: 0.4743326895520222\n",
      "Epoch 267, Loss: 0.47424442089802893\n",
      "Epoch 268, Loss: 0.47409961152836255\n",
      "Epoch 269, Loss: 0.47397674971911014\n",
      "Epoch 270, Loss: 0.4739006464998255\n",
      "Epoch 271, Loss: 0.4738391701966867\n",
      "Epoch 272, Loss: 0.4737234441817917\n",
      "Epoch 273, Loss: 0.47358564948211956\n",
      "Epoch 274, Loss: 0.47351060057458727\n",
      "Epoch 275, Loss: 0.4734065036258047\n",
      "Epoch 276, Loss: 0.4733269559263822\n",
      "Epoch 277, Loss: 0.4732341071370639\n",
      "Epoch 278, Loss: 0.4731407519682655\n",
      "Epoch 279, Loss: 0.4730939966069229\n",
      "Epoch 280, Loss: 0.47298024635218033\n",
      "Epoch 281, Loss: 0.47280886054292864\n",
      "Epoch 282, Loss: 0.47271222185116163\n",
      "Epoch 283, Loss: 0.47268730250728586\n",
      "Epoch 284, Loss: 0.4725869656059248\n",
      "Epoch 285, Loss: 0.4724867493169173\n",
      "Epoch 286, Loss: 0.4723455337808729\n",
      "Epoch 287, Loss: 0.4722632042859861\n",
      "Epoch 288, Loss: 0.4722048755884816\n",
      "Epoch 289, Loss: 0.472146413443235\n",
      "Epoch 290, Loss: 0.4720312036831835\n",
      "Epoch 291, Loss: 0.47193135878647013\n",
      "Epoch 292, Loss: 0.4718300873811956\n",
      "Epoch 293, Loss: 0.4717440511887379\n",
      "Epoch 294, Loss: 0.47169887906475694\n",
      "Epoch 295, Loss: 0.4716194856818237\n",
      "Epoch 296, Loss: 0.4715081133867502\n",
      "Epoch 297, Loss: 0.47138116207763575\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.27789663424810584\n",
      "Valence RMSE: 0.2557050062151933\n",
      "Arousal RMSE: 0.2984426719645483\n",
      "Test R^2 score: tensor([0.2076, 0.1181], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1133, 0.0132], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.16283785631248543\n",
      "Num of epochs: 298\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Epoch 121, Loss: 0.5116998508836337\n",
      "Epoch 122, Loss: 0.5111129902664598\n",
      "Epoch 123, Loss: 0.5105405741436312\n",
      "Epoch 124, Loss: 0.5099849010083686\n",
      "Epoch 125, Loss: 0.5094623759868415\n",
      "Epoch 126, Loss: 0.5089652841754122\n",
      "Epoch 127, Loss: 0.5084717799132713\n",
      "Epoch 128, Loss: 0.5079809643002384\n",
      "Epoch 129, Loss: 0.507507731608986\n",
      "Epoch 130, Loss: 0.5070515727009828\n",
      "Epoch 131, Loss: 0.5065962678643066\n",
      "Epoch 132, Loss: 0.5061448812256618\n",
      "Epoch 133, Loss: 0.5057069703510908\n",
      "Epoch 134, Loss: 0.5052756693952567\n",
      "Epoch 135, Loss: 0.5048445017462813\n",
      "Epoch 136, Loss: 0.504425284231979\n",
      "Epoch 137, Loss: 0.504022717877823\n",
      "Epoch 138, Loss: 0.5036312802042249\n",
      "Epoch 139, Loss: 0.5032368731068207\n",
      "Epoch 140, Loss: 0.5028511060044271\n",
      "Epoch 141, Loss: 0.5024804933282331\n",
      "Epoch 142, Loss: 0.5021182727396313\n",
      "Epoch 143, Loss: 0.5017581664990297\n",
      "Epoch 144, Loss: 0.5014050827892457\n",
      "Epoch 145, Loss: 0.501047675902349\n",
      "Epoch 146, Loss: 0.500690966247362\n",
      "Epoch 147, Loss: 0.5003476722262875\n",
      "Epoch 148, Loss: 0.5000068544871652\n",
      "Epoch 149, Loss: 0.49966338867073357\n",
      "Epoch 150, Loss: 0.4993280873087794\n",
      "Epoch 151, Loss: 0.49900257940428383\n",
      "Epoch 152, Loss: 0.4986865554458311\n",
      "Epoch 153, Loss: 0.49836949389755036\n",
      "Epoch 154, Loss: 0.49805375636713634\n",
      "Epoch 155, Loss: 0.49773873165141763\n",
      "Epoch 156, Loss: 0.49742224924400874\n",
      "Epoch 157, Loss: 0.49711425826812494\n",
      "Epoch 158, Loss: 0.4968010973402242\n",
      "Epoch 159, Loss: 0.4964901399315246\n",
      "Epoch 160, Loss: 0.49617675026847124\n",
      "Epoch 161, Loss: 0.4958670390985117\n",
      "Epoch 162, Loss: 0.4955578109301908\n",
      "Epoch 163, Loss: 0.4952510223984696\n",
      "Epoch 164, Loss: 0.49494577484111674\n",
      "Epoch 165, Loss: 0.49464511374066544\n",
      "Epoch 166, Loss: 0.49434383269975535\n",
      "Epoch 167, Loss: 0.49404631909698943\n",
      "Epoch 168, Loss: 0.49374530645143583\n",
      "Epoch 169, Loss: 0.49345064806438055\n",
      "Epoch 170, Loss: 0.49317001490258333\n",
      "Epoch 171, Loss: 0.4928927591216884\n",
      "Epoch 172, Loss: 0.4926138499628376\n",
      "Epoch 173, Loss: 0.49233528219511824\n",
      "Epoch 174, Loss: 0.49205852514110515\n",
      "Epoch 175, Loss: 0.4917773399684819\n",
      "Epoch 176, Loss: 0.49149629710928233\n",
      "Epoch 177, Loss: 0.4912165647150265\n",
      "Epoch 178, Loss: 0.49094181765497674\n",
      "Epoch 179, Loss: 0.49066901222163783\n",
      "Epoch 180, Loss: 0.490393928005003\n",
      "Epoch 181, Loss: 0.4901079113527018\n",
      "Epoch 182, Loss: 0.48981984154671765\n",
      "Epoch 183, Loss: 0.4895234443341763\n",
      "Epoch 184, Loss: 0.48921459258368105\n",
      "Epoch 185, Loss: 0.4889145977883947\n",
      "Epoch 186, Loss: 0.4886175142174919\n",
      "Epoch 187, Loss: 0.4883282295514753\n",
      "Epoch 188, Loss: 0.4880435212302353\n",
      "Epoch 189, Loss: 0.48775699700499114\n",
      "Epoch 190, Loss: 0.48748008613994775\n",
      "Epoch 191, Loss: 0.4872132179171775\n",
      "Epoch 192, Loss: 0.48694738158534456\n",
      "Epoch 193, Loss: 0.48668121634032785\n",
      "Epoch 194, Loss: 0.4864280322301768\n",
      "Epoch 195, Loss: 0.486178532155915\n",
      "Epoch 196, Loss: 0.4859301919172944\n",
      "Epoch 197, Loss: 0.48568442460989747\n",
      "Epoch 198, Loss: 0.48544224711238787\n",
      "Epoch 199, Loss: 0.48519729219516017\n",
      "Epoch 200, Loss: 0.4849575446659644\n",
      "Epoch 201, Loss: 0.4847146197220469\n",
      "Epoch 202, Loss: 0.48447155759133576\n",
      "Epoch 203, Loss: 0.48422783492589483\n",
      "Epoch 204, Loss: 0.48397707745066637\n",
      "Epoch 205, Loss: 0.483737926529631\n",
      "Epoch 206, Loss: 0.4835020782648923\n",
      "Epoch 207, Loss: 0.48327083252052233\n",
      "Epoch 208, Loss: 0.4830508282762034\n",
      "Epoch 209, Loss: 0.48283269895503794\n",
      "Epoch 210, Loss: 0.48261215534620283\n",
      "Epoch 211, Loss: 0.48240746542225255\n",
      "Epoch 212, Loss: 0.4822187420754367\n",
      "Epoch 213, Loss: 0.48202997575341067\n",
      "Epoch 214, Loss: 0.4818441197783379\n",
      "Epoch 215, Loss: 0.4816502256898402\n",
      "Epoch 216, Loss: 0.4814589616492725\n",
      "Epoch 217, Loss: 0.48127338055379854\n",
      "Epoch 218, Loss: 0.4810898650639108\n",
      "Epoch 219, Loss: 0.48091365405597575\n",
      "Epoch 220, Loss: 0.480720996546175\n",
      "Epoch 221, Loss: 0.48054865040929456\n",
      "Epoch 222, Loss: 0.4803774677186426\n",
      "Epoch 223, Loss: 0.4801885671816001\n",
      "Epoch 224, Loss: 0.48000935557804647\n",
      "Epoch 225, Loss: 0.4798350303004208\n",
      "Epoch 226, Loss: 0.47967842665052907\n",
      "Epoch 227, Loss: 0.47947854451894606\n",
      "Epoch 228, Loss: 0.47928199900429136\n",
      "Epoch 229, Loss: 0.4791424752085709\n",
      "Epoch 230, Loss: 0.4789254904984497\n",
      "Epoch 231, Loss: 0.4787155823499749\n",
      "Epoch 232, Loss: 0.4785186611779873\n",
      "Epoch 233, Loss: 0.4783287773458093\n",
      "Epoch 234, Loss: 0.4781774454711345\n",
      "Epoch 235, Loss: 0.4780410437330356\n",
      "Epoch 236, Loss: 0.4778263811299359\n",
      "Epoch 237, Loss: 0.47765285017795805\n",
      "Epoch 238, Loss: 0.4775436338740249\n",
      "Epoch 239, Loss: 0.4774342833479412\n",
      "Epoch 240, Loss: 0.477263591547843\n",
      "Epoch 241, Loss: 0.47707859610269837\n",
      "Epoch 242, Loss: 0.47697972966255553\n",
      "Epoch 243, Loss: 0.4768431884384324\n",
      "Epoch 244, Loss: 0.4767408349977067\n",
      "Epoch 245, Loss: 0.4765819952057135\n",
      "Epoch 246, Loss: 0.4764706413665339\n",
      "Epoch 247, Loss: 0.47634462160009333\n",
      "Epoch 248, Loss: 0.4762527991635621\n",
      "Epoch 249, Loss: 0.4762014052862575\n",
      "Epoch 250, Loss: 0.4760283306209857\n",
      "Epoch 251, Loss: 0.47592260832103706\n",
      "Epoch 252, Loss: 0.475834869470249\n",
      "Epoch 253, Loss: 0.47570980894919\n",
      "Epoch 254, Loss: 0.47559397414543253\n",
      "Epoch 255, Loss: 0.4755005182088922\n",
      "Epoch 256, Loss: 0.4753894909333468\n",
      "Epoch 257, Loss: 0.47527958208571863\n",
      "Epoch 258, Loss: 0.47517947896910767\n",
      "Epoch 259, Loss: 0.4750690667233416\n",
      "Epoch 260, Loss: 0.47496625253354696\n",
      "Epoch 261, Loss: 0.47486506352442986\n",
      "Epoch 262, Loss: 0.47477116593942054\n",
      "Epoch 263, Loss: 0.474752914637718\n",
      "Epoch 264, Loss: 0.47457458619766507\n",
      "Epoch 265, Loss: 0.47441594770111134\n",
      "Epoch 266, Loss: 0.4743326895520222\n",
      "Epoch 267, Loss: 0.47424442089802893\n",
      "Epoch 268, Loss: 0.47409961152836255\n",
      "Epoch 269, Loss: 0.47397674971911014\n",
      "Epoch 270, Loss: 0.4739006464998255\n",
      "Epoch 271, Loss: 0.4738391701966867\n",
      "Epoch 272, Loss: 0.4737234441817917\n",
      "Epoch 273, Loss: 0.47358564948211956\n",
      "Epoch 274, Loss: 0.47351060057458727\n",
      "Epoch 275, Loss: 0.4734065036258047\n",
      "Epoch 276, Loss: 0.4733269559263822\n",
      "Epoch 277, Loss: 0.4732341071370639\n",
      "Epoch 278, Loss: 0.4731407519682655\n",
      "Epoch 279, Loss: 0.4730939966069229\n",
      "Epoch 280, Loss: 0.47298024635218033\n",
      "Epoch 281, Loss: 0.47280886054292864\n",
      "Epoch 282, Loss: 0.47271222185116163\n",
      "Epoch 283, Loss: 0.47268730250728586\n",
      "Epoch 284, Loss: 0.4725869656059248\n",
      "Epoch 285, Loss: 0.4724867493169173\n",
      "Epoch 286, Loss: 0.4723455337808729\n",
      "Epoch 287, Loss: 0.4722632042859861\n",
      "Epoch 288, Loss: 0.4722048755884816\n",
      "Epoch 289, Loss: 0.472146413443235\n",
      "Epoch 290, Loss: 0.4720312036831835\n",
      "Epoch 291, Loss: 0.47193135878647013\n",
      "Epoch 292, Loss: 0.4718300873811956\n",
      "Epoch 293, Loss: 0.4717440511887379\n",
      "Epoch 294, Loss: 0.47169887906475694\n",
      "Epoch 295, Loss: 0.4716194856818237\n",
      "Epoch 296, Loss: 0.4715081133867502\n",
      "Epoch 297, Loss: 0.47138116207763575\n",
      "Epoch 298, Loss: 0.4712942218774766\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2780158693441045\n",
      "Valence RMSE: 0.2557535288165629\n",
      "Arousal RMSE: 0.29862313994764683\n",
      "Test R^2 score: tensor([0.2073, 0.1170], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1130, 0.0120], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.16215401667380996\n",
      "Num of epochs: 299\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Epoch 121, Loss: 0.5116998508836337\n",
      "Epoch 122, Loss: 0.5111129902664598\n",
      "Epoch 123, Loss: 0.5105405741436312\n",
      "Epoch 124, Loss: 0.5099849010083686\n",
      "Epoch 125, Loss: 0.5094623759868415\n",
      "Epoch 126, Loss: 0.5089652841754122\n",
      "Epoch 127, Loss: 0.5084717799132713\n",
      "Epoch 128, Loss: 0.5079809643002384\n",
      "Epoch 129, Loss: 0.507507731608986\n",
      "Epoch 130, Loss: 0.5070515727009828\n",
      "Epoch 131, Loss: 0.5065962678643066\n",
      "Epoch 132, Loss: 0.5061448812256618\n",
      "Epoch 133, Loss: 0.5057069703510908\n",
      "Epoch 134, Loss: 0.5052756693952567\n",
      "Epoch 135, Loss: 0.5048445017462813\n",
      "Epoch 136, Loss: 0.504425284231979\n",
      "Epoch 137, Loss: 0.504022717877823\n",
      "Epoch 138, Loss: 0.5036312802042249\n",
      "Epoch 139, Loss: 0.5032368731068207\n",
      "Epoch 140, Loss: 0.5028511060044271\n",
      "Epoch 141, Loss: 0.5024804933282331\n",
      "Epoch 142, Loss: 0.5021182727396313\n",
      "Epoch 143, Loss: 0.5017581664990297\n",
      "Epoch 144, Loss: 0.5014050827892457\n",
      "Epoch 145, Loss: 0.501047675902349\n",
      "Epoch 146, Loss: 0.500690966247362\n",
      "Epoch 147, Loss: 0.5003476722262875\n",
      "Epoch 148, Loss: 0.5000068544871652\n",
      "Epoch 149, Loss: 0.49966338867073357\n",
      "Epoch 150, Loss: 0.4993280873087794\n",
      "Epoch 151, Loss: 0.49900257940428383\n",
      "Epoch 152, Loss: 0.4986865554458311\n",
      "Epoch 153, Loss: 0.49836949389755036\n",
      "Epoch 154, Loss: 0.49805375636713634\n",
      "Epoch 155, Loss: 0.49773873165141763\n",
      "Epoch 156, Loss: 0.49742224924400874\n",
      "Epoch 157, Loss: 0.49711425826812494\n",
      "Epoch 158, Loss: 0.4968010973402242\n",
      "Epoch 159, Loss: 0.4964901399315246\n",
      "Epoch 160, Loss: 0.49617675026847124\n",
      "Epoch 161, Loss: 0.4958670390985117\n",
      "Epoch 162, Loss: 0.4955578109301908\n",
      "Epoch 163, Loss: 0.4952510223984696\n",
      "Epoch 164, Loss: 0.49494577484111674\n",
      "Epoch 165, Loss: 0.49464511374066544\n",
      "Epoch 166, Loss: 0.49434383269975535\n",
      "Epoch 167, Loss: 0.49404631909698943\n",
      "Epoch 168, Loss: 0.49374530645143583\n",
      "Epoch 169, Loss: 0.49345064806438055\n",
      "Epoch 170, Loss: 0.49317001490258333\n",
      "Epoch 171, Loss: 0.4928927591216884\n",
      "Epoch 172, Loss: 0.4926138499628376\n",
      "Epoch 173, Loss: 0.49233528219511824\n",
      "Epoch 174, Loss: 0.49205852514110515\n",
      "Epoch 175, Loss: 0.4917773399684819\n",
      "Epoch 176, Loss: 0.49149629710928233\n",
      "Epoch 177, Loss: 0.4912165647150265\n",
      "Epoch 178, Loss: 0.49094181765497674\n",
      "Epoch 179, Loss: 0.49066901222163783\n",
      "Epoch 180, Loss: 0.490393928005003\n",
      "Epoch 181, Loss: 0.4901079113527018\n",
      "Epoch 182, Loss: 0.48981984154671765\n",
      "Epoch 183, Loss: 0.4895234443341763\n",
      "Epoch 184, Loss: 0.48921459258368105\n",
      "Epoch 185, Loss: 0.4889145977883947\n",
      "Epoch 186, Loss: 0.4886175142174919\n",
      "Epoch 187, Loss: 0.4883282295514753\n",
      "Epoch 188, Loss: 0.4880435212302353\n",
      "Epoch 189, Loss: 0.48775699700499114\n",
      "Epoch 190, Loss: 0.48748008613994775\n",
      "Epoch 191, Loss: 0.4872132179171775\n",
      "Epoch 192, Loss: 0.48694738158534456\n",
      "Epoch 193, Loss: 0.48668121634032785\n",
      "Epoch 194, Loss: 0.4864280322301768\n",
      "Epoch 195, Loss: 0.486178532155915\n",
      "Epoch 196, Loss: 0.4859301919172944\n",
      "Epoch 197, Loss: 0.48568442460989747\n",
      "Epoch 198, Loss: 0.48544224711238787\n",
      "Epoch 199, Loss: 0.48519729219516017\n",
      "Epoch 200, Loss: 0.4849575446659644\n",
      "Epoch 201, Loss: 0.4847146197220469\n",
      "Epoch 202, Loss: 0.48447155759133576\n",
      "Epoch 203, Loss: 0.48422783492589483\n",
      "Epoch 204, Loss: 0.48397707745066637\n",
      "Epoch 205, Loss: 0.483737926529631\n",
      "Epoch 206, Loss: 0.4835020782648923\n",
      "Epoch 207, Loss: 0.48327083252052233\n",
      "Epoch 208, Loss: 0.4830508282762034\n",
      "Epoch 209, Loss: 0.48283269895503794\n",
      "Epoch 210, Loss: 0.48261215534620283\n",
      "Epoch 211, Loss: 0.48240746542225255\n",
      "Epoch 212, Loss: 0.4822187420754367\n",
      "Epoch 213, Loss: 0.48202997575341067\n",
      "Epoch 214, Loss: 0.4818441197783379\n",
      "Epoch 215, Loss: 0.4816502256898402\n",
      "Epoch 216, Loss: 0.4814589616492725\n",
      "Epoch 217, Loss: 0.48127338055379854\n",
      "Epoch 218, Loss: 0.4810898650639108\n",
      "Epoch 219, Loss: 0.48091365405597575\n",
      "Epoch 220, Loss: 0.480720996546175\n",
      "Epoch 221, Loss: 0.48054865040929456\n",
      "Epoch 222, Loss: 0.4803774677186426\n",
      "Epoch 223, Loss: 0.4801885671816001\n",
      "Epoch 224, Loss: 0.48000935557804647\n",
      "Epoch 225, Loss: 0.4798350303004208\n",
      "Epoch 226, Loss: 0.47967842665052907\n",
      "Epoch 227, Loss: 0.47947854451894606\n",
      "Epoch 228, Loss: 0.47928199900429136\n",
      "Epoch 229, Loss: 0.4791424752085709\n",
      "Epoch 230, Loss: 0.4789254904984497\n",
      "Epoch 231, Loss: 0.4787155823499749\n",
      "Epoch 232, Loss: 0.4785186611779873\n",
      "Epoch 233, Loss: 0.4783287773458093\n",
      "Epoch 234, Loss: 0.4781774454711345\n",
      "Epoch 235, Loss: 0.4780410437330356\n",
      "Epoch 236, Loss: 0.4778263811299359\n",
      "Epoch 237, Loss: 0.47765285017795805\n",
      "Epoch 238, Loss: 0.4775436338740249\n",
      "Epoch 239, Loss: 0.4774342833479412\n",
      "Epoch 240, Loss: 0.477263591547843\n",
      "Epoch 241, Loss: 0.47707859610269837\n",
      "Epoch 242, Loss: 0.47697972966255553\n",
      "Epoch 243, Loss: 0.4768431884384324\n",
      "Epoch 244, Loss: 0.4767408349977067\n",
      "Epoch 245, Loss: 0.4765819952057135\n",
      "Epoch 246, Loss: 0.4764706413665339\n",
      "Epoch 247, Loss: 0.47634462160009333\n",
      "Epoch 248, Loss: 0.4762527991635621\n",
      "Epoch 249, Loss: 0.4762014052862575\n",
      "Epoch 250, Loss: 0.4760283306209857\n",
      "Epoch 251, Loss: 0.47592260832103706\n",
      "Epoch 252, Loss: 0.475834869470249\n",
      "Epoch 253, Loss: 0.47570980894919\n",
      "Epoch 254, Loss: 0.47559397414543253\n",
      "Epoch 255, Loss: 0.4755005182088922\n",
      "Epoch 256, Loss: 0.4753894909333468\n",
      "Epoch 257, Loss: 0.47527958208571863\n",
      "Epoch 258, Loss: 0.47517947896910767\n",
      "Epoch 259, Loss: 0.4750690667233416\n",
      "Epoch 260, Loss: 0.47496625253354696\n",
      "Epoch 261, Loss: 0.47486506352442986\n",
      "Epoch 262, Loss: 0.47477116593942054\n",
      "Epoch 263, Loss: 0.474752914637718\n",
      "Epoch 264, Loss: 0.47457458619766507\n",
      "Epoch 265, Loss: 0.47441594770111134\n",
      "Epoch 266, Loss: 0.4743326895520222\n",
      "Epoch 267, Loss: 0.47424442089802893\n",
      "Epoch 268, Loss: 0.47409961152836255\n",
      "Epoch 269, Loss: 0.47397674971911014\n",
      "Epoch 270, Loss: 0.4739006464998255\n",
      "Epoch 271, Loss: 0.4738391701966867\n",
      "Epoch 272, Loss: 0.4737234441817917\n",
      "Epoch 273, Loss: 0.47358564948211956\n",
      "Epoch 274, Loss: 0.47351060057458727\n",
      "Epoch 275, Loss: 0.4734065036258047\n",
      "Epoch 276, Loss: 0.4733269559263822\n",
      "Epoch 277, Loss: 0.4732341071370639\n",
      "Epoch 278, Loss: 0.4731407519682655\n",
      "Epoch 279, Loss: 0.4730939966069229\n",
      "Epoch 280, Loss: 0.47298024635218033\n",
      "Epoch 281, Loss: 0.47280886054292864\n",
      "Epoch 282, Loss: 0.47271222185116163\n",
      "Epoch 283, Loss: 0.47268730250728586\n",
      "Epoch 284, Loss: 0.4725869656059248\n",
      "Epoch 285, Loss: 0.4724867493169173\n",
      "Epoch 286, Loss: 0.4723455337808729\n",
      "Epoch 287, Loss: 0.4722632042859861\n",
      "Epoch 288, Loss: 0.4722048755884816\n",
      "Epoch 289, Loss: 0.472146413443235\n",
      "Epoch 290, Loss: 0.4720312036831835\n",
      "Epoch 291, Loss: 0.47193135878647013\n",
      "Epoch 292, Loss: 0.4718300873811956\n",
      "Epoch 293, Loss: 0.4717440511887379\n",
      "Epoch 294, Loss: 0.47169887906475694\n",
      "Epoch 295, Loss: 0.4716194856818237\n",
      "Epoch 296, Loss: 0.4715081133867502\n",
      "Epoch 297, Loss: 0.47138116207763575\n",
      "Epoch 298, Loss: 0.4712942218774766\n",
      "Epoch 299, Loss: 0.4711807328828787\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.27795863186885456\n",
      "Valence RMSE: 0.2557016232008034\n",
      "Arousal RMSE: 0.2985610188107034\n",
      "Test R^2 score: tensor([0.2076, 0.1174], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1133, 0.0124], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.16249854562897614\n",
      "Num of epochs: 300\n",
      "Epoch 1, Loss: 0.6117395351186724\n",
      "Epoch 2, Loss: 0.6100289674822418\n",
      "Epoch 3, Loss: 0.6083395548288977\n",
      "Epoch 4, Loss: 0.606698049564977\n",
      "Epoch 5, Loss: 0.605190631609765\n",
      "Epoch 6, Loss: 0.6036950002224712\n",
      "Epoch 7, Loss: 0.6022123072156196\n",
      "Epoch 8, Loss: 0.6007437645933451\n",
      "Epoch 9, Loss: 0.5992894017869131\n",
      "Epoch 10, Loss: 0.597849023184856\n",
      "Epoch 11, Loss: 0.5964229549619942\n",
      "Epoch 12, Loss: 0.59539822138981\n",
      "Epoch 13, Loss: 0.5943946851966364\n",
      "Epoch 14, Loss: 0.5933733548594994\n",
      "Epoch 15, Loss: 0.5923438990458686\n",
      "Epoch 16, Loss: 0.5913118949816885\n",
      "Epoch 17, Loss: 0.5902809392308364\n",
      "Epoch 18, Loss: 0.5892537937147383\n",
      "Epoch 19, Loss: 0.5882323783037903\n",
      "Epoch 20, Loss: 0.5872183469554043\n",
      "Epoch 21, Loss: 0.5862129581193154\n",
      "Epoch 22, Loss: 0.5852172239186628\n",
      "Epoch 23, Loss: 0.5842319843908202\n",
      "Epoch 24, Loss: 0.583258008067213\n",
      "Epoch 25, Loss: 0.5822958632734793\n",
      "Epoch 26, Loss: 0.5813460957656645\n",
      "Epoch 27, Loss: 0.5804091257352615\n",
      "Epoch 28, Loss: 0.5794852466894013\n",
      "Epoch 29, Loss: 0.5785748303992896\n",
      "Epoch 30, Loss: 0.5776780694911778\n",
      "Epoch 31, Loss: 0.5767951568284447\n",
      "Epoch 32, Loss: 0.575926130227074\n",
      "Epoch 33, Loss: 0.5750710785512724\n",
      "Epoch 34, Loss: 0.5742299344796574\n",
      "Epoch 35, Loss: 0.573402707242346\n",
      "Epoch 36, Loss: 0.5725892749876537\n",
      "Epoch 37, Loss: 0.571789514166149\n",
      "Epoch 38, Loss: 0.5710033778376303\n",
      "Epoch 39, Loss: 0.5702309484832246\n",
      "Epoch 40, Loss: 0.5694705025466389\n",
      "Epoch 41, Loss: 0.5687203588202522\n",
      "Epoch 42, Loss: 0.5679816337708823\n",
      "Epoch 43, Loss: 0.5672548448499469\n",
      "Epoch 44, Loss: 0.5665403273168403\n",
      "Epoch 45, Loss: 0.5658383910056566\n",
      "Epoch 46, Loss: 0.5651494519302341\n",
      "Epoch 47, Loss: 0.5645200960233066\n",
      "Epoch 48, Loss: 0.5639459780225204\n",
      "Epoch 49, Loss: 0.563377649362385\n",
      "Epoch 50, Loss: 0.5628166631939656\n",
      "Epoch 51, Loss: 0.562264790631749\n",
      "Epoch 52, Loss: 0.5617237297760538\n",
      "Epoch 53, Loss: 0.5611951316062225\n",
      "Epoch 54, Loss: 0.5606806525651147\n",
      "Epoch 55, Loss: 0.5601771128665154\n",
      "Epoch 56, Loss: 0.5596898402298777\n",
      "Epoch 57, Loss: 0.5592225810130512\n",
      "Epoch 58, Loss: 0.5587763987882572\n",
      "Epoch 59, Loss: 0.5583530787871823\n",
      "Epoch 60, Loss: 0.5579541953353617\n",
      "Epoch 61, Loss: 0.5575796405290059\n",
      "Epoch 62, Loss: 0.5572266288193036\n",
      "Epoch 63, Loss: 0.5568932746260548\n",
      "Epoch 64, Loss: 0.5565874576273963\n",
      "Epoch 65, Loss: 0.5563124643035867\n",
      "Epoch 66, Loss: 0.5560675632606019\n",
      "Epoch 67, Loss: 0.555852070463524\n",
      "Epoch 68, Loss: 0.5556653228906372\n",
      "Epoch 69, Loss: 0.555506383852325\n",
      "Epoch 70, Loss: 0.5553724868137146\n",
      "Epoch 71, Loss: 0.5552561625431086\n",
      "Epoch 72, Loss: 0.5551480812268414\n",
      "Epoch 73, Loss: 0.5550383948886488\n",
      "Epoch 74, Loss: 0.5549175161635991\n",
      "Epoch 75, Loss: 0.5547778364953306\n",
      "Epoch 76, Loss: 0.5546125441935013\n",
      "Epoch 77, Loss: 0.5544163484431495\n",
      "Epoch 78, Loss: 0.5541852369519318\n",
      "Epoch 79, Loss: 0.5539145389684554\n",
      "Epoch 80, Loss: 0.5536000781622181\n",
      "Epoch 81, Loss: 0.5532359351427226\n",
      "Epoch 82, Loss: 0.5528163781685054\n",
      "Epoch 83, Loss: 0.5523370723509559\n",
      "Epoch 84, Loss: 0.5517927580713204\n",
      "Epoch 85, Loss: 0.5511788090000028\n",
      "Epoch 86, Loss: 0.5504902280482418\n",
      "Epoch 87, Loss: 0.5497214218698392\n",
      "Epoch 88, Loss: 0.5488686054546167\n",
      "Epoch 89, Loss: 0.5479294284692942\n",
      "Epoch 90, Loss: 0.5469017022289654\n",
      "Epoch 91, Loss: 0.545785308744168\n",
      "Epoch 92, Loss: 0.5445838891833775\n",
      "Epoch 93, Loss: 0.5433048059086992\n",
      "Epoch 94, Loss: 0.5419539979419274\n",
      "Epoch 95, Loss: 0.5405380123915382\n",
      "Epoch 96, Loss: 0.5390663423263058\n",
      "Epoch 97, Loss: 0.5375557449177183\n",
      "Epoch 98, Loss: 0.5360447823495331\n",
      "Epoch 99, Loss: 0.5345641989144069\n",
      "Epoch 100, Loss: 0.5331462228895412\n",
      "Epoch 101, Loss: 0.5317420083608043\n",
      "Epoch 102, Loss: 0.5302773154101125\n",
      "Epoch 103, Loss: 0.5287778498482397\n",
      "Epoch 104, Loss: 0.5273572284247571\n",
      "Epoch 105, Loss: 0.5260858296907607\n",
      "Epoch 106, Loss: 0.5249305327187342\n",
      "Epoch 107, Loss: 0.5238287303446003\n",
      "Epoch 108, Loss: 0.5227438187619126\n",
      "Epoch 109, Loss: 0.5216694478437166\n",
      "Epoch 110, Loss: 0.5206360506897616\n",
      "Epoch 111, Loss: 0.5196487179230432\n",
      "Epoch 112, Loss: 0.5187028576215635\n",
      "Epoch 113, Loss: 0.5177784370172358\n",
      "Epoch 114, Loss: 0.5168751963623117\n",
      "Epoch 115, Loss: 0.5160162048567605\n",
      "Epoch 116, Loss: 0.5152230574584603\n",
      "Epoch 117, Loss: 0.5144674428049537\n",
      "Epoch 118, Loss: 0.5137208690412804\n",
      "Epoch 119, Loss: 0.5129949656773026\n",
      "Epoch 120, Loss: 0.5123219611120455\n",
      "Epoch 121, Loss: 0.5116998508836337\n",
      "Epoch 122, Loss: 0.5111129902664598\n",
      "Epoch 123, Loss: 0.5105405741436312\n",
      "Epoch 124, Loss: 0.5099849010083686\n",
      "Epoch 125, Loss: 0.5094623759868415\n",
      "Epoch 126, Loss: 0.5089652841754122\n",
      "Epoch 127, Loss: 0.5084717799132713\n",
      "Epoch 128, Loss: 0.5079809643002384\n",
      "Epoch 129, Loss: 0.507507731608986\n",
      "Epoch 130, Loss: 0.5070515727009828\n",
      "Epoch 131, Loss: 0.5065962678643066\n",
      "Epoch 132, Loss: 0.5061448812256618\n",
      "Epoch 133, Loss: 0.5057069703510908\n",
      "Epoch 134, Loss: 0.5052756693952567\n",
      "Epoch 135, Loss: 0.5048445017462813\n",
      "Epoch 136, Loss: 0.504425284231979\n",
      "Epoch 137, Loss: 0.504022717877823\n",
      "Epoch 138, Loss: 0.5036312802042249\n",
      "Epoch 139, Loss: 0.5032368731068207\n",
      "Epoch 140, Loss: 0.5028511060044271\n",
      "Epoch 141, Loss: 0.5024804933282331\n",
      "Epoch 142, Loss: 0.5021182727396313\n",
      "Epoch 143, Loss: 0.5017581664990297\n",
      "Epoch 144, Loss: 0.5014050827892457\n",
      "Epoch 145, Loss: 0.501047675902349\n",
      "Epoch 146, Loss: 0.500690966247362\n",
      "Epoch 147, Loss: 0.5003476722262875\n",
      "Epoch 148, Loss: 0.5000068544871652\n",
      "Epoch 149, Loss: 0.49966338867073357\n",
      "Epoch 150, Loss: 0.4993280873087794\n",
      "Epoch 151, Loss: 0.49900257940428383\n",
      "Epoch 152, Loss: 0.4986865554458311\n",
      "Epoch 153, Loss: 0.49836949389755036\n",
      "Epoch 154, Loss: 0.49805375636713634\n",
      "Epoch 155, Loss: 0.49773873165141763\n",
      "Epoch 156, Loss: 0.49742224924400874\n",
      "Epoch 157, Loss: 0.49711425826812494\n",
      "Epoch 158, Loss: 0.4968010973402242\n",
      "Epoch 159, Loss: 0.4964901399315246\n",
      "Epoch 160, Loss: 0.49617675026847124\n",
      "Epoch 161, Loss: 0.4958670390985117\n",
      "Epoch 162, Loss: 0.4955578109301908\n",
      "Epoch 163, Loss: 0.4952510223984696\n",
      "Epoch 164, Loss: 0.49494577484111674\n",
      "Epoch 165, Loss: 0.49464511374066544\n",
      "Epoch 166, Loss: 0.49434383269975535\n",
      "Epoch 167, Loss: 0.49404631909698943\n",
      "Epoch 168, Loss: 0.49374530645143583\n",
      "Epoch 169, Loss: 0.49345064806438055\n",
      "Epoch 170, Loss: 0.49317001490258333\n",
      "Epoch 171, Loss: 0.4928927591216884\n",
      "Epoch 172, Loss: 0.4926138499628376\n",
      "Epoch 173, Loss: 0.49233528219511824\n",
      "Epoch 174, Loss: 0.49205852514110515\n",
      "Epoch 175, Loss: 0.4917773399684819\n",
      "Epoch 176, Loss: 0.49149629710928233\n",
      "Epoch 177, Loss: 0.4912165647150265\n",
      "Epoch 178, Loss: 0.49094181765497674\n",
      "Epoch 179, Loss: 0.49066901222163783\n",
      "Epoch 180, Loss: 0.490393928005003\n",
      "Epoch 181, Loss: 0.4901079113527018\n",
      "Epoch 182, Loss: 0.48981984154671765\n",
      "Epoch 183, Loss: 0.4895234443341763\n",
      "Epoch 184, Loss: 0.48921459258368105\n",
      "Epoch 185, Loss: 0.4889145977883947\n",
      "Epoch 186, Loss: 0.4886175142174919\n",
      "Epoch 187, Loss: 0.4883282295514753\n",
      "Epoch 188, Loss: 0.4880435212302353\n",
      "Epoch 189, Loss: 0.48775699700499114\n",
      "Epoch 190, Loss: 0.48748008613994775\n",
      "Epoch 191, Loss: 0.4872132179171775\n",
      "Epoch 192, Loss: 0.48694738158534456\n",
      "Epoch 193, Loss: 0.48668121634032785\n",
      "Epoch 194, Loss: 0.4864280322301768\n",
      "Epoch 195, Loss: 0.486178532155915\n",
      "Epoch 196, Loss: 0.4859301919172944\n",
      "Epoch 197, Loss: 0.48568442460989747\n",
      "Epoch 198, Loss: 0.48544224711238787\n",
      "Epoch 199, Loss: 0.48519729219516017\n",
      "Epoch 200, Loss: 0.4849575446659644\n",
      "Epoch 201, Loss: 0.4847146197220469\n",
      "Epoch 202, Loss: 0.48447155759133576\n",
      "Epoch 203, Loss: 0.48422783492589483\n",
      "Epoch 204, Loss: 0.48397707745066637\n",
      "Epoch 205, Loss: 0.483737926529631\n",
      "Epoch 206, Loss: 0.4835020782648923\n",
      "Epoch 207, Loss: 0.48327083252052233\n",
      "Epoch 208, Loss: 0.4830508282762034\n",
      "Epoch 209, Loss: 0.48283269895503794\n",
      "Epoch 210, Loss: 0.48261215534620283\n",
      "Epoch 211, Loss: 0.48240746542225255\n",
      "Epoch 212, Loss: 0.4822187420754367\n",
      "Epoch 213, Loss: 0.48202997575341067\n",
      "Epoch 214, Loss: 0.4818441197783379\n",
      "Epoch 215, Loss: 0.4816502256898402\n",
      "Epoch 216, Loss: 0.4814589616492725\n",
      "Epoch 217, Loss: 0.48127338055379854\n",
      "Epoch 218, Loss: 0.4810898650639108\n",
      "Epoch 219, Loss: 0.48091365405597575\n",
      "Epoch 220, Loss: 0.480720996546175\n",
      "Epoch 221, Loss: 0.48054865040929456\n",
      "Epoch 222, Loss: 0.4803774677186426\n",
      "Epoch 223, Loss: 0.4801885671816001\n",
      "Epoch 224, Loss: 0.48000935557804647\n",
      "Epoch 225, Loss: 0.4798350303004208\n",
      "Epoch 226, Loss: 0.47967842665052907\n",
      "Epoch 227, Loss: 0.47947854451894606\n",
      "Epoch 228, Loss: 0.47928199900429136\n",
      "Epoch 229, Loss: 0.4791424752085709\n",
      "Epoch 230, Loss: 0.4789254904984497\n",
      "Epoch 231, Loss: 0.4787155823499749\n",
      "Epoch 232, Loss: 0.4785186611779873\n",
      "Epoch 233, Loss: 0.4783287773458093\n",
      "Epoch 234, Loss: 0.4781774454711345\n",
      "Epoch 235, Loss: 0.4780410437330356\n",
      "Epoch 236, Loss: 0.4778263811299359\n",
      "Epoch 237, Loss: 0.47765285017795805\n",
      "Epoch 238, Loss: 0.4775436338740249\n",
      "Epoch 239, Loss: 0.4774342833479412\n",
      "Epoch 240, Loss: 0.477263591547843\n",
      "Epoch 241, Loss: 0.47707859610269837\n",
      "Epoch 242, Loss: 0.47697972966255553\n",
      "Epoch 243, Loss: 0.4768431884384324\n",
      "Epoch 244, Loss: 0.4767408349977067\n",
      "Epoch 245, Loss: 0.4765819952057135\n",
      "Epoch 246, Loss: 0.4764706413665339\n",
      "Epoch 247, Loss: 0.47634462160009333\n",
      "Epoch 248, Loss: 0.4762527991635621\n",
      "Epoch 249, Loss: 0.4762014052862575\n",
      "Epoch 250, Loss: 0.4760283306209857\n",
      "Epoch 251, Loss: 0.47592260832103706\n",
      "Epoch 252, Loss: 0.475834869470249\n",
      "Epoch 253, Loss: 0.47570980894919\n",
      "Epoch 254, Loss: 0.47559397414543253\n",
      "Epoch 255, Loss: 0.4755005182088922\n",
      "Epoch 256, Loss: 0.4753894909333468\n",
      "Epoch 257, Loss: 0.47527958208571863\n",
      "Epoch 258, Loss: 0.47517947896910767\n",
      "Epoch 259, Loss: 0.4750690667233416\n",
      "Epoch 260, Loss: 0.47496625253354696\n",
      "Epoch 261, Loss: 0.47486506352442986\n",
      "Epoch 262, Loss: 0.47477116593942054\n",
      "Epoch 263, Loss: 0.474752914637718\n",
      "Epoch 264, Loss: 0.47457458619766507\n",
      "Epoch 265, Loss: 0.47441594770111134\n",
      "Epoch 266, Loss: 0.4743326895520222\n",
      "Epoch 267, Loss: 0.47424442089802893\n",
      "Epoch 268, Loss: 0.47409961152836255\n",
      "Epoch 269, Loss: 0.47397674971911014\n",
      "Epoch 270, Loss: 0.4739006464998255\n",
      "Epoch 271, Loss: 0.4738391701966867\n",
      "Epoch 272, Loss: 0.4737234441817917\n",
      "Epoch 273, Loss: 0.47358564948211956\n",
      "Epoch 274, Loss: 0.47351060057458727\n",
      "Epoch 275, Loss: 0.4734065036258047\n",
      "Epoch 276, Loss: 0.4733269559263822\n",
      "Epoch 277, Loss: 0.4732341071370639\n",
      "Epoch 278, Loss: 0.4731407519682655\n",
      "Epoch 279, Loss: 0.4730939966069229\n",
      "Epoch 280, Loss: 0.47298024635218033\n",
      "Epoch 281, Loss: 0.47280886054292864\n",
      "Epoch 282, Loss: 0.47271222185116163\n",
      "Epoch 283, Loss: 0.47268730250728586\n",
      "Epoch 284, Loss: 0.4725869656059248\n",
      "Epoch 285, Loss: 0.4724867493169173\n",
      "Epoch 286, Loss: 0.4723455337808729\n",
      "Epoch 287, Loss: 0.4722632042859861\n",
      "Epoch 288, Loss: 0.4722048755884816\n",
      "Epoch 289, Loss: 0.472146413443235\n",
      "Epoch 290, Loss: 0.4720312036831835\n",
      "Epoch 291, Loss: 0.47193135878647013\n",
      "Epoch 292, Loss: 0.4718300873811956\n",
      "Epoch 293, Loss: 0.4717440511887379\n",
      "Epoch 294, Loss: 0.47169887906475694\n",
      "Epoch 295, Loss: 0.4716194856818237\n",
      "Epoch 296, Loss: 0.4715081133867502\n",
      "Epoch 297, Loss: 0.47138116207763575\n",
      "Epoch 298, Loss: 0.4712942218774766\n",
      "Epoch 299, Loss: 0.4711807328828787\n",
      "Epoch 300, Loss: 0.4710748083504045\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.27790729427898564\n",
      "Valence RMSE: 0.2558190285938765\n",
      "Arousal RMSE: 0.29836479858759957\n",
      "Test R^2 score: tensor([0.2069, 0.1185], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1125, 0.0137], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.16271452729304892\n",
      "Completed.\n"
     ]
    }
   ],
   "source": [
    "for num_epochs in num_epochs_list:\n",
    "  # Set the seed\n",
    "  torch.manual_seed(seed)\n",
    "\n",
    "  print(f'Num of epochs: {num_epochs}')\n",
    "  \n",
    "  model = train_model(num_epochs)\n",
    "\n",
    "  print(\"Training completed.\")\n",
    "  print(\"Testing model...\")\n",
    "\n",
    "  test_pred, rmse, adjusted_r2_score, r2_score = test_model(model)\n",
    "  adjusted_r2_scores_valence_list.append(adjusted_r2_score[0])\n",
    "  adjusted_r2_scores_arousal_list.append(adjusted_r2_score[1])\n",
    "  r2_scores_list.append(r2_score)\n",
    "  rmse_list.append(rmse)\n",
    "\n",
    "print(\"Completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot graphs to visualise relationship between the evaluation metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import relevant libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RMSE vs. num_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(num_epochs_list, rmse_list, color='b', linestyle='-')\n",
    "plt.title('num_epochs vs. Test RMSE')\n",
    "plt.xlabel('num_epochs')\n",
    "plt.ylabel('Tets RMSE') \n",
    "plt.grid(True)\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min RMSE score: 0.2612383610913183\n",
      "Corresponding R^2 SCore: 0.2571213472447835\n",
      "Corresponding num_epochs: 164\n"
     ]
    }
   ],
   "source": [
    "min_rmse = min(rmse_list)\n",
    "corresponding_r2_score = r2_scores_list[rmse_list.index(min_rmse)]\n",
    "corresponding_num_epochs = num_epochs_list[rmse_list.index(min_rmse)]\n",
    "\n",
    "print(f'Min RMSE score: {min_rmse}')\n",
    "print(f'Corresponding R^2 SCore: {corresponding_r2_score}')\n",
    "print(f'Corresponding num_epochs: {corresponding_num_epochs}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test R^2 Score vs. num_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(num_epochs_list, r2_scores_list, color='b', linestyle='-')\n",
    "plt.title('num_epochs vs. Test R^2 Score')\n",
    "plt.xlabel('num_epochs')\n",
    "plt.ylabel('Test R^2 SCore') \n",
    "plt.grid(True)\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max R^2 score: 0.2571213472447835\n",
      "Corresponding RMSE: 0.2612383610913183\n",
      "Corresponding num_epochs: 164\n"
     ]
    }
   ],
   "source": [
    "max_r2_score = max(r2_scores_list)\n",
    "corresponding_rmse = rmse_list[r2_scores_list.index(max_r2_score)]\n",
    "corresponding_num_epochs = num_epochs_list[r2_scores_list.index(max_r2_score)]\n",
    "\n",
    "print(f'Max R^2 score: {max_r2_score}')\n",
    "print(f'Corresponding RMSE: {corresponding_rmse}')\n",
    "print(f'Corresponding num_epochs: {corresponding_num_epochs}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Adjusted R^2 Score (Valence) vs. num_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(num_epochs_list, adjusted_r2_scores_valence_list, color='b', linestyle='-')\n",
    "plt.title('num_epochs vs. Test Adjusted R^2 Score (Valence)')\n",
    "plt.xlabel('num_epochs')\n",
    "plt.ylabel('Test Adjusted R^2 Score (Valence)') \n",
    "plt.grid(True)\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max R^2 score: 0.17883829347754054\n",
      "Corresponding RMSE: 0.2612383610913183\n",
      "Corresponding num_epochs: 164\n"
     ]
    }
   ],
   "source": [
    "max_r2_score_valence = max(adjusted_r2_scores_valence_list)\n",
    "corresponding_rmse = rmse_list[adjusted_r2_scores_valence_list.index(max_r2_score_valence)]\n",
    "corresponding_num_epochs = num_epochs_list[adjusted_r2_scores_valence_list.index(max_r2_score_valence)]\n",
    "\n",
    "print(f'Max R^2 score: {max_r2_score_valence}')\n",
    "print(f'Corresponding RMSE: {corresponding_rmse}')\n",
    "print(f'Corresponding num_epochs: {corresponding_num_epochs}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Adjusted R^2 Score (Arousal) vs. num_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(num_epochs_list, adjusted_r2_scores_arousal_list, color='b', linestyle='-')\n",
    "plt.title('num_epochs vs. Test Adjusted R^2 Score (Arousal)')\n",
    "plt.xlabel('num_epochs')\n",
    "plt.ylabel('Test Adjusted R^2 SCore (Arousal)') \n",
    "plt.grid(True)\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max R^2 score: 0.1586422778484058\n",
      "Corresponding RMSE: 0.2612383610913183\n",
      "Corresponding num_epochs: 164\n"
     ]
    }
   ],
   "source": [
    "max_r2_score_arousal = max(adjusted_r2_scores_arousal_list)\n",
    "corresponding_rmse = rmse_list[adjusted_r2_scores_arousal_list.index(max_r2_score_arousal)]\n",
    "corresponding_num_epochs = num_epochs_list[adjusted_r2_scores_arousal_list.index(max_r2_score_arousal)]\n",
    "\n",
    "print(f'Max R^2 score: {max_r2_score_arousal}')\n",
    "print(f'Corresponding RMSE: {corresponding_rmse}')\n",
    "print(f'Corresponding num_epochs: {corresponding_num_epochs}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
