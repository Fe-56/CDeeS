{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEAM Dataset - Feed Forward Neural Network\n",
    "## Essentia Best Valence Mean Featureset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import relevant libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torcheval.metrics import R2Score\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import math\n",
    "\n",
    "import sys\n",
    "sys.path.insert(1, '../../utils')\n",
    "from paths import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import annotations dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>song_id</th>\n",
       "      <th>valence_mean_mapped</th>\n",
       "      <th>arousal_mean_mapped</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>-0.475</td>\n",
       "      <td>-0.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>-0.375</td>\n",
       "      <td>-0.425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>0.175</td>\n",
       "      <td>0.125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>-0.150</td>\n",
       "      <td>0.075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1739</th>\n",
       "      <td>1996</td>\n",
       "      <td>-0.275</td>\n",
       "      <td>0.225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1740</th>\n",
       "      <td>1997</td>\n",
       "      <td>0.075</td>\n",
       "      <td>-0.275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1741</th>\n",
       "      <td>1998</td>\n",
       "      <td>0.350</td>\n",
       "      <td>0.300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1742</th>\n",
       "      <td>1999</td>\n",
       "      <td>-0.100</td>\n",
       "      <td>0.100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1743</th>\n",
       "      <td>2000</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.250</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1744 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      song_id  valence_mean_mapped  arousal_mean_mapped\n",
       "0           2               -0.475               -0.500\n",
       "1           3               -0.375               -0.425\n",
       "2           4                0.175                0.125\n",
       "3           5               -0.150                0.075\n",
       "4           7                0.200                0.350\n",
       "...       ...                  ...                  ...\n",
       "1739     1996               -0.275                0.225\n",
       "1740     1997                0.075               -0.275\n",
       "1741     1998                0.350                0.300\n",
       "1742     1999               -0.100                0.100\n",
       "1743     2000                0.200                0.250\n",
       "\n",
       "[1744 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_annotations = pd.read_csv(get_deam_path('processed/annotations/deam_static_annotations.csv'))\n",
    "df_annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the featureset\n",
    "\n",
    "This is where you should change between normalised and standardised, and untouched featuresets!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>song_id</th>\n",
       "      <th>lowlevel.melbands_kurtosis.mean</th>\n",
       "      <th>lowlevel.melbands_skewness.mean</th>\n",
       "      <th>lowlevel.spectral_energy.mean</th>\n",
       "      <th>lowlevel.zerocrossingrate.mean</th>\n",
       "      <th>rhythm.beats_loudness.mean</th>\n",
       "      <th>rhythm.onset_rate</th>\n",
       "      <th>tonal.chords_strength.mean</th>\n",
       "      <th>tonal.hpcp_entropy.mean</th>\n",
       "      <th>tonal.key_edma.strength</th>\n",
       "      <th>...</th>\n",
       "      <th>tonal.chords_histogram_14</th>\n",
       "      <th>tonal.chords_histogram_15</th>\n",
       "      <th>tonal.chords_histogram_16</th>\n",
       "      <th>tonal.chords_histogram_17</th>\n",
       "      <th>tonal.chords_histogram_18</th>\n",
       "      <th>tonal.chords_histogram_19</th>\n",
       "      <th>tonal.chords_histogram_20</th>\n",
       "      <th>tonal.chords_histogram_21</th>\n",
       "      <th>tonal.chords_histogram_22</th>\n",
       "      <th>tonal.chords_histogram_23</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>0.023745</td>\n",
       "      <td>0.224209</td>\n",
       "      <td>0.054855</td>\n",
       "      <td>0.087567</td>\n",
       "      <td>0.069568</td>\n",
       "      <td>0.373016</td>\n",
       "      <td>0.392382</td>\n",
       "      <td>0.631265</td>\n",
       "      <td>0.625832</td>\n",
       "      <td>...</td>\n",
       "      <td>0.023256</td>\n",
       "      <td>0.010375</td>\n",
       "      <td>0.064544</td>\n",
       "      <td>0.026229</td>\n",
       "      <td>0.046262</td>\n",
       "      <td>0.005850</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.083526</td>\n",
       "      <td>0.035887</td>\n",
       "      <td>0.012672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0.082083</td>\n",
       "      <td>0.348482</td>\n",
       "      <td>0.211289</td>\n",
       "      <td>0.019437</td>\n",
       "      <td>0.338074</td>\n",
       "      <td>0.357421</td>\n",
       "      <td>0.402792</td>\n",
       "      <td>0.543954</td>\n",
       "      <td>0.677312</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.038179</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.023489</td>\n",
       "      <td>0.035942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>0.018062</td>\n",
       "      <td>0.213582</td>\n",
       "      <td>0.084731</td>\n",
       "      <td>0.090074</td>\n",
       "      <td>0.135489</td>\n",
       "      <td>0.513626</td>\n",
       "      <td>0.661722</td>\n",
       "      <td>0.554446</td>\n",
       "      <td>0.593904</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001484</td>\n",
       "      <td>0.002937</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.049618</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>0.016018</td>\n",
       "      <td>0.181322</td>\n",
       "      <td>0.041096</td>\n",
       "      <td>0.086854</td>\n",
       "      <td>0.063757</td>\n",
       "      <td>0.378601</td>\n",
       "      <td>0.651524</td>\n",
       "      <td>0.395875</td>\n",
       "      <td>0.900862</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.049780</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.005017</td>\n",
       "      <td>0.127117</td>\n",
       "      <td>0.022199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>0.107675</td>\n",
       "      <td>0.349349</td>\n",
       "      <td>0.197274</td>\n",
       "      <td>0.017629</td>\n",
       "      <td>0.260381</td>\n",
       "      <td>0.637566</td>\n",
       "      <td>0.181959</td>\n",
       "      <td>0.602087</td>\n",
       "      <td>0.450784</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.068694</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.162351</td>\n",
       "      <td>0.18304</td>\n",
       "      <td>0.125289</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1739</th>\n",
       "      <td>1996</td>\n",
       "      <td>0.013781</td>\n",
       "      <td>0.186248</td>\n",
       "      <td>0.077709</td>\n",
       "      <td>0.077239</td>\n",
       "      <td>0.093014</td>\n",
       "      <td>0.402743</td>\n",
       "      <td>0.476088</td>\n",
       "      <td>0.626271</td>\n",
       "      <td>0.772039</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009682</td>\n",
       "      <td>0.021164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1740</th>\n",
       "      <td>1997</td>\n",
       "      <td>0.012825</td>\n",
       "      <td>0.189153</td>\n",
       "      <td>0.073405</td>\n",
       "      <td>0.089305</td>\n",
       "      <td>0.078782</td>\n",
       "      <td>0.288810</td>\n",
       "      <td>0.471430</td>\n",
       "      <td>0.576407</td>\n",
       "      <td>0.551288</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021985</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.140613</td>\n",
       "      <td>0.222683</td>\n",
       "      <td>0.275132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1741</th>\n",
       "      <td>1998</td>\n",
       "      <td>0.016787</td>\n",
       "      <td>0.199106</td>\n",
       "      <td>0.055710</td>\n",
       "      <td>0.170043</td>\n",
       "      <td>0.097281</td>\n",
       "      <td>0.548471</td>\n",
       "      <td>0.467379</td>\n",
       "      <td>0.654543</td>\n",
       "      <td>0.722595</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017823</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.426784</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.353219</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.038728</td>\n",
       "      <td>0.001058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1742</th>\n",
       "      <td>1999</td>\n",
       "      <td>0.022245</td>\n",
       "      <td>0.224935</td>\n",
       "      <td>0.054033</td>\n",
       "      <td>0.087392</td>\n",
       "      <td>0.090900</td>\n",
       "      <td>0.508727</td>\n",
       "      <td>0.255286</td>\n",
       "      <td>0.625434</td>\n",
       "      <td>0.669501</td>\n",
       "      <td>...</td>\n",
       "      <td>0.068617</td>\n",
       "      <td>0.016337</td>\n",
       "      <td>0.336617</td>\n",
       "      <td>0.005006</td>\n",
       "      <td>0.185430</td>\n",
       "      <td>0.054229</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.468710</td>\n",
       "      <td>0.100968</td>\n",
       "      <td>0.176720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1743</th>\n",
       "      <td>2000</td>\n",
       "      <td>0.026978</td>\n",
       "      <td>0.227259</td>\n",
       "      <td>0.061269</td>\n",
       "      <td>0.136189</td>\n",
       "      <td>0.103416</td>\n",
       "      <td>0.633258</td>\n",
       "      <td>0.375683</td>\n",
       "      <td>0.733147</td>\n",
       "      <td>0.651635</td>\n",
       "      <td>...</td>\n",
       "      <td>0.028482</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.005022</td>\n",
       "      <td>0.002766</td>\n",
       "      <td>0.003175</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1744 rows Ã— 67 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      song_id  lowlevel.melbands_kurtosis.mean  \\\n",
       "0           2                         0.023745   \n",
       "1           3                         0.082083   \n",
       "2           4                         0.018062   \n",
       "3           5                         0.016018   \n",
       "4           7                         0.107675   \n",
       "...       ...                              ...   \n",
       "1739     1996                         0.013781   \n",
       "1740     1997                         0.012825   \n",
       "1741     1998                         0.016787   \n",
       "1742     1999                         0.022245   \n",
       "1743     2000                         0.026978   \n",
       "\n",
       "      lowlevel.melbands_skewness.mean  lowlevel.spectral_energy.mean  \\\n",
       "0                            0.224209                       0.054855   \n",
       "1                            0.348482                       0.211289   \n",
       "2                            0.213582                       0.084731   \n",
       "3                            0.181322                       0.041096   \n",
       "4                            0.349349                       0.197274   \n",
       "...                               ...                            ...   \n",
       "1739                         0.186248                       0.077709   \n",
       "1740                         0.189153                       0.073405   \n",
       "1741                         0.199106                       0.055710   \n",
       "1742                         0.224935                       0.054033   \n",
       "1743                         0.227259                       0.061269   \n",
       "\n",
       "      lowlevel.zerocrossingrate.mean  rhythm.beats_loudness.mean  \\\n",
       "0                           0.087567                    0.069568   \n",
       "1                           0.019437                    0.338074   \n",
       "2                           0.090074                    0.135489   \n",
       "3                           0.086854                    0.063757   \n",
       "4                           0.017629                    0.260381   \n",
       "...                              ...                         ...   \n",
       "1739                        0.077239                    0.093014   \n",
       "1740                        0.089305                    0.078782   \n",
       "1741                        0.170043                    0.097281   \n",
       "1742                        0.087392                    0.090900   \n",
       "1743                        0.136189                    0.103416   \n",
       "\n",
       "      rhythm.onset_rate  tonal.chords_strength.mean  tonal.hpcp_entropy.mean  \\\n",
       "0              0.373016                    0.392382                 0.631265   \n",
       "1              0.357421                    0.402792                 0.543954   \n",
       "2              0.513626                    0.661722                 0.554446   \n",
       "3              0.378601                    0.651524                 0.395875   \n",
       "4              0.637566                    0.181959                 0.602087   \n",
       "...                 ...                         ...                      ...   \n",
       "1739           0.402743                    0.476088                 0.626271   \n",
       "1740           0.288810                    0.471430                 0.576407   \n",
       "1741           0.548471                    0.467379                 0.654543   \n",
       "1742           0.508727                    0.255286                 0.625434   \n",
       "1743           0.633258                    0.375683                 0.733147   \n",
       "\n",
       "      tonal.key_edma.strength  ...  tonal.chords_histogram_14  \\\n",
       "0                    0.625832  ...                   0.023256   \n",
       "1                    0.677312  ...                   0.000000   \n",
       "2                    0.593904  ...                   0.000000   \n",
       "3                    0.900862  ...                   0.000000   \n",
       "4                    0.450784  ...                   0.000000   \n",
       "...                       ...  ...                        ...   \n",
       "1739                 0.772039  ...                   0.000000   \n",
       "1740                 0.551288  ...                   0.000000   \n",
       "1741                 0.722595  ...                   0.000000   \n",
       "1742                 0.669501  ...                   0.068617   \n",
       "1743                 0.651635  ...                   0.028482   \n",
       "\n",
       "      tonal.chords_histogram_15  tonal.chords_histogram_16  \\\n",
       "0                      0.010375                   0.064544   \n",
       "1                      0.000000                   0.038179   \n",
       "2                      0.001484                   0.002937   \n",
       "3                      0.000000                   0.000000   \n",
       "4                      0.000000                   0.000000   \n",
       "...                         ...                        ...   \n",
       "1739                   0.000000                   0.000000   \n",
       "1740                   0.000000                   0.000000   \n",
       "1741                   0.017823                   0.000000   \n",
       "1742                   0.016337                   0.336617   \n",
       "1743                   0.000000                   0.000000   \n",
       "\n",
       "      tonal.chords_histogram_17  tonal.chords_histogram_18  \\\n",
       "0                      0.026229                   0.046262   \n",
       "1                      0.000000                   0.000000   \n",
       "2                      0.000000                   0.049618   \n",
       "3                      0.000000                   0.000000   \n",
       "4                      0.068694                   0.000000   \n",
       "...                         ...                        ...   \n",
       "1739                   0.000000                   0.000000   \n",
       "1740                   0.000000                   0.000000   \n",
       "1741                   0.426784                   0.000000   \n",
       "1742                   0.005006                   0.185430   \n",
       "1743                   0.000000                   0.000000   \n",
       "\n",
       "      tonal.chords_histogram_19  tonal.chords_histogram_20  \\\n",
       "0                      0.005850                    0.00000   \n",
       "1                      0.000000                    0.00000   \n",
       "2                      0.000000                    0.00000   \n",
       "3                      0.049780                    0.00000   \n",
       "4                      0.162351                    0.18304   \n",
       "...                         ...                        ...   \n",
       "1739                   0.000000                    0.00000   \n",
       "1740                   0.021985                    0.00000   \n",
       "1741                   0.353219                    0.00000   \n",
       "1742                   0.054229                    0.00000   \n",
       "1743                   0.000000                    0.00000   \n",
       "\n",
       "      tonal.chords_histogram_21  tonal.chords_histogram_22  \\\n",
       "0                      0.083526                   0.035887   \n",
       "1                      0.000000                   0.023489   \n",
       "2                      0.000000                   0.000000   \n",
       "3                      0.005017                   0.127117   \n",
       "4                      0.125289                   0.000000   \n",
       "...                         ...                        ...   \n",
       "1739                   0.000000                   0.009682   \n",
       "1740                   0.140613                   0.222683   \n",
       "1741                   0.000000                   0.038728   \n",
       "1742                   0.468710                   0.100968   \n",
       "1743                   0.005022                   0.002766   \n",
       "\n",
       "      tonal.chords_histogram_23  \n",
       "0                      0.012672  \n",
       "1                      0.035942  \n",
       "2                      0.001057  \n",
       "3                      0.022199  \n",
       "4                      0.015840  \n",
       "...                         ...  \n",
       "1739                   0.021164  \n",
       "1740                   0.275132  \n",
       "1741                   0.001058  \n",
       "1742                   0.176720  \n",
       "1743                   0.003175  \n",
       "\n",
       "[1744 rows x 67 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_essentia_best_valence_features_mean = pd.read_csv(get_deam_path('processed/features/normalised_essentia_best_valence_features.csv'))\n",
    "\n",
    "# drop Unnamed:0 column\n",
    "df_essentia_best_valence_features_mean = df_essentia_best_valence_features_mean[df_essentia_best_valence_features_mean.columns[1:]]\n",
    "\n",
    "df_essentia_best_valence_features_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1744 entries, 0 to 1743\n",
      "Data columns (total 67 columns):\n",
      " #   Column                                   Non-Null Count  Dtype  \n",
      "---  ------                                   --------------  -----  \n",
      " 0   song_id                                  1744 non-null   int64  \n",
      " 1   lowlevel.melbands_kurtosis.mean          1744 non-null   float64\n",
      " 2   lowlevel.melbands_skewness.mean          1744 non-null   float64\n",
      " 3   lowlevel.spectral_energy.mean            1744 non-null   float64\n",
      " 4   lowlevel.zerocrossingrate.mean           1744 non-null   float64\n",
      " 5   rhythm.beats_loudness.mean               1744 non-null   float64\n",
      " 6   rhythm.onset_rate                        1744 non-null   float64\n",
      " 7   tonal.chords_strength.mean               1744 non-null   float64\n",
      " 8   tonal.hpcp_entropy.mean                  1744 non-null   float64\n",
      " 9   tonal.key_edma.strength                  1744 non-null   float64\n",
      " 10  tonal.key_temperley.strength             1744 non-null   float64\n",
      " 11  lowlevel.gfcc.mean_0                     1744 non-null   float64\n",
      " 12  lowlevel.gfcc.mean_1                     1744 non-null   float64\n",
      " 13  lowlevel.gfcc.mean_2                     1744 non-null   float64\n",
      " 14  lowlevel.gfcc.mean_3                     1744 non-null   float64\n",
      " 15  lowlevel.gfcc.mean_4                     1744 non-null   float64\n",
      " 16  lowlevel.gfcc.mean_5                     1744 non-null   float64\n",
      " 17  lowlevel.gfcc.mean_6                     1744 non-null   float64\n",
      " 18  lowlevel.gfcc.mean_7                     1744 non-null   float64\n",
      " 19  lowlevel.gfcc.mean_8                     1744 non-null   float64\n",
      " 20  lowlevel.gfcc.mean_9                     1744 non-null   float64\n",
      " 21  lowlevel.gfcc.mean_10                    1744 non-null   float64\n",
      " 22  lowlevel.gfcc.mean_11                    1744 non-null   float64\n",
      " 23  lowlevel.gfcc.mean_12                    1744 non-null   float64\n",
      " 24  lowlevel.mfcc.mean_0                     1744 non-null   float64\n",
      " 25  lowlevel.mfcc.mean_1                     1744 non-null   float64\n",
      " 26  lowlevel.mfcc.mean_2                     1744 non-null   float64\n",
      " 27  lowlevel.mfcc.mean_3                     1744 non-null   float64\n",
      " 28  lowlevel.mfcc.mean_4                     1744 non-null   float64\n",
      " 29  lowlevel.mfcc.mean_5                     1744 non-null   float64\n",
      " 30  lowlevel.mfcc.mean_6                     1744 non-null   float64\n",
      " 31  lowlevel.mfcc.mean_7                     1744 non-null   float64\n",
      " 32  lowlevel.mfcc.mean_8                     1744 non-null   float64\n",
      " 33  lowlevel.mfcc.mean_9                     1744 non-null   float64\n",
      " 34  lowlevel.mfcc.mean_10                    1744 non-null   float64\n",
      " 35  lowlevel.mfcc.mean_11                    1744 non-null   float64\n",
      " 36  lowlevel.mfcc.mean_12                    1744 non-null   float64\n",
      " 37  rhythm.beats_loudness_band_ratio.mean_0  1744 non-null   float64\n",
      " 38  rhythm.beats_loudness_band_ratio.mean_1  1744 non-null   float64\n",
      " 39  rhythm.beats_loudness_band_ratio.mean_2  1744 non-null   float64\n",
      " 40  rhythm.beats_loudness_band_ratio.mean_3  1744 non-null   float64\n",
      " 41  rhythm.beats_loudness_band_ratio.mean_4  1744 non-null   float64\n",
      " 42  rhythm.beats_loudness_band_ratio.mean_5  1744 non-null   float64\n",
      " 43  tonal.chords_histogram_0                 1744 non-null   float64\n",
      " 44  tonal.chords_histogram_1                 1744 non-null   float64\n",
      " 45  tonal.chords_histogram_2                 1744 non-null   float64\n",
      " 46  tonal.chords_histogram_3                 1744 non-null   float64\n",
      " 47  tonal.chords_histogram_4                 1744 non-null   float64\n",
      " 48  tonal.chords_histogram_5                 1744 non-null   float64\n",
      " 49  tonal.chords_histogram_6                 1744 non-null   float64\n",
      " 50  tonal.chords_histogram_7                 1744 non-null   float64\n",
      " 51  tonal.chords_histogram_8                 1744 non-null   float64\n",
      " 52  tonal.chords_histogram_9                 1744 non-null   float64\n",
      " 53  tonal.chords_histogram_10                1744 non-null   float64\n",
      " 54  tonal.chords_histogram_11                1744 non-null   float64\n",
      " 55  tonal.chords_histogram_12                1744 non-null   float64\n",
      " 56  tonal.chords_histogram_13                1744 non-null   float64\n",
      " 57  tonal.chords_histogram_14                1744 non-null   float64\n",
      " 58  tonal.chords_histogram_15                1744 non-null   float64\n",
      " 59  tonal.chords_histogram_16                1744 non-null   float64\n",
      " 60  tonal.chords_histogram_17                1744 non-null   float64\n",
      " 61  tonal.chords_histogram_18                1744 non-null   float64\n",
      " 62  tonal.chords_histogram_19                1744 non-null   float64\n",
      " 63  tonal.chords_histogram_20                1744 non-null   float64\n",
      " 64  tonal.chords_histogram_21                1744 non-null   float64\n",
      " 65  tonal.chords_histogram_22                1744 non-null   float64\n",
      " 66  tonal.chords_histogram_23                1744 non-null   float64\n",
      "dtypes: float64(66), int64(1)\n",
      "memory usage: 913.0 KB\n"
     ]
    }
   ],
   "source": [
    "df_essentia_best_valence_features_mean.info(verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join both the featureset and annotation set together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lowlevel.melbands_kurtosis.mean</th>\n",
       "      <th>lowlevel.melbands_skewness.mean</th>\n",
       "      <th>lowlevel.spectral_energy.mean</th>\n",
       "      <th>lowlevel.zerocrossingrate.mean</th>\n",
       "      <th>rhythm.beats_loudness.mean</th>\n",
       "      <th>rhythm.onset_rate</th>\n",
       "      <th>tonal.chords_strength.mean</th>\n",
       "      <th>tonal.hpcp_entropy.mean</th>\n",
       "      <th>tonal.key_edma.strength</th>\n",
       "      <th>tonal.key_temperley.strength</th>\n",
       "      <th>...</th>\n",
       "      <th>tonal.chords_histogram_16</th>\n",
       "      <th>tonal.chords_histogram_17</th>\n",
       "      <th>tonal.chords_histogram_18</th>\n",
       "      <th>tonal.chords_histogram_19</th>\n",
       "      <th>tonal.chords_histogram_20</th>\n",
       "      <th>tonal.chords_histogram_21</th>\n",
       "      <th>tonal.chords_histogram_22</th>\n",
       "      <th>tonal.chords_histogram_23</th>\n",
       "      <th>valence_mean_mapped</th>\n",
       "      <th>arousal_mean_mapped</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.023745</td>\n",
       "      <td>0.224209</td>\n",
       "      <td>0.054855</td>\n",
       "      <td>0.087567</td>\n",
       "      <td>0.069568</td>\n",
       "      <td>0.373016</td>\n",
       "      <td>0.392382</td>\n",
       "      <td>0.631265</td>\n",
       "      <td>0.625832</td>\n",
       "      <td>0.593381</td>\n",
       "      <td>...</td>\n",
       "      <td>0.064544</td>\n",
       "      <td>0.026229</td>\n",
       "      <td>0.046262</td>\n",
       "      <td>0.005850</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.083526</td>\n",
       "      <td>0.035887</td>\n",
       "      <td>0.012672</td>\n",
       "      <td>-0.475</td>\n",
       "      <td>-0.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.082083</td>\n",
       "      <td>0.348482</td>\n",
       "      <td>0.211289</td>\n",
       "      <td>0.019437</td>\n",
       "      <td>0.338074</td>\n",
       "      <td>0.357421</td>\n",
       "      <td>0.402792</td>\n",
       "      <td>0.543954</td>\n",
       "      <td>0.677312</td>\n",
       "      <td>0.677951</td>\n",
       "      <td>...</td>\n",
       "      <td>0.038179</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.023489</td>\n",
       "      <td>0.035942</td>\n",
       "      <td>-0.375</td>\n",
       "      <td>-0.425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.018062</td>\n",
       "      <td>0.213582</td>\n",
       "      <td>0.084731</td>\n",
       "      <td>0.090074</td>\n",
       "      <td>0.135489</td>\n",
       "      <td>0.513626</td>\n",
       "      <td>0.661722</td>\n",
       "      <td>0.554446</td>\n",
       "      <td>0.593904</td>\n",
       "      <td>0.570916</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002937</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.049618</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001057</td>\n",
       "      <td>0.175</td>\n",
       "      <td>0.125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.016018</td>\n",
       "      <td>0.181322</td>\n",
       "      <td>0.041096</td>\n",
       "      <td>0.086854</td>\n",
       "      <td>0.063757</td>\n",
       "      <td>0.378601</td>\n",
       "      <td>0.651524</td>\n",
       "      <td>0.395875</td>\n",
       "      <td>0.900862</td>\n",
       "      <td>0.900318</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.049780</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.005017</td>\n",
       "      <td>0.127117</td>\n",
       "      <td>0.022199</td>\n",
       "      <td>-0.150</td>\n",
       "      <td>0.075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.107675</td>\n",
       "      <td>0.349349</td>\n",
       "      <td>0.197274</td>\n",
       "      <td>0.017629</td>\n",
       "      <td>0.260381</td>\n",
       "      <td>0.637566</td>\n",
       "      <td>0.181959</td>\n",
       "      <td>0.602087</td>\n",
       "      <td>0.450784</td>\n",
       "      <td>0.432106</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.068694</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.162351</td>\n",
       "      <td>0.18304</td>\n",
       "      <td>0.125289</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015840</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1739</th>\n",
       "      <td>0.013781</td>\n",
       "      <td>0.186248</td>\n",
       "      <td>0.077709</td>\n",
       "      <td>0.077239</td>\n",
       "      <td>0.093014</td>\n",
       "      <td>0.402743</td>\n",
       "      <td>0.476088</td>\n",
       "      <td>0.626271</td>\n",
       "      <td>0.772039</td>\n",
       "      <td>0.770549</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009682</td>\n",
       "      <td>0.021164</td>\n",
       "      <td>-0.275</td>\n",
       "      <td>0.225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1740</th>\n",
       "      <td>0.012825</td>\n",
       "      <td>0.189153</td>\n",
       "      <td>0.073405</td>\n",
       "      <td>0.089305</td>\n",
       "      <td>0.078782</td>\n",
       "      <td>0.288810</td>\n",
       "      <td>0.471430</td>\n",
       "      <td>0.576407</td>\n",
       "      <td>0.551288</td>\n",
       "      <td>0.577018</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021985</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.140613</td>\n",
       "      <td>0.222683</td>\n",
       "      <td>0.275132</td>\n",
       "      <td>0.075</td>\n",
       "      <td>-0.275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1741</th>\n",
       "      <td>0.016787</td>\n",
       "      <td>0.199106</td>\n",
       "      <td>0.055710</td>\n",
       "      <td>0.170043</td>\n",
       "      <td>0.097281</td>\n",
       "      <td>0.548471</td>\n",
       "      <td>0.467379</td>\n",
       "      <td>0.654543</td>\n",
       "      <td>0.722595</td>\n",
       "      <td>0.743935</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.426784</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.353219</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.038728</td>\n",
       "      <td>0.001058</td>\n",
       "      <td>0.350</td>\n",
       "      <td>0.300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1742</th>\n",
       "      <td>0.022245</td>\n",
       "      <td>0.224935</td>\n",
       "      <td>0.054033</td>\n",
       "      <td>0.087392</td>\n",
       "      <td>0.090900</td>\n",
       "      <td>0.508727</td>\n",
       "      <td>0.255286</td>\n",
       "      <td>0.625434</td>\n",
       "      <td>0.669501</td>\n",
       "      <td>0.701341</td>\n",
       "      <td>...</td>\n",
       "      <td>0.336617</td>\n",
       "      <td>0.005006</td>\n",
       "      <td>0.185430</td>\n",
       "      <td>0.054229</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.468710</td>\n",
       "      <td>0.100968</td>\n",
       "      <td>0.176720</td>\n",
       "      <td>-0.100</td>\n",
       "      <td>0.100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1743</th>\n",
       "      <td>0.026978</td>\n",
       "      <td>0.227259</td>\n",
       "      <td>0.061269</td>\n",
       "      <td>0.136189</td>\n",
       "      <td>0.103416</td>\n",
       "      <td>0.633258</td>\n",
       "      <td>0.375683</td>\n",
       "      <td>0.733147</td>\n",
       "      <td>0.651635</td>\n",
       "      <td>0.590380</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.005022</td>\n",
       "      <td>0.002766</td>\n",
       "      <td>0.003175</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.250</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1744 rows Ã— 68 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      lowlevel.melbands_kurtosis.mean  lowlevel.melbands_skewness.mean  \\\n",
       "0                            0.023745                         0.224209   \n",
       "1                            0.082083                         0.348482   \n",
       "2                            0.018062                         0.213582   \n",
       "3                            0.016018                         0.181322   \n",
       "4                            0.107675                         0.349349   \n",
       "...                               ...                              ...   \n",
       "1739                         0.013781                         0.186248   \n",
       "1740                         0.012825                         0.189153   \n",
       "1741                         0.016787                         0.199106   \n",
       "1742                         0.022245                         0.224935   \n",
       "1743                         0.026978                         0.227259   \n",
       "\n",
       "      lowlevel.spectral_energy.mean  lowlevel.zerocrossingrate.mean  \\\n",
       "0                          0.054855                        0.087567   \n",
       "1                          0.211289                        0.019437   \n",
       "2                          0.084731                        0.090074   \n",
       "3                          0.041096                        0.086854   \n",
       "4                          0.197274                        0.017629   \n",
       "...                             ...                             ...   \n",
       "1739                       0.077709                        0.077239   \n",
       "1740                       0.073405                        0.089305   \n",
       "1741                       0.055710                        0.170043   \n",
       "1742                       0.054033                        0.087392   \n",
       "1743                       0.061269                        0.136189   \n",
       "\n",
       "      rhythm.beats_loudness.mean  rhythm.onset_rate  \\\n",
       "0                       0.069568           0.373016   \n",
       "1                       0.338074           0.357421   \n",
       "2                       0.135489           0.513626   \n",
       "3                       0.063757           0.378601   \n",
       "4                       0.260381           0.637566   \n",
       "...                          ...                ...   \n",
       "1739                    0.093014           0.402743   \n",
       "1740                    0.078782           0.288810   \n",
       "1741                    0.097281           0.548471   \n",
       "1742                    0.090900           0.508727   \n",
       "1743                    0.103416           0.633258   \n",
       "\n",
       "      tonal.chords_strength.mean  tonal.hpcp_entropy.mean  \\\n",
       "0                       0.392382                 0.631265   \n",
       "1                       0.402792                 0.543954   \n",
       "2                       0.661722                 0.554446   \n",
       "3                       0.651524                 0.395875   \n",
       "4                       0.181959                 0.602087   \n",
       "...                          ...                      ...   \n",
       "1739                    0.476088                 0.626271   \n",
       "1740                    0.471430                 0.576407   \n",
       "1741                    0.467379                 0.654543   \n",
       "1742                    0.255286                 0.625434   \n",
       "1743                    0.375683                 0.733147   \n",
       "\n",
       "      tonal.key_edma.strength  tonal.key_temperley.strength  ...  \\\n",
       "0                    0.625832                      0.593381  ...   \n",
       "1                    0.677312                      0.677951  ...   \n",
       "2                    0.593904                      0.570916  ...   \n",
       "3                    0.900862                      0.900318  ...   \n",
       "4                    0.450784                      0.432106  ...   \n",
       "...                       ...                           ...  ...   \n",
       "1739                 0.772039                      0.770549  ...   \n",
       "1740                 0.551288                      0.577018  ...   \n",
       "1741                 0.722595                      0.743935  ...   \n",
       "1742                 0.669501                      0.701341  ...   \n",
       "1743                 0.651635                      0.590380  ...   \n",
       "\n",
       "      tonal.chords_histogram_16  tonal.chords_histogram_17  \\\n",
       "0                      0.064544                   0.026229   \n",
       "1                      0.038179                   0.000000   \n",
       "2                      0.002937                   0.000000   \n",
       "3                      0.000000                   0.000000   \n",
       "4                      0.000000                   0.068694   \n",
       "...                         ...                        ...   \n",
       "1739                   0.000000                   0.000000   \n",
       "1740                   0.000000                   0.000000   \n",
       "1741                   0.000000                   0.426784   \n",
       "1742                   0.336617                   0.005006   \n",
       "1743                   0.000000                   0.000000   \n",
       "\n",
       "      tonal.chords_histogram_18  tonal.chords_histogram_19  \\\n",
       "0                      0.046262                   0.005850   \n",
       "1                      0.000000                   0.000000   \n",
       "2                      0.049618                   0.000000   \n",
       "3                      0.000000                   0.049780   \n",
       "4                      0.000000                   0.162351   \n",
       "...                         ...                        ...   \n",
       "1739                   0.000000                   0.000000   \n",
       "1740                   0.000000                   0.021985   \n",
       "1741                   0.000000                   0.353219   \n",
       "1742                   0.185430                   0.054229   \n",
       "1743                   0.000000                   0.000000   \n",
       "\n",
       "      tonal.chords_histogram_20  tonal.chords_histogram_21  \\\n",
       "0                       0.00000                   0.083526   \n",
       "1                       0.00000                   0.000000   \n",
       "2                       0.00000                   0.000000   \n",
       "3                       0.00000                   0.005017   \n",
       "4                       0.18304                   0.125289   \n",
       "...                         ...                        ...   \n",
       "1739                    0.00000                   0.000000   \n",
       "1740                    0.00000                   0.140613   \n",
       "1741                    0.00000                   0.000000   \n",
       "1742                    0.00000                   0.468710   \n",
       "1743                    0.00000                   0.005022   \n",
       "\n",
       "      tonal.chords_histogram_22  tonal.chords_histogram_23  \\\n",
       "0                      0.035887                   0.012672   \n",
       "1                      0.023489                   0.035942   \n",
       "2                      0.000000                   0.001057   \n",
       "3                      0.127117                   0.022199   \n",
       "4                      0.000000                   0.015840   \n",
       "...                         ...                        ...   \n",
       "1739                   0.009682                   0.021164   \n",
       "1740                   0.222683                   0.275132   \n",
       "1741                   0.038728                   0.001058   \n",
       "1742                   0.100968                   0.176720   \n",
       "1743                   0.002766                   0.003175   \n",
       "\n",
       "      valence_mean_mapped  arousal_mean_mapped  \n",
       "0                  -0.475               -0.500  \n",
       "1                  -0.375               -0.425  \n",
       "2                   0.175                0.125  \n",
       "3                  -0.150                0.075  \n",
       "4                   0.200                0.350  \n",
       "...                   ...                  ...  \n",
       "1739               -0.275                0.225  \n",
       "1740                0.075               -0.275  \n",
       "1741                0.350                0.300  \n",
       "1742               -0.100                0.100  \n",
       "1743                0.200                0.250  \n",
       "\n",
       "[1744 rows x 68 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_essentia_best_valence_features_mean_whole = pd.merge(df_essentia_best_valence_features_mean, df_annotations, how='inner', on='song_id')\n",
    "df_essentia_best_valence_features_mean_whole = df_essentia_best_valence_features_mean_whole.drop('song_id', axis=1)\n",
    "df_essentia_best_valence_features_mean_whole"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare dataframes for the neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform splitting of the dataframe into training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lowlevel.melbands_kurtosis.mean</th>\n",
       "      <th>lowlevel.melbands_skewness.mean</th>\n",
       "      <th>lowlevel.spectral_energy.mean</th>\n",
       "      <th>lowlevel.zerocrossingrate.mean</th>\n",
       "      <th>rhythm.beats_loudness.mean</th>\n",
       "      <th>rhythm.onset_rate</th>\n",
       "      <th>tonal.chords_strength.mean</th>\n",
       "      <th>tonal.hpcp_entropy.mean</th>\n",
       "      <th>tonal.key_edma.strength</th>\n",
       "      <th>tonal.key_temperley.strength</th>\n",
       "      <th>...</th>\n",
       "      <th>tonal.chords_histogram_14</th>\n",
       "      <th>tonal.chords_histogram_15</th>\n",
       "      <th>tonal.chords_histogram_16</th>\n",
       "      <th>tonal.chords_histogram_17</th>\n",
       "      <th>tonal.chords_histogram_18</th>\n",
       "      <th>tonal.chords_histogram_19</th>\n",
       "      <th>tonal.chords_histogram_20</th>\n",
       "      <th>tonal.chords_histogram_21</th>\n",
       "      <th>tonal.chords_histogram_22</th>\n",
       "      <th>tonal.chords_histogram_23</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.023745</td>\n",
       "      <td>0.224209</td>\n",
       "      <td>0.054855</td>\n",
       "      <td>0.087567</td>\n",
       "      <td>0.069568</td>\n",
       "      <td>0.373016</td>\n",
       "      <td>0.392382</td>\n",
       "      <td>0.631265</td>\n",
       "      <td>0.625832</td>\n",
       "      <td>0.593381</td>\n",
       "      <td>...</td>\n",
       "      <td>0.023256</td>\n",
       "      <td>0.010375</td>\n",
       "      <td>0.064544</td>\n",
       "      <td>0.026229</td>\n",
       "      <td>0.046262</td>\n",
       "      <td>0.005850</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.083526</td>\n",
       "      <td>0.035887</td>\n",
       "      <td>0.012672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.082083</td>\n",
       "      <td>0.348482</td>\n",
       "      <td>0.211289</td>\n",
       "      <td>0.019437</td>\n",
       "      <td>0.338074</td>\n",
       "      <td>0.357421</td>\n",
       "      <td>0.402792</td>\n",
       "      <td>0.543954</td>\n",
       "      <td>0.677312</td>\n",
       "      <td>0.677951</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.038179</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.023489</td>\n",
       "      <td>0.035942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.018062</td>\n",
       "      <td>0.213582</td>\n",
       "      <td>0.084731</td>\n",
       "      <td>0.090074</td>\n",
       "      <td>0.135489</td>\n",
       "      <td>0.513626</td>\n",
       "      <td>0.661722</td>\n",
       "      <td>0.554446</td>\n",
       "      <td>0.593904</td>\n",
       "      <td>0.570916</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001484</td>\n",
       "      <td>0.002937</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.049618</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.016018</td>\n",
       "      <td>0.181322</td>\n",
       "      <td>0.041096</td>\n",
       "      <td>0.086854</td>\n",
       "      <td>0.063757</td>\n",
       "      <td>0.378601</td>\n",
       "      <td>0.651524</td>\n",
       "      <td>0.395875</td>\n",
       "      <td>0.900862</td>\n",
       "      <td>0.900318</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.049780</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.005017</td>\n",
       "      <td>0.127117</td>\n",
       "      <td>0.022199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.107675</td>\n",
       "      <td>0.349349</td>\n",
       "      <td>0.197274</td>\n",
       "      <td>0.017629</td>\n",
       "      <td>0.260381</td>\n",
       "      <td>0.637566</td>\n",
       "      <td>0.181959</td>\n",
       "      <td>0.602087</td>\n",
       "      <td>0.450784</td>\n",
       "      <td>0.432106</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.068694</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.162351</td>\n",
       "      <td>0.18304</td>\n",
       "      <td>0.125289</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1739</th>\n",
       "      <td>0.013781</td>\n",
       "      <td>0.186248</td>\n",
       "      <td>0.077709</td>\n",
       "      <td>0.077239</td>\n",
       "      <td>0.093014</td>\n",
       "      <td>0.402743</td>\n",
       "      <td>0.476088</td>\n",
       "      <td>0.626271</td>\n",
       "      <td>0.772039</td>\n",
       "      <td>0.770549</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009682</td>\n",
       "      <td>0.021164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1740</th>\n",
       "      <td>0.012825</td>\n",
       "      <td>0.189153</td>\n",
       "      <td>0.073405</td>\n",
       "      <td>0.089305</td>\n",
       "      <td>0.078782</td>\n",
       "      <td>0.288810</td>\n",
       "      <td>0.471430</td>\n",
       "      <td>0.576407</td>\n",
       "      <td>0.551288</td>\n",
       "      <td>0.577018</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021985</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.140613</td>\n",
       "      <td>0.222683</td>\n",
       "      <td>0.275132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1741</th>\n",
       "      <td>0.016787</td>\n",
       "      <td>0.199106</td>\n",
       "      <td>0.055710</td>\n",
       "      <td>0.170043</td>\n",
       "      <td>0.097281</td>\n",
       "      <td>0.548471</td>\n",
       "      <td>0.467379</td>\n",
       "      <td>0.654543</td>\n",
       "      <td>0.722595</td>\n",
       "      <td>0.743935</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017823</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.426784</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.353219</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.038728</td>\n",
       "      <td>0.001058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1742</th>\n",
       "      <td>0.022245</td>\n",
       "      <td>0.224935</td>\n",
       "      <td>0.054033</td>\n",
       "      <td>0.087392</td>\n",
       "      <td>0.090900</td>\n",
       "      <td>0.508727</td>\n",
       "      <td>0.255286</td>\n",
       "      <td>0.625434</td>\n",
       "      <td>0.669501</td>\n",
       "      <td>0.701341</td>\n",
       "      <td>...</td>\n",
       "      <td>0.068617</td>\n",
       "      <td>0.016337</td>\n",
       "      <td>0.336617</td>\n",
       "      <td>0.005006</td>\n",
       "      <td>0.185430</td>\n",
       "      <td>0.054229</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.468710</td>\n",
       "      <td>0.100968</td>\n",
       "      <td>0.176720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1743</th>\n",
       "      <td>0.026978</td>\n",
       "      <td>0.227259</td>\n",
       "      <td>0.061269</td>\n",
       "      <td>0.136189</td>\n",
       "      <td>0.103416</td>\n",
       "      <td>0.633258</td>\n",
       "      <td>0.375683</td>\n",
       "      <td>0.733147</td>\n",
       "      <td>0.651635</td>\n",
       "      <td>0.590380</td>\n",
       "      <td>...</td>\n",
       "      <td>0.028482</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.005022</td>\n",
       "      <td>0.002766</td>\n",
       "      <td>0.003175</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1744 rows Ã— 66 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      lowlevel.melbands_kurtosis.mean  lowlevel.melbands_skewness.mean  \\\n",
       "0                            0.023745                         0.224209   \n",
       "1                            0.082083                         0.348482   \n",
       "2                            0.018062                         0.213582   \n",
       "3                            0.016018                         0.181322   \n",
       "4                            0.107675                         0.349349   \n",
       "...                               ...                              ...   \n",
       "1739                         0.013781                         0.186248   \n",
       "1740                         0.012825                         0.189153   \n",
       "1741                         0.016787                         0.199106   \n",
       "1742                         0.022245                         0.224935   \n",
       "1743                         0.026978                         0.227259   \n",
       "\n",
       "      lowlevel.spectral_energy.mean  lowlevel.zerocrossingrate.mean  \\\n",
       "0                          0.054855                        0.087567   \n",
       "1                          0.211289                        0.019437   \n",
       "2                          0.084731                        0.090074   \n",
       "3                          0.041096                        0.086854   \n",
       "4                          0.197274                        0.017629   \n",
       "...                             ...                             ...   \n",
       "1739                       0.077709                        0.077239   \n",
       "1740                       0.073405                        0.089305   \n",
       "1741                       0.055710                        0.170043   \n",
       "1742                       0.054033                        0.087392   \n",
       "1743                       0.061269                        0.136189   \n",
       "\n",
       "      rhythm.beats_loudness.mean  rhythm.onset_rate  \\\n",
       "0                       0.069568           0.373016   \n",
       "1                       0.338074           0.357421   \n",
       "2                       0.135489           0.513626   \n",
       "3                       0.063757           0.378601   \n",
       "4                       0.260381           0.637566   \n",
       "...                          ...                ...   \n",
       "1739                    0.093014           0.402743   \n",
       "1740                    0.078782           0.288810   \n",
       "1741                    0.097281           0.548471   \n",
       "1742                    0.090900           0.508727   \n",
       "1743                    0.103416           0.633258   \n",
       "\n",
       "      tonal.chords_strength.mean  tonal.hpcp_entropy.mean  \\\n",
       "0                       0.392382                 0.631265   \n",
       "1                       0.402792                 0.543954   \n",
       "2                       0.661722                 0.554446   \n",
       "3                       0.651524                 0.395875   \n",
       "4                       0.181959                 0.602087   \n",
       "...                          ...                      ...   \n",
       "1739                    0.476088                 0.626271   \n",
       "1740                    0.471430                 0.576407   \n",
       "1741                    0.467379                 0.654543   \n",
       "1742                    0.255286                 0.625434   \n",
       "1743                    0.375683                 0.733147   \n",
       "\n",
       "      tonal.key_edma.strength  tonal.key_temperley.strength  ...  \\\n",
       "0                    0.625832                      0.593381  ...   \n",
       "1                    0.677312                      0.677951  ...   \n",
       "2                    0.593904                      0.570916  ...   \n",
       "3                    0.900862                      0.900318  ...   \n",
       "4                    0.450784                      0.432106  ...   \n",
       "...                       ...                           ...  ...   \n",
       "1739                 0.772039                      0.770549  ...   \n",
       "1740                 0.551288                      0.577018  ...   \n",
       "1741                 0.722595                      0.743935  ...   \n",
       "1742                 0.669501                      0.701341  ...   \n",
       "1743                 0.651635                      0.590380  ...   \n",
       "\n",
       "      tonal.chords_histogram_14  tonal.chords_histogram_15  \\\n",
       "0                      0.023256                   0.010375   \n",
       "1                      0.000000                   0.000000   \n",
       "2                      0.000000                   0.001484   \n",
       "3                      0.000000                   0.000000   \n",
       "4                      0.000000                   0.000000   \n",
       "...                         ...                        ...   \n",
       "1739                   0.000000                   0.000000   \n",
       "1740                   0.000000                   0.000000   \n",
       "1741                   0.000000                   0.017823   \n",
       "1742                   0.068617                   0.016337   \n",
       "1743                   0.028482                   0.000000   \n",
       "\n",
       "      tonal.chords_histogram_16  tonal.chords_histogram_17  \\\n",
       "0                      0.064544                   0.026229   \n",
       "1                      0.038179                   0.000000   \n",
       "2                      0.002937                   0.000000   \n",
       "3                      0.000000                   0.000000   \n",
       "4                      0.000000                   0.068694   \n",
       "...                         ...                        ...   \n",
       "1739                   0.000000                   0.000000   \n",
       "1740                   0.000000                   0.000000   \n",
       "1741                   0.000000                   0.426784   \n",
       "1742                   0.336617                   0.005006   \n",
       "1743                   0.000000                   0.000000   \n",
       "\n",
       "      tonal.chords_histogram_18  tonal.chords_histogram_19  \\\n",
       "0                      0.046262                   0.005850   \n",
       "1                      0.000000                   0.000000   \n",
       "2                      0.049618                   0.000000   \n",
       "3                      0.000000                   0.049780   \n",
       "4                      0.000000                   0.162351   \n",
       "...                         ...                        ...   \n",
       "1739                   0.000000                   0.000000   \n",
       "1740                   0.000000                   0.021985   \n",
       "1741                   0.000000                   0.353219   \n",
       "1742                   0.185430                   0.054229   \n",
       "1743                   0.000000                   0.000000   \n",
       "\n",
       "      tonal.chords_histogram_20  tonal.chords_histogram_21  \\\n",
       "0                       0.00000                   0.083526   \n",
       "1                       0.00000                   0.000000   \n",
       "2                       0.00000                   0.000000   \n",
       "3                       0.00000                   0.005017   \n",
       "4                       0.18304                   0.125289   \n",
       "...                         ...                        ...   \n",
       "1739                    0.00000                   0.000000   \n",
       "1740                    0.00000                   0.140613   \n",
       "1741                    0.00000                   0.000000   \n",
       "1742                    0.00000                   0.468710   \n",
       "1743                    0.00000                   0.005022   \n",
       "\n",
       "      tonal.chords_histogram_22  tonal.chords_histogram_23  \n",
       "0                      0.035887                   0.012672  \n",
       "1                      0.023489                   0.035942  \n",
       "2                      0.000000                   0.001057  \n",
       "3                      0.127117                   0.022199  \n",
       "4                      0.000000                   0.015840  \n",
       "...                         ...                        ...  \n",
       "1739                   0.009682                   0.021164  \n",
       "1740                   0.222683                   0.275132  \n",
       "1741                   0.038728                   0.001058  \n",
       "1742                   0.100968                   0.176720  \n",
       "1743                   0.002766                   0.003175  \n",
       "\n",
       "[1744 rows x 66 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = df_essentia_best_valence_features_mean.drop('song_id', axis=1)\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>valence_mean_mapped</th>\n",
       "      <th>arousal_mean_mapped</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.475</td>\n",
       "      <td>-0.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.375</td>\n",
       "      <td>-0.425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.175</td>\n",
       "      <td>0.125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.150</td>\n",
       "      <td>0.075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.200</td>\n",
       "      <td>0.350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1739</th>\n",
       "      <td>-0.275</td>\n",
       "      <td>0.225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1740</th>\n",
       "      <td>0.075</td>\n",
       "      <td>-0.275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1741</th>\n",
       "      <td>0.350</td>\n",
       "      <td>0.300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1742</th>\n",
       "      <td>-0.100</td>\n",
       "      <td>0.100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1743</th>\n",
       "      <td>0.200</td>\n",
       "      <td>0.250</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1744 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      valence_mean_mapped  arousal_mean_mapped\n",
       "0                  -0.475               -0.500\n",
       "1                  -0.375               -0.425\n",
       "2                   0.175                0.125\n",
       "3                  -0.150                0.075\n",
       "4                   0.200                0.350\n",
       "...                   ...                  ...\n",
       "1739               -0.275                0.225\n",
       "1740                0.075               -0.275\n",
       "1741                0.350                0.300\n",
       "1742               -0.100                0.100\n",
       "1743                0.200                0.250\n",
       "\n",
       "[1744 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets = df_annotations.drop('song_id', axis=1)\n",
    "targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform 80-20 train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(features, targets, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create tensors for X_train and X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tensor = torch.tensor(X_train.values, dtype=torch.float64)\n",
    "X_test_tensor = torch.tensor(X_test.values, dtype=torch.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create tensors for Y_train and Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float64)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define neural network parameters and instantitate neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 1\n",
    "hidden_size = 20 \n",
    "output_size = 2  # Output size for valence and arousal\n",
    "learning_rate = 0.001\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 298"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set a random seed to ensure consistent initial weights of the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1307f99b0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the seed\n",
    "seed = 42\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_size, math.ceil((input_size**0.5) * 2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(math.ceil((input_size**0.5) * 2), math.ceil((input_size**0.5) * 2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(math.ceil((input_size**0.5) * 2), math.ceil((input_size**0.5) * 2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(math.ceil((input_size**0.5) * 2), math.ceil((input_size**0.5) * 2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(math.ceil((input_size**0.5) * 2), math.ceil((input_size**0.5) * 2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(math.ceil((input_size**0.5) * 2), math.ceil((input_size**0.5) * 2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(math.ceil((input_size**0.5) * 2), math.ceil((input_size**0.5) * 2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(math.ceil((input_size**0.5) * 2), math.ceil((input_size**0.5) * 2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(math.ceil((input_size**0.5) * 2), math.ceil((input_size**0.5) * 2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(math.ceil((input_size**0.5) * 2), math.ceil((input_size**0.5) * 2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(math.ceil((input_size**0.5) * 2), math.ceil((input_size**0.5) * 2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(math.ceil((input_size**0.5) * 2), 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare input_train_data and target_train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1395, 66])\n"
     ]
    }
   ],
   "source": [
    "input_train_data = X_train_tensor.float()\n",
    "\n",
    "# input_train_data = input_train_data.view(input_train_data.shape[1], -1)\n",
    "print(input_train_data.shape)\n",
    "\n",
    "target_train_labels = y_train_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(num_epochs):\n",
    "  model = NeuralNetwork(input_size=input_train_data.shape[1])\n",
    "  optimiser = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "  \n",
    "  for epoch in range(num_epochs):\n",
    "    optimiser.zero_grad()\n",
    "    \n",
    "    # forward pass\n",
    "    output = model(input_train_data)\n",
    "\n",
    "    # calculate loss\n",
    "    loss = torch.sqrt(criterion(output.float(), target_train_labels.float()))\n",
    "\n",
    "    # backward pass\n",
    "    loss.backward()\n",
    "    # update weights\n",
    "    optimiser.step()\n",
    "\n",
    "    print(f'Epoch {epoch + 1}, Loss: {math.sqrt(loss.item())}')\n",
    "\n",
    "  print(\"Training completed.\")\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Epoch 121, Loss: 0.49295912913079615\n",
      "Epoch 122, Loss: 0.4924773619139838\n",
      "Epoch 123, Loss: 0.49197607191949805\n",
      "Epoch 124, Loss: 0.4914699197831528\n",
      "Epoch 125, Loss: 0.49091483381312667\n",
      "Epoch 126, Loss: 0.49035147677946317\n",
      "Epoch 127, Loss: 0.4898087222825037\n",
      "Epoch 128, Loss: 0.48924014731550836\n",
      "Epoch 129, Loss: 0.48873550647212044\n",
      "Epoch 130, Loss: 0.48832307254971563\n",
      "Epoch 131, Loss: 0.48791418392821295\n",
      "Epoch 132, Loss: 0.4875902245243796\n",
      "Epoch 133, Loss: 0.48728851108037585\n",
      "Epoch 134, Loss: 0.48697062262181345\n",
      "Epoch 135, Loss: 0.4866339553030548\n",
      "Epoch 136, Loss: 0.48633446746734893\n",
      "Epoch 137, Loss: 0.4860310087358514\n",
      "Epoch 138, Loss: 0.4856786412545615\n",
      "Epoch 139, Loss: 0.48531372107004\n",
      "Epoch 140, Loss: 0.48495712985488587\n",
      "Epoch 141, Loss: 0.4846131600652309\n",
      "Epoch 142, Loss: 0.48425966858652364\n",
      "Epoch 143, Loss: 0.4838914149236435\n",
      "Epoch 144, Loss: 0.48352437546282767\n",
      "Epoch 145, Loss: 0.4832299911937133\n",
      "Epoch 146, Loss: 0.4832586529465723\n",
      "Epoch 147, Loss: 0.48356744143113517\n",
      "Epoch 148, Loss: 0.4828108019011922\n",
      "Epoch 149, Loss: 0.4818912939738211\n",
      "Epoch 150, Loss: 0.48239416745575797\n",
      "Epoch 151, Loss: 0.4814952491721961\n",
      "Epoch 152, Loss: 0.4811822818820502\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 153, Loss: 0.4812753156716337\n",
      "Epoch 154, Loss: 0.4804387742282827\n",
      "Epoch 155, Loss: 0.48058189052417905\n",
      "Epoch 156, Loss: 0.4801288115497346\n",
      "Epoch 157, Loss: 0.4797465937246525\n",
      "Epoch 158, Loss: 0.4797622479529733\n",
      "Epoch 159, Loss: 0.4791664679844762\n",
      "Epoch 160, Loss: 0.47904359923301915\n",
      "Epoch 161, Loss: 0.4788483533353555\n",
      "Epoch 162, Loss: 0.4783709093142749\n",
      "Epoch 163, Loss: 0.4783027642591491\n",
      "Epoch 164, Loss: 0.4780003946284736\n",
      "Epoch 165, Loss: 0.47766356610156796\n",
      "Epoch 166, Loss: 0.47756940749162413\n",
      "Epoch 167, Loss: 0.4772720526560437\n",
      "Epoch 168, Loss: 0.477001191512605\n",
      "Epoch 169, Loss: 0.47690651155802954\n",
      "Epoch 170, Loss: 0.47663605231839845\n",
      "Epoch 171, Loss: 0.4763715079887003\n",
      "Epoch 172, Loss: 0.4762479807342815\n",
      "Epoch 173, Loss: 0.47603155482909787\n",
      "Epoch 174, Loss: 0.47580085926956445\n",
      "Epoch 175, Loss: 0.4756396378860013\n",
      "Epoch 176, Loss: 0.4754627232666358\n",
      "Epoch 177, Loss: 0.4752513954316692\n",
      "Epoch 178, Loss: 0.47507140350730925\n",
      "Epoch 179, Loss: 0.47492471273973497\n",
      "Epoch 180, Loss: 0.47475493910732763\n",
      "Epoch 181, Loss: 0.47455157018218247\n",
      "Epoch 182, Loss: 0.4743853224635176\n",
      "Epoch 183, Loss: 0.47424550491599726\n",
      "Epoch 184, Loss: 0.47408413178207975\n",
      "Epoch 185, Loss: 0.47390385374022453\n",
      "Epoch 186, Loss: 0.4737383066259127\n",
      "Epoch 187, Loss: 0.47358797785338436\n",
      "Epoch 188, Loss: 0.4734560449826861\n",
      "Epoch 189, Loss: 0.47335738206045025\n",
      "Epoch 190, Loss: 0.4733162834925985\n",
      "Epoch 191, Loss: 0.4732413020737725\n",
      "Epoch 192, Loss: 0.47313239020014447\n",
      "Epoch 193, Loss: 0.4729457158004841\n",
      "Epoch 194, Loss: 0.47274703738102797\n",
      "Epoch 195, Loss: 0.47253750657270627\n",
      "Epoch 196, Loss: 0.4723544299968521\n",
      "Epoch 197, Loss: 0.47221102907746554\n",
      "Epoch 198, Loss: 0.47210844467503205\n",
      "Epoch 199, Loss: 0.47205337980155637\n",
      "Epoch 200, Loss: 0.47204973383515075\n",
      "Epoch 201, Loss: 0.4720427890593303\n",
      "Epoch 202, Loss: 0.4719159657967507\n",
      "Epoch 203, Loss: 0.471735017109987\n",
      "Epoch 204, Loss: 0.4714985533244086\n",
      "Epoch 205, Loss: 0.4713143302002759\n",
      "Epoch 206, Loss: 0.47118574544243713\n",
      "Epoch 207, Loss: 0.47112002449718005\n",
      "Epoch 208, Loss: 0.4710910670518786\n",
      "Epoch 209, Loss: 0.47102495326923743\n",
      "Epoch 210, Loss: 0.4709626586360175\n",
      "Epoch 211, Loss: 0.47085840979895394\n",
      "Epoch 212, Loss: 0.47078064719388485\n",
      "Epoch 213, Loss: 0.47066614789276784\n",
      "Epoch 214, Loss: 0.47055654499036753\n",
      "Epoch 215, Loss: 0.47041652390832933\n",
      "Epoch 216, Loss: 0.4702446473560784\n",
      "Epoch 217, Loss: 0.4701045016774931\n",
      "Epoch 218, Loss: 0.47002059077245106\n",
      "Epoch 219, Loss: 0.4699643617672827\n",
      "Epoch 220, Loss: 0.4698888296176042\n",
      "Epoch 221, Loss: 0.4698141575467262\n",
      "Epoch 222, Loss: 0.4697265783615268\n",
      "Epoch 223, Loss: 0.46966066909468496\n",
      "Epoch 224, Loss: 0.4696348739248034\n",
      "Epoch 225, Loss: 0.4697176006517021\n",
      "Epoch 226, Loss: 0.4699424675720642\n",
      "Epoch 227, Loss: 0.46999269112017344\n",
      "Epoch 228, Loss: 0.4698759384724347\n",
      "Epoch 229, Loss: 0.46938041574678274\n",
      "Epoch 230, Loss: 0.4691415581864629\n",
      "Epoch 231, Loss: 0.46924839544758656\n",
      "Epoch 232, Loss: 0.46933531775018905\n",
      "Epoch 233, Loss: 0.4692253881106229\n",
      "Epoch 234, Loss: 0.46894320638835874\n",
      "Epoch 235, Loss: 0.4688628060980674\n",
      "Epoch 236, Loss: 0.46897798398181567\n",
      "Epoch 237, Loss: 0.4689811295666005\n",
      "Epoch 238, Loss: 0.4688031484354944\n",
      "Epoch 239, Loss: 0.46862983752908405\n",
      "Epoch 240, Loss: 0.4685963696810109\n",
      "Epoch 241, Loss: 0.46867106090894\n",
      "Epoch 242, Loss: 0.46866360503981047\n",
      "Epoch 243, Loss: 0.468513635806399\n",
      "Epoch 244, Loss: 0.46835730315983837\n",
      "Epoch 245, Loss: 0.468344910742699\n",
      "Epoch 246, Loss: 0.46842993299443175\n",
      "Epoch 247, Loss: 0.4683803372309361\n",
      "Epoch 248, Loss: 0.46823408027364943\n",
      "Epoch 249, Loss: 0.46809559282800506\n",
      "Epoch 250, Loss: 0.4681194355765096\n",
      "Epoch 251, Loss: 0.4682059468617626\n",
      "Epoch 252, Loss: 0.46815125054377077\n",
      "Epoch 253, Loss: 0.46800465135626707\n",
      "Epoch 254, Loss: 0.4678516840148294\n",
      "Epoch 255, Loss: 0.46781997608436393\n",
      "Epoch 256, Loss: 0.4678506648079015\n",
      "Epoch 257, Loss: 0.4677995105079106\n",
      "Epoch 258, Loss: 0.4676942061377158\n",
      "Epoch 259, Loss: 0.46760336187164103\n",
      "Epoch 260, Loss: 0.46754277859764004\n",
      "Epoch 261, Loss: 0.4675358146830681\n",
      "Epoch 262, Loss: 0.4675934670336611\n",
      "Epoch 263, Loss: 0.4675883044263754\n",
      "Epoch 264, Loss: 0.46749525618568777\n",
      "Epoch 265, Loss: 0.46735425416294335\n",
      "Epoch 266, Loss: 0.4672686694514329\n",
      "Epoch 267, Loss: 0.46723722493080344\n",
      "Epoch 268, Loss: 0.4672375438513713\n",
      "Epoch 269, Loss: 0.46722038561562945\n",
      "Epoch 270, Loss: 0.46721867932539873\n",
      "Epoch 271, Loss: 0.46717319721795353\n",
      "Epoch 272, Loss: 0.4671238189214812\n",
      "Epoch 273, Loss: 0.46705709570299314\n",
      "Epoch 274, Loss: 0.4669974785875546\n",
      "Epoch 275, Loss: 0.4669422896788895\n",
      "Epoch 276, Loss: 0.4669034987743203\n",
      "Epoch 277, Loss: 0.46684804340658115\n",
      "Epoch 278, Loss: 0.4668104417178138\n",
      "Epoch 279, Loss: 0.46677111311176783\n",
      "Epoch 280, Loss: 0.46673031256560493\n",
      "Epoch 281, Loss: 0.46669448942736436\n",
      "Epoch 282, Loss: 0.46666225583115023\n",
      "Epoch 283, Loss: 0.4666367100427352\n",
      "Epoch 284, Loss: 0.4666213020629852\n",
      "Epoch 285, Loss: 0.46662540558501797\n",
      "Epoch 286, Loss: 0.4666499301628323\n",
      "Epoch 287, Loss: 0.466693164365506\n",
      "Epoch 288, Loss: 0.46673154174214454\n",
      "Epoch 289, Loss: 0.4666228668343984\n",
      "Epoch 290, Loss: 0.46649923372154634\n",
      "Epoch 291, Loss: 0.46640938673130766\n",
      "Epoch 292, Loss: 0.4663389345853611\n",
      "Epoch 293, Loss: 0.4663027458423396\n",
      "Epoch 294, Loss: 0.46628067971608783\n",
      "Epoch 295, Loss: 0.46627999262949893\n",
      "Epoch 296, Loss: 0.46630041304996855\n",
      "Epoch 297, Loss: 0.46631715776658106\n",
      "Epoch 298, Loss: 0.4663076031259929\n",
      "Training completed.\n"
     ]
    }
   ],
   "source": [
    "model = train_model(num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare input_test_data and target_test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([349, 66])\n"
     ]
    }
   ],
   "source": [
    "input_test_data = X_test_tensor.float()\n",
    "\n",
    "# input_test_data = input_test_data.view(input_test_data.shape[1], -1)\n",
    "print(input_test_data.shape)\n",
    "\n",
    "target_test_labels = y_test_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(trained_model):\n",
    "  with torch.no_grad():\n",
    "    test_pred = trained_model(input_test_data)\n",
    "    test_loss = criterion(test_pred.float(), target_test_labels)\n",
    "\n",
    "    # Separate the output into valence and arousal\n",
    "    valence_pred = test_pred[:, 0]\n",
    "    arousal_pred = test_pred[:, 1]\n",
    "        \n",
    "    valence_target = target_test_labels[:, 0]\n",
    "    arousal_target = target_test_labels[:, 1]\n",
    "\n",
    "     # Calculate RMSE for valence and arousal separately\n",
    "    valence_rmse = math.sqrt(mean_squared_error(valence_pred, valence_target))\n",
    "    arousal_rmse = math.sqrt(mean_squared_error(arousal_pred, arousal_target))\n",
    "\n",
    "  rmse = math.sqrt(test_loss.item())\n",
    "  print(f'Test RMSE: {rmse}')\n",
    "\n",
    "  print(f'Valence RMSE: {valence_rmse}')\n",
    "  print(f'Arousal RMSE: {arousal_rmse}')\n",
    "\n",
    "  metric = R2Score(multioutput=\"raw_values\")\n",
    "  metric.update(test_pred, target_test_labels)\n",
    "  adjusted_r2_score = metric.compute()\n",
    "  print(f'Test R^2 score: {adjusted_r2_score}')\n",
    "\n",
    "  metric = R2Score(multioutput=\"raw_values\", num_regressors=input_test_data.shape[1])\n",
    "  metric.update(test_pred, target_test_labels)\n",
    "  adjusted_r2_score = metric.compute()\n",
    "  print(f'Test Adjusted R^2 score: {adjusted_r2_score}')\n",
    "\n",
    "  metric = R2Score()\n",
    "  metric.update(test_pred, target_test_labels)\n",
    "  r2_score = metric.compute()\n",
    "  print(f'Test R^2 score (overall): {r2_score}')\n",
    "  return test_pred, rmse, adjusted_r2_score, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE: 0.2170219585144529\n",
      "Valence RMSE: 0.20481310343588993\n",
      "Arousal RMSE: 0.22857964392276336\n",
      "Test R^2 score: tensor([0.4916, 0.4827], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.3726, 0.3616], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4871389825139597\n"
     ]
    }
   ],
   "source": [
    "test_pred, rmse, adjusted_r2_score, r2_score = test_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), '../../models/deam_feedforward_nn_essentia_best_valence_mean_normalised.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "True values (test set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1500, -0.1500],\n",
       "        [-0.3000, -0.1000],\n",
       "        [ 0.2000,  0.3500],\n",
       "        [ 0.2250,  0.4500],\n",
       "        [-0.1750, -0.2000],\n",
       "        [-0.5250, -0.3000],\n",
       "        [-0.2500, -0.7750],\n",
       "        [ 0.3000,  0.3000],\n",
       "        [-0.1750, -0.4000],\n",
       "        [ 0.4500,  0.1500],\n",
       "        [ 0.1750,  0.0250],\n",
       "        [-0.1750, -0.0250],\n",
       "        [-0.0500, -0.3000],\n",
       "        [ 0.1250,  0.3000],\n",
       "        [-0.0750, -0.1500],\n",
       "        [-0.2000, -0.2750],\n",
       "        [-0.6000, -0.2250],\n",
       "        [ 0.1500, -0.2000],\n",
       "        [ 0.2750,  0.6000],\n",
       "        [-0.1500, -0.4500],\n",
       "        [-0.2250, -0.6250],\n",
       "        [-0.0250, -0.4500],\n",
       "        [-0.5250, -0.1250],\n",
       "        [ 0.0000,  0.3250],\n",
       "        [ 0.1250,  0.3750],\n",
       "        [ 0.1500, -0.2500],\n",
       "        [ 0.4500,  0.3250],\n",
       "        [ 0.2500,  0.2250],\n",
       "        [-0.1000,  0.0750],\n",
       "        [ 0.4250,  0.1250],\n",
       "        [-0.4500, -0.3500],\n",
       "        [-0.0500,  0.3750],\n",
       "        [-0.4750, -0.2000],\n",
       "        [-0.2750, -0.4000],\n",
       "        [-0.4000, -0.2250],\n",
       "        [ 0.1000, -0.4500],\n",
       "        [-0.2250, -0.6750],\n",
       "        [ 0.3000,  0.1250],\n",
       "        [-0.2000, -0.2250],\n",
       "        [ 0.2500,  0.3750],\n",
       "        [-0.3250, -0.4750],\n",
       "        [ 0.2250,  0.2000],\n",
       "        [ 0.0500,  0.1250],\n",
       "        [-0.5750, -0.6000],\n",
       "        [-0.1250, -0.3500],\n",
       "        [ 0.5000,  0.6000],\n",
       "        [-0.1500,  0.3250],\n",
       "        [-0.1750,  0.0250],\n",
       "        [-0.2750, -0.3250],\n",
       "        [ 0.2500,  0.3500],\n",
       "        [-0.3250, -0.7500],\n",
       "        [ 0.3000,  0.4000],\n",
       "        [ 0.0250,  0.2000],\n",
       "        [ 0.3750,  0.2250],\n",
       "        [-0.4250, -0.3750],\n",
       "        [-0.4250, -0.2500],\n",
       "        [-0.5250, -0.1750],\n",
       "        [-0.0500, -0.1500],\n",
       "        [ 0.1250, -0.1000],\n",
       "        [-0.3250, -0.5000],\n",
       "        [-0.4000, -0.0750],\n",
       "        [ 0.1500, -0.0500],\n",
       "        [-0.3000, -0.6500],\n",
       "        [-0.7000, -0.3750],\n",
       "        [ 0.5500,  0.2500],\n",
       "        [-0.2000, -0.1500],\n",
       "        [ 0.0750,  0.0750],\n",
       "        [-0.3000, -0.4000],\n",
       "        [-0.4250, -0.3750],\n",
       "        [-0.5750, -0.1000],\n",
       "        [ 0.2750, -0.1250],\n",
       "        [-0.1750, -0.2000],\n",
       "        [-0.2750, -0.6250],\n",
       "        [-0.4750, -0.3750],\n",
       "        [ 0.2750,  0.1250],\n",
       "        [ 0.3250,  0.4250],\n",
       "        [-0.3000, -0.1500],\n",
       "        [ 0.0500,  0.0750],\n",
       "        [ 0.2750, -0.2500],\n",
       "        [-0.3000, -0.6500],\n",
       "        [-0.3000,  0.2250],\n",
       "        [-0.4000, -0.0500],\n",
       "        [-0.0250, -0.3500],\n",
       "        [ 0.0000, -0.0500],\n",
       "        [-0.3000, -0.1000],\n",
       "        [ 0.3500, -0.3750],\n",
       "        [ 0.0250,  0.2000],\n",
       "        [-0.2500, -0.2250],\n",
       "        [ 0.0000, -0.3250],\n",
       "        [ 0.1500,  0.0000],\n",
       "        [-0.3500, -0.4750],\n",
       "        [-0.1750, -0.1250],\n",
       "        [-0.6750, -0.6000],\n",
       "        [ 0.2500,  0.2750],\n",
       "        [-0.3000, -0.5750],\n",
       "        [-0.1750, -0.5250],\n",
       "        [ 0.2750,  0.3250],\n",
       "        [ 0.3250, -0.1000],\n",
       "        [ 0.1000,  0.1750],\n",
       "        [-0.0750,  0.2000],\n",
       "        [ 0.2250, -0.3250],\n",
       "        [ 0.3750,  0.5500],\n",
       "        [-0.5250, -0.2500],\n",
       "        [-0.1000,  0.1500],\n",
       "        [ 0.1250,  0.1000],\n",
       "        [-0.3750, -0.3250],\n",
       "        [-0.4750, -0.3500],\n",
       "        [-0.2750, -0.2750],\n",
       "        [-0.2000, -0.0750],\n",
       "        [ 0.2750,  0.6000],\n",
       "        [-0.0500, -0.2500],\n",
       "        [-0.0500,  0.2500],\n",
       "        [-0.4750, -0.2000],\n",
       "        [-0.0250,  0.2000],\n",
       "        [-0.0750,  0.1500],\n",
       "        [ 0.6000,  0.6500],\n",
       "        [ 0.3250,  0.1500],\n",
       "        [-0.3500,  0.0000],\n",
       "        [ 0.2500,  0.2000],\n",
       "        [-0.1500,  0.3750],\n",
       "        [ 0.2250,  0.1000],\n",
       "        [ 0.2750, -0.4250],\n",
       "        [-0.0750,  0.6250],\n",
       "        [-0.1750, -0.3000],\n",
       "        [-0.0750, -0.6500],\n",
       "        [-0.1250,  0.1000],\n",
       "        [ 0.0000, -0.0250],\n",
       "        [ 0.0500,  0.0500],\n",
       "        [-0.1000,  0.0250],\n",
       "        [ 0.2000, -0.0750],\n",
       "        [-0.2750,  0.2250],\n",
       "        [-0.3750,  0.1250],\n",
       "        [-0.0750,  0.0500],\n",
       "        [ 0.3000,  0.0000],\n",
       "        [ 0.2000,  0.3000],\n",
       "        [ 0.3500,  0.1000],\n",
       "        [ 0.5750,  0.6000],\n",
       "        [-0.2750,  0.1250],\n",
       "        [ 0.0750, -0.3500],\n",
       "        [-0.0750, -0.3750],\n",
       "        [ 0.3750,  0.1000],\n",
       "        [ 0.0500, -0.0750],\n",
       "        [-0.4000, -0.0500],\n",
       "        [-0.1750, -0.2750],\n",
       "        [ 0.2000, -0.1750],\n",
       "        [ 0.2750,  0.1000],\n",
       "        [-0.0500, -0.2750],\n",
       "        [-0.3000, -0.4000],\n",
       "        [-0.1250, -0.0500],\n",
       "        [ 0.0250, -0.3500],\n",
       "        [-0.2000, -0.8500],\n",
       "        [ 0.1500,  0.0250],\n",
       "        [ 0.2250, -0.4000],\n",
       "        [-0.1250, -0.2000],\n",
       "        [ 0.3500,  0.1000],\n",
       "        [-0.2000,  0.0000],\n",
       "        [ 0.0250, -0.3500],\n",
       "        [-0.2250,  0.0000],\n",
       "        [-0.0750,  0.1250],\n",
       "        [-0.1000,  0.2000],\n",
       "        [-0.2500, -0.6000],\n",
       "        [ 0.2250,  0.1000],\n",
       "        [ 0.0250,  0.4250],\n",
       "        [-0.2250, -0.2500],\n",
       "        [ 0.1750,  0.3000],\n",
       "        [-0.1500,  0.0500],\n",
       "        [-0.3500, -0.0500],\n",
       "        [-0.4000,  0.2250],\n",
       "        [-0.1000,  0.1500],\n",
       "        [ 0.0000, -0.4750],\n",
       "        [-0.1500, -0.4500],\n",
       "        [ 0.1500,  0.2250],\n",
       "        [ 0.2250,  0.0750],\n",
       "        [ 0.3500,  0.0750],\n",
       "        [ 0.5250,  0.3750],\n",
       "        [ 0.2500,  0.2000],\n",
       "        [ 0.3500,  0.2000],\n",
       "        [-0.0250, -0.3000],\n",
       "        [-0.4000,  0.0750],\n",
       "        [-0.1500,  0.4750],\n",
       "        [-0.4750, -0.6750],\n",
       "        [-0.0750, -0.1750],\n",
       "        [-0.5250, -0.3750],\n",
       "        [ 0.2750, -0.2750],\n",
       "        [ 0.6000,  0.4000],\n",
       "        [-0.4250, -0.5500],\n",
       "        [-0.1500, -0.5750],\n",
       "        [ 0.2250,  0.4500],\n",
       "        [ 0.6500,  0.7000],\n",
       "        [ 0.1750,  0.3250],\n",
       "        [-0.2750, -0.1250],\n",
       "        [-0.2500, -0.4000],\n",
       "        [-0.0250, -0.1250],\n",
       "        [-0.0250, -0.2500],\n",
       "        [-0.1500, -0.3750],\n",
       "        [ 0.3500,  0.4000],\n",
       "        [ 0.5750, -0.4250],\n",
       "        [-0.0750,  0.0750],\n",
       "        [-0.0500, -0.1250],\n",
       "        [ 0.0750,  0.2000],\n",
       "        [-0.1500, -0.0500],\n",
       "        [-0.1500, -0.3000],\n",
       "        [-0.0500,  0.2500],\n",
       "        [-0.3000, -0.4250],\n",
       "        [-0.3000, -0.3500],\n",
       "        [-0.4000, -0.1000],\n",
       "        [-0.1500, -0.5750],\n",
       "        [-0.0750, -0.3750],\n",
       "        [-0.3500, -0.3500],\n",
       "        [ 0.2250,  0.2250],\n",
       "        [ 0.1250,  0.0000],\n",
       "        [-0.1750, -0.2000],\n",
       "        [-0.0750, -0.3000],\n",
       "        [ 0.5000,  0.4000],\n",
       "        [-0.2000, -0.2250],\n",
       "        [-0.2000, -0.4750],\n",
       "        [ 0.3000,  0.1750],\n",
       "        [ 0.1250,  0.0000],\n",
       "        [ 0.1750,  0.4750],\n",
       "        [ 0.1750, -0.2500],\n",
       "        [-0.1250,  0.4250],\n",
       "        [ 0.2000,  0.4750],\n",
       "        [-0.3000, -0.4000],\n",
       "        [-0.1250, -0.5250],\n",
       "        [-0.5750, -0.0750],\n",
       "        [ 0.1750, -0.0250],\n",
       "        [ 0.4000,  0.3500],\n",
       "        [-0.2500,  0.0000],\n",
       "        [-0.4750, -0.3000],\n",
       "        [ 0.1250,  0.2750],\n",
       "        [ 0.0750,  0.1750],\n",
       "        [ 0.3750,  0.1500],\n",
       "        [-0.1750, -0.2250],\n",
       "        [ 0.1250,  0.2750],\n",
       "        [-0.4500, -0.3250],\n",
       "        [ 0.3000,  0.0750],\n",
       "        [-0.3000,  0.0250],\n",
       "        [-0.3250, -0.5250],\n",
       "        [-0.1250, -0.0250],\n",
       "        [ 0.1250,  0.2000],\n",
       "        [-0.3750,  0.0500],\n",
       "        [-0.3250, -0.0500],\n",
       "        [ 0.0500,  0.3000],\n",
       "        [-0.5500, -0.3250],\n",
       "        [-0.2750, -0.3000],\n",
       "        [-0.2750, -0.5500],\n",
       "        [-0.1750, -0.5750],\n",
       "        [ 0.4500,  0.3000],\n",
       "        [-0.2500,  0.1250],\n",
       "        [-0.1000, -0.3250],\n",
       "        [ 0.1250,  0.2250],\n",
       "        [ 0.4750,  0.2750],\n",
       "        [-0.2250, -0.0250],\n",
       "        [ 0.3750,  0.3500],\n",
       "        [ 0.0000,  0.1750],\n",
       "        [-0.4250, -0.1000],\n",
       "        [ 0.1000, -0.1000],\n",
       "        [ 0.2000,  0.1500],\n",
       "        [ 0.1250,  0.0250],\n",
       "        [-0.2500,  0.1750],\n",
       "        [-0.3250, -0.6500],\n",
       "        [-0.0750, -0.3000],\n",
       "        [ 0.0750, -0.1500],\n",
       "        [ 0.5250,  0.5250],\n",
       "        [-0.0750,  0.2250],\n",
       "        [-0.1750,  0.0000],\n",
       "        [ 0.3000, -0.0500],\n",
       "        [-0.3500, -0.4000],\n",
       "        [-0.2250, -0.2000],\n",
       "        [ 0.4750,  0.5500],\n",
       "        [ 0.1000, -0.4750],\n",
       "        [-0.1250,  0.0000],\n",
       "        [-0.3000,  0.0000],\n",
       "        [-0.2750, -0.4500],\n",
       "        [-0.1500, -0.3000],\n",
       "        [ 0.1500, -0.1250],\n",
       "        [ 0.0500, -0.1250],\n",
       "        [ 0.0750,  0.1750],\n",
       "        [-0.1250, -0.1250],\n",
       "        [ 0.5750,  0.2500],\n",
       "        [-0.3750, -0.0500],\n",
       "        [ 0.2250,  0.1000],\n",
       "        [ 0.3250, -0.2000],\n",
       "        [ 0.4750,  0.4500],\n",
       "        [-0.1750, -0.4000],\n",
       "        [ 0.3500,  0.3750],\n",
       "        [-0.4000,  0.1250],\n",
       "        [-0.1500, -0.2750],\n",
       "        [ 0.5750, -0.3750],\n",
       "        [-0.2500,  0.2000],\n",
       "        [ 0.0000,  0.1500],\n",
       "        [ 0.4500, -0.1250],\n",
       "        [-0.1000, -0.0250],\n",
       "        [ 0.1500,  0.0750],\n",
       "        [ 0.2000, -0.1000],\n",
       "        [ 0.0500,  0.0250],\n",
       "        [ 0.3500,  0.4250],\n",
       "        [-0.3500, -0.5500],\n",
       "        [-0.4250, -0.6000],\n",
       "        [ 0.1750,  0.5500],\n",
       "        [ 0.2000,  0.0250],\n",
       "        [-0.2250, -0.1250],\n",
       "        [ 0.2500,  0.1750],\n",
       "        [-0.3750, -0.0500],\n",
       "        [-0.4750, -0.4500],\n",
       "        [-0.3250, -0.5500],\n",
       "        [-0.1250,  0.1750],\n",
       "        [-0.2500, -0.0500],\n",
       "        [ 0.0000,  0.1250],\n",
       "        [-0.6250, -0.1500],\n",
       "        [-0.4250, -0.5500],\n",
       "        [ 0.0250, -0.2000],\n",
       "        [ 0.3250,  0.3750],\n",
       "        [ 0.1750,  0.1500],\n",
       "        [-0.1750, -0.6500],\n",
       "        [ 0.0750,  0.4250],\n",
       "        [-0.4500, -0.3750],\n",
       "        [-0.1250, -0.1750],\n",
       "        [ 0.0500, -0.3000],\n",
       "        [-0.0500,  0.3750],\n",
       "        [-0.2750,  0.0500],\n",
       "        [-0.4750, -0.3250],\n",
       "        [ 0.0000, -0.3000],\n",
       "        [ 0.3750, -0.1000],\n",
       "        [ 0.1750,  0.1000],\n",
       "        [-0.2250, -0.5500],\n",
       "        [ 0.1500,  0.2250],\n",
       "        [-0.6250, -0.5750],\n",
       "        [ 0.0750, -0.2750],\n",
       "        [ 0.3500,  0.6250],\n",
       "        [-0.0500,  0.2500],\n",
       "        [ 0.0750,  0.3500],\n",
       "        [ 0.2000, -0.3000],\n",
       "        [ 0.2000, -0.1500],\n",
       "        [-0.0250, -0.1500],\n",
       "        [-0.3000, -0.1000],\n",
       "        [-0.1250,  0.1000],\n",
       "        [-0.6750, -0.6250],\n",
       "        [-0.0750, -0.1250],\n",
       "        [ 0.2750,  0.2000],\n",
       "        [ 0.0250,  0.0500],\n",
       "        [-0.4750, -0.1500],\n",
       "        [-0.1500, -0.0500],\n",
       "        [ 0.2000,  0.0250],\n",
       "        [-0.1750,  0.1750],\n",
       "        [ 0.2750,  0.2250],\n",
       "        [ 0.2500,  0.2500],\n",
       "        [-0.1750, -0.2250],\n",
       "        [-0.5750, -0.7000]], dtype=torch.float64)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_test_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicted values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.3062e-01,  1.5398e-01],\n",
       "        [-1.3116e-01, -1.7997e-01],\n",
       "        [ 1.8579e-01,  2.0577e-01],\n",
       "        [ 1.3060e-02, -2.7389e-03],\n",
       "        [-5.7588e-02, -9.2318e-02],\n",
       "        [-2.4046e-01, -2.9545e-01],\n",
       "        [-4.3352e-01, -4.8303e-01],\n",
       "        [ 2.9241e-01,  2.7527e-01],\n",
       "        [-2.6228e-01, -3.2282e-01],\n",
       "        [ 1.1073e-01,  1.2818e-01],\n",
       "        [ 2.2035e-01,  2.3310e-01],\n",
       "        [-1.0118e-01, -1.4447e-01],\n",
       "        [-3.0334e-01, -3.6760e-01],\n",
       "        [ 2.4245e-01,  2.4627e-01],\n",
       "        [ 1.2790e-01,  1.5179e-01],\n",
       "        [-5.8624e-02, -9.3221e-02],\n",
       "        [-4.0606e-01, -4.6032e-01],\n",
       "        [-3.1405e-02, -5.9840e-02],\n",
       "        [ 2.0429e-01,  2.2168e-01],\n",
       "        [-2.4310e-01, -3.0052e-01],\n",
       "        [-2.0354e-01, -2.6260e-01],\n",
       "        [-2.3799e-01, -2.9479e-01],\n",
       "        [-3.7230e-01, -4.3175e-01],\n",
       "        [ 4.2392e-02,  3.0712e-02],\n",
       "        [ 2.1326e-01,  2.2945e-01],\n",
       "        [ 1.1060e-01,  1.2939e-01],\n",
       "        [ 3.4090e-01,  3.0593e-01],\n",
       "        [ 1.2097e-01,  1.4433e-01],\n",
       "        [ 7.3400e-02,  7.1687e-02],\n",
       "        [ 1.3712e-01,  1.5971e-01],\n",
       "        [-3.7002e-01, -4.3001e-01],\n",
       "        [ 1.6653e-02,  2.3147e-03],\n",
       "        [-3.3324e-02, -6.2069e-02],\n",
       "        [ 2.1990e-02,  8.4835e-03],\n",
       "        [-1.1194e-01, -1.5722e-01],\n",
       "        [ 9.0103e-02,  9.8763e-02],\n",
       "        [-4.2989e-01, -4.8003e-01],\n",
       "        [ 2.7429e-01,  2.6467e-01],\n",
       "        [-3.4851e-02, -6.3911e-02],\n",
       "        [ 9.1926e-02,  1.0233e-01],\n",
       "        [-2.5343e-01, -3.1280e-01],\n",
       "        [ 1.6520e-01,  1.8678e-01],\n",
       "        [ 9.0863e-02,  9.8183e-02],\n",
       "        [-1.8738e-01, -2.4802e-01],\n",
       "        [-4.6353e-02, -7.8634e-02],\n",
       "        [ 2.2780e-01,  2.3776e-01],\n",
       "        [ 1.8045e-01,  2.0103e-01],\n",
       "        [ 6.2916e-02,  5.8449e-02],\n",
       "        [-2.1408e-01, -2.7199e-01],\n",
       "        [ 4.8367e-02,  3.6981e-02],\n",
       "        [-2.8413e-01, -3.4838e-01],\n",
       "        [ 2.1836e-01,  2.3218e-01],\n",
       "        [ 1.3914e-01,  1.6206e-01],\n",
       "        [ 2.1993e-01,  2.3311e-01],\n",
       "        [-1.8452e-01, -2.4307e-01],\n",
       "        [ 1.0761e-01,  1.2287e-01],\n",
       "        [-3.8462e-01, -4.4242e-01],\n",
       "        [-2.7387e-02, -5.4819e-02],\n",
       "        [ 6.2977e-02,  5.7795e-02],\n",
       "        [-3.2433e-01, -3.8874e-01],\n",
       "        [-2.0691e-01, -2.6572e-01],\n",
       "        [ 9.9123e-02,  1.1120e-01],\n",
       "        [-4.0911e-01, -4.6257e-01],\n",
       "        [-3.7125e-01, -4.3116e-01],\n",
       "        [ 3.8971e-02,  2.7638e-02],\n",
       "        [ 1.4787e-01,  1.7012e-01],\n",
       "        [ 1.6866e-01,  1.8935e-01],\n",
       "        [-9.0400e-02, -1.3180e-01],\n",
       "        [-8.6628e-02, -1.2721e-01],\n",
       "        [-3.0074e-01, -3.6481e-01],\n",
       "        [ 1.0436e-02, -5.7492e-03],\n",
       "        [-6.5695e-02, -1.0253e-01],\n",
       "        [-3.9347e-01, -4.4959e-01],\n",
       "        [-2.6604e-01, -3.2798e-01],\n",
       "        [ 4.9612e-02,  3.8919e-02],\n",
       "        [ 1.2993e-01,  1.5328e-01],\n",
       "        [-2.3230e-01, -2.8766e-01],\n",
       "        [ 2.6229e-01,  2.5771e-01],\n",
       "        [ 2.7376e-03, -1.5754e-02],\n",
       "        [-3.4094e-01, -4.0566e-01],\n",
       "        [ 6.0216e-02,  5.2815e-02],\n",
       "        [-8.0747e-02, -1.2028e-01],\n",
       "        [ 5.6610e-02,  4.9271e-02],\n",
       "        [ 4.1796e-02,  3.0175e-02],\n",
       "        [-1.6550e-01, -2.2045e-01],\n",
       "        [ 2.2023e-02,  9.4106e-03],\n",
       "        [ 1.1053e-01,  1.2871e-01],\n",
       "        [-5.0090e-02, -8.2796e-02],\n",
       "        [-5.3850e-02, -8.8398e-02],\n",
       "        [ 1.3183e-01,  1.5383e-01],\n",
       "        [-4.0919e-01, -4.6255e-01],\n",
       "        [-6.2275e-02, -9.8399e-02],\n",
       "        [-4.2645e-01, -4.7719e-01],\n",
       "        [ 2.2375e-01,  2.3533e-01],\n",
       "        [-3.9091e-01, -4.4744e-01],\n",
       "        [-2.3276e-01, -2.8832e-01],\n",
       "        [ 1.6026e-01,  1.8185e-01],\n",
       "        [-3.0578e-03, -2.3548e-02],\n",
       "        [ 9.2671e-02,  1.0206e-01],\n",
       "        [ 1.2260e-01,  1.4541e-01],\n",
       "        [ 1.9812e-02,  5.5531e-03],\n",
       "        [ 9.8775e-02,  1.1023e-01],\n",
       "        [-3.7464e-01, -4.3386e-01],\n",
       "        [ 6.2206e-02,  5.5644e-02],\n",
       "        [ 6.6301e-02,  6.2638e-02],\n",
       "        [-2.2329e-01, -2.8028e-01],\n",
       "        [-1.5164e-01, -2.0403e-01],\n",
       "        [-1.9169e-01, -2.5067e-01],\n",
       "        [-1.2557e-01, -1.7330e-01],\n",
       "        [ 9.0641e-02,  9.8286e-02],\n",
       "        [ 2.8972e-03, -1.5038e-02],\n",
       "        [ 1.1805e-01,  1.3946e-01],\n",
       "        [-3.6300e-01, -4.2435e-01],\n",
       "        [ 1.1267e-01,  1.3261e-01],\n",
       "        [ 3.4637e-02,  2.3247e-02],\n",
       "        [ 3.0592e-01,  2.8346e-01],\n",
       "        [ 1.1444e-01,  1.3387e-01],\n",
       "        [-2.0090e-02, -4.4433e-02],\n",
       "        [ 2.6865e-01,  2.6124e-01],\n",
       "        [ 1.2356e-01,  1.4719e-01],\n",
       "        [ 1.0699e-01,  1.2318e-01],\n",
       "        [-7.7051e-02, -1.1592e-01],\n",
       "        [ 1.5277e-01,  1.7353e-01],\n",
       "        [-4.4577e-02, -7.5209e-02],\n",
       "        [-2.9806e-01, -3.6220e-01],\n",
       "        [ 3.1099e-02,  1.8365e-02],\n",
       "        [ 2.9018e-01,  2.7393e-01],\n",
       "        [ 2.9040e-01,  2.7415e-01],\n",
       "        [ 3.0348e-01,  2.8209e-01],\n",
       "        [-1.8663e-02, -4.2671e-02],\n",
       "        [-4.0093e-02, -7.0506e-02],\n",
       "        [ 1.9157e-03, -1.6127e-02],\n",
       "        [ 4.1928e-02,  3.0150e-02],\n",
       "        [ 1.1612e-01,  1.3666e-01],\n",
       "        [ 1.4620e-01,  1.6955e-01],\n",
       "        [-2.4141e-02, -4.9900e-02],\n",
       "        [ 2.3284e-01,  2.4067e-01],\n",
       "        [ 4.8653e-02,  3.8238e-02],\n",
       "        [ 4.0890e-02,  2.9911e-02],\n",
       "        [-1.1940e-02, -3.3528e-02],\n",
       "        [ 6.8804e-03, -1.0225e-02],\n",
       "        [ 1.2655e-01,  1.5030e-01],\n",
       "        [-3.0796e-01, -3.7215e-01],\n",
       "        [-2.0498e-01, -2.6513e-01],\n",
       "        [ 2.4378e-01,  2.4680e-01],\n",
       "        [ 1.2330e-01,  1.4538e-01],\n",
       "        [ 1.4964e-01,  1.7135e-01],\n",
       "        [-3.1275e-01, -3.7700e-01],\n",
       "        [ 9.7649e-02,  1.0928e-01],\n",
       "        [ 2.6578e-01,  2.5977e-01],\n",
       "        [-1.5924e-01, -2.1308e-01],\n",
       "        [-8.4982e-04, -2.0043e-02],\n",
       "        [-2.4115e-01, -2.9836e-01],\n",
       "        [-2.0836e-01, -2.6769e-01],\n",
       "        [ 2.1194e-01,  2.2903e-01],\n",
       "        [-1.1858e-01, -1.6495e-01],\n",
       "        [-1.0344e-01, -1.4715e-01],\n",
       "        [-3.2953e-01, -3.9371e-01],\n",
       "        [ 3.7682e-02,  2.5265e-02],\n",
       "        [-2.5370e-02, -5.0493e-02],\n",
       "        [-3.8931e-01, -4.4608e-01],\n",
       "        [ 2.2000e-01,  2.3348e-01],\n",
       "        [ 2.1142e-01,  2.2858e-01],\n",
       "        [ 1.0444e-01,  1.2037e-01],\n",
       "        [-2.5131e-02, -5.1490e-02],\n",
       "        [-2.7814e-01, -3.4206e-01],\n",
       "        [-3.8755e-01, -4.4420e-01],\n",
       "        [ 1.0183e-01,  1.1535e-01],\n",
       "        [-8.5548e-02, -1.2587e-01],\n",
       "        [-1.7501e-01, -2.3176e-01],\n",
       "        [-3.4427e-01, -4.0867e-01],\n",
       "        [ 1.4223e-01,  1.6476e-01],\n",
       "        [ 1.9418e-01,  2.1365e-01],\n",
       "        [ 2.3447e-01,  2.4156e-01],\n",
       "        [ 2.7782e-01,  2.6685e-01],\n",
       "        [ 1.7823e-01,  1.9781e-01],\n",
       "        [ 1.1626e-01,  1.3756e-01],\n",
       "        [ 3.2616e-02,  1.9820e-02],\n",
       "        [-2.6665e-01, -3.2879e-01],\n",
       "        [ 2.9139e-01,  2.7454e-01],\n",
       "        [-3.1550e-01, -3.7973e-01],\n",
       "        [-3.1738e-02, -6.0042e-02],\n",
       "        [-2.6349e-01, -3.2448e-01],\n",
       "        [ 2.3203e-01,  2.4007e-01],\n",
       "        [ 2.1619e-01,  2.3111e-01],\n",
       "        [-2.4200e-01, -2.9900e-01],\n",
       "        [-1.6804e-01, -2.2360e-01],\n",
       "        [ 3.1733e-01,  2.9158e-01],\n",
       "        [ 2.1601e-01,  2.3107e-01],\n",
       "        [ 2.8503e-01,  2.7094e-01],\n",
       "        [-2.9307e-01, -3.5720e-01],\n",
       "        [-2.5562e-01, -3.1441e-01],\n",
       "        [-2.0784e-01, -2.6768e-01],\n",
       "        [ 2.8704e-02,  1.5896e-02],\n",
       "        [-1.1643e-01, -1.6245e-01],\n",
       "        [ 3.0436e-01,  2.8237e-01],\n",
       "        [ 8.2234e-02,  8.5978e-02],\n",
       "        [ 2.0814e-01,  2.2710e-01],\n",
       "        [-1.4573e-02, -3.8097e-02],\n",
       "        [ 3.3723e-02,  2.1649e-02],\n",
       "        [-1.0798e-02, -3.3368e-02],\n",
       "        [ 2.9722e-02,  1.7083e-02],\n",
       "        [ 1.9780e-01,  2.1693e-01],\n",
       "        [-3.1391e-01, -3.7815e-01],\n",
       "        [-2.7960e-01, -3.4364e-01],\n",
       "        [ 1.7782e-04, -1.9279e-02],\n",
       "        [-3.0389e-01, -3.6806e-01],\n",
       "        [-2.6075e-01, -3.2147e-01],\n",
       "        [ 5.7543e-03, -1.2235e-02],\n",
       "        [ 2.6743e-01,  2.6034e-01],\n",
       "        [ 1.0548e-01,  1.1804e-01],\n",
       "        [-1.6181e-01, -2.1602e-01],\n",
       "        [-2.6799e-01, -3.3048e-01],\n",
       "        [ 2.1072e-01,  2.2797e-01],\n",
       "        [-2.1143e-01, -2.7085e-01],\n",
       "        [-1.5182e-02, -3.8760e-02],\n",
       "        [ 1.3956e-02, -1.4351e-03],\n",
       "        [ 2.3401e-01,  2.4113e-01],\n",
       "        [-9.9309e-02, -1.4233e-01],\n",
       "        [-6.7155e-03, -2.8259e-02],\n",
       "        [ 1.1060e-01,  1.2912e-01],\n",
       "        [ 6.4799e-03, -1.0932e-02],\n",
       "        [-3.5065e-01, -4.1378e-01],\n",
       "        [ 1.8324e-03, -1.7297e-02],\n",
       "        [-3.2162e-01, -3.8610e-01],\n",
       "        [ 2.6209e-01,  2.5750e-01],\n",
       "        [ 3.2159e-01,  2.9433e-01],\n",
       "        [-9.2993e-02, -1.3475e-01],\n",
       "        [-4.2081e-01, -4.7184e-01],\n",
       "        [ 2.8875e-01,  2.7276e-01],\n",
       "        [ 1.0875e-01,  1.2821e-01],\n",
       "        [ 2.1952e-01,  2.3288e-01],\n",
       "        [-1.1641e-01, -1.6240e-01],\n",
       "        [ 2.5400e-01,  2.5336e-01],\n",
       "        [-2.1848e-01, -2.7550e-01],\n",
       "        [ 9.8199e-02,  1.0968e-01],\n",
       "        [ 1.6148e-01,  1.8422e-01],\n",
       "        [-2.1332e-01, -2.7113e-01],\n",
       "        [ 1.6732e-01,  1.8787e-01],\n",
       "        [ 2.2385e-01,  2.3551e-01],\n",
       "        [-2.2819e-01, -2.8557e-01],\n",
       "        [-2.7966e-01, -3.4329e-01],\n",
       "        [ 3.8731e-01,  3.2858e-01],\n",
       "        [-3.6187e-01, -4.2307e-01],\n",
       "        [-2.0086e-01, -2.6033e-01],\n",
       "        [-1.0428e-01, -1.4808e-01],\n",
       "        [-2.8506e-03, -2.2894e-02],\n",
       "        [ 2.1143e-01,  2.2842e-01],\n",
       "        [ 2.5111e-02,  1.1160e-02],\n",
       "        [-1.6866e-02, -4.1236e-02],\n",
       "        [ 1.4061e-01,  1.6383e-01],\n",
       "        [ 5.6637e-02,  4.9720e-02],\n",
       "        [-1.9952e-01, -2.5940e-01],\n",
       "        [ 3.4845e-01,  3.1050e-01],\n",
       "        [ 2.1235e-01,  2.2897e-01],\n",
       "        [-9.8849e-02, -1.4173e-01],\n",
       "        [-6.2856e-02, -9.8858e-02],\n",
       "        [-1.1030e-01, -1.5524e-01],\n",
       "        [-2.7837e-02, -5.4669e-02],\n",
       "        [ 2.8353e-02,  1.6994e-02],\n",
       "        [-3.6151e-01, -4.2275e-01],\n",
       "        [-7.6506e-02, -1.1532e-01],\n",
       "        [ 4.2633e-01,  3.4488e-01],\n",
       "        [ 2.2316e-01,  2.3497e-01],\n",
       "        [-6.9792e-02, -1.0729e-01],\n",
       "        [ 2.5850e-01,  2.5528e-01],\n",
       "        [ 4.8938e-02,  3.8596e-02],\n",
       "        [-2.6438e-01, -3.2587e-01],\n",
       "        [-5.7740e-03, -2.6211e-02],\n",
       "        [ 2.5911e-01,  2.5594e-01],\n",
       "        [ 6.8860e-02,  6.6626e-02],\n",
       "        [-8.9066e-02, -1.3010e-01],\n",
       "        [-3.3414e-01, -3.9865e-01],\n",
       "        [-3.6782e-01, -4.2819e-01],\n",
       "        [-2.3725e-01, -2.9366e-01],\n",
       "        [ 7.4212e-02,  7.3200e-02],\n",
       "        [ 1.9176e-01,  2.1016e-01],\n",
       "        [ 2.5973e-01,  2.5631e-01],\n",
       "        [ 1.7665e-01,  1.9679e-01],\n",
       "        [ 1.2035e-01,  1.4442e-01],\n",
       "        [-3.1712e-01, -3.8142e-01],\n",
       "        [ 2.2322e-01,  2.3479e-01],\n",
       "        [ 2.0657e-02,  6.7098e-03],\n",
       "        [ 1.1150e-01,  1.2984e-01],\n",
       "        [-2.3058e-01, -2.8728e-01],\n",
       "        [ 1.6880e-01,  1.8850e-01],\n",
       "        [-2.2803e-01, -2.8486e-01],\n",
       "        [-1.2480e-01, -1.7243e-01],\n",
       "        [-3.7118e-02, -6.6745e-02],\n",
       "        [ 2.5233e-01,  2.5181e-01],\n",
       "        [ 2.5152e-01,  2.5150e-01],\n",
       "        [ 1.6488e-01,  1.8674e-01],\n",
       "        [ 1.1072e-01,  1.2729e-01],\n",
       "        [ 2.2881e-01,  2.3840e-01],\n",
       "        [-7.8312e-02, -1.1736e-01],\n",
       "        [ 1.1491e-01,  1.3420e-01],\n",
       "        [ 3.3274e-01,  3.0114e-01],\n",
       "        [-4.7057e-01, -5.1096e-01],\n",
       "        [-3.0778e-01, -3.7205e-01],\n",
       "        [ 3.8391e-01,  3.2755e-01],\n",
       "        [-2.7098e-03, -2.2222e-02],\n",
       "        [-1.9811e-01, -2.6013e-01],\n",
       "        [ 2.2560e-01,  2.3649e-01],\n",
       "        [-2.4279e-01, -2.9907e-01],\n",
       "        [-2.1791e-01, -2.7633e-01],\n",
       "        [-2.4324e-01, -2.9986e-01],\n",
       "        [-3.7947e-02, -6.7987e-02],\n",
       "        [-1.1518e-01, -1.6100e-01],\n",
       "        [ 3.2592e-01,  2.9696e-01],\n",
       "        [-2.7460e-01, -3.3821e-01],\n",
       "        [-3.2726e-01, -3.9174e-01],\n",
       "        [ 1.0824e-01,  1.2579e-01],\n",
       "        [ 1.4673e-01,  1.6898e-01],\n",
       "        [ 1.2481e-01,  1.4931e-01],\n",
       "        [-3.6427e-01, -4.2498e-01],\n",
       "        [ 1.1107e-01,  1.2919e-01],\n",
       "        [-3.1502e-01, -3.7930e-01],\n",
       "        [ 2.9825e-02,  1.6829e-02],\n",
       "        [-5.4298e-02, -8.9074e-02],\n",
       "        [ 7.9069e-02,  8.0958e-02],\n",
       "        [-4.0647e-02, -7.1273e-02],\n",
       "        [-2.9671e-01, -3.6083e-01],\n",
       "        [-1.6773e-01, -2.2318e-01],\n",
       "        [-7.2403e-02, -1.1044e-01],\n",
       "        [ 3.3934e-01,  3.0520e-01],\n",
       "        [-2.6216e-01, -3.2268e-01],\n",
       "        [ 2.7154e-01,  2.6326e-01],\n",
       "        [-2.5969e-01, -3.1936e-01],\n",
       "        [-1.7781e-02, -4.1662e-02],\n",
       "        [ 2.2190e-01,  2.3471e-01],\n",
       "        [-2.2290e-01, -2.8067e-01],\n",
       "        [ 1.5555e-01,  1.7648e-01],\n",
       "        [ 1.9296e-01,  2.1238e-01],\n",
       "        [ 1.4987e-01,  1.7124e-01],\n",
       "        [ 9.4893e-02,  1.0508e-01],\n",
       "        [ 8.6686e-02,  9.3104e-02],\n",
       "        [ 2.2286e-01,  2.3475e-01],\n",
       "        [-3.3143e-01, -3.9586e-01],\n",
       "        [ 1.2419e-02, -3.3400e-03],\n",
       "        [ 1.7369e-01,  1.9429e-01],\n",
       "        [ 4.3479e-02,  3.1205e-02],\n",
       "        [-2.6561e-01, -3.2745e-01],\n",
       "        [-8.7827e-02, -1.2865e-01],\n",
       "        [ 2.2366e-01,  2.3543e-01],\n",
       "        [ 1.7055e-01,  1.9093e-01],\n",
       "        [ 1.6434e-01,  1.8760e-01],\n",
       "        [ 3.4023e-01,  3.0586e-01],\n",
       "        [-1.3969e-03, -2.0606e-02],\n",
       "        [-3.6575e-01, -4.2627e-01]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4855, 0.4762], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "pred_valence = test_pred[:, 0]\n",
    "pred_arousal = test_pred[1]\n",
    "real_valence = target_test_labels[0]\n",
    "real_arousal = target_test_labels[1]\n",
    "\n",
    "\n",
    "metric = R2Score(multioutput='raw_values')\n",
    "metric.update(test_pred, target_test_labels)\n",
    "print(metric.compute())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse relationship between epochs and r^2 score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create lists to store the epochs and R^2 scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs_list = [i for i in range(1, 301)]\n",
    "adjusted_r2_scores_valence_list = []\n",
    "adjusted_r2_scores_arousal_list = []\n",
    "r2_scores_list = []\n",
    "rmse_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conduct training and testing for each num_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of epochs: 1\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.30628741899299206\n",
      "Valence RMSE: 0.29255720015915865\n",
      "Arousal RMSE: 0.3194280055064229\n",
      "Test R^2 score: tensor([-0.0373, -0.0103], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.2800, -0.2468], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.023785964414342553\n",
      "Num of epochs: 2\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3053696914054762\n",
      "Valence RMSE: 0.29119191624875757\n",
      "Arousal RMSE: 0.31891780252837576\n",
      "Test R^2 score: tensor([-0.0276, -0.0071], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.2681, -0.2428], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.017344226880689595\n",
      "Num of epochs: 3\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.30460095715340424\n",
      "Valence RMSE: 0.2900296619349325\n",
      "Arousal RMSE: 0.31850632865839373\n",
      "Test R^2 score: tensor([-0.0194, -0.0045], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.2580, -0.2396], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.011952347599869295\n",
      "Num of epochs: 4\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3039804408375489\n",
      "Valence RMSE: 0.28907222396802845\n",
      "Arousal RMSE: 0.3181909272021424\n",
      "Test R^2 score: tensor([-0.0127, -0.0025], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.2497, -0.2371], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.0075984297478562945\n",
      "Num of epochs: 5\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.30350718848485475\n",
      "Valence RMSE: 0.28832044478870933\n",
      "Arousal RMSE: 0.3179694136875481\n",
      "Test R^2 score: tensor([-0.0074, -0.0011], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.2432, -0.2354], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.004270509583158444\n",
      "Num of epochs: 6\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.30317799960027547\n",
      "Valence RMSE: 0.28777261971839885\n",
      "Arousal RMSE: 0.3178375657842526\n",
      "Test R^2 score: tensor([-0.0036, -0.0003], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.2385, -0.2344], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.0019431147594275444\n",
      "Num of epochs: 7\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3029871903412841\n",
      "Valence RMSE: 0.2874239515503472\n",
      "Arousal RMSE: 0.3177891550965745\n",
      "Test R^2 score: tensor([-1.1809e-03,  2.9851e-05], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.2355, -0.2340], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.0005755229358211378\n",
      "Num of epochs: 8\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3029267033307572\n",
      "Valence RMSE: 0.2872657778985386\n",
      "Arousal RMSE: 0.3178168466743522\n",
      "Test R^2 score: tensor([-7.9271e-05, -1.4443e-04], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.2341, -0.2342], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.00011184928573815256\n",
      "Num of epochs: 9\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3029828243490278\n",
      "Valence RMSE: 0.28728318568088634\n",
      "Arousal RMSE: 0.3179080919480801\n",
      "Test R^2 score: tensor([-0.0002, -0.0007], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.2343, -0.2349], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.00045963696610518134\n",
      "Num of epochs: 10\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3031396426781053\n",
      "Valence RMSE: 0.28745617828228615\n",
      "Arousal RMSE: 0.3180506744111089\n",
      "Test R^2 score: tensor([-0.0014, -0.0016], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.2358, -0.2360], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.0015110317217230662\n",
      "Num of epochs: 11\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3033775593708414\n",
      "Valence RMSE: 0.28775877248323883\n",
      "Arousal RMSE: 0.3182306960658516\n",
      "Test R^2 score: tensor([-0.0035, -0.0028], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.2384, -0.2374], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.003132819127351749\n",
      "Num of epochs: 12\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.30367351019410405\n",
      "Valence RMSE: 0.28815902646466546\n",
      "Arousal RMSE: 0.3184330024575869\n",
      "Test R^2 score: tensor([-0.0063, -0.0040], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.2418, -0.2390], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.005167288387742497\n",
      "Num of epochs: 13\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.30400163677433795\n",
      "Valence RMSE: 0.2886199011993547\n",
      "Arousal RMSE: 0.31864171565353416\n",
      "Test R^2 score: tensor([-0.0095, -0.0053], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.2458, -0.2406], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.007436334234060649\n",
      "Num of epochs: 14\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.30433481817651736\n",
      "Valence RMSE: 0.2891012155962777\n",
      "Arousal RMSE: 0.3188414186548264\n",
      "Test R^2 score: tensor([-0.0129, -0.0066], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.2500, -0.2422], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.009751549578371543\n",
      "Num of epochs: 15\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3046471061806914\n",
      "Valence RMSE: 0.2895629312413686\n",
      "Arousal RMSE: 0.3190188512603062\n",
      "Test R^2 score: tensor([-0.0161, -0.0077], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.2540, -0.2436], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.011930838155945755\n",
      "Num of epochs: 16\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3049165933768434\n",
      "Valence RMSE: 0.28996952818080646\n",
      "Arousal RMSE: 0.319164425586059\n",
      "Test R^2 score: tensor([-0.0190, -0.0086], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.2575, -0.2447], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.013818623171499844\n",
      "Num of epochs: 17\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.30512744999420877\n",
      "Valence RMSE: 0.2902935960952931\n",
      "Arousal RMSE: 0.3192728449868538\n",
      "Test R^2 score: tensor([-0.0213, -0.0093], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.2603, -0.2456], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.015300771547403058\n",
      "Num of epochs: 18\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3052696895528051\n",
      "Valence RMSE: 0.2905178083493089\n",
      "Arousal RMSE: 0.31934083633517063\n",
      "Test R^2 score: tensor([-0.0229, -0.0098], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.2622, -0.2461], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.016304836373735143\n",
      "Num of epochs: 19\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.30534028326073104\n",
      "Valence RMSE: 0.29063566974054644\n",
      "Arousal RMSE: 0.3193685717755441\n",
      "Test R^2 score: tensor([-0.0237, -0.0099], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.2633, -0.2463], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.01680758863836951\n",
      "Num of epochs: 20\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3053422125250588\n",
      "Valence RMSE: 0.2906502210600181\n",
      "Arousal RMSE: 0.31935901818667795\n",
      "Test R^2 score: tensor([-0.0238, -0.0099], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.2634, -0.2462], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.01682863203812124\n",
      "Num of epochs: 21\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3052828684218353\n",
      "Valence RMSE: 0.2905717914361281\n",
      "Arousal RMSE: 0.3193169170672397\n",
      "Test R^2 score: tensor([-0.0232, -0.0096], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.2627, -0.2459], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.0164192866783176\n",
      "Num of epochs: 22\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3051724683257303\n",
      "Valence RMSE: 0.2904155062004804\n",
      "Arousal RMSE: 0.31924802991773726\n",
      "Test R^2 score: tensor([-0.0221, -0.0092], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.2614, -0.2454], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.015651303752665435\n",
      "Num of epochs: 23\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3050227607144301\n",
      "Valence RMSE: 0.29019910260933834\n",
      "Arousal RMSE: 0.3191586595291434\n",
      "Test R^2 score: tensor([-0.0206, -0.0086], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.2595, -0.2447], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.014607477017038129\n",
      "Num of epochs: 24\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.30484582248760517\n",
      "Valence RMSE: 0.28994101221509205\n",
      "Arousal RMSE: 0.31905510560399564\n",
      "Test R^2 score: tensor([-0.0188, -0.0080], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.2572, -0.2439], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.013372998174445594\n",
      "Num of epochs: 25\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3046531494768817\n",
      "Valence RMSE: 0.2896589076515853\n",
      "Arousal RMSE: 0.3189432554396735\n",
      "Test R^2 score: tensor([-0.0168, -0.0072], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.2548, -0.2430], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.012028930049826969\n",
      "Num of epochs: 26\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.30445500487723365\n",
      "Valence RMSE: 0.2893686447036151\n",
      "Arousal RMSE: 0.31882830403210083\n",
      "Test R^2 score: tensor([-0.0148, -0.0065], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.2523, -0.2421], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.010647549097287934\n",
      "Num of epochs: 27\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.30426002172744016\n",
      "Valence RMSE: 0.28908357905471455\n",
      "Arousal RMSE: 0.31871461523450934\n",
      "Test R^2 score: tensor([-0.0128, -0.0058], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.2498, -0.2412], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.009289512658158361\n",
      "Num of epochs: 28\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3040750109163293\n",
      "Valence RMSE: 0.2888142167131625\n",
      "Arousal RMSE: 0.318605669679455\n",
      "Test R^2 score: tensor([-0.0109, -0.0051], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.2475, -0.2404], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.008002514817448847\n",
      "Num of epochs: 29\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3039048814132281\n",
      "Valence RMSE: 0.2885680944844563\n",
      "Arousal RMSE: 0.3185040168336757\n",
      "Test R^2 score: tensor([-0.0092, -0.0045], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.2454, -0.2396], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.006820783761019911\n",
      "Num of epochs: 30\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3037527325444509\n",
      "Valence RMSE: 0.28834992732592074\n",
      "Arousal RMSE: 0.3184113133473395\n",
      "Test R^2 score: tensor([-0.0076, -0.0039], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.2435, -0.2388], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.005765788932211469\n",
      "Num of epochs: 31\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3036201140853582\n",
      "Valence RMSE: 0.2881619296196228\n",
      "Arousal RMSE: 0.31832852475439183\n",
      "Test R^2 score: tensor([-0.0063, -0.0034], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.2419, -0.2382], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.004848060404855303\n",
      "Num of epochs: 32\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3035072798489942\n",
      "Valence RMSE: 0.2880042477048846\n",
      "Arousal RMSE: 0.3182560150988743\n",
      "Test R^2 score: tensor([-0.0052, -0.0029], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.2405, -0.2376], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.004069025558153916\n",
      "Num of epochs: 33\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.30341277520217314\n",
      "Valence RMSE: 0.2878752787125134\n",
      "Arousal RMSE: 0.3181924703979678\n",
      "Test R^2 score: tensor([-0.0043, -0.0025], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.2394, -0.2371], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.003418756520809363\n",
      "Num of epochs: 34\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.30333409015698326\n",
      "Valence RMSE: 0.2877722288745387\n",
      "Arousal RMSE: 0.3181356389832956\n",
      "Test R^2 score: tensor([-0.0036, -0.0022], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.2385, -0.2367], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.002880265440866392\n",
      "Num of epochs: 35\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.30327289082506764\n",
      "Valence RMSE: 0.28769308327945275\n",
      "Arousal RMSE: 0.318090525561433\n",
      "Test R^2 score: tensor([-0.0031, -0.0019], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.2378, -0.2363], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.0024621816129508645\n",
      "Num of epochs: 36\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.30322565970099197\n",
      "Valence RMSE: 0.2876342872700256\n",
      "Arousal RMSE: 0.3180536404270008\n",
      "Test R^2 score: tensor([-0.0026, -0.0016], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.2373, -0.2361], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.0021410394421828016\n",
      "Num of epochs: 37\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.30319056325621224\n",
      "Valence RMSE: 0.2875929251512937\n",
      "Arousal RMSE: 0.3180241259687084\n",
      "Test R^2 score: tensor([-0.0024, -0.0014], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.2370, -0.2358], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.001903923581702549\n",
      "Num of epochs: 38\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3031659480015754\n",
      "Valence RMSE: 0.28756650372640685\n",
      "Arousal RMSE: 0.318001084888021\n",
      "Test R^2 score: tensor([-0.0022, -0.0013], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.2367, -0.2357], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.001739287112180965\n",
      "Num of epochs: 39\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.30315059704658887\n",
      "Valence RMSE: 0.28755306066078673\n",
      "Arousal RMSE: 0.31798397174074755\n",
      "Test R^2 score: tensor([-0.0021, -0.0012], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.2366, -0.2355], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.001638555403578601\n",
      "Num of epochs: 40\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3031436805482352\n",
      "Valence RMSE: 0.28755118887540404\n",
      "Arousal RMSE: 0.3179724766218024\n",
      "Test R^2 score: tensor([-0.0021, -0.0011], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.2366, -0.2354], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.0015958399288817926\n",
      "Num of epochs: 41\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.30314488984241045\n",
      "Valence RMSE: 0.2875599297524383\n",
      "Arousal RMSE: 0.31796687763964826\n",
      "Test R^2 score: tensor([-0.0021, -0.0011], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.2367, -0.2354], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.0016086728514428383\n",
      "Num of epochs: 42\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.30315069130758515\n",
      "Valence RMSE: 0.2875778120242146\n",
      "Arousal RMSE: 0.3179617670599706\n",
      "Test R^2 score: tensor([-0.0023, -0.0011], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.2368, -0.2353], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.0016549033392523338\n",
      "Num of epochs: 43\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.30316188491927243\n",
      "Valence RMSE: 0.2876044560733157\n",
      "Arousal RMSE: 0.31795901273966837\n",
      "Test R^2 score: tensor([-0.0024, -0.0010], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.2371, -0.2353], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.0017390947223525632\n",
      "Num of epochs: 44\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3031784575641406\n",
      "Valence RMSE: 0.28763932407400516\n",
      "Arousal RMSE: 0.31795907520967576\n",
      "Test R^2 score: tensor([-0.0027, -0.0010], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.2374, -0.2353], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.001860830388945911\n",
      "Num of epochs: 45\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.30319979993500223\n",
      "Valence RMSE: 0.28768162794179114\n",
      "Arousal RMSE: 0.31796150444041427\n",
      "Test R^2 score: tensor([-0.0030, -0.0011], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.2377, -0.2353], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.002015956325314372\n",
      "Num of epochs: 46\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3032253926452807\n",
      "Valence RMSE: 0.2877303307581809\n",
      "Arousal RMSE: 0.31796624703190923\n",
      "Test R^2 score: tensor([-0.0033, -0.0011], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.2381, -0.2354], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.0022007002702821854\n",
      "Num of epochs: 47\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3032541963690129\n",
      "Valence RMSE: 0.2877840535952125\n",
      "Arousal RMSE: 0.3179725675701278\n",
      "Test R^2 score: tensor([-0.0037, -0.0011], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.2386, -0.2354], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.002407949219137784\n",
      "Num of epochs: 48\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3032850359735428\n",
      "Valence RMSE: 0.28784115611189715\n",
      "Arousal RMSE: 0.3179797083763565\n",
      "Test R^2 score: tensor([-0.0041, -0.0012], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.2391, -0.2355], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.002629605600766305\n",
      "Num of epochs: 49\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3033162226588065\n",
      "Valence RMSE: 0.2878995876820719\n",
      "Arousal RMSE: 0.3179863035863437\n",
      "Test R^2 score: tensor([-0.0045, -0.0012], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.2396, -0.2355], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.002854221267243995\n",
      "Num of epochs: 50\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.30334540316636316\n",
      "Valence RMSE: 0.2879570704053427\n",
      "Arousal RMSE: 0.3179899257018992\n",
      "Test R^2 score: tensor([-0.0049, -0.0012], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.2401, -0.2356], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.003066206265666338\n",
      "Num of epochs: 51\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.30337021722520763\n",
      "Valence RMSE: 0.28801117880306487\n",
      "Arousal RMSE: 0.31798826752414666\n",
      "Test R^2 score: tensor([-0.0053, -0.0012], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.2406, -0.2356], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.0032498278369949007\n",
      "Num of epochs: 52\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.30338934492826974\n",
      "Valence RMSE: 0.2880598363418793\n",
      "Arousal RMSE: 0.31798069110985183\n",
      "Test R^2 score: tensor([-0.0056, -0.0012], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.2410, -0.2355], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.0033958218137106755\n",
      "Num of epochs: 53\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3034012019738438\n",
      "Valence RMSE: 0.2881010981464664\n",
      "Arousal RMSE: 0.31796593522757505\n",
      "Test R^2 score: tensor([-0.0059, -0.0011], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.2413, -0.2354], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.0034934184724028094\n",
      "Num of epochs: 54\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3034043934073423\n",
      "Valence RMSE: 0.2881332044018632\n",
      "Arousal RMSE: 0.31794293261349404\n",
      "Test R^2 score: tensor([-0.0061, -0.0009], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.2416, -0.2352], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.0035331048420031985\n",
      "Num of epochs: 55\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.30339779449646076\n",
      "Valence RMSE: 0.28815481281264366\n",
      "Arousal RMSE: 0.31791075361419213\n",
      "Test R^2 score: tensor([-0.0063, -0.0007], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.2418, -0.2350], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.003507261952069962\n",
      "Num of epochs: 56\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.30337904171088864\n",
      "Valence RMSE: 0.28816466712930927\n",
      "Arousal RMSE: 0.3178660260504238\n",
      "Test R^2 score: tensor([-0.0063, -0.0005], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.2419, -0.2346], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.003400889480056679\n",
      "Num of epochs: 57\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.30334680722633084\n",
      "Valence RMSE: 0.2881620058846995\n",
      "Arousal RMSE: 0.31780690564134034\n",
      "Test R^2 score: tensor([-6.3292e-03, -8.1861e-05], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.2419, -0.2341], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.0032055370210990475\n",
      "Num of epochs: 58\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3033009399334336\n",
      "Valence RMSE: 0.28814692736819647\n",
      "Arousal RMSE: 0.3177330146164794\n",
      "Test R^2 score: tensor([-0.0062,  0.0004], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.2417, -0.2336], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.0029203857931712762\n",
      "Num of epochs: 59\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3032447327181086\n",
      "Valence RMSE: 0.28812016044334565\n",
      "Arousal RMSE: 0.31764997873233836\n",
      "Test R^2 score: tensor([-0.0060,  0.0009], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.2415, -0.2329], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.002565714371296046\n",
      "Num of epochs: 60\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.30317949515918424\n",
      "Valence RMSE: 0.28808260085431786\n",
      "Arousal RMSE: 0.3175594867973045\n",
      "Test R^2 score: tensor([-0.0058,  0.0015], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.2412, -0.2322], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.0021499941968158853\n",
      "Num of epochs: 61\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3031039937759731\n",
      "Valence RMSE: 0.28803510383317865\n",
      "Arousal RMSE: 0.31745840837139633\n",
      "Test R^2 score: tensor([-0.0054,  0.0021], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.2408, -0.2314], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.0016664052163517518\n",
      "Num of epochs: 62\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3030131303635429\n",
      "Valence RMSE: 0.2879767397304172\n",
      "Arousal RMSE: 0.31733785106675566\n",
      "Test R^2 score: tensor([-0.0050,  0.0029], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.2403, -0.2305], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.001083810048991718\n",
      "Num of epochs: 63\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3029043888596193\n",
      "Valence RMSE: 0.2879076295584761\n",
      "Arousal RMSE: 0.31719289781276344\n",
      "Test R^2 score: tensor([-0.0046,  0.0038], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.2397, -0.2294], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.000387280368817855\n",
      "Num of epochs: 64\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3027808028332051\n",
      "Valence RMSE: 0.28782940890628117\n",
      "Arousal RMSE: 0.3170278544502064\n",
      "Test R^2 score: tensor([-0.0040,  0.0048], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.2390, -0.2281], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.00040383011817002323\n",
      "Num of epochs: 65\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.30264945344353966\n",
      "Valence RMSE: 0.28774455518062864\n",
      "Arousal RMSE: 0.31685399524583674\n",
      "Test R^2 score: tensor([-0.0034,  0.0059], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.2383, -0.2268], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.0012453867871810576\n",
      "Num of epochs: 66\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3025120464571579\n",
      "Valence RMSE: 0.28765391244202415\n",
      "Arousal RMSE: 0.3166738119267074\n",
      "Test R^2 score: tensor([-0.0028,  0.0070], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.2375, -0.2254], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.0021265681548336413\n",
      "Num of epochs: 67\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.30236609923482416\n",
      "Valence RMSE: 0.2875579649352121\n",
      "Arousal RMSE: 0.3164821207197754\n",
      "Test R^2 score: tensor([-0.0021,  0.0082], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.2367, -0.2239], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.00306187829182919\n",
      "Num of epochs: 68\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3022012957161513\n",
      "Valence RMSE: 0.28745384715476685\n",
      "Arousal RMSE: 0.3162618092988234\n",
      "Test R^2 score: tensor([-0.0014,  0.0096], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.2358, -0.2222], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.004114804735962707\n",
      "Num of epochs: 69\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3020146772863144\n",
      "Valence RMSE: 0.2873392828155005\n",
      "Arousal RMSE: 0.3160092833189986\n",
      "Test R^2 score: tensor([-0.0006,  0.0112], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.2348, -0.2202], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.00530430276600552\n",
      "Num of epochs: 70\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3018176203781982\n",
      "Valence RMSE: 0.2872170194007911\n",
      "Arousal RMSE: 0.315743781740895\n",
      "Test R^2 score: tensor([0.0003, 0.0129], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.2337, -0.2182], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.006560377137306395\n",
      "Num of epochs: 71\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.30162401716079457\n",
      "Valence RMSE: 0.2870894898243315\n",
      "Arousal RMSE: 0.31548965163509823\n",
      "Test R^2 score: tensor([0.0011, 0.0144], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.2326, -0.2162], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.007798372311664581\n",
      "Num of epochs: 72\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.30140105805179124\n",
      "Valence RMSE: 0.2869464571915366\n",
      "Arousal RMSE: 0.3151934743847089\n",
      "Test R^2 score: tensor([0.0021, 0.0163], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.2314, -0.2139], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.009220679871136062\n",
      "Num of epochs: 73\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.30111912955049874\n",
      "Valence RMSE: 0.2867757033745221\n",
      "Arousal RMSE: 0.31480971445707995\n",
      "Test R^2 score: tensor([0.0033, 0.0187], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.2299, -0.2110], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.011011264591200332\n",
      "Num of epochs: 74\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3007998169823541\n",
      "Valence RMSE: 0.28658121414655235\n",
      "Arousal RMSE: 0.3143759970028299\n",
      "Test R^2 score: tensor([0.0047, 0.0214], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.2283, -0.2076], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.013037998706553644\n",
      "Num of epochs: 75\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.30045656715019764\n",
      "Valence RMSE: 0.2863608232514969\n",
      "Arousal RMSE: 0.31392001591820706\n",
      "Test R^2 score: tensor([0.0062, 0.0242], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.2264, -0.2041], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.01522151044081771\n",
      "Num of epochs: 76\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.30007626965599754\n",
      "Valence RMSE: 0.28611056617387165\n",
      "Arousal RMSE: 0.3134202915335645\n",
      "Test R^2 score: tensor([0.0079, 0.0273], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.2242, -0.2003], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.01764169926106479\n",
      "Num of epochs: 77\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.29962961267520727\n",
      "Valence RMSE: 0.28582038208026656\n",
      "Arousal RMSE: 0.3128298559461355\n",
      "Test R^2 score: tensor([0.0100, 0.0310], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.2218, -0.1958], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.020477989912512873\n",
      "Num of epochs: 78\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2991075845226839\n",
      "Valence RMSE: 0.28548019927216606\n",
      "Arousal RMSE: 0.3121405934214795\n",
      "Test R^2 score: tensor([0.0123, 0.0353], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.2188, -0.1905], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.023788299945707636\n",
      "Num of epochs: 79\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.29856448483851583\n",
      "Valence RMSE: 0.28509532470090904\n",
      "Arousal RMSE: 0.3114516961704654\n",
      "Test R^2 score: tensor([0.0150, 0.0395], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.2156, -0.1853], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.027245802166086852\n",
      "Num of epochs: 80\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.29799253872134274\n",
      "Valence RMSE: 0.2846639130879717\n",
      "Arousal RMSE: 0.31075000056737834\n",
      "Test R^2 score: tensor([0.0180, 0.0438], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.2119, -0.1799], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.03089674925762448\n",
      "Num of epochs: 81\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.29734294215642815\n",
      "Valence RMSE: 0.28417063466106546\n",
      "Arousal RMSE: 0.30995596606100234\n",
      "Test R^2 score: tensor([0.0214, 0.0487], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.2077, -0.1739], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.03503708760438179\n",
      "Num of epochs: 82\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.29658599551651665\n",
      "Valence RMSE: 0.28359837468535115\n",
      "Arousal RMSE: 0.30902826302601133\n",
      "Test R^2 score: tensor([0.0253, 0.0544], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.2028, -0.1669], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.03984882651281013\n",
      "Num of epochs: 83\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2957177204419412\n",
      "Valence RMSE: 0.2829442764562363\n",
      "Arousal RMSE: 0.30796181059899796\n",
      "Test R^2 score: tensor([0.0298, 0.0609], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.1973, -0.1589], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.04535192825143963\n",
      "Num of epochs: 84\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.294805790538713\n",
      "Valence RMSE: 0.2822145519861866\n",
      "Arousal RMSE: 0.3068808480787731\n",
      "Test R^2 score: tensor([0.0348, 0.0675], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.1911, -0.1507], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.05114136376762829\n",
      "Num of epochs: 85\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.29376322912962693\n",
      "Valence RMSE: 0.2813786877754516\n",
      "Arousal RMSE: 0.30564637024361396\n",
      "Test R^2 score: tensor([0.0405, 0.0750], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.1841, -0.1415], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.057739501053295894\n",
      "Num of epochs: 86\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.29254768163356526\n",
      "Valence RMSE: 0.2804222190271432\n",
      "Arousal RMSE: 0.30419018908281803\n",
      "Test R^2 score: tensor([0.0470, 0.0838], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.1760, -0.1307], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.06539204898002732\n",
      "Num of epochs: 87\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2912776291320935\n",
      "Valence RMSE: 0.27942400702244335\n",
      "Arousal RMSE: 0.30266737314277586\n",
      "Test R^2 score: tensor([0.0538, 0.0929], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.1677, -0.1194], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.07335360204140157\n",
      "Num of epochs: 88\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2899678930624289\n",
      "Valence RMSE: 0.27829728278267063\n",
      "Arousal RMSE: 0.3011866205692257\n",
      "Test R^2 score: tensor([0.0614, 0.1018], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.1583, -0.1084], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.08158821404803279\n",
      "Num of epochs: 89\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2886057818132321\n",
      "Valence RMSE: 0.2770518655798119\n",
      "Arousal RMSE: 0.2997146282229143\n",
      "Test R^2 score: tensor([0.0698, 0.1105], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.1479, -0.0976], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.09015834076487916\n",
      "Num of epochs: 90\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.28707802770321694\n",
      "Valence RMSE: 0.27568592387198276\n",
      "Arousal RMSE: 0.29803499686914176\n",
      "Test R^2 score: tensor([0.0789, 0.1205], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.1366, -0.0854], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.09970394633041918\n",
      "Num of epochs: 91\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.28534260673089673\n",
      "Valence RMSE: 0.2740580399950878\n",
      "Arousal RMSE: 0.29619756438231765\n",
      "Test R^2 score: tensor([0.0898, 0.1313], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.1233, -0.0720], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.11053234342934509\n",
      "Num of epochs: 92\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2835673828293721\n",
      "Valence RMSE: 0.2722740831206451\n",
      "Arousal RMSE: 0.2944278262498492\n",
      "Test R^2 score: tensor([0.1016, 0.1416], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.1087, -0.0592], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.12161300665652636\n",
      "Num of epochs: 93\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2814643092107949\n",
      "Valence RMSE: 0.27023366069450605\n",
      "Arousal RMSE: 0.2922637222555452\n",
      "Test R^2 score: tensor([0.1150, 0.1542], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.0921, -0.0437], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.13460642859648353\n",
      "Num of epochs: 94\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2791968658491264\n",
      "Valence RMSE: 0.2680633231640262\n",
      "Arousal RMSE: 0.28990314688566077\n",
      "Test R^2 score: tensor([0.1292, 0.1678], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.0747, -0.0269], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.14848934203347192\n",
      "Num of epochs: 95\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.27726111160151684\n",
      "Valence RMSE: 0.26588348621831553\n",
      "Arousal RMSE: 0.28818990226830016\n",
      "Test R^2 score: tensor([0.1433, 0.1776], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.0573, -0.0148], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.16044547994905478\n",
      "Num of epochs: 96\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.27502449189237144\n",
      "Valence RMSE: 0.26343399549577295\n",
      "Arousal RMSE: 0.28614589338036506\n",
      "Test R^2 score: tensor([0.1590, 0.1893], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.0379, -0.0005], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.17411399830184887\n",
      "Num of epochs: 97\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2720880964566229\n",
      "Valence RMSE: 0.26058181456489105\n",
      "Arousal RMSE: 0.2831271487951055\n",
      "Test R^2 score: tensor([0.1771, 0.2063], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([-0.0155,  0.0205], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.19167841939706853\n",
      "Num of epochs: 98\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.26966289288171086\n",
      "Valence RMSE: 0.257738104582318\n",
      "Arousal RMSE: 0.28108223181300557\n",
      "Test R^2 score: tensor([0.1949, 0.2177], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.0065, 0.0346], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.20632191361508184\n",
      "Num of epochs: 99\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2669770214913877\n",
      "Valence RMSE: 0.2546530935010733\n",
      "Arousal RMSE: 0.27875663575807375\n",
      "Test R^2 score: tensor([0.2141, 0.2306], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.0302, 0.0505], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.2223461502613217\n",
      "Num of epochs: 100\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.263288396353158\n",
      "Valence RMSE: 0.25099838269336405\n",
      "Arousal RMSE: 0.27502976419607905\n",
      "Test R^2 score: tensor([0.2365, 0.2510], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.0578, 0.0757], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.24376215581563115\n",
      "Num of epochs: 101\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2604203032872975\n",
      "Valence RMSE: 0.24752264935408874\n",
      "Arousal RMSE: 0.27270864816728657\n",
      "Test R^2 score: tensor([0.2575, 0.2636], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.0837, 0.0913], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.2605559284312337\n",
      "Num of epochs: 102\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.25804265314727687\n",
      "Valence RMSE: 0.2443423852315237\n",
      "Arousal RMSE: 0.2710513244128155\n",
      "Test R^2 score: tensor([0.2765, 0.2725], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1071, 0.1023], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.27449619248684387\n",
      "Num of epochs: 103\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.25431847307762295\n",
      "Valence RMSE: 0.24068121746607685\n",
      "Arousal RMSE: 0.2672607772497011\n",
      "Test R^2 score: tensor([0.2980, 0.2927], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1337, 0.1272], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.2953585392128623\n",
      "Num of epochs: 104\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.25439132124659325\n",
      "Valence RMSE: 0.23884107860008466\n",
      "Arousal RMSE: 0.26904428599084385\n",
      "Test R^2 score: tensor([0.3087, 0.2833], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1469, 0.1155], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.2959698630952234\n",
      "Num of epochs: 105\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.25054524150771645\n",
      "Valence RMSE: 0.23528472628692776\n",
      "Arousal RMSE: 0.2649281669819301\n",
      "Test R^2 score: tensor([0.3291, 0.3050], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1721, 0.1424], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.3170685590267825\n",
      "Num of epochs: 106\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.24916312295970433\n",
      "Valence RMSE: 0.23307532295145153\n",
      "Arousal RMSE: 0.2642733764818962\n",
      "Test R^2 score: tensor([0.3416, 0.3085], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1876, 0.1466], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.32505445087423096\n",
      "Num of epochs: 107\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2507278623301838\n",
      "Valence RMSE: 0.2328693506597984\n",
      "Arousal RMSE: 0.26739631153142523\n",
      "Test R^2 score: tensor([0.3428, 0.2920], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.1890, 0.1263], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.3174157653436968\n",
      "Num of epochs: 108\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2477039763708449\n",
      "Valence RMSE: 0.2304173426625731\n",
      "Arousal RMSE: 0.2638605086407864\n",
      "Test R^2 score: tensor([0.3566, 0.3106], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.2060, 0.1493], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.3335990041058359\n",
      "Num of epochs: 109\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.24725066699900286\n",
      "Valence RMSE: 0.22957040033094298\n",
      "Arousal RMSE: 0.26374839517008286\n",
      "Test R^2 score: tensor([0.3613, 0.3112], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.2118, 0.1500], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.33625253448841214\n",
      "Num of epochs: 110\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.24946985233664132\n",
      "Valence RMSE: 0.23048813751349978\n",
      "Arousal RMSE: 0.2671060330941427\n",
      "Test R^2 score: tensor([0.3562, 0.2936], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.2055, 0.1282], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.3248696754410694\n",
      "Num of epochs: 111\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.24665295594129358\n",
      "Valence RMSE: 0.22809897594466666\n",
      "Arousal RMSE: 0.263905700056194\n",
      "Test R^2 score: tensor([0.3695, 0.3104], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.2219, 0.1490], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.33992222484118495\n",
      "Num of epochs: 112\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.24691354996262735\n",
      "Valence RMSE: 0.22762207306421847\n",
      "Arousal RMSE: 0.2648033122229428\n",
      "Test R^2 score: tensor([0.3721, 0.3057], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.2251, 0.1432], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.33888961581462235\n",
      "Num of epochs: 113\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.24841549688038383\n",
      "Valence RMSE: 0.2278604042960576\n",
      "Arousal RMSE: 0.2673951277317766\n",
      "Test R^2 score: tensor([0.3708, 0.2920], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.2235, 0.1263], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.33140283119777064\n",
      "Num of epochs: 114\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.24515171539515868\n",
      "Valence RMSE: 0.22518246968481415\n",
      "Arousal RMSE: 0.26361256128839067\n",
      "Test R^2 score: tensor([0.3855, 0.3119], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.2417, 0.1509], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.34869844850931975\n",
      "Num of epochs: 115\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.24530968709157572\n",
      "Valence RMSE: 0.22455654560549063\n",
      "Arousal RMSE: 0.2644391101703052\n",
      "Test R^2 score: tensor([0.3889, 0.3076], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.2459, 0.1455], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.348243366007427\n",
      "Num of epochs: 116\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2456419181177757\n",
      "Valence RMSE: 0.22433656300318672\n",
      "Arousal RMSE: 0.26524141903759546\n",
      "Test R^2 score: tensor([0.3901, 0.3034], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.2473, 0.1403], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.34673778785248316\n",
      "Num of epochs: 117\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.24419720345590853\n",
      "Valence RMSE: 0.22313581924825146\n",
      "Arousal RMSE: 0.2635810207882662\n",
      "Test R^2 score: tensor([0.3966, 0.3121], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.2554, 0.1511], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.35434066820579446\n",
      "Num of epochs: 118\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.24284150014906014\n",
      "Valence RMSE: 0.2221674569690658\n",
      "Arousal RMSE: 0.26188854395179306\n",
      "Test R^2 score: tensor([0.4018, 0.3209], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.2618, 0.1619], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.36135661917735984\n",
      "Num of epochs: 119\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.24309573595202913\n",
      "Valence RMSE: 0.22235831429288044\n",
      "Arousal RMSE: 0.2621981192551654\n",
      "Test R^2 score: tensor([0.4008, 0.3193], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.2606, 0.1600], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.3600392797952552\n",
      "Num of epochs: 120\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.24315111970260753\n",
      "Valence RMSE: 0.22251661543505613\n",
      "Arousal RMSE: 0.2621665308169426\n",
      "Test R^2 score: tensor([0.3999, 0.3194], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.2595, 0.1602], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.35969454946100915\n",
      "Num of epochs: 121\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Epoch 121, Loss: 0.49295912913079615\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.24253801228547678\n",
      "Valence RMSE: 0.22226238098633697\n",
      "Arousal RMSE: 0.26124472971729895\n",
      "Test R^2 score: tensor([0.4013, 0.3242], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.2612, 0.1661], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.36276843287330124\n",
      "Num of epochs: 122\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Epoch 121, Loss: 0.49295912913079615\n",
      "Epoch 122, Loss: 0.4924773619139838\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.24139811428568875\n",
      "Valence RMSE: 0.22165283671454528\n",
      "Arousal RMSE: 0.2596461421584539\n",
      "Test R^2 score: tensor([0.4046, 0.3325], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.2652, 0.1762], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.36853056141987234\n",
      "Num of epochs: 123\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Epoch 121, Loss: 0.49295912913079615\n",
      "Epoch 122, Loss: 0.4924773619139838\n",
      "Epoch 123, Loss: 0.49197607191949805\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.24071673589676382\n",
      "Valence RMSE: 0.2214057507289813\n",
      "Arousal RMSE: 0.25858961198339137\n",
      "Test R^2 score: tensor([0.4059, 0.3379], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.2669, 0.1829], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.3719046582120907\n",
      "Num of epochs: 124\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Epoch 121, Loss: 0.49295912913079615\n",
      "Epoch 122, Loss: 0.4924773619139838\n",
      "Epoch 123, Loss: 0.49197607191949805\n",
      "Epoch 124, Loss: 0.4914699197831528\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.24068461063503865\n",
      "Valence RMSE: 0.22165598684749926\n",
      "Arousal RMSE: 0.2583152862061043\n",
      "Test R^2 score: tensor([0.4046, 0.3393], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.2652, 0.1847], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.37193487215956\n",
      "Num of epochs: 125\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Epoch 121, Loss: 0.49295912913079615\n",
      "Epoch 122, Loss: 0.4924773619139838\n",
      "Epoch 123, Loss: 0.49197607191949805\n",
      "Epoch 124, Loss: 0.4914699197831528\n",
      "Epoch 125, Loss: 0.49091483381312667\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.24039416664285826\n",
      "Valence RMSE: 0.22168173663281113\n",
      "Arousal RMSE: 0.2577516602376978\n",
      "Test R^2 score: tensor([0.4044, 0.3422], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.2651, 0.1882], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.3733057424005497\n",
      "Num of epochs: 126\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Epoch 121, Loss: 0.49295912913079615\n",
      "Epoch 122, Loss: 0.4924773619139838\n",
      "Epoch 123, Loss: 0.49197607191949805\n",
      "Epoch 124, Loss: 0.4914699197831528\n",
      "Epoch 125, Loss: 0.49091483381312667\n",
      "Epoch 126, Loss: 0.49035147677946317\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23929420393292009\n",
      "Valence RMSE: 0.22111665278364118\n",
      "Arousal RMSE: 0.25618520240938697\n",
      "Test R^2 score: tensor([0.4075, 0.3501], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.2688, 0.1981], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.3788076645717687\n",
      "Num of epochs: 127\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Epoch 121, Loss: 0.49295912913079615\n",
      "Epoch 122, Loss: 0.4924773619139838\n",
      "Epoch 123, Loss: 0.49197607191949805\n",
      "Epoch 124, Loss: 0.4914699197831528\n",
      "Epoch 125, Loss: 0.49091483381312667\n",
      "Epoch 126, Loss: 0.49035147677946317\n",
      "Epoch 127, Loss: 0.4898087222825037\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2395351571838326\n",
      "Valence RMSE: 0.2215478824411556\n",
      "Arousal RMSE: 0.25626298765137084\n",
      "Test R^2 score: tensor([0.4052, 0.3497], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.2659, 0.1976], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.37745362156441825\n",
      "Num of epochs: 128\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Epoch 121, Loss: 0.49295912913079615\n",
      "Epoch 122, Loss: 0.4924773619139838\n",
      "Epoch 123, Loss: 0.49197607191949805\n",
      "Epoch 124, Loss: 0.4914699197831528\n",
      "Epoch 125, Loss: 0.49091483381312667\n",
      "Epoch 126, Loss: 0.49035147677946317\n",
      "Epoch 127, Loss: 0.4898087222825037\n",
      "Epoch 128, Loss: 0.48924014731550836\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23911445564340938\n",
      "Valence RMSE: 0.22154623241557775\n",
      "Arousal RMSE: 0.2554774211114376\n",
      "Test R^2 score: tensor([0.4052, 0.3537], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.2660, 0.2025], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.3794483198717922\n",
      "Num of epochs: 129\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Epoch 121, Loss: 0.49295912913079615\n",
      "Epoch 122, Loss: 0.4924773619139838\n",
      "Epoch 123, Loss: 0.49197607191949805\n",
      "Epoch 124, Loss: 0.4914699197831528\n",
      "Epoch 125, Loss: 0.49091483381312667\n",
      "Epoch 126, Loss: 0.49035147677946317\n",
      "Epoch 127, Loss: 0.4898087222825037\n",
      "Epoch 128, Loss: 0.48924014731550836\n",
      "Epoch 129, Loss: 0.48873550647212044\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23803116392883428\n",
      "Valence RMSE: 0.2209736744955574\n",
      "Arousal RMSE: 0.25394547679091\n",
      "Test R^2 score: tensor([0.4082, 0.3615], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.2697, 0.2120], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.38484727722324086\n",
      "Num of epochs: 130\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Epoch 121, Loss: 0.49295912913079615\n",
      "Epoch 122, Loss: 0.4924773619139838\n",
      "Epoch 123, Loss: 0.49197607191949805\n",
      "Epoch 124, Loss: 0.4914699197831528\n",
      "Epoch 125, Loss: 0.49091483381312667\n",
      "Epoch 126, Loss: 0.49035147677946317\n",
      "Epoch 127, Loss: 0.4898087222825037\n",
      "Epoch 128, Loss: 0.48924014731550836\n",
      "Epoch 129, Loss: 0.48873550647212044\n",
      "Epoch 130, Loss: 0.48832307254971563\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23831048344855027\n",
      "Valence RMSE: 0.22143436654728602\n",
      "Arousal RMSE: 0.25406808999708264\n",
      "Test R^2 score: tensor([0.4058, 0.3608], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.2667, 0.2113], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.3833038837665421\n",
      "Num of epochs: 131\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Epoch 121, Loss: 0.49295912913079615\n",
      "Epoch 122, Loss: 0.4924773619139838\n",
      "Epoch 123, Loss: 0.49197607191949805\n",
      "Epoch 124, Loss: 0.4914699197831528\n",
      "Epoch 125, Loss: 0.49091483381312667\n",
      "Epoch 126, Loss: 0.49035147677946317\n",
      "Epoch 127, Loss: 0.4898087222825037\n",
      "Epoch 128, Loss: 0.48924014731550836\n",
      "Epoch 129, Loss: 0.48873550647212044\n",
      "Epoch 130, Loss: 0.48832307254971563\n",
      "Epoch 131, Loss: 0.48791418392821295\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23816241487557074\n",
      "Valence RMSE: 0.22155964606777645\n",
      "Arousal RMSE: 0.2536808919746392\n",
      "Test R^2 score: tensor([0.4051, 0.3628], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.2659, 0.2137], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.383940925467392\n",
      "Num of epochs: 132\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Epoch 121, Loss: 0.49295912913079615\n",
      "Epoch 122, Loss: 0.4924773619139838\n",
      "Epoch 123, Loss: 0.49197607191949805\n",
      "Epoch 124, Loss: 0.4914699197831528\n",
      "Epoch 125, Loss: 0.49091483381312667\n",
      "Epoch 126, Loss: 0.49035147677946317\n",
      "Epoch 127, Loss: 0.4898087222825037\n",
      "Epoch 128, Loss: 0.48924014731550836\n",
      "Epoch 129, Loss: 0.48873550647212044\n",
      "Epoch 130, Loss: 0.48832307254971563\n",
      "Epoch 131, Loss: 0.48791418392821295\n",
      "Epoch 132, Loss: 0.4875902245243796\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2368756244094923\n",
      "Valence RMSE: 0.22073846251236134\n",
      "Arousal RMSE: 0.25198145575905495\n",
      "Test R^2 score: tensor([0.4095, 0.3713], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.2713, 0.2242], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.39039624259034517\n",
      "Num of epochs: 133\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Epoch 121, Loss: 0.49295912913079615\n",
      "Epoch 122, Loss: 0.4924773619139838\n",
      "Epoch 123, Loss: 0.49197607191949805\n",
      "Epoch 124, Loss: 0.4914699197831528\n",
      "Epoch 125, Loss: 0.49091483381312667\n",
      "Epoch 126, Loss: 0.49035147677946317\n",
      "Epoch 127, Loss: 0.4898087222825037\n",
      "Epoch 128, Loss: 0.48924014731550836\n",
      "Epoch 129, Loss: 0.48873550647212044\n",
      "Epoch 130, Loss: 0.48832307254971563\n",
      "Epoch 131, Loss: 0.48791418392821295\n",
      "Epoch 132, Loss: 0.4875902245243796\n",
      "Epoch 133, Loss: 0.48728851108037585\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23758000159956577\n",
      "Valence RMSE: 0.22153223191308177\n",
      "Arousal RMSE: 0.2526103413237627\n",
      "Test R^2 score: tensor([0.4052, 0.3682], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.2660, 0.2203], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.3866979369645042\n",
      "Num of epochs: 134\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Epoch 121, Loss: 0.49295912913079615\n",
      "Epoch 122, Loss: 0.4924773619139838\n",
      "Epoch 123, Loss: 0.49197607191949805\n",
      "Epoch 124, Loss: 0.4914699197831528\n",
      "Epoch 125, Loss: 0.49091483381312667\n",
      "Epoch 126, Loss: 0.49035147677946317\n",
      "Epoch 127, Loss: 0.4898087222825037\n",
      "Epoch 128, Loss: 0.48924014731550836\n",
      "Epoch 129, Loss: 0.48873550647212044\n",
      "Epoch 130, Loss: 0.48832307254971563\n",
      "Epoch 131, Loss: 0.48791418392821295\n",
      "Epoch 132, Loss: 0.4875902245243796\n",
      "Epoch 133, Loss: 0.48728851108037585\n",
      "Epoch 134, Loss: 0.48697062262181345\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23641892257415636\n",
      "Valence RMSE: 0.22064853753292393\n",
      "Arousal RMSE: 0.2512011878690699\n",
      "Test R^2 score: tensor([0.4100, 0.3752], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.2719, 0.2290], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.39258053972149204\n",
      "Num of epochs: 135\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Epoch 121, Loss: 0.49295912913079615\n",
      "Epoch 122, Loss: 0.4924773619139838\n",
      "Epoch 123, Loss: 0.49197607191949805\n",
      "Epoch 124, Loss: 0.4914699197831528\n",
      "Epoch 125, Loss: 0.49091483381312667\n",
      "Epoch 126, Loss: 0.49035147677946317\n",
      "Epoch 127, Loss: 0.4898087222825037\n",
      "Epoch 128, Loss: 0.48924014731550836\n",
      "Epoch 129, Loss: 0.48873550647212044\n",
      "Epoch 130, Loss: 0.48832307254971563\n",
      "Epoch 131, Loss: 0.48791418392821295\n",
      "Epoch 132, Loss: 0.4875902245243796\n",
      "Epoch 133, Loss: 0.48728851108037585\n",
      "Epoch 134, Loss: 0.48697062262181345\n",
      "Epoch 135, Loss: 0.4866339553030548\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23588033781515771\n",
      "Valence RMSE: 0.2202748940007111\n",
      "Arousal RMSE: 0.25051554564250367\n",
      "Test R^2 score: tensor([0.4120, 0.3786], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.2744, 0.2332], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.3952819109979735\n",
      "Num of epochs: 136\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Epoch 121, Loss: 0.49295912913079615\n",
      "Epoch 122, Loss: 0.4924773619139838\n",
      "Epoch 123, Loss: 0.49197607191949805\n",
      "Epoch 124, Loss: 0.4914699197831528\n",
      "Epoch 125, Loss: 0.49091483381312667\n",
      "Epoch 126, Loss: 0.49035147677946317\n",
      "Epoch 127, Loss: 0.4898087222825037\n",
      "Epoch 128, Loss: 0.48924014731550836\n",
      "Epoch 129, Loss: 0.48873550647212044\n",
      "Epoch 130, Loss: 0.48832307254971563\n",
      "Epoch 131, Loss: 0.48791418392821295\n",
      "Epoch 132, Loss: 0.4875902245243796\n",
      "Epoch 133, Loss: 0.48728851108037585\n",
      "Epoch 134, Loss: 0.48697062262181345\n",
      "Epoch 135, Loss: 0.4866339553030548\n",
      "Epoch 136, Loss: 0.48633446746734893\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23632101491475835\n",
      "Valence RMSE: 0.22074576517508723\n",
      "Arousal RMSE: 0.2509313677839981\n",
      "Test R^2 score: tensor([0.4095, 0.3765], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.2712, 0.2306], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.3929912587431025\n",
      "Num of epochs: 137\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Epoch 121, Loss: 0.49295912913079615\n",
      "Epoch 122, Loss: 0.4924773619139838\n",
      "Epoch 123, Loss: 0.49197607191949805\n",
      "Epoch 124, Loss: 0.4914699197831528\n",
      "Epoch 125, Loss: 0.49091483381312667\n",
      "Epoch 126, Loss: 0.49035147677946317\n",
      "Epoch 127, Loss: 0.4898087222825037\n",
      "Epoch 128, Loss: 0.48924014731550836\n",
      "Epoch 129, Loss: 0.48873550647212044\n",
      "Epoch 130, Loss: 0.48832307254971563\n",
      "Epoch 131, Loss: 0.48791418392821295\n",
      "Epoch 132, Loss: 0.4875902245243796\n",
      "Epoch 133, Loss: 0.48728851108037585\n",
      "Epoch 134, Loss: 0.48697062262181345\n",
      "Epoch 135, Loss: 0.4866339553030548\n",
      "Epoch 136, Loss: 0.48633446746734893\n",
      "Epoch 137, Loss: 0.4860310087358514\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2350235098772303\n",
      "Valence RMSE: 0.21958559946410308\n",
      "Arousal RMSE: 0.24950804575807903\n",
      "Test R^2 score: tensor([0.4156, 0.3836], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.2789, 0.2393], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.3996132140744866\n",
      "Num of epochs: 138\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Epoch 121, Loss: 0.49295912913079615\n",
      "Epoch 122, Loss: 0.4924773619139838\n",
      "Epoch 123, Loss: 0.49197607191949805\n",
      "Epoch 124, Loss: 0.4914699197831528\n",
      "Epoch 125, Loss: 0.49091483381312667\n",
      "Epoch 126, Loss: 0.49035147677946317\n",
      "Epoch 127, Loss: 0.4898087222825037\n",
      "Epoch 128, Loss: 0.48924014731550836\n",
      "Epoch 129, Loss: 0.48873550647212044\n",
      "Epoch 130, Loss: 0.48832307254971563\n",
      "Epoch 131, Loss: 0.48791418392821295\n",
      "Epoch 132, Loss: 0.4875902245243796\n",
      "Epoch 133, Loss: 0.48728851108037585\n",
      "Epoch 134, Loss: 0.48697062262181345\n",
      "Epoch 135, Loss: 0.4866339553030548\n",
      "Epoch 136, Loss: 0.48633446746734893\n",
      "Epoch 137, Loss: 0.4860310087358514\n",
      "Epoch 138, Loss: 0.4856786412545615\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23544454514438126\n",
      "Valence RMSE: 0.21994115761210625\n",
      "Arousal RMSE: 0.249988309456134\n",
      "Test R^2 score: tensor([0.4138, 0.3812], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.2765, 0.2364], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.3974785951125574\n",
      "Num of epochs: 139\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Epoch 121, Loss: 0.49295912913079615\n",
      "Epoch 122, Loss: 0.4924773619139838\n",
      "Epoch 123, Loss: 0.49197607191949805\n",
      "Epoch 124, Loss: 0.4914699197831528\n",
      "Epoch 125, Loss: 0.49091483381312667\n",
      "Epoch 126, Loss: 0.49035147677946317\n",
      "Epoch 127, Loss: 0.4898087222825037\n",
      "Epoch 128, Loss: 0.48924014731550836\n",
      "Epoch 129, Loss: 0.48873550647212044\n",
      "Epoch 130, Loss: 0.48832307254971563\n",
      "Epoch 131, Loss: 0.48791418392821295\n",
      "Epoch 132, Loss: 0.4875902245243796\n",
      "Epoch 133, Loss: 0.48728851108037585\n",
      "Epoch 134, Loss: 0.48697062262181345\n",
      "Epoch 135, Loss: 0.4866339553030548\n",
      "Epoch 136, Loss: 0.48633446746734893\n",
      "Epoch 137, Loss: 0.4860310087358514\n",
      "Epoch 138, Loss: 0.4856786412545615\n",
      "Epoch 139, Loss: 0.48531372107004\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23452508979536213\n",
      "Valence RMSE: 0.21908406186405208\n",
      "Arousal RMSE: 0.2490104602706351\n",
      "Test R^2 score: tensor([0.4183, 0.3860], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.2822, 0.2423], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4021744436353346\n",
      "Num of epochs: 140\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Epoch 121, Loss: 0.49295912913079615\n",
      "Epoch 122, Loss: 0.4924773619139838\n",
      "Epoch 123, Loss: 0.49197607191949805\n",
      "Epoch 124, Loss: 0.4914699197831528\n",
      "Epoch 125, Loss: 0.49091483381312667\n",
      "Epoch 126, Loss: 0.49035147677946317\n",
      "Epoch 127, Loss: 0.4898087222825037\n",
      "Epoch 128, Loss: 0.48924014731550836\n",
      "Epoch 129, Loss: 0.48873550647212044\n",
      "Epoch 130, Loss: 0.48832307254971563\n",
      "Epoch 131, Loss: 0.48791418392821295\n",
      "Epoch 132, Loss: 0.4875902245243796\n",
      "Epoch 133, Loss: 0.48728851108037585\n",
      "Epoch 134, Loss: 0.48697062262181345\n",
      "Epoch 135, Loss: 0.4866339553030548\n",
      "Epoch 136, Loss: 0.48633446746734893\n",
      "Epoch 137, Loss: 0.4860310087358514\n",
      "Epoch 138, Loss: 0.4856786412545615\n",
      "Epoch 139, Loss: 0.48531372107004\n",
      "Epoch 140, Loss: 0.48495712985488587\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23435170989152443\n",
      "Valence RMSE: 0.2188500732973877\n",
      "Arousal RMSE: 0.24888972111336882\n",
      "Test R^2 score: tensor([0.4196, 0.3866], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.2837, 0.2431], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.40309299491336603\n",
      "Num of epochs: 141\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Epoch 121, Loss: 0.49295912913079615\n",
      "Epoch 122, Loss: 0.4924773619139838\n",
      "Epoch 123, Loss: 0.49197607191949805\n",
      "Epoch 124, Loss: 0.4914699197831528\n",
      "Epoch 125, Loss: 0.49091483381312667\n",
      "Epoch 126, Loss: 0.49035147677946317\n",
      "Epoch 127, Loss: 0.4898087222825037\n",
      "Epoch 128, Loss: 0.48924014731550836\n",
      "Epoch 129, Loss: 0.48873550647212044\n",
      "Epoch 130, Loss: 0.48832307254971563\n",
      "Epoch 131, Loss: 0.48791418392821295\n",
      "Epoch 132, Loss: 0.4875902245243796\n",
      "Epoch 133, Loss: 0.48728851108037585\n",
      "Epoch 134, Loss: 0.48697062262181345\n",
      "Epoch 135, Loss: 0.4866339553030548\n",
      "Epoch 136, Loss: 0.48633446746734893\n",
      "Epoch 137, Loss: 0.4860310087358514\n",
      "Epoch 138, Loss: 0.4856786412545615\n",
      "Epoch 139, Loss: 0.48531372107004\n",
      "Epoch 140, Loss: 0.48495712985488587\n",
      "Epoch 141, Loss: 0.4846131600652309\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23381483466034209\n",
      "Valence RMSE: 0.21829160263832673\n",
      "Arousal RMSE: 0.24836974459880795\n",
      "Test R^2 score: tensor([0.4225, 0.3892], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.2874, 0.2462], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.40585241027135366\n",
      "Num of epochs: 142\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Epoch 121, Loss: 0.49295912913079615\n",
      "Epoch 122, Loss: 0.4924773619139838\n",
      "Epoch 123, Loss: 0.49197607191949805\n",
      "Epoch 124, Loss: 0.4914699197831528\n",
      "Epoch 125, Loss: 0.49091483381312667\n",
      "Epoch 126, Loss: 0.49035147677946317\n",
      "Epoch 127, Loss: 0.4898087222825037\n",
      "Epoch 128, Loss: 0.48924014731550836\n",
      "Epoch 129, Loss: 0.48873550647212044\n",
      "Epoch 130, Loss: 0.48832307254971563\n",
      "Epoch 131, Loss: 0.48791418392821295\n",
      "Epoch 132, Loss: 0.4875902245243796\n",
      "Epoch 133, Loss: 0.48728851108037585\n",
      "Epoch 134, Loss: 0.48697062262181345\n",
      "Epoch 135, Loss: 0.4866339553030548\n",
      "Epoch 136, Loss: 0.48633446746734893\n",
      "Epoch 137, Loss: 0.4860310087358514\n",
      "Epoch 138, Loss: 0.4856786412545615\n",
      "Epoch 139, Loss: 0.48531372107004\n",
      "Epoch 140, Loss: 0.48495712985488587\n",
      "Epoch 141, Loss: 0.4846131600652309\n",
      "Epoch 142, Loss: 0.48425966858652364\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23328853761696897\n",
      "Valence RMSE: 0.21774146099325256\n",
      "Arousal RMSE: 0.24786234028478743\n",
      "Test R^2 score: tensor([0.4254, 0.3917], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.2909, 0.2493], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4085525358361056\n",
      "Num of epochs: 143\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Epoch 121, Loss: 0.49295912913079615\n",
      "Epoch 122, Loss: 0.4924773619139838\n",
      "Epoch 123, Loss: 0.49197607191949805\n",
      "Epoch 124, Loss: 0.4914699197831528\n",
      "Epoch 125, Loss: 0.49091483381312667\n",
      "Epoch 126, Loss: 0.49035147677946317\n",
      "Epoch 127, Loss: 0.4898087222825037\n",
      "Epoch 128, Loss: 0.48924014731550836\n",
      "Epoch 129, Loss: 0.48873550647212044\n",
      "Epoch 130, Loss: 0.48832307254971563\n",
      "Epoch 131, Loss: 0.48791418392821295\n",
      "Epoch 132, Loss: 0.4875902245243796\n",
      "Epoch 133, Loss: 0.48728851108037585\n",
      "Epoch 134, Loss: 0.48697062262181345\n",
      "Epoch 135, Loss: 0.4866339553030548\n",
      "Epoch 136, Loss: 0.48633446746734893\n",
      "Epoch 137, Loss: 0.4860310087358514\n",
      "Epoch 138, Loss: 0.4856786412545615\n",
      "Epoch 139, Loss: 0.48531372107004\n",
      "Epoch 140, Loss: 0.48495712985488587\n",
      "Epoch 141, Loss: 0.4846131600652309\n",
      "Epoch 142, Loss: 0.48425966858652364\n",
      "Epoch 143, Loss: 0.4838914149236435\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23322319777572073\n",
      "Valence RMSE: 0.2176034256716672\n",
      "Arousal RMSE: 0.2478605839931414\n",
      "Test R^2 score: tensor([0.4262, 0.3917], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.2918, 0.2493], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4089209794467948\n",
      "Num of epochs: 144\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Epoch 121, Loss: 0.49295912913079615\n",
      "Epoch 122, Loss: 0.4924773619139838\n",
      "Epoch 123, Loss: 0.49197607191949805\n",
      "Epoch 124, Loss: 0.4914699197831528\n",
      "Epoch 125, Loss: 0.49091483381312667\n",
      "Epoch 126, Loss: 0.49035147677946317\n",
      "Epoch 127, Loss: 0.4898087222825037\n",
      "Epoch 128, Loss: 0.48924014731550836\n",
      "Epoch 129, Loss: 0.48873550647212044\n",
      "Epoch 130, Loss: 0.48832307254971563\n",
      "Epoch 131, Loss: 0.48791418392821295\n",
      "Epoch 132, Loss: 0.4875902245243796\n",
      "Epoch 133, Loss: 0.48728851108037585\n",
      "Epoch 134, Loss: 0.48697062262181345\n",
      "Epoch 135, Loss: 0.4866339553030548\n",
      "Epoch 136, Loss: 0.48633446746734893\n",
      "Epoch 137, Loss: 0.4860310087358514\n",
      "Epoch 138, Loss: 0.4856786412545615\n",
      "Epoch 139, Loss: 0.48531372107004\n",
      "Epoch 140, Loss: 0.48495712985488587\n",
      "Epoch 141, Loss: 0.4846131600652309\n",
      "Epoch 142, Loss: 0.48425966858652364\n",
      "Epoch 143, Loss: 0.4838914149236435\n",
      "Epoch 144, Loss: 0.48352437546282767\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23207902360283236\n",
      "Valence RMSE: 0.21653079896719016\n",
      "Arousal RMSE: 0.24664906140408951\n",
      "Test R^2 score: tensor([0.4318, 0.3976], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.2988, 0.2566], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.41470876513103033\n",
      "Num of epochs: 145\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Epoch 121, Loss: 0.49295912913079615\n",
      "Epoch 122, Loss: 0.4924773619139838\n",
      "Epoch 123, Loss: 0.49197607191949805\n",
      "Epoch 124, Loss: 0.4914699197831528\n",
      "Epoch 125, Loss: 0.49091483381312667\n",
      "Epoch 126, Loss: 0.49035147677946317\n",
      "Epoch 127, Loss: 0.4898087222825037\n",
      "Epoch 128, Loss: 0.48924014731550836\n",
      "Epoch 129, Loss: 0.48873550647212044\n",
      "Epoch 130, Loss: 0.48832307254971563\n",
      "Epoch 131, Loss: 0.48791418392821295\n",
      "Epoch 132, Loss: 0.4875902245243796\n",
      "Epoch 133, Loss: 0.48728851108037585\n",
      "Epoch 134, Loss: 0.48697062262181345\n",
      "Epoch 135, Loss: 0.4866339553030548\n",
      "Epoch 136, Loss: 0.48633446746734893\n",
      "Epoch 137, Loss: 0.4860310087358514\n",
      "Epoch 138, Loss: 0.4856786412545615\n",
      "Epoch 139, Loss: 0.48531372107004\n",
      "Epoch 140, Loss: 0.48495712985488587\n",
      "Epoch 141, Loss: 0.4846131600652309\n",
      "Epoch 142, Loss: 0.48425966858652364\n",
      "Epoch 143, Loss: 0.4838914149236435\n",
      "Epoch 144, Loss: 0.48352437546282767\n",
      "Epoch 145, Loss: 0.4832299911937133\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23411897007313368\n",
      "Valence RMSE: 0.21814949890073282\n",
      "Arousal RMSE: 0.24906661844889788\n",
      "Test R^2 score: tensor([0.4233, 0.3858], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.2883, 0.2420], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.40451200915344065\n",
      "Num of epochs: 146\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Epoch 121, Loss: 0.49295912913079615\n",
      "Epoch 122, Loss: 0.4924773619139838\n",
      "Epoch 123, Loss: 0.49197607191949805\n",
      "Epoch 124, Loss: 0.4914699197831528\n",
      "Epoch 125, Loss: 0.49091483381312667\n",
      "Epoch 126, Loss: 0.49035147677946317\n",
      "Epoch 127, Loss: 0.4898087222825037\n",
      "Epoch 128, Loss: 0.48924014731550836\n",
      "Epoch 129, Loss: 0.48873550647212044\n",
      "Epoch 130, Loss: 0.48832307254971563\n",
      "Epoch 131, Loss: 0.48791418392821295\n",
      "Epoch 132, Loss: 0.4875902245243796\n",
      "Epoch 133, Loss: 0.48728851108037585\n",
      "Epoch 134, Loss: 0.48697062262181345\n",
      "Epoch 135, Loss: 0.4866339553030548\n",
      "Epoch 136, Loss: 0.48633446746734893\n",
      "Epoch 137, Loss: 0.4860310087358514\n",
      "Epoch 138, Loss: 0.4856786412545615\n",
      "Epoch 139, Loss: 0.48531372107004\n",
      "Epoch 140, Loss: 0.48495712985488587\n",
      "Epoch 141, Loss: 0.4846131600652309\n",
      "Epoch 142, Loss: 0.48425966858652364\n",
      "Epoch 143, Loss: 0.4838914149236435\n",
      "Epoch 144, Loss: 0.48352437546282767\n",
      "Epoch 145, Loss: 0.4832299911937133\n",
      "Epoch 146, Loss: 0.4832586529465723\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2304401571136632\n",
      "Valence RMSE: 0.21514223334761554\n",
      "Arousal RMSE: 0.24478388723798733\n",
      "Test R^2 score: tensor([0.4391, 0.4067], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.3078, 0.2678], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.42287884639639794\n",
      "Num of epochs: 147\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Epoch 121, Loss: 0.49295912913079615\n",
      "Epoch 122, Loss: 0.4924773619139838\n",
      "Epoch 123, Loss: 0.49197607191949805\n",
      "Epoch 124, Loss: 0.4914699197831528\n",
      "Epoch 125, Loss: 0.49091483381312667\n",
      "Epoch 126, Loss: 0.49035147677946317\n",
      "Epoch 127, Loss: 0.4898087222825037\n",
      "Epoch 128, Loss: 0.48924014731550836\n",
      "Epoch 129, Loss: 0.48873550647212044\n",
      "Epoch 130, Loss: 0.48832307254971563\n",
      "Epoch 131, Loss: 0.48791418392821295\n",
      "Epoch 132, Loss: 0.4875902245243796\n",
      "Epoch 133, Loss: 0.48728851108037585\n",
      "Epoch 134, Loss: 0.48697062262181345\n",
      "Epoch 135, Loss: 0.4866339553030548\n",
      "Epoch 136, Loss: 0.48633446746734893\n",
      "Epoch 137, Loss: 0.4860310087358514\n",
      "Epoch 138, Loss: 0.4856786412545615\n",
      "Epoch 139, Loss: 0.48531372107004\n",
      "Epoch 140, Loss: 0.48495712985488587\n",
      "Epoch 141, Loss: 0.4846131600652309\n",
      "Epoch 142, Loss: 0.48425966858652364\n",
      "Epoch 143, Loss: 0.4838914149236435\n",
      "Epoch 144, Loss: 0.48352437546282767\n",
      "Epoch 145, Loss: 0.4832299911937133\n",
      "Epoch 146, Loss: 0.4832586529465723\n",
      "Epoch 147, Loss: 0.48356744143113517\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23387362013022742\n",
      "Valence RMSE: 0.21785173767697463\n",
      "Arousal RMSE: 0.2488661503233384\n",
      "Test R^2 score: tensor([0.4248, 0.3867], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.2902, 0.2432], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4057928701240465\n",
      "Num of epochs: 148\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Epoch 121, Loss: 0.49295912913079615\n",
      "Epoch 122, Loss: 0.4924773619139838\n",
      "Epoch 123, Loss: 0.49197607191949805\n",
      "Epoch 124, Loss: 0.4914699197831528\n",
      "Epoch 125, Loss: 0.49091483381312667\n",
      "Epoch 126, Loss: 0.49035147677946317\n",
      "Epoch 127, Loss: 0.4898087222825037\n",
      "Epoch 128, Loss: 0.48924014731550836\n",
      "Epoch 129, Loss: 0.48873550647212044\n",
      "Epoch 130, Loss: 0.48832307254971563\n",
      "Epoch 131, Loss: 0.48791418392821295\n",
      "Epoch 132, Loss: 0.4875902245243796\n",
      "Epoch 133, Loss: 0.48728851108037585\n",
      "Epoch 134, Loss: 0.48697062262181345\n",
      "Epoch 135, Loss: 0.4866339553030548\n",
      "Epoch 136, Loss: 0.48633446746734893\n",
      "Epoch 137, Loss: 0.4860310087358514\n",
      "Epoch 138, Loss: 0.4856786412545615\n",
      "Epoch 139, Loss: 0.48531372107004\n",
      "Epoch 140, Loss: 0.48495712985488587\n",
      "Epoch 141, Loss: 0.4846131600652309\n",
      "Epoch 142, Loss: 0.48425966858652364\n",
      "Epoch 143, Loss: 0.4838914149236435\n",
      "Epoch 144, Loss: 0.48352437546282767\n",
      "Epoch 145, Loss: 0.4832299911937133\n",
      "Epoch 146, Loss: 0.4832586529465723\n",
      "Epoch 147, Loss: 0.48356744143113517\n",
      "Epoch 148, Loss: 0.4828108019011922\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2317091916931353\n",
      "Valence RMSE: 0.21605810980987744\n",
      "Arousal RMSE: 0.24636800160644887\n",
      "Test R^2 score: tensor([0.4343, 0.3990], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.3019, 0.2583], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.416633836228849\n",
      "Num of epochs: 149\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Epoch 121, Loss: 0.49295912913079615\n",
      "Epoch 122, Loss: 0.4924773619139838\n",
      "Epoch 123, Loss: 0.49197607191949805\n",
      "Epoch 124, Loss: 0.4914699197831528\n",
      "Epoch 125, Loss: 0.49091483381312667\n",
      "Epoch 126, Loss: 0.49035147677946317\n",
      "Epoch 127, Loss: 0.4898087222825037\n",
      "Epoch 128, Loss: 0.48924014731550836\n",
      "Epoch 129, Loss: 0.48873550647212044\n",
      "Epoch 130, Loss: 0.48832307254971563\n",
      "Epoch 131, Loss: 0.48791418392821295\n",
      "Epoch 132, Loss: 0.4875902245243796\n",
      "Epoch 133, Loss: 0.48728851108037585\n",
      "Epoch 134, Loss: 0.48697062262181345\n",
      "Epoch 135, Loss: 0.4866339553030548\n",
      "Epoch 136, Loss: 0.48633446746734893\n",
      "Epoch 137, Loss: 0.4860310087358514\n",
      "Epoch 138, Loss: 0.4856786412545615\n",
      "Epoch 139, Loss: 0.48531372107004\n",
      "Epoch 140, Loss: 0.48495712985488587\n",
      "Epoch 141, Loss: 0.4846131600652309\n",
      "Epoch 142, Loss: 0.48425966858652364\n",
      "Epoch 143, Loss: 0.4838914149236435\n",
      "Epoch 144, Loss: 0.48352437546282767\n",
      "Epoch 145, Loss: 0.4832299911937133\n",
      "Epoch 146, Loss: 0.4832586529465723\n",
      "Epoch 147, Loss: 0.48356744143113517\n",
      "Epoch 148, Loss: 0.4828108019011922\n",
      "Epoch 149, Loss: 0.4818912939738211\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.22949096744933128\n",
      "Valence RMSE: 0.21434548693755665\n",
      "Arousal RMSE: 0.24369698502681927\n",
      "Test R^2 score: tensor([0.4432, 0.4120], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.3129, 0.2743], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.42758091714322727\n",
      "Num of epochs: 150\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Epoch 121, Loss: 0.49295912913079615\n",
      "Epoch 122, Loss: 0.4924773619139838\n",
      "Epoch 123, Loss: 0.49197607191949805\n",
      "Epoch 124, Loss: 0.4914699197831528\n",
      "Epoch 125, Loss: 0.49091483381312667\n",
      "Epoch 126, Loss: 0.49035147677946317\n",
      "Epoch 127, Loss: 0.4898087222825037\n",
      "Epoch 128, Loss: 0.48924014731550836\n",
      "Epoch 129, Loss: 0.48873550647212044\n",
      "Epoch 130, Loss: 0.48832307254971563\n",
      "Epoch 131, Loss: 0.48791418392821295\n",
      "Epoch 132, Loss: 0.4875902245243796\n",
      "Epoch 133, Loss: 0.48728851108037585\n",
      "Epoch 134, Loss: 0.48697062262181345\n",
      "Epoch 135, Loss: 0.4866339553030548\n",
      "Epoch 136, Loss: 0.48633446746734893\n",
      "Epoch 137, Loss: 0.4860310087358514\n",
      "Epoch 138, Loss: 0.4856786412545615\n",
      "Epoch 139, Loss: 0.48531372107004\n",
      "Epoch 140, Loss: 0.48495712985488587\n",
      "Epoch 141, Loss: 0.4846131600652309\n",
      "Epoch 142, Loss: 0.48425966858652364\n",
      "Epoch 143, Loss: 0.4838914149236435\n",
      "Epoch 144, Loss: 0.48352437546282767\n",
      "Epoch 145, Loss: 0.4832299911937133\n",
      "Epoch 146, Loss: 0.4832586529465723\n",
      "Epoch 147, Loss: 0.48356744143113517\n",
      "Epoch 148, Loss: 0.4828108019011922\n",
      "Epoch 149, Loss: 0.4818912939738211\n",
      "Epoch 150, Loss: 0.48239416745575797\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2318185342825712\n",
      "Valence RMSE: 0.21623929937628417\n",
      "Arousal RMSE: 0.24641475418305017\n",
      "Test R^2 score: tensor([0.4333, 0.3988], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.3007, 0.2581], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.41604514747274\n",
      "Num of epochs: 151\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Epoch 121, Loss: 0.49295912913079615\n",
      "Epoch 122, Loss: 0.4924773619139838\n",
      "Epoch 123, Loss: 0.49197607191949805\n",
      "Epoch 124, Loss: 0.4914699197831528\n",
      "Epoch 125, Loss: 0.49091483381312667\n",
      "Epoch 126, Loss: 0.49035147677946317\n",
      "Epoch 127, Loss: 0.4898087222825037\n",
      "Epoch 128, Loss: 0.48924014731550836\n",
      "Epoch 129, Loss: 0.48873550647212044\n",
      "Epoch 130, Loss: 0.48832307254971563\n",
      "Epoch 131, Loss: 0.48791418392821295\n",
      "Epoch 132, Loss: 0.4875902245243796\n",
      "Epoch 133, Loss: 0.48728851108037585\n",
      "Epoch 134, Loss: 0.48697062262181345\n",
      "Epoch 135, Loss: 0.4866339553030548\n",
      "Epoch 136, Loss: 0.48633446746734893\n",
      "Epoch 137, Loss: 0.4860310087358514\n",
      "Epoch 138, Loss: 0.4856786412545615\n",
      "Epoch 139, Loss: 0.48531372107004\n",
      "Epoch 140, Loss: 0.48495712985488587\n",
      "Epoch 141, Loss: 0.4846131600652309\n",
      "Epoch 142, Loss: 0.48425966858652364\n",
      "Epoch 143, Loss: 0.4838914149236435\n",
      "Epoch 144, Loss: 0.48352437546282767\n",
      "Epoch 145, Loss: 0.4832299911937133\n",
      "Epoch 146, Loss: 0.4832586529465723\n",
      "Epoch 147, Loss: 0.48356744143113517\n",
      "Epoch 148, Loss: 0.4828108019011922\n",
      "Epoch 149, Loss: 0.4818912939738211\n",
      "Epoch 150, Loss: 0.48239416745575797\n",
      "Epoch 151, Loss: 0.4814952491721961\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23140078331847702\n",
      "Valence RMSE: 0.21595731948838637\n",
      "Arousal RMSE: 0.24587615012481487\n",
      "Test R^2 score: tensor([0.4348, 0.4014], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.3025, 0.2613], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.41809633770354054\n",
      "Num of epochs: 152\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Epoch 121, Loss: 0.49295912913079615\n",
      "Epoch 122, Loss: 0.4924773619139838\n",
      "Epoch 123, Loss: 0.49197607191949805\n",
      "Epoch 124, Loss: 0.4914699197831528\n",
      "Epoch 125, Loss: 0.49091483381312667\n",
      "Epoch 126, Loss: 0.49035147677946317\n",
      "Epoch 127, Loss: 0.4898087222825037\n",
      "Epoch 128, Loss: 0.48924014731550836\n",
      "Epoch 129, Loss: 0.48873550647212044\n",
      "Epoch 130, Loss: 0.48832307254971563\n",
      "Epoch 131, Loss: 0.48791418392821295\n",
      "Epoch 132, Loss: 0.4875902245243796\n",
      "Epoch 133, Loss: 0.48728851108037585\n",
      "Epoch 134, Loss: 0.48697062262181345\n",
      "Epoch 135, Loss: 0.4866339553030548\n",
      "Epoch 136, Loss: 0.48633446746734893\n",
      "Epoch 137, Loss: 0.4860310087358514\n",
      "Epoch 138, Loss: 0.4856786412545615\n",
      "Epoch 139, Loss: 0.48531372107004\n",
      "Epoch 140, Loss: 0.48495712985488587\n",
      "Epoch 141, Loss: 0.4846131600652309\n",
      "Epoch 142, Loss: 0.48425966858652364\n",
      "Epoch 143, Loss: 0.4838914149236435\n",
      "Epoch 144, Loss: 0.48352437546282767\n",
      "Epoch 145, Loss: 0.4832299911937133\n",
      "Epoch 146, Loss: 0.4832586529465723\n",
      "Epoch 147, Loss: 0.48356744143113517\n",
      "Epoch 148, Loss: 0.4828108019011922\n",
      "Epoch 149, Loss: 0.4818912939738211\n",
      "Epoch 150, Loss: 0.48239416745575797\n",
      "Epoch 151, Loss: 0.4814952491721961\n",
      "Epoch 152, Loss: 0.4811822818820502\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.228691240016579\n",
      "Valence RMSE: 0.21381075547229478\n",
      "Arousal RMSE: 0.24266093085828144\n",
      "Test R^2 score: tensor([0.4460, 0.4169], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.3163, 0.2805], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4314629251863044\n",
      "Num of epochs: 153\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Epoch 121, Loss: 0.49295912913079615\n",
      "Epoch 122, Loss: 0.4924773619139838\n",
      "Epoch 123, Loss: 0.49197607191949805\n",
      "Epoch 124, Loss: 0.4914699197831528\n",
      "Epoch 125, Loss: 0.49091483381312667\n",
      "Epoch 126, Loss: 0.49035147677946317\n",
      "Epoch 127, Loss: 0.4898087222825037\n",
      "Epoch 128, Loss: 0.48924014731550836\n",
      "Epoch 129, Loss: 0.48873550647212044\n",
      "Epoch 130, Loss: 0.48832307254971563\n",
      "Epoch 131, Loss: 0.48791418392821295\n",
      "Epoch 132, Loss: 0.4875902245243796\n",
      "Epoch 133, Loss: 0.48728851108037585\n",
      "Epoch 134, Loss: 0.48697062262181345\n",
      "Epoch 135, Loss: 0.4866339553030548\n",
      "Epoch 136, Loss: 0.48633446746734893\n",
      "Epoch 137, Loss: 0.4860310087358514\n",
      "Epoch 138, Loss: 0.4856786412545615\n",
      "Epoch 139, Loss: 0.48531372107004\n",
      "Epoch 140, Loss: 0.48495712985488587\n",
      "Epoch 141, Loss: 0.4846131600652309\n",
      "Epoch 142, Loss: 0.48425966858652364\n",
      "Epoch 143, Loss: 0.4838914149236435\n",
      "Epoch 144, Loss: 0.48352437546282767\n",
      "Epoch 145, Loss: 0.4832299911937133\n",
      "Epoch 146, Loss: 0.4832586529465723\n",
      "Epoch 147, Loss: 0.48356744143113517\n",
      "Epoch 148, Loss: 0.4828108019011922\n",
      "Epoch 149, Loss: 0.4818912939738211\n",
      "Epoch 150, Loss: 0.48239416745575797\n",
      "Epoch 151, Loss: 0.4814952491721961\n",
      "Epoch 152, Loss: 0.4811822818820502\n",
      "Epoch 153, Loss: 0.4812753156716337\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.22974327187232196\n",
      "Valence RMSE: 0.21471661045486076\n",
      "Arousal RMSE: 0.24384568713835167\n",
      "Test R^2 score: tensor([0.4413, 0.4112], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.3105, 0.2734], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4262571036720436\n",
      "Num of epochs: 154\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Epoch 121, Loss: 0.49295912913079615\n",
      "Epoch 122, Loss: 0.4924773619139838\n",
      "Epoch 123, Loss: 0.49197607191949805\n",
      "Epoch 124, Loss: 0.4914699197831528\n",
      "Epoch 125, Loss: 0.49091483381312667\n",
      "Epoch 126, Loss: 0.49035147677946317\n",
      "Epoch 127, Loss: 0.4898087222825037\n",
      "Epoch 128, Loss: 0.48924014731550836\n",
      "Epoch 129, Loss: 0.48873550647212044\n",
      "Epoch 130, Loss: 0.48832307254971563\n",
      "Epoch 131, Loss: 0.48791418392821295\n",
      "Epoch 132, Loss: 0.4875902245243796\n",
      "Epoch 133, Loss: 0.48728851108037585\n",
      "Epoch 134, Loss: 0.48697062262181345\n",
      "Epoch 135, Loss: 0.4866339553030548\n",
      "Epoch 136, Loss: 0.48633446746734893\n",
      "Epoch 137, Loss: 0.4860310087358514\n",
      "Epoch 138, Loss: 0.4856786412545615\n",
      "Epoch 139, Loss: 0.48531372107004\n",
      "Epoch 140, Loss: 0.48495712985488587\n",
      "Epoch 141, Loss: 0.4846131600652309\n",
      "Epoch 142, Loss: 0.48425966858652364\n",
      "Epoch 143, Loss: 0.4838914149236435\n",
      "Epoch 144, Loss: 0.48352437546282767\n",
      "Epoch 145, Loss: 0.4832299911937133\n",
      "Epoch 146, Loss: 0.4832586529465723\n",
      "Epoch 147, Loss: 0.48356744143113517\n",
      "Epoch 148, Loss: 0.4828108019011922\n",
      "Epoch 149, Loss: 0.4818912939738211\n",
      "Epoch 150, Loss: 0.48239416745575797\n",
      "Epoch 151, Loss: 0.4814952491721961\n",
      "Epoch 152, Loss: 0.4811822818820502\n",
      "Epoch 153, Loss: 0.4812753156716337\n",
      "Epoch 154, Loss: 0.4804387742282827\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23106462493644914\n",
      "Valence RMSE: 0.2158605353211091\n",
      "Arousal RMSE: 0.2453282517056037\n",
      "Test R^2 score: tensor([0.4353, 0.4041], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.3031, 0.2646], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4196820032528621\n",
      "Num of epochs: 155\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Epoch 121, Loss: 0.49295912913079615\n",
      "Epoch 122, Loss: 0.4924773619139838\n",
      "Epoch 123, Loss: 0.49197607191949805\n",
      "Epoch 124, Loss: 0.4914699197831528\n",
      "Epoch 125, Loss: 0.49091483381312667\n",
      "Epoch 126, Loss: 0.49035147677946317\n",
      "Epoch 127, Loss: 0.4898087222825037\n",
      "Epoch 128, Loss: 0.48924014731550836\n",
      "Epoch 129, Loss: 0.48873550647212044\n",
      "Epoch 130, Loss: 0.48832307254971563\n",
      "Epoch 131, Loss: 0.48791418392821295\n",
      "Epoch 132, Loss: 0.4875902245243796\n",
      "Epoch 133, Loss: 0.48728851108037585\n",
      "Epoch 134, Loss: 0.48697062262181345\n",
      "Epoch 135, Loss: 0.4866339553030548\n",
      "Epoch 136, Loss: 0.48633446746734893\n",
      "Epoch 137, Loss: 0.4860310087358514\n",
      "Epoch 138, Loss: 0.4856786412545615\n",
      "Epoch 139, Loss: 0.48531372107004\n",
      "Epoch 140, Loss: 0.48495712985488587\n",
      "Epoch 141, Loss: 0.4846131600652309\n",
      "Epoch 142, Loss: 0.48425966858652364\n",
      "Epoch 143, Loss: 0.4838914149236435\n",
      "Epoch 144, Loss: 0.48352437546282767\n",
      "Epoch 145, Loss: 0.4832299911937133\n",
      "Epoch 146, Loss: 0.4832586529465723\n",
      "Epoch 147, Loss: 0.48356744143113517\n",
      "Epoch 148, Loss: 0.4828108019011922\n",
      "Epoch 149, Loss: 0.4818912939738211\n",
      "Epoch 150, Loss: 0.48239416745575797\n",
      "Epoch 151, Loss: 0.4814952491721961\n",
      "Epoch 152, Loss: 0.4811822818820502\n",
      "Epoch 153, Loss: 0.4812753156716337\n",
      "Epoch 154, Loss: 0.4804387742282827\n",
      "Epoch 155, Loss: 0.48058189052417905\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.22818948091962588\n",
      "Valence RMSE: 0.21350369068701913\n",
      "Arousal RMSE: 0.24198564516879562\n",
      "Test R^2 score: tensor([0.4476, 0.4202], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.3183, 0.2845], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.43387829709098275\n",
      "Num of epochs: 156\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Epoch 121, Loss: 0.49295912913079615\n",
      "Epoch 122, Loss: 0.4924773619139838\n",
      "Epoch 123, Loss: 0.49197607191949805\n",
      "Epoch 124, Loss: 0.4914699197831528\n",
      "Epoch 125, Loss: 0.49091483381312667\n",
      "Epoch 126, Loss: 0.49035147677946317\n",
      "Epoch 127, Loss: 0.4898087222825037\n",
      "Epoch 128, Loss: 0.48924014731550836\n",
      "Epoch 129, Loss: 0.48873550647212044\n",
      "Epoch 130, Loss: 0.48832307254971563\n",
      "Epoch 131, Loss: 0.48791418392821295\n",
      "Epoch 132, Loss: 0.4875902245243796\n",
      "Epoch 133, Loss: 0.48728851108037585\n",
      "Epoch 134, Loss: 0.48697062262181345\n",
      "Epoch 135, Loss: 0.4866339553030548\n",
      "Epoch 136, Loss: 0.48633446746734893\n",
      "Epoch 137, Loss: 0.4860310087358514\n",
      "Epoch 138, Loss: 0.4856786412545615\n",
      "Epoch 139, Loss: 0.48531372107004\n",
      "Epoch 140, Loss: 0.48495712985488587\n",
      "Epoch 141, Loss: 0.4846131600652309\n",
      "Epoch 142, Loss: 0.48425966858652364\n",
      "Epoch 143, Loss: 0.4838914149236435\n",
      "Epoch 144, Loss: 0.48352437546282767\n",
      "Epoch 145, Loss: 0.4832299911937133\n",
      "Epoch 146, Loss: 0.4832586529465723\n",
      "Epoch 147, Loss: 0.48356744143113517\n",
      "Epoch 148, Loss: 0.4828108019011922\n",
      "Epoch 149, Loss: 0.4818912939738211\n",
      "Epoch 150, Loss: 0.48239416745575797\n",
      "Epoch 151, Loss: 0.4814952491721961\n",
      "Epoch 152, Loss: 0.4811822818820502\n",
      "Epoch 153, Loss: 0.4812753156716337\n",
      "Epoch 154, Loss: 0.4804387742282827\n",
      "Epoch 155, Loss: 0.48058189052417905\n",
      "Epoch 156, Loss: 0.4801288115497346\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.228158363228405\n",
      "Valence RMSE: 0.2135092260835327\n",
      "Arousal RMSE: 0.24192206968224392\n",
      "Test R^2 score: tensor([0.4475, 0.4205], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.3182, 0.2849], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4340162852939058\n",
      "Num of epochs: 157\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Epoch 121, Loss: 0.49295912913079615\n",
      "Epoch 122, Loss: 0.4924773619139838\n",
      "Epoch 123, Loss: 0.49197607191949805\n",
      "Epoch 124, Loss: 0.4914699197831528\n",
      "Epoch 125, Loss: 0.49091483381312667\n",
      "Epoch 126, Loss: 0.49035147677946317\n",
      "Epoch 127, Loss: 0.4898087222825037\n",
      "Epoch 128, Loss: 0.48924014731550836\n",
      "Epoch 129, Loss: 0.48873550647212044\n",
      "Epoch 130, Loss: 0.48832307254971563\n",
      "Epoch 131, Loss: 0.48791418392821295\n",
      "Epoch 132, Loss: 0.4875902245243796\n",
      "Epoch 133, Loss: 0.48728851108037585\n",
      "Epoch 134, Loss: 0.48697062262181345\n",
      "Epoch 135, Loss: 0.4866339553030548\n",
      "Epoch 136, Loss: 0.48633446746734893\n",
      "Epoch 137, Loss: 0.4860310087358514\n",
      "Epoch 138, Loss: 0.4856786412545615\n",
      "Epoch 139, Loss: 0.48531372107004\n",
      "Epoch 140, Loss: 0.48495712985488587\n",
      "Epoch 141, Loss: 0.4846131600652309\n",
      "Epoch 142, Loss: 0.48425966858652364\n",
      "Epoch 143, Loss: 0.4838914149236435\n",
      "Epoch 144, Loss: 0.48352437546282767\n",
      "Epoch 145, Loss: 0.4832299911937133\n",
      "Epoch 146, Loss: 0.4832586529465723\n",
      "Epoch 147, Loss: 0.48356744143113517\n",
      "Epoch 148, Loss: 0.4828108019011922\n",
      "Epoch 149, Loss: 0.4818912939738211\n",
      "Epoch 150, Loss: 0.48239416745575797\n",
      "Epoch 151, Loss: 0.4814952491721961\n",
      "Epoch 152, Loss: 0.4811822818820502\n",
      "Epoch 153, Loss: 0.4812753156716337\n",
      "Epoch 154, Loss: 0.4804387742282827\n",
      "Epoch 155, Loss: 0.48058189052417905\n",
      "Epoch 156, Loss: 0.4801288115497346\n",
      "Epoch 157, Loss: 0.4797465937246525\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2301148438778489\n",
      "Valence RMSE: 0.21516886418450745\n",
      "Arousal RMSE: 0.2441475837099408\n",
      "Test R^2 score: tensor([0.4389, 0.4098], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.3076, 0.2716], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.42434965731265667\n",
      "Num of epochs: 158\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Epoch 121, Loss: 0.49295912913079615\n",
      "Epoch 122, Loss: 0.4924773619139838\n",
      "Epoch 123, Loss: 0.49197607191949805\n",
      "Epoch 124, Loss: 0.4914699197831528\n",
      "Epoch 125, Loss: 0.49091483381312667\n",
      "Epoch 126, Loss: 0.49035147677946317\n",
      "Epoch 127, Loss: 0.4898087222825037\n",
      "Epoch 128, Loss: 0.48924014731550836\n",
      "Epoch 129, Loss: 0.48873550647212044\n",
      "Epoch 130, Loss: 0.48832307254971563\n",
      "Epoch 131, Loss: 0.48791418392821295\n",
      "Epoch 132, Loss: 0.4875902245243796\n",
      "Epoch 133, Loss: 0.48728851108037585\n",
      "Epoch 134, Loss: 0.48697062262181345\n",
      "Epoch 135, Loss: 0.4866339553030548\n",
      "Epoch 136, Loss: 0.48633446746734893\n",
      "Epoch 137, Loss: 0.4860310087358514\n",
      "Epoch 138, Loss: 0.4856786412545615\n",
      "Epoch 139, Loss: 0.48531372107004\n",
      "Epoch 140, Loss: 0.48495712985488587\n",
      "Epoch 141, Loss: 0.4846131600652309\n",
      "Epoch 142, Loss: 0.48425966858652364\n",
      "Epoch 143, Loss: 0.4838914149236435\n",
      "Epoch 144, Loss: 0.48352437546282767\n",
      "Epoch 145, Loss: 0.4832299911937133\n",
      "Epoch 146, Loss: 0.4832586529465723\n",
      "Epoch 147, Loss: 0.48356744143113517\n",
      "Epoch 148, Loss: 0.4828108019011922\n",
      "Epoch 149, Loss: 0.4818912939738211\n",
      "Epoch 150, Loss: 0.48239416745575797\n",
      "Epoch 151, Loss: 0.4814952491721961\n",
      "Epoch 152, Loss: 0.4811822818820502\n",
      "Epoch 153, Loss: 0.4812753156716337\n",
      "Epoch 154, Loss: 0.4804387742282827\n",
      "Epoch 155, Loss: 0.48058189052417905\n",
      "Epoch 156, Loss: 0.4801288115497346\n",
      "Epoch 157, Loss: 0.4797465937246525\n",
      "Epoch 158, Loss: 0.4797622479529733\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.22782157327833866\n",
      "Valence RMSE: 0.2132646526043938\n",
      "Arousal RMSE: 0.24150264274239747\n",
      "Test R^2 score: tensor([0.4488, 0.4225], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.3198, 0.2873], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.43565259998603056\n",
      "Num of epochs: 159\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Epoch 121, Loss: 0.49295912913079615\n",
      "Epoch 122, Loss: 0.4924773619139838\n",
      "Epoch 123, Loss: 0.49197607191949805\n",
      "Epoch 124, Loss: 0.4914699197831528\n",
      "Epoch 125, Loss: 0.49091483381312667\n",
      "Epoch 126, Loss: 0.49035147677946317\n",
      "Epoch 127, Loss: 0.4898087222825037\n",
      "Epoch 128, Loss: 0.48924014731550836\n",
      "Epoch 129, Loss: 0.48873550647212044\n",
      "Epoch 130, Loss: 0.48832307254971563\n",
      "Epoch 131, Loss: 0.48791418392821295\n",
      "Epoch 132, Loss: 0.4875902245243796\n",
      "Epoch 133, Loss: 0.48728851108037585\n",
      "Epoch 134, Loss: 0.48697062262181345\n",
      "Epoch 135, Loss: 0.4866339553030548\n",
      "Epoch 136, Loss: 0.48633446746734893\n",
      "Epoch 137, Loss: 0.4860310087358514\n",
      "Epoch 138, Loss: 0.4856786412545615\n",
      "Epoch 139, Loss: 0.48531372107004\n",
      "Epoch 140, Loss: 0.48495712985488587\n",
      "Epoch 141, Loss: 0.4846131600652309\n",
      "Epoch 142, Loss: 0.48425966858652364\n",
      "Epoch 143, Loss: 0.4838914149236435\n",
      "Epoch 144, Loss: 0.48352437546282767\n",
      "Epoch 145, Loss: 0.4832299911937133\n",
      "Epoch 146, Loss: 0.4832586529465723\n",
      "Epoch 147, Loss: 0.48356744143113517\n",
      "Epoch 148, Loss: 0.4828108019011922\n",
      "Epoch 149, Loss: 0.4818912939738211\n",
      "Epoch 150, Loss: 0.48239416745575797\n",
      "Epoch 151, Loss: 0.4814952491721961\n",
      "Epoch 152, Loss: 0.4811822818820502\n",
      "Epoch 153, Loss: 0.4812753156716337\n",
      "Epoch 154, Loss: 0.4804387742282827\n",
      "Epoch 155, Loss: 0.48058189052417905\n",
      "Epoch 156, Loss: 0.4801288115497346\n",
      "Epoch 157, Loss: 0.4797465937246525\n",
      "Epoch 158, Loss: 0.4797622479529733\n",
      "Epoch 159, Loss: 0.4791664679844762\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.22717974987237013\n",
      "Valence RMSE: 0.2127140259626012\n",
      "Arousal RMSE: 0.24077794887183324\n",
      "Test R^2 score: tensor([0.4516, 0.4260], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.3233, 0.2916], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4388042340645231\n",
      "Num of epochs: 160\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Epoch 121, Loss: 0.49295912913079615\n",
      "Epoch 122, Loss: 0.4924773619139838\n",
      "Epoch 123, Loss: 0.49197607191949805\n",
      "Epoch 124, Loss: 0.4914699197831528\n",
      "Epoch 125, Loss: 0.49091483381312667\n",
      "Epoch 126, Loss: 0.49035147677946317\n",
      "Epoch 127, Loss: 0.4898087222825037\n",
      "Epoch 128, Loss: 0.48924014731550836\n",
      "Epoch 129, Loss: 0.48873550647212044\n",
      "Epoch 130, Loss: 0.48832307254971563\n",
      "Epoch 131, Loss: 0.48791418392821295\n",
      "Epoch 132, Loss: 0.4875902245243796\n",
      "Epoch 133, Loss: 0.48728851108037585\n",
      "Epoch 134, Loss: 0.48697062262181345\n",
      "Epoch 135, Loss: 0.4866339553030548\n",
      "Epoch 136, Loss: 0.48633446746734893\n",
      "Epoch 137, Loss: 0.4860310087358514\n",
      "Epoch 138, Loss: 0.4856786412545615\n",
      "Epoch 139, Loss: 0.48531372107004\n",
      "Epoch 140, Loss: 0.48495712985488587\n",
      "Epoch 141, Loss: 0.4846131600652309\n",
      "Epoch 142, Loss: 0.48425966858652364\n",
      "Epoch 143, Loss: 0.4838914149236435\n",
      "Epoch 144, Loss: 0.48352437546282767\n",
      "Epoch 145, Loss: 0.4832299911937133\n",
      "Epoch 146, Loss: 0.4832586529465723\n",
      "Epoch 147, Loss: 0.48356744143113517\n",
      "Epoch 148, Loss: 0.4828108019011922\n",
      "Epoch 149, Loss: 0.4818912939738211\n",
      "Epoch 150, Loss: 0.48239416745575797\n",
      "Epoch 151, Loss: 0.4814952491721961\n",
      "Epoch 152, Loss: 0.4811822818820502\n",
      "Epoch 153, Loss: 0.4812753156716337\n",
      "Epoch 154, Loss: 0.4804387742282827\n",
      "Epoch 155, Loss: 0.48058189052417905\n",
      "Epoch 156, Loss: 0.4801288115497346\n",
      "Epoch 157, Loss: 0.4797465937246525\n",
      "Epoch 158, Loss: 0.4797622479529733\n",
      "Epoch 159, Loss: 0.4791664679844762\n",
      "Epoch 160, Loss: 0.47904359923301915\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.22891771646666237\n",
      "Valence RMSE: 0.21418075574830214\n",
      "Arousal RMSE: 0.2427617055709335\n",
      "Test R^2 score: tensor([0.4441, 0.4165], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.3139, 0.2799], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.43026117458664703\n",
      "Num of epochs: 161\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Epoch 121, Loss: 0.49295912913079615\n",
      "Epoch 122, Loss: 0.4924773619139838\n",
      "Epoch 123, Loss: 0.49197607191949805\n",
      "Epoch 124, Loss: 0.4914699197831528\n",
      "Epoch 125, Loss: 0.49091483381312667\n",
      "Epoch 126, Loss: 0.49035147677946317\n",
      "Epoch 127, Loss: 0.4898087222825037\n",
      "Epoch 128, Loss: 0.48924014731550836\n",
      "Epoch 129, Loss: 0.48873550647212044\n",
      "Epoch 130, Loss: 0.48832307254971563\n",
      "Epoch 131, Loss: 0.48791418392821295\n",
      "Epoch 132, Loss: 0.4875902245243796\n",
      "Epoch 133, Loss: 0.48728851108037585\n",
      "Epoch 134, Loss: 0.48697062262181345\n",
      "Epoch 135, Loss: 0.4866339553030548\n",
      "Epoch 136, Loss: 0.48633446746734893\n",
      "Epoch 137, Loss: 0.4860310087358514\n",
      "Epoch 138, Loss: 0.4856786412545615\n",
      "Epoch 139, Loss: 0.48531372107004\n",
      "Epoch 140, Loss: 0.48495712985488587\n",
      "Epoch 141, Loss: 0.4846131600652309\n",
      "Epoch 142, Loss: 0.48425966858652364\n",
      "Epoch 143, Loss: 0.4838914149236435\n",
      "Epoch 144, Loss: 0.48352437546282767\n",
      "Epoch 145, Loss: 0.4832299911937133\n",
      "Epoch 146, Loss: 0.4832586529465723\n",
      "Epoch 147, Loss: 0.48356744143113517\n",
      "Epoch 148, Loss: 0.4828108019011922\n",
      "Epoch 149, Loss: 0.4818912939738211\n",
      "Epoch 150, Loss: 0.48239416745575797\n",
      "Epoch 151, Loss: 0.4814952491721961\n",
      "Epoch 152, Loss: 0.4811822818820502\n",
      "Epoch 153, Loss: 0.4812753156716337\n",
      "Epoch 154, Loss: 0.4804387742282827\n",
      "Epoch 155, Loss: 0.48058189052417905\n",
      "Epoch 156, Loss: 0.4801288115497346\n",
      "Epoch 157, Loss: 0.4797465937246525\n",
      "Epoch 158, Loss: 0.4797622479529733\n",
      "Epoch 159, Loss: 0.4791664679844762\n",
      "Epoch 160, Loss: 0.47904359923301915\n",
      "Epoch 161, Loss: 0.4788483533353555\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2273916302653892\n",
      "Valence RMSE: 0.21289858179516472\n",
      "Arousal RMSE: 0.24101473170557536\n",
      "Test R^2 score: tensor([0.4507, 0.4248], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.3221, 0.2902], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4377634718012976\n",
      "Num of epochs: 162\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Epoch 121, Loss: 0.49295912913079615\n",
      "Epoch 122, Loss: 0.4924773619139838\n",
      "Epoch 123, Loss: 0.49197607191949805\n",
      "Epoch 124, Loss: 0.4914699197831528\n",
      "Epoch 125, Loss: 0.49091483381312667\n",
      "Epoch 126, Loss: 0.49035147677946317\n",
      "Epoch 127, Loss: 0.4898087222825037\n",
      "Epoch 128, Loss: 0.48924014731550836\n",
      "Epoch 129, Loss: 0.48873550647212044\n",
      "Epoch 130, Loss: 0.48832307254971563\n",
      "Epoch 131, Loss: 0.48791418392821295\n",
      "Epoch 132, Loss: 0.4875902245243796\n",
      "Epoch 133, Loss: 0.48728851108037585\n",
      "Epoch 134, Loss: 0.48697062262181345\n",
      "Epoch 135, Loss: 0.4866339553030548\n",
      "Epoch 136, Loss: 0.48633446746734893\n",
      "Epoch 137, Loss: 0.4860310087358514\n",
      "Epoch 138, Loss: 0.4856786412545615\n",
      "Epoch 139, Loss: 0.48531372107004\n",
      "Epoch 140, Loss: 0.48495712985488587\n",
      "Epoch 141, Loss: 0.4846131600652309\n",
      "Epoch 142, Loss: 0.48425966858652364\n",
      "Epoch 143, Loss: 0.4838914149236435\n",
      "Epoch 144, Loss: 0.48352437546282767\n",
      "Epoch 145, Loss: 0.4832299911937133\n",
      "Epoch 146, Loss: 0.4832586529465723\n",
      "Epoch 147, Loss: 0.48356744143113517\n",
      "Epoch 148, Loss: 0.4828108019011922\n",
      "Epoch 149, Loss: 0.4818912939738211\n",
      "Epoch 150, Loss: 0.48239416745575797\n",
      "Epoch 151, Loss: 0.4814952491721961\n",
      "Epoch 152, Loss: 0.4811822818820502\n",
      "Epoch 153, Loss: 0.4812753156716337\n",
      "Epoch 154, Loss: 0.4804387742282827\n",
      "Epoch 155, Loss: 0.48058189052417905\n",
      "Epoch 156, Loss: 0.4801288115497346\n",
      "Epoch 157, Loss: 0.4797465937246525\n",
      "Epoch 158, Loss: 0.4797622479529733\n",
      "Epoch 159, Loss: 0.4791664679844762\n",
      "Epoch 160, Loss: 0.47904359923301915\n",
      "Epoch 161, Loss: 0.4788483533353555\n",
      "Epoch 162, Loss: 0.4783709093142749\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.22641860779840262\n",
      "Valence RMSE: 0.21207480139960996\n",
      "Arousal RMSE: 0.23990633698601987\n",
      "Test R^2 score: tensor([0.4549, 0.4301], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.3274, 0.2967], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4425238567306347\n",
      "Num of epochs: 163\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Epoch 121, Loss: 0.49295912913079615\n",
      "Epoch 122, Loss: 0.4924773619139838\n",
      "Epoch 123, Loss: 0.49197607191949805\n",
      "Epoch 124, Loss: 0.4914699197831528\n",
      "Epoch 125, Loss: 0.49091483381312667\n",
      "Epoch 126, Loss: 0.49035147677946317\n",
      "Epoch 127, Loss: 0.4898087222825037\n",
      "Epoch 128, Loss: 0.48924014731550836\n",
      "Epoch 129, Loss: 0.48873550647212044\n",
      "Epoch 130, Loss: 0.48832307254971563\n",
      "Epoch 131, Loss: 0.48791418392821295\n",
      "Epoch 132, Loss: 0.4875902245243796\n",
      "Epoch 133, Loss: 0.48728851108037585\n",
      "Epoch 134, Loss: 0.48697062262181345\n",
      "Epoch 135, Loss: 0.4866339553030548\n",
      "Epoch 136, Loss: 0.48633446746734893\n",
      "Epoch 137, Loss: 0.4860310087358514\n",
      "Epoch 138, Loss: 0.4856786412545615\n",
      "Epoch 139, Loss: 0.48531372107004\n",
      "Epoch 140, Loss: 0.48495712985488587\n",
      "Epoch 141, Loss: 0.4846131600652309\n",
      "Epoch 142, Loss: 0.48425966858652364\n",
      "Epoch 143, Loss: 0.4838914149236435\n",
      "Epoch 144, Loss: 0.48352437546282767\n",
      "Epoch 145, Loss: 0.4832299911937133\n",
      "Epoch 146, Loss: 0.4832586529465723\n",
      "Epoch 147, Loss: 0.48356744143113517\n",
      "Epoch 148, Loss: 0.4828108019011922\n",
      "Epoch 149, Loss: 0.4818912939738211\n",
      "Epoch 150, Loss: 0.48239416745575797\n",
      "Epoch 151, Loss: 0.4814952491721961\n",
      "Epoch 152, Loss: 0.4811822818820502\n",
      "Epoch 153, Loss: 0.4812753156716337\n",
      "Epoch 154, Loss: 0.4804387742282827\n",
      "Epoch 155, Loss: 0.48058189052417905\n",
      "Epoch 156, Loss: 0.4801288115497346\n",
      "Epoch 157, Loss: 0.4797465937246525\n",
      "Epoch 158, Loss: 0.4797622479529733\n",
      "Epoch 159, Loss: 0.4791664679844762\n",
      "Epoch 160, Loss: 0.47904359923301915\n",
      "Epoch 161, Loss: 0.4788483533353555\n",
      "Epoch 162, Loss: 0.4783709093142749\n",
      "Epoch 163, Loss: 0.4783027642591491\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.22779785474939837\n",
      "Valence RMSE: 0.2132176217360965\n",
      "Arousal RMSE: 0.24149942243835465\n",
      "Test R^2 score: tensor([0.4490, 0.4225], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.3201, 0.2874], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4357818409806339\n",
      "Num of epochs: 164\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Epoch 121, Loss: 0.49295912913079615\n",
      "Epoch 122, Loss: 0.4924773619139838\n",
      "Epoch 123, Loss: 0.49197607191949805\n",
      "Epoch 124, Loss: 0.4914699197831528\n",
      "Epoch 125, Loss: 0.49091483381312667\n",
      "Epoch 126, Loss: 0.49035147677946317\n",
      "Epoch 127, Loss: 0.4898087222825037\n",
      "Epoch 128, Loss: 0.48924014731550836\n",
      "Epoch 129, Loss: 0.48873550647212044\n",
      "Epoch 130, Loss: 0.48832307254971563\n",
      "Epoch 131, Loss: 0.48791418392821295\n",
      "Epoch 132, Loss: 0.4875902245243796\n",
      "Epoch 133, Loss: 0.48728851108037585\n",
      "Epoch 134, Loss: 0.48697062262181345\n",
      "Epoch 135, Loss: 0.4866339553030548\n",
      "Epoch 136, Loss: 0.48633446746734893\n",
      "Epoch 137, Loss: 0.4860310087358514\n",
      "Epoch 138, Loss: 0.4856786412545615\n",
      "Epoch 139, Loss: 0.48531372107004\n",
      "Epoch 140, Loss: 0.48495712985488587\n",
      "Epoch 141, Loss: 0.4846131600652309\n",
      "Epoch 142, Loss: 0.48425966858652364\n",
      "Epoch 143, Loss: 0.4838914149236435\n",
      "Epoch 144, Loss: 0.48352437546282767\n",
      "Epoch 145, Loss: 0.4832299911937133\n",
      "Epoch 146, Loss: 0.4832586529465723\n",
      "Epoch 147, Loss: 0.48356744143113517\n",
      "Epoch 148, Loss: 0.4828108019011922\n",
      "Epoch 149, Loss: 0.4818912939738211\n",
      "Epoch 150, Loss: 0.48239416745575797\n",
      "Epoch 151, Loss: 0.4814952491721961\n",
      "Epoch 152, Loss: 0.4811822818820502\n",
      "Epoch 153, Loss: 0.4812753156716337\n",
      "Epoch 154, Loss: 0.4804387742282827\n",
      "Epoch 155, Loss: 0.48058189052417905\n",
      "Epoch 156, Loss: 0.4801288115497346\n",
      "Epoch 157, Loss: 0.4797465937246525\n",
      "Epoch 158, Loss: 0.4797622479529733\n",
      "Epoch 159, Loss: 0.4791664679844762\n",
      "Epoch 160, Loss: 0.47904359923301915\n",
      "Epoch 161, Loss: 0.4788483533353555\n",
      "Epoch 162, Loss: 0.4783709093142749\n",
      "Epoch 163, Loss: 0.4783027642591491\n",
      "Epoch 164, Loss: 0.4780003946284736\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.22691761136459154\n",
      "Valence RMSE: 0.21248840790403203\n",
      "Arousal RMSE: 0.2404826006205715\n",
      "Test R^2 score: tensor([0.4528, 0.4274], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.3247, 0.2933], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4400892517326772\n",
      "Num of epochs: 165\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Epoch 121, Loss: 0.49295912913079615\n",
      "Epoch 122, Loss: 0.4924773619139838\n",
      "Epoch 123, Loss: 0.49197607191949805\n",
      "Epoch 124, Loss: 0.4914699197831528\n",
      "Epoch 125, Loss: 0.49091483381312667\n",
      "Epoch 126, Loss: 0.49035147677946317\n",
      "Epoch 127, Loss: 0.4898087222825037\n",
      "Epoch 128, Loss: 0.48924014731550836\n",
      "Epoch 129, Loss: 0.48873550647212044\n",
      "Epoch 130, Loss: 0.48832307254971563\n",
      "Epoch 131, Loss: 0.48791418392821295\n",
      "Epoch 132, Loss: 0.4875902245243796\n",
      "Epoch 133, Loss: 0.48728851108037585\n",
      "Epoch 134, Loss: 0.48697062262181345\n",
      "Epoch 135, Loss: 0.4866339553030548\n",
      "Epoch 136, Loss: 0.48633446746734893\n",
      "Epoch 137, Loss: 0.4860310087358514\n",
      "Epoch 138, Loss: 0.4856786412545615\n",
      "Epoch 139, Loss: 0.48531372107004\n",
      "Epoch 140, Loss: 0.48495712985488587\n",
      "Epoch 141, Loss: 0.4846131600652309\n",
      "Epoch 142, Loss: 0.48425966858652364\n",
      "Epoch 143, Loss: 0.4838914149236435\n",
      "Epoch 144, Loss: 0.48352437546282767\n",
      "Epoch 145, Loss: 0.4832299911937133\n",
      "Epoch 146, Loss: 0.4832586529465723\n",
      "Epoch 147, Loss: 0.48356744143113517\n",
      "Epoch 148, Loss: 0.4828108019011922\n",
      "Epoch 149, Loss: 0.4818912939738211\n",
      "Epoch 150, Loss: 0.48239416745575797\n",
      "Epoch 151, Loss: 0.4814952491721961\n",
      "Epoch 152, Loss: 0.4811822818820502\n",
      "Epoch 153, Loss: 0.4812753156716337\n",
      "Epoch 154, Loss: 0.4804387742282827\n",
      "Epoch 155, Loss: 0.48058189052417905\n",
      "Epoch 156, Loss: 0.4801288115497346\n",
      "Epoch 157, Loss: 0.4797465937246525\n",
      "Epoch 158, Loss: 0.4797622479529733\n",
      "Epoch 159, Loss: 0.4791664679844762\n",
      "Epoch 160, Loss: 0.47904359923301915\n",
      "Epoch 161, Loss: 0.4788483533353555\n",
      "Epoch 162, Loss: 0.4783709093142749\n",
      "Epoch 163, Loss: 0.4783027642591491\n",
      "Epoch 164, Loss: 0.4780003946284736\n",
      "Epoch 165, Loss: 0.47766356610156796\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.22575662325153892\n",
      "Valence RMSE: 0.21153996477906617\n",
      "Arousal RMSE: 0.2391295656859394\n",
      "Test R^2 score: tensor([0.4577, 0.4338], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.3308, 0.3013], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.44573893923835123\n",
      "Num of epochs: 166\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Epoch 121, Loss: 0.49295912913079615\n",
      "Epoch 122, Loss: 0.4924773619139838\n",
      "Epoch 123, Loss: 0.49197607191949805\n",
      "Epoch 124, Loss: 0.4914699197831528\n",
      "Epoch 125, Loss: 0.49091483381312667\n",
      "Epoch 126, Loss: 0.49035147677946317\n",
      "Epoch 127, Loss: 0.4898087222825037\n",
      "Epoch 128, Loss: 0.48924014731550836\n",
      "Epoch 129, Loss: 0.48873550647212044\n",
      "Epoch 130, Loss: 0.48832307254971563\n",
      "Epoch 131, Loss: 0.48791418392821295\n",
      "Epoch 132, Loss: 0.4875902245243796\n",
      "Epoch 133, Loss: 0.48728851108037585\n",
      "Epoch 134, Loss: 0.48697062262181345\n",
      "Epoch 135, Loss: 0.4866339553030548\n",
      "Epoch 136, Loss: 0.48633446746734893\n",
      "Epoch 137, Loss: 0.4860310087358514\n",
      "Epoch 138, Loss: 0.4856786412545615\n",
      "Epoch 139, Loss: 0.48531372107004\n",
      "Epoch 140, Loss: 0.48495712985488587\n",
      "Epoch 141, Loss: 0.4846131600652309\n",
      "Epoch 142, Loss: 0.48425966858652364\n",
      "Epoch 143, Loss: 0.4838914149236435\n",
      "Epoch 144, Loss: 0.48352437546282767\n",
      "Epoch 145, Loss: 0.4832299911937133\n",
      "Epoch 146, Loss: 0.4832586529465723\n",
      "Epoch 147, Loss: 0.48356744143113517\n",
      "Epoch 148, Loss: 0.4828108019011922\n",
      "Epoch 149, Loss: 0.4818912939738211\n",
      "Epoch 150, Loss: 0.48239416745575797\n",
      "Epoch 151, Loss: 0.4814952491721961\n",
      "Epoch 152, Loss: 0.4811822818820502\n",
      "Epoch 153, Loss: 0.4812753156716337\n",
      "Epoch 154, Loss: 0.4804387742282827\n",
      "Epoch 155, Loss: 0.48058189052417905\n",
      "Epoch 156, Loss: 0.4801288115497346\n",
      "Epoch 157, Loss: 0.4797465937246525\n",
      "Epoch 158, Loss: 0.4797622479529733\n",
      "Epoch 159, Loss: 0.4791664679844762\n",
      "Epoch 160, Loss: 0.47904359923301915\n",
      "Epoch 161, Loss: 0.4788483533353555\n",
      "Epoch 162, Loss: 0.4783709093142749\n",
      "Epoch 163, Loss: 0.4783027642591491\n",
      "Epoch 164, Loss: 0.4780003946284736\n",
      "Epoch 165, Loss: 0.47766356610156796\n",
      "Epoch 166, Loss: 0.47756940749162413\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.22680723388524823\n",
      "Valence RMSE: 0.21242739815484826\n",
      "Arousal RMSE: 0.24032819892496374\n",
      "Test R^2 score: tensor([0.4531, 0.4281], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.3251, 0.2943], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4406138786767806\n",
      "Num of epochs: 167\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Epoch 121, Loss: 0.49295912913079615\n",
      "Epoch 122, Loss: 0.4924773619139838\n",
      "Epoch 123, Loss: 0.49197607191949805\n",
      "Epoch 124, Loss: 0.4914699197831528\n",
      "Epoch 125, Loss: 0.49091483381312667\n",
      "Epoch 126, Loss: 0.49035147677946317\n",
      "Epoch 127, Loss: 0.4898087222825037\n",
      "Epoch 128, Loss: 0.48924014731550836\n",
      "Epoch 129, Loss: 0.48873550647212044\n",
      "Epoch 130, Loss: 0.48832307254971563\n",
      "Epoch 131, Loss: 0.48791418392821295\n",
      "Epoch 132, Loss: 0.4875902245243796\n",
      "Epoch 133, Loss: 0.48728851108037585\n",
      "Epoch 134, Loss: 0.48697062262181345\n",
      "Epoch 135, Loss: 0.4866339553030548\n",
      "Epoch 136, Loss: 0.48633446746734893\n",
      "Epoch 137, Loss: 0.4860310087358514\n",
      "Epoch 138, Loss: 0.4856786412545615\n",
      "Epoch 139, Loss: 0.48531372107004\n",
      "Epoch 140, Loss: 0.48495712985488587\n",
      "Epoch 141, Loss: 0.4846131600652309\n",
      "Epoch 142, Loss: 0.48425966858652364\n",
      "Epoch 143, Loss: 0.4838914149236435\n",
      "Epoch 144, Loss: 0.48352437546282767\n",
      "Epoch 145, Loss: 0.4832299911937133\n",
      "Epoch 146, Loss: 0.4832586529465723\n",
      "Epoch 147, Loss: 0.48356744143113517\n",
      "Epoch 148, Loss: 0.4828108019011922\n",
      "Epoch 149, Loss: 0.4818912939738211\n",
      "Epoch 150, Loss: 0.48239416745575797\n",
      "Epoch 151, Loss: 0.4814952491721961\n",
      "Epoch 152, Loss: 0.4811822818820502\n",
      "Epoch 153, Loss: 0.4812753156716337\n",
      "Epoch 154, Loss: 0.4804387742282827\n",
      "Epoch 155, Loss: 0.48058189052417905\n",
      "Epoch 156, Loss: 0.4801288115497346\n",
      "Epoch 157, Loss: 0.4797465937246525\n",
      "Epoch 158, Loss: 0.4797622479529733\n",
      "Epoch 159, Loss: 0.4791664679844762\n",
      "Epoch 160, Loss: 0.47904359923301915\n",
      "Epoch 161, Loss: 0.4788483533353555\n",
      "Epoch 162, Loss: 0.4783709093142749\n",
      "Epoch 163, Loss: 0.4783027642591491\n",
      "Epoch 164, Loss: 0.4780003946284736\n",
      "Epoch 165, Loss: 0.47766356610156796\n",
      "Epoch 166, Loss: 0.47756940749162413\n",
      "Epoch 167, Loss: 0.4772720526560437\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.22623794262792876\n",
      "Valence RMSE: 0.21196849964508677\n",
      "Arousal RMSE: 0.23965927590487046\n",
      "Test R^2 score: tensor([0.4555, 0.4313], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.3280, 0.2982], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4433835834012426\n",
      "Num of epochs: 168\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Epoch 121, Loss: 0.49295912913079615\n",
      "Epoch 122, Loss: 0.4924773619139838\n",
      "Epoch 123, Loss: 0.49197607191949805\n",
      "Epoch 124, Loss: 0.4914699197831528\n",
      "Epoch 125, Loss: 0.49091483381312667\n",
      "Epoch 126, Loss: 0.49035147677946317\n",
      "Epoch 127, Loss: 0.4898087222825037\n",
      "Epoch 128, Loss: 0.48924014731550836\n",
      "Epoch 129, Loss: 0.48873550647212044\n",
      "Epoch 130, Loss: 0.48832307254971563\n",
      "Epoch 131, Loss: 0.48791418392821295\n",
      "Epoch 132, Loss: 0.4875902245243796\n",
      "Epoch 133, Loss: 0.48728851108037585\n",
      "Epoch 134, Loss: 0.48697062262181345\n",
      "Epoch 135, Loss: 0.4866339553030548\n",
      "Epoch 136, Loss: 0.48633446746734893\n",
      "Epoch 137, Loss: 0.4860310087358514\n",
      "Epoch 138, Loss: 0.4856786412545615\n",
      "Epoch 139, Loss: 0.48531372107004\n",
      "Epoch 140, Loss: 0.48495712985488587\n",
      "Epoch 141, Loss: 0.4846131600652309\n",
      "Epoch 142, Loss: 0.48425966858652364\n",
      "Epoch 143, Loss: 0.4838914149236435\n",
      "Epoch 144, Loss: 0.48352437546282767\n",
      "Epoch 145, Loss: 0.4832299911937133\n",
      "Epoch 146, Loss: 0.4832586529465723\n",
      "Epoch 147, Loss: 0.48356744143113517\n",
      "Epoch 148, Loss: 0.4828108019011922\n",
      "Epoch 149, Loss: 0.4818912939738211\n",
      "Epoch 150, Loss: 0.48239416745575797\n",
      "Epoch 151, Loss: 0.4814952491721961\n",
      "Epoch 152, Loss: 0.4811822818820502\n",
      "Epoch 153, Loss: 0.4812753156716337\n",
      "Epoch 154, Loss: 0.4804387742282827\n",
      "Epoch 155, Loss: 0.48058189052417905\n",
      "Epoch 156, Loss: 0.4801288115497346\n",
      "Epoch 157, Loss: 0.4797465937246525\n",
      "Epoch 158, Loss: 0.4797622479529733\n",
      "Epoch 159, Loss: 0.4791664679844762\n",
      "Epoch 160, Loss: 0.47904359923301915\n",
      "Epoch 161, Loss: 0.4788483533353555\n",
      "Epoch 162, Loss: 0.4783709093142749\n",
      "Epoch 163, Loss: 0.4783027642591491\n",
      "Epoch 164, Loss: 0.4780003946284736\n",
      "Epoch 165, Loss: 0.47766356610156796\n",
      "Epoch 166, Loss: 0.47756940749162413\n",
      "Epoch 167, Loss: 0.4772720526560437\n",
      "Epoch 168, Loss: 0.477001191512605\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.22510985344720832\n",
      "Valence RMSE: 0.2110531180903129\n",
      "Arousal RMSE: 0.2383389888004136\n",
      "Test R^2 score: tensor([0.4602, 0.4375], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.3338, 0.3059], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.44885442983150586\n",
      "Num of epochs: 169\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Epoch 121, Loss: 0.49295912913079615\n",
      "Epoch 122, Loss: 0.4924773619139838\n",
      "Epoch 123, Loss: 0.49197607191949805\n",
      "Epoch 124, Loss: 0.4914699197831528\n",
      "Epoch 125, Loss: 0.49091483381312667\n",
      "Epoch 126, Loss: 0.49035147677946317\n",
      "Epoch 127, Loss: 0.4898087222825037\n",
      "Epoch 128, Loss: 0.48924014731550836\n",
      "Epoch 129, Loss: 0.48873550647212044\n",
      "Epoch 130, Loss: 0.48832307254971563\n",
      "Epoch 131, Loss: 0.48791418392821295\n",
      "Epoch 132, Loss: 0.4875902245243796\n",
      "Epoch 133, Loss: 0.48728851108037585\n",
      "Epoch 134, Loss: 0.48697062262181345\n",
      "Epoch 135, Loss: 0.4866339553030548\n",
      "Epoch 136, Loss: 0.48633446746734893\n",
      "Epoch 137, Loss: 0.4860310087358514\n",
      "Epoch 138, Loss: 0.4856786412545615\n",
      "Epoch 139, Loss: 0.48531372107004\n",
      "Epoch 140, Loss: 0.48495712985488587\n",
      "Epoch 141, Loss: 0.4846131600652309\n",
      "Epoch 142, Loss: 0.48425966858652364\n",
      "Epoch 143, Loss: 0.4838914149236435\n",
      "Epoch 144, Loss: 0.48352437546282767\n",
      "Epoch 145, Loss: 0.4832299911937133\n",
      "Epoch 146, Loss: 0.4832586529465723\n",
      "Epoch 147, Loss: 0.48356744143113517\n",
      "Epoch 148, Loss: 0.4828108019011922\n",
      "Epoch 149, Loss: 0.4818912939738211\n",
      "Epoch 150, Loss: 0.48239416745575797\n",
      "Epoch 151, Loss: 0.4814952491721961\n",
      "Epoch 152, Loss: 0.4811822818820502\n",
      "Epoch 153, Loss: 0.4812753156716337\n",
      "Epoch 154, Loss: 0.4804387742282827\n",
      "Epoch 155, Loss: 0.48058189052417905\n",
      "Epoch 156, Loss: 0.4801288115497346\n",
      "Epoch 157, Loss: 0.4797465937246525\n",
      "Epoch 158, Loss: 0.4797622479529733\n",
      "Epoch 159, Loss: 0.4791664679844762\n",
      "Epoch 160, Loss: 0.47904359923301915\n",
      "Epoch 161, Loss: 0.4788483533353555\n",
      "Epoch 162, Loss: 0.4783709093142749\n",
      "Epoch 163, Loss: 0.4783027642591491\n",
      "Epoch 164, Loss: 0.4780003946284736\n",
      "Epoch 165, Loss: 0.47766356610156796\n",
      "Epoch 166, Loss: 0.47756940749162413\n",
      "Epoch 167, Loss: 0.4772720526560437\n",
      "Epoch 168, Loss: 0.477001191512605\n",
      "Epoch 169, Loss: 0.47690651155802954\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2260782013768788\n",
      "Valence RMSE: 0.21185667061247648\n",
      "Arousal RMSE: 0.23945658769932762\n",
      "Test R^2 score: tensor([0.4561, 0.4322], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.3288, 0.2994], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.44415156081348894\n",
      "Num of epochs: 170\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Epoch 121, Loss: 0.49295912913079615\n",
      "Epoch 122, Loss: 0.4924773619139838\n",
      "Epoch 123, Loss: 0.49197607191949805\n",
      "Epoch 124, Loss: 0.4914699197831528\n",
      "Epoch 125, Loss: 0.49091483381312667\n",
      "Epoch 126, Loss: 0.49035147677946317\n",
      "Epoch 127, Loss: 0.4898087222825037\n",
      "Epoch 128, Loss: 0.48924014731550836\n",
      "Epoch 129, Loss: 0.48873550647212044\n",
      "Epoch 130, Loss: 0.48832307254971563\n",
      "Epoch 131, Loss: 0.48791418392821295\n",
      "Epoch 132, Loss: 0.4875902245243796\n",
      "Epoch 133, Loss: 0.48728851108037585\n",
      "Epoch 134, Loss: 0.48697062262181345\n",
      "Epoch 135, Loss: 0.4866339553030548\n",
      "Epoch 136, Loss: 0.48633446746734893\n",
      "Epoch 137, Loss: 0.4860310087358514\n",
      "Epoch 138, Loss: 0.4856786412545615\n",
      "Epoch 139, Loss: 0.48531372107004\n",
      "Epoch 140, Loss: 0.48495712985488587\n",
      "Epoch 141, Loss: 0.4846131600652309\n",
      "Epoch 142, Loss: 0.48425966858652364\n",
      "Epoch 143, Loss: 0.4838914149236435\n",
      "Epoch 144, Loss: 0.48352437546282767\n",
      "Epoch 145, Loss: 0.4832299911937133\n",
      "Epoch 146, Loss: 0.4832586529465723\n",
      "Epoch 147, Loss: 0.48356744143113517\n",
      "Epoch 148, Loss: 0.4828108019011922\n",
      "Epoch 149, Loss: 0.4818912939738211\n",
      "Epoch 150, Loss: 0.48239416745575797\n",
      "Epoch 151, Loss: 0.4814952491721961\n",
      "Epoch 152, Loss: 0.4811822818820502\n",
      "Epoch 153, Loss: 0.4812753156716337\n",
      "Epoch 154, Loss: 0.4804387742282827\n",
      "Epoch 155, Loss: 0.48058189052417905\n",
      "Epoch 156, Loss: 0.4801288115497346\n",
      "Epoch 157, Loss: 0.4797465937246525\n",
      "Epoch 158, Loss: 0.4797622479529733\n",
      "Epoch 159, Loss: 0.4791664679844762\n",
      "Epoch 160, Loss: 0.47904359923301915\n",
      "Epoch 161, Loss: 0.4788483533353555\n",
      "Epoch 162, Loss: 0.4783709093142749\n",
      "Epoch 163, Loss: 0.4783027642591491\n",
      "Epoch 164, Loss: 0.4780003946284736\n",
      "Epoch 165, Loss: 0.47766356610156796\n",
      "Epoch 166, Loss: 0.47756940749162413\n",
      "Epoch 167, Loss: 0.4772720526560437\n",
      "Epoch 168, Loss: 0.477001191512605\n",
      "Epoch 169, Loss: 0.47690651155802954\n",
      "Epoch 170, Loss: 0.47663605231839845\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2254042425465934\n",
      "Valence RMSE: 0.21129581785059018\n",
      "Arousal RMSE: 0.23868016774515896\n",
      "Test R^2 score: tensor([0.4589, 0.4359], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.3323, 0.3039], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.44742756340107476\n",
      "Num of epochs: 171\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Epoch 121, Loss: 0.49295912913079615\n",
      "Epoch 122, Loss: 0.4924773619139838\n",
      "Epoch 123, Loss: 0.49197607191949805\n",
      "Epoch 124, Loss: 0.4914699197831528\n",
      "Epoch 125, Loss: 0.49091483381312667\n",
      "Epoch 126, Loss: 0.49035147677946317\n",
      "Epoch 127, Loss: 0.4898087222825037\n",
      "Epoch 128, Loss: 0.48924014731550836\n",
      "Epoch 129, Loss: 0.48873550647212044\n",
      "Epoch 130, Loss: 0.48832307254971563\n",
      "Epoch 131, Loss: 0.48791418392821295\n",
      "Epoch 132, Loss: 0.4875902245243796\n",
      "Epoch 133, Loss: 0.48728851108037585\n",
      "Epoch 134, Loss: 0.48697062262181345\n",
      "Epoch 135, Loss: 0.4866339553030548\n",
      "Epoch 136, Loss: 0.48633446746734893\n",
      "Epoch 137, Loss: 0.4860310087358514\n",
      "Epoch 138, Loss: 0.4856786412545615\n",
      "Epoch 139, Loss: 0.48531372107004\n",
      "Epoch 140, Loss: 0.48495712985488587\n",
      "Epoch 141, Loss: 0.4846131600652309\n",
      "Epoch 142, Loss: 0.48425966858652364\n",
      "Epoch 143, Loss: 0.4838914149236435\n",
      "Epoch 144, Loss: 0.48352437546282767\n",
      "Epoch 145, Loss: 0.4832299911937133\n",
      "Epoch 146, Loss: 0.4832586529465723\n",
      "Epoch 147, Loss: 0.48356744143113517\n",
      "Epoch 148, Loss: 0.4828108019011922\n",
      "Epoch 149, Loss: 0.4818912939738211\n",
      "Epoch 150, Loss: 0.48239416745575797\n",
      "Epoch 151, Loss: 0.4814952491721961\n",
      "Epoch 152, Loss: 0.4811822818820502\n",
      "Epoch 153, Loss: 0.4812753156716337\n",
      "Epoch 154, Loss: 0.4804387742282827\n",
      "Epoch 155, Loss: 0.48058189052417905\n",
      "Epoch 156, Loss: 0.4801288115497346\n",
      "Epoch 157, Loss: 0.4797465937246525\n",
      "Epoch 158, Loss: 0.4797622479529733\n",
      "Epoch 159, Loss: 0.4791664679844762\n",
      "Epoch 160, Loss: 0.47904359923301915\n",
      "Epoch 161, Loss: 0.4788483533353555\n",
      "Epoch 162, Loss: 0.4783709093142749\n",
      "Epoch 163, Loss: 0.4783027642591491\n",
      "Epoch 164, Loss: 0.4780003946284736\n",
      "Epoch 165, Loss: 0.47766356610156796\n",
      "Epoch 166, Loss: 0.47756940749162413\n",
      "Epoch 167, Loss: 0.4772720526560437\n",
      "Epoch 168, Loss: 0.477001191512605\n",
      "Epoch 169, Loss: 0.47690651155802954\n",
      "Epoch 170, Loss: 0.47663605231839845\n",
      "Epoch 171, Loss: 0.4763715079887003\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.22461163589175087\n",
      "Valence RMSE: 0.21067485568912317\n",
      "Arousal RMSE: 0.23773278935877576\n",
      "Test R^2 score: tensor([0.4621, 0.4404], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.3362, 0.3094], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4512498491392079\n",
      "Num of epochs: 172\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Epoch 121, Loss: 0.49295912913079615\n",
      "Epoch 122, Loss: 0.4924773619139838\n",
      "Epoch 123, Loss: 0.49197607191949805\n",
      "Epoch 124, Loss: 0.4914699197831528\n",
      "Epoch 125, Loss: 0.49091483381312667\n",
      "Epoch 126, Loss: 0.49035147677946317\n",
      "Epoch 127, Loss: 0.4898087222825037\n",
      "Epoch 128, Loss: 0.48924014731550836\n",
      "Epoch 129, Loss: 0.48873550647212044\n",
      "Epoch 130, Loss: 0.48832307254971563\n",
      "Epoch 131, Loss: 0.48791418392821295\n",
      "Epoch 132, Loss: 0.4875902245243796\n",
      "Epoch 133, Loss: 0.48728851108037585\n",
      "Epoch 134, Loss: 0.48697062262181345\n",
      "Epoch 135, Loss: 0.4866339553030548\n",
      "Epoch 136, Loss: 0.48633446746734893\n",
      "Epoch 137, Loss: 0.4860310087358514\n",
      "Epoch 138, Loss: 0.4856786412545615\n",
      "Epoch 139, Loss: 0.48531372107004\n",
      "Epoch 140, Loss: 0.48495712985488587\n",
      "Epoch 141, Loss: 0.4846131600652309\n",
      "Epoch 142, Loss: 0.48425966858652364\n",
      "Epoch 143, Loss: 0.4838914149236435\n",
      "Epoch 144, Loss: 0.48352437546282767\n",
      "Epoch 145, Loss: 0.4832299911937133\n",
      "Epoch 146, Loss: 0.4832586529465723\n",
      "Epoch 147, Loss: 0.48356744143113517\n",
      "Epoch 148, Loss: 0.4828108019011922\n",
      "Epoch 149, Loss: 0.4818912939738211\n",
      "Epoch 150, Loss: 0.48239416745575797\n",
      "Epoch 151, Loss: 0.4814952491721961\n",
      "Epoch 152, Loss: 0.4811822818820502\n",
      "Epoch 153, Loss: 0.4812753156716337\n",
      "Epoch 154, Loss: 0.4804387742282827\n",
      "Epoch 155, Loss: 0.48058189052417905\n",
      "Epoch 156, Loss: 0.4801288115497346\n",
      "Epoch 157, Loss: 0.4797465937246525\n",
      "Epoch 158, Loss: 0.4797622479529733\n",
      "Epoch 159, Loss: 0.4791664679844762\n",
      "Epoch 160, Loss: 0.47904359923301915\n",
      "Epoch 161, Loss: 0.4788483533353555\n",
      "Epoch 162, Loss: 0.4783709093142749\n",
      "Epoch 163, Loss: 0.4783027642591491\n",
      "Epoch 164, Loss: 0.4780003946284736\n",
      "Epoch 165, Loss: 0.47766356610156796\n",
      "Epoch 166, Loss: 0.47756940749162413\n",
      "Epoch 167, Loss: 0.4772720526560437\n",
      "Epoch 168, Loss: 0.477001191512605\n",
      "Epoch 169, Loss: 0.47690651155802954\n",
      "Epoch 170, Loss: 0.47663605231839845\n",
      "Epoch 171, Loss: 0.4763715079887003\n",
      "Epoch 172, Loss: 0.4762479807342815\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2252949268178937\n",
      "Valence RMSE: 0.21122233626014125\n",
      "Arousal RMSE: 0.23853874478702194\n",
      "Test R^2 score: tensor([0.4593, 0.4366], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.3328, 0.3047], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4479498252501376\n",
      "Num of epochs: 173\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Epoch 121, Loss: 0.49295912913079615\n",
      "Epoch 122, Loss: 0.4924773619139838\n",
      "Epoch 123, Loss: 0.49197607191949805\n",
      "Epoch 124, Loss: 0.4914699197831528\n",
      "Epoch 125, Loss: 0.49091483381312667\n",
      "Epoch 126, Loss: 0.49035147677946317\n",
      "Epoch 127, Loss: 0.4898087222825037\n",
      "Epoch 128, Loss: 0.48924014731550836\n",
      "Epoch 129, Loss: 0.48873550647212044\n",
      "Epoch 130, Loss: 0.48832307254971563\n",
      "Epoch 131, Loss: 0.48791418392821295\n",
      "Epoch 132, Loss: 0.4875902245243796\n",
      "Epoch 133, Loss: 0.48728851108037585\n",
      "Epoch 134, Loss: 0.48697062262181345\n",
      "Epoch 135, Loss: 0.4866339553030548\n",
      "Epoch 136, Loss: 0.48633446746734893\n",
      "Epoch 137, Loss: 0.4860310087358514\n",
      "Epoch 138, Loss: 0.4856786412545615\n",
      "Epoch 139, Loss: 0.48531372107004\n",
      "Epoch 140, Loss: 0.48495712985488587\n",
      "Epoch 141, Loss: 0.4846131600652309\n",
      "Epoch 142, Loss: 0.48425966858652364\n",
      "Epoch 143, Loss: 0.4838914149236435\n",
      "Epoch 144, Loss: 0.48352437546282767\n",
      "Epoch 145, Loss: 0.4832299911937133\n",
      "Epoch 146, Loss: 0.4832586529465723\n",
      "Epoch 147, Loss: 0.48356744143113517\n",
      "Epoch 148, Loss: 0.4828108019011922\n",
      "Epoch 149, Loss: 0.4818912939738211\n",
      "Epoch 150, Loss: 0.48239416745575797\n",
      "Epoch 151, Loss: 0.4814952491721961\n",
      "Epoch 152, Loss: 0.4811822818820502\n",
      "Epoch 153, Loss: 0.4812753156716337\n",
      "Epoch 154, Loss: 0.4804387742282827\n",
      "Epoch 155, Loss: 0.48058189052417905\n",
      "Epoch 156, Loss: 0.4801288115497346\n",
      "Epoch 157, Loss: 0.4797465937246525\n",
      "Epoch 158, Loss: 0.4797622479529733\n",
      "Epoch 159, Loss: 0.4791664679844762\n",
      "Epoch 160, Loss: 0.47904359923301915\n",
      "Epoch 161, Loss: 0.4788483533353555\n",
      "Epoch 162, Loss: 0.4783709093142749\n",
      "Epoch 163, Loss: 0.4783027642591491\n",
      "Epoch 164, Loss: 0.4780003946284736\n",
      "Epoch 165, Loss: 0.47766356610156796\n",
      "Epoch 166, Loss: 0.47756940749162413\n",
      "Epoch 167, Loss: 0.4772720526560437\n",
      "Epoch 168, Loss: 0.477001191512605\n",
      "Epoch 169, Loss: 0.47690651155802954\n",
      "Epoch 170, Loss: 0.47663605231839845\n",
      "Epoch 171, Loss: 0.4763715079887003\n",
      "Epoch 172, Loss: 0.4762479807342815\n",
      "Epoch 173, Loss: 0.47603155482909787\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.22463158644289646\n",
      "Valence RMSE: 0.2107011652734872\n",
      "Arousal RMSE: 0.2377471728708873\n",
      "Test R^2 score: tensor([0.4620, 0.4403], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.3361, 0.3093], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4511488130123389\n",
      "Num of epochs: 174\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Epoch 121, Loss: 0.49295912913079615\n",
      "Epoch 122, Loss: 0.4924773619139838\n",
      "Epoch 123, Loss: 0.49197607191949805\n",
      "Epoch 124, Loss: 0.4914699197831528\n",
      "Epoch 125, Loss: 0.49091483381312667\n",
      "Epoch 126, Loss: 0.49035147677946317\n",
      "Epoch 127, Loss: 0.4898087222825037\n",
      "Epoch 128, Loss: 0.48924014731550836\n",
      "Epoch 129, Loss: 0.48873550647212044\n",
      "Epoch 130, Loss: 0.48832307254971563\n",
      "Epoch 131, Loss: 0.48791418392821295\n",
      "Epoch 132, Loss: 0.4875902245243796\n",
      "Epoch 133, Loss: 0.48728851108037585\n",
      "Epoch 134, Loss: 0.48697062262181345\n",
      "Epoch 135, Loss: 0.4866339553030548\n",
      "Epoch 136, Loss: 0.48633446746734893\n",
      "Epoch 137, Loss: 0.4860310087358514\n",
      "Epoch 138, Loss: 0.4856786412545615\n",
      "Epoch 139, Loss: 0.48531372107004\n",
      "Epoch 140, Loss: 0.48495712985488587\n",
      "Epoch 141, Loss: 0.4846131600652309\n",
      "Epoch 142, Loss: 0.48425966858652364\n",
      "Epoch 143, Loss: 0.4838914149236435\n",
      "Epoch 144, Loss: 0.48352437546282767\n",
      "Epoch 145, Loss: 0.4832299911937133\n",
      "Epoch 146, Loss: 0.4832586529465723\n",
      "Epoch 147, Loss: 0.48356744143113517\n",
      "Epoch 148, Loss: 0.4828108019011922\n",
      "Epoch 149, Loss: 0.4818912939738211\n",
      "Epoch 150, Loss: 0.48239416745575797\n",
      "Epoch 151, Loss: 0.4814952491721961\n",
      "Epoch 152, Loss: 0.4811822818820502\n",
      "Epoch 153, Loss: 0.4812753156716337\n",
      "Epoch 154, Loss: 0.4804387742282827\n",
      "Epoch 155, Loss: 0.48058189052417905\n",
      "Epoch 156, Loss: 0.4801288115497346\n",
      "Epoch 157, Loss: 0.4797465937246525\n",
      "Epoch 158, Loss: 0.4797622479529733\n",
      "Epoch 159, Loss: 0.4791664679844762\n",
      "Epoch 160, Loss: 0.47904359923301915\n",
      "Epoch 161, Loss: 0.4788483533353555\n",
      "Epoch 162, Loss: 0.4783709093142749\n",
      "Epoch 163, Loss: 0.4783027642591491\n",
      "Epoch 164, Loss: 0.4780003946284736\n",
      "Epoch 165, Loss: 0.47766356610156796\n",
      "Epoch 166, Loss: 0.47756940749162413\n",
      "Epoch 167, Loss: 0.4772720526560437\n",
      "Epoch 168, Loss: 0.477001191512605\n",
      "Epoch 169, Loss: 0.47690651155802954\n",
      "Epoch 170, Loss: 0.47663605231839845\n",
      "Epoch 171, Loss: 0.4763715079887003\n",
      "Epoch 172, Loss: 0.4762479807342815\n",
      "Epoch 173, Loss: 0.47603155482909787\n",
      "Epoch 174, Loss: 0.47580085926956445\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2241624678067744\n",
      "Valence RMSE: 0.21036111773459795\n",
      "Arousal RMSE: 0.2371620207619613\n",
      "Test R^2 score: tensor([0.4637, 0.4431], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.3382, 0.3127], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.45339222848580674\n",
      "Num of epochs: 175\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Epoch 121, Loss: 0.49295912913079615\n",
      "Epoch 122, Loss: 0.4924773619139838\n",
      "Epoch 123, Loss: 0.49197607191949805\n",
      "Epoch 124, Loss: 0.4914699197831528\n",
      "Epoch 125, Loss: 0.49091483381312667\n",
      "Epoch 126, Loss: 0.49035147677946317\n",
      "Epoch 127, Loss: 0.4898087222825037\n",
      "Epoch 128, Loss: 0.48924014731550836\n",
      "Epoch 129, Loss: 0.48873550647212044\n",
      "Epoch 130, Loss: 0.48832307254971563\n",
      "Epoch 131, Loss: 0.48791418392821295\n",
      "Epoch 132, Loss: 0.4875902245243796\n",
      "Epoch 133, Loss: 0.48728851108037585\n",
      "Epoch 134, Loss: 0.48697062262181345\n",
      "Epoch 135, Loss: 0.4866339553030548\n",
      "Epoch 136, Loss: 0.48633446746734893\n",
      "Epoch 137, Loss: 0.4860310087358514\n",
      "Epoch 138, Loss: 0.4856786412545615\n",
      "Epoch 139, Loss: 0.48531372107004\n",
      "Epoch 140, Loss: 0.48495712985488587\n",
      "Epoch 141, Loss: 0.4846131600652309\n",
      "Epoch 142, Loss: 0.48425966858652364\n",
      "Epoch 143, Loss: 0.4838914149236435\n",
      "Epoch 144, Loss: 0.48352437546282767\n",
      "Epoch 145, Loss: 0.4832299911937133\n",
      "Epoch 146, Loss: 0.4832586529465723\n",
      "Epoch 147, Loss: 0.48356744143113517\n",
      "Epoch 148, Loss: 0.4828108019011922\n",
      "Epoch 149, Loss: 0.4818912939738211\n",
      "Epoch 150, Loss: 0.48239416745575797\n",
      "Epoch 151, Loss: 0.4814952491721961\n",
      "Epoch 152, Loss: 0.4811822818820502\n",
      "Epoch 153, Loss: 0.4812753156716337\n",
      "Epoch 154, Loss: 0.4804387742282827\n",
      "Epoch 155, Loss: 0.48058189052417905\n",
      "Epoch 156, Loss: 0.4801288115497346\n",
      "Epoch 157, Loss: 0.4797465937246525\n",
      "Epoch 158, Loss: 0.4797622479529733\n",
      "Epoch 159, Loss: 0.4791664679844762\n",
      "Epoch 160, Loss: 0.47904359923301915\n",
      "Epoch 161, Loss: 0.4788483533353555\n",
      "Epoch 162, Loss: 0.4783709093142749\n",
      "Epoch 163, Loss: 0.4783027642591491\n",
      "Epoch 164, Loss: 0.4780003946284736\n",
      "Epoch 165, Loss: 0.47766356610156796\n",
      "Epoch 166, Loss: 0.47756940749162413\n",
      "Epoch 167, Loss: 0.4772720526560437\n",
      "Epoch 168, Loss: 0.477001191512605\n",
      "Epoch 169, Loss: 0.47690651155802954\n",
      "Epoch 170, Loss: 0.47663605231839845\n",
      "Epoch 171, Loss: 0.4763715079887003\n",
      "Epoch 172, Loss: 0.4762479807342815\n",
      "Epoch 173, Loss: 0.47603155482909787\n",
      "Epoch 174, Loss: 0.47580085926956445\n",
      "Epoch 175, Loss: 0.4756396378860013\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.22460693445653027\n",
      "Valence RMSE: 0.21074413351366966\n",
      "Arousal RMSE: 0.23766249220584376\n",
      "Test R^2 score: tensor([0.4618, 0.4407], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.3358, 0.3098], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.45123839395130094\n",
      "Num of epochs: 176\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Epoch 121, Loss: 0.49295912913079615\n",
      "Epoch 122, Loss: 0.4924773619139838\n",
      "Epoch 123, Loss: 0.49197607191949805\n",
      "Epoch 124, Loss: 0.4914699197831528\n",
      "Epoch 125, Loss: 0.49091483381312667\n",
      "Epoch 126, Loss: 0.49035147677946317\n",
      "Epoch 127, Loss: 0.4898087222825037\n",
      "Epoch 128, Loss: 0.48924014731550836\n",
      "Epoch 129, Loss: 0.48873550647212044\n",
      "Epoch 130, Loss: 0.48832307254971563\n",
      "Epoch 131, Loss: 0.48791418392821295\n",
      "Epoch 132, Loss: 0.4875902245243796\n",
      "Epoch 133, Loss: 0.48728851108037585\n",
      "Epoch 134, Loss: 0.48697062262181345\n",
      "Epoch 135, Loss: 0.4866339553030548\n",
      "Epoch 136, Loss: 0.48633446746734893\n",
      "Epoch 137, Loss: 0.4860310087358514\n",
      "Epoch 138, Loss: 0.4856786412545615\n",
      "Epoch 139, Loss: 0.48531372107004\n",
      "Epoch 140, Loss: 0.48495712985488587\n",
      "Epoch 141, Loss: 0.4846131600652309\n",
      "Epoch 142, Loss: 0.48425966858652364\n",
      "Epoch 143, Loss: 0.4838914149236435\n",
      "Epoch 144, Loss: 0.48352437546282767\n",
      "Epoch 145, Loss: 0.4832299911937133\n",
      "Epoch 146, Loss: 0.4832586529465723\n",
      "Epoch 147, Loss: 0.48356744143113517\n",
      "Epoch 148, Loss: 0.4828108019011922\n",
      "Epoch 149, Loss: 0.4818912939738211\n",
      "Epoch 150, Loss: 0.48239416745575797\n",
      "Epoch 151, Loss: 0.4814952491721961\n",
      "Epoch 152, Loss: 0.4811822818820502\n",
      "Epoch 153, Loss: 0.4812753156716337\n",
      "Epoch 154, Loss: 0.4804387742282827\n",
      "Epoch 155, Loss: 0.48058189052417905\n",
      "Epoch 156, Loss: 0.4801288115497346\n",
      "Epoch 157, Loss: 0.4797465937246525\n",
      "Epoch 158, Loss: 0.4797622479529733\n",
      "Epoch 159, Loss: 0.4791664679844762\n",
      "Epoch 160, Loss: 0.47904359923301915\n",
      "Epoch 161, Loss: 0.4788483533353555\n",
      "Epoch 162, Loss: 0.4783709093142749\n",
      "Epoch 163, Loss: 0.4783027642591491\n",
      "Epoch 164, Loss: 0.4780003946284736\n",
      "Epoch 165, Loss: 0.47766356610156796\n",
      "Epoch 166, Loss: 0.47756940749162413\n",
      "Epoch 167, Loss: 0.4772720526560437\n",
      "Epoch 168, Loss: 0.477001191512605\n",
      "Epoch 169, Loss: 0.47690651155802954\n",
      "Epoch 170, Loss: 0.47663605231839845\n",
      "Epoch 171, Loss: 0.4763715079887003\n",
      "Epoch 172, Loss: 0.4762479807342815\n",
      "Epoch 173, Loss: 0.47603155482909787\n",
      "Epoch 174, Loss: 0.47580085926956445\n",
      "Epoch 175, Loss: 0.4756396378860013\n",
      "Epoch 176, Loss: 0.4754627232666358\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.22395351141878067\n",
      "Valence RMSE: 0.21025416013139617\n",
      "Arousal RMSE: 0.23686185573250298\n",
      "Test R^2 score: tensor([0.4643, 0.4445], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.3389, 0.3145], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4543692654053203\n",
      "Num of epochs: 177\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Epoch 121, Loss: 0.49295912913079615\n",
      "Epoch 122, Loss: 0.4924773619139838\n",
      "Epoch 123, Loss: 0.49197607191949805\n",
      "Epoch 124, Loss: 0.4914699197831528\n",
      "Epoch 125, Loss: 0.49091483381312667\n",
      "Epoch 126, Loss: 0.49035147677946317\n",
      "Epoch 127, Loss: 0.4898087222825037\n",
      "Epoch 128, Loss: 0.48924014731550836\n",
      "Epoch 129, Loss: 0.48873550647212044\n",
      "Epoch 130, Loss: 0.48832307254971563\n",
      "Epoch 131, Loss: 0.48791418392821295\n",
      "Epoch 132, Loss: 0.4875902245243796\n",
      "Epoch 133, Loss: 0.48728851108037585\n",
      "Epoch 134, Loss: 0.48697062262181345\n",
      "Epoch 135, Loss: 0.4866339553030548\n",
      "Epoch 136, Loss: 0.48633446746734893\n",
      "Epoch 137, Loss: 0.4860310087358514\n",
      "Epoch 138, Loss: 0.4856786412545615\n",
      "Epoch 139, Loss: 0.48531372107004\n",
      "Epoch 140, Loss: 0.48495712985488587\n",
      "Epoch 141, Loss: 0.4846131600652309\n",
      "Epoch 142, Loss: 0.48425966858652364\n",
      "Epoch 143, Loss: 0.4838914149236435\n",
      "Epoch 144, Loss: 0.48352437546282767\n",
      "Epoch 145, Loss: 0.4832299911937133\n",
      "Epoch 146, Loss: 0.4832586529465723\n",
      "Epoch 147, Loss: 0.48356744143113517\n",
      "Epoch 148, Loss: 0.4828108019011922\n",
      "Epoch 149, Loss: 0.4818912939738211\n",
      "Epoch 150, Loss: 0.48239416745575797\n",
      "Epoch 151, Loss: 0.4814952491721961\n",
      "Epoch 152, Loss: 0.4811822818820502\n",
      "Epoch 153, Loss: 0.4812753156716337\n",
      "Epoch 154, Loss: 0.4804387742282827\n",
      "Epoch 155, Loss: 0.48058189052417905\n",
      "Epoch 156, Loss: 0.4801288115497346\n",
      "Epoch 157, Loss: 0.4797465937246525\n",
      "Epoch 158, Loss: 0.4797622479529733\n",
      "Epoch 159, Loss: 0.4791664679844762\n",
      "Epoch 160, Loss: 0.47904359923301915\n",
      "Epoch 161, Loss: 0.4788483533353555\n",
      "Epoch 162, Loss: 0.4783709093142749\n",
      "Epoch 163, Loss: 0.4783027642591491\n",
      "Epoch 164, Loss: 0.4780003946284736\n",
      "Epoch 165, Loss: 0.47766356610156796\n",
      "Epoch 166, Loss: 0.47756940749162413\n",
      "Epoch 167, Loss: 0.4772720526560437\n",
      "Epoch 168, Loss: 0.477001191512605\n",
      "Epoch 169, Loss: 0.47690651155802954\n",
      "Epoch 170, Loss: 0.47663605231839845\n",
      "Epoch 171, Loss: 0.4763715079887003\n",
      "Epoch 172, Loss: 0.4762479807342815\n",
      "Epoch 173, Loss: 0.47603155482909787\n",
      "Epoch 174, Loss: 0.47580085926956445\n",
      "Epoch 175, Loss: 0.4756396378860013\n",
      "Epoch 176, Loss: 0.4754627232666358\n",
      "Epoch 177, Loss: 0.4752513954316692\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.22376699710709358\n",
      "Valence RMSE: 0.21014545602126727\n",
      "Arousal RMSE: 0.23660563243985908\n",
      "Test R^2 score: tensor([0.4648, 0.4457], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.3396, 0.3159], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.45524678264459073\n",
      "Num of epochs: 178\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Epoch 121, Loss: 0.49295912913079615\n",
      "Epoch 122, Loss: 0.4924773619139838\n",
      "Epoch 123, Loss: 0.49197607191949805\n",
      "Epoch 124, Loss: 0.4914699197831528\n",
      "Epoch 125, Loss: 0.49091483381312667\n",
      "Epoch 126, Loss: 0.49035147677946317\n",
      "Epoch 127, Loss: 0.4898087222825037\n",
      "Epoch 128, Loss: 0.48924014731550836\n",
      "Epoch 129, Loss: 0.48873550647212044\n",
      "Epoch 130, Loss: 0.48832307254971563\n",
      "Epoch 131, Loss: 0.48791418392821295\n",
      "Epoch 132, Loss: 0.4875902245243796\n",
      "Epoch 133, Loss: 0.48728851108037585\n",
      "Epoch 134, Loss: 0.48697062262181345\n",
      "Epoch 135, Loss: 0.4866339553030548\n",
      "Epoch 136, Loss: 0.48633446746734893\n",
      "Epoch 137, Loss: 0.4860310087358514\n",
      "Epoch 138, Loss: 0.4856786412545615\n",
      "Epoch 139, Loss: 0.48531372107004\n",
      "Epoch 140, Loss: 0.48495712985488587\n",
      "Epoch 141, Loss: 0.4846131600652309\n",
      "Epoch 142, Loss: 0.48425966858652364\n",
      "Epoch 143, Loss: 0.4838914149236435\n",
      "Epoch 144, Loss: 0.48352437546282767\n",
      "Epoch 145, Loss: 0.4832299911937133\n",
      "Epoch 146, Loss: 0.4832586529465723\n",
      "Epoch 147, Loss: 0.48356744143113517\n",
      "Epoch 148, Loss: 0.4828108019011922\n",
      "Epoch 149, Loss: 0.4818912939738211\n",
      "Epoch 150, Loss: 0.48239416745575797\n",
      "Epoch 151, Loss: 0.4814952491721961\n",
      "Epoch 152, Loss: 0.4811822818820502\n",
      "Epoch 153, Loss: 0.4812753156716337\n",
      "Epoch 154, Loss: 0.4804387742282827\n",
      "Epoch 155, Loss: 0.48058189052417905\n",
      "Epoch 156, Loss: 0.4801288115497346\n",
      "Epoch 157, Loss: 0.4797465937246525\n",
      "Epoch 158, Loss: 0.4797622479529733\n",
      "Epoch 159, Loss: 0.4791664679844762\n",
      "Epoch 160, Loss: 0.47904359923301915\n",
      "Epoch 161, Loss: 0.4788483533353555\n",
      "Epoch 162, Loss: 0.4783709093142749\n",
      "Epoch 163, Loss: 0.4783027642591491\n",
      "Epoch 164, Loss: 0.4780003946284736\n",
      "Epoch 165, Loss: 0.47766356610156796\n",
      "Epoch 166, Loss: 0.47756940749162413\n",
      "Epoch 167, Loss: 0.4772720526560437\n",
      "Epoch 168, Loss: 0.477001191512605\n",
      "Epoch 169, Loss: 0.47690651155802954\n",
      "Epoch 170, Loss: 0.47663605231839845\n",
      "Epoch 171, Loss: 0.4763715079887003\n",
      "Epoch 172, Loss: 0.4762479807342815\n",
      "Epoch 173, Loss: 0.47603155482909787\n",
      "Epoch 174, Loss: 0.47580085926956445\n",
      "Epoch 175, Loss: 0.4756396378860013\n",
      "Epoch 176, Loss: 0.4754627232666358\n",
      "Epoch 177, Loss: 0.4752513954316692\n",
      "Epoch 178, Loss: 0.47507140350730925\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.22409524391661087\n",
      "Valence RMSE: 0.21046428192503194\n",
      "Arousal RMSE: 0.2369433323093759\n",
      "Test R^2 score: tensor([0.4632, 0.4441], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.3375, 0.3140], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4536424706213929\n",
      "Num of epochs: 179\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Epoch 121, Loss: 0.49295912913079615\n",
      "Epoch 122, Loss: 0.4924773619139838\n",
      "Epoch 123, Loss: 0.49197607191949805\n",
      "Epoch 124, Loss: 0.4914699197831528\n",
      "Epoch 125, Loss: 0.49091483381312667\n",
      "Epoch 126, Loss: 0.49035147677946317\n",
      "Epoch 127, Loss: 0.4898087222825037\n",
      "Epoch 128, Loss: 0.48924014731550836\n",
      "Epoch 129, Loss: 0.48873550647212044\n",
      "Epoch 130, Loss: 0.48832307254971563\n",
      "Epoch 131, Loss: 0.48791418392821295\n",
      "Epoch 132, Loss: 0.4875902245243796\n",
      "Epoch 133, Loss: 0.48728851108037585\n",
      "Epoch 134, Loss: 0.48697062262181345\n",
      "Epoch 135, Loss: 0.4866339553030548\n",
      "Epoch 136, Loss: 0.48633446746734893\n",
      "Epoch 137, Loss: 0.4860310087358514\n",
      "Epoch 138, Loss: 0.4856786412545615\n",
      "Epoch 139, Loss: 0.48531372107004\n",
      "Epoch 140, Loss: 0.48495712985488587\n",
      "Epoch 141, Loss: 0.4846131600652309\n",
      "Epoch 142, Loss: 0.48425966858652364\n",
      "Epoch 143, Loss: 0.4838914149236435\n",
      "Epoch 144, Loss: 0.48352437546282767\n",
      "Epoch 145, Loss: 0.4832299911937133\n",
      "Epoch 146, Loss: 0.4832586529465723\n",
      "Epoch 147, Loss: 0.48356744143113517\n",
      "Epoch 148, Loss: 0.4828108019011922\n",
      "Epoch 149, Loss: 0.4818912939738211\n",
      "Epoch 150, Loss: 0.48239416745575797\n",
      "Epoch 151, Loss: 0.4814952491721961\n",
      "Epoch 152, Loss: 0.4811822818820502\n",
      "Epoch 153, Loss: 0.4812753156716337\n",
      "Epoch 154, Loss: 0.4804387742282827\n",
      "Epoch 155, Loss: 0.48058189052417905\n",
      "Epoch 156, Loss: 0.4801288115497346\n",
      "Epoch 157, Loss: 0.4797465937246525\n",
      "Epoch 158, Loss: 0.4797622479529733\n",
      "Epoch 159, Loss: 0.4791664679844762\n",
      "Epoch 160, Loss: 0.47904359923301915\n",
      "Epoch 161, Loss: 0.4788483533353555\n",
      "Epoch 162, Loss: 0.4783709093142749\n",
      "Epoch 163, Loss: 0.4783027642591491\n",
      "Epoch 164, Loss: 0.4780003946284736\n",
      "Epoch 165, Loss: 0.47766356610156796\n",
      "Epoch 166, Loss: 0.47756940749162413\n",
      "Epoch 167, Loss: 0.4772720526560437\n",
      "Epoch 168, Loss: 0.477001191512605\n",
      "Epoch 169, Loss: 0.47690651155802954\n",
      "Epoch 170, Loss: 0.47663605231839845\n",
      "Epoch 171, Loss: 0.4763715079887003\n",
      "Epoch 172, Loss: 0.4762479807342815\n",
      "Epoch 173, Loss: 0.47603155482909787\n",
      "Epoch 174, Loss: 0.47580085926956445\n",
      "Epoch 175, Loss: 0.4756396378860013\n",
      "Epoch 176, Loss: 0.4754627232666358\n",
      "Epoch 177, Loss: 0.4752513954316692\n",
      "Epoch 178, Loss: 0.47507140350730925\n",
      "Epoch 179, Loss: 0.47492471273973497\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.22327128582109726\n",
      "Valence RMSE: 0.2098139629899308\n",
      "Arousal RMSE: 0.23596235945351995\n",
      "Test R^2 score: tensor([0.4665, 0.4487], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.3416, 0.3197], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4575953542746602\n",
      "Num of epochs: 180\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Epoch 121, Loss: 0.49295912913079615\n",
      "Epoch 122, Loss: 0.4924773619139838\n",
      "Epoch 123, Loss: 0.49197607191949805\n",
      "Epoch 124, Loss: 0.4914699197831528\n",
      "Epoch 125, Loss: 0.49091483381312667\n",
      "Epoch 126, Loss: 0.49035147677946317\n",
      "Epoch 127, Loss: 0.4898087222825037\n",
      "Epoch 128, Loss: 0.48924014731550836\n",
      "Epoch 129, Loss: 0.48873550647212044\n",
      "Epoch 130, Loss: 0.48832307254971563\n",
      "Epoch 131, Loss: 0.48791418392821295\n",
      "Epoch 132, Loss: 0.4875902245243796\n",
      "Epoch 133, Loss: 0.48728851108037585\n",
      "Epoch 134, Loss: 0.48697062262181345\n",
      "Epoch 135, Loss: 0.4866339553030548\n",
      "Epoch 136, Loss: 0.48633446746734893\n",
      "Epoch 137, Loss: 0.4860310087358514\n",
      "Epoch 138, Loss: 0.4856786412545615\n",
      "Epoch 139, Loss: 0.48531372107004\n",
      "Epoch 140, Loss: 0.48495712985488587\n",
      "Epoch 141, Loss: 0.4846131600652309\n",
      "Epoch 142, Loss: 0.48425966858652364\n",
      "Epoch 143, Loss: 0.4838914149236435\n",
      "Epoch 144, Loss: 0.48352437546282767\n",
      "Epoch 145, Loss: 0.4832299911937133\n",
      "Epoch 146, Loss: 0.4832586529465723\n",
      "Epoch 147, Loss: 0.48356744143113517\n",
      "Epoch 148, Loss: 0.4828108019011922\n",
      "Epoch 149, Loss: 0.4818912939738211\n",
      "Epoch 150, Loss: 0.48239416745575797\n",
      "Epoch 151, Loss: 0.4814952491721961\n",
      "Epoch 152, Loss: 0.4811822818820502\n",
      "Epoch 153, Loss: 0.4812753156716337\n",
      "Epoch 154, Loss: 0.4804387742282827\n",
      "Epoch 155, Loss: 0.48058189052417905\n",
      "Epoch 156, Loss: 0.4801288115497346\n",
      "Epoch 157, Loss: 0.4797465937246525\n",
      "Epoch 158, Loss: 0.4797622479529733\n",
      "Epoch 159, Loss: 0.4791664679844762\n",
      "Epoch 160, Loss: 0.47904359923301915\n",
      "Epoch 161, Loss: 0.4788483533353555\n",
      "Epoch 162, Loss: 0.4783709093142749\n",
      "Epoch 163, Loss: 0.4783027642591491\n",
      "Epoch 164, Loss: 0.4780003946284736\n",
      "Epoch 165, Loss: 0.47766356610156796\n",
      "Epoch 166, Loss: 0.47756940749162413\n",
      "Epoch 167, Loss: 0.4772720526560437\n",
      "Epoch 168, Loss: 0.477001191512605\n",
      "Epoch 169, Loss: 0.47690651155802954\n",
      "Epoch 170, Loss: 0.47663605231839845\n",
      "Epoch 171, Loss: 0.4763715079887003\n",
      "Epoch 172, Loss: 0.4762479807342815\n",
      "Epoch 173, Loss: 0.47603155482909787\n",
      "Epoch 174, Loss: 0.47580085926956445\n",
      "Epoch 175, Loss: 0.4756396378860013\n",
      "Epoch 176, Loss: 0.4754627232666358\n",
      "Epoch 177, Loss: 0.4752513954316692\n",
      "Epoch 178, Loss: 0.47507140350730925\n",
      "Epoch 179, Loss: 0.47492471273973497\n",
      "Epoch 180, Loss: 0.47475493910732763\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2235685347204999\n",
      "Valence RMSE: 0.21009465584020995\n",
      "Arousal RMSE: 0.23627529498770383\n",
      "Test R^2 score: tensor([0.4651, 0.4472], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.3399, 0.3179], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4561495145804555\n",
      "Num of epochs: 181\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Epoch 121, Loss: 0.49295912913079615\n",
      "Epoch 122, Loss: 0.4924773619139838\n",
      "Epoch 123, Loss: 0.49197607191949805\n",
      "Epoch 124, Loss: 0.4914699197831528\n",
      "Epoch 125, Loss: 0.49091483381312667\n",
      "Epoch 126, Loss: 0.49035147677946317\n",
      "Epoch 127, Loss: 0.4898087222825037\n",
      "Epoch 128, Loss: 0.48924014731550836\n",
      "Epoch 129, Loss: 0.48873550647212044\n",
      "Epoch 130, Loss: 0.48832307254971563\n",
      "Epoch 131, Loss: 0.48791418392821295\n",
      "Epoch 132, Loss: 0.4875902245243796\n",
      "Epoch 133, Loss: 0.48728851108037585\n",
      "Epoch 134, Loss: 0.48697062262181345\n",
      "Epoch 135, Loss: 0.4866339553030548\n",
      "Epoch 136, Loss: 0.48633446746734893\n",
      "Epoch 137, Loss: 0.4860310087358514\n",
      "Epoch 138, Loss: 0.4856786412545615\n",
      "Epoch 139, Loss: 0.48531372107004\n",
      "Epoch 140, Loss: 0.48495712985488587\n",
      "Epoch 141, Loss: 0.4846131600652309\n",
      "Epoch 142, Loss: 0.48425966858652364\n",
      "Epoch 143, Loss: 0.4838914149236435\n",
      "Epoch 144, Loss: 0.48352437546282767\n",
      "Epoch 145, Loss: 0.4832299911937133\n",
      "Epoch 146, Loss: 0.4832586529465723\n",
      "Epoch 147, Loss: 0.48356744143113517\n",
      "Epoch 148, Loss: 0.4828108019011922\n",
      "Epoch 149, Loss: 0.4818912939738211\n",
      "Epoch 150, Loss: 0.48239416745575797\n",
      "Epoch 151, Loss: 0.4814952491721961\n",
      "Epoch 152, Loss: 0.4811822818820502\n",
      "Epoch 153, Loss: 0.4812753156716337\n",
      "Epoch 154, Loss: 0.4804387742282827\n",
      "Epoch 155, Loss: 0.48058189052417905\n",
      "Epoch 156, Loss: 0.4801288115497346\n",
      "Epoch 157, Loss: 0.4797465937246525\n",
      "Epoch 158, Loss: 0.4797622479529733\n",
      "Epoch 159, Loss: 0.4791664679844762\n",
      "Epoch 160, Loss: 0.47904359923301915\n",
      "Epoch 161, Loss: 0.4788483533353555\n",
      "Epoch 162, Loss: 0.4783709093142749\n",
      "Epoch 163, Loss: 0.4783027642591491\n",
      "Epoch 164, Loss: 0.4780003946284736\n",
      "Epoch 165, Loss: 0.47766356610156796\n",
      "Epoch 166, Loss: 0.47756940749162413\n",
      "Epoch 167, Loss: 0.4772720526560437\n",
      "Epoch 168, Loss: 0.477001191512605\n",
      "Epoch 169, Loss: 0.47690651155802954\n",
      "Epoch 170, Loss: 0.47663605231839845\n",
      "Epoch 171, Loss: 0.4763715079887003\n",
      "Epoch 172, Loss: 0.4762479807342815\n",
      "Epoch 173, Loss: 0.47603155482909787\n",
      "Epoch 174, Loss: 0.47580085926956445\n",
      "Epoch 175, Loss: 0.4756396378860013\n",
      "Epoch 176, Loss: 0.4754627232666358\n",
      "Epoch 177, Loss: 0.4752513954316692\n",
      "Epoch 178, Loss: 0.47507140350730925\n",
      "Epoch 179, Loss: 0.47492471273973497\n",
      "Epoch 180, Loss: 0.47475493910732763\n",
      "Epoch 181, Loss: 0.47455157018218247\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.22345340487342696\n",
      "Valence RMSE: 0.2100176497799907\n",
      "Arousal RMSE: 0.2361258882036117\n",
      "Test R^2 score: tensor([0.4655, 0.4479], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.3404, 0.3187], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.45669497657435504\n",
      "Num of epochs: 182\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Epoch 121, Loss: 0.49295912913079615\n",
      "Epoch 122, Loss: 0.4924773619139838\n",
      "Epoch 123, Loss: 0.49197607191949805\n",
      "Epoch 124, Loss: 0.4914699197831528\n",
      "Epoch 125, Loss: 0.49091483381312667\n",
      "Epoch 126, Loss: 0.49035147677946317\n",
      "Epoch 127, Loss: 0.4898087222825037\n",
      "Epoch 128, Loss: 0.48924014731550836\n",
      "Epoch 129, Loss: 0.48873550647212044\n",
      "Epoch 130, Loss: 0.48832307254971563\n",
      "Epoch 131, Loss: 0.48791418392821295\n",
      "Epoch 132, Loss: 0.4875902245243796\n",
      "Epoch 133, Loss: 0.48728851108037585\n",
      "Epoch 134, Loss: 0.48697062262181345\n",
      "Epoch 135, Loss: 0.4866339553030548\n",
      "Epoch 136, Loss: 0.48633446746734893\n",
      "Epoch 137, Loss: 0.4860310087358514\n",
      "Epoch 138, Loss: 0.4856786412545615\n",
      "Epoch 139, Loss: 0.48531372107004\n",
      "Epoch 140, Loss: 0.48495712985488587\n",
      "Epoch 141, Loss: 0.4846131600652309\n",
      "Epoch 142, Loss: 0.48425966858652364\n",
      "Epoch 143, Loss: 0.4838914149236435\n",
      "Epoch 144, Loss: 0.48352437546282767\n",
      "Epoch 145, Loss: 0.4832299911937133\n",
      "Epoch 146, Loss: 0.4832586529465723\n",
      "Epoch 147, Loss: 0.48356744143113517\n",
      "Epoch 148, Loss: 0.4828108019011922\n",
      "Epoch 149, Loss: 0.4818912939738211\n",
      "Epoch 150, Loss: 0.48239416745575797\n",
      "Epoch 151, Loss: 0.4814952491721961\n",
      "Epoch 152, Loss: 0.4811822818820502\n",
      "Epoch 153, Loss: 0.4812753156716337\n",
      "Epoch 154, Loss: 0.4804387742282827\n",
      "Epoch 155, Loss: 0.48058189052417905\n",
      "Epoch 156, Loss: 0.4801288115497346\n",
      "Epoch 157, Loss: 0.4797465937246525\n",
      "Epoch 158, Loss: 0.4797622479529733\n",
      "Epoch 159, Loss: 0.4791664679844762\n",
      "Epoch 160, Loss: 0.47904359923301915\n",
      "Epoch 161, Loss: 0.4788483533353555\n",
      "Epoch 162, Loss: 0.4783709093142749\n",
      "Epoch 163, Loss: 0.4783027642591491\n",
      "Epoch 164, Loss: 0.4780003946284736\n",
      "Epoch 165, Loss: 0.47766356610156796\n",
      "Epoch 166, Loss: 0.47756940749162413\n",
      "Epoch 167, Loss: 0.4772720526560437\n",
      "Epoch 168, Loss: 0.477001191512605\n",
      "Epoch 169, Loss: 0.47690651155802954\n",
      "Epoch 170, Loss: 0.47663605231839845\n",
      "Epoch 171, Loss: 0.4763715079887003\n",
      "Epoch 172, Loss: 0.4762479807342815\n",
      "Epoch 173, Loss: 0.47603155482909787\n",
      "Epoch 174, Loss: 0.47580085926956445\n",
      "Epoch 175, Loss: 0.4756396378860013\n",
      "Epoch 176, Loss: 0.4754627232666358\n",
      "Epoch 177, Loss: 0.4752513954316692\n",
      "Epoch 178, Loss: 0.47507140350730925\n",
      "Epoch 179, Loss: 0.47492471273973497\n",
      "Epoch 180, Loss: 0.47475493910732763\n",
      "Epoch 181, Loss: 0.47455157018218247\n",
      "Epoch 182, Loss: 0.4743853224635176\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.22288925321039724\n",
      "Valence RMSE: 0.20953108114133703\n",
      "Arousal RMSE: 0.23549090094761532\n",
      "Test R^2 score: tensor([0.4679, 0.4509], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.3434, 0.3224], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.45941458927369294\n",
      "Num of epochs: 183\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Epoch 121, Loss: 0.49295912913079615\n",
      "Epoch 122, Loss: 0.4924773619139838\n",
      "Epoch 123, Loss: 0.49197607191949805\n",
      "Epoch 124, Loss: 0.4914699197831528\n",
      "Epoch 125, Loss: 0.49091483381312667\n",
      "Epoch 126, Loss: 0.49035147677946317\n",
      "Epoch 127, Loss: 0.4898087222825037\n",
      "Epoch 128, Loss: 0.48924014731550836\n",
      "Epoch 129, Loss: 0.48873550647212044\n",
      "Epoch 130, Loss: 0.48832307254971563\n",
      "Epoch 131, Loss: 0.48791418392821295\n",
      "Epoch 132, Loss: 0.4875902245243796\n",
      "Epoch 133, Loss: 0.48728851108037585\n",
      "Epoch 134, Loss: 0.48697062262181345\n",
      "Epoch 135, Loss: 0.4866339553030548\n",
      "Epoch 136, Loss: 0.48633446746734893\n",
      "Epoch 137, Loss: 0.4860310087358514\n",
      "Epoch 138, Loss: 0.4856786412545615\n",
      "Epoch 139, Loss: 0.48531372107004\n",
      "Epoch 140, Loss: 0.48495712985488587\n",
      "Epoch 141, Loss: 0.4846131600652309\n",
      "Epoch 142, Loss: 0.48425966858652364\n",
      "Epoch 143, Loss: 0.4838914149236435\n",
      "Epoch 144, Loss: 0.48352437546282767\n",
      "Epoch 145, Loss: 0.4832299911937133\n",
      "Epoch 146, Loss: 0.4832586529465723\n",
      "Epoch 147, Loss: 0.48356744143113517\n",
      "Epoch 148, Loss: 0.4828108019011922\n",
      "Epoch 149, Loss: 0.4818912939738211\n",
      "Epoch 150, Loss: 0.48239416745575797\n",
      "Epoch 151, Loss: 0.4814952491721961\n",
      "Epoch 152, Loss: 0.4811822818820502\n",
      "Epoch 153, Loss: 0.4812753156716337\n",
      "Epoch 154, Loss: 0.4804387742282827\n",
      "Epoch 155, Loss: 0.48058189052417905\n",
      "Epoch 156, Loss: 0.4801288115497346\n",
      "Epoch 157, Loss: 0.4797465937246525\n",
      "Epoch 158, Loss: 0.4797622479529733\n",
      "Epoch 159, Loss: 0.4791664679844762\n",
      "Epoch 160, Loss: 0.47904359923301915\n",
      "Epoch 161, Loss: 0.4788483533353555\n",
      "Epoch 162, Loss: 0.4783709093142749\n",
      "Epoch 163, Loss: 0.4783027642591491\n",
      "Epoch 164, Loss: 0.4780003946284736\n",
      "Epoch 165, Loss: 0.47766356610156796\n",
      "Epoch 166, Loss: 0.47756940749162413\n",
      "Epoch 167, Loss: 0.4772720526560437\n",
      "Epoch 168, Loss: 0.477001191512605\n",
      "Epoch 169, Loss: 0.47690651155802954\n",
      "Epoch 170, Loss: 0.47663605231839845\n",
      "Epoch 171, Loss: 0.4763715079887003\n",
      "Epoch 172, Loss: 0.4762479807342815\n",
      "Epoch 173, Loss: 0.47603155482909787\n",
      "Epoch 174, Loss: 0.47580085926956445\n",
      "Epoch 175, Loss: 0.4756396378860013\n",
      "Epoch 176, Loss: 0.4754627232666358\n",
      "Epoch 177, Loss: 0.4752513954316692\n",
      "Epoch 178, Loss: 0.47507140350730925\n",
      "Epoch 179, Loss: 0.47492471273973497\n",
      "Epoch 180, Loss: 0.47475493910732763\n",
      "Epoch 181, Loss: 0.47455157018218247\n",
      "Epoch 182, Loss: 0.4743853224635176\n",
      "Epoch 183, Loss: 0.47424550491599726\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.22336353422914834\n",
      "Valence RMSE: 0.20996673485492537\n",
      "Arousal RMSE: 0.23600107436415052\n",
      "Test R^2 score: tensor([0.4657, 0.4485], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.3407, 0.3194], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.45711629268712833\n",
      "Num of epochs: 184\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Epoch 121, Loss: 0.49295912913079615\n",
      "Epoch 122, Loss: 0.4924773619139838\n",
      "Epoch 123, Loss: 0.49197607191949805\n",
      "Epoch 124, Loss: 0.4914699197831528\n",
      "Epoch 125, Loss: 0.49091483381312667\n",
      "Epoch 126, Loss: 0.49035147677946317\n",
      "Epoch 127, Loss: 0.4898087222825037\n",
      "Epoch 128, Loss: 0.48924014731550836\n",
      "Epoch 129, Loss: 0.48873550647212044\n",
      "Epoch 130, Loss: 0.48832307254971563\n",
      "Epoch 131, Loss: 0.48791418392821295\n",
      "Epoch 132, Loss: 0.4875902245243796\n",
      "Epoch 133, Loss: 0.48728851108037585\n",
      "Epoch 134, Loss: 0.48697062262181345\n",
      "Epoch 135, Loss: 0.4866339553030548\n",
      "Epoch 136, Loss: 0.48633446746734893\n",
      "Epoch 137, Loss: 0.4860310087358514\n",
      "Epoch 138, Loss: 0.4856786412545615\n",
      "Epoch 139, Loss: 0.48531372107004\n",
      "Epoch 140, Loss: 0.48495712985488587\n",
      "Epoch 141, Loss: 0.4846131600652309\n",
      "Epoch 142, Loss: 0.48425966858652364\n",
      "Epoch 143, Loss: 0.4838914149236435\n",
      "Epoch 144, Loss: 0.48352437546282767\n",
      "Epoch 145, Loss: 0.4832299911937133\n",
      "Epoch 146, Loss: 0.4832586529465723\n",
      "Epoch 147, Loss: 0.48356744143113517\n",
      "Epoch 148, Loss: 0.4828108019011922\n",
      "Epoch 149, Loss: 0.4818912939738211\n",
      "Epoch 150, Loss: 0.48239416745575797\n",
      "Epoch 151, Loss: 0.4814952491721961\n",
      "Epoch 152, Loss: 0.4811822818820502\n",
      "Epoch 153, Loss: 0.4812753156716337\n",
      "Epoch 154, Loss: 0.4804387742282827\n",
      "Epoch 155, Loss: 0.48058189052417905\n",
      "Epoch 156, Loss: 0.4801288115497346\n",
      "Epoch 157, Loss: 0.4797465937246525\n",
      "Epoch 158, Loss: 0.4797622479529733\n",
      "Epoch 159, Loss: 0.4791664679844762\n",
      "Epoch 160, Loss: 0.47904359923301915\n",
      "Epoch 161, Loss: 0.4788483533353555\n",
      "Epoch 162, Loss: 0.4783709093142749\n",
      "Epoch 163, Loss: 0.4783027642591491\n",
      "Epoch 164, Loss: 0.4780003946284736\n",
      "Epoch 165, Loss: 0.47766356610156796\n",
      "Epoch 166, Loss: 0.47756940749162413\n",
      "Epoch 167, Loss: 0.4772720526560437\n",
      "Epoch 168, Loss: 0.477001191512605\n",
      "Epoch 169, Loss: 0.47690651155802954\n",
      "Epoch 170, Loss: 0.47663605231839845\n",
      "Epoch 171, Loss: 0.4763715079887003\n",
      "Epoch 172, Loss: 0.4762479807342815\n",
      "Epoch 173, Loss: 0.47603155482909787\n",
      "Epoch 174, Loss: 0.47580085926956445\n",
      "Epoch 175, Loss: 0.4756396378860013\n",
      "Epoch 176, Loss: 0.4754627232666358\n",
      "Epoch 177, Loss: 0.4752513954316692\n",
      "Epoch 178, Loss: 0.47507140350730925\n",
      "Epoch 179, Loss: 0.47492471273973497\n",
      "Epoch 180, Loss: 0.47475493910732763\n",
      "Epoch 181, Loss: 0.47455157018218247\n",
      "Epoch 182, Loss: 0.4743853224635176\n",
      "Epoch 183, Loss: 0.47424550491599726\n",
      "Epoch 184, Loss: 0.47408413178207975\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.22273936814615267\n",
      "Valence RMSE: 0.20940948156987546\n",
      "Arousal RMSE: 0.23531536556912439\n",
      "Test R^2 score: tensor([0.4686, 0.4517], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.3442, 0.3234], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.460132431284961\n",
      "Num of epochs: 185\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Epoch 121, Loss: 0.49295912913079615\n",
      "Epoch 122, Loss: 0.4924773619139838\n",
      "Epoch 123, Loss: 0.49197607191949805\n",
      "Epoch 124, Loss: 0.4914699197831528\n",
      "Epoch 125, Loss: 0.49091483381312667\n",
      "Epoch 126, Loss: 0.49035147677946317\n",
      "Epoch 127, Loss: 0.4898087222825037\n",
      "Epoch 128, Loss: 0.48924014731550836\n",
      "Epoch 129, Loss: 0.48873550647212044\n",
      "Epoch 130, Loss: 0.48832307254971563\n",
      "Epoch 131, Loss: 0.48791418392821295\n",
      "Epoch 132, Loss: 0.4875902245243796\n",
      "Epoch 133, Loss: 0.48728851108037585\n",
      "Epoch 134, Loss: 0.48697062262181345\n",
      "Epoch 135, Loss: 0.4866339553030548\n",
      "Epoch 136, Loss: 0.48633446746734893\n",
      "Epoch 137, Loss: 0.4860310087358514\n",
      "Epoch 138, Loss: 0.4856786412545615\n",
      "Epoch 139, Loss: 0.48531372107004\n",
      "Epoch 140, Loss: 0.48495712985488587\n",
      "Epoch 141, Loss: 0.4846131600652309\n",
      "Epoch 142, Loss: 0.48425966858652364\n",
      "Epoch 143, Loss: 0.4838914149236435\n",
      "Epoch 144, Loss: 0.48352437546282767\n",
      "Epoch 145, Loss: 0.4832299911937133\n",
      "Epoch 146, Loss: 0.4832586529465723\n",
      "Epoch 147, Loss: 0.48356744143113517\n",
      "Epoch 148, Loss: 0.4828108019011922\n",
      "Epoch 149, Loss: 0.4818912939738211\n",
      "Epoch 150, Loss: 0.48239416745575797\n",
      "Epoch 151, Loss: 0.4814952491721961\n",
      "Epoch 152, Loss: 0.4811822818820502\n",
      "Epoch 153, Loss: 0.4812753156716337\n",
      "Epoch 154, Loss: 0.4804387742282827\n",
      "Epoch 155, Loss: 0.48058189052417905\n",
      "Epoch 156, Loss: 0.4801288115497346\n",
      "Epoch 157, Loss: 0.4797465937246525\n",
      "Epoch 158, Loss: 0.4797622479529733\n",
      "Epoch 159, Loss: 0.4791664679844762\n",
      "Epoch 160, Loss: 0.47904359923301915\n",
      "Epoch 161, Loss: 0.4788483533353555\n",
      "Epoch 162, Loss: 0.4783709093142749\n",
      "Epoch 163, Loss: 0.4783027642591491\n",
      "Epoch 164, Loss: 0.4780003946284736\n",
      "Epoch 165, Loss: 0.47766356610156796\n",
      "Epoch 166, Loss: 0.47756940749162413\n",
      "Epoch 167, Loss: 0.4772720526560437\n",
      "Epoch 168, Loss: 0.477001191512605\n",
      "Epoch 169, Loss: 0.47690651155802954\n",
      "Epoch 170, Loss: 0.47663605231839845\n",
      "Epoch 171, Loss: 0.4763715079887003\n",
      "Epoch 172, Loss: 0.4762479807342815\n",
      "Epoch 173, Loss: 0.47603155482909787\n",
      "Epoch 174, Loss: 0.47580085926956445\n",
      "Epoch 175, Loss: 0.4756396378860013\n",
      "Epoch 176, Loss: 0.4754627232666358\n",
      "Epoch 177, Loss: 0.4752513954316692\n",
      "Epoch 178, Loss: 0.47507140350730925\n",
      "Epoch 179, Loss: 0.47492471273973497\n",
      "Epoch 180, Loss: 0.47475493910732763\n",
      "Epoch 181, Loss: 0.47455157018218247\n",
      "Epoch 182, Loss: 0.4743853224635176\n",
      "Epoch 183, Loss: 0.47424550491599726\n",
      "Epoch 184, Loss: 0.47408413178207975\n",
      "Epoch 185, Loss: 0.47390385374022453\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.22279690275593597\n",
      "Valence RMSE: 0.2094732764907968\n",
      "Arousal RMSE: 0.23536751303331174\n",
      "Test R^2 score: tensor([0.4682, 0.4515], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.3438, 0.3231], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.459848987823607\n",
      "Num of epochs: 186\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Epoch 121, Loss: 0.49295912913079615\n",
      "Epoch 122, Loss: 0.4924773619139838\n",
      "Epoch 123, Loss: 0.49197607191949805\n",
      "Epoch 124, Loss: 0.4914699197831528\n",
      "Epoch 125, Loss: 0.49091483381312667\n",
      "Epoch 126, Loss: 0.49035147677946317\n",
      "Epoch 127, Loss: 0.4898087222825037\n",
      "Epoch 128, Loss: 0.48924014731550836\n",
      "Epoch 129, Loss: 0.48873550647212044\n",
      "Epoch 130, Loss: 0.48832307254971563\n",
      "Epoch 131, Loss: 0.48791418392821295\n",
      "Epoch 132, Loss: 0.4875902245243796\n",
      "Epoch 133, Loss: 0.48728851108037585\n",
      "Epoch 134, Loss: 0.48697062262181345\n",
      "Epoch 135, Loss: 0.4866339553030548\n",
      "Epoch 136, Loss: 0.48633446746734893\n",
      "Epoch 137, Loss: 0.4860310087358514\n",
      "Epoch 138, Loss: 0.4856786412545615\n",
      "Epoch 139, Loss: 0.48531372107004\n",
      "Epoch 140, Loss: 0.48495712985488587\n",
      "Epoch 141, Loss: 0.4846131600652309\n",
      "Epoch 142, Loss: 0.48425966858652364\n",
      "Epoch 143, Loss: 0.4838914149236435\n",
      "Epoch 144, Loss: 0.48352437546282767\n",
      "Epoch 145, Loss: 0.4832299911937133\n",
      "Epoch 146, Loss: 0.4832586529465723\n",
      "Epoch 147, Loss: 0.48356744143113517\n",
      "Epoch 148, Loss: 0.4828108019011922\n",
      "Epoch 149, Loss: 0.4818912939738211\n",
      "Epoch 150, Loss: 0.48239416745575797\n",
      "Epoch 151, Loss: 0.4814952491721961\n",
      "Epoch 152, Loss: 0.4811822818820502\n",
      "Epoch 153, Loss: 0.4812753156716337\n",
      "Epoch 154, Loss: 0.4804387742282827\n",
      "Epoch 155, Loss: 0.48058189052417905\n",
      "Epoch 156, Loss: 0.4801288115497346\n",
      "Epoch 157, Loss: 0.4797465937246525\n",
      "Epoch 158, Loss: 0.4797622479529733\n",
      "Epoch 159, Loss: 0.4791664679844762\n",
      "Epoch 160, Loss: 0.47904359923301915\n",
      "Epoch 161, Loss: 0.4788483533353555\n",
      "Epoch 162, Loss: 0.4783709093142749\n",
      "Epoch 163, Loss: 0.4783027642591491\n",
      "Epoch 164, Loss: 0.4780003946284736\n",
      "Epoch 165, Loss: 0.47766356610156796\n",
      "Epoch 166, Loss: 0.47756940749162413\n",
      "Epoch 167, Loss: 0.4772720526560437\n",
      "Epoch 168, Loss: 0.477001191512605\n",
      "Epoch 169, Loss: 0.47690651155802954\n",
      "Epoch 170, Loss: 0.47663605231839845\n",
      "Epoch 171, Loss: 0.4763715079887003\n",
      "Epoch 172, Loss: 0.4762479807342815\n",
      "Epoch 173, Loss: 0.47603155482909787\n",
      "Epoch 174, Loss: 0.47580085926956445\n",
      "Epoch 175, Loss: 0.4756396378860013\n",
      "Epoch 176, Loss: 0.4754627232666358\n",
      "Epoch 177, Loss: 0.4752513954316692\n",
      "Epoch 178, Loss: 0.47507140350730925\n",
      "Epoch 179, Loss: 0.47492471273973497\n",
      "Epoch 180, Loss: 0.47475493910732763\n",
      "Epoch 181, Loss: 0.47455157018218247\n",
      "Epoch 182, Loss: 0.4743853224635176\n",
      "Epoch 183, Loss: 0.47424550491599726\n",
      "Epoch 184, Loss: 0.47408413178207975\n",
      "Epoch 185, Loss: 0.47390385374022453\n",
      "Epoch 186, Loss: 0.4737383066259127\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.22273583936038724\n",
      "Valence RMSE: 0.20943188591962086\n",
      "Arousal RMSE: 0.2352887448037057\n",
      "Test R^2 score: tensor([0.4684, 0.4518], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.3440, 0.3235], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4601375930925531\n",
      "Num of epochs: 187\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Epoch 121, Loss: 0.49295912913079615\n",
      "Epoch 122, Loss: 0.4924773619139838\n",
      "Epoch 123, Loss: 0.49197607191949805\n",
      "Epoch 124, Loss: 0.4914699197831528\n",
      "Epoch 125, Loss: 0.49091483381312667\n",
      "Epoch 126, Loss: 0.49035147677946317\n",
      "Epoch 127, Loss: 0.4898087222825037\n",
      "Epoch 128, Loss: 0.48924014731550836\n",
      "Epoch 129, Loss: 0.48873550647212044\n",
      "Epoch 130, Loss: 0.48832307254971563\n",
      "Epoch 131, Loss: 0.48791418392821295\n",
      "Epoch 132, Loss: 0.4875902245243796\n",
      "Epoch 133, Loss: 0.48728851108037585\n",
      "Epoch 134, Loss: 0.48697062262181345\n",
      "Epoch 135, Loss: 0.4866339553030548\n",
      "Epoch 136, Loss: 0.48633446746734893\n",
      "Epoch 137, Loss: 0.4860310087358514\n",
      "Epoch 138, Loss: 0.4856786412545615\n",
      "Epoch 139, Loss: 0.48531372107004\n",
      "Epoch 140, Loss: 0.48495712985488587\n",
      "Epoch 141, Loss: 0.4846131600652309\n",
      "Epoch 142, Loss: 0.48425966858652364\n",
      "Epoch 143, Loss: 0.4838914149236435\n",
      "Epoch 144, Loss: 0.48352437546282767\n",
      "Epoch 145, Loss: 0.4832299911937133\n",
      "Epoch 146, Loss: 0.4832586529465723\n",
      "Epoch 147, Loss: 0.48356744143113517\n",
      "Epoch 148, Loss: 0.4828108019011922\n",
      "Epoch 149, Loss: 0.4818912939738211\n",
      "Epoch 150, Loss: 0.48239416745575797\n",
      "Epoch 151, Loss: 0.4814952491721961\n",
      "Epoch 152, Loss: 0.4811822818820502\n",
      "Epoch 153, Loss: 0.4812753156716337\n",
      "Epoch 154, Loss: 0.4804387742282827\n",
      "Epoch 155, Loss: 0.48058189052417905\n",
      "Epoch 156, Loss: 0.4801288115497346\n",
      "Epoch 157, Loss: 0.4797465937246525\n",
      "Epoch 158, Loss: 0.4797622479529733\n",
      "Epoch 159, Loss: 0.4791664679844762\n",
      "Epoch 160, Loss: 0.47904359923301915\n",
      "Epoch 161, Loss: 0.4788483533353555\n",
      "Epoch 162, Loss: 0.4783709093142749\n",
      "Epoch 163, Loss: 0.4783027642591491\n",
      "Epoch 164, Loss: 0.4780003946284736\n",
      "Epoch 165, Loss: 0.47766356610156796\n",
      "Epoch 166, Loss: 0.47756940749162413\n",
      "Epoch 167, Loss: 0.4772720526560437\n",
      "Epoch 168, Loss: 0.477001191512605\n",
      "Epoch 169, Loss: 0.47690651155802954\n",
      "Epoch 170, Loss: 0.47663605231839845\n",
      "Epoch 171, Loss: 0.4763715079887003\n",
      "Epoch 172, Loss: 0.4762479807342815\n",
      "Epoch 173, Loss: 0.47603155482909787\n",
      "Epoch 174, Loss: 0.47580085926956445\n",
      "Epoch 175, Loss: 0.4756396378860013\n",
      "Epoch 176, Loss: 0.4754627232666358\n",
      "Epoch 177, Loss: 0.4752513954316692\n",
      "Epoch 178, Loss: 0.47507140350730925\n",
      "Epoch 179, Loss: 0.47492471273973497\n",
      "Epoch 180, Loss: 0.47475493910732763\n",
      "Epoch 181, Loss: 0.47455157018218247\n",
      "Epoch 182, Loss: 0.4743853224635176\n",
      "Epoch 183, Loss: 0.47424550491599726\n",
      "Epoch 184, Loss: 0.47408413178207975\n",
      "Epoch 185, Loss: 0.47390385374022453\n",
      "Epoch 186, Loss: 0.4737383066259127\n",
      "Epoch 187, Loss: 0.47358797785338436\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2223180509271933\n",
      "Valence RMSE: 0.20906291298079155\n",
      "Arousal RMSE: 0.23482616964920716\n",
      "Test R^2 score: tensor([0.4703, 0.4540], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.3463, 0.3262], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.46214988681036423\n",
      "Num of epochs: 188\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Epoch 121, Loss: 0.49295912913079615\n",
      "Epoch 122, Loss: 0.4924773619139838\n",
      "Epoch 123, Loss: 0.49197607191949805\n",
      "Epoch 124, Loss: 0.4914699197831528\n",
      "Epoch 125, Loss: 0.49091483381312667\n",
      "Epoch 126, Loss: 0.49035147677946317\n",
      "Epoch 127, Loss: 0.4898087222825037\n",
      "Epoch 128, Loss: 0.48924014731550836\n",
      "Epoch 129, Loss: 0.48873550647212044\n",
      "Epoch 130, Loss: 0.48832307254971563\n",
      "Epoch 131, Loss: 0.48791418392821295\n",
      "Epoch 132, Loss: 0.4875902245243796\n",
      "Epoch 133, Loss: 0.48728851108037585\n",
      "Epoch 134, Loss: 0.48697062262181345\n",
      "Epoch 135, Loss: 0.4866339553030548\n",
      "Epoch 136, Loss: 0.48633446746734893\n",
      "Epoch 137, Loss: 0.4860310087358514\n",
      "Epoch 138, Loss: 0.4856786412545615\n",
      "Epoch 139, Loss: 0.48531372107004\n",
      "Epoch 140, Loss: 0.48495712985488587\n",
      "Epoch 141, Loss: 0.4846131600652309\n",
      "Epoch 142, Loss: 0.48425966858652364\n",
      "Epoch 143, Loss: 0.4838914149236435\n",
      "Epoch 144, Loss: 0.48352437546282767\n",
      "Epoch 145, Loss: 0.4832299911937133\n",
      "Epoch 146, Loss: 0.4832586529465723\n",
      "Epoch 147, Loss: 0.48356744143113517\n",
      "Epoch 148, Loss: 0.4828108019011922\n",
      "Epoch 149, Loss: 0.4818912939738211\n",
      "Epoch 150, Loss: 0.48239416745575797\n",
      "Epoch 151, Loss: 0.4814952491721961\n",
      "Epoch 152, Loss: 0.4811822818820502\n",
      "Epoch 153, Loss: 0.4812753156716337\n",
      "Epoch 154, Loss: 0.4804387742282827\n",
      "Epoch 155, Loss: 0.48058189052417905\n",
      "Epoch 156, Loss: 0.4801288115497346\n",
      "Epoch 157, Loss: 0.4797465937246525\n",
      "Epoch 158, Loss: 0.4797622479529733\n",
      "Epoch 159, Loss: 0.4791664679844762\n",
      "Epoch 160, Loss: 0.47904359923301915\n",
      "Epoch 161, Loss: 0.4788483533353555\n",
      "Epoch 162, Loss: 0.4783709093142749\n",
      "Epoch 163, Loss: 0.4783027642591491\n",
      "Epoch 164, Loss: 0.4780003946284736\n",
      "Epoch 165, Loss: 0.47766356610156796\n",
      "Epoch 166, Loss: 0.47756940749162413\n",
      "Epoch 167, Loss: 0.4772720526560437\n",
      "Epoch 168, Loss: 0.477001191512605\n",
      "Epoch 169, Loss: 0.47690651155802954\n",
      "Epoch 170, Loss: 0.47663605231839845\n",
      "Epoch 171, Loss: 0.4763715079887003\n",
      "Epoch 172, Loss: 0.4762479807342815\n",
      "Epoch 173, Loss: 0.47603155482909787\n",
      "Epoch 174, Loss: 0.47580085926956445\n",
      "Epoch 175, Loss: 0.4756396378860013\n",
      "Epoch 176, Loss: 0.4754627232666358\n",
      "Epoch 177, Loss: 0.4752513954316692\n",
      "Epoch 178, Loss: 0.47507140350730925\n",
      "Epoch 179, Loss: 0.47492471273973497\n",
      "Epoch 180, Loss: 0.47475493910732763\n",
      "Epoch 181, Loss: 0.47455157018218247\n",
      "Epoch 182, Loss: 0.4743853224635176\n",
      "Epoch 183, Loss: 0.47424550491599726\n",
      "Epoch 184, Loss: 0.47408413178207975\n",
      "Epoch 185, Loss: 0.47390385374022453\n",
      "Epoch 186, Loss: 0.4737383066259127\n",
      "Epoch 187, Loss: 0.47358797785338436\n",
      "Epoch 188, Loss: 0.4734560449826861\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2229041425111708\n",
      "Valence RMSE: 0.2096334097154532\n",
      "Arousal RMSE: 0.23542800816460563\n",
      "Test R^2 score: tensor([0.4674, 0.4512], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.3428, 0.3227], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4593013132407185\n",
      "Num of epochs: 189\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Epoch 121, Loss: 0.49295912913079615\n",
      "Epoch 122, Loss: 0.4924773619139838\n",
      "Epoch 123, Loss: 0.49197607191949805\n",
      "Epoch 124, Loss: 0.4914699197831528\n",
      "Epoch 125, Loss: 0.49091483381312667\n",
      "Epoch 126, Loss: 0.49035147677946317\n",
      "Epoch 127, Loss: 0.4898087222825037\n",
      "Epoch 128, Loss: 0.48924014731550836\n",
      "Epoch 129, Loss: 0.48873550647212044\n",
      "Epoch 130, Loss: 0.48832307254971563\n",
      "Epoch 131, Loss: 0.48791418392821295\n",
      "Epoch 132, Loss: 0.4875902245243796\n",
      "Epoch 133, Loss: 0.48728851108037585\n",
      "Epoch 134, Loss: 0.48697062262181345\n",
      "Epoch 135, Loss: 0.4866339553030548\n",
      "Epoch 136, Loss: 0.48633446746734893\n",
      "Epoch 137, Loss: 0.4860310087358514\n",
      "Epoch 138, Loss: 0.4856786412545615\n",
      "Epoch 139, Loss: 0.48531372107004\n",
      "Epoch 140, Loss: 0.48495712985488587\n",
      "Epoch 141, Loss: 0.4846131600652309\n",
      "Epoch 142, Loss: 0.48425966858652364\n",
      "Epoch 143, Loss: 0.4838914149236435\n",
      "Epoch 144, Loss: 0.48352437546282767\n",
      "Epoch 145, Loss: 0.4832299911937133\n",
      "Epoch 146, Loss: 0.4832586529465723\n",
      "Epoch 147, Loss: 0.48356744143113517\n",
      "Epoch 148, Loss: 0.4828108019011922\n",
      "Epoch 149, Loss: 0.4818912939738211\n",
      "Epoch 150, Loss: 0.48239416745575797\n",
      "Epoch 151, Loss: 0.4814952491721961\n",
      "Epoch 152, Loss: 0.4811822818820502\n",
      "Epoch 153, Loss: 0.4812753156716337\n",
      "Epoch 154, Loss: 0.4804387742282827\n",
      "Epoch 155, Loss: 0.48058189052417905\n",
      "Epoch 156, Loss: 0.4801288115497346\n",
      "Epoch 157, Loss: 0.4797465937246525\n",
      "Epoch 158, Loss: 0.4797622479529733\n",
      "Epoch 159, Loss: 0.4791664679844762\n",
      "Epoch 160, Loss: 0.47904359923301915\n",
      "Epoch 161, Loss: 0.4788483533353555\n",
      "Epoch 162, Loss: 0.4783709093142749\n",
      "Epoch 163, Loss: 0.4783027642591491\n",
      "Epoch 164, Loss: 0.4780003946284736\n",
      "Epoch 165, Loss: 0.47766356610156796\n",
      "Epoch 166, Loss: 0.47756940749162413\n",
      "Epoch 167, Loss: 0.4772720526560437\n",
      "Epoch 168, Loss: 0.477001191512605\n",
      "Epoch 169, Loss: 0.47690651155802954\n",
      "Epoch 170, Loss: 0.47663605231839845\n",
      "Epoch 171, Loss: 0.4763715079887003\n",
      "Epoch 172, Loss: 0.4762479807342815\n",
      "Epoch 173, Loss: 0.47603155482909787\n",
      "Epoch 174, Loss: 0.47580085926956445\n",
      "Epoch 175, Loss: 0.4756396378860013\n",
      "Epoch 176, Loss: 0.4754627232666358\n",
      "Epoch 177, Loss: 0.4752513954316692\n",
      "Epoch 178, Loss: 0.47507140350730925\n",
      "Epoch 179, Loss: 0.47492471273973497\n",
      "Epoch 180, Loss: 0.47475493910732763\n",
      "Epoch 181, Loss: 0.47455157018218247\n",
      "Epoch 182, Loss: 0.4743853224635176\n",
      "Epoch 183, Loss: 0.47424550491599726\n",
      "Epoch 184, Loss: 0.47408413178207975\n",
      "Epoch 185, Loss: 0.47390385374022453\n",
      "Epoch 186, Loss: 0.4737383066259127\n",
      "Epoch 187, Loss: 0.47358797785338436\n",
      "Epoch 188, Loss: 0.4734560449826861\n",
      "Epoch 189, Loss: 0.47335738206045025\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.22180988569978757\n",
      "Valence RMSE: 0.2086436036216873\n",
      "Arousal RMSE: 0.23423726743637932\n",
      "Test R^2 score: tensor([0.4724, 0.4567], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.3490, 0.3296], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4645787801969688\n",
      "Num of epochs: 190\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Epoch 121, Loss: 0.49295912913079615\n",
      "Epoch 122, Loss: 0.4924773619139838\n",
      "Epoch 123, Loss: 0.49197607191949805\n",
      "Epoch 124, Loss: 0.4914699197831528\n",
      "Epoch 125, Loss: 0.49091483381312667\n",
      "Epoch 126, Loss: 0.49035147677946317\n",
      "Epoch 127, Loss: 0.4898087222825037\n",
      "Epoch 128, Loss: 0.48924014731550836\n",
      "Epoch 129, Loss: 0.48873550647212044\n",
      "Epoch 130, Loss: 0.48832307254971563\n",
      "Epoch 131, Loss: 0.48791418392821295\n",
      "Epoch 132, Loss: 0.4875902245243796\n",
      "Epoch 133, Loss: 0.48728851108037585\n",
      "Epoch 134, Loss: 0.48697062262181345\n",
      "Epoch 135, Loss: 0.4866339553030548\n",
      "Epoch 136, Loss: 0.48633446746734893\n",
      "Epoch 137, Loss: 0.4860310087358514\n",
      "Epoch 138, Loss: 0.4856786412545615\n",
      "Epoch 139, Loss: 0.48531372107004\n",
      "Epoch 140, Loss: 0.48495712985488587\n",
      "Epoch 141, Loss: 0.4846131600652309\n",
      "Epoch 142, Loss: 0.48425966858652364\n",
      "Epoch 143, Loss: 0.4838914149236435\n",
      "Epoch 144, Loss: 0.48352437546282767\n",
      "Epoch 145, Loss: 0.4832299911937133\n",
      "Epoch 146, Loss: 0.4832586529465723\n",
      "Epoch 147, Loss: 0.48356744143113517\n",
      "Epoch 148, Loss: 0.4828108019011922\n",
      "Epoch 149, Loss: 0.4818912939738211\n",
      "Epoch 150, Loss: 0.48239416745575797\n",
      "Epoch 151, Loss: 0.4814952491721961\n",
      "Epoch 152, Loss: 0.4811822818820502\n",
      "Epoch 153, Loss: 0.4812753156716337\n",
      "Epoch 154, Loss: 0.4804387742282827\n",
      "Epoch 155, Loss: 0.48058189052417905\n",
      "Epoch 156, Loss: 0.4801288115497346\n",
      "Epoch 157, Loss: 0.4797465937246525\n",
      "Epoch 158, Loss: 0.4797622479529733\n",
      "Epoch 159, Loss: 0.4791664679844762\n",
      "Epoch 160, Loss: 0.47904359923301915\n",
      "Epoch 161, Loss: 0.4788483533353555\n",
      "Epoch 162, Loss: 0.4783709093142749\n",
      "Epoch 163, Loss: 0.4783027642591491\n",
      "Epoch 164, Loss: 0.4780003946284736\n",
      "Epoch 165, Loss: 0.47766356610156796\n",
      "Epoch 166, Loss: 0.47756940749162413\n",
      "Epoch 167, Loss: 0.4772720526560437\n",
      "Epoch 168, Loss: 0.477001191512605\n",
      "Epoch 169, Loss: 0.47690651155802954\n",
      "Epoch 170, Loss: 0.47663605231839845\n",
      "Epoch 171, Loss: 0.4763715079887003\n",
      "Epoch 172, Loss: 0.4762479807342815\n",
      "Epoch 173, Loss: 0.47603155482909787\n",
      "Epoch 174, Loss: 0.47580085926956445\n",
      "Epoch 175, Loss: 0.4756396378860013\n",
      "Epoch 176, Loss: 0.4754627232666358\n",
      "Epoch 177, Loss: 0.4752513954316692\n",
      "Epoch 178, Loss: 0.47507140350730925\n",
      "Epoch 179, Loss: 0.47492471273973497\n",
      "Epoch 180, Loss: 0.47475493910732763\n",
      "Epoch 181, Loss: 0.47455157018218247\n",
      "Epoch 182, Loss: 0.4743853224635176\n",
      "Epoch 183, Loss: 0.47424550491599726\n",
      "Epoch 184, Loss: 0.47408413178207975\n",
      "Epoch 185, Loss: 0.47390385374022453\n",
      "Epoch 186, Loss: 0.4737383066259127\n",
      "Epoch 187, Loss: 0.47358797785338436\n",
      "Epoch 188, Loss: 0.4734560449826861\n",
      "Epoch 189, Loss: 0.47335738206045025\n",
      "Epoch 190, Loss: 0.4733162834925985\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.22318049291529388\n",
      "Valence RMSE: 0.20994005594611936\n",
      "Arousal RMSE: 0.23567825047120355\n",
      "Test R^2 score: tensor([0.4659, 0.4500], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.3408, 0.3213], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4579380363774273\n",
      "Num of epochs: 191\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Epoch 121, Loss: 0.49295912913079615\n",
      "Epoch 122, Loss: 0.4924773619139838\n",
      "Epoch 123, Loss: 0.49197607191949805\n",
      "Epoch 124, Loss: 0.4914699197831528\n",
      "Epoch 125, Loss: 0.49091483381312667\n",
      "Epoch 126, Loss: 0.49035147677946317\n",
      "Epoch 127, Loss: 0.4898087222825037\n",
      "Epoch 128, Loss: 0.48924014731550836\n",
      "Epoch 129, Loss: 0.48873550647212044\n",
      "Epoch 130, Loss: 0.48832307254971563\n",
      "Epoch 131, Loss: 0.48791418392821295\n",
      "Epoch 132, Loss: 0.4875902245243796\n",
      "Epoch 133, Loss: 0.48728851108037585\n",
      "Epoch 134, Loss: 0.48697062262181345\n",
      "Epoch 135, Loss: 0.4866339553030548\n",
      "Epoch 136, Loss: 0.48633446746734893\n",
      "Epoch 137, Loss: 0.4860310087358514\n",
      "Epoch 138, Loss: 0.4856786412545615\n",
      "Epoch 139, Loss: 0.48531372107004\n",
      "Epoch 140, Loss: 0.48495712985488587\n",
      "Epoch 141, Loss: 0.4846131600652309\n",
      "Epoch 142, Loss: 0.48425966858652364\n",
      "Epoch 143, Loss: 0.4838914149236435\n",
      "Epoch 144, Loss: 0.48352437546282767\n",
      "Epoch 145, Loss: 0.4832299911937133\n",
      "Epoch 146, Loss: 0.4832586529465723\n",
      "Epoch 147, Loss: 0.48356744143113517\n",
      "Epoch 148, Loss: 0.4828108019011922\n",
      "Epoch 149, Loss: 0.4818912939738211\n",
      "Epoch 150, Loss: 0.48239416745575797\n",
      "Epoch 151, Loss: 0.4814952491721961\n",
      "Epoch 152, Loss: 0.4811822818820502\n",
      "Epoch 153, Loss: 0.4812753156716337\n",
      "Epoch 154, Loss: 0.4804387742282827\n",
      "Epoch 155, Loss: 0.48058189052417905\n",
      "Epoch 156, Loss: 0.4801288115497346\n",
      "Epoch 157, Loss: 0.4797465937246525\n",
      "Epoch 158, Loss: 0.4797622479529733\n",
      "Epoch 159, Loss: 0.4791664679844762\n",
      "Epoch 160, Loss: 0.47904359923301915\n",
      "Epoch 161, Loss: 0.4788483533353555\n",
      "Epoch 162, Loss: 0.4783709093142749\n",
      "Epoch 163, Loss: 0.4783027642591491\n",
      "Epoch 164, Loss: 0.4780003946284736\n",
      "Epoch 165, Loss: 0.47766356610156796\n",
      "Epoch 166, Loss: 0.47756940749162413\n",
      "Epoch 167, Loss: 0.4772720526560437\n",
      "Epoch 168, Loss: 0.477001191512605\n",
      "Epoch 169, Loss: 0.47690651155802954\n",
      "Epoch 170, Loss: 0.47663605231839845\n",
      "Epoch 171, Loss: 0.4763715079887003\n",
      "Epoch 172, Loss: 0.4762479807342815\n",
      "Epoch 173, Loss: 0.47603155482909787\n",
      "Epoch 174, Loss: 0.47580085926956445\n",
      "Epoch 175, Loss: 0.4756396378860013\n",
      "Epoch 176, Loss: 0.4754627232666358\n",
      "Epoch 177, Loss: 0.4752513954316692\n",
      "Epoch 178, Loss: 0.47507140350730925\n",
      "Epoch 179, Loss: 0.47492471273973497\n",
      "Epoch 180, Loss: 0.47475493910732763\n",
      "Epoch 181, Loss: 0.47455157018218247\n",
      "Epoch 182, Loss: 0.4743853224635176\n",
      "Epoch 183, Loss: 0.47424550491599726\n",
      "Epoch 184, Loss: 0.47408413178207975\n",
      "Epoch 185, Loss: 0.47390385374022453\n",
      "Epoch 186, Loss: 0.4737383066259127\n",
      "Epoch 187, Loss: 0.47358797785338436\n",
      "Epoch 188, Loss: 0.4734560449826861\n",
      "Epoch 189, Loss: 0.47335738206045025\n",
      "Epoch 190, Loss: 0.4733162834925985\n",
      "Epoch 191, Loss: 0.4732413020737725\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2215445374881442\n",
      "Valence RMSE: 0.2084424268223289\n",
      "Arousal RMSE: 0.2339139133999718\n",
      "Test R^2 score: tensor([0.4735, 0.4582], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.3502, 0.3314], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4658366718600585\n",
      "Num of epochs: 192\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Epoch 121, Loss: 0.49295912913079615\n",
      "Epoch 122, Loss: 0.4924773619139838\n",
      "Epoch 123, Loss: 0.49197607191949805\n",
      "Epoch 124, Loss: 0.4914699197831528\n",
      "Epoch 125, Loss: 0.49091483381312667\n",
      "Epoch 126, Loss: 0.49035147677946317\n",
      "Epoch 127, Loss: 0.4898087222825037\n",
      "Epoch 128, Loss: 0.48924014731550836\n",
      "Epoch 129, Loss: 0.48873550647212044\n",
      "Epoch 130, Loss: 0.48832307254971563\n",
      "Epoch 131, Loss: 0.48791418392821295\n",
      "Epoch 132, Loss: 0.4875902245243796\n",
      "Epoch 133, Loss: 0.48728851108037585\n",
      "Epoch 134, Loss: 0.48697062262181345\n",
      "Epoch 135, Loss: 0.4866339553030548\n",
      "Epoch 136, Loss: 0.48633446746734893\n",
      "Epoch 137, Loss: 0.4860310087358514\n",
      "Epoch 138, Loss: 0.4856786412545615\n",
      "Epoch 139, Loss: 0.48531372107004\n",
      "Epoch 140, Loss: 0.48495712985488587\n",
      "Epoch 141, Loss: 0.4846131600652309\n",
      "Epoch 142, Loss: 0.48425966858652364\n",
      "Epoch 143, Loss: 0.4838914149236435\n",
      "Epoch 144, Loss: 0.48352437546282767\n",
      "Epoch 145, Loss: 0.4832299911937133\n",
      "Epoch 146, Loss: 0.4832586529465723\n",
      "Epoch 147, Loss: 0.48356744143113517\n",
      "Epoch 148, Loss: 0.4828108019011922\n",
      "Epoch 149, Loss: 0.4818912939738211\n",
      "Epoch 150, Loss: 0.48239416745575797\n",
      "Epoch 151, Loss: 0.4814952491721961\n",
      "Epoch 152, Loss: 0.4811822818820502\n",
      "Epoch 153, Loss: 0.4812753156716337\n",
      "Epoch 154, Loss: 0.4804387742282827\n",
      "Epoch 155, Loss: 0.48058189052417905\n",
      "Epoch 156, Loss: 0.4801288115497346\n",
      "Epoch 157, Loss: 0.4797465937246525\n",
      "Epoch 158, Loss: 0.4797622479529733\n",
      "Epoch 159, Loss: 0.4791664679844762\n",
      "Epoch 160, Loss: 0.47904359923301915\n",
      "Epoch 161, Loss: 0.4788483533353555\n",
      "Epoch 162, Loss: 0.4783709093142749\n",
      "Epoch 163, Loss: 0.4783027642591491\n",
      "Epoch 164, Loss: 0.4780003946284736\n",
      "Epoch 165, Loss: 0.47766356610156796\n",
      "Epoch 166, Loss: 0.47756940749162413\n",
      "Epoch 167, Loss: 0.4772720526560437\n",
      "Epoch 168, Loss: 0.477001191512605\n",
      "Epoch 169, Loss: 0.47690651155802954\n",
      "Epoch 170, Loss: 0.47663605231839845\n",
      "Epoch 171, Loss: 0.4763715079887003\n",
      "Epoch 172, Loss: 0.4762479807342815\n",
      "Epoch 173, Loss: 0.47603155482909787\n",
      "Epoch 174, Loss: 0.47580085926956445\n",
      "Epoch 175, Loss: 0.4756396378860013\n",
      "Epoch 176, Loss: 0.4754627232666358\n",
      "Epoch 177, Loss: 0.4752513954316692\n",
      "Epoch 178, Loss: 0.47507140350730925\n",
      "Epoch 179, Loss: 0.47492471273973497\n",
      "Epoch 180, Loss: 0.47475493910732763\n",
      "Epoch 181, Loss: 0.47455157018218247\n",
      "Epoch 182, Loss: 0.4743853224635176\n",
      "Epoch 183, Loss: 0.47424550491599726\n",
      "Epoch 184, Loss: 0.47408413178207975\n",
      "Epoch 185, Loss: 0.47390385374022453\n",
      "Epoch 186, Loss: 0.4737383066259127\n",
      "Epoch 187, Loss: 0.47358797785338436\n",
      "Epoch 188, Loss: 0.4734560449826861\n",
      "Epoch 189, Loss: 0.47335738206045025\n",
      "Epoch 190, Loss: 0.4733162834925985\n",
      "Epoch 191, Loss: 0.4732413020737725\n",
      "Epoch 192, Loss: 0.47313239020014447\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.222900531355448\n",
      "Valence RMSE: 0.20974014502660374\n",
      "Arousal RMSE: 0.23532608296000965\n",
      "Test R^2 score: tensor([0.4669, 0.4517], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.3421, 0.3233], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4592676279483285\n",
      "Num of epochs: 193\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Epoch 121, Loss: 0.49295912913079615\n",
      "Epoch 122, Loss: 0.4924773619139838\n",
      "Epoch 123, Loss: 0.49197607191949805\n",
      "Epoch 124, Loss: 0.4914699197831528\n",
      "Epoch 125, Loss: 0.49091483381312667\n",
      "Epoch 126, Loss: 0.49035147677946317\n",
      "Epoch 127, Loss: 0.4898087222825037\n",
      "Epoch 128, Loss: 0.48924014731550836\n",
      "Epoch 129, Loss: 0.48873550647212044\n",
      "Epoch 130, Loss: 0.48832307254971563\n",
      "Epoch 131, Loss: 0.48791418392821295\n",
      "Epoch 132, Loss: 0.4875902245243796\n",
      "Epoch 133, Loss: 0.48728851108037585\n",
      "Epoch 134, Loss: 0.48697062262181345\n",
      "Epoch 135, Loss: 0.4866339553030548\n",
      "Epoch 136, Loss: 0.48633446746734893\n",
      "Epoch 137, Loss: 0.4860310087358514\n",
      "Epoch 138, Loss: 0.4856786412545615\n",
      "Epoch 139, Loss: 0.48531372107004\n",
      "Epoch 140, Loss: 0.48495712985488587\n",
      "Epoch 141, Loss: 0.4846131600652309\n",
      "Epoch 142, Loss: 0.48425966858652364\n",
      "Epoch 143, Loss: 0.4838914149236435\n",
      "Epoch 144, Loss: 0.48352437546282767\n",
      "Epoch 145, Loss: 0.4832299911937133\n",
      "Epoch 146, Loss: 0.4832586529465723\n",
      "Epoch 147, Loss: 0.48356744143113517\n",
      "Epoch 148, Loss: 0.4828108019011922\n",
      "Epoch 149, Loss: 0.4818912939738211\n",
      "Epoch 150, Loss: 0.48239416745575797\n",
      "Epoch 151, Loss: 0.4814952491721961\n",
      "Epoch 152, Loss: 0.4811822818820502\n",
      "Epoch 153, Loss: 0.4812753156716337\n",
      "Epoch 154, Loss: 0.4804387742282827\n",
      "Epoch 155, Loss: 0.48058189052417905\n",
      "Epoch 156, Loss: 0.4801288115497346\n",
      "Epoch 157, Loss: 0.4797465937246525\n",
      "Epoch 158, Loss: 0.4797622479529733\n",
      "Epoch 159, Loss: 0.4791664679844762\n",
      "Epoch 160, Loss: 0.47904359923301915\n",
      "Epoch 161, Loss: 0.4788483533353555\n",
      "Epoch 162, Loss: 0.4783709093142749\n",
      "Epoch 163, Loss: 0.4783027642591491\n",
      "Epoch 164, Loss: 0.4780003946284736\n",
      "Epoch 165, Loss: 0.47766356610156796\n",
      "Epoch 166, Loss: 0.47756940749162413\n",
      "Epoch 167, Loss: 0.4772720526560437\n",
      "Epoch 168, Loss: 0.477001191512605\n",
      "Epoch 169, Loss: 0.47690651155802954\n",
      "Epoch 170, Loss: 0.47663605231839845\n",
      "Epoch 171, Loss: 0.4763715079887003\n",
      "Epoch 172, Loss: 0.4762479807342815\n",
      "Epoch 173, Loss: 0.47603155482909787\n",
      "Epoch 174, Loss: 0.47580085926956445\n",
      "Epoch 175, Loss: 0.4756396378860013\n",
      "Epoch 176, Loss: 0.4754627232666358\n",
      "Epoch 177, Loss: 0.4752513954316692\n",
      "Epoch 178, Loss: 0.47507140350730925\n",
      "Epoch 179, Loss: 0.47492471273973497\n",
      "Epoch 180, Loss: 0.47475493910732763\n",
      "Epoch 181, Loss: 0.47455157018218247\n",
      "Epoch 182, Loss: 0.4743853224635176\n",
      "Epoch 183, Loss: 0.47424550491599726\n",
      "Epoch 184, Loss: 0.47408413178207975\n",
      "Epoch 185, Loss: 0.47390385374022453\n",
      "Epoch 186, Loss: 0.4737383066259127\n",
      "Epoch 187, Loss: 0.47358797785338436\n",
      "Epoch 188, Loss: 0.4734560449826861\n",
      "Epoch 189, Loss: 0.47335738206045025\n",
      "Epoch 190, Loss: 0.4733162834925985\n",
      "Epoch 191, Loss: 0.4732413020737725\n",
      "Epoch 192, Loss: 0.47313239020014447\n",
      "Epoch 193, Loss: 0.4729457158004841\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.22146444155640693\n",
      "Valence RMSE: 0.2084352366753672\n",
      "Arousal RMSE: 0.23376858184937094\n",
      "Test R^2 score: tensor([0.4735, 0.4589], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.3503, 0.3323], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4661913384271678\n",
      "Num of epochs: 194\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Epoch 121, Loss: 0.49295912913079615\n",
      "Epoch 122, Loss: 0.4924773619139838\n",
      "Epoch 123, Loss: 0.49197607191949805\n",
      "Epoch 124, Loss: 0.4914699197831528\n",
      "Epoch 125, Loss: 0.49091483381312667\n",
      "Epoch 126, Loss: 0.49035147677946317\n",
      "Epoch 127, Loss: 0.4898087222825037\n",
      "Epoch 128, Loss: 0.48924014731550836\n",
      "Epoch 129, Loss: 0.48873550647212044\n",
      "Epoch 130, Loss: 0.48832307254971563\n",
      "Epoch 131, Loss: 0.48791418392821295\n",
      "Epoch 132, Loss: 0.4875902245243796\n",
      "Epoch 133, Loss: 0.48728851108037585\n",
      "Epoch 134, Loss: 0.48697062262181345\n",
      "Epoch 135, Loss: 0.4866339553030548\n",
      "Epoch 136, Loss: 0.48633446746734893\n",
      "Epoch 137, Loss: 0.4860310087358514\n",
      "Epoch 138, Loss: 0.4856786412545615\n",
      "Epoch 139, Loss: 0.48531372107004\n",
      "Epoch 140, Loss: 0.48495712985488587\n",
      "Epoch 141, Loss: 0.4846131600652309\n",
      "Epoch 142, Loss: 0.48425966858652364\n",
      "Epoch 143, Loss: 0.4838914149236435\n",
      "Epoch 144, Loss: 0.48352437546282767\n",
      "Epoch 145, Loss: 0.4832299911937133\n",
      "Epoch 146, Loss: 0.4832586529465723\n",
      "Epoch 147, Loss: 0.48356744143113517\n",
      "Epoch 148, Loss: 0.4828108019011922\n",
      "Epoch 149, Loss: 0.4818912939738211\n",
      "Epoch 150, Loss: 0.48239416745575797\n",
      "Epoch 151, Loss: 0.4814952491721961\n",
      "Epoch 152, Loss: 0.4811822818820502\n",
      "Epoch 153, Loss: 0.4812753156716337\n",
      "Epoch 154, Loss: 0.4804387742282827\n",
      "Epoch 155, Loss: 0.48058189052417905\n",
      "Epoch 156, Loss: 0.4801288115497346\n",
      "Epoch 157, Loss: 0.4797465937246525\n",
      "Epoch 158, Loss: 0.4797622479529733\n",
      "Epoch 159, Loss: 0.4791664679844762\n",
      "Epoch 160, Loss: 0.47904359923301915\n",
      "Epoch 161, Loss: 0.4788483533353555\n",
      "Epoch 162, Loss: 0.4783709093142749\n",
      "Epoch 163, Loss: 0.4783027642591491\n",
      "Epoch 164, Loss: 0.4780003946284736\n",
      "Epoch 165, Loss: 0.47766356610156796\n",
      "Epoch 166, Loss: 0.47756940749162413\n",
      "Epoch 167, Loss: 0.4772720526560437\n",
      "Epoch 168, Loss: 0.477001191512605\n",
      "Epoch 169, Loss: 0.47690651155802954\n",
      "Epoch 170, Loss: 0.47663605231839845\n",
      "Epoch 171, Loss: 0.4763715079887003\n",
      "Epoch 172, Loss: 0.4762479807342815\n",
      "Epoch 173, Loss: 0.47603155482909787\n",
      "Epoch 174, Loss: 0.47580085926956445\n",
      "Epoch 175, Loss: 0.4756396378860013\n",
      "Epoch 176, Loss: 0.4754627232666358\n",
      "Epoch 177, Loss: 0.4752513954316692\n",
      "Epoch 178, Loss: 0.47507140350730925\n",
      "Epoch 179, Loss: 0.47492471273973497\n",
      "Epoch 180, Loss: 0.47475493910732763\n",
      "Epoch 181, Loss: 0.47455157018218247\n",
      "Epoch 182, Loss: 0.4743853224635176\n",
      "Epoch 183, Loss: 0.47424550491599726\n",
      "Epoch 184, Loss: 0.47408413178207975\n",
      "Epoch 185, Loss: 0.47390385374022453\n",
      "Epoch 186, Loss: 0.4737383066259127\n",
      "Epoch 187, Loss: 0.47358797785338436\n",
      "Epoch 188, Loss: 0.4734560449826861\n",
      "Epoch 189, Loss: 0.47335738206045025\n",
      "Epoch 190, Loss: 0.4733162834925985\n",
      "Epoch 191, Loss: 0.4732413020737725\n",
      "Epoch 192, Loss: 0.47313239020014447\n",
      "Epoch 193, Loss: 0.4729457158004841\n",
      "Epoch 194, Loss: 0.47274703738102797\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2222834690067795\n",
      "Valence RMSE: 0.2092074311128542\n",
      "Arousal RMSE: 0.23463190736670106\n",
      "Test R^2 score: tensor([0.4696, 0.4549], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.3454, 0.3273], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.46223511110104043\n",
      "Num of epochs: 195\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Epoch 121, Loss: 0.49295912913079615\n",
      "Epoch 122, Loss: 0.4924773619139838\n",
      "Epoch 123, Loss: 0.49197607191949805\n",
      "Epoch 124, Loss: 0.4914699197831528\n",
      "Epoch 125, Loss: 0.49091483381312667\n",
      "Epoch 126, Loss: 0.49035147677946317\n",
      "Epoch 127, Loss: 0.4898087222825037\n",
      "Epoch 128, Loss: 0.48924014731550836\n",
      "Epoch 129, Loss: 0.48873550647212044\n",
      "Epoch 130, Loss: 0.48832307254971563\n",
      "Epoch 131, Loss: 0.48791418392821295\n",
      "Epoch 132, Loss: 0.4875902245243796\n",
      "Epoch 133, Loss: 0.48728851108037585\n",
      "Epoch 134, Loss: 0.48697062262181345\n",
      "Epoch 135, Loss: 0.4866339553030548\n",
      "Epoch 136, Loss: 0.48633446746734893\n",
      "Epoch 137, Loss: 0.4860310087358514\n",
      "Epoch 138, Loss: 0.4856786412545615\n",
      "Epoch 139, Loss: 0.48531372107004\n",
      "Epoch 140, Loss: 0.48495712985488587\n",
      "Epoch 141, Loss: 0.4846131600652309\n",
      "Epoch 142, Loss: 0.48425966858652364\n",
      "Epoch 143, Loss: 0.4838914149236435\n",
      "Epoch 144, Loss: 0.48352437546282767\n",
      "Epoch 145, Loss: 0.4832299911937133\n",
      "Epoch 146, Loss: 0.4832586529465723\n",
      "Epoch 147, Loss: 0.48356744143113517\n",
      "Epoch 148, Loss: 0.4828108019011922\n",
      "Epoch 149, Loss: 0.4818912939738211\n",
      "Epoch 150, Loss: 0.48239416745575797\n",
      "Epoch 151, Loss: 0.4814952491721961\n",
      "Epoch 152, Loss: 0.4811822818820502\n",
      "Epoch 153, Loss: 0.4812753156716337\n",
      "Epoch 154, Loss: 0.4804387742282827\n",
      "Epoch 155, Loss: 0.48058189052417905\n",
      "Epoch 156, Loss: 0.4801288115497346\n",
      "Epoch 157, Loss: 0.4797465937246525\n",
      "Epoch 158, Loss: 0.4797622479529733\n",
      "Epoch 159, Loss: 0.4791664679844762\n",
      "Epoch 160, Loss: 0.47904359923301915\n",
      "Epoch 161, Loss: 0.4788483533353555\n",
      "Epoch 162, Loss: 0.4783709093142749\n",
      "Epoch 163, Loss: 0.4783027642591491\n",
      "Epoch 164, Loss: 0.4780003946284736\n",
      "Epoch 165, Loss: 0.47766356610156796\n",
      "Epoch 166, Loss: 0.47756940749162413\n",
      "Epoch 167, Loss: 0.4772720526560437\n",
      "Epoch 168, Loss: 0.477001191512605\n",
      "Epoch 169, Loss: 0.47690651155802954\n",
      "Epoch 170, Loss: 0.47663605231839845\n",
      "Epoch 171, Loss: 0.4763715079887003\n",
      "Epoch 172, Loss: 0.4762479807342815\n",
      "Epoch 173, Loss: 0.47603155482909787\n",
      "Epoch 174, Loss: 0.47580085926956445\n",
      "Epoch 175, Loss: 0.4756396378860013\n",
      "Epoch 176, Loss: 0.4754627232666358\n",
      "Epoch 177, Loss: 0.4752513954316692\n",
      "Epoch 178, Loss: 0.47507140350730925\n",
      "Epoch 179, Loss: 0.47492471273973497\n",
      "Epoch 180, Loss: 0.47475493910732763\n",
      "Epoch 181, Loss: 0.47455157018218247\n",
      "Epoch 182, Loss: 0.4743853224635176\n",
      "Epoch 183, Loss: 0.47424550491599726\n",
      "Epoch 184, Loss: 0.47408413178207975\n",
      "Epoch 185, Loss: 0.47390385374022453\n",
      "Epoch 186, Loss: 0.4737383066259127\n",
      "Epoch 187, Loss: 0.47358797785338436\n",
      "Epoch 188, Loss: 0.4734560449826861\n",
      "Epoch 189, Loss: 0.47335738206045025\n",
      "Epoch 190, Loss: 0.4733162834925985\n",
      "Epoch 191, Loss: 0.4732413020737725\n",
      "Epoch 192, Loss: 0.47313239020014447\n",
      "Epoch 193, Loss: 0.4729457158004841\n",
      "Epoch 194, Loss: 0.47274703738102797\n",
      "Epoch 195, Loss: 0.47253750657270627\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.22157274929230783\n",
      "Valence RMSE: 0.2085659268166907\n",
      "Arousal RMSE: 0.2338572655039792\n",
      "Test R^2 score: tensor([0.4728, 0.4585], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.3494, 0.3317], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.46565579311759797\n",
      "Num of epochs: 196\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Epoch 121, Loss: 0.49295912913079615\n",
      "Epoch 122, Loss: 0.4924773619139838\n",
      "Epoch 123, Loss: 0.49197607191949805\n",
      "Epoch 124, Loss: 0.4914699197831528\n",
      "Epoch 125, Loss: 0.49091483381312667\n",
      "Epoch 126, Loss: 0.49035147677946317\n",
      "Epoch 127, Loss: 0.4898087222825037\n",
      "Epoch 128, Loss: 0.48924014731550836\n",
      "Epoch 129, Loss: 0.48873550647212044\n",
      "Epoch 130, Loss: 0.48832307254971563\n",
      "Epoch 131, Loss: 0.48791418392821295\n",
      "Epoch 132, Loss: 0.4875902245243796\n",
      "Epoch 133, Loss: 0.48728851108037585\n",
      "Epoch 134, Loss: 0.48697062262181345\n",
      "Epoch 135, Loss: 0.4866339553030548\n",
      "Epoch 136, Loss: 0.48633446746734893\n",
      "Epoch 137, Loss: 0.4860310087358514\n",
      "Epoch 138, Loss: 0.4856786412545615\n",
      "Epoch 139, Loss: 0.48531372107004\n",
      "Epoch 140, Loss: 0.48495712985488587\n",
      "Epoch 141, Loss: 0.4846131600652309\n",
      "Epoch 142, Loss: 0.48425966858652364\n",
      "Epoch 143, Loss: 0.4838914149236435\n",
      "Epoch 144, Loss: 0.48352437546282767\n",
      "Epoch 145, Loss: 0.4832299911937133\n",
      "Epoch 146, Loss: 0.4832586529465723\n",
      "Epoch 147, Loss: 0.48356744143113517\n",
      "Epoch 148, Loss: 0.4828108019011922\n",
      "Epoch 149, Loss: 0.4818912939738211\n",
      "Epoch 150, Loss: 0.48239416745575797\n",
      "Epoch 151, Loss: 0.4814952491721961\n",
      "Epoch 152, Loss: 0.4811822818820502\n",
      "Epoch 153, Loss: 0.4812753156716337\n",
      "Epoch 154, Loss: 0.4804387742282827\n",
      "Epoch 155, Loss: 0.48058189052417905\n",
      "Epoch 156, Loss: 0.4801288115497346\n",
      "Epoch 157, Loss: 0.4797465937246525\n",
      "Epoch 158, Loss: 0.4797622479529733\n",
      "Epoch 159, Loss: 0.4791664679844762\n",
      "Epoch 160, Loss: 0.47904359923301915\n",
      "Epoch 161, Loss: 0.4788483533353555\n",
      "Epoch 162, Loss: 0.4783709093142749\n",
      "Epoch 163, Loss: 0.4783027642591491\n",
      "Epoch 164, Loss: 0.4780003946284736\n",
      "Epoch 165, Loss: 0.47766356610156796\n",
      "Epoch 166, Loss: 0.47756940749162413\n",
      "Epoch 167, Loss: 0.4772720526560437\n",
      "Epoch 168, Loss: 0.477001191512605\n",
      "Epoch 169, Loss: 0.47690651155802954\n",
      "Epoch 170, Loss: 0.47663605231839845\n",
      "Epoch 171, Loss: 0.4763715079887003\n",
      "Epoch 172, Loss: 0.4762479807342815\n",
      "Epoch 173, Loss: 0.47603155482909787\n",
      "Epoch 174, Loss: 0.47580085926956445\n",
      "Epoch 175, Loss: 0.4756396378860013\n",
      "Epoch 176, Loss: 0.4754627232666358\n",
      "Epoch 177, Loss: 0.4752513954316692\n",
      "Epoch 178, Loss: 0.47507140350730925\n",
      "Epoch 179, Loss: 0.47492471273973497\n",
      "Epoch 180, Loss: 0.47475493910732763\n",
      "Epoch 181, Loss: 0.47455157018218247\n",
      "Epoch 182, Loss: 0.4743853224635176\n",
      "Epoch 183, Loss: 0.47424550491599726\n",
      "Epoch 184, Loss: 0.47408413178207975\n",
      "Epoch 185, Loss: 0.47390385374022453\n",
      "Epoch 186, Loss: 0.4737383066259127\n",
      "Epoch 187, Loss: 0.47358797785338436\n",
      "Epoch 188, Loss: 0.4734560449826861\n",
      "Epoch 189, Loss: 0.47335738206045025\n",
      "Epoch 190, Loss: 0.4733162834925985\n",
      "Epoch 191, Loss: 0.4732413020737725\n",
      "Epoch 192, Loss: 0.47313239020014447\n",
      "Epoch 193, Loss: 0.4729457158004841\n",
      "Epoch 194, Loss: 0.47274703738102797\n",
      "Epoch 195, Loss: 0.47253750657270627\n",
      "Epoch 196, Loss: 0.4723544299968521\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2216604791763664\n",
      "Valence RMSE: 0.20863016366525727\n",
      "Arousal RMSE: 0.23396621736139747\n",
      "Test R^2 score: tensor([0.4725, 0.4580], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.3490, 0.3311], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.46524105694387263\n",
      "Num of epochs: 197\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Epoch 121, Loss: 0.49295912913079615\n",
      "Epoch 122, Loss: 0.4924773619139838\n",
      "Epoch 123, Loss: 0.49197607191949805\n",
      "Epoch 124, Loss: 0.4914699197831528\n",
      "Epoch 125, Loss: 0.49091483381312667\n",
      "Epoch 126, Loss: 0.49035147677946317\n",
      "Epoch 127, Loss: 0.4898087222825037\n",
      "Epoch 128, Loss: 0.48924014731550836\n",
      "Epoch 129, Loss: 0.48873550647212044\n",
      "Epoch 130, Loss: 0.48832307254971563\n",
      "Epoch 131, Loss: 0.48791418392821295\n",
      "Epoch 132, Loss: 0.4875902245243796\n",
      "Epoch 133, Loss: 0.48728851108037585\n",
      "Epoch 134, Loss: 0.48697062262181345\n",
      "Epoch 135, Loss: 0.4866339553030548\n",
      "Epoch 136, Loss: 0.48633446746734893\n",
      "Epoch 137, Loss: 0.4860310087358514\n",
      "Epoch 138, Loss: 0.4856786412545615\n",
      "Epoch 139, Loss: 0.48531372107004\n",
      "Epoch 140, Loss: 0.48495712985488587\n",
      "Epoch 141, Loss: 0.4846131600652309\n",
      "Epoch 142, Loss: 0.48425966858652364\n",
      "Epoch 143, Loss: 0.4838914149236435\n",
      "Epoch 144, Loss: 0.48352437546282767\n",
      "Epoch 145, Loss: 0.4832299911937133\n",
      "Epoch 146, Loss: 0.4832586529465723\n",
      "Epoch 147, Loss: 0.48356744143113517\n",
      "Epoch 148, Loss: 0.4828108019011922\n",
      "Epoch 149, Loss: 0.4818912939738211\n",
      "Epoch 150, Loss: 0.48239416745575797\n",
      "Epoch 151, Loss: 0.4814952491721961\n",
      "Epoch 152, Loss: 0.4811822818820502\n",
      "Epoch 153, Loss: 0.4812753156716337\n",
      "Epoch 154, Loss: 0.4804387742282827\n",
      "Epoch 155, Loss: 0.48058189052417905\n",
      "Epoch 156, Loss: 0.4801288115497346\n",
      "Epoch 157, Loss: 0.4797465937246525\n",
      "Epoch 158, Loss: 0.4797622479529733\n",
      "Epoch 159, Loss: 0.4791664679844762\n",
      "Epoch 160, Loss: 0.47904359923301915\n",
      "Epoch 161, Loss: 0.4788483533353555\n",
      "Epoch 162, Loss: 0.4783709093142749\n",
      "Epoch 163, Loss: 0.4783027642591491\n",
      "Epoch 164, Loss: 0.4780003946284736\n",
      "Epoch 165, Loss: 0.47766356610156796\n",
      "Epoch 166, Loss: 0.47756940749162413\n",
      "Epoch 167, Loss: 0.4772720526560437\n",
      "Epoch 168, Loss: 0.477001191512605\n",
      "Epoch 169, Loss: 0.47690651155802954\n",
      "Epoch 170, Loss: 0.47663605231839845\n",
      "Epoch 171, Loss: 0.4763715079887003\n",
      "Epoch 172, Loss: 0.4762479807342815\n",
      "Epoch 173, Loss: 0.47603155482909787\n",
      "Epoch 174, Loss: 0.47580085926956445\n",
      "Epoch 175, Loss: 0.4756396378860013\n",
      "Epoch 176, Loss: 0.4754627232666358\n",
      "Epoch 177, Loss: 0.4752513954316692\n",
      "Epoch 178, Loss: 0.47507140350730925\n",
      "Epoch 179, Loss: 0.47492471273973497\n",
      "Epoch 180, Loss: 0.47475493910732763\n",
      "Epoch 181, Loss: 0.47455157018218247\n",
      "Epoch 182, Loss: 0.4743853224635176\n",
      "Epoch 183, Loss: 0.47424550491599726\n",
      "Epoch 184, Loss: 0.47408413178207975\n",
      "Epoch 185, Loss: 0.47390385374022453\n",
      "Epoch 186, Loss: 0.4737383066259127\n",
      "Epoch 187, Loss: 0.47358797785338436\n",
      "Epoch 188, Loss: 0.4734560449826861\n",
      "Epoch 189, Loss: 0.47335738206045025\n",
      "Epoch 190, Loss: 0.4733162834925985\n",
      "Epoch 191, Loss: 0.4732413020737725\n",
      "Epoch 192, Loss: 0.47313239020014447\n",
      "Epoch 193, Loss: 0.4729457158004841\n",
      "Epoch 194, Loss: 0.47274703738102797\n",
      "Epoch 195, Loss: 0.47253750657270627\n",
      "Epoch 196, Loss: 0.4723544299968521\n",
      "Epoch 197, Loss: 0.47221102907746554\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.22179324331602404\n",
      "Valence RMSE: 0.20873046193491335\n",
      "Arousal RMSE: 0.23412834049238843\n",
      "Test R^2 score: tensor([0.4720, 0.4572], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.3484, 0.3302], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.46461168895405913\n",
      "Num of epochs: 198\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Epoch 121, Loss: 0.49295912913079615\n",
      "Epoch 122, Loss: 0.4924773619139838\n",
      "Epoch 123, Loss: 0.49197607191949805\n",
      "Epoch 124, Loss: 0.4914699197831528\n",
      "Epoch 125, Loss: 0.49091483381312667\n",
      "Epoch 126, Loss: 0.49035147677946317\n",
      "Epoch 127, Loss: 0.4898087222825037\n",
      "Epoch 128, Loss: 0.48924014731550836\n",
      "Epoch 129, Loss: 0.48873550647212044\n",
      "Epoch 130, Loss: 0.48832307254971563\n",
      "Epoch 131, Loss: 0.48791418392821295\n",
      "Epoch 132, Loss: 0.4875902245243796\n",
      "Epoch 133, Loss: 0.48728851108037585\n",
      "Epoch 134, Loss: 0.48697062262181345\n",
      "Epoch 135, Loss: 0.4866339553030548\n",
      "Epoch 136, Loss: 0.48633446746734893\n",
      "Epoch 137, Loss: 0.4860310087358514\n",
      "Epoch 138, Loss: 0.4856786412545615\n",
      "Epoch 139, Loss: 0.48531372107004\n",
      "Epoch 140, Loss: 0.48495712985488587\n",
      "Epoch 141, Loss: 0.4846131600652309\n",
      "Epoch 142, Loss: 0.48425966858652364\n",
      "Epoch 143, Loss: 0.4838914149236435\n",
      "Epoch 144, Loss: 0.48352437546282767\n",
      "Epoch 145, Loss: 0.4832299911937133\n",
      "Epoch 146, Loss: 0.4832586529465723\n",
      "Epoch 147, Loss: 0.48356744143113517\n",
      "Epoch 148, Loss: 0.4828108019011922\n",
      "Epoch 149, Loss: 0.4818912939738211\n",
      "Epoch 150, Loss: 0.48239416745575797\n",
      "Epoch 151, Loss: 0.4814952491721961\n",
      "Epoch 152, Loss: 0.4811822818820502\n",
      "Epoch 153, Loss: 0.4812753156716337\n",
      "Epoch 154, Loss: 0.4804387742282827\n",
      "Epoch 155, Loss: 0.48058189052417905\n",
      "Epoch 156, Loss: 0.4801288115497346\n",
      "Epoch 157, Loss: 0.4797465937246525\n",
      "Epoch 158, Loss: 0.4797622479529733\n",
      "Epoch 159, Loss: 0.4791664679844762\n",
      "Epoch 160, Loss: 0.47904359923301915\n",
      "Epoch 161, Loss: 0.4788483533353555\n",
      "Epoch 162, Loss: 0.4783709093142749\n",
      "Epoch 163, Loss: 0.4783027642591491\n",
      "Epoch 164, Loss: 0.4780003946284736\n",
      "Epoch 165, Loss: 0.47766356610156796\n",
      "Epoch 166, Loss: 0.47756940749162413\n",
      "Epoch 167, Loss: 0.4772720526560437\n",
      "Epoch 168, Loss: 0.477001191512605\n",
      "Epoch 169, Loss: 0.47690651155802954\n",
      "Epoch 170, Loss: 0.47663605231839845\n",
      "Epoch 171, Loss: 0.4763715079887003\n",
      "Epoch 172, Loss: 0.4762479807342815\n",
      "Epoch 173, Loss: 0.47603155482909787\n",
      "Epoch 174, Loss: 0.47580085926956445\n",
      "Epoch 175, Loss: 0.4756396378860013\n",
      "Epoch 176, Loss: 0.4754627232666358\n",
      "Epoch 177, Loss: 0.4752513954316692\n",
      "Epoch 178, Loss: 0.47507140350730925\n",
      "Epoch 179, Loss: 0.47492471273973497\n",
      "Epoch 180, Loss: 0.47475493910732763\n",
      "Epoch 181, Loss: 0.47455157018218247\n",
      "Epoch 182, Loss: 0.4743853224635176\n",
      "Epoch 183, Loss: 0.47424550491599726\n",
      "Epoch 184, Loss: 0.47408413178207975\n",
      "Epoch 185, Loss: 0.47390385374022453\n",
      "Epoch 186, Loss: 0.4737383066259127\n",
      "Epoch 187, Loss: 0.47358797785338436\n",
      "Epoch 188, Loss: 0.4734560449826861\n",
      "Epoch 189, Loss: 0.47335738206045025\n",
      "Epoch 190, Loss: 0.4733162834925985\n",
      "Epoch 191, Loss: 0.4732413020737725\n",
      "Epoch 192, Loss: 0.47313239020014447\n",
      "Epoch 193, Loss: 0.4729457158004841\n",
      "Epoch 194, Loss: 0.47274703738102797\n",
      "Epoch 195, Loss: 0.47253750657270627\n",
      "Epoch 196, Loss: 0.4723544299968521\n",
      "Epoch 197, Loss: 0.47221102907746554\n",
      "Epoch 198, Loss: 0.47210844467503205\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.22115767107259496\n",
      "Valence RMSE: 0.20816225465185853\n",
      "Arousal RMSE: 0.23343073209576118\n",
      "Test R^2 score: tensor([0.4749, 0.4605], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.3520, 0.3342], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.46766190281607944\n",
      "Num of epochs: 199\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Epoch 121, Loss: 0.49295912913079615\n",
      "Epoch 122, Loss: 0.4924773619139838\n",
      "Epoch 123, Loss: 0.49197607191949805\n",
      "Epoch 124, Loss: 0.4914699197831528\n",
      "Epoch 125, Loss: 0.49091483381312667\n",
      "Epoch 126, Loss: 0.49035147677946317\n",
      "Epoch 127, Loss: 0.4898087222825037\n",
      "Epoch 128, Loss: 0.48924014731550836\n",
      "Epoch 129, Loss: 0.48873550647212044\n",
      "Epoch 130, Loss: 0.48832307254971563\n",
      "Epoch 131, Loss: 0.48791418392821295\n",
      "Epoch 132, Loss: 0.4875902245243796\n",
      "Epoch 133, Loss: 0.48728851108037585\n",
      "Epoch 134, Loss: 0.48697062262181345\n",
      "Epoch 135, Loss: 0.4866339553030548\n",
      "Epoch 136, Loss: 0.48633446746734893\n",
      "Epoch 137, Loss: 0.4860310087358514\n",
      "Epoch 138, Loss: 0.4856786412545615\n",
      "Epoch 139, Loss: 0.48531372107004\n",
      "Epoch 140, Loss: 0.48495712985488587\n",
      "Epoch 141, Loss: 0.4846131600652309\n",
      "Epoch 142, Loss: 0.48425966858652364\n",
      "Epoch 143, Loss: 0.4838914149236435\n",
      "Epoch 144, Loss: 0.48352437546282767\n",
      "Epoch 145, Loss: 0.4832299911937133\n",
      "Epoch 146, Loss: 0.4832586529465723\n",
      "Epoch 147, Loss: 0.48356744143113517\n",
      "Epoch 148, Loss: 0.4828108019011922\n",
      "Epoch 149, Loss: 0.4818912939738211\n",
      "Epoch 150, Loss: 0.48239416745575797\n",
      "Epoch 151, Loss: 0.4814952491721961\n",
      "Epoch 152, Loss: 0.4811822818820502\n",
      "Epoch 153, Loss: 0.4812753156716337\n",
      "Epoch 154, Loss: 0.4804387742282827\n",
      "Epoch 155, Loss: 0.48058189052417905\n",
      "Epoch 156, Loss: 0.4801288115497346\n",
      "Epoch 157, Loss: 0.4797465937246525\n",
      "Epoch 158, Loss: 0.4797622479529733\n",
      "Epoch 159, Loss: 0.4791664679844762\n",
      "Epoch 160, Loss: 0.47904359923301915\n",
      "Epoch 161, Loss: 0.4788483533353555\n",
      "Epoch 162, Loss: 0.4783709093142749\n",
      "Epoch 163, Loss: 0.4783027642591491\n",
      "Epoch 164, Loss: 0.4780003946284736\n",
      "Epoch 165, Loss: 0.47766356610156796\n",
      "Epoch 166, Loss: 0.47756940749162413\n",
      "Epoch 167, Loss: 0.4772720526560437\n",
      "Epoch 168, Loss: 0.477001191512605\n",
      "Epoch 169, Loss: 0.47690651155802954\n",
      "Epoch 170, Loss: 0.47663605231839845\n",
      "Epoch 171, Loss: 0.4763715079887003\n",
      "Epoch 172, Loss: 0.4762479807342815\n",
      "Epoch 173, Loss: 0.47603155482909787\n",
      "Epoch 174, Loss: 0.47580085926956445\n",
      "Epoch 175, Loss: 0.4756396378860013\n",
      "Epoch 176, Loss: 0.4754627232666358\n",
      "Epoch 177, Loss: 0.4752513954316692\n",
      "Epoch 178, Loss: 0.47507140350730925\n",
      "Epoch 179, Loss: 0.47492471273973497\n",
      "Epoch 180, Loss: 0.47475493910732763\n",
      "Epoch 181, Loss: 0.47455157018218247\n",
      "Epoch 182, Loss: 0.4743853224635176\n",
      "Epoch 183, Loss: 0.47424550491599726\n",
      "Epoch 184, Loss: 0.47408413178207975\n",
      "Epoch 185, Loss: 0.47390385374022453\n",
      "Epoch 186, Loss: 0.4737383066259127\n",
      "Epoch 187, Loss: 0.47358797785338436\n",
      "Epoch 188, Loss: 0.4734560449826861\n",
      "Epoch 189, Loss: 0.47335738206045025\n",
      "Epoch 190, Loss: 0.4733162834925985\n",
      "Epoch 191, Loss: 0.4732413020737725\n",
      "Epoch 192, Loss: 0.47313239020014447\n",
      "Epoch 193, Loss: 0.4729457158004841\n",
      "Epoch 194, Loss: 0.47274703738102797\n",
      "Epoch 195, Loss: 0.47253750657270627\n",
      "Epoch 196, Loss: 0.4723544299968521\n",
      "Epoch 197, Loss: 0.47221102907746554\n",
      "Epoch 198, Loss: 0.47210844467503205\n",
      "Epoch 199, Loss: 0.47205337980155637\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.22223455024437375\n",
      "Valence RMSE: 0.20908487270404477\n",
      "Arousal RMSE: 0.23464847464019786\n",
      "Test R^2 score: tensor([0.4702, 0.4548], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.3462, 0.3272], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4625072614373896\n",
      "Num of epochs: 200\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Epoch 121, Loss: 0.49295912913079615\n",
      "Epoch 122, Loss: 0.4924773619139838\n",
      "Epoch 123, Loss: 0.49197607191949805\n",
      "Epoch 124, Loss: 0.4914699197831528\n",
      "Epoch 125, Loss: 0.49091483381312667\n",
      "Epoch 126, Loss: 0.49035147677946317\n",
      "Epoch 127, Loss: 0.4898087222825037\n",
      "Epoch 128, Loss: 0.48924014731550836\n",
      "Epoch 129, Loss: 0.48873550647212044\n",
      "Epoch 130, Loss: 0.48832307254971563\n",
      "Epoch 131, Loss: 0.48791418392821295\n",
      "Epoch 132, Loss: 0.4875902245243796\n",
      "Epoch 133, Loss: 0.48728851108037585\n",
      "Epoch 134, Loss: 0.48697062262181345\n",
      "Epoch 135, Loss: 0.4866339553030548\n",
      "Epoch 136, Loss: 0.48633446746734893\n",
      "Epoch 137, Loss: 0.4860310087358514\n",
      "Epoch 138, Loss: 0.4856786412545615\n",
      "Epoch 139, Loss: 0.48531372107004\n",
      "Epoch 140, Loss: 0.48495712985488587\n",
      "Epoch 141, Loss: 0.4846131600652309\n",
      "Epoch 142, Loss: 0.48425966858652364\n",
      "Epoch 143, Loss: 0.4838914149236435\n",
      "Epoch 144, Loss: 0.48352437546282767\n",
      "Epoch 145, Loss: 0.4832299911937133\n",
      "Epoch 146, Loss: 0.4832586529465723\n",
      "Epoch 147, Loss: 0.48356744143113517\n",
      "Epoch 148, Loss: 0.4828108019011922\n",
      "Epoch 149, Loss: 0.4818912939738211\n",
      "Epoch 150, Loss: 0.48239416745575797\n",
      "Epoch 151, Loss: 0.4814952491721961\n",
      "Epoch 152, Loss: 0.4811822818820502\n",
      "Epoch 153, Loss: 0.4812753156716337\n",
      "Epoch 154, Loss: 0.4804387742282827\n",
      "Epoch 155, Loss: 0.48058189052417905\n",
      "Epoch 156, Loss: 0.4801288115497346\n",
      "Epoch 157, Loss: 0.4797465937246525\n",
      "Epoch 158, Loss: 0.4797622479529733\n",
      "Epoch 159, Loss: 0.4791664679844762\n",
      "Epoch 160, Loss: 0.47904359923301915\n",
      "Epoch 161, Loss: 0.4788483533353555\n",
      "Epoch 162, Loss: 0.4783709093142749\n",
      "Epoch 163, Loss: 0.4783027642591491\n",
      "Epoch 164, Loss: 0.4780003946284736\n",
      "Epoch 165, Loss: 0.47766356610156796\n",
      "Epoch 166, Loss: 0.47756940749162413\n",
      "Epoch 167, Loss: 0.4772720526560437\n",
      "Epoch 168, Loss: 0.477001191512605\n",
      "Epoch 169, Loss: 0.47690651155802954\n",
      "Epoch 170, Loss: 0.47663605231839845\n",
      "Epoch 171, Loss: 0.4763715079887003\n",
      "Epoch 172, Loss: 0.4762479807342815\n",
      "Epoch 173, Loss: 0.47603155482909787\n",
      "Epoch 174, Loss: 0.47580085926956445\n",
      "Epoch 175, Loss: 0.4756396378860013\n",
      "Epoch 176, Loss: 0.4754627232666358\n",
      "Epoch 177, Loss: 0.4752513954316692\n",
      "Epoch 178, Loss: 0.47507140350730925\n",
      "Epoch 179, Loss: 0.47492471273973497\n",
      "Epoch 180, Loss: 0.47475493910732763\n",
      "Epoch 181, Loss: 0.47455157018218247\n",
      "Epoch 182, Loss: 0.4743853224635176\n",
      "Epoch 183, Loss: 0.47424550491599726\n",
      "Epoch 184, Loss: 0.47408413178207975\n",
      "Epoch 185, Loss: 0.47390385374022453\n",
      "Epoch 186, Loss: 0.4737383066259127\n",
      "Epoch 187, Loss: 0.47358797785338436\n",
      "Epoch 188, Loss: 0.4734560449826861\n",
      "Epoch 189, Loss: 0.47335738206045025\n",
      "Epoch 190, Loss: 0.4733162834925985\n",
      "Epoch 191, Loss: 0.4732413020737725\n",
      "Epoch 192, Loss: 0.47313239020014447\n",
      "Epoch 193, Loss: 0.4729457158004841\n",
      "Epoch 194, Loss: 0.47274703738102797\n",
      "Epoch 195, Loss: 0.47253750657270627\n",
      "Epoch 196, Loss: 0.4723544299968521\n",
      "Epoch 197, Loss: 0.47221102907746554\n",
      "Epoch 198, Loss: 0.47210844467503205\n",
      "Epoch 199, Loss: 0.47205337980155637\n",
      "Epoch 200, Loss: 0.47204973383515075\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.22082012325067127\n",
      "Valence RMSE: 0.2078603242689502\n",
      "Arousal RMSE: 0.23306037685477587\n",
      "Test R^2 score: tensor([0.4764, 0.4622], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.3538, 0.3363], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4692783793885474\n",
      "Num of epochs: 201\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Epoch 121, Loss: 0.49295912913079615\n",
      "Epoch 122, Loss: 0.4924773619139838\n",
      "Epoch 123, Loss: 0.49197607191949805\n",
      "Epoch 124, Loss: 0.4914699197831528\n",
      "Epoch 125, Loss: 0.49091483381312667\n",
      "Epoch 126, Loss: 0.49035147677946317\n",
      "Epoch 127, Loss: 0.4898087222825037\n",
      "Epoch 128, Loss: 0.48924014731550836\n",
      "Epoch 129, Loss: 0.48873550647212044\n",
      "Epoch 130, Loss: 0.48832307254971563\n",
      "Epoch 131, Loss: 0.48791418392821295\n",
      "Epoch 132, Loss: 0.4875902245243796\n",
      "Epoch 133, Loss: 0.48728851108037585\n",
      "Epoch 134, Loss: 0.48697062262181345\n",
      "Epoch 135, Loss: 0.4866339553030548\n",
      "Epoch 136, Loss: 0.48633446746734893\n",
      "Epoch 137, Loss: 0.4860310087358514\n",
      "Epoch 138, Loss: 0.4856786412545615\n",
      "Epoch 139, Loss: 0.48531372107004\n",
      "Epoch 140, Loss: 0.48495712985488587\n",
      "Epoch 141, Loss: 0.4846131600652309\n",
      "Epoch 142, Loss: 0.48425966858652364\n",
      "Epoch 143, Loss: 0.4838914149236435\n",
      "Epoch 144, Loss: 0.48352437546282767\n",
      "Epoch 145, Loss: 0.4832299911937133\n",
      "Epoch 146, Loss: 0.4832586529465723\n",
      "Epoch 147, Loss: 0.48356744143113517\n",
      "Epoch 148, Loss: 0.4828108019011922\n",
      "Epoch 149, Loss: 0.4818912939738211\n",
      "Epoch 150, Loss: 0.48239416745575797\n",
      "Epoch 151, Loss: 0.4814952491721961\n",
      "Epoch 152, Loss: 0.4811822818820502\n",
      "Epoch 153, Loss: 0.4812753156716337\n",
      "Epoch 154, Loss: 0.4804387742282827\n",
      "Epoch 155, Loss: 0.48058189052417905\n",
      "Epoch 156, Loss: 0.4801288115497346\n",
      "Epoch 157, Loss: 0.4797465937246525\n",
      "Epoch 158, Loss: 0.4797622479529733\n",
      "Epoch 159, Loss: 0.4791664679844762\n",
      "Epoch 160, Loss: 0.47904359923301915\n",
      "Epoch 161, Loss: 0.4788483533353555\n",
      "Epoch 162, Loss: 0.4783709093142749\n",
      "Epoch 163, Loss: 0.4783027642591491\n",
      "Epoch 164, Loss: 0.4780003946284736\n",
      "Epoch 165, Loss: 0.47766356610156796\n",
      "Epoch 166, Loss: 0.47756940749162413\n",
      "Epoch 167, Loss: 0.4772720526560437\n",
      "Epoch 168, Loss: 0.477001191512605\n",
      "Epoch 169, Loss: 0.47690651155802954\n",
      "Epoch 170, Loss: 0.47663605231839845\n",
      "Epoch 171, Loss: 0.4763715079887003\n",
      "Epoch 172, Loss: 0.4762479807342815\n",
      "Epoch 173, Loss: 0.47603155482909787\n",
      "Epoch 174, Loss: 0.47580085926956445\n",
      "Epoch 175, Loss: 0.4756396378860013\n",
      "Epoch 176, Loss: 0.4754627232666358\n",
      "Epoch 177, Loss: 0.4752513954316692\n",
      "Epoch 178, Loss: 0.47507140350730925\n",
      "Epoch 179, Loss: 0.47492471273973497\n",
      "Epoch 180, Loss: 0.47475493910732763\n",
      "Epoch 181, Loss: 0.47455157018218247\n",
      "Epoch 182, Loss: 0.4743853224635176\n",
      "Epoch 183, Loss: 0.47424550491599726\n",
      "Epoch 184, Loss: 0.47408413178207975\n",
      "Epoch 185, Loss: 0.47390385374022453\n",
      "Epoch 186, Loss: 0.4737383066259127\n",
      "Epoch 187, Loss: 0.47358797785338436\n",
      "Epoch 188, Loss: 0.4734560449826861\n",
      "Epoch 189, Loss: 0.47335738206045025\n",
      "Epoch 190, Loss: 0.4733162834925985\n",
      "Epoch 191, Loss: 0.4732413020737725\n",
      "Epoch 192, Loss: 0.47313239020014447\n",
      "Epoch 193, Loss: 0.4729457158004841\n",
      "Epoch 194, Loss: 0.47274703738102797\n",
      "Epoch 195, Loss: 0.47253750657270627\n",
      "Epoch 196, Loss: 0.4723544299968521\n",
      "Epoch 197, Loss: 0.47221102907746554\n",
      "Epoch 198, Loss: 0.47210844467503205\n",
      "Epoch 199, Loss: 0.47205337980155637\n",
      "Epoch 200, Loss: 0.47204973383515075\n",
      "Epoch 201, Loss: 0.4720427890593303\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.22235232305998925\n",
      "Valence RMSE: 0.20917856609355912\n",
      "Arousal RMSE: 0.23478807173148764\n",
      "Test R^2 score: tensor([0.4697, 0.4542], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.3456, 0.3264], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.46194536034957184\n",
      "Num of epochs: 202\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Epoch 121, Loss: 0.49295912913079615\n",
      "Epoch 122, Loss: 0.4924773619139838\n",
      "Epoch 123, Loss: 0.49197607191949805\n",
      "Epoch 124, Loss: 0.4914699197831528\n",
      "Epoch 125, Loss: 0.49091483381312667\n",
      "Epoch 126, Loss: 0.49035147677946317\n",
      "Epoch 127, Loss: 0.4898087222825037\n",
      "Epoch 128, Loss: 0.48924014731550836\n",
      "Epoch 129, Loss: 0.48873550647212044\n",
      "Epoch 130, Loss: 0.48832307254971563\n",
      "Epoch 131, Loss: 0.48791418392821295\n",
      "Epoch 132, Loss: 0.4875902245243796\n",
      "Epoch 133, Loss: 0.48728851108037585\n",
      "Epoch 134, Loss: 0.48697062262181345\n",
      "Epoch 135, Loss: 0.4866339553030548\n",
      "Epoch 136, Loss: 0.48633446746734893\n",
      "Epoch 137, Loss: 0.4860310087358514\n",
      "Epoch 138, Loss: 0.4856786412545615\n",
      "Epoch 139, Loss: 0.48531372107004\n",
      "Epoch 140, Loss: 0.48495712985488587\n",
      "Epoch 141, Loss: 0.4846131600652309\n",
      "Epoch 142, Loss: 0.48425966858652364\n",
      "Epoch 143, Loss: 0.4838914149236435\n",
      "Epoch 144, Loss: 0.48352437546282767\n",
      "Epoch 145, Loss: 0.4832299911937133\n",
      "Epoch 146, Loss: 0.4832586529465723\n",
      "Epoch 147, Loss: 0.48356744143113517\n",
      "Epoch 148, Loss: 0.4828108019011922\n",
      "Epoch 149, Loss: 0.4818912939738211\n",
      "Epoch 150, Loss: 0.48239416745575797\n",
      "Epoch 151, Loss: 0.4814952491721961\n",
      "Epoch 152, Loss: 0.4811822818820502\n",
      "Epoch 153, Loss: 0.4812753156716337\n",
      "Epoch 154, Loss: 0.4804387742282827\n",
      "Epoch 155, Loss: 0.48058189052417905\n",
      "Epoch 156, Loss: 0.4801288115497346\n",
      "Epoch 157, Loss: 0.4797465937246525\n",
      "Epoch 158, Loss: 0.4797622479529733\n",
      "Epoch 159, Loss: 0.4791664679844762\n",
      "Epoch 160, Loss: 0.47904359923301915\n",
      "Epoch 161, Loss: 0.4788483533353555\n",
      "Epoch 162, Loss: 0.4783709093142749\n",
      "Epoch 163, Loss: 0.4783027642591491\n",
      "Epoch 164, Loss: 0.4780003946284736\n",
      "Epoch 165, Loss: 0.47766356610156796\n",
      "Epoch 166, Loss: 0.47756940749162413\n",
      "Epoch 167, Loss: 0.4772720526560437\n",
      "Epoch 168, Loss: 0.477001191512605\n",
      "Epoch 169, Loss: 0.47690651155802954\n",
      "Epoch 170, Loss: 0.47663605231839845\n",
      "Epoch 171, Loss: 0.4763715079887003\n",
      "Epoch 172, Loss: 0.4762479807342815\n",
      "Epoch 173, Loss: 0.47603155482909787\n",
      "Epoch 174, Loss: 0.47580085926956445\n",
      "Epoch 175, Loss: 0.4756396378860013\n",
      "Epoch 176, Loss: 0.4754627232666358\n",
      "Epoch 177, Loss: 0.4752513954316692\n",
      "Epoch 178, Loss: 0.47507140350730925\n",
      "Epoch 179, Loss: 0.47492471273973497\n",
      "Epoch 180, Loss: 0.47475493910732763\n",
      "Epoch 181, Loss: 0.47455157018218247\n",
      "Epoch 182, Loss: 0.4743853224635176\n",
      "Epoch 183, Loss: 0.47424550491599726\n",
      "Epoch 184, Loss: 0.47408413178207975\n",
      "Epoch 185, Loss: 0.47390385374022453\n",
      "Epoch 186, Loss: 0.4737383066259127\n",
      "Epoch 187, Loss: 0.47358797785338436\n",
      "Epoch 188, Loss: 0.4734560449826861\n",
      "Epoch 189, Loss: 0.47335738206045025\n",
      "Epoch 190, Loss: 0.4733162834925985\n",
      "Epoch 191, Loss: 0.4732413020737725\n",
      "Epoch 192, Loss: 0.47313239020014447\n",
      "Epoch 193, Loss: 0.4729457158004841\n",
      "Epoch 194, Loss: 0.47274703738102797\n",
      "Epoch 195, Loss: 0.47253750657270627\n",
      "Epoch 196, Loss: 0.4723544299968521\n",
      "Epoch 197, Loss: 0.47221102907746554\n",
      "Epoch 198, Loss: 0.47210844467503205\n",
      "Epoch 199, Loss: 0.47205337980155637\n",
      "Epoch 200, Loss: 0.47204973383515075\n",
      "Epoch 201, Loss: 0.4720427890593303\n",
      "Epoch 202, Loss: 0.4719159657967507\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.22076397315292146\n",
      "Valence RMSE: 0.2078098433478525\n",
      "Arousal RMSE: 0.23299899719155157\n",
      "Test R^2 score: tensor([0.4766, 0.4625], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.3542, 0.3366], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.46954715487704357\n",
      "Num of epochs: 203\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Epoch 121, Loss: 0.49295912913079615\n",
      "Epoch 122, Loss: 0.4924773619139838\n",
      "Epoch 123, Loss: 0.49197607191949805\n",
      "Epoch 124, Loss: 0.4914699197831528\n",
      "Epoch 125, Loss: 0.49091483381312667\n",
      "Epoch 126, Loss: 0.49035147677946317\n",
      "Epoch 127, Loss: 0.4898087222825037\n",
      "Epoch 128, Loss: 0.48924014731550836\n",
      "Epoch 129, Loss: 0.48873550647212044\n",
      "Epoch 130, Loss: 0.48832307254971563\n",
      "Epoch 131, Loss: 0.48791418392821295\n",
      "Epoch 132, Loss: 0.4875902245243796\n",
      "Epoch 133, Loss: 0.48728851108037585\n",
      "Epoch 134, Loss: 0.48697062262181345\n",
      "Epoch 135, Loss: 0.4866339553030548\n",
      "Epoch 136, Loss: 0.48633446746734893\n",
      "Epoch 137, Loss: 0.4860310087358514\n",
      "Epoch 138, Loss: 0.4856786412545615\n",
      "Epoch 139, Loss: 0.48531372107004\n",
      "Epoch 140, Loss: 0.48495712985488587\n",
      "Epoch 141, Loss: 0.4846131600652309\n",
      "Epoch 142, Loss: 0.48425966858652364\n",
      "Epoch 143, Loss: 0.4838914149236435\n",
      "Epoch 144, Loss: 0.48352437546282767\n",
      "Epoch 145, Loss: 0.4832299911937133\n",
      "Epoch 146, Loss: 0.4832586529465723\n",
      "Epoch 147, Loss: 0.48356744143113517\n",
      "Epoch 148, Loss: 0.4828108019011922\n",
      "Epoch 149, Loss: 0.4818912939738211\n",
      "Epoch 150, Loss: 0.48239416745575797\n",
      "Epoch 151, Loss: 0.4814952491721961\n",
      "Epoch 152, Loss: 0.4811822818820502\n",
      "Epoch 153, Loss: 0.4812753156716337\n",
      "Epoch 154, Loss: 0.4804387742282827\n",
      "Epoch 155, Loss: 0.48058189052417905\n",
      "Epoch 156, Loss: 0.4801288115497346\n",
      "Epoch 157, Loss: 0.4797465937246525\n",
      "Epoch 158, Loss: 0.4797622479529733\n",
      "Epoch 159, Loss: 0.4791664679844762\n",
      "Epoch 160, Loss: 0.47904359923301915\n",
      "Epoch 161, Loss: 0.4788483533353555\n",
      "Epoch 162, Loss: 0.4783709093142749\n",
      "Epoch 163, Loss: 0.4783027642591491\n",
      "Epoch 164, Loss: 0.4780003946284736\n",
      "Epoch 165, Loss: 0.47766356610156796\n",
      "Epoch 166, Loss: 0.47756940749162413\n",
      "Epoch 167, Loss: 0.4772720526560437\n",
      "Epoch 168, Loss: 0.477001191512605\n",
      "Epoch 169, Loss: 0.47690651155802954\n",
      "Epoch 170, Loss: 0.47663605231839845\n",
      "Epoch 171, Loss: 0.4763715079887003\n",
      "Epoch 172, Loss: 0.4762479807342815\n",
      "Epoch 173, Loss: 0.47603155482909787\n",
      "Epoch 174, Loss: 0.47580085926956445\n",
      "Epoch 175, Loss: 0.4756396378860013\n",
      "Epoch 176, Loss: 0.4754627232666358\n",
      "Epoch 177, Loss: 0.4752513954316692\n",
      "Epoch 178, Loss: 0.47507140350730925\n",
      "Epoch 179, Loss: 0.47492471273973497\n",
      "Epoch 180, Loss: 0.47475493910732763\n",
      "Epoch 181, Loss: 0.47455157018218247\n",
      "Epoch 182, Loss: 0.4743853224635176\n",
      "Epoch 183, Loss: 0.47424550491599726\n",
      "Epoch 184, Loss: 0.47408413178207975\n",
      "Epoch 185, Loss: 0.47390385374022453\n",
      "Epoch 186, Loss: 0.4737383066259127\n",
      "Epoch 187, Loss: 0.47358797785338436\n",
      "Epoch 188, Loss: 0.4734560449826861\n",
      "Epoch 189, Loss: 0.47335738206045025\n",
      "Epoch 190, Loss: 0.4733162834925985\n",
      "Epoch 191, Loss: 0.4732413020737725\n",
      "Epoch 192, Loss: 0.47313239020014447\n",
      "Epoch 193, Loss: 0.4729457158004841\n",
      "Epoch 194, Loss: 0.47274703738102797\n",
      "Epoch 195, Loss: 0.47253750657270627\n",
      "Epoch 196, Loss: 0.4723544299968521\n",
      "Epoch 197, Loss: 0.47221102907746554\n",
      "Epoch 198, Loss: 0.47210844467503205\n",
      "Epoch 199, Loss: 0.47205337980155637\n",
      "Epoch 200, Loss: 0.47204973383515075\n",
      "Epoch 201, Loss: 0.4720427890593303\n",
      "Epoch 202, Loss: 0.4719159657967507\n",
      "Epoch 203, Loss: 0.471735017109987\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2217821535616868\n",
      "Valence RMSE: 0.20867108093696174\n",
      "Arousal RMSE: 0.23416025977419655\n",
      "Test R^2 score: tensor([0.4723, 0.4571], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.3488, 0.3300], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.46468787561234964\n",
      "Num of epochs: 204\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Epoch 121, Loss: 0.49295912913079615\n",
      "Epoch 122, Loss: 0.4924773619139838\n",
      "Epoch 123, Loss: 0.49197607191949805\n",
      "Epoch 124, Loss: 0.4914699197831528\n",
      "Epoch 125, Loss: 0.49091483381312667\n",
      "Epoch 126, Loss: 0.49035147677946317\n",
      "Epoch 127, Loss: 0.4898087222825037\n",
      "Epoch 128, Loss: 0.48924014731550836\n",
      "Epoch 129, Loss: 0.48873550647212044\n",
      "Epoch 130, Loss: 0.48832307254971563\n",
      "Epoch 131, Loss: 0.48791418392821295\n",
      "Epoch 132, Loss: 0.4875902245243796\n",
      "Epoch 133, Loss: 0.48728851108037585\n",
      "Epoch 134, Loss: 0.48697062262181345\n",
      "Epoch 135, Loss: 0.4866339553030548\n",
      "Epoch 136, Loss: 0.48633446746734893\n",
      "Epoch 137, Loss: 0.4860310087358514\n",
      "Epoch 138, Loss: 0.4856786412545615\n",
      "Epoch 139, Loss: 0.48531372107004\n",
      "Epoch 140, Loss: 0.48495712985488587\n",
      "Epoch 141, Loss: 0.4846131600652309\n",
      "Epoch 142, Loss: 0.48425966858652364\n",
      "Epoch 143, Loss: 0.4838914149236435\n",
      "Epoch 144, Loss: 0.48352437546282767\n",
      "Epoch 145, Loss: 0.4832299911937133\n",
      "Epoch 146, Loss: 0.4832586529465723\n",
      "Epoch 147, Loss: 0.48356744143113517\n",
      "Epoch 148, Loss: 0.4828108019011922\n",
      "Epoch 149, Loss: 0.4818912939738211\n",
      "Epoch 150, Loss: 0.48239416745575797\n",
      "Epoch 151, Loss: 0.4814952491721961\n",
      "Epoch 152, Loss: 0.4811822818820502\n",
      "Epoch 153, Loss: 0.4812753156716337\n",
      "Epoch 154, Loss: 0.4804387742282827\n",
      "Epoch 155, Loss: 0.48058189052417905\n",
      "Epoch 156, Loss: 0.4801288115497346\n",
      "Epoch 157, Loss: 0.4797465937246525\n",
      "Epoch 158, Loss: 0.4797622479529733\n",
      "Epoch 159, Loss: 0.4791664679844762\n",
      "Epoch 160, Loss: 0.47904359923301915\n",
      "Epoch 161, Loss: 0.4788483533353555\n",
      "Epoch 162, Loss: 0.4783709093142749\n",
      "Epoch 163, Loss: 0.4783027642591491\n",
      "Epoch 164, Loss: 0.4780003946284736\n",
      "Epoch 165, Loss: 0.47766356610156796\n",
      "Epoch 166, Loss: 0.47756940749162413\n",
      "Epoch 167, Loss: 0.4772720526560437\n",
      "Epoch 168, Loss: 0.477001191512605\n",
      "Epoch 169, Loss: 0.47690651155802954\n",
      "Epoch 170, Loss: 0.47663605231839845\n",
      "Epoch 171, Loss: 0.4763715079887003\n",
      "Epoch 172, Loss: 0.4762479807342815\n",
      "Epoch 173, Loss: 0.47603155482909787\n",
      "Epoch 174, Loss: 0.47580085926956445\n",
      "Epoch 175, Loss: 0.4756396378860013\n",
      "Epoch 176, Loss: 0.4754627232666358\n",
      "Epoch 177, Loss: 0.4752513954316692\n",
      "Epoch 178, Loss: 0.47507140350730925\n",
      "Epoch 179, Loss: 0.47492471273973497\n",
      "Epoch 180, Loss: 0.47475493910732763\n",
      "Epoch 181, Loss: 0.47455157018218247\n",
      "Epoch 182, Loss: 0.4743853224635176\n",
      "Epoch 183, Loss: 0.47424550491599726\n",
      "Epoch 184, Loss: 0.47408413178207975\n",
      "Epoch 185, Loss: 0.47390385374022453\n",
      "Epoch 186, Loss: 0.4737383066259127\n",
      "Epoch 187, Loss: 0.47358797785338436\n",
      "Epoch 188, Loss: 0.4734560449826861\n",
      "Epoch 189, Loss: 0.47335738206045025\n",
      "Epoch 190, Loss: 0.4733162834925985\n",
      "Epoch 191, Loss: 0.4732413020737725\n",
      "Epoch 192, Loss: 0.47313239020014447\n",
      "Epoch 193, Loss: 0.4729457158004841\n",
      "Epoch 194, Loss: 0.47274703738102797\n",
      "Epoch 195, Loss: 0.47253750657270627\n",
      "Epoch 196, Loss: 0.4723544299968521\n",
      "Epoch 197, Loss: 0.47221102907746554\n",
      "Epoch 198, Loss: 0.47210844467503205\n",
      "Epoch 199, Loss: 0.47205337980155637\n",
      "Epoch 200, Loss: 0.47204973383515075\n",
      "Epoch 201, Loss: 0.4720427890593303\n",
      "Epoch 202, Loss: 0.4719159657967507\n",
      "Epoch 203, Loss: 0.471735017109987\n",
      "Epoch 204, Loss: 0.4714985533244086\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.22096656982627289\n",
      "Valence RMSE: 0.20800958026245156\n",
      "Arousal RMSE: 0.23320476942081822\n",
      "Test R^2 score: tensor([0.4756, 0.4615], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.3529, 0.3355], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.46856894342854544\n",
      "Num of epochs: 205\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Epoch 121, Loss: 0.49295912913079615\n",
      "Epoch 122, Loss: 0.4924773619139838\n",
      "Epoch 123, Loss: 0.49197607191949805\n",
      "Epoch 124, Loss: 0.4914699197831528\n",
      "Epoch 125, Loss: 0.49091483381312667\n",
      "Epoch 126, Loss: 0.49035147677946317\n",
      "Epoch 127, Loss: 0.4898087222825037\n",
      "Epoch 128, Loss: 0.48924014731550836\n",
      "Epoch 129, Loss: 0.48873550647212044\n",
      "Epoch 130, Loss: 0.48832307254971563\n",
      "Epoch 131, Loss: 0.48791418392821295\n",
      "Epoch 132, Loss: 0.4875902245243796\n",
      "Epoch 133, Loss: 0.48728851108037585\n",
      "Epoch 134, Loss: 0.48697062262181345\n",
      "Epoch 135, Loss: 0.4866339553030548\n",
      "Epoch 136, Loss: 0.48633446746734893\n",
      "Epoch 137, Loss: 0.4860310087358514\n",
      "Epoch 138, Loss: 0.4856786412545615\n",
      "Epoch 139, Loss: 0.48531372107004\n",
      "Epoch 140, Loss: 0.48495712985488587\n",
      "Epoch 141, Loss: 0.4846131600652309\n",
      "Epoch 142, Loss: 0.48425966858652364\n",
      "Epoch 143, Loss: 0.4838914149236435\n",
      "Epoch 144, Loss: 0.48352437546282767\n",
      "Epoch 145, Loss: 0.4832299911937133\n",
      "Epoch 146, Loss: 0.4832586529465723\n",
      "Epoch 147, Loss: 0.48356744143113517\n",
      "Epoch 148, Loss: 0.4828108019011922\n",
      "Epoch 149, Loss: 0.4818912939738211\n",
      "Epoch 150, Loss: 0.48239416745575797\n",
      "Epoch 151, Loss: 0.4814952491721961\n",
      "Epoch 152, Loss: 0.4811822818820502\n",
      "Epoch 153, Loss: 0.4812753156716337\n",
      "Epoch 154, Loss: 0.4804387742282827\n",
      "Epoch 155, Loss: 0.48058189052417905\n",
      "Epoch 156, Loss: 0.4801288115497346\n",
      "Epoch 157, Loss: 0.4797465937246525\n",
      "Epoch 158, Loss: 0.4797622479529733\n",
      "Epoch 159, Loss: 0.4791664679844762\n",
      "Epoch 160, Loss: 0.47904359923301915\n",
      "Epoch 161, Loss: 0.4788483533353555\n",
      "Epoch 162, Loss: 0.4783709093142749\n",
      "Epoch 163, Loss: 0.4783027642591491\n",
      "Epoch 164, Loss: 0.4780003946284736\n",
      "Epoch 165, Loss: 0.47766356610156796\n",
      "Epoch 166, Loss: 0.47756940749162413\n",
      "Epoch 167, Loss: 0.4772720526560437\n",
      "Epoch 168, Loss: 0.477001191512605\n",
      "Epoch 169, Loss: 0.47690651155802954\n",
      "Epoch 170, Loss: 0.47663605231839845\n",
      "Epoch 171, Loss: 0.4763715079887003\n",
      "Epoch 172, Loss: 0.4762479807342815\n",
      "Epoch 173, Loss: 0.47603155482909787\n",
      "Epoch 174, Loss: 0.47580085926956445\n",
      "Epoch 175, Loss: 0.4756396378860013\n",
      "Epoch 176, Loss: 0.4754627232666358\n",
      "Epoch 177, Loss: 0.4752513954316692\n",
      "Epoch 178, Loss: 0.47507140350730925\n",
      "Epoch 179, Loss: 0.47492471273973497\n",
      "Epoch 180, Loss: 0.47475493910732763\n",
      "Epoch 181, Loss: 0.47455157018218247\n",
      "Epoch 182, Loss: 0.4743853224635176\n",
      "Epoch 183, Loss: 0.47424550491599726\n",
      "Epoch 184, Loss: 0.47408413178207975\n",
      "Epoch 185, Loss: 0.47390385374022453\n",
      "Epoch 186, Loss: 0.4737383066259127\n",
      "Epoch 187, Loss: 0.47358797785338436\n",
      "Epoch 188, Loss: 0.4734560449826861\n",
      "Epoch 189, Loss: 0.47335738206045025\n",
      "Epoch 190, Loss: 0.4733162834925985\n",
      "Epoch 191, Loss: 0.4732413020737725\n",
      "Epoch 192, Loss: 0.47313239020014447\n",
      "Epoch 193, Loss: 0.4729457158004841\n",
      "Epoch 194, Loss: 0.47274703738102797\n",
      "Epoch 195, Loss: 0.47253750657270627\n",
      "Epoch 196, Loss: 0.4723544299968521\n",
      "Epoch 197, Loss: 0.47221102907746554\n",
      "Epoch 198, Loss: 0.47210844467503205\n",
      "Epoch 199, Loss: 0.47205337980155637\n",
      "Epoch 200, Loss: 0.47204973383515075\n",
      "Epoch 201, Loss: 0.4720427890593303\n",
      "Epoch 202, Loss: 0.4719159657967507\n",
      "Epoch 203, Loss: 0.471735017109987\n",
      "Epoch 204, Loss: 0.4714985533244086\n",
      "Epoch 205, Loss: 0.4713143302002759\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.22108176302705324\n",
      "Valence RMSE: 0.20811641510745918\n",
      "Arousal RMSE: 0.23332777299138652\n",
      "Test R^2 score: tensor([0.4751, 0.4609], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.3522, 0.3348], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4680154530124125\n",
      "Num of epochs: 206\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Epoch 121, Loss: 0.49295912913079615\n",
      "Epoch 122, Loss: 0.4924773619139838\n",
      "Epoch 123, Loss: 0.49197607191949805\n",
      "Epoch 124, Loss: 0.4914699197831528\n",
      "Epoch 125, Loss: 0.49091483381312667\n",
      "Epoch 126, Loss: 0.49035147677946317\n",
      "Epoch 127, Loss: 0.4898087222825037\n",
      "Epoch 128, Loss: 0.48924014731550836\n",
      "Epoch 129, Loss: 0.48873550647212044\n",
      "Epoch 130, Loss: 0.48832307254971563\n",
      "Epoch 131, Loss: 0.48791418392821295\n",
      "Epoch 132, Loss: 0.4875902245243796\n",
      "Epoch 133, Loss: 0.48728851108037585\n",
      "Epoch 134, Loss: 0.48697062262181345\n",
      "Epoch 135, Loss: 0.4866339553030548\n",
      "Epoch 136, Loss: 0.48633446746734893\n",
      "Epoch 137, Loss: 0.4860310087358514\n",
      "Epoch 138, Loss: 0.4856786412545615\n",
      "Epoch 139, Loss: 0.48531372107004\n",
      "Epoch 140, Loss: 0.48495712985488587\n",
      "Epoch 141, Loss: 0.4846131600652309\n",
      "Epoch 142, Loss: 0.48425966858652364\n",
      "Epoch 143, Loss: 0.4838914149236435\n",
      "Epoch 144, Loss: 0.48352437546282767\n",
      "Epoch 145, Loss: 0.4832299911937133\n",
      "Epoch 146, Loss: 0.4832586529465723\n",
      "Epoch 147, Loss: 0.48356744143113517\n",
      "Epoch 148, Loss: 0.4828108019011922\n",
      "Epoch 149, Loss: 0.4818912939738211\n",
      "Epoch 150, Loss: 0.48239416745575797\n",
      "Epoch 151, Loss: 0.4814952491721961\n",
      "Epoch 152, Loss: 0.4811822818820502\n",
      "Epoch 153, Loss: 0.4812753156716337\n",
      "Epoch 154, Loss: 0.4804387742282827\n",
      "Epoch 155, Loss: 0.48058189052417905\n",
      "Epoch 156, Loss: 0.4801288115497346\n",
      "Epoch 157, Loss: 0.4797465937246525\n",
      "Epoch 158, Loss: 0.4797622479529733\n",
      "Epoch 159, Loss: 0.4791664679844762\n",
      "Epoch 160, Loss: 0.47904359923301915\n",
      "Epoch 161, Loss: 0.4788483533353555\n",
      "Epoch 162, Loss: 0.4783709093142749\n",
      "Epoch 163, Loss: 0.4783027642591491\n",
      "Epoch 164, Loss: 0.4780003946284736\n",
      "Epoch 165, Loss: 0.47766356610156796\n",
      "Epoch 166, Loss: 0.47756940749162413\n",
      "Epoch 167, Loss: 0.4772720526560437\n",
      "Epoch 168, Loss: 0.477001191512605\n",
      "Epoch 169, Loss: 0.47690651155802954\n",
      "Epoch 170, Loss: 0.47663605231839845\n",
      "Epoch 171, Loss: 0.4763715079887003\n",
      "Epoch 172, Loss: 0.4762479807342815\n",
      "Epoch 173, Loss: 0.47603155482909787\n",
      "Epoch 174, Loss: 0.47580085926956445\n",
      "Epoch 175, Loss: 0.4756396378860013\n",
      "Epoch 176, Loss: 0.4754627232666358\n",
      "Epoch 177, Loss: 0.4752513954316692\n",
      "Epoch 178, Loss: 0.47507140350730925\n",
      "Epoch 179, Loss: 0.47492471273973497\n",
      "Epoch 180, Loss: 0.47475493910732763\n",
      "Epoch 181, Loss: 0.47455157018218247\n",
      "Epoch 182, Loss: 0.4743853224635176\n",
      "Epoch 183, Loss: 0.47424550491599726\n",
      "Epoch 184, Loss: 0.47408413178207975\n",
      "Epoch 185, Loss: 0.47390385374022453\n",
      "Epoch 186, Loss: 0.4737383066259127\n",
      "Epoch 187, Loss: 0.47358797785338436\n",
      "Epoch 188, Loss: 0.4734560449826861\n",
      "Epoch 189, Loss: 0.47335738206045025\n",
      "Epoch 190, Loss: 0.4733162834925985\n",
      "Epoch 191, Loss: 0.4732413020737725\n",
      "Epoch 192, Loss: 0.47313239020014447\n",
      "Epoch 193, Loss: 0.4729457158004841\n",
      "Epoch 194, Loss: 0.47274703738102797\n",
      "Epoch 195, Loss: 0.47253750657270627\n",
      "Epoch 196, Loss: 0.4723544299968521\n",
      "Epoch 197, Loss: 0.47221102907746554\n",
      "Epoch 198, Loss: 0.47210844467503205\n",
      "Epoch 199, Loss: 0.47205337980155637\n",
      "Epoch 200, Loss: 0.47204973383515075\n",
      "Epoch 201, Loss: 0.4720427890593303\n",
      "Epoch 202, Loss: 0.4719159657967507\n",
      "Epoch 203, Loss: 0.471735017109987\n",
      "Epoch 204, Loss: 0.4714985533244086\n",
      "Epoch 205, Loss: 0.4713143302002759\n",
      "Epoch 206, Loss: 0.47118574544243713\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.22136580344695952\n",
      "Valence RMSE: 0.20836262915136952\n",
      "Arousal RMSE: 0.23364642656065746\n",
      "Test R^2 score: tensor([0.4739, 0.4595], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.3507, 0.3330], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.46665739414158486\n",
      "Num of epochs: 207\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Epoch 121, Loss: 0.49295912913079615\n",
      "Epoch 122, Loss: 0.4924773619139838\n",
      "Epoch 123, Loss: 0.49197607191949805\n",
      "Epoch 124, Loss: 0.4914699197831528\n",
      "Epoch 125, Loss: 0.49091483381312667\n",
      "Epoch 126, Loss: 0.49035147677946317\n",
      "Epoch 127, Loss: 0.4898087222825037\n",
      "Epoch 128, Loss: 0.48924014731550836\n",
      "Epoch 129, Loss: 0.48873550647212044\n",
      "Epoch 130, Loss: 0.48832307254971563\n",
      "Epoch 131, Loss: 0.48791418392821295\n",
      "Epoch 132, Loss: 0.4875902245243796\n",
      "Epoch 133, Loss: 0.48728851108037585\n",
      "Epoch 134, Loss: 0.48697062262181345\n",
      "Epoch 135, Loss: 0.4866339553030548\n",
      "Epoch 136, Loss: 0.48633446746734893\n",
      "Epoch 137, Loss: 0.4860310087358514\n",
      "Epoch 138, Loss: 0.4856786412545615\n",
      "Epoch 139, Loss: 0.48531372107004\n",
      "Epoch 140, Loss: 0.48495712985488587\n",
      "Epoch 141, Loss: 0.4846131600652309\n",
      "Epoch 142, Loss: 0.48425966858652364\n",
      "Epoch 143, Loss: 0.4838914149236435\n",
      "Epoch 144, Loss: 0.48352437546282767\n",
      "Epoch 145, Loss: 0.4832299911937133\n",
      "Epoch 146, Loss: 0.4832586529465723\n",
      "Epoch 147, Loss: 0.48356744143113517\n",
      "Epoch 148, Loss: 0.4828108019011922\n",
      "Epoch 149, Loss: 0.4818912939738211\n",
      "Epoch 150, Loss: 0.48239416745575797\n",
      "Epoch 151, Loss: 0.4814952491721961\n",
      "Epoch 152, Loss: 0.4811822818820502\n",
      "Epoch 153, Loss: 0.4812753156716337\n",
      "Epoch 154, Loss: 0.4804387742282827\n",
      "Epoch 155, Loss: 0.48058189052417905\n",
      "Epoch 156, Loss: 0.4801288115497346\n",
      "Epoch 157, Loss: 0.4797465937246525\n",
      "Epoch 158, Loss: 0.4797622479529733\n",
      "Epoch 159, Loss: 0.4791664679844762\n",
      "Epoch 160, Loss: 0.47904359923301915\n",
      "Epoch 161, Loss: 0.4788483533353555\n",
      "Epoch 162, Loss: 0.4783709093142749\n",
      "Epoch 163, Loss: 0.4783027642591491\n",
      "Epoch 164, Loss: 0.4780003946284736\n",
      "Epoch 165, Loss: 0.47766356610156796\n",
      "Epoch 166, Loss: 0.47756940749162413\n",
      "Epoch 167, Loss: 0.4772720526560437\n",
      "Epoch 168, Loss: 0.477001191512605\n",
      "Epoch 169, Loss: 0.47690651155802954\n",
      "Epoch 170, Loss: 0.47663605231839845\n",
      "Epoch 171, Loss: 0.4763715079887003\n",
      "Epoch 172, Loss: 0.4762479807342815\n",
      "Epoch 173, Loss: 0.47603155482909787\n",
      "Epoch 174, Loss: 0.47580085926956445\n",
      "Epoch 175, Loss: 0.4756396378860013\n",
      "Epoch 176, Loss: 0.4754627232666358\n",
      "Epoch 177, Loss: 0.4752513954316692\n",
      "Epoch 178, Loss: 0.47507140350730925\n",
      "Epoch 179, Loss: 0.47492471273973497\n",
      "Epoch 180, Loss: 0.47475493910732763\n",
      "Epoch 181, Loss: 0.47455157018218247\n",
      "Epoch 182, Loss: 0.4743853224635176\n",
      "Epoch 183, Loss: 0.47424550491599726\n",
      "Epoch 184, Loss: 0.47408413178207975\n",
      "Epoch 185, Loss: 0.47390385374022453\n",
      "Epoch 186, Loss: 0.4737383066259127\n",
      "Epoch 187, Loss: 0.47358797785338436\n",
      "Epoch 188, Loss: 0.4734560449826861\n",
      "Epoch 189, Loss: 0.47335738206045025\n",
      "Epoch 190, Loss: 0.4733162834925985\n",
      "Epoch 191, Loss: 0.4732413020737725\n",
      "Epoch 192, Loss: 0.47313239020014447\n",
      "Epoch 193, Loss: 0.4729457158004841\n",
      "Epoch 194, Loss: 0.47274703738102797\n",
      "Epoch 195, Loss: 0.47253750657270627\n",
      "Epoch 196, Loss: 0.4723544299968521\n",
      "Epoch 197, Loss: 0.47221102907746554\n",
      "Epoch 198, Loss: 0.47210844467503205\n",
      "Epoch 199, Loss: 0.47205337980155637\n",
      "Epoch 200, Loss: 0.47204973383515075\n",
      "Epoch 201, Loss: 0.4720427890593303\n",
      "Epoch 202, Loss: 0.4719159657967507\n",
      "Epoch 203, Loss: 0.471735017109987\n",
      "Epoch 204, Loss: 0.4714985533244086\n",
      "Epoch 205, Loss: 0.4713143302002759\n",
      "Epoch 206, Loss: 0.47118574544243713\n",
      "Epoch 207, Loss: 0.47112002449718005\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.22057956052100608\n",
      "Valence RMSE: 0.20770715101639303\n",
      "Arousal RMSE: 0.23274111036929782\n",
      "Test R^2 score: tensor([0.4772, 0.4636], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.3548, 0.3381], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4704003528012292\n",
      "Num of epochs: 208\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Epoch 121, Loss: 0.49295912913079615\n",
      "Epoch 122, Loss: 0.4924773619139838\n",
      "Epoch 123, Loss: 0.49197607191949805\n",
      "Epoch 124, Loss: 0.4914699197831528\n",
      "Epoch 125, Loss: 0.49091483381312667\n",
      "Epoch 126, Loss: 0.49035147677946317\n",
      "Epoch 127, Loss: 0.4898087222825037\n",
      "Epoch 128, Loss: 0.48924014731550836\n",
      "Epoch 129, Loss: 0.48873550647212044\n",
      "Epoch 130, Loss: 0.48832307254971563\n",
      "Epoch 131, Loss: 0.48791418392821295\n",
      "Epoch 132, Loss: 0.4875902245243796\n",
      "Epoch 133, Loss: 0.48728851108037585\n",
      "Epoch 134, Loss: 0.48697062262181345\n",
      "Epoch 135, Loss: 0.4866339553030548\n",
      "Epoch 136, Loss: 0.48633446746734893\n",
      "Epoch 137, Loss: 0.4860310087358514\n",
      "Epoch 138, Loss: 0.4856786412545615\n",
      "Epoch 139, Loss: 0.48531372107004\n",
      "Epoch 140, Loss: 0.48495712985488587\n",
      "Epoch 141, Loss: 0.4846131600652309\n",
      "Epoch 142, Loss: 0.48425966858652364\n",
      "Epoch 143, Loss: 0.4838914149236435\n",
      "Epoch 144, Loss: 0.48352437546282767\n",
      "Epoch 145, Loss: 0.4832299911937133\n",
      "Epoch 146, Loss: 0.4832586529465723\n",
      "Epoch 147, Loss: 0.48356744143113517\n",
      "Epoch 148, Loss: 0.4828108019011922\n",
      "Epoch 149, Loss: 0.4818912939738211\n",
      "Epoch 150, Loss: 0.48239416745575797\n",
      "Epoch 151, Loss: 0.4814952491721961\n",
      "Epoch 152, Loss: 0.4811822818820502\n",
      "Epoch 153, Loss: 0.4812753156716337\n",
      "Epoch 154, Loss: 0.4804387742282827\n",
      "Epoch 155, Loss: 0.48058189052417905\n",
      "Epoch 156, Loss: 0.4801288115497346\n",
      "Epoch 157, Loss: 0.4797465937246525\n",
      "Epoch 158, Loss: 0.4797622479529733\n",
      "Epoch 159, Loss: 0.4791664679844762\n",
      "Epoch 160, Loss: 0.47904359923301915\n",
      "Epoch 161, Loss: 0.4788483533353555\n",
      "Epoch 162, Loss: 0.4783709093142749\n",
      "Epoch 163, Loss: 0.4783027642591491\n",
      "Epoch 164, Loss: 0.4780003946284736\n",
      "Epoch 165, Loss: 0.47766356610156796\n",
      "Epoch 166, Loss: 0.47756940749162413\n",
      "Epoch 167, Loss: 0.4772720526560437\n",
      "Epoch 168, Loss: 0.477001191512605\n",
      "Epoch 169, Loss: 0.47690651155802954\n",
      "Epoch 170, Loss: 0.47663605231839845\n",
      "Epoch 171, Loss: 0.4763715079887003\n",
      "Epoch 172, Loss: 0.4762479807342815\n",
      "Epoch 173, Loss: 0.47603155482909787\n",
      "Epoch 174, Loss: 0.47580085926956445\n",
      "Epoch 175, Loss: 0.4756396378860013\n",
      "Epoch 176, Loss: 0.4754627232666358\n",
      "Epoch 177, Loss: 0.4752513954316692\n",
      "Epoch 178, Loss: 0.47507140350730925\n",
      "Epoch 179, Loss: 0.47492471273973497\n",
      "Epoch 180, Loss: 0.47475493910732763\n",
      "Epoch 181, Loss: 0.47455157018218247\n",
      "Epoch 182, Loss: 0.4743853224635176\n",
      "Epoch 183, Loss: 0.47424550491599726\n",
      "Epoch 184, Loss: 0.47408413178207975\n",
      "Epoch 185, Loss: 0.47390385374022453\n",
      "Epoch 186, Loss: 0.4737383066259127\n",
      "Epoch 187, Loss: 0.47358797785338436\n",
      "Epoch 188, Loss: 0.4734560449826861\n",
      "Epoch 189, Loss: 0.47335738206045025\n",
      "Epoch 190, Loss: 0.4733162834925985\n",
      "Epoch 191, Loss: 0.4732413020737725\n",
      "Epoch 192, Loss: 0.47313239020014447\n",
      "Epoch 193, Loss: 0.4729457158004841\n",
      "Epoch 194, Loss: 0.47274703738102797\n",
      "Epoch 195, Loss: 0.47253750657270627\n",
      "Epoch 196, Loss: 0.4723544299968521\n",
      "Epoch 197, Loss: 0.47221102907746554\n",
      "Epoch 198, Loss: 0.47210844467503205\n",
      "Epoch 199, Loss: 0.47205337980155637\n",
      "Epoch 200, Loss: 0.47204973383515075\n",
      "Epoch 201, Loss: 0.4720427890593303\n",
      "Epoch 202, Loss: 0.4719159657967507\n",
      "Epoch 203, Loss: 0.471735017109987\n",
      "Epoch 204, Loss: 0.4714985533244086\n",
      "Epoch 205, Loss: 0.4713143302002759\n",
      "Epoch 206, Loss: 0.47118574544243713\n",
      "Epoch 207, Loss: 0.47112002449718005\n",
      "Epoch 208, Loss: 0.4710910670518786\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2216063398455289\n",
      "Valence RMSE: 0.20856813286798379\n",
      "Arousal RMSE: 0.23391894679873015\n",
      "Test R^2 score: tensor([0.4728, 0.4582], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.3494, 0.3314], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.46550737025370714\n",
      "Num of epochs: 209\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Epoch 121, Loss: 0.49295912913079615\n",
      "Epoch 122, Loss: 0.4924773619139838\n",
      "Epoch 123, Loss: 0.49197607191949805\n",
      "Epoch 124, Loss: 0.4914699197831528\n",
      "Epoch 125, Loss: 0.49091483381312667\n",
      "Epoch 126, Loss: 0.49035147677946317\n",
      "Epoch 127, Loss: 0.4898087222825037\n",
      "Epoch 128, Loss: 0.48924014731550836\n",
      "Epoch 129, Loss: 0.48873550647212044\n",
      "Epoch 130, Loss: 0.48832307254971563\n",
      "Epoch 131, Loss: 0.48791418392821295\n",
      "Epoch 132, Loss: 0.4875902245243796\n",
      "Epoch 133, Loss: 0.48728851108037585\n",
      "Epoch 134, Loss: 0.48697062262181345\n",
      "Epoch 135, Loss: 0.4866339553030548\n",
      "Epoch 136, Loss: 0.48633446746734893\n",
      "Epoch 137, Loss: 0.4860310087358514\n",
      "Epoch 138, Loss: 0.4856786412545615\n",
      "Epoch 139, Loss: 0.48531372107004\n",
      "Epoch 140, Loss: 0.48495712985488587\n",
      "Epoch 141, Loss: 0.4846131600652309\n",
      "Epoch 142, Loss: 0.48425966858652364\n",
      "Epoch 143, Loss: 0.4838914149236435\n",
      "Epoch 144, Loss: 0.48352437546282767\n",
      "Epoch 145, Loss: 0.4832299911937133\n",
      "Epoch 146, Loss: 0.4832586529465723\n",
      "Epoch 147, Loss: 0.48356744143113517\n",
      "Epoch 148, Loss: 0.4828108019011922\n",
      "Epoch 149, Loss: 0.4818912939738211\n",
      "Epoch 150, Loss: 0.48239416745575797\n",
      "Epoch 151, Loss: 0.4814952491721961\n",
      "Epoch 152, Loss: 0.4811822818820502\n",
      "Epoch 153, Loss: 0.4812753156716337\n",
      "Epoch 154, Loss: 0.4804387742282827\n",
      "Epoch 155, Loss: 0.48058189052417905\n",
      "Epoch 156, Loss: 0.4801288115497346\n",
      "Epoch 157, Loss: 0.4797465937246525\n",
      "Epoch 158, Loss: 0.4797622479529733\n",
      "Epoch 159, Loss: 0.4791664679844762\n",
      "Epoch 160, Loss: 0.47904359923301915\n",
      "Epoch 161, Loss: 0.4788483533353555\n",
      "Epoch 162, Loss: 0.4783709093142749\n",
      "Epoch 163, Loss: 0.4783027642591491\n",
      "Epoch 164, Loss: 0.4780003946284736\n",
      "Epoch 165, Loss: 0.47766356610156796\n",
      "Epoch 166, Loss: 0.47756940749162413\n",
      "Epoch 167, Loss: 0.4772720526560437\n",
      "Epoch 168, Loss: 0.477001191512605\n",
      "Epoch 169, Loss: 0.47690651155802954\n",
      "Epoch 170, Loss: 0.47663605231839845\n",
      "Epoch 171, Loss: 0.4763715079887003\n",
      "Epoch 172, Loss: 0.4762479807342815\n",
      "Epoch 173, Loss: 0.47603155482909787\n",
      "Epoch 174, Loss: 0.47580085926956445\n",
      "Epoch 175, Loss: 0.4756396378860013\n",
      "Epoch 176, Loss: 0.4754627232666358\n",
      "Epoch 177, Loss: 0.4752513954316692\n",
      "Epoch 178, Loss: 0.47507140350730925\n",
      "Epoch 179, Loss: 0.47492471273973497\n",
      "Epoch 180, Loss: 0.47475493910732763\n",
      "Epoch 181, Loss: 0.47455157018218247\n",
      "Epoch 182, Loss: 0.4743853224635176\n",
      "Epoch 183, Loss: 0.47424550491599726\n",
      "Epoch 184, Loss: 0.47408413178207975\n",
      "Epoch 185, Loss: 0.47390385374022453\n",
      "Epoch 186, Loss: 0.4737383066259127\n",
      "Epoch 187, Loss: 0.47358797785338436\n",
      "Epoch 188, Loss: 0.4734560449826861\n",
      "Epoch 189, Loss: 0.47335738206045025\n",
      "Epoch 190, Loss: 0.4733162834925985\n",
      "Epoch 191, Loss: 0.4732413020737725\n",
      "Epoch 192, Loss: 0.47313239020014447\n",
      "Epoch 193, Loss: 0.4729457158004841\n",
      "Epoch 194, Loss: 0.47274703738102797\n",
      "Epoch 195, Loss: 0.47253750657270627\n",
      "Epoch 196, Loss: 0.4723544299968521\n",
      "Epoch 197, Loss: 0.47221102907746554\n",
      "Epoch 198, Loss: 0.47210844467503205\n",
      "Epoch 199, Loss: 0.47205337980155637\n",
      "Epoch 200, Loss: 0.47204973383515075\n",
      "Epoch 201, Loss: 0.4720427890593303\n",
      "Epoch 202, Loss: 0.4719159657967507\n",
      "Epoch 203, Loss: 0.471735017109987\n",
      "Epoch 204, Loss: 0.4714985533244086\n",
      "Epoch 205, Loss: 0.4713143302002759\n",
      "Epoch 206, Loss: 0.47118574544243713\n",
      "Epoch 207, Loss: 0.47112002449718005\n",
      "Epoch 208, Loss: 0.4710910670518786\n",
      "Epoch 209, Loss: 0.47102495326923743\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.22030389185292906\n",
      "Valence RMSE: 0.207450294788304\n",
      "Arousal RMSE: 0.23244781075187587\n",
      "Test R^2 score: tensor([0.4785, 0.4650], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.3564, 0.3398], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.47172200336327047\n",
      "Num of epochs: 210\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Epoch 121, Loss: 0.49295912913079615\n",
      "Epoch 122, Loss: 0.4924773619139838\n",
      "Epoch 123, Loss: 0.49197607191949805\n",
      "Epoch 124, Loss: 0.4914699197831528\n",
      "Epoch 125, Loss: 0.49091483381312667\n",
      "Epoch 126, Loss: 0.49035147677946317\n",
      "Epoch 127, Loss: 0.4898087222825037\n",
      "Epoch 128, Loss: 0.48924014731550836\n",
      "Epoch 129, Loss: 0.48873550647212044\n",
      "Epoch 130, Loss: 0.48832307254971563\n",
      "Epoch 131, Loss: 0.48791418392821295\n",
      "Epoch 132, Loss: 0.4875902245243796\n",
      "Epoch 133, Loss: 0.48728851108037585\n",
      "Epoch 134, Loss: 0.48697062262181345\n",
      "Epoch 135, Loss: 0.4866339553030548\n",
      "Epoch 136, Loss: 0.48633446746734893\n",
      "Epoch 137, Loss: 0.4860310087358514\n",
      "Epoch 138, Loss: 0.4856786412545615\n",
      "Epoch 139, Loss: 0.48531372107004\n",
      "Epoch 140, Loss: 0.48495712985488587\n",
      "Epoch 141, Loss: 0.4846131600652309\n",
      "Epoch 142, Loss: 0.48425966858652364\n",
      "Epoch 143, Loss: 0.4838914149236435\n",
      "Epoch 144, Loss: 0.48352437546282767\n",
      "Epoch 145, Loss: 0.4832299911937133\n",
      "Epoch 146, Loss: 0.4832586529465723\n",
      "Epoch 147, Loss: 0.48356744143113517\n",
      "Epoch 148, Loss: 0.4828108019011922\n",
      "Epoch 149, Loss: 0.4818912939738211\n",
      "Epoch 150, Loss: 0.48239416745575797\n",
      "Epoch 151, Loss: 0.4814952491721961\n",
      "Epoch 152, Loss: 0.4811822818820502\n",
      "Epoch 153, Loss: 0.4812753156716337\n",
      "Epoch 154, Loss: 0.4804387742282827\n",
      "Epoch 155, Loss: 0.48058189052417905\n",
      "Epoch 156, Loss: 0.4801288115497346\n",
      "Epoch 157, Loss: 0.4797465937246525\n",
      "Epoch 158, Loss: 0.4797622479529733\n",
      "Epoch 159, Loss: 0.4791664679844762\n",
      "Epoch 160, Loss: 0.47904359923301915\n",
      "Epoch 161, Loss: 0.4788483533353555\n",
      "Epoch 162, Loss: 0.4783709093142749\n",
      "Epoch 163, Loss: 0.4783027642591491\n",
      "Epoch 164, Loss: 0.4780003946284736\n",
      "Epoch 165, Loss: 0.47766356610156796\n",
      "Epoch 166, Loss: 0.47756940749162413\n",
      "Epoch 167, Loss: 0.4772720526560437\n",
      "Epoch 168, Loss: 0.477001191512605\n",
      "Epoch 169, Loss: 0.47690651155802954\n",
      "Epoch 170, Loss: 0.47663605231839845\n",
      "Epoch 171, Loss: 0.4763715079887003\n",
      "Epoch 172, Loss: 0.4762479807342815\n",
      "Epoch 173, Loss: 0.47603155482909787\n",
      "Epoch 174, Loss: 0.47580085926956445\n",
      "Epoch 175, Loss: 0.4756396378860013\n",
      "Epoch 176, Loss: 0.4754627232666358\n",
      "Epoch 177, Loss: 0.4752513954316692\n",
      "Epoch 178, Loss: 0.47507140350730925\n",
      "Epoch 179, Loss: 0.47492471273973497\n",
      "Epoch 180, Loss: 0.47475493910732763\n",
      "Epoch 181, Loss: 0.47455157018218247\n",
      "Epoch 182, Loss: 0.4743853224635176\n",
      "Epoch 183, Loss: 0.47424550491599726\n",
      "Epoch 184, Loss: 0.47408413178207975\n",
      "Epoch 185, Loss: 0.47390385374022453\n",
      "Epoch 186, Loss: 0.4737383066259127\n",
      "Epoch 187, Loss: 0.47358797785338436\n",
      "Epoch 188, Loss: 0.4734560449826861\n",
      "Epoch 189, Loss: 0.47335738206045025\n",
      "Epoch 190, Loss: 0.4733162834925985\n",
      "Epoch 191, Loss: 0.4732413020737725\n",
      "Epoch 192, Loss: 0.47313239020014447\n",
      "Epoch 193, Loss: 0.4729457158004841\n",
      "Epoch 194, Loss: 0.47274703738102797\n",
      "Epoch 195, Loss: 0.47253750657270627\n",
      "Epoch 196, Loss: 0.4723544299968521\n",
      "Epoch 197, Loss: 0.47221102907746554\n",
      "Epoch 198, Loss: 0.47210844467503205\n",
      "Epoch 199, Loss: 0.47205337980155637\n",
      "Epoch 200, Loss: 0.47204973383515075\n",
      "Epoch 201, Loss: 0.4720427890593303\n",
      "Epoch 202, Loss: 0.4719159657967507\n",
      "Epoch 203, Loss: 0.471735017109987\n",
      "Epoch 204, Loss: 0.4714985533244086\n",
      "Epoch 205, Loss: 0.4713143302002759\n",
      "Epoch 206, Loss: 0.47118574544243713\n",
      "Epoch 207, Loss: 0.47112002449718005\n",
      "Epoch 208, Loss: 0.4710910670518786\n",
      "Epoch 209, Loss: 0.47102495326923743\n",
      "Epoch 210, Loss: 0.4709626586360175\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2215338544225652\n",
      "Valence RMSE: 0.20848356711042973\n",
      "Arousal RMSE: 0.23385700664198109\n",
      "Test R^2 score: tensor([0.4732, 0.4585], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.3500, 0.3317], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.46586452461785927\n",
      "Num of epochs: 211\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Epoch 121, Loss: 0.49295912913079615\n",
      "Epoch 122, Loss: 0.4924773619139838\n",
      "Epoch 123, Loss: 0.49197607191949805\n",
      "Epoch 124, Loss: 0.4914699197831528\n",
      "Epoch 125, Loss: 0.49091483381312667\n",
      "Epoch 126, Loss: 0.49035147677946317\n",
      "Epoch 127, Loss: 0.4898087222825037\n",
      "Epoch 128, Loss: 0.48924014731550836\n",
      "Epoch 129, Loss: 0.48873550647212044\n",
      "Epoch 130, Loss: 0.48832307254971563\n",
      "Epoch 131, Loss: 0.48791418392821295\n",
      "Epoch 132, Loss: 0.4875902245243796\n",
      "Epoch 133, Loss: 0.48728851108037585\n",
      "Epoch 134, Loss: 0.48697062262181345\n",
      "Epoch 135, Loss: 0.4866339553030548\n",
      "Epoch 136, Loss: 0.48633446746734893\n",
      "Epoch 137, Loss: 0.4860310087358514\n",
      "Epoch 138, Loss: 0.4856786412545615\n",
      "Epoch 139, Loss: 0.48531372107004\n",
      "Epoch 140, Loss: 0.48495712985488587\n",
      "Epoch 141, Loss: 0.4846131600652309\n",
      "Epoch 142, Loss: 0.48425966858652364\n",
      "Epoch 143, Loss: 0.4838914149236435\n",
      "Epoch 144, Loss: 0.48352437546282767\n",
      "Epoch 145, Loss: 0.4832299911937133\n",
      "Epoch 146, Loss: 0.4832586529465723\n",
      "Epoch 147, Loss: 0.48356744143113517\n",
      "Epoch 148, Loss: 0.4828108019011922\n",
      "Epoch 149, Loss: 0.4818912939738211\n",
      "Epoch 150, Loss: 0.48239416745575797\n",
      "Epoch 151, Loss: 0.4814952491721961\n",
      "Epoch 152, Loss: 0.4811822818820502\n",
      "Epoch 153, Loss: 0.4812753156716337\n",
      "Epoch 154, Loss: 0.4804387742282827\n",
      "Epoch 155, Loss: 0.48058189052417905\n",
      "Epoch 156, Loss: 0.4801288115497346\n",
      "Epoch 157, Loss: 0.4797465937246525\n",
      "Epoch 158, Loss: 0.4797622479529733\n",
      "Epoch 159, Loss: 0.4791664679844762\n",
      "Epoch 160, Loss: 0.47904359923301915\n",
      "Epoch 161, Loss: 0.4788483533353555\n",
      "Epoch 162, Loss: 0.4783709093142749\n",
      "Epoch 163, Loss: 0.4783027642591491\n",
      "Epoch 164, Loss: 0.4780003946284736\n",
      "Epoch 165, Loss: 0.47766356610156796\n",
      "Epoch 166, Loss: 0.47756940749162413\n",
      "Epoch 167, Loss: 0.4772720526560437\n",
      "Epoch 168, Loss: 0.477001191512605\n",
      "Epoch 169, Loss: 0.47690651155802954\n",
      "Epoch 170, Loss: 0.47663605231839845\n",
      "Epoch 171, Loss: 0.4763715079887003\n",
      "Epoch 172, Loss: 0.4762479807342815\n",
      "Epoch 173, Loss: 0.47603155482909787\n",
      "Epoch 174, Loss: 0.47580085926956445\n",
      "Epoch 175, Loss: 0.4756396378860013\n",
      "Epoch 176, Loss: 0.4754627232666358\n",
      "Epoch 177, Loss: 0.4752513954316692\n",
      "Epoch 178, Loss: 0.47507140350730925\n",
      "Epoch 179, Loss: 0.47492471273973497\n",
      "Epoch 180, Loss: 0.47475493910732763\n",
      "Epoch 181, Loss: 0.47455157018218247\n",
      "Epoch 182, Loss: 0.4743853224635176\n",
      "Epoch 183, Loss: 0.47424550491599726\n",
      "Epoch 184, Loss: 0.47408413178207975\n",
      "Epoch 185, Loss: 0.47390385374022453\n",
      "Epoch 186, Loss: 0.4737383066259127\n",
      "Epoch 187, Loss: 0.47358797785338436\n",
      "Epoch 188, Loss: 0.4734560449826861\n",
      "Epoch 189, Loss: 0.47335738206045025\n",
      "Epoch 190, Loss: 0.4733162834925985\n",
      "Epoch 191, Loss: 0.4732413020737725\n",
      "Epoch 192, Loss: 0.47313239020014447\n",
      "Epoch 193, Loss: 0.4729457158004841\n",
      "Epoch 194, Loss: 0.47274703738102797\n",
      "Epoch 195, Loss: 0.47253750657270627\n",
      "Epoch 196, Loss: 0.4723544299968521\n",
      "Epoch 197, Loss: 0.47221102907746554\n",
      "Epoch 198, Loss: 0.47210844467503205\n",
      "Epoch 199, Loss: 0.47205337980155637\n",
      "Epoch 200, Loss: 0.47204973383515075\n",
      "Epoch 201, Loss: 0.4720427890593303\n",
      "Epoch 202, Loss: 0.4719159657967507\n",
      "Epoch 203, Loss: 0.471735017109987\n",
      "Epoch 204, Loss: 0.4714985533244086\n",
      "Epoch 205, Loss: 0.4713143302002759\n",
      "Epoch 206, Loss: 0.47118574544243713\n",
      "Epoch 207, Loss: 0.47112002449718005\n",
      "Epoch 208, Loss: 0.4710910670518786\n",
      "Epoch 209, Loss: 0.47102495326923743\n",
      "Epoch 210, Loss: 0.4709626586360175\n",
      "Epoch 211, Loss: 0.47085840979895394\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.22011355363627974\n",
      "Valence RMSE: 0.20728335266828649\n",
      "Arousal RMSE: 0.23223601076357114\n",
      "Test R^2 score: tensor([0.4793, 0.4660], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.3574, 0.3410], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.47262880369998245\n",
      "Num of epochs: 212\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Epoch 121, Loss: 0.49295912913079615\n",
      "Epoch 122, Loss: 0.4924773619139838\n",
      "Epoch 123, Loss: 0.49197607191949805\n",
      "Epoch 124, Loss: 0.4914699197831528\n",
      "Epoch 125, Loss: 0.49091483381312667\n",
      "Epoch 126, Loss: 0.49035147677946317\n",
      "Epoch 127, Loss: 0.4898087222825037\n",
      "Epoch 128, Loss: 0.48924014731550836\n",
      "Epoch 129, Loss: 0.48873550647212044\n",
      "Epoch 130, Loss: 0.48832307254971563\n",
      "Epoch 131, Loss: 0.48791418392821295\n",
      "Epoch 132, Loss: 0.4875902245243796\n",
      "Epoch 133, Loss: 0.48728851108037585\n",
      "Epoch 134, Loss: 0.48697062262181345\n",
      "Epoch 135, Loss: 0.4866339553030548\n",
      "Epoch 136, Loss: 0.48633446746734893\n",
      "Epoch 137, Loss: 0.4860310087358514\n",
      "Epoch 138, Loss: 0.4856786412545615\n",
      "Epoch 139, Loss: 0.48531372107004\n",
      "Epoch 140, Loss: 0.48495712985488587\n",
      "Epoch 141, Loss: 0.4846131600652309\n",
      "Epoch 142, Loss: 0.48425966858652364\n",
      "Epoch 143, Loss: 0.4838914149236435\n",
      "Epoch 144, Loss: 0.48352437546282767\n",
      "Epoch 145, Loss: 0.4832299911937133\n",
      "Epoch 146, Loss: 0.4832586529465723\n",
      "Epoch 147, Loss: 0.48356744143113517\n",
      "Epoch 148, Loss: 0.4828108019011922\n",
      "Epoch 149, Loss: 0.4818912939738211\n",
      "Epoch 150, Loss: 0.48239416745575797\n",
      "Epoch 151, Loss: 0.4814952491721961\n",
      "Epoch 152, Loss: 0.4811822818820502\n",
      "Epoch 153, Loss: 0.4812753156716337\n",
      "Epoch 154, Loss: 0.4804387742282827\n",
      "Epoch 155, Loss: 0.48058189052417905\n",
      "Epoch 156, Loss: 0.4801288115497346\n",
      "Epoch 157, Loss: 0.4797465937246525\n",
      "Epoch 158, Loss: 0.4797622479529733\n",
      "Epoch 159, Loss: 0.4791664679844762\n",
      "Epoch 160, Loss: 0.47904359923301915\n",
      "Epoch 161, Loss: 0.4788483533353555\n",
      "Epoch 162, Loss: 0.4783709093142749\n",
      "Epoch 163, Loss: 0.4783027642591491\n",
      "Epoch 164, Loss: 0.4780003946284736\n",
      "Epoch 165, Loss: 0.47766356610156796\n",
      "Epoch 166, Loss: 0.47756940749162413\n",
      "Epoch 167, Loss: 0.4772720526560437\n",
      "Epoch 168, Loss: 0.477001191512605\n",
      "Epoch 169, Loss: 0.47690651155802954\n",
      "Epoch 170, Loss: 0.47663605231839845\n",
      "Epoch 171, Loss: 0.4763715079887003\n",
      "Epoch 172, Loss: 0.4762479807342815\n",
      "Epoch 173, Loss: 0.47603155482909787\n",
      "Epoch 174, Loss: 0.47580085926956445\n",
      "Epoch 175, Loss: 0.4756396378860013\n",
      "Epoch 176, Loss: 0.4754627232666358\n",
      "Epoch 177, Loss: 0.4752513954316692\n",
      "Epoch 178, Loss: 0.47507140350730925\n",
      "Epoch 179, Loss: 0.47492471273973497\n",
      "Epoch 180, Loss: 0.47475493910732763\n",
      "Epoch 181, Loss: 0.47455157018218247\n",
      "Epoch 182, Loss: 0.4743853224635176\n",
      "Epoch 183, Loss: 0.47424550491599726\n",
      "Epoch 184, Loss: 0.47408413178207975\n",
      "Epoch 185, Loss: 0.47390385374022453\n",
      "Epoch 186, Loss: 0.4737383066259127\n",
      "Epoch 187, Loss: 0.47358797785338436\n",
      "Epoch 188, Loss: 0.4734560449826861\n",
      "Epoch 189, Loss: 0.47335738206045025\n",
      "Epoch 190, Loss: 0.4733162834925985\n",
      "Epoch 191, Loss: 0.4732413020737725\n",
      "Epoch 192, Loss: 0.47313239020014447\n",
      "Epoch 193, Loss: 0.4729457158004841\n",
      "Epoch 194, Loss: 0.47274703738102797\n",
      "Epoch 195, Loss: 0.47253750657270627\n",
      "Epoch 196, Loss: 0.4723544299968521\n",
      "Epoch 197, Loss: 0.47221102907746554\n",
      "Epoch 198, Loss: 0.47210844467503205\n",
      "Epoch 199, Loss: 0.47205337980155637\n",
      "Epoch 200, Loss: 0.47204973383515075\n",
      "Epoch 201, Loss: 0.4720427890593303\n",
      "Epoch 202, Loss: 0.4719159657967507\n",
      "Epoch 203, Loss: 0.471735017109987\n",
      "Epoch 204, Loss: 0.4714985533244086\n",
      "Epoch 205, Loss: 0.4713143302002759\n",
      "Epoch 206, Loss: 0.47118574544243713\n",
      "Epoch 207, Loss: 0.47112002449718005\n",
      "Epoch 208, Loss: 0.4710910670518786\n",
      "Epoch 209, Loss: 0.47102495326923743\n",
      "Epoch 210, Loss: 0.4709626586360175\n",
      "Epoch 211, Loss: 0.47085840979895394\n",
      "Epoch 212, Loss: 0.47078064719388485\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2213516868436922\n",
      "Valence RMSE: 0.20832401967491246\n",
      "Arousal RMSE: 0.23365410624164648\n",
      "Test R^2 score: tensor([0.4740, 0.4594], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.3510, 0.3329], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.46673711249508165\n",
      "Num of epochs: 213\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Epoch 121, Loss: 0.49295912913079615\n",
      "Epoch 122, Loss: 0.4924773619139838\n",
      "Epoch 123, Loss: 0.49197607191949805\n",
      "Epoch 124, Loss: 0.4914699197831528\n",
      "Epoch 125, Loss: 0.49091483381312667\n",
      "Epoch 126, Loss: 0.49035147677946317\n",
      "Epoch 127, Loss: 0.4898087222825037\n",
      "Epoch 128, Loss: 0.48924014731550836\n",
      "Epoch 129, Loss: 0.48873550647212044\n",
      "Epoch 130, Loss: 0.48832307254971563\n",
      "Epoch 131, Loss: 0.48791418392821295\n",
      "Epoch 132, Loss: 0.4875902245243796\n",
      "Epoch 133, Loss: 0.48728851108037585\n",
      "Epoch 134, Loss: 0.48697062262181345\n",
      "Epoch 135, Loss: 0.4866339553030548\n",
      "Epoch 136, Loss: 0.48633446746734893\n",
      "Epoch 137, Loss: 0.4860310087358514\n",
      "Epoch 138, Loss: 0.4856786412545615\n",
      "Epoch 139, Loss: 0.48531372107004\n",
      "Epoch 140, Loss: 0.48495712985488587\n",
      "Epoch 141, Loss: 0.4846131600652309\n",
      "Epoch 142, Loss: 0.48425966858652364\n",
      "Epoch 143, Loss: 0.4838914149236435\n",
      "Epoch 144, Loss: 0.48352437546282767\n",
      "Epoch 145, Loss: 0.4832299911937133\n",
      "Epoch 146, Loss: 0.4832586529465723\n",
      "Epoch 147, Loss: 0.48356744143113517\n",
      "Epoch 148, Loss: 0.4828108019011922\n",
      "Epoch 149, Loss: 0.4818912939738211\n",
      "Epoch 150, Loss: 0.48239416745575797\n",
      "Epoch 151, Loss: 0.4814952491721961\n",
      "Epoch 152, Loss: 0.4811822818820502\n",
      "Epoch 153, Loss: 0.4812753156716337\n",
      "Epoch 154, Loss: 0.4804387742282827\n",
      "Epoch 155, Loss: 0.48058189052417905\n",
      "Epoch 156, Loss: 0.4801288115497346\n",
      "Epoch 157, Loss: 0.4797465937246525\n",
      "Epoch 158, Loss: 0.4797622479529733\n",
      "Epoch 159, Loss: 0.4791664679844762\n",
      "Epoch 160, Loss: 0.47904359923301915\n",
      "Epoch 161, Loss: 0.4788483533353555\n",
      "Epoch 162, Loss: 0.4783709093142749\n",
      "Epoch 163, Loss: 0.4783027642591491\n",
      "Epoch 164, Loss: 0.4780003946284736\n",
      "Epoch 165, Loss: 0.47766356610156796\n",
      "Epoch 166, Loss: 0.47756940749162413\n",
      "Epoch 167, Loss: 0.4772720526560437\n",
      "Epoch 168, Loss: 0.477001191512605\n",
      "Epoch 169, Loss: 0.47690651155802954\n",
      "Epoch 170, Loss: 0.47663605231839845\n",
      "Epoch 171, Loss: 0.4763715079887003\n",
      "Epoch 172, Loss: 0.4762479807342815\n",
      "Epoch 173, Loss: 0.47603155482909787\n",
      "Epoch 174, Loss: 0.47580085926956445\n",
      "Epoch 175, Loss: 0.4756396378860013\n",
      "Epoch 176, Loss: 0.4754627232666358\n",
      "Epoch 177, Loss: 0.4752513954316692\n",
      "Epoch 178, Loss: 0.47507140350730925\n",
      "Epoch 179, Loss: 0.47492471273973497\n",
      "Epoch 180, Loss: 0.47475493910732763\n",
      "Epoch 181, Loss: 0.47455157018218247\n",
      "Epoch 182, Loss: 0.4743853224635176\n",
      "Epoch 183, Loss: 0.47424550491599726\n",
      "Epoch 184, Loss: 0.47408413178207975\n",
      "Epoch 185, Loss: 0.47390385374022453\n",
      "Epoch 186, Loss: 0.4737383066259127\n",
      "Epoch 187, Loss: 0.47358797785338436\n",
      "Epoch 188, Loss: 0.4734560449826861\n",
      "Epoch 189, Loss: 0.47335738206045025\n",
      "Epoch 190, Loss: 0.4733162834925985\n",
      "Epoch 191, Loss: 0.4732413020737725\n",
      "Epoch 192, Loss: 0.47313239020014447\n",
      "Epoch 193, Loss: 0.4729457158004841\n",
      "Epoch 194, Loss: 0.47274703738102797\n",
      "Epoch 195, Loss: 0.47253750657270627\n",
      "Epoch 196, Loss: 0.4723544299968521\n",
      "Epoch 197, Loss: 0.47221102907746554\n",
      "Epoch 198, Loss: 0.47210844467503205\n",
      "Epoch 199, Loss: 0.47205337980155637\n",
      "Epoch 200, Loss: 0.47204973383515075\n",
      "Epoch 201, Loss: 0.4720427890593303\n",
      "Epoch 202, Loss: 0.4719159657967507\n",
      "Epoch 203, Loss: 0.471735017109987\n",
      "Epoch 204, Loss: 0.4714985533244086\n",
      "Epoch 205, Loss: 0.4713143302002759\n",
      "Epoch 206, Loss: 0.47118574544243713\n",
      "Epoch 207, Loss: 0.47112002449718005\n",
      "Epoch 208, Loss: 0.4710910670518786\n",
      "Epoch 209, Loss: 0.47102495326923743\n",
      "Epoch 210, Loss: 0.4709626586360175\n",
      "Epoch 211, Loss: 0.47085840979895394\n",
      "Epoch 212, Loss: 0.47078064719388485\n",
      "Epoch 213, Loss: 0.47066614789276784\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.21998976929142205\n",
      "Valence RMSE: 0.20716115804328028\n",
      "Arousal RMSE: 0.23211043014899885\n",
      "Test R^2 score: tensor([0.4799, 0.4665], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.3582, 0.3417], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4732243720977915\n",
      "Num of epochs: 214\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Epoch 121, Loss: 0.49295912913079615\n",
      "Epoch 122, Loss: 0.4924773619139838\n",
      "Epoch 123, Loss: 0.49197607191949805\n",
      "Epoch 124, Loss: 0.4914699197831528\n",
      "Epoch 125, Loss: 0.49091483381312667\n",
      "Epoch 126, Loss: 0.49035147677946317\n",
      "Epoch 127, Loss: 0.4898087222825037\n",
      "Epoch 128, Loss: 0.48924014731550836\n",
      "Epoch 129, Loss: 0.48873550647212044\n",
      "Epoch 130, Loss: 0.48832307254971563\n",
      "Epoch 131, Loss: 0.48791418392821295\n",
      "Epoch 132, Loss: 0.4875902245243796\n",
      "Epoch 133, Loss: 0.48728851108037585\n",
      "Epoch 134, Loss: 0.48697062262181345\n",
      "Epoch 135, Loss: 0.4866339553030548\n",
      "Epoch 136, Loss: 0.48633446746734893\n",
      "Epoch 137, Loss: 0.4860310087358514\n",
      "Epoch 138, Loss: 0.4856786412545615\n",
      "Epoch 139, Loss: 0.48531372107004\n",
      "Epoch 140, Loss: 0.48495712985488587\n",
      "Epoch 141, Loss: 0.4846131600652309\n",
      "Epoch 142, Loss: 0.48425966858652364\n",
      "Epoch 143, Loss: 0.4838914149236435\n",
      "Epoch 144, Loss: 0.48352437546282767\n",
      "Epoch 145, Loss: 0.4832299911937133\n",
      "Epoch 146, Loss: 0.4832586529465723\n",
      "Epoch 147, Loss: 0.48356744143113517\n",
      "Epoch 148, Loss: 0.4828108019011922\n",
      "Epoch 149, Loss: 0.4818912939738211\n",
      "Epoch 150, Loss: 0.48239416745575797\n",
      "Epoch 151, Loss: 0.4814952491721961\n",
      "Epoch 152, Loss: 0.4811822818820502\n",
      "Epoch 153, Loss: 0.4812753156716337\n",
      "Epoch 154, Loss: 0.4804387742282827\n",
      "Epoch 155, Loss: 0.48058189052417905\n",
      "Epoch 156, Loss: 0.4801288115497346\n",
      "Epoch 157, Loss: 0.4797465937246525\n",
      "Epoch 158, Loss: 0.4797622479529733\n",
      "Epoch 159, Loss: 0.4791664679844762\n",
      "Epoch 160, Loss: 0.47904359923301915\n",
      "Epoch 161, Loss: 0.4788483533353555\n",
      "Epoch 162, Loss: 0.4783709093142749\n",
      "Epoch 163, Loss: 0.4783027642591491\n",
      "Epoch 164, Loss: 0.4780003946284736\n",
      "Epoch 165, Loss: 0.47766356610156796\n",
      "Epoch 166, Loss: 0.47756940749162413\n",
      "Epoch 167, Loss: 0.4772720526560437\n",
      "Epoch 168, Loss: 0.477001191512605\n",
      "Epoch 169, Loss: 0.47690651155802954\n",
      "Epoch 170, Loss: 0.47663605231839845\n",
      "Epoch 171, Loss: 0.4763715079887003\n",
      "Epoch 172, Loss: 0.4762479807342815\n",
      "Epoch 173, Loss: 0.47603155482909787\n",
      "Epoch 174, Loss: 0.47580085926956445\n",
      "Epoch 175, Loss: 0.4756396378860013\n",
      "Epoch 176, Loss: 0.4754627232666358\n",
      "Epoch 177, Loss: 0.4752513954316692\n",
      "Epoch 178, Loss: 0.47507140350730925\n",
      "Epoch 179, Loss: 0.47492471273973497\n",
      "Epoch 180, Loss: 0.47475493910732763\n",
      "Epoch 181, Loss: 0.47455157018218247\n",
      "Epoch 182, Loss: 0.4743853224635176\n",
      "Epoch 183, Loss: 0.47424550491599726\n",
      "Epoch 184, Loss: 0.47408413178207975\n",
      "Epoch 185, Loss: 0.47390385374022453\n",
      "Epoch 186, Loss: 0.4737383066259127\n",
      "Epoch 187, Loss: 0.47358797785338436\n",
      "Epoch 188, Loss: 0.4734560449826861\n",
      "Epoch 189, Loss: 0.47335738206045025\n",
      "Epoch 190, Loss: 0.4733162834925985\n",
      "Epoch 191, Loss: 0.4732413020737725\n",
      "Epoch 192, Loss: 0.47313239020014447\n",
      "Epoch 193, Loss: 0.4729457158004841\n",
      "Epoch 194, Loss: 0.47274703738102797\n",
      "Epoch 195, Loss: 0.47253750657270627\n",
      "Epoch 196, Loss: 0.4723544299968521\n",
      "Epoch 197, Loss: 0.47221102907746554\n",
      "Epoch 198, Loss: 0.47210844467503205\n",
      "Epoch 199, Loss: 0.47205337980155637\n",
      "Epoch 200, Loss: 0.47204973383515075\n",
      "Epoch 201, Loss: 0.4720427890593303\n",
      "Epoch 202, Loss: 0.4719159657967507\n",
      "Epoch 203, Loss: 0.471735017109987\n",
      "Epoch 204, Loss: 0.4714985533244086\n",
      "Epoch 205, Loss: 0.4713143302002759\n",
      "Epoch 206, Loss: 0.47118574544243713\n",
      "Epoch 207, Loss: 0.47112002449718005\n",
      "Epoch 208, Loss: 0.4710910670518786\n",
      "Epoch 209, Loss: 0.47102495326923743\n",
      "Epoch 210, Loss: 0.4709626586360175\n",
      "Epoch 211, Loss: 0.47085840979895394\n",
      "Epoch 212, Loss: 0.47078064719388485\n",
      "Epoch 213, Loss: 0.47066614789276784\n",
      "Epoch 214, Loss: 0.47055654499036753\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.22103686211506\n",
      "Valence RMSE: 0.20803166907933962\n",
      "Arousal RMSE: 0.2333182665103789\n",
      "Test R^2 score: tensor([0.4755, 0.4610], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.3528, 0.3348], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.46825111543317144\n",
      "Num of epochs: 215\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Epoch 121, Loss: 0.49295912913079615\n",
      "Epoch 122, Loss: 0.4924773619139838\n",
      "Epoch 123, Loss: 0.49197607191949805\n",
      "Epoch 124, Loss: 0.4914699197831528\n",
      "Epoch 125, Loss: 0.49091483381312667\n",
      "Epoch 126, Loss: 0.49035147677946317\n",
      "Epoch 127, Loss: 0.4898087222825037\n",
      "Epoch 128, Loss: 0.48924014731550836\n",
      "Epoch 129, Loss: 0.48873550647212044\n",
      "Epoch 130, Loss: 0.48832307254971563\n",
      "Epoch 131, Loss: 0.48791418392821295\n",
      "Epoch 132, Loss: 0.4875902245243796\n",
      "Epoch 133, Loss: 0.48728851108037585\n",
      "Epoch 134, Loss: 0.48697062262181345\n",
      "Epoch 135, Loss: 0.4866339553030548\n",
      "Epoch 136, Loss: 0.48633446746734893\n",
      "Epoch 137, Loss: 0.4860310087358514\n",
      "Epoch 138, Loss: 0.4856786412545615\n",
      "Epoch 139, Loss: 0.48531372107004\n",
      "Epoch 140, Loss: 0.48495712985488587\n",
      "Epoch 141, Loss: 0.4846131600652309\n",
      "Epoch 142, Loss: 0.48425966858652364\n",
      "Epoch 143, Loss: 0.4838914149236435\n",
      "Epoch 144, Loss: 0.48352437546282767\n",
      "Epoch 145, Loss: 0.4832299911937133\n",
      "Epoch 146, Loss: 0.4832586529465723\n",
      "Epoch 147, Loss: 0.48356744143113517\n",
      "Epoch 148, Loss: 0.4828108019011922\n",
      "Epoch 149, Loss: 0.4818912939738211\n",
      "Epoch 150, Loss: 0.48239416745575797\n",
      "Epoch 151, Loss: 0.4814952491721961\n",
      "Epoch 152, Loss: 0.4811822818820502\n",
      "Epoch 153, Loss: 0.4812753156716337\n",
      "Epoch 154, Loss: 0.4804387742282827\n",
      "Epoch 155, Loss: 0.48058189052417905\n",
      "Epoch 156, Loss: 0.4801288115497346\n",
      "Epoch 157, Loss: 0.4797465937246525\n",
      "Epoch 158, Loss: 0.4797622479529733\n",
      "Epoch 159, Loss: 0.4791664679844762\n",
      "Epoch 160, Loss: 0.47904359923301915\n",
      "Epoch 161, Loss: 0.4788483533353555\n",
      "Epoch 162, Loss: 0.4783709093142749\n",
      "Epoch 163, Loss: 0.4783027642591491\n",
      "Epoch 164, Loss: 0.4780003946284736\n",
      "Epoch 165, Loss: 0.47766356610156796\n",
      "Epoch 166, Loss: 0.47756940749162413\n",
      "Epoch 167, Loss: 0.4772720526560437\n",
      "Epoch 168, Loss: 0.477001191512605\n",
      "Epoch 169, Loss: 0.47690651155802954\n",
      "Epoch 170, Loss: 0.47663605231839845\n",
      "Epoch 171, Loss: 0.4763715079887003\n",
      "Epoch 172, Loss: 0.4762479807342815\n",
      "Epoch 173, Loss: 0.47603155482909787\n",
      "Epoch 174, Loss: 0.47580085926956445\n",
      "Epoch 175, Loss: 0.4756396378860013\n",
      "Epoch 176, Loss: 0.4754627232666358\n",
      "Epoch 177, Loss: 0.4752513954316692\n",
      "Epoch 178, Loss: 0.47507140350730925\n",
      "Epoch 179, Loss: 0.47492471273973497\n",
      "Epoch 180, Loss: 0.47475493910732763\n",
      "Epoch 181, Loss: 0.47455157018218247\n",
      "Epoch 182, Loss: 0.4743853224635176\n",
      "Epoch 183, Loss: 0.47424550491599726\n",
      "Epoch 184, Loss: 0.47408413178207975\n",
      "Epoch 185, Loss: 0.47390385374022453\n",
      "Epoch 186, Loss: 0.4737383066259127\n",
      "Epoch 187, Loss: 0.47358797785338436\n",
      "Epoch 188, Loss: 0.4734560449826861\n",
      "Epoch 189, Loss: 0.47335738206045025\n",
      "Epoch 190, Loss: 0.4733162834925985\n",
      "Epoch 191, Loss: 0.4732413020737725\n",
      "Epoch 192, Loss: 0.47313239020014447\n",
      "Epoch 193, Loss: 0.4729457158004841\n",
      "Epoch 194, Loss: 0.47274703738102797\n",
      "Epoch 195, Loss: 0.47253750657270627\n",
      "Epoch 196, Loss: 0.4723544299968521\n",
      "Epoch 197, Loss: 0.47221102907746554\n",
      "Epoch 198, Loss: 0.47210844467503205\n",
      "Epoch 199, Loss: 0.47205337980155637\n",
      "Epoch 200, Loss: 0.47204973383515075\n",
      "Epoch 201, Loss: 0.4720427890593303\n",
      "Epoch 202, Loss: 0.4719159657967507\n",
      "Epoch 203, Loss: 0.471735017109987\n",
      "Epoch 204, Loss: 0.4714985533244086\n",
      "Epoch 205, Loss: 0.4713143302002759\n",
      "Epoch 206, Loss: 0.47118574544243713\n",
      "Epoch 207, Loss: 0.47112002449718005\n",
      "Epoch 208, Loss: 0.4710910670518786\n",
      "Epoch 209, Loss: 0.47102495326923743\n",
      "Epoch 210, Loss: 0.4709626586360175\n",
      "Epoch 211, Loss: 0.47085840979895394\n",
      "Epoch 212, Loss: 0.47078064719388485\n",
      "Epoch 213, Loss: 0.47066614789276784\n",
      "Epoch 214, Loss: 0.47055654499036753\n",
      "Epoch 215, Loss: 0.47041652390832933\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.22004692301937456\n",
      "Valence RMSE: 0.20716127332332246\n",
      "Arousal RMSE: 0.23221865449538887\n",
      "Test R^2 score: tensor([0.4799, 0.4660], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.3582, 0.3411], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.47297529449071596\n",
      "Num of epochs: 216\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Epoch 121, Loss: 0.49295912913079615\n",
      "Epoch 122, Loss: 0.4924773619139838\n",
      "Epoch 123, Loss: 0.49197607191949805\n",
      "Epoch 124, Loss: 0.4914699197831528\n",
      "Epoch 125, Loss: 0.49091483381312667\n",
      "Epoch 126, Loss: 0.49035147677946317\n",
      "Epoch 127, Loss: 0.4898087222825037\n",
      "Epoch 128, Loss: 0.48924014731550836\n",
      "Epoch 129, Loss: 0.48873550647212044\n",
      "Epoch 130, Loss: 0.48832307254971563\n",
      "Epoch 131, Loss: 0.48791418392821295\n",
      "Epoch 132, Loss: 0.4875902245243796\n",
      "Epoch 133, Loss: 0.48728851108037585\n",
      "Epoch 134, Loss: 0.48697062262181345\n",
      "Epoch 135, Loss: 0.4866339553030548\n",
      "Epoch 136, Loss: 0.48633446746734893\n",
      "Epoch 137, Loss: 0.4860310087358514\n",
      "Epoch 138, Loss: 0.4856786412545615\n",
      "Epoch 139, Loss: 0.48531372107004\n",
      "Epoch 140, Loss: 0.48495712985488587\n",
      "Epoch 141, Loss: 0.4846131600652309\n",
      "Epoch 142, Loss: 0.48425966858652364\n",
      "Epoch 143, Loss: 0.4838914149236435\n",
      "Epoch 144, Loss: 0.48352437546282767\n",
      "Epoch 145, Loss: 0.4832299911937133\n",
      "Epoch 146, Loss: 0.4832586529465723\n",
      "Epoch 147, Loss: 0.48356744143113517\n",
      "Epoch 148, Loss: 0.4828108019011922\n",
      "Epoch 149, Loss: 0.4818912939738211\n",
      "Epoch 150, Loss: 0.48239416745575797\n",
      "Epoch 151, Loss: 0.4814952491721961\n",
      "Epoch 152, Loss: 0.4811822818820502\n",
      "Epoch 153, Loss: 0.4812753156716337\n",
      "Epoch 154, Loss: 0.4804387742282827\n",
      "Epoch 155, Loss: 0.48058189052417905\n",
      "Epoch 156, Loss: 0.4801288115497346\n",
      "Epoch 157, Loss: 0.4797465937246525\n",
      "Epoch 158, Loss: 0.4797622479529733\n",
      "Epoch 159, Loss: 0.4791664679844762\n",
      "Epoch 160, Loss: 0.47904359923301915\n",
      "Epoch 161, Loss: 0.4788483533353555\n",
      "Epoch 162, Loss: 0.4783709093142749\n",
      "Epoch 163, Loss: 0.4783027642591491\n",
      "Epoch 164, Loss: 0.4780003946284736\n",
      "Epoch 165, Loss: 0.47766356610156796\n",
      "Epoch 166, Loss: 0.47756940749162413\n",
      "Epoch 167, Loss: 0.4772720526560437\n",
      "Epoch 168, Loss: 0.477001191512605\n",
      "Epoch 169, Loss: 0.47690651155802954\n",
      "Epoch 170, Loss: 0.47663605231839845\n",
      "Epoch 171, Loss: 0.4763715079887003\n",
      "Epoch 172, Loss: 0.4762479807342815\n",
      "Epoch 173, Loss: 0.47603155482909787\n",
      "Epoch 174, Loss: 0.47580085926956445\n",
      "Epoch 175, Loss: 0.4756396378860013\n",
      "Epoch 176, Loss: 0.4754627232666358\n",
      "Epoch 177, Loss: 0.4752513954316692\n",
      "Epoch 178, Loss: 0.47507140350730925\n",
      "Epoch 179, Loss: 0.47492471273973497\n",
      "Epoch 180, Loss: 0.47475493910732763\n",
      "Epoch 181, Loss: 0.47455157018218247\n",
      "Epoch 182, Loss: 0.4743853224635176\n",
      "Epoch 183, Loss: 0.47424550491599726\n",
      "Epoch 184, Loss: 0.47408413178207975\n",
      "Epoch 185, Loss: 0.47390385374022453\n",
      "Epoch 186, Loss: 0.4737383066259127\n",
      "Epoch 187, Loss: 0.47358797785338436\n",
      "Epoch 188, Loss: 0.4734560449826861\n",
      "Epoch 189, Loss: 0.47335738206045025\n",
      "Epoch 190, Loss: 0.4733162834925985\n",
      "Epoch 191, Loss: 0.4732413020737725\n",
      "Epoch 192, Loss: 0.47313239020014447\n",
      "Epoch 193, Loss: 0.4729457158004841\n",
      "Epoch 194, Loss: 0.47274703738102797\n",
      "Epoch 195, Loss: 0.47253750657270627\n",
      "Epoch 196, Loss: 0.4723544299968521\n",
      "Epoch 197, Loss: 0.47221102907746554\n",
      "Epoch 198, Loss: 0.47210844467503205\n",
      "Epoch 199, Loss: 0.47205337980155637\n",
      "Epoch 200, Loss: 0.47204973383515075\n",
      "Epoch 201, Loss: 0.4720427890593303\n",
      "Epoch 202, Loss: 0.4719159657967507\n",
      "Epoch 203, Loss: 0.471735017109987\n",
      "Epoch 204, Loss: 0.4714985533244086\n",
      "Epoch 205, Loss: 0.4713143302002759\n",
      "Epoch 206, Loss: 0.47118574544243713\n",
      "Epoch 207, Loss: 0.47112002449718005\n",
      "Epoch 208, Loss: 0.4710910670518786\n",
      "Epoch 209, Loss: 0.47102495326923743\n",
      "Epoch 210, Loss: 0.4709626586360175\n",
      "Epoch 211, Loss: 0.47085840979895394\n",
      "Epoch 212, Loss: 0.47078064719388485\n",
      "Epoch 213, Loss: 0.47066614789276784\n",
      "Epoch 214, Loss: 0.47055654499036753\n",
      "Epoch 215, Loss: 0.47041652390832933\n",
      "Epoch 216, Loss: 0.4702446473560784\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.22038623450535552\n",
      "Valence RMSE: 0.20743747333072476\n",
      "Arousal RMSE: 0.23261530340255773\n",
      "Test R^2 score: tensor([0.4785, 0.4642], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.3565, 0.3388], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.47136859268543274\n",
      "Num of epochs: 217\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Epoch 121, Loss: 0.49295912913079615\n",
      "Epoch 122, Loss: 0.4924773619139838\n",
      "Epoch 123, Loss: 0.49197607191949805\n",
      "Epoch 124, Loss: 0.4914699197831528\n",
      "Epoch 125, Loss: 0.49091483381312667\n",
      "Epoch 126, Loss: 0.49035147677946317\n",
      "Epoch 127, Loss: 0.4898087222825037\n",
      "Epoch 128, Loss: 0.48924014731550836\n",
      "Epoch 129, Loss: 0.48873550647212044\n",
      "Epoch 130, Loss: 0.48832307254971563\n",
      "Epoch 131, Loss: 0.48791418392821295\n",
      "Epoch 132, Loss: 0.4875902245243796\n",
      "Epoch 133, Loss: 0.48728851108037585\n",
      "Epoch 134, Loss: 0.48697062262181345\n",
      "Epoch 135, Loss: 0.4866339553030548\n",
      "Epoch 136, Loss: 0.48633446746734893\n",
      "Epoch 137, Loss: 0.4860310087358514\n",
      "Epoch 138, Loss: 0.4856786412545615\n",
      "Epoch 139, Loss: 0.48531372107004\n",
      "Epoch 140, Loss: 0.48495712985488587\n",
      "Epoch 141, Loss: 0.4846131600652309\n",
      "Epoch 142, Loss: 0.48425966858652364\n",
      "Epoch 143, Loss: 0.4838914149236435\n",
      "Epoch 144, Loss: 0.48352437546282767\n",
      "Epoch 145, Loss: 0.4832299911937133\n",
      "Epoch 146, Loss: 0.4832586529465723\n",
      "Epoch 147, Loss: 0.48356744143113517\n",
      "Epoch 148, Loss: 0.4828108019011922\n",
      "Epoch 149, Loss: 0.4818912939738211\n",
      "Epoch 150, Loss: 0.48239416745575797\n",
      "Epoch 151, Loss: 0.4814952491721961\n",
      "Epoch 152, Loss: 0.4811822818820502\n",
      "Epoch 153, Loss: 0.4812753156716337\n",
      "Epoch 154, Loss: 0.4804387742282827\n",
      "Epoch 155, Loss: 0.48058189052417905\n",
      "Epoch 156, Loss: 0.4801288115497346\n",
      "Epoch 157, Loss: 0.4797465937246525\n",
      "Epoch 158, Loss: 0.4797622479529733\n",
      "Epoch 159, Loss: 0.4791664679844762\n",
      "Epoch 160, Loss: 0.47904359923301915\n",
      "Epoch 161, Loss: 0.4788483533353555\n",
      "Epoch 162, Loss: 0.4783709093142749\n",
      "Epoch 163, Loss: 0.4783027642591491\n",
      "Epoch 164, Loss: 0.4780003946284736\n",
      "Epoch 165, Loss: 0.47766356610156796\n",
      "Epoch 166, Loss: 0.47756940749162413\n",
      "Epoch 167, Loss: 0.4772720526560437\n",
      "Epoch 168, Loss: 0.477001191512605\n",
      "Epoch 169, Loss: 0.47690651155802954\n",
      "Epoch 170, Loss: 0.47663605231839845\n",
      "Epoch 171, Loss: 0.4763715079887003\n",
      "Epoch 172, Loss: 0.4762479807342815\n",
      "Epoch 173, Loss: 0.47603155482909787\n",
      "Epoch 174, Loss: 0.47580085926956445\n",
      "Epoch 175, Loss: 0.4756396378860013\n",
      "Epoch 176, Loss: 0.4754627232666358\n",
      "Epoch 177, Loss: 0.4752513954316692\n",
      "Epoch 178, Loss: 0.47507140350730925\n",
      "Epoch 179, Loss: 0.47492471273973497\n",
      "Epoch 180, Loss: 0.47475493910732763\n",
      "Epoch 181, Loss: 0.47455157018218247\n",
      "Epoch 182, Loss: 0.4743853224635176\n",
      "Epoch 183, Loss: 0.47424550491599726\n",
      "Epoch 184, Loss: 0.47408413178207975\n",
      "Epoch 185, Loss: 0.47390385374022453\n",
      "Epoch 186, Loss: 0.4737383066259127\n",
      "Epoch 187, Loss: 0.47358797785338436\n",
      "Epoch 188, Loss: 0.4734560449826861\n",
      "Epoch 189, Loss: 0.47335738206045025\n",
      "Epoch 190, Loss: 0.4733162834925985\n",
      "Epoch 191, Loss: 0.4732413020737725\n",
      "Epoch 192, Loss: 0.47313239020014447\n",
      "Epoch 193, Loss: 0.4729457158004841\n",
      "Epoch 194, Loss: 0.47274703738102797\n",
      "Epoch 195, Loss: 0.47253750657270627\n",
      "Epoch 196, Loss: 0.4723544299968521\n",
      "Epoch 197, Loss: 0.47221102907746554\n",
      "Epoch 198, Loss: 0.47210844467503205\n",
      "Epoch 199, Loss: 0.47205337980155637\n",
      "Epoch 200, Loss: 0.47204973383515075\n",
      "Epoch 201, Loss: 0.4720427890593303\n",
      "Epoch 202, Loss: 0.4719159657967507\n",
      "Epoch 203, Loss: 0.471735017109987\n",
      "Epoch 204, Loss: 0.4714985533244086\n",
      "Epoch 205, Loss: 0.4713143302002759\n",
      "Epoch 206, Loss: 0.47118574544243713\n",
      "Epoch 207, Loss: 0.47112002449718005\n",
      "Epoch 208, Loss: 0.4710910670518786\n",
      "Epoch 209, Loss: 0.47102495326923743\n",
      "Epoch 210, Loss: 0.4709626586360175\n",
      "Epoch 211, Loss: 0.47085840979895394\n",
      "Epoch 212, Loss: 0.47078064719388485\n",
      "Epoch 213, Loss: 0.47066614789276784\n",
      "Epoch 214, Loss: 0.47055654499036753\n",
      "Epoch 215, Loss: 0.47041652390832933\n",
      "Epoch 216, Loss: 0.4702446473560784\n",
      "Epoch 217, Loss: 0.4701045016774931\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2203850513526474\n",
      "Valence RMSE: 0.2074268828919827\n",
      "Arousal RMSE: 0.23262250530233264\n",
      "Test R^2 score: tensor([0.4786, 0.4642], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.3565, 0.3388], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4713786274037354\n",
      "Num of epochs: 218\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Epoch 121, Loss: 0.49295912913079615\n",
      "Epoch 122, Loss: 0.4924773619139838\n",
      "Epoch 123, Loss: 0.49197607191949805\n",
      "Epoch 124, Loss: 0.4914699197831528\n",
      "Epoch 125, Loss: 0.49091483381312667\n",
      "Epoch 126, Loss: 0.49035147677946317\n",
      "Epoch 127, Loss: 0.4898087222825037\n",
      "Epoch 128, Loss: 0.48924014731550836\n",
      "Epoch 129, Loss: 0.48873550647212044\n",
      "Epoch 130, Loss: 0.48832307254971563\n",
      "Epoch 131, Loss: 0.48791418392821295\n",
      "Epoch 132, Loss: 0.4875902245243796\n",
      "Epoch 133, Loss: 0.48728851108037585\n",
      "Epoch 134, Loss: 0.48697062262181345\n",
      "Epoch 135, Loss: 0.4866339553030548\n",
      "Epoch 136, Loss: 0.48633446746734893\n",
      "Epoch 137, Loss: 0.4860310087358514\n",
      "Epoch 138, Loss: 0.4856786412545615\n",
      "Epoch 139, Loss: 0.48531372107004\n",
      "Epoch 140, Loss: 0.48495712985488587\n",
      "Epoch 141, Loss: 0.4846131600652309\n",
      "Epoch 142, Loss: 0.48425966858652364\n",
      "Epoch 143, Loss: 0.4838914149236435\n",
      "Epoch 144, Loss: 0.48352437546282767\n",
      "Epoch 145, Loss: 0.4832299911937133\n",
      "Epoch 146, Loss: 0.4832586529465723\n",
      "Epoch 147, Loss: 0.48356744143113517\n",
      "Epoch 148, Loss: 0.4828108019011922\n",
      "Epoch 149, Loss: 0.4818912939738211\n",
      "Epoch 150, Loss: 0.48239416745575797\n",
      "Epoch 151, Loss: 0.4814952491721961\n",
      "Epoch 152, Loss: 0.4811822818820502\n",
      "Epoch 153, Loss: 0.4812753156716337\n",
      "Epoch 154, Loss: 0.4804387742282827\n",
      "Epoch 155, Loss: 0.48058189052417905\n",
      "Epoch 156, Loss: 0.4801288115497346\n",
      "Epoch 157, Loss: 0.4797465937246525\n",
      "Epoch 158, Loss: 0.4797622479529733\n",
      "Epoch 159, Loss: 0.4791664679844762\n",
      "Epoch 160, Loss: 0.47904359923301915\n",
      "Epoch 161, Loss: 0.4788483533353555\n",
      "Epoch 162, Loss: 0.4783709093142749\n",
      "Epoch 163, Loss: 0.4783027642591491\n",
      "Epoch 164, Loss: 0.4780003946284736\n",
      "Epoch 165, Loss: 0.47766356610156796\n",
      "Epoch 166, Loss: 0.47756940749162413\n",
      "Epoch 167, Loss: 0.4772720526560437\n",
      "Epoch 168, Loss: 0.477001191512605\n",
      "Epoch 169, Loss: 0.47690651155802954\n",
      "Epoch 170, Loss: 0.47663605231839845\n",
      "Epoch 171, Loss: 0.4763715079887003\n",
      "Epoch 172, Loss: 0.4762479807342815\n",
      "Epoch 173, Loss: 0.47603155482909787\n",
      "Epoch 174, Loss: 0.47580085926956445\n",
      "Epoch 175, Loss: 0.4756396378860013\n",
      "Epoch 176, Loss: 0.4754627232666358\n",
      "Epoch 177, Loss: 0.4752513954316692\n",
      "Epoch 178, Loss: 0.47507140350730925\n",
      "Epoch 179, Loss: 0.47492471273973497\n",
      "Epoch 180, Loss: 0.47475493910732763\n",
      "Epoch 181, Loss: 0.47455157018218247\n",
      "Epoch 182, Loss: 0.4743853224635176\n",
      "Epoch 183, Loss: 0.47424550491599726\n",
      "Epoch 184, Loss: 0.47408413178207975\n",
      "Epoch 185, Loss: 0.47390385374022453\n",
      "Epoch 186, Loss: 0.4737383066259127\n",
      "Epoch 187, Loss: 0.47358797785338436\n",
      "Epoch 188, Loss: 0.4734560449826861\n",
      "Epoch 189, Loss: 0.47335738206045025\n",
      "Epoch 190, Loss: 0.4733162834925985\n",
      "Epoch 191, Loss: 0.4732413020737725\n",
      "Epoch 192, Loss: 0.47313239020014447\n",
      "Epoch 193, Loss: 0.4729457158004841\n",
      "Epoch 194, Loss: 0.47274703738102797\n",
      "Epoch 195, Loss: 0.47253750657270627\n",
      "Epoch 196, Loss: 0.4723544299968521\n",
      "Epoch 197, Loss: 0.47221102907746554\n",
      "Epoch 198, Loss: 0.47210844467503205\n",
      "Epoch 199, Loss: 0.47205337980155637\n",
      "Epoch 200, Loss: 0.47204973383515075\n",
      "Epoch 201, Loss: 0.4720427890593303\n",
      "Epoch 202, Loss: 0.4719159657967507\n",
      "Epoch 203, Loss: 0.471735017109987\n",
      "Epoch 204, Loss: 0.4714985533244086\n",
      "Epoch 205, Loss: 0.4713143302002759\n",
      "Epoch 206, Loss: 0.47118574544243713\n",
      "Epoch 207, Loss: 0.47112002449718005\n",
      "Epoch 208, Loss: 0.4710910670518786\n",
      "Epoch 209, Loss: 0.47102495326923743\n",
      "Epoch 210, Loss: 0.4709626586360175\n",
      "Epoch 211, Loss: 0.47085840979895394\n",
      "Epoch 212, Loss: 0.47078064719388485\n",
      "Epoch 213, Loss: 0.47066614789276784\n",
      "Epoch 214, Loss: 0.47055654499036753\n",
      "Epoch 215, Loss: 0.47041652390832933\n",
      "Epoch 216, Loss: 0.4702446473560784\n",
      "Epoch 217, Loss: 0.4701045016774931\n",
      "Epoch 218, Loss: 0.47002059077245106\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2199748554926991\n",
      "Valence RMSE: 0.2070513126422796\n",
      "Arousal RMSE: 0.2321801628718024\n",
      "Test R^2 score: tensor([0.4805, 0.4662], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.3589, 0.3413], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4733397856060958\n",
      "Num of epochs: 219\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Epoch 121, Loss: 0.49295912913079615\n",
      "Epoch 122, Loss: 0.4924773619139838\n",
      "Epoch 123, Loss: 0.49197607191949805\n",
      "Epoch 124, Loss: 0.4914699197831528\n",
      "Epoch 125, Loss: 0.49091483381312667\n",
      "Epoch 126, Loss: 0.49035147677946317\n",
      "Epoch 127, Loss: 0.4898087222825037\n",
      "Epoch 128, Loss: 0.48924014731550836\n",
      "Epoch 129, Loss: 0.48873550647212044\n",
      "Epoch 130, Loss: 0.48832307254971563\n",
      "Epoch 131, Loss: 0.48791418392821295\n",
      "Epoch 132, Loss: 0.4875902245243796\n",
      "Epoch 133, Loss: 0.48728851108037585\n",
      "Epoch 134, Loss: 0.48697062262181345\n",
      "Epoch 135, Loss: 0.4866339553030548\n",
      "Epoch 136, Loss: 0.48633446746734893\n",
      "Epoch 137, Loss: 0.4860310087358514\n",
      "Epoch 138, Loss: 0.4856786412545615\n",
      "Epoch 139, Loss: 0.48531372107004\n",
      "Epoch 140, Loss: 0.48495712985488587\n",
      "Epoch 141, Loss: 0.4846131600652309\n",
      "Epoch 142, Loss: 0.48425966858652364\n",
      "Epoch 143, Loss: 0.4838914149236435\n",
      "Epoch 144, Loss: 0.48352437546282767\n",
      "Epoch 145, Loss: 0.4832299911937133\n",
      "Epoch 146, Loss: 0.4832586529465723\n",
      "Epoch 147, Loss: 0.48356744143113517\n",
      "Epoch 148, Loss: 0.4828108019011922\n",
      "Epoch 149, Loss: 0.4818912939738211\n",
      "Epoch 150, Loss: 0.48239416745575797\n",
      "Epoch 151, Loss: 0.4814952491721961\n",
      "Epoch 152, Loss: 0.4811822818820502\n",
      "Epoch 153, Loss: 0.4812753156716337\n",
      "Epoch 154, Loss: 0.4804387742282827\n",
      "Epoch 155, Loss: 0.48058189052417905\n",
      "Epoch 156, Loss: 0.4801288115497346\n",
      "Epoch 157, Loss: 0.4797465937246525\n",
      "Epoch 158, Loss: 0.4797622479529733\n",
      "Epoch 159, Loss: 0.4791664679844762\n",
      "Epoch 160, Loss: 0.47904359923301915\n",
      "Epoch 161, Loss: 0.4788483533353555\n",
      "Epoch 162, Loss: 0.4783709093142749\n",
      "Epoch 163, Loss: 0.4783027642591491\n",
      "Epoch 164, Loss: 0.4780003946284736\n",
      "Epoch 165, Loss: 0.47766356610156796\n",
      "Epoch 166, Loss: 0.47756940749162413\n",
      "Epoch 167, Loss: 0.4772720526560437\n",
      "Epoch 168, Loss: 0.477001191512605\n",
      "Epoch 169, Loss: 0.47690651155802954\n",
      "Epoch 170, Loss: 0.47663605231839845\n",
      "Epoch 171, Loss: 0.4763715079887003\n",
      "Epoch 172, Loss: 0.4762479807342815\n",
      "Epoch 173, Loss: 0.47603155482909787\n",
      "Epoch 174, Loss: 0.47580085926956445\n",
      "Epoch 175, Loss: 0.4756396378860013\n",
      "Epoch 176, Loss: 0.4754627232666358\n",
      "Epoch 177, Loss: 0.4752513954316692\n",
      "Epoch 178, Loss: 0.47507140350730925\n",
      "Epoch 179, Loss: 0.47492471273973497\n",
      "Epoch 180, Loss: 0.47475493910732763\n",
      "Epoch 181, Loss: 0.47455157018218247\n",
      "Epoch 182, Loss: 0.4743853224635176\n",
      "Epoch 183, Loss: 0.47424550491599726\n",
      "Epoch 184, Loss: 0.47408413178207975\n",
      "Epoch 185, Loss: 0.47390385374022453\n",
      "Epoch 186, Loss: 0.4737383066259127\n",
      "Epoch 187, Loss: 0.47358797785338436\n",
      "Epoch 188, Loss: 0.4734560449826861\n",
      "Epoch 189, Loss: 0.47335738206045025\n",
      "Epoch 190, Loss: 0.4733162834925985\n",
      "Epoch 191, Loss: 0.4732413020737725\n",
      "Epoch 192, Loss: 0.47313239020014447\n",
      "Epoch 193, Loss: 0.4729457158004841\n",
      "Epoch 194, Loss: 0.47274703738102797\n",
      "Epoch 195, Loss: 0.47253750657270627\n",
      "Epoch 196, Loss: 0.4723544299968521\n",
      "Epoch 197, Loss: 0.47221102907746554\n",
      "Epoch 198, Loss: 0.47210844467503205\n",
      "Epoch 199, Loss: 0.47205337980155637\n",
      "Epoch 200, Loss: 0.47204973383515075\n",
      "Epoch 201, Loss: 0.4720427890593303\n",
      "Epoch 202, Loss: 0.4719159657967507\n",
      "Epoch 203, Loss: 0.471735017109987\n",
      "Epoch 204, Loss: 0.4714985533244086\n",
      "Epoch 205, Loss: 0.4713143302002759\n",
      "Epoch 206, Loss: 0.47118574544243713\n",
      "Epoch 207, Loss: 0.47112002449718005\n",
      "Epoch 208, Loss: 0.4710910670518786\n",
      "Epoch 209, Loss: 0.47102495326923743\n",
      "Epoch 210, Loss: 0.4709626586360175\n",
      "Epoch 211, Loss: 0.47085840979895394\n",
      "Epoch 212, Loss: 0.47078064719388485\n",
      "Epoch 213, Loss: 0.47066614789276784\n",
      "Epoch 214, Loss: 0.47055654499036753\n",
      "Epoch 215, Loss: 0.47041652390832933\n",
      "Epoch 216, Loss: 0.4702446473560784\n",
      "Epoch 217, Loss: 0.4701045016774931\n",
      "Epoch 218, Loss: 0.47002059077245106\n",
      "Epoch 219, Loss: 0.4699643617672827\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.22048518092349415\n",
      "Valence RMSE: 0.20751884141310337\n",
      "Arousal RMSE: 0.23273023110952978\n",
      "Test R^2 score: tensor([0.4781, 0.4637], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.3560, 0.3382], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.47089922223715064\n",
      "Num of epochs: 220\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Epoch 121, Loss: 0.49295912913079615\n",
      "Epoch 122, Loss: 0.4924773619139838\n",
      "Epoch 123, Loss: 0.49197607191949805\n",
      "Epoch 124, Loss: 0.4914699197831528\n",
      "Epoch 125, Loss: 0.49091483381312667\n",
      "Epoch 126, Loss: 0.49035147677946317\n",
      "Epoch 127, Loss: 0.4898087222825037\n",
      "Epoch 128, Loss: 0.48924014731550836\n",
      "Epoch 129, Loss: 0.48873550647212044\n",
      "Epoch 130, Loss: 0.48832307254971563\n",
      "Epoch 131, Loss: 0.48791418392821295\n",
      "Epoch 132, Loss: 0.4875902245243796\n",
      "Epoch 133, Loss: 0.48728851108037585\n",
      "Epoch 134, Loss: 0.48697062262181345\n",
      "Epoch 135, Loss: 0.4866339553030548\n",
      "Epoch 136, Loss: 0.48633446746734893\n",
      "Epoch 137, Loss: 0.4860310087358514\n",
      "Epoch 138, Loss: 0.4856786412545615\n",
      "Epoch 139, Loss: 0.48531372107004\n",
      "Epoch 140, Loss: 0.48495712985488587\n",
      "Epoch 141, Loss: 0.4846131600652309\n",
      "Epoch 142, Loss: 0.48425966858652364\n",
      "Epoch 143, Loss: 0.4838914149236435\n",
      "Epoch 144, Loss: 0.48352437546282767\n",
      "Epoch 145, Loss: 0.4832299911937133\n",
      "Epoch 146, Loss: 0.4832586529465723\n",
      "Epoch 147, Loss: 0.48356744143113517\n",
      "Epoch 148, Loss: 0.4828108019011922\n",
      "Epoch 149, Loss: 0.4818912939738211\n",
      "Epoch 150, Loss: 0.48239416745575797\n",
      "Epoch 151, Loss: 0.4814952491721961\n",
      "Epoch 152, Loss: 0.4811822818820502\n",
      "Epoch 153, Loss: 0.4812753156716337\n",
      "Epoch 154, Loss: 0.4804387742282827\n",
      "Epoch 155, Loss: 0.48058189052417905\n",
      "Epoch 156, Loss: 0.4801288115497346\n",
      "Epoch 157, Loss: 0.4797465937246525\n",
      "Epoch 158, Loss: 0.4797622479529733\n",
      "Epoch 159, Loss: 0.4791664679844762\n",
      "Epoch 160, Loss: 0.47904359923301915\n",
      "Epoch 161, Loss: 0.4788483533353555\n",
      "Epoch 162, Loss: 0.4783709093142749\n",
      "Epoch 163, Loss: 0.4783027642591491\n",
      "Epoch 164, Loss: 0.4780003946284736\n",
      "Epoch 165, Loss: 0.47766356610156796\n",
      "Epoch 166, Loss: 0.47756940749162413\n",
      "Epoch 167, Loss: 0.4772720526560437\n",
      "Epoch 168, Loss: 0.477001191512605\n",
      "Epoch 169, Loss: 0.47690651155802954\n",
      "Epoch 170, Loss: 0.47663605231839845\n",
      "Epoch 171, Loss: 0.4763715079887003\n",
      "Epoch 172, Loss: 0.4762479807342815\n",
      "Epoch 173, Loss: 0.47603155482909787\n",
      "Epoch 174, Loss: 0.47580085926956445\n",
      "Epoch 175, Loss: 0.4756396378860013\n",
      "Epoch 176, Loss: 0.4754627232666358\n",
      "Epoch 177, Loss: 0.4752513954316692\n",
      "Epoch 178, Loss: 0.47507140350730925\n",
      "Epoch 179, Loss: 0.47492471273973497\n",
      "Epoch 180, Loss: 0.47475493910732763\n",
      "Epoch 181, Loss: 0.47455157018218247\n",
      "Epoch 182, Loss: 0.4743853224635176\n",
      "Epoch 183, Loss: 0.47424550491599726\n",
      "Epoch 184, Loss: 0.47408413178207975\n",
      "Epoch 185, Loss: 0.47390385374022453\n",
      "Epoch 186, Loss: 0.4737383066259127\n",
      "Epoch 187, Loss: 0.47358797785338436\n",
      "Epoch 188, Loss: 0.4734560449826861\n",
      "Epoch 189, Loss: 0.47335738206045025\n",
      "Epoch 190, Loss: 0.4733162834925985\n",
      "Epoch 191, Loss: 0.4732413020737725\n",
      "Epoch 192, Loss: 0.47313239020014447\n",
      "Epoch 193, Loss: 0.4729457158004841\n",
      "Epoch 194, Loss: 0.47274703738102797\n",
      "Epoch 195, Loss: 0.47253750657270627\n",
      "Epoch 196, Loss: 0.4723544299968521\n",
      "Epoch 197, Loss: 0.47221102907746554\n",
      "Epoch 198, Loss: 0.47210844467503205\n",
      "Epoch 199, Loss: 0.47205337980155637\n",
      "Epoch 200, Loss: 0.47204973383515075\n",
      "Epoch 201, Loss: 0.4720427890593303\n",
      "Epoch 202, Loss: 0.4719159657967507\n",
      "Epoch 203, Loss: 0.471735017109987\n",
      "Epoch 204, Loss: 0.4714985533244086\n",
      "Epoch 205, Loss: 0.4713143302002759\n",
      "Epoch 206, Loss: 0.47118574544243713\n",
      "Epoch 207, Loss: 0.47112002449718005\n",
      "Epoch 208, Loss: 0.4710910670518786\n",
      "Epoch 209, Loss: 0.47102495326923743\n",
      "Epoch 210, Loss: 0.4709626586360175\n",
      "Epoch 211, Loss: 0.47085840979895394\n",
      "Epoch 212, Loss: 0.47078064719388485\n",
      "Epoch 213, Loss: 0.47066614789276784\n",
      "Epoch 214, Loss: 0.47055654499036753\n",
      "Epoch 215, Loss: 0.47041652390832933\n",
      "Epoch 216, Loss: 0.4702446473560784\n",
      "Epoch 217, Loss: 0.4701045016774931\n",
      "Epoch 218, Loss: 0.47002059077245106\n",
      "Epoch 219, Loss: 0.4699643617672827\n",
      "Epoch 220, Loss: 0.4698888296176042\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2198679035548424\n",
      "Valence RMSE: 0.20696559246920726\n",
      "Arousal RMSE: 0.23205394536847027\n",
      "Test R^2 score: tensor([0.4809, 0.4668], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.3594, 0.3420], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4738449266203587\n",
      "Num of epochs: 221\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Epoch 121, Loss: 0.49295912913079615\n",
      "Epoch 122, Loss: 0.4924773619139838\n",
      "Epoch 123, Loss: 0.49197607191949805\n",
      "Epoch 124, Loss: 0.4914699197831528\n",
      "Epoch 125, Loss: 0.49091483381312667\n",
      "Epoch 126, Loss: 0.49035147677946317\n",
      "Epoch 127, Loss: 0.4898087222825037\n",
      "Epoch 128, Loss: 0.48924014731550836\n",
      "Epoch 129, Loss: 0.48873550647212044\n",
      "Epoch 130, Loss: 0.48832307254971563\n",
      "Epoch 131, Loss: 0.48791418392821295\n",
      "Epoch 132, Loss: 0.4875902245243796\n",
      "Epoch 133, Loss: 0.48728851108037585\n",
      "Epoch 134, Loss: 0.48697062262181345\n",
      "Epoch 135, Loss: 0.4866339553030548\n",
      "Epoch 136, Loss: 0.48633446746734893\n",
      "Epoch 137, Loss: 0.4860310087358514\n",
      "Epoch 138, Loss: 0.4856786412545615\n",
      "Epoch 139, Loss: 0.48531372107004\n",
      "Epoch 140, Loss: 0.48495712985488587\n",
      "Epoch 141, Loss: 0.4846131600652309\n",
      "Epoch 142, Loss: 0.48425966858652364\n",
      "Epoch 143, Loss: 0.4838914149236435\n",
      "Epoch 144, Loss: 0.48352437546282767\n",
      "Epoch 145, Loss: 0.4832299911937133\n",
      "Epoch 146, Loss: 0.4832586529465723\n",
      "Epoch 147, Loss: 0.48356744143113517\n",
      "Epoch 148, Loss: 0.4828108019011922\n",
      "Epoch 149, Loss: 0.4818912939738211\n",
      "Epoch 150, Loss: 0.48239416745575797\n",
      "Epoch 151, Loss: 0.4814952491721961\n",
      "Epoch 152, Loss: 0.4811822818820502\n",
      "Epoch 153, Loss: 0.4812753156716337\n",
      "Epoch 154, Loss: 0.4804387742282827\n",
      "Epoch 155, Loss: 0.48058189052417905\n",
      "Epoch 156, Loss: 0.4801288115497346\n",
      "Epoch 157, Loss: 0.4797465937246525\n",
      "Epoch 158, Loss: 0.4797622479529733\n",
      "Epoch 159, Loss: 0.4791664679844762\n",
      "Epoch 160, Loss: 0.47904359923301915\n",
      "Epoch 161, Loss: 0.4788483533353555\n",
      "Epoch 162, Loss: 0.4783709093142749\n",
      "Epoch 163, Loss: 0.4783027642591491\n",
      "Epoch 164, Loss: 0.4780003946284736\n",
      "Epoch 165, Loss: 0.47766356610156796\n",
      "Epoch 166, Loss: 0.47756940749162413\n",
      "Epoch 167, Loss: 0.4772720526560437\n",
      "Epoch 168, Loss: 0.477001191512605\n",
      "Epoch 169, Loss: 0.47690651155802954\n",
      "Epoch 170, Loss: 0.47663605231839845\n",
      "Epoch 171, Loss: 0.4763715079887003\n",
      "Epoch 172, Loss: 0.4762479807342815\n",
      "Epoch 173, Loss: 0.47603155482909787\n",
      "Epoch 174, Loss: 0.47580085926956445\n",
      "Epoch 175, Loss: 0.4756396378860013\n",
      "Epoch 176, Loss: 0.4754627232666358\n",
      "Epoch 177, Loss: 0.4752513954316692\n",
      "Epoch 178, Loss: 0.47507140350730925\n",
      "Epoch 179, Loss: 0.47492471273973497\n",
      "Epoch 180, Loss: 0.47475493910732763\n",
      "Epoch 181, Loss: 0.47455157018218247\n",
      "Epoch 182, Loss: 0.4743853224635176\n",
      "Epoch 183, Loss: 0.47424550491599726\n",
      "Epoch 184, Loss: 0.47408413178207975\n",
      "Epoch 185, Loss: 0.47390385374022453\n",
      "Epoch 186, Loss: 0.4737383066259127\n",
      "Epoch 187, Loss: 0.47358797785338436\n",
      "Epoch 188, Loss: 0.4734560449826861\n",
      "Epoch 189, Loss: 0.47335738206045025\n",
      "Epoch 190, Loss: 0.4733162834925985\n",
      "Epoch 191, Loss: 0.4732413020737725\n",
      "Epoch 192, Loss: 0.47313239020014447\n",
      "Epoch 193, Loss: 0.4729457158004841\n",
      "Epoch 194, Loss: 0.47274703738102797\n",
      "Epoch 195, Loss: 0.47253750657270627\n",
      "Epoch 196, Loss: 0.4723544299968521\n",
      "Epoch 197, Loss: 0.47221102907746554\n",
      "Epoch 198, Loss: 0.47210844467503205\n",
      "Epoch 199, Loss: 0.47205337980155637\n",
      "Epoch 200, Loss: 0.47204973383515075\n",
      "Epoch 201, Loss: 0.4720427890593303\n",
      "Epoch 202, Loss: 0.4719159657967507\n",
      "Epoch 203, Loss: 0.471735017109987\n",
      "Epoch 204, Loss: 0.4714985533244086\n",
      "Epoch 205, Loss: 0.4713143302002759\n",
      "Epoch 206, Loss: 0.47118574544243713\n",
      "Epoch 207, Loss: 0.47112002449718005\n",
      "Epoch 208, Loss: 0.4710910670518786\n",
      "Epoch 209, Loss: 0.47102495326923743\n",
      "Epoch 210, Loss: 0.4709626586360175\n",
      "Epoch 211, Loss: 0.47085840979895394\n",
      "Epoch 212, Loss: 0.47078064719388485\n",
      "Epoch 213, Loss: 0.47066614789276784\n",
      "Epoch 214, Loss: 0.47055654499036753\n",
      "Epoch 215, Loss: 0.47041652390832933\n",
      "Epoch 216, Loss: 0.4702446473560784\n",
      "Epoch 217, Loss: 0.4701045016774931\n",
      "Epoch 218, Loss: 0.47002059077245106\n",
      "Epoch 219, Loss: 0.4699643617672827\n",
      "Epoch 220, Loss: 0.4698888296176042\n",
      "Epoch 221, Loss: 0.4698141575467262\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.22038777580799288\n",
      "Valence RMSE: 0.2074767571644462\n",
      "Arousal RMSE: 0.23258318659721228\n",
      "Test R^2 score: tensor([0.4783, 0.4644], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.3562, 0.3390], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.47134379533557197\n",
      "Num of epochs: 222\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Epoch 121, Loss: 0.49295912913079615\n",
      "Epoch 122, Loss: 0.4924773619139838\n",
      "Epoch 123, Loss: 0.49197607191949805\n",
      "Epoch 124, Loss: 0.4914699197831528\n",
      "Epoch 125, Loss: 0.49091483381312667\n",
      "Epoch 126, Loss: 0.49035147677946317\n",
      "Epoch 127, Loss: 0.4898087222825037\n",
      "Epoch 128, Loss: 0.48924014731550836\n",
      "Epoch 129, Loss: 0.48873550647212044\n",
      "Epoch 130, Loss: 0.48832307254971563\n",
      "Epoch 131, Loss: 0.48791418392821295\n",
      "Epoch 132, Loss: 0.4875902245243796\n",
      "Epoch 133, Loss: 0.48728851108037585\n",
      "Epoch 134, Loss: 0.48697062262181345\n",
      "Epoch 135, Loss: 0.4866339553030548\n",
      "Epoch 136, Loss: 0.48633446746734893\n",
      "Epoch 137, Loss: 0.4860310087358514\n",
      "Epoch 138, Loss: 0.4856786412545615\n",
      "Epoch 139, Loss: 0.48531372107004\n",
      "Epoch 140, Loss: 0.48495712985488587\n",
      "Epoch 141, Loss: 0.4846131600652309\n",
      "Epoch 142, Loss: 0.48425966858652364\n",
      "Epoch 143, Loss: 0.4838914149236435\n",
      "Epoch 144, Loss: 0.48352437546282767\n",
      "Epoch 145, Loss: 0.4832299911937133\n",
      "Epoch 146, Loss: 0.4832586529465723\n",
      "Epoch 147, Loss: 0.48356744143113517\n",
      "Epoch 148, Loss: 0.4828108019011922\n",
      "Epoch 149, Loss: 0.4818912939738211\n",
      "Epoch 150, Loss: 0.48239416745575797\n",
      "Epoch 151, Loss: 0.4814952491721961\n",
      "Epoch 152, Loss: 0.4811822818820502\n",
      "Epoch 153, Loss: 0.4812753156716337\n",
      "Epoch 154, Loss: 0.4804387742282827\n",
      "Epoch 155, Loss: 0.48058189052417905\n",
      "Epoch 156, Loss: 0.4801288115497346\n",
      "Epoch 157, Loss: 0.4797465937246525\n",
      "Epoch 158, Loss: 0.4797622479529733\n",
      "Epoch 159, Loss: 0.4791664679844762\n",
      "Epoch 160, Loss: 0.47904359923301915\n",
      "Epoch 161, Loss: 0.4788483533353555\n",
      "Epoch 162, Loss: 0.4783709093142749\n",
      "Epoch 163, Loss: 0.4783027642591491\n",
      "Epoch 164, Loss: 0.4780003946284736\n",
      "Epoch 165, Loss: 0.47766356610156796\n",
      "Epoch 166, Loss: 0.47756940749162413\n",
      "Epoch 167, Loss: 0.4772720526560437\n",
      "Epoch 168, Loss: 0.477001191512605\n",
      "Epoch 169, Loss: 0.47690651155802954\n",
      "Epoch 170, Loss: 0.47663605231839845\n",
      "Epoch 171, Loss: 0.4763715079887003\n",
      "Epoch 172, Loss: 0.4762479807342815\n",
      "Epoch 173, Loss: 0.47603155482909787\n",
      "Epoch 174, Loss: 0.47580085926956445\n",
      "Epoch 175, Loss: 0.4756396378860013\n",
      "Epoch 176, Loss: 0.4754627232666358\n",
      "Epoch 177, Loss: 0.4752513954316692\n",
      "Epoch 178, Loss: 0.47507140350730925\n",
      "Epoch 179, Loss: 0.47492471273973497\n",
      "Epoch 180, Loss: 0.47475493910732763\n",
      "Epoch 181, Loss: 0.47455157018218247\n",
      "Epoch 182, Loss: 0.4743853224635176\n",
      "Epoch 183, Loss: 0.47424550491599726\n",
      "Epoch 184, Loss: 0.47408413178207975\n",
      "Epoch 185, Loss: 0.47390385374022453\n",
      "Epoch 186, Loss: 0.4737383066259127\n",
      "Epoch 187, Loss: 0.47358797785338436\n",
      "Epoch 188, Loss: 0.4734560449826861\n",
      "Epoch 189, Loss: 0.47335738206045025\n",
      "Epoch 190, Loss: 0.4733162834925985\n",
      "Epoch 191, Loss: 0.4732413020737725\n",
      "Epoch 192, Loss: 0.47313239020014447\n",
      "Epoch 193, Loss: 0.4729457158004841\n",
      "Epoch 194, Loss: 0.47274703738102797\n",
      "Epoch 195, Loss: 0.47253750657270627\n",
      "Epoch 196, Loss: 0.4723544299968521\n",
      "Epoch 197, Loss: 0.47221102907746554\n",
      "Epoch 198, Loss: 0.47210844467503205\n",
      "Epoch 199, Loss: 0.47205337980155637\n",
      "Epoch 200, Loss: 0.47204973383515075\n",
      "Epoch 201, Loss: 0.4720427890593303\n",
      "Epoch 202, Loss: 0.4719159657967507\n",
      "Epoch 203, Loss: 0.471735017109987\n",
      "Epoch 204, Loss: 0.4714985533244086\n",
      "Epoch 205, Loss: 0.4713143302002759\n",
      "Epoch 206, Loss: 0.47118574544243713\n",
      "Epoch 207, Loss: 0.47112002449718005\n",
      "Epoch 208, Loss: 0.4710910670518786\n",
      "Epoch 209, Loss: 0.47102495326923743\n",
      "Epoch 210, Loss: 0.4709626586360175\n",
      "Epoch 211, Loss: 0.47085840979895394\n",
      "Epoch 212, Loss: 0.47078064719388485\n",
      "Epoch 213, Loss: 0.47066614789276784\n",
      "Epoch 214, Loss: 0.47055654499036753\n",
      "Epoch 215, Loss: 0.47041652390832933\n",
      "Epoch 216, Loss: 0.4702446473560784\n",
      "Epoch 217, Loss: 0.4701045016774931\n",
      "Epoch 218, Loss: 0.47002059077245106\n",
      "Epoch 219, Loss: 0.4699643617672827\n",
      "Epoch 220, Loss: 0.4698888296176042\n",
      "Epoch 221, Loss: 0.4698141575467262\n",
      "Epoch 222, Loss: 0.4697265783615268\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.21975806030524345\n",
      "Valence RMSE: 0.20689975078584014\n",
      "Arousal RMSE: 0.23190451324414382\n",
      "Test R^2 score: tensor([0.4812, 0.4675], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.3598, 0.3429], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.47435328867819704\n",
      "Num of epochs: 223\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Epoch 121, Loss: 0.49295912913079615\n",
      "Epoch 122, Loss: 0.4924773619139838\n",
      "Epoch 123, Loss: 0.49197607191949805\n",
      "Epoch 124, Loss: 0.4914699197831528\n",
      "Epoch 125, Loss: 0.49091483381312667\n",
      "Epoch 126, Loss: 0.49035147677946317\n",
      "Epoch 127, Loss: 0.4898087222825037\n",
      "Epoch 128, Loss: 0.48924014731550836\n",
      "Epoch 129, Loss: 0.48873550647212044\n",
      "Epoch 130, Loss: 0.48832307254971563\n",
      "Epoch 131, Loss: 0.48791418392821295\n",
      "Epoch 132, Loss: 0.4875902245243796\n",
      "Epoch 133, Loss: 0.48728851108037585\n",
      "Epoch 134, Loss: 0.48697062262181345\n",
      "Epoch 135, Loss: 0.4866339553030548\n",
      "Epoch 136, Loss: 0.48633446746734893\n",
      "Epoch 137, Loss: 0.4860310087358514\n",
      "Epoch 138, Loss: 0.4856786412545615\n",
      "Epoch 139, Loss: 0.48531372107004\n",
      "Epoch 140, Loss: 0.48495712985488587\n",
      "Epoch 141, Loss: 0.4846131600652309\n",
      "Epoch 142, Loss: 0.48425966858652364\n",
      "Epoch 143, Loss: 0.4838914149236435\n",
      "Epoch 144, Loss: 0.48352437546282767\n",
      "Epoch 145, Loss: 0.4832299911937133\n",
      "Epoch 146, Loss: 0.4832586529465723\n",
      "Epoch 147, Loss: 0.48356744143113517\n",
      "Epoch 148, Loss: 0.4828108019011922\n",
      "Epoch 149, Loss: 0.4818912939738211\n",
      "Epoch 150, Loss: 0.48239416745575797\n",
      "Epoch 151, Loss: 0.4814952491721961\n",
      "Epoch 152, Loss: 0.4811822818820502\n",
      "Epoch 153, Loss: 0.4812753156716337\n",
      "Epoch 154, Loss: 0.4804387742282827\n",
      "Epoch 155, Loss: 0.48058189052417905\n",
      "Epoch 156, Loss: 0.4801288115497346\n",
      "Epoch 157, Loss: 0.4797465937246525\n",
      "Epoch 158, Loss: 0.4797622479529733\n",
      "Epoch 159, Loss: 0.4791664679844762\n",
      "Epoch 160, Loss: 0.47904359923301915\n",
      "Epoch 161, Loss: 0.4788483533353555\n",
      "Epoch 162, Loss: 0.4783709093142749\n",
      "Epoch 163, Loss: 0.4783027642591491\n",
      "Epoch 164, Loss: 0.4780003946284736\n",
      "Epoch 165, Loss: 0.47766356610156796\n",
      "Epoch 166, Loss: 0.47756940749162413\n",
      "Epoch 167, Loss: 0.4772720526560437\n",
      "Epoch 168, Loss: 0.477001191512605\n",
      "Epoch 169, Loss: 0.47690651155802954\n",
      "Epoch 170, Loss: 0.47663605231839845\n",
      "Epoch 171, Loss: 0.4763715079887003\n",
      "Epoch 172, Loss: 0.4762479807342815\n",
      "Epoch 173, Loss: 0.47603155482909787\n",
      "Epoch 174, Loss: 0.47580085926956445\n",
      "Epoch 175, Loss: 0.4756396378860013\n",
      "Epoch 176, Loss: 0.4754627232666358\n",
      "Epoch 177, Loss: 0.4752513954316692\n",
      "Epoch 178, Loss: 0.47507140350730925\n",
      "Epoch 179, Loss: 0.47492471273973497\n",
      "Epoch 180, Loss: 0.47475493910732763\n",
      "Epoch 181, Loss: 0.47455157018218247\n",
      "Epoch 182, Loss: 0.4743853224635176\n",
      "Epoch 183, Loss: 0.47424550491599726\n",
      "Epoch 184, Loss: 0.47408413178207975\n",
      "Epoch 185, Loss: 0.47390385374022453\n",
      "Epoch 186, Loss: 0.4737383066259127\n",
      "Epoch 187, Loss: 0.47358797785338436\n",
      "Epoch 188, Loss: 0.4734560449826861\n",
      "Epoch 189, Loss: 0.47335738206045025\n",
      "Epoch 190, Loss: 0.4733162834925985\n",
      "Epoch 191, Loss: 0.4732413020737725\n",
      "Epoch 192, Loss: 0.47313239020014447\n",
      "Epoch 193, Loss: 0.4729457158004841\n",
      "Epoch 194, Loss: 0.47274703738102797\n",
      "Epoch 195, Loss: 0.47253750657270627\n",
      "Epoch 196, Loss: 0.4723544299968521\n",
      "Epoch 197, Loss: 0.47221102907746554\n",
      "Epoch 198, Loss: 0.47210844467503205\n",
      "Epoch 199, Loss: 0.47205337980155637\n",
      "Epoch 200, Loss: 0.47204973383515075\n",
      "Epoch 201, Loss: 0.4720427890593303\n",
      "Epoch 202, Loss: 0.4719159657967507\n",
      "Epoch 203, Loss: 0.471735017109987\n",
      "Epoch 204, Loss: 0.4714985533244086\n",
      "Epoch 205, Loss: 0.4713143302002759\n",
      "Epoch 206, Loss: 0.47118574544243713\n",
      "Epoch 207, Loss: 0.47112002449718005\n",
      "Epoch 208, Loss: 0.4710910670518786\n",
      "Epoch 209, Loss: 0.47102495326923743\n",
      "Epoch 210, Loss: 0.4709626586360175\n",
      "Epoch 211, Loss: 0.47085840979895394\n",
      "Epoch 212, Loss: 0.47078064719388485\n",
      "Epoch 213, Loss: 0.47066614789276784\n",
      "Epoch 214, Loss: 0.47055654499036753\n",
      "Epoch 215, Loss: 0.47041652390832933\n",
      "Epoch 216, Loss: 0.4702446473560784\n",
      "Epoch 217, Loss: 0.4701045016774931\n",
      "Epoch 218, Loss: 0.47002059077245106\n",
      "Epoch 219, Loss: 0.4699643617672827\n",
      "Epoch 220, Loss: 0.4698888296176042\n",
      "Epoch 221, Loss: 0.4698141575467262\n",
      "Epoch 222, Loss: 0.4697265783615268\n",
      "Epoch 223, Loss: 0.46966066909468496\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2204648130169315\n",
      "Valence RMSE: 0.20759742248122282\n",
      "Arousal RMSE: 0.23262153326021612\n",
      "Test R^2 score: tensor([0.4777, 0.4642], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.3555, 0.3388], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4709519867537557\n",
      "Num of epochs: 224\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Epoch 121, Loss: 0.49295912913079615\n",
      "Epoch 122, Loss: 0.4924773619139838\n",
      "Epoch 123, Loss: 0.49197607191949805\n",
      "Epoch 124, Loss: 0.4914699197831528\n",
      "Epoch 125, Loss: 0.49091483381312667\n",
      "Epoch 126, Loss: 0.49035147677946317\n",
      "Epoch 127, Loss: 0.4898087222825037\n",
      "Epoch 128, Loss: 0.48924014731550836\n",
      "Epoch 129, Loss: 0.48873550647212044\n",
      "Epoch 130, Loss: 0.48832307254971563\n",
      "Epoch 131, Loss: 0.48791418392821295\n",
      "Epoch 132, Loss: 0.4875902245243796\n",
      "Epoch 133, Loss: 0.48728851108037585\n",
      "Epoch 134, Loss: 0.48697062262181345\n",
      "Epoch 135, Loss: 0.4866339553030548\n",
      "Epoch 136, Loss: 0.48633446746734893\n",
      "Epoch 137, Loss: 0.4860310087358514\n",
      "Epoch 138, Loss: 0.4856786412545615\n",
      "Epoch 139, Loss: 0.48531372107004\n",
      "Epoch 140, Loss: 0.48495712985488587\n",
      "Epoch 141, Loss: 0.4846131600652309\n",
      "Epoch 142, Loss: 0.48425966858652364\n",
      "Epoch 143, Loss: 0.4838914149236435\n",
      "Epoch 144, Loss: 0.48352437546282767\n",
      "Epoch 145, Loss: 0.4832299911937133\n",
      "Epoch 146, Loss: 0.4832586529465723\n",
      "Epoch 147, Loss: 0.48356744143113517\n",
      "Epoch 148, Loss: 0.4828108019011922\n",
      "Epoch 149, Loss: 0.4818912939738211\n",
      "Epoch 150, Loss: 0.48239416745575797\n",
      "Epoch 151, Loss: 0.4814952491721961\n",
      "Epoch 152, Loss: 0.4811822818820502\n",
      "Epoch 153, Loss: 0.4812753156716337\n",
      "Epoch 154, Loss: 0.4804387742282827\n",
      "Epoch 155, Loss: 0.48058189052417905\n",
      "Epoch 156, Loss: 0.4801288115497346\n",
      "Epoch 157, Loss: 0.4797465937246525\n",
      "Epoch 158, Loss: 0.4797622479529733\n",
      "Epoch 159, Loss: 0.4791664679844762\n",
      "Epoch 160, Loss: 0.47904359923301915\n",
      "Epoch 161, Loss: 0.4788483533353555\n",
      "Epoch 162, Loss: 0.4783709093142749\n",
      "Epoch 163, Loss: 0.4783027642591491\n",
      "Epoch 164, Loss: 0.4780003946284736\n",
      "Epoch 165, Loss: 0.47766356610156796\n",
      "Epoch 166, Loss: 0.47756940749162413\n",
      "Epoch 167, Loss: 0.4772720526560437\n",
      "Epoch 168, Loss: 0.477001191512605\n",
      "Epoch 169, Loss: 0.47690651155802954\n",
      "Epoch 170, Loss: 0.47663605231839845\n",
      "Epoch 171, Loss: 0.4763715079887003\n",
      "Epoch 172, Loss: 0.4762479807342815\n",
      "Epoch 173, Loss: 0.47603155482909787\n",
      "Epoch 174, Loss: 0.47580085926956445\n",
      "Epoch 175, Loss: 0.4756396378860013\n",
      "Epoch 176, Loss: 0.4754627232666358\n",
      "Epoch 177, Loss: 0.4752513954316692\n",
      "Epoch 178, Loss: 0.47507140350730925\n",
      "Epoch 179, Loss: 0.47492471273973497\n",
      "Epoch 180, Loss: 0.47475493910732763\n",
      "Epoch 181, Loss: 0.47455157018218247\n",
      "Epoch 182, Loss: 0.4743853224635176\n",
      "Epoch 183, Loss: 0.47424550491599726\n",
      "Epoch 184, Loss: 0.47408413178207975\n",
      "Epoch 185, Loss: 0.47390385374022453\n",
      "Epoch 186, Loss: 0.4737383066259127\n",
      "Epoch 187, Loss: 0.47358797785338436\n",
      "Epoch 188, Loss: 0.4734560449826861\n",
      "Epoch 189, Loss: 0.47335738206045025\n",
      "Epoch 190, Loss: 0.4733162834925985\n",
      "Epoch 191, Loss: 0.4732413020737725\n",
      "Epoch 192, Loss: 0.47313239020014447\n",
      "Epoch 193, Loss: 0.4729457158004841\n",
      "Epoch 194, Loss: 0.47274703738102797\n",
      "Epoch 195, Loss: 0.47253750657270627\n",
      "Epoch 196, Loss: 0.4723544299968521\n",
      "Epoch 197, Loss: 0.47221102907746554\n",
      "Epoch 198, Loss: 0.47210844467503205\n",
      "Epoch 199, Loss: 0.47205337980155637\n",
      "Epoch 200, Loss: 0.47204973383515075\n",
      "Epoch 201, Loss: 0.4720427890593303\n",
      "Epoch 202, Loss: 0.4719159657967507\n",
      "Epoch 203, Loss: 0.471735017109987\n",
      "Epoch 204, Loss: 0.4714985533244086\n",
      "Epoch 205, Loss: 0.4713143302002759\n",
      "Epoch 206, Loss: 0.47118574544243713\n",
      "Epoch 207, Loss: 0.47112002449718005\n",
      "Epoch 208, Loss: 0.4710910670518786\n",
      "Epoch 209, Loss: 0.47102495326923743\n",
      "Epoch 210, Loss: 0.4709626586360175\n",
      "Epoch 211, Loss: 0.47085840979895394\n",
      "Epoch 212, Loss: 0.47078064719388485\n",
      "Epoch 213, Loss: 0.47066614789276784\n",
      "Epoch 214, Loss: 0.47055654499036753\n",
      "Epoch 215, Loss: 0.47041652390832933\n",
      "Epoch 216, Loss: 0.4702446473560784\n",
      "Epoch 217, Loss: 0.4701045016774931\n",
      "Epoch 218, Loss: 0.47002059077245106\n",
      "Epoch 219, Loss: 0.4699643617672827\n",
      "Epoch 220, Loss: 0.4698888296176042\n",
      "Epoch 221, Loss: 0.4698141575467262\n",
      "Epoch 222, Loss: 0.4697265783615268\n",
      "Epoch 223, Loss: 0.46966066909468496\n",
      "Epoch 224, Loss: 0.4696348739248034\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.21948238423733407\n",
      "Valence RMSE: 0.20665436475395255\n",
      "Arousal RMSE: 0.2316009661230925\n",
      "Test R^2 score: tensor([0.4824, 0.4689], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.3613, 0.3446], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.47566477115307676\n",
      "Num of epochs: 225\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Epoch 121, Loss: 0.49295912913079615\n",
      "Epoch 122, Loss: 0.4924773619139838\n",
      "Epoch 123, Loss: 0.49197607191949805\n",
      "Epoch 124, Loss: 0.4914699197831528\n",
      "Epoch 125, Loss: 0.49091483381312667\n",
      "Epoch 126, Loss: 0.49035147677946317\n",
      "Epoch 127, Loss: 0.4898087222825037\n",
      "Epoch 128, Loss: 0.48924014731550836\n",
      "Epoch 129, Loss: 0.48873550647212044\n",
      "Epoch 130, Loss: 0.48832307254971563\n",
      "Epoch 131, Loss: 0.48791418392821295\n",
      "Epoch 132, Loss: 0.4875902245243796\n",
      "Epoch 133, Loss: 0.48728851108037585\n",
      "Epoch 134, Loss: 0.48697062262181345\n",
      "Epoch 135, Loss: 0.4866339553030548\n",
      "Epoch 136, Loss: 0.48633446746734893\n",
      "Epoch 137, Loss: 0.4860310087358514\n",
      "Epoch 138, Loss: 0.4856786412545615\n",
      "Epoch 139, Loss: 0.48531372107004\n",
      "Epoch 140, Loss: 0.48495712985488587\n",
      "Epoch 141, Loss: 0.4846131600652309\n",
      "Epoch 142, Loss: 0.48425966858652364\n",
      "Epoch 143, Loss: 0.4838914149236435\n",
      "Epoch 144, Loss: 0.48352437546282767\n",
      "Epoch 145, Loss: 0.4832299911937133\n",
      "Epoch 146, Loss: 0.4832586529465723\n",
      "Epoch 147, Loss: 0.48356744143113517\n",
      "Epoch 148, Loss: 0.4828108019011922\n",
      "Epoch 149, Loss: 0.4818912939738211\n",
      "Epoch 150, Loss: 0.48239416745575797\n",
      "Epoch 151, Loss: 0.4814952491721961\n",
      "Epoch 152, Loss: 0.4811822818820502\n",
      "Epoch 153, Loss: 0.4812753156716337\n",
      "Epoch 154, Loss: 0.4804387742282827\n",
      "Epoch 155, Loss: 0.48058189052417905\n",
      "Epoch 156, Loss: 0.4801288115497346\n",
      "Epoch 157, Loss: 0.4797465937246525\n",
      "Epoch 158, Loss: 0.4797622479529733\n",
      "Epoch 159, Loss: 0.4791664679844762\n",
      "Epoch 160, Loss: 0.47904359923301915\n",
      "Epoch 161, Loss: 0.4788483533353555\n",
      "Epoch 162, Loss: 0.4783709093142749\n",
      "Epoch 163, Loss: 0.4783027642591491\n",
      "Epoch 164, Loss: 0.4780003946284736\n",
      "Epoch 165, Loss: 0.47766356610156796\n",
      "Epoch 166, Loss: 0.47756940749162413\n",
      "Epoch 167, Loss: 0.4772720526560437\n",
      "Epoch 168, Loss: 0.477001191512605\n",
      "Epoch 169, Loss: 0.47690651155802954\n",
      "Epoch 170, Loss: 0.47663605231839845\n",
      "Epoch 171, Loss: 0.4763715079887003\n",
      "Epoch 172, Loss: 0.4762479807342815\n",
      "Epoch 173, Loss: 0.47603155482909787\n",
      "Epoch 174, Loss: 0.47580085926956445\n",
      "Epoch 175, Loss: 0.4756396378860013\n",
      "Epoch 176, Loss: 0.4754627232666358\n",
      "Epoch 177, Loss: 0.4752513954316692\n",
      "Epoch 178, Loss: 0.47507140350730925\n",
      "Epoch 179, Loss: 0.47492471273973497\n",
      "Epoch 180, Loss: 0.47475493910732763\n",
      "Epoch 181, Loss: 0.47455157018218247\n",
      "Epoch 182, Loss: 0.4743853224635176\n",
      "Epoch 183, Loss: 0.47424550491599726\n",
      "Epoch 184, Loss: 0.47408413178207975\n",
      "Epoch 185, Loss: 0.47390385374022453\n",
      "Epoch 186, Loss: 0.4737383066259127\n",
      "Epoch 187, Loss: 0.47358797785338436\n",
      "Epoch 188, Loss: 0.4734560449826861\n",
      "Epoch 189, Loss: 0.47335738206045025\n",
      "Epoch 190, Loss: 0.4733162834925985\n",
      "Epoch 191, Loss: 0.4732413020737725\n",
      "Epoch 192, Loss: 0.47313239020014447\n",
      "Epoch 193, Loss: 0.4729457158004841\n",
      "Epoch 194, Loss: 0.47274703738102797\n",
      "Epoch 195, Loss: 0.47253750657270627\n",
      "Epoch 196, Loss: 0.4723544299968521\n",
      "Epoch 197, Loss: 0.47221102907746554\n",
      "Epoch 198, Loss: 0.47210844467503205\n",
      "Epoch 199, Loss: 0.47205337980155637\n",
      "Epoch 200, Loss: 0.47204973383515075\n",
      "Epoch 201, Loss: 0.4720427890593303\n",
      "Epoch 202, Loss: 0.4719159657967507\n",
      "Epoch 203, Loss: 0.471735017109987\n",
      "Epoch 204, Loss: 0.4714985533244086\n",
      "Epoch 205, Loss: 0.4713143302002759\n",
      "Epoch 206, Loss: 0.47118574544243713\n",
      "Epoch 207, Loss: 0.47112002449718005\n",
      "Epoch 208, Loss: 0.4710910670518786\n",
      "Epoch 209, Loss: 0.47102495326923743\n",
      "Epoch 210, Loss: 0.4709626586360175\n",
      "Epoch 211, Loss: 0.47085840979895394\n",
      "Epoch 212, Loss: 0.47078064719388485\n",
      "Epoch 213, Loss: 0.47066614789276784\n",
      "Epoch 214, Loss: 0.47055654499036753\n",
      "Epoch 215, Loss: 0.47041652390832933\n",
      "Epoch 216, Loss: 0.4702446473560784\n",
      "Epoch 217, Loss: 0.4701045016774931\n",
      "Epoch 218, Loss: 0.47002059077245106\n",
      "Epoch 219, Loss: 0.4699643617672827\n",
      "Epoch 220, Loss: 0.4698888296176042\n",
      "Epoch 221, Loss: 0.4698141575467262\n",
      "Epoch 222, Loss: 0.4697265783615268\n",
      "Epoch 223, Loss: 0.46966066909468496\n",
      "Epoch 224, Loss: 0.4696348739248034\n",
      "Epoch 225, Loss: 0.4697176006517021\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.22134906767281823\n",
      "Valence RMSE: 0.2084906827381727\n",
      "Arousal RMSE: 0.2335004383949254\n",
      "Test R^2 score: tensor([0.4732, 0.4601], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.3499, 0.3338], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4666715777566525\n",
      "Num of epochs: 226\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Epoch 121, Loss: 0.49295912913079615\n",
      "Epoch 122, Loss: 0.4924773619139838\n",
      "Epoch 123, Loss: 0.49197607191949805\n",
      "Epoch 124, Loss: 0.4914699197831528\n",
      "Epoch 125, Loss: 0.49091483381312667\n",
      "Epoch 126, Loss: 0.49035147677946317\n",
      "Epoch 127, Loss: 0.4898087222825037\n",
      "Epoch 128, Loss: 0.48924014731550836\n",
      "Epoch 129, Loss: 0.48873550647212044\n",
      "Epoch 130, Loss: 0.48832307254971563\n",
      "Epoch 131, Loss: 0.48791418392821295\n",
      "Epoch 132, Loss: 0.4875902245243796\n",
      "Epoch 133, Loss: 0.48728851108037585\n",
      "Epoch 134, Loss: 0.48697062262181345\n",
      "Epoch 135, Loss: 0.4866339553030548\n",
      "Epoch 136, Loss: 0.48633446746734893\n",
      "Epoch 137, Loss: 0.4860310087358514\n",
      "Epoch 138, Loss: 0.4856786412545615\n",
      "Epoch 139, Loss: 0.48531372107004\n",
      "Epoch 140, Loss: 0.48495712985488587\n",
      "Epoch 141, Loss: 0.4846131600652309\n",
      "Epoch 142, Loss: 0.48425966858652364\n",
      "Epoch 143, Loss: 0.4838914149236435\n",
      "Epoch 144, Loss: 0.48352437546282767\n",
      "Epoch 145, Loss: 0.4832299911937133\n",
      "Epoch 146, Loss: 0.4832586529465723\n",
      "Epoch 147, Loss: 0.48356744143113517\n",
      "Epoch 148, Loss: 0.4828108019011922\n",
      "Epoch 149, Loss: 0.4818912939738211\n",
      "Epoch 150, Loss: 0.48239416745575797\n",
      "Epoch 151, Loss: 0.4814952491721961\n",
      "Epoch 152, Loss: 0.4811822818820502\n",
      "Epoch 153, Loss: 0.4812753156716337\n",
      "Epoch 154, Loss: 0.4804387742282827\n",
      "Epoch 155, Loss: 0.48058189052417905\n",
      "Epoch 156, Loss: 0.4801288115497346\n",
      "Epoch 157, Loss: 0.4797465937246525\n",
      "Epoch 158, Loss: 0.4797622479529733\n",
      "Epoch 159, Loss: 0.4791664679844762\n",
      "Epoch 160, Loss: 0.47904359923301915\n",
      "Epoch 161, Loss: 0.4788483533353555\n",
      "Epoch 162, Loss: 0.4783709093142749\n",
      "Epoch 163, Loss: 0.4783027642591491\n",
      "Epoch 164, Loss: 0.4780003946284736\n",
      "Epoch 165, Loss: 0.47766356610156796\n",
      "Epoch 166, Loss: 0.47756940749162413\n",
      "Epoch 167, Loss: 0.4772720526560437\n",
      "Epoch 168, Loss: 0.477001191512605\n",
      "Epoch 169, Loss: 0.47690651155802954\n",
      "Epoch 170, Loss: 0.47663605231839845\n",
      "Epoch 171, Loss: 0.4763715079887003\n",
      "Epoch 172, Loss: 0.4762479807342815\n",
      "Epoch 173, Loss: 0.47603155482909787\n",
      "Epoch 174, Loss: 0.47580085926956445\n",
      "Epoch 175, Loss: 0.4756396378860013\n",
      "Epoch 176, Loss: 0.4754627232666358\n",
      "Epoch 177, Loss: 0.4752513954316692\n",
      "Epoch 178, Loss: 0.47507140350730925\n",
      "Epoch 179, Loss: 0.47492471273973497\n",
      "Epoch 180, Loss: 0.47475493910732763\n",
      "Epoch 181, Loss: 0.47455157018218247\n",
      "Epoch 182, Loss: 0.4743853224635176\n",
      "Epoch 183, Loss: 0.47424550491599726\n",
      "Epoch 184, Loss: 0.47408413178207975\n",
      "Epoch 185, Loss: 0.47390385374022453\n",
      "Epoch 186, Loss: 0.4737383066259127\n",
      "Epoch 187, Loss: 0.47358797785338436\n",
      "Epoch 188, Loss: 0.4734560449826861\n",
      "Epoch 189, Loss: 0.47335738206045025\n",
      "Epoch 190, Loss: 0.4733162834925985\n",
      "Epoch 191, Loss: 0.4732413020737725\n",
      "Epoch 192, Loss: 0.47313239020014447\n",
      "Epoch 193, Loss: 0.4729457158004841\n",
      "Epoch 194, Loss: 0.47274703738102797\n",
      "Epoch 195, Loss: 0.47253750657270627\n",
      "Epoch 196, Loss: 0.4723544299968521\n",
      "Epoch 197, Loss: 0.47221102907746554\n",
      "Epoch 198, Loss: 0.47210844467503205\n",
      "Epoch 199, Loss: 0.47205337980155637\n",
      "Epoch 200, Loss: 0.47204973383515075\n",
      "Epoch 201, Loss: 0.4720427890593303\n",
      "Epoch 202, Loss: 0.4719159657967507\n",
      "Epoch 203, Loss: 0.471735017109987\n",
      "Epoch 204, Loss: 0.4714985533244086\n",
      "Epoch 205, Loss: 0.4713143302002759\n",
      "Epoch 206, Loss: 0.47118574544243713\n",
      "Epoch 207, Loss: 0.47112002449718005\n",
      "Epoch 208, Loss: 0.4710910670518786\n",
      "Epoch 209, Loss: 0.47102495326923743\n",
      "Epoch 210, Loss: 0.4709626586360175\n",
      "Epoch 211, Loss: 0.47085840979895394\n",
      "Epoch 212, Loss: 0.47078064719388485\n",
      "Epoch 213, Loss: 0.47066614789276784\n",
      "Epoch 214, Loss: 0.47055654499036753\n",
      "Epoch 215, Loss: 0.47041652390832933\n",
      "Epoch 216, Loss: 0.4702446473560784\n",
      "Epoch 217, Loss: 0.4701045016774931\n",
      "Epoch 218, Loss: 0.47002059077245106\n",
      "Epoch 219, Loss: 0.4699643617672827\n",
      "Epoch 220, Loss: 0.4698888296176042\n",
      "Epoch 221, Loss: 0.4698141575467262\n",
      "Epoch 222, Loss: 0.4697265783615268\n",
      "Epoch 223, Loss: 0.46966066909468496\n",
      "Epoch 224, Loss: 0.4696348739248034\n",
      "Epoch 225, Loss: 0.4697176006517021\n",
      "Epoch 226, Loss: 0.4699424675720642\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2193824666750285\n",
      "Valence RMSE: 0.20653297367259613\n",
      "Arousal RMSE: 0.23151990012695384\n",
      "Test R^2 score: tensor([0.4831, 0.4693], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.3621, 0.3450], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4761545700992287\n",
      "Num of epochs: 227\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Epoch 121, Loss: 0.49295912913079615\n",
      "Epoch 122, Loss: 0.4924773619139838\n",
      "Epoch 123, Loss: 0.49197607191949805\n",
      "Epoch 124, Loss: 0.4914699197831528\n",
      "Epoch 125, Loss: 0.49091483381312667\n",
      "Epoch 126, Loss: 0.49035147677946317\n",
      "Epoch 127, Loss: 0.4898087222825037\n",
      "Epoch 128, Loss: 0.48924014731550836\n",
      "Epoch 129, Loss: 0.48873550647212044\n",
      "Epoch 130, Loss: 0.48832307254971563\n",
      "Epoch 131, Loss: 0.48791418392821295\n",
      "Epoch 132, Loss: 0.4875902245243796\n",
      "Epoch 133, Loss: 0.48728851108037585\n",
      "Epoch 134, Loss: 0.48697062262181345\n",
      "Epoch 135, Loss: 0.4866339553030548\n",
      "Epoch 136, Loss: 0.48633446746734893\n",
      "Epoch 137, Loss: 0.4860310087358514\n",
      "Epoch 138, Loss: 0.4856786412545615\n",
      "Epoch 139, Loss: 0.48531372107004\n",
      "Epoch 140, Loss: 0.48495712985488587\n",
      "Epoch 141, Loss: 0.4846131600652309\n",
      "Epoch 142, Loss: 0.48425966858652364\n",
      "Epoch 143, Loss: 0.4838914149236435\n",
      "Epoch 144, Loss: 0.48352437546282767\n",
      "Epoch 145, Loss: 0.4832299911937133\n",
      "Epoch 146, Loss: 0.4832586529465723\n",
      "Epoch 147, Loss: 0.48356744143113517\n",
      "Epoch 148, Loss: 0.4828108019011922\n",
      "Epoch 149, Loss: 0.4818912939738211\n",
      "Epoch 150, Loss: 0.48239416745575797\n",
      "Epoch 151, Loss: 0.4814952491721961\n",
      "Epoch 152, Loss: 0.4811822818820502\n",
      "Epoch 153, Loss: 0.4812753156716337\n",
      "Epoch 154, Loss: 0.4804387742282827\n",
      "Epoch 155, Loss: 0.48058189052417905\n",
      "Epoch 156, Loss: 0.4801288115497346\n",
      "Epoch 157, Loss: 0.4797465937246525\n",
      "Epoch 158, Loss: 0.4797622479529733\n",
      "Epoch 159, Loss: 0.4791664679844762\n",
      "Epoch 160, Loss: 0.47904359923301915\n",
      "Epoch 161, Loss: 0.4788483533353555\n",
      "Epoch 162, Loss: 0.4783709093142749\n",
      "Epoch 163, Loss: 0.4783027642591491\n",
      "Epoch 164, Loss: 0.4780003946284736\n",
      "Epoch 165, Loss: 0.47766356610156796\n",
      "Epoch 166, Loss: 0.47756940749162413\n",
      "Epoch 167, Loss: 0.4772720526560437\n",
      "Epoch 168, Loss: 0.477001191512605\n",
      "Epoch 169, Loss: 0.47690651155802954\n",
      "Epoch 170, Loss: 0.47663605231839845\n",
      "Epoch 171, Loss: 0.4763715079887003\n",
      "Epoch 172, Loss: 0.4762479807342815\n",
      "Epoch 173, Loss: 0.47603155482909787\n",
      "Epoch 174, Loss: 0.47580085926956445\n",
      "Epoch 175, Loss: 0.4756396378860013\n",
      "Epoch 176, Loss: 0.4754627232666358\n",
      "Epoch 177, Loss: 0.4752513954316692\n",
      "Epoch 178, Loss: 0.47507140350730925\n",
      "Epoch 179, Loss: 0.47492471273973497\n",
      "Epoch 180, Loss: 0.47475493910732763\n",
      "Epoch 181, Loss: 0.47455157018218247\n",
      "Epoch 182, Loss: 0.4743853224635176\n",
      "Epoch 183, Loss: 0.47424550491599726\n",
      "Epoch 184, Loss: 0.47408413178207975\n",
      "Epoch 185, Loss: 0.47390385374022453\n",
      "Epoch 186, Loss: 0.4737383066259127\n",
      "Epoch 187, Loss: 0.47358797785338436\n",
      "Epoch 188, Loss: 0.4734560449826861\n",
      "Epoch 189, Loss: 0.47335738206045025\n",
      "Epoch 190, Loss: 0.4733162834925985\n",
      "Epoch 191, Loss: 0.4732413020737725\n",
      "Epoch 192, Loss: 0.47313239020014447\n",
      "Epoch 193, Loss: 0.4729457158004841\n",
      "Epoch 194, Loss: 0.47274703738102797\n",
      "Epoch 195, Loss: 0.47253750657270627\n",
      "Epoch 196, Loss: 0.4723544299968521\n",
      "Epoch 197, Loss: 0.47221102907746554\n",
      "Epoch 198, Loss: 0.47210844467503205\n",
      "Epoch 199, Loss: 0.47205337980155637\n",
      "Epoch 200, Loss: 0.47204973383515075\n",
      "Epoch 201, Loss: 0.4720427890593303\n",
      "Epoch 202, Loss: 0.4719159657967507\n",
      "Epoch 203, Loss: 0.471735017109987\n",
      "Epoch 204, Loss: 0.4714985533244086\n",
      "Epoch 205, Loss: 0.4713143302002759\n",
      "Epoch 206, Loss: 0.47118574544243713\n",
      "Epoch 207, Loss: 0.47112002449718005\n",
      "Epoch 208, Loss: 0.4710910670518786\n",
      "Epoch 209, Loss: 0.47102495326923743\n",
      "Epoch 210, Loss: 0.4709626586360175\n",
      "Epoch 211, Loss: 0.47085840979895394\n",
      "Epoch 212, Loss: 0.47078064719388485\n",
      "Epoch 213, Loss: 0.47066614789276784\n",
      "Epoch 214, Loss: 0.47055654499036753\n",
      "Epoch 215, Loss: 0.47041652390832933\n",
      "Epoch 216, Loss: 0.4702446473560784\n",
      "Epoch 217, Loss: 0.4701045016774931\n",
      "Epoch 218, Loss: 0.47002059077245106\n",
      "Epoch 219, Loss: 0.4699643617672827\n",
      "Epoch 220, Loss: 0.4698888296176042\n",
      "Epoch 221, Loss: 0.4698141575467262\n",
      "Epoch 222, Loss: 0.4697265783615268\n",
      "Epoch 223, Loss: 0.46966066909468496\n",
      "Epoch 224, Loss: 0.4696348739248034\n",
      "Epoch 225, Loss: 0.4697176006517021\n",
      "Epoch 226, Loss: 0.4699424675720642\n",
      "Epoch 227, Loss: 0.46999269112017344\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.22144576778209568\n",
      "Valence RMSE: 0.20859927123268762\n",
      "Arousal RMSE: 0.23358681507823872\n",
      "Test R^2 score: tensor([0.4727, 0.4597], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.3492, 0.3333], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4661973921425443\n",
      "Num of epochs: 228\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Epoch 121, Loss: 0.49295912913079615\n",
      "Epoch 122, Loss: 0.4924773619139838\n",
      "Epoch 123, Loss: 0.49197607191949805\n",
      "Epoch 124, Loss: 0.4914699197831528\n",
      "Epoch 125, Loss: 0.49091483381312667\n",
      "Epoch 126, Loss: 0.49035147677946317\n",
      "Epoch 127, Loss: 0.4898087222825037\n",
      "Epoch 128, Loss: 0.48924014731550836\n",
      "Epoch 129, Loss: 0.48873550647212044\n",
      "Epoch 130, Loss: 0.48832307254971563\n",
      "Epoch 131, Loss: 0.48791418392821295\n",
      "Epoch 132, Loss: 0.4875902245243796\n",
      "Epoch 133, Loss: 0.48728851108037585\n",
      "Epoch 134, Loss: 0.48697062262181345\n",
      "Epoch 135, Loss: 0.4866339553030548\n",
      "Epoch 136, Loss: 0.48633446746734893\n",
      "Epoch 137, Loss: 0.4860310087358514\n",
      "Epoch 138, Loss: 0.4856786412545615\n",
      "Epoch 139, Loss: 0.48531372107004\n",
      "Epoch 140, Loss: 0.48495712985488587\n",
      "Epoch 141, Loss: 0.4846131600652309\n",
      "Epoch 142, Loss: 0.48425966858652364\n",
      "Epoch 143, Loss: 0.4838914149236435\n",
      "Epoch 144, Loss: 0.48352437546282767\n",
      "Epoch 145, Loss: 0.4832299911937133\n",
      "Epoch 146, Loss: 0.4832586529465723\n",
      "Epoch 147, Loss: 0.48356744143113517\n",
      "Epoch 148, Loss: 0.4828108019011922\n",
      "Epoch 149, Loss: 0.4818912939738211\n",
      "Epoch 150, Loss: 0.48239416745575797\n",
      "Epoch 151, Loss: 0.4814952491721961\n",
      "Epoch 152, Loss: 0.4811822818820502\n",
      "Epoch 153, Loss: 0.4812753156716337\n",
      "Epoch 154, Loss: 0.4804387742282827\n",
      "Epoch 155, Loss: 0.48058189052417905\n",
      "Epoch 156, Loss: 0.4801288115497346\n",
      "Epoch 157, Loss: 0.4797465937246525\n",
      "Epoch 158, Loss: 0.4797622479529733\n",
      "Epoch 159, Loss: 0.4791664679844762\n",
      "Epoch 160, Loss: 0.47904359923301915\n",
      "Epoch 161, Loss: 0.4788483533353555\n",
      "Epoch 162, Loss: 0.4783709093142749\n",
      "Epoch 163, Loss: 0.4783027642591491\n",
      "Epoch 164, Loss: 0.4780003946284736\n",
      "Epoch 165, Loss: 0.47766356610156796\n",
      "Epoch 166, Loss: 0.47756940749162413\n",
      "Epoch 167, Loss: 0.4772720526560437\n",
      "Epoch 168, Loss: 0.477001191512605\n",
      "Epoch 169, Loss: 0.47690651155802954\n",
      "Epoch 170, Loss: 0.47663605231839845\n",
      "Epoch 171, Loss: 0.4763715079887003\n",
      "Epoch 172, Loss: 0.4762479807342815\n",
      "Epoch 173, Loss: 0.47603155482909787\n",
      "Epoch 174, Loss: 0.47580085926956445\n",
      "Epoch 175, Loss: 0.4756396378860013\n",
      "Epoch 176, Loss: 0.4754627232666358\n",
      "Epoch 177, Loss: 0.4752513954316692\n",
      "Epoch 178, Loss: 0.47507140350730925\n",
      "Epoch 179, Loss: 0.47492471273973497\n",
      "Epoch 180, Loss: 0.47475493910732763\n",
      "Epoch 181, Loss: 0.47455157018218247\n",
      "Epoch 182, Loss: 0.4743853224635176\n",
      "Epoch 183, Loss: 0.47424550491599726\n",
      "Epoch 184, Loss: 0.47408413178207975\n",
      "Epoch 185, Loss: 0.47390385374022453\n",
      "Epoch 186, Loss: 0.4737383066259127\n",
      "Epoch 187, Loss: 0.47358797785338436\n",
      "Epoch 188, Loss: 0.4734560449826861\n",
      "Epoch 189, Loss: 0.47335738206045025\n",
      "Epoch 190, Loss: 0.4733162834925985\n",
      "Epoch 191, Loss: 0.4732413020737725\n",
      "Epoch 192, Loss: 0.47313239020014447\n",
      "Epoch 193, Loss: 0.4729457158004841\n",
      "Epoch 194, Loss: 0.47274703738102797\n",
      "Epoch 195, Loss: 0.47253750657270627\n",
      "Epoch 196, Loss: 0.4723544299968521\n",
      "Epoch 197, Loss: 0.47221102907746554\n",
      "Epoch 198, Loss: 0.47210844467503205\n",
      "Epoch 199, Loss: 0.47205337980155637\n",
      "Epoch 200, Loss: 0.47204973383515075\n",
      "Epoch 201, Loss: 0.4720427890593303\n",
      "Epoch 202, Loss: 0.4719159657967507\n",
      "Epoch 203, Loss: 0.471735017109987\n",
      "Epoch 204, Loss: 0.4714985533244086\n",
      "Epoch 205, Loss: 0.4713143302002759\n",
      "Epoch 206, Loss: 0.47118574544243713\n",
      "Epoch 207, Loss: 0.47112002449718005\n",
      "Epoch 208, Loss: 0.4710910670518786\n",
      "Epoch 209, Loss: 0.47102495326923743\n",
      "Epoch 210, Loss: 0.4709626586360175\n",
      "Epoch 211, Loss: 0.47085840979895394\n",
      "Epoch 212, Loss: 0.47078064719388485\n",
      "Epoch 213, Loss: 0.47066614789276784\n",
      "Epoch 214, Loss: 0.47055654499036753\n",
      "Epoch 215, Loss: 0.47041652390832933\n",
      "Epoch 216, Loss: 0.4702446473560784\n",
      "Epoch 217, Loss: 0.4701045016774931\n",
      "Epoch 218, Loss: 0.47002059077245106\n",
      "Epoch 219, Loss: 0.4699643617672827\n",
      "Epoch 220, Loss: 0.4698888296176042\n",
      "Epoch 221, Loss: 0.4698141575467262\n",
      "Epoch 222, Loss: 0.4697265783615268\n",
      "Epoch 223, Loss: 0.46966066909468496\n",
      "Epoch 224, Loss: 0.4696348739248034\n",
      "Epoch 225, Loss: 0.4697176006517021\n",
      "Epoch 226, Loss: 0.4699424675720642\n",
      "Epoch 227, Loss: 0.46999269112017344\n",
      "Epoch 228, Loss: 0.4698759384724347\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.21940699659790225\n",
      "Valence RMSE: 0.20653749160629425\n",
      "Arousal RMSE: 0.23156235633885686\n",
      "Test R^2 score: tensor([0.4830, 0.4691], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.3620, 0.3448], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.47604592464068135\n",
      "Num of epochs: 229\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Epoch 121, Loss: 0.49295912913079615\n",
      "Epoch 122, Loss: 0.4924773619139838\n",
      "Epoch 123, Loss: 0.49197607191949805\n",
      "Epoch 124, Loss: 0.4914699197831528\n",
      "Epoch 125, Loss: 0.49091483381312667\n",
      "Epoch 126, Loss: 0.49035147677946317\n",
      "Epoch 127, Loss: 0.4898087222825037\n",
      "Epoch 128, Loss: 0.48924014731550836\n",
      "Epoch 129, Loss: 0.48873550647212044\n",
      "Epoch 130, Loss: 0.48832307254971563\n",
      "Epoch 131, Loss: 0.48791418392821295\n",
      "Epoch 132, Loss: 0.4875902245243796\n",
      "Epoch 133, Loss: 0.48728851108037585\n",
      "Epoch 134, Loss: 0.48697062262181345\n",
      "Epoch 135, Loss: 0.4866339553030548\n",
      "Epoch 136, Loss: 0.48633446746734893\n",
      "Epoch 137, Loss: 0.4860310087358514\n",
      "Epoch 138, Loss: 0.4856786412545615\n",
      "Epoch 139, Loss: 0.48531372107004\n",
      "Epoch 140, Loss: 0.48495712985488587\n",
      "Epoch 141, Loss: 0.4846131600652309\n",
      "Epoch 142, Loss: 0.48425966858652364\n",
      "Epoch 143, Loss: 0.4838914149236435\n",
      "Epoch 144, Loss: 0.48352437546282767\n",
      "Epoch 145, Loss: 0.4832299911937133\n",
      "Epoch 146, Loss: 0.4832586529465723\n",
      "Epoch 147, Loss: 0.48356744143113517\n",
      "Epoch 148, Loss: 0.4828108019011922\n",
      "Epoch 149, Loss: 0.4818912939738211\n",
      "Epoch 150, Loss: 0.48239416745575797\n",
      "Epoch 151, Loss: 0.4814952491721961\n",
      "Epoch 152, Loss: 0.4811822818820502\n",
      "Epoch 153, Loss: 0.4812753156716337\n",
      "Epoch 154, Loss: 0.4804387742282827\n",
      "Epoch 155, Loss: 0.48058189052417905\n",
      "Epoch 156, Loss: 0.4801288115497346\n",
      "Epoch 157, Loss: 0.4797465937246525\n",
      "Epoch 158, Loss: 0.4797622479529733\n",
      "Epoch 159, Loss: 0.4791664679844762\n",
      "Epoch 160, Loss: 0.47904359923301915\n",
      "Epoch 161, Loss: 0.4788483533353555\n",
      "Epoch 162, Loss: 0.4783709093142749\n",
      "Epoch 163, Loss: 0.4783027642591491\n",
      "Epoch 164, Loss: 0.4780003946284736\n",
      "Epoch 165, Loss: 0.47766356610156796\n",
      "Epoch 166, Loss: 0.47756940749162413\n",
      "Epoch 167, Loss: 0.4772720526560437\n",
      "Epoch 168, Loss: 0.477001191512605\n",
      "Epoch 169, Loss: 0.47690651155802954\n",
      "Epoch 170, Loss: 0.47663605231839845\n",
      "Epoch 171, Loss: 0.4763715079887003\n",
      "Epoch 172, Loss: 0.4762479807342815\n",
      "Epoch 173, Loss: 0.47603155482909787\n",
      "Epoch 174, Loss: 0.47580085926956445\n",
      "Epoch 175, Loss: 0.4756396378860013\n",
      "Epoch 176, Loss: 0.4754627232666358\n",
      "Epoch 177, Loss: 0.4752513954316692\n",
      "Epoch 178, Loss: 0.47507140350730925\n",
      "Epoch 179, Loss: 0.47492471273973497\n",
      "Epoch 180, Loss: 0.47475493910732763\n",
      "Epoch 181, Loss: 0.47455157018218247\n",
      "Epoch 182, Loss: 0.4743853224635176\n",
      "Epoch 183, Loss: 0.47424550491599726\n",
      "Epoch 184, Loss: 0.47408413178207975\n",
      "Epoch 185, Loss: 0.47390385374022453\n",
      "Epoch 186, Loss: 0.4737383066259127\n",
      "Epoch 187, Loss: 0.47358797785338436\n",
      "Epoch 188, Loss: 0.4734560449826861\n",
      "Epoch 189, Loss: 0.47335738206045025\n",
      "Epoch 190, Loss: 0.4733162834925985\n",
      "Epoch 191, Loss: 0.4732413020737725\n",
      "Epoch 192, Loss: 0.47313239020014447\n",
      "Epoch 193, Loss: 0.4729457158004841\n",
      "Epoch 194, Loss: 0.47274703738102797\n",
      "Epoch 195, Loss: 0.47253750657270627\n",
      "Epoch 196, Loss: 0.4723544299968521\n",
      "Epoch 197, Loss: 0.47221102907746554\n",
      "Epoch 198, Loss: 0.47210844467503205\n",
      "Epoch 199, Loss: 0.47205337980155637\n",
      "Epoch 200, Loss: 0.47204973383515075\n",
      "Epoch 201, Loss: 0.4720427890593303\n",
      "Epoch 202, Loss: 0.4719159657967507\n",
      "Epoch 203, Loss: 0.471735017109987\n",
      "Epoch 204, Loss: 0.4714985533244086\n",
      "Epoch 205, Loss: 0.4713143302002759\n",
      "Epoch 206, Loss: 0.47118574544243713\n",
      "Epoch 207, Loss: 0.47112002449718005\n",
      "Epoch 208, Loss: 0.4710910670518786\n",
      "Epoch 209, Loss: 0.47102495326923743\n",
      "Epoch 210, Loss: 0.4709626586360175\n",
      "Epoch 211, Loss: 0.47085840979895394\n",
      "Epoch 212, Loss: 0.47078064719388485\n",
      "Epoch 213, Loss: 0.47066614789276784\n",
      "Epoch 214, Loss: 0.47055654499036753\n",
      "Epoch 215, Loss: 0.47041652390832933\n",
      "Epoch 216, Loss: 0.4702446473560784\n",
      "Epoch 217, Loss: 0.4701045016774931\n",
      "Epoch 218, Loss: 0.47002059077245106\n",
      "Epoch 219, Loss: 0.4699643617672827\n",
      "Epoch 220, Loss: 0.4698888296176042\n",
      "Epoch 221, Loss: 0.4698141575467262\n",
      "Epoch 222, Loss: 0.4697265783615268\n",
      "Epoch 223, Loss: 0.46966066909468496\n",
      "Epoch 224, Loss: 0.4696348739248034\n",
      "Epoch 225, Loss: 0.4697176006517021\n",
      "Epoch 226, Loss: 0.4699424675720642\n",
      "Epoch 227, Loss: 0.46999269112017344\n",
      "Epoch 228, Loss: 0.4698759384724347\n",
      "Epoch 229, Loss: 0.46938041574678274\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2197151788672948\n",
      "Valence RMSE: 0.20686374008800906\n",
      "Arousal RMSE: 0.23185537019050328\n",
      "Test R^2 score: tensor([0.4814, 0.4677], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.3600, 0.3431], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4745564071660102\n",
      "Num of epochs: 230\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Epoch 121, Loss: 0.49295912913079615\n",
      "Epoch 122, Loss: 0.4924773619139838\n",
      "Epoch 123, Loss: 0.49197607191949805\n",
      "Epoch 124, Loss: 0.4914699197831528\n",
      "Epoch 125, Loss: 0.49091483381312667\n",
      "Epoch 126, Loss: 0.49035147677946317\n",
      "Epoch 127, Loss: 0.4898087222825037\n",
      "Epoch 128, Loss: 0.48924014731550836\n",
      "Epoch 129, Loss: 0.48873550647212044\n",
      "Epoch 130, Loss: 0.48832307254971563\n",
      "Epoch 131, Loss: 0.48791418392821295\n",
      "Epoch 132, Loss: 0.4875902245243796\n",
      "Epoch 133, Loss: 0.48728851108037585\n",
      "Epoch 134, Loss: 0.48697062262181345\n",
      "Epoch 135, Loss: 0.4866339553030548\n",
      "Epoch 136, Loss: 0.48633446746734893\n",
      "Epoch 137, Loss: 0.4860310087358514\n",
      "Epoch 138, Loss: 0.4856786412545615\n",
      "Epoch 139, Loss: 0.48531372107004\n",
      "Epoch 140, Loss: 0.48495712985488587\n",
      "Epoch 141, Loss: 0.4846131600652309\n",
      "Epoch 142, Loss: 0.48425966858652364\n",
      "Epoch 143, Loss: 0.4838914149236435\n",
      "Epoch 144, Loss: 0.48352437546282767\n",
      "Epoch 145, Loss: 0.4832299911937133\n",
      "Epoch 146, Loss: 0.4832586529465723\n",
      "Epoch 147, Loss: 0.48356744143113517\n",
      "Epoch 148, Loss: 0.4828108019011922\n",
      "Epoch 149, Loss: 0.4818912939738211\n",
      "Epoch 150, Loss: 0.48239416745575797\n",
      "Epoch 151, Loss: 0.4814952491721961\n",
      "Epoch 152, Loss: 0.4811822818820502\n",
      "Epoch 153, Loss: 0.4812753156716337\n",
      "Epoch 154, Loss: 0.4804387742282827\n",
      "Epoch 155, Loss: 0.48058189052417905\n",
      "Epoch 156, Loss: 0.4801288115497346\n",
      "Epoch 157, Loss: 0.4797465937246525\n",
      "Epoch 158, Loss: 0.4797622479529733\n",
      "Epoch 159, Loss: 0.4791664679844762\n",
      "Epoch 160, Loss: 0.47904359923301915\n",
      "Epoch 161, Loss: 0.4788483533353555\n",
      "Epoch 162, Loss: 0.4783709093142749\n",
      "Epoch 163, Loss: 0.4783027642591491\n",
      "Epoch 164, Loss: 0.4780003946284736\n",
      "Epoch 165, Loss: 0.47766356610156796\n",
      "Epoch 166, Loss: 0.47756940749162413\n",
      "Epoch 167, Loss: 0.4772720526560437\n",
      "Epoch 168, Loss: 0.477001191512605\n",
      "Epoch 169, Loss: 0.47690651155802954\n",
      "Epoch 170, Loss: 0.47663605231839845\n",
      "Epoch 171, Loss: 0.4763715079887003\n",
      "Epoch 172, Loss: 0.4762479807342815\n",
      "Epoch 173, Loss: 0.47603155482909787\n",
      "Epoch 174, Loss: 0.47580085926956445\n",
      "Epoch 175, Loss: 0.4756396378860013\n",
      "Epoch 176, Loss: 0.4754627232666358\n",
      "Epoch 177, Loss: 0.4752513954316692\n",
      "Epoch 178, Loss: 0.47507140350730925\n",
      "Epoch 179, Loss: 0.47492471273973497\n",
      "Epoch 180, Loss: 0.47475493910732763\n",
      "Epoch 181, Loss: 0.47455157018218247\n",
      "Epoch 182, Loss: 0.4743853224635176\n",
      "Epoch 183, Loss: 0.47424550491599726\n",
      "Epoch 184, Loss: 0.47408413178207975\n",
      "Epoch 185, Loss: 0.47390385374022453\n",
      "Epoch 186, Loss: 0.4737383066259127\n",
      "Epoch 187, Loss: 0.47358797785338436\n",
      "Epoch 188, Loss: 0.4734560449826861\n",
      "Epoch 189, Loss: 0.47335738206045025\n",
      "Epoch 190, Loss: 0.4733162834925985\n",
      "Epoch 191, Loss: 0.4732413020737725\n",
      "Epoch 192, Loss: 0.47313239020014447\n",
      "Epoch 193, Loss: 0.4729457158004841\n",
      "Epoch 194, Loss: 0.47274703738102797\n",
      "Epoch 195, Loss: 0.47253750657270627\n",
      "Epoch 196, Loss: 0.4723544299968521\n",
      "Epoch 197, Loss: 0.47221102907746554\n",
      "Epoch 198, Loss: 0.47210844467503205\n",
      "Epoch 199, Loss: 0.47205337980155637\n",
      "Epoch 200, Loss: 0.47204973383515075\n",
      "Epoch 201, Loss: 0.4720427890593303\n",
      "Epoch 202, Loss: 0.4719159657967507\n",
      "Epoch 203, Loss: 0.471735017109987\n",
      "Epoch 204, Loss: 0.4714985533244086\n",
      "Epoch 205, Loss: 0.4713143302002759\n",
      "Epoch 206, Loss: 0.47118574544243713\n",
      "Epoch 207, Loss: 0.47112002449718005\n",
      "Epoch 208, Loss: 0.4710910670518786\n",
      "Epoch 209, Loss: 0.47102495326923743\n",
      "Epoch 210, Loss: 0.4709626586360175\n",
      "Epoch 211, Loss: 0.47085840979895394\n",
      "Epoch 212, Loss: 0.47078064719388485\n",
      "Epoch 213, Loss: 0.47066614789276784\n",
      "Epoch 214, Loss: 0.47055654499036753\n",
      "Epoch 215, Loss: 0.47041652390832933\n",
      "Epoch 216, Loss: 0.4702446473560784\n",
      "Epoch 217, Loss: 0.4701045016774931\n",
      "Epoch 218, Loss: 0.47002059077245106\n",
      "Epoch 219, Loss: 0.4699643617672827\n",
      "Epoch 220, Loss: 0.4698888296176042\n",
      "Epoch 221, Loss: 0.4698141575467262\n",
      "Epoch 222, Loss: 0.4697265783615268\n",
      "Epoch 223, Loss: 0.46966066909468496\n",
      "Epoch 224, Loss: 0.4696348739248034\n",
      "Epoch 225, Loss: 0.4697176006517021\n",
      "Epoch 226, Loss: 0.4699424675720642\n",
      "Epoch 227, Loss: 0.46999269112017344\n",
      "Epoch 228, Loss: 0.4698759384724347\n",
      "Epoch 229, Loss: 0.46938041574678274\n",
      "Epoch 230, Loss: 0.4691415581864629\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.22037697605785633\n",
      "Valence RMSE: 0.20756531135989562\n",
      "Arousal RMSE: 0.2324836868962628\n",
      "Test R^2 score: tensor([0.4779, 0.4648], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.3557, 0.3396], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.47135018150952057\n",
      "Num of epochs: 231\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Epoch 121, Loss: 0.49295912913079615\n",
      "Epoch 122, Loss: 0.4924773619139838\n",
      "Epoch 123, Loss: 0.49197607191949805\n",
      "Epoch 124, Loss: 0.4914699197831528\n",
      "Epoch 125, Loss: 0.49091483381312667\n",
      "Epoch 126, Loss: 0.49035147677946317\n",
      "Epoch 127, Loss: 0.4898087222825037\n",
      "Epoch 128, Loss: 0.48924014731550836\n",
      "Epoch 129, Loss: 0.48873550647212044\n",
      "Epoch 130, Loss: 0.48832307254971563\n",
      "Epoch 131, Loss: 0.48791418392821295\n",
      "Epoch 132, Loss: 0.4875902245243796\n",
      "Epoch 133, Loss: 0.48728851108037585\n",
      "Epoch 134, Loss: 0.48697062262181345\n",
      "Epoch 135, Loss: 0.4866339553030548\n",
      "Epoch 136, Loss: 0.48633446746734893\n",
      "Epoch 137, Loss: 0.4860310087358514\n",
      "Epoch 138, Loss: 0.4856786412545615\n",
      "Epoch 139, Loss: 0.48531372107004\n",
      "Epoch 140, Loss: 0.48495712985488587\n",
      "Epoch 141, Loss: 0.4846131600652309\n",
      "Epoch 142, Loss: 0.48425966858652364\n",
      "Epoch 143, Loss: 0.4838914149236435\n",
      "Epoch 144, Loss: 0.48352437546282767\n",
      "Epoch 145, Loss: 0.4832299911937133\n",
      "Epoch 146, Loss: 0.4832586529465723\n",
      "Epoch 147, Loss: 0.48356744143113517\n",
      "Epoch 148, Loss: 0.4828108019011922\n",
      "Epoch 149, Loss: 0.4818912939738211\n",
      "Epoch 150, Loss: 0.48239416745575797\n",
      "Epoch 151, Loss: 0.4814952491721961\n",
      "Epoch 152, Loss: 0.4811822818820502\n",
      "Epoch 153, Loss: 0.4812753156716337\n",
      "Epoch 154, Loss: 0.4804387742282827\n",
      "Epoch 155, Loss: 0.48058189052417905\n",
      "Epoch 156, Loss: 0.4801288115497346\n",
      "Epoch 157, Loss: 0.4797465937246525\n",
      "Epoch 158, Loss: 0.4797622479529733\n",
      "Epoch 159, Loss: 0.4791664679844762\n",
      "Epoch 160, Loss: 0.47904359923301915\n",
      "Epoch 161, Loss: 0.4788483533353555\n",
      "Epoch 162, Loss: 0.4783709093142749\n",
      "Epoch 163, Loss: 0.4783027642591491\n",
      "Epoch 164, Loss: 0.4780003946284736\n",
      "Epoch 165, Loss: 0.47766356610156796\n",
      "Epoch 166, Loss: 0.47756940749162413\n",
      "Epoch 167, Loss: 0.4772720526560437\n",
      "Epoch 168, Loss: 0.477001191512605\n",
      "Epoch 169, Loss: 0.47690651155802954\n",
      "Epoch 170, Loss: 0.47663605231839845\n",
      "Epoch 171, Loss: 0.4763715079887003\n",
      "Epoch 172, Loss: 0.4762479807342815\n",
      "Epoch 173, Loss: 0.47603155482909787\n",
      "Epoch 174, Loss: 0.47580085926956445\n",
      "Epoch 175, Loss: 0.4756396378860013\n",
      "Epoch 176, Loss: 0.4754627232666358\n",
      "Epoch 177, Loss: 0.4752513954316692\n",
      "Epoch 178, Loss: 0.47507140350730925\n",
      "Epoch 179, Loss: 0.47492471273973497\n",
      "Epoch 180, Loss: 0.47475493910732763\n",
      "Epoch 181, Loss: 0.47455157018218247\n",
      "Epoch 182, Loss: 0.4743853224635176\n",
      "Epoch 183, Loss: 0.47424550491599726\n",
      "Epoch 184, Loss: 0.47408413178207975\n",
      "Epoch 185, Loss: 0.47390385374022453\n",
      "Epoch 186, Loss: 0.4737383066259127\n",
      "Epoch 187, Loss: 0.47358797785338436\n",
      "Epoch 188, Loss: 0.4734560449826861\n",
      "Epoch 189, Loss: 0.47335738206045025\n",
      "Epoch 190, Loss: 0.4733162834925985\n",
      "Epoch 191, Loss: 0.4732413020737725\n",
      "Epoch 192, Loss: 0.47313239020014447\n",
      "Epoch 193, Loss: 0.4729457158004841\n",
      "Epoch 194, Loss: 0.47274703738102797\n",
      "Epoch 195, Loss: 0.47253750657270627\n",
      "Epoch 196, Loss: 0.4723544299968521\n",
      "Epoch 197, Loss: 0.47221102907746554\n",
      "Epoch 198, Loss: 0.47210844467503205\n",
      "Epoch 199, Loss: 0.47205337980155637\n",
      "Epoch 200, Loss: 0.47204973383515075\n",
      "Epoch 201, Loss: 0.4720427890593303\n",
      "Epoch 202, Loss: 0.4719159657967507\n",
      "Epoch 203, Loss: 0.471735017109987\n",
      "Epoch 204, Loss: 0.4714985533244086\n",
      "Epoch 205, Loss: 0.4713143302002759\n",
      "Epoch 206, Loss: 0.47118574544243713\n",
      "Epoch 207, Loss: 0.47112002449718005\n",
      "Epoch 208, Loss: 0.4710910670518786\n",
      "Epoch 209, Loss: 0.47102495326923743\n",
      "Epoch 210, Loss: 0.4709626586360175\n",
      "Epoch 211, Loss: 0.47085840979895394\n",
      "Epoch 212, Loss: 0.47078064719388485\n",
      "Epoch 213, Loss: 0.47066614789276784\n",
      "Epoch 214, Loss: 0.47055654499036753\n",
      "Epoch 215, Loss: 0.47041652390832933\n",
      "Epoch 216, Loss: 0.4702446473560784\n",
      "Epoch 217, Loss: 0.4701045016774931\n",
      "Epoch 218, Loss: 0.47002059077245106\n",
      "Epoch 219, Loss: 0.4699643617672827\n",
      "Epoch 220, Loss: 0.4698888296176042\n",
      "Epoch 221, Loss: 0.4698141575467262\n",
      "Epoch 222, Loss: 0.4697265783615268\n",
      "Epoch 223, Loss: 0.46966066909468496\n",
      "Epoch 224, Loss: 0.4696348739248034\n",
      "Epoch 225, Loss: 0.4697176006517021\n",
      "Epoch 226, Loss: 0.4699424675720642\n",
      "Epoch 227, Loss: 0.46999269112017344\n",
      "Epoch 228, Loss: 0.4698759384724347\n",
      "Epoch 229, Loss: 0.46938041574678274\n",
      "Epoch 230, Loss: 0.4691415581864629\n",
      "Epoch 231, Loss: 0.46924839544758656\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.21921458804309849\n",
      "Valence RMSE: 0.20635475721890337\n",
      "Arousal RMSE: 0.23136072569677463\n",
      "Test R^2 score: tensor([0.4839, 0.4700], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.3632, 0.3459], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.47696522024225324\n",
      "Num of epochs: 232\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Epoch 121, Loss: 0.49295912913079615\n",
      "Epoch 122, Loss: 0.4924773619139838\n",
      "Epoch 123, Loss: 0.49197607191949805\n",
      "Epoch 124, Loss: 0.4914699197831528\n",
      "Epoch 125, Loss: 0.49091483381312667\n",
      "Epoch 126, Loss: 0.49035147677946317\n",
      "Epoch 127, Loss: 0.4898087222825037\n",
      "Epoch 128, Loss: 0.48924014731550836\n",
      "Epoch 129, Loss: 0.48873550647212044\n",
      "Epoch 130, Loss: 0.48832307254971563\n",
      "Epoch 131, Loss: 0.48791418392821295\n",
      "Epoch 132, Loss: 0.4875902245243796\n",
      "Epoch 133, Loss: 0.48728851108037585\n",
      "Epoch 134, Loss: 0.48697062262181345\n",
      "Epoch 135, Loss: 0.4866339553030548\n",
      "Epoch 136, Loss: 0.48633446746734893\n",
      "Epoch 137, Loss: 0.4860310087358514\n",
      "Epoch 138, Loss: 0.4856786412545615\n",
      "Epoch 139, Loss: 0.48531372107004\n",
      "Epoch 140, Loss: 0.48495712985488587\n",
      "Epoch 141, Loss: 0.4846131600652309\n",
      "Epoch 142, Loss: 0.48425966858652364\n",
      "Epoch 143, Loss: 0.4838914149236435\n",
      "Epoch 144, Loss: 0.48352437546282767\n",
      "Epoch 145, Loss: 0.4832299911937133\n",
      "Epoch 146, Loss: 0.4832586529465723\n",
      "Epoch 147, Loss: 0.48356744143113517\n",
      "Epoch 148, Loss: 0.4828108019011922\n",
      "Epoch 149, Loss: 0.4818912939738211\n",
      "Epoch 150, Loss: 0.48239416745575797\n",
      "Epoch 151, Loss: 0.4814952491721961\n",
      "Epoch 152, Loss: 0.4811822818820502\n",
      "Epoch 153, Loss: 0.4812753156716337\n",
      "Epoch 154, Loss: 0.4804387742282827\n",
      "Epoch 155, Loss: 0.48058189052417905\n",
      "Epoch 156, Loss: 0.4801288115497346\n",
      "Epoch 157, Loss: 0.4797465937246525\n",
      "Epoch 158, Loss: 0.4797622479529733\n",
      "Epoch 159, Loss: 0.4791664679844762\n",
      "Epoch 160, Loss: 0.47904359923301915\n",
      "Epoch 161, Loss: 0.4788483533353555\n",
      "Epoch 162, Loss: 0.4783709093142749\n",
      "Epoch 163, Loss: 0.4783027642591491\n",
      "Epoch 164, Loss: 0.4780003946284736\n",
      "Epoch 165, Loss: 0.47766356610156796\n",
      "Epoch 166, Loss: 0.47756940749162413\n",
      "Epoch 167, Loss: 0.4772720526560437\n",
      "Epoch 168, Loss: 0.477001191512605\n",
      "Epoch 169, Loss: 0.47690651155802954\n",
      "Epoch 170, Loss: 0.47663605231839845\n",
      "Epoch 171, Loss: 0.4763715079887003\n",
      "Epoch 172, Loss: 0.4762479807342815\n",
      "Epoch 173, Loss: 0.47603155482909787\n",
      "Epoch 174, Loss: 0.47580085926956445\n",
      "Epoch 175, Loss: 0.4756396378860013\n",
      "Epoch 176, Loss: 0.4754627232666358\n",
      "Epoch 177, Loss: 0.4752513954316692\n",
      "Epoch 178, Loss: 0.47507140350730925\n",
      "Epoch 179, Loss: 0.47492471273973497\n",
      "Epoch 180, Loss: 0.47475493910732763\n",
      "Epoch 181, Loss: 0.47455157018218247\n",
      "Epoch 182, Loss: 0.4743853224635176\n",
      "Epoch 183, Loss: 0.47424550491599726\n",
      "Epoch 184, Loss: 0.47408413178207975\n",
      "Epoch 185, Loss: 0.47390385374022453\n",
      "Epoch 186, Loss: 0.4737383066259127\n",
      "Epoch 187, Loss: 0.47358797785338436\n",
      "Epoch 188, Loss: 0.4734560449826861\n",
      "Epoch 189, Loss: 0.47335738206045025\n",
      "Epoch 190, Loss: 0.4733162834925985\n",
      "Epoch 191, Loss: 0.4732413020737725\n",
      "Epoch 192, Loss: 0.47313239020014447\n",
      "Epoch 193, Loss: 0.4729457158004841\n",
      "Epoch 194, Loss: 0.47274703738102797\n",
      "Epoch 195, Loss: 0.47253750657270627\n",
      "Epoch 196, Loss: 0.4723544299968521\n",
      "Epoch 197, Loss: 0.47221102907746554\n",
      "Epoch 198, Loss: 0.47210844467503205\n",
      "Epoch 199, Loss: 0.47205337980155637\n",
      "Epoch 200, Loss: 0.47204973383515075\n",
      "Epoch 201, Loss: 0.4720427890593303\n",
      "Epoch 202, Loss: 0.4719159657967507\n",
      "Epoch 203, Loss: 0.471735017109987\n",
      "Epoch 204, Loss: 0.4714985533244086\n",
      "Epoch 205, Loss: 0.4713143302002759\n",
      "Epoch 206, Loss: 0.47118574544243713\n",
      "Epoch 207, Loss: 0.47112002449718005\n",
      "Epoch 208, Loss: 0.4710910670518786\n",
      "Epoch 209, Loss: 0.47102495326923743\n",
      "Epoch 210, Loss: 0.4709626586360175\n",
      "Epoch 211, Loss: 0.47085840979895394\n",
      "Epoch 212, Loss: 0.47078064719388485\n",
      "Epoch 213, Loss: 0.47066614789276784\n",
      "Epoch 214, Loss: 0.47055654499036753\n",
      "Epoch 215, Loss: 0.47041652390832933\n",
      "Epoch 216, Loss: 0.4702446473560784\n",
      "Epoch 217, Loss: 0.4701045016774931\n",
      "Epoch 218, Loss: 0.47002059077245106\n",
      "Epoch 219, Loss: 0.4699643617672827\n",
      "Epoch 220, Loss: 0.4698888296176042\n",
      "Epoch 221, Loss: 0.4698141575467262\n",
      "Epoch 222, Loss: 0.4697265783615268\n",
      "Epoch 223, Loss: 0.46966066909468496\n",
      "Epoch 224, Loss: 0.4696348739248034\n",
      "Epoch 225, Loss: 0.4697176006517021\n",
      "Epoch 226, Loss: 0.4699424675720642\n",
      "Epoch 227, Loss: 0.46999269112017344\n",
      "Epoch 228, Loss: 0.4698759384724347\n",
      "Epoch 229, Loss: 0.46938041574678274\n",
      "Epoch 230, Loss: 0.4691415581864629\n",
      "Epoch 231, Loss: 0.46924839544758656\n",
      "Epoch 232, Loss: 0.46933531775018905\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.22052911783487586\n",
      "Valence RMSE: 0.20769088820525475\n",
      "Arousal RMSE: 0.23266000640971693\n",
      "Test R^2 score: tensor([0.4772, 0.4640], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.3549, 0.3386], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4706281617629755\n",
      "Num of epochs: 233\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Epoch 121, Loss: 0.49295912913079615\n",
      "Epoch 122, Loss: 0.4924773619139838\n",
      "Epoch 123, Loss: 0.49197607191949805\n",
      "Epoch 124, Loss: 0.4914699197831528\n",
      "Epoch 125, Loss: 0.49091483381312667\n",
      "Epoch 126, Loss: 0.49035147677946317\n",
      "Epoch 127, Loss: 0.4898087222825037\n",
      "Epoch 128, Loss: 0.48924014731550836\n",
      "Epoch 129, Loss: 0.48873550647212044\n",
      "Epoch 130, Loss: 0.48832307254971563\n",
      "Epoch 131, Loss: 0.48791418392821295\n",
      "Epoch 132, Loss: 0.4875902245243796\n",
      "Epoch 133, Loss: 0.48728851108037585\n",
      "Epoch 134, Loss: 0.48697062262181345\n",
      "Epoch 135, Loss: 0.4866339553030548\n",
      "Epoch 136, Loss: 0.48633446746734893\n",
      "Epoch 137, Loss: 0.4860310087358514\n",
      "Epoch 138, Loss: 0.4856786412545615\n",
      "Epoch 139, Loss: 0.48531372107004\n",
      "Epoch 140, Loss: 0.48495712985488587\n",
      "Epoch 141, Loss: 0.4846131600652309\n",
      "Epoch 142, Loss: 0.48425966858652364\n",
      "Epoch 143, Loss: 0.4838914149236435\n",
      "Epoch 144, Loss: 0.48352437546282767\n",
      "Epoch 145, Loss: 0.4832299911937133\n",
      "Epoch 146, Loss: 0.4832586529465723\n",
      "Epoch 147, Loss: 0.48356744143113517\n",
      "Epoch 148, Loss: 0.4828108019011922\n",
      "Epoch 149, Loss: 0.4818912939738211\n",
      "Epoch 150, Loss: 0.48239416745575797\n",
      "Epoch 151, Loss: 0.4814952491721961\n",
      "Epoch 152, Loss: 0.4811822818820502\n",
      "Epoch 153, Loss: 0.4812753156716337\n",
      "Epoch 154, Loss: 0.4804387742282827\n",
      "Epoch 155, Loss: 0.48058189052417905\n",
      "Epoch 156, Loss: 0.4801288115497346\n",
      "Epoch 157, Loss: 0.4797465937246525\n",
      "Epoch 158, Loss: 0.4797622479529733\n",
      "Epoch 159, Loss: 0.4791664679844762\n",
      "Epoch 160, Loss: 0.47904359923301915\n",
      "Epoch 161, Loss: 0.4788483533353555\n",
      "Epoch 162, Loss: 0.4783709093142749\n",
      "Epoch 163, Loss: 0.4783027642591491\n",
      "Epoch 164, Loss: 0.4780003946284736\n",
      "Epoch 165, Loss: 0.47766356610156796\n",
      "Epoch 166, Loss: 0.47756940749162413\n",
      "Epoch 167, Loss: 0.4772720526560437\n",
      "Epoch 168, Loss: 0.477001191512605\n",
      "Epoch 169, Loss: 0.47690651155802954\n",
      "Epoch 170, Loss: 0.47663605231839845\n",
      "Epoch 171, Loss: 0.4763715079887003\n",
      "Epoch 172, Loss: 0.4762479807342815\n",
      "Epoch 173, Loss: 0.47603155482909787\n",
      "Epoch 174, Loss: 0.47580085926956445\n",
      "Epoch 175, Loss: 0.4756396378860013\n",
      "Epoch 176, Loss: 0.4754627232666358\n",
      "Epoch 177, Loss: 0.4752513954316692\n",
      "Epoch 178, Loss: 0.47507140350730925\n",
      "Epoch 179, Loss: 0.47492471273973497\n",
      "Epoch 180, Loss: 0.47475493910732763\n",
      "Epoch 181, Loss: 0.47455157018218247\n",
      "Epoch 182, Loss: 0.4743853224635176\n",
      "Epoch 183, Loss: 0.47424550491599726\n",
      "Epoch 184, Loss: 0.47408413178207975\n",
      "Epoch 185, Loss: 0.47390385374022453\n",
      "Epoch 186, Loss: 0.4737383066259127\n",
      "Epoch 187, Loss: 0.47358797785338436\n",
      "Epoch 188, Loss: 0.4734560449826861\n",
      "Epoch 189, Loss: 0.47335738206045025\n",
      "Epoch 190, Loss: 0.4733162834925985\n",
      "Epoch 191, Loss: 0.4732413020737725\n",
      "Epoch 192, Loss: 0.47313239020014447\n",
      "Epoch 193, Loss: 0.4729457158004841\n",
      "Epoch 194, Loss: 0.47274703738102797\n",
      "Epoch 195, Loss: 0.47253750657270627\n",
      "Epoch 196, Loss: 0.4723544299968521\n",
      "Epoch 197, Loss: 0.47221102907746554\n",
      "Epoch 198, Loss: 0.47210844467503205\n",
      "Epoch 199, Loss: 0.47205337980155637\n",
      "Epoch 200, Loss: 0.47204973383515075\n",
      "Epoch 201, Loss: 0.4720427890593303\n",
      "Epoch 202, Loss: 0.4719159657967507\n",
      "Epoch 203, Loss: 0.471735017109987\n",
      "Epoch 204, Loss: 0.4714985533244086\n",
      "Epoch 205, Loss: 0.4713143302002759\n",
      "Epoch 206, Loss: 0.47118574544243713\n",
      "Epoch 207, Loss: 0.47112002449718005\n",
      "Epoch 208, Loss: 0.4710910670518786\n",
      "Epoch 209, Loss: 0.47102495326923743\n",
      "Epoch 210, Loss: 0.4709626586360175\n",
      "Epoch 211, Loss: 0.47085840979895394\n",
      "Epoch 212, Loss: 0.47078064719388485\n",
      "Epoch 213, Loss: 0.47066614789276784\n",
      "Epoch 214, Loss: 0.47055654499036753\n",
      "Epoch 215, Loss: 0.47041652390832933\n",
      "Epoch 216, Loss: 0.4702446473560784\n",
      "Epoch 217, Loss: 0.4701045016774931\n",
      "Epoch 218, Loss: 0.47002059077245106\n",
      "Epoch 219, Loss: 0.4699643617672827\n",
      "Epoch 220, Loss: 0.4698888296176042\n",
      "Epoch 221, Loss: 0.4698141575467262\n",
      "Epoch 222, Loss: 0.4697265783615268\n",
      "Epoch 223, Loss: 0.46966066909468496\n",
      "Epoch 224, Loss: 0.4696348739248034\n",
      "Epoch 225, Loss: 0.4697176006517021\n",
      "Epoch 226, Loss: 0.4699424675720642\n",
      "Epoch 227, Loss: 0.46999269112017344\n",
      "Epoch 228, Loss: 0.4698759384724347\n",
      "Epoch 229, Loss: 0.46938041574678274\n",
      "Epoch 230, Loss: 0.4691415581864629\n",
      "Epoch 231, Loss: 0.46924839544758656\n",
      "Epoch 232, Loss: 0.46933531775018905\n",
      "Epoch 233, Loss: 0.4692253881106229\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.21933508888018952\n",
      "Valence RMSE: 0.206451705594403\n",
      "Arousal RMSE: 0.23150260405732634\n",
      "Test R^2 score: tensor([0.4835, 0.4693], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.3626, 0.3451], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.47639759047811725\n",
      "Num of epochs: 234\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Epoch 121, Loss: 0.49295912913079615\n",
      "Epoch 122, Loss: 0.4924773619139838\n",
      "Epoch 123, Loss: 0.49197607191949805\n",
      "Epoch 124, Loss: 0.4914699197831528\n",
      "Epoch 125, Loss: 0.49091483381312667\n",
      "Epoch 126, Loss: 0.49035147677946317\n",
      "Epoch 127, Loss: 0.4898087222825037\n",
      "Epoch 128, Loss: 0.48924014731550836\n",
      "Epoch 129, Loss: 0.48873550647212044\n",
      "Epoch 130, Loss: 0.48832307254971563\n",
      "Epoch 131, Loss: 0.48791418392821295\n",
      "Epoch 132, Loss: 0.4875902245243796\n",
      "Epoch 133, Loss: 0.48728851108037585\n",
      "Epoch 134, Loss: 0.48697062262181345\n",
      "Epoch 135, Loss: 0.4866339553030548\n",
      "Epoch 136, Loss: 0.48633446746734893\n",
      "Epoch 137, Loss: 0.4860310087358514\n",
      "Epoch 138, Loss: 0.4856786412545615\n",
      "Epoch 139, Loss: 0.48531372107004\n",
      "Epoch 140, Loss: 0.48495712985488587\n",
      "Epoch 141, Loss: 0.4846131600652309\n",
      "Epoch 142, Loss: 0.48425966858652364\n",
      "Epoch 143, Loss: 0.4838914149236435\n",
      "Epoch 144, Loss: 0.48352437546282767\n",
      "Epoch 145, Loss: 0.4832299911937133\n",
      "Epoch 146, Loss: 0.4832586529465723\n",
      "Epoch 147, Loss: 0.48356744143113517\n",
      "Epoch 148, Loss: 0.4828108019011922\n",
      "Epoch 149, Loss: 0.4818912939738211\n",
      "Epoch 150, Loss: 0.48239416745575797\n",
      "Epoch 151, Loss: 0.4814952491721961\n",
      "Epoch 152, Loss: 0.4811822818820502\n",
      "Epoch 153, Loss: 0.4812753156716337\n",
      "Epoch 154, Loss: 0.4804387742282827\n",
      "Epoch 155, Loss: 0.48058189052417905\n",
      "Epoch 156, Loss: 0.4801288115497346\n",
      "Epoch 157, Loss: 0.4797465937246525\n",
      "Epoch 158, Loss: 0.4797622479529733\n",
      "Epoch 159, Loss: 0.4791664679844762\n",
      "Epoch 160, Loss: 0.47904359923301915\n",
      "Epoch 161, Loss: 0.4788483533353555\n",
      "Epoch 162, Loss: 0.4783709093142749\n",
      "Epoch 163, Loss: 0.4783027642591491\n",
      "Epoch 164, Loss: 0.4780003946284736\n",
      "Epoch 165, Loss: 0.47766356610156796\n",
      "Epoch 166, Loss: 0.47756940749162413\n",
      "Epoch 167, Loss: 0.4772720526560437\n",
      "Epoch 168, Loss: 0.477001191512605\n",
      "Epoch 169, Loss: 0.47690651155802954\n",
      "Epoch 170, Loss: 0.47663605231839845\n",
      "Epoch 171, Loss: 0.4763715079887003\n",
      "Epoch 172, Loss: 0.4762479807342815\n",
      "Epoch 173, Loss: 0.47603155482909787\n",
      "Epoch 174, Loss: 0.47580085926956445\n",
      "Epoch 175, Loss: 0.4756396378860013\n",
      "Epoch 176, Loss: 0.4754627232666358\n",
      "Epoch 177, Loss: 0.4752513954316692\n",
      "Epoch 178, Loss: 0.47507140350730925\n",
      "Epoch 179, Loss: 0.47492471273973497\n",
      "Epoch 180, Loss: 0.47475493910732763\n",
      "Epoch 181, Loss: 0.47455157018218247\n",
      "Epoch 182, Loss: 0.4743853224635176\n",
      "Epoch 183, Loss: 0.47424550491599726\n",
      "Epoch 184, Loss: 0.47408413178207975\n",
      "Epoch 185, Loss: 0.47390385374022453\n",
      "Epoch 186, Loss: 0.4737383066259127\n",
      "Epoch 187, Loss: 0.47358797785338436\n",
      "Epoch 188, Loss: 0.4734560449826861\n",
      "Epoch 189, Loss: 0.47335738206045025\n",
      "Epoch 190, Loss: 0.4733162834925985\n",
      "Epoch 191, Loss: 0.4732413020737725\n",
      "Epoch 192, Loss: 0.47313239020014447\n",
      "Epoch 193, Loss: 0.4729457158004841\n",
      "Epoch 194, Loss: 0.47274703738102797\n",
      "Epoch 195, Loss: 0.47253750657270627\n",
      "Epoch 196, Loss: 0.4723544299968521\n",
      "Epoch 197, Loss: 0.47221102907746554\n",
      "Epoch 198, Loss: 0.47210844467503205\n",
      "Epoch 199, Loss: 0.47205337980155637\n",
      "Epoch 200, Loss: 0.47204973383515075\n",
      "Epoch 201, Loss: 0.4720427890593303\n",
      "Epoch 202, Loss: 0.4719159657967507\n",
      "Epoch 203, Loss: 0.471735017109987\n",
      "Epoch 204, Loss: 0.4714985533244086\n",
      "Epoch 205, Loss: 0.4713143302002759\n",
      "Epoch 206, Loss: 0.47118574544243713\n",
      "Epoch 207, Loss: 0.47112002449718005\n",
      "Epoch 208, Loss: 0.4710910670518786\n",
      "Epoch 209, Loss: 0.47102495326923743\n",
      "Epoch 210, Loss: 0.4709626586360175\n",
      "Epoch 211, Loss: 0.47085840979895394\n",
      "Epoch 212, Loss: 0.47078064719388485\n",
      "Epoch 213, Loss: 0.47066614789276784\n",
      "Epoch 214, Loss: 0.47055654499036753\n",
      "Epoch 215, Loss: 0.47041652390832933\n",
      "Epoch 216, Loss: 0.4702446473560784\n",
      "Epoch 217, Loss: 0.4701045016774931\n",
      "Epoch 218, Loss: 0.47002059077245106\n",
      "Epoch 219, Loss: 0.4699643617672827\n",
      "Epoch 220, Loss: 0.4698888296176042\n",
      "Epoch 221, Loss: 0.4698141575467262\n",
      "Epoch 222, Loss: 0.4697265783615268\n",
      "Epoch 223, Loss: 0.46966066909468496\n",
      "Epoch 224, Loss: 0.4696348739248034\n",
      "Epoch 225, Loss: 0.4697176006517021\n",
      "Epoch 226, Loss: 0.4699424675720642\n",
      "Epoch 227, Loss: 0.46999269112017344\n",
      "Epoch 228, Loss: 0.4698759384724347\n",
      "Epoch 229, Loss: 0.46938041574678274\n",
      "Epoch 230, Loss: 0.4691415581864629\n",
      "Epoch 231, Loss: 0.46924839544758656\n",
      "Epoch 232, Loss: 0.46933531775018905\n",
      "Epoch 233, Loss: 0.4692253881106229\n",
      "Epoch 234, Loss: 0.46894320638835874\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.21935284092897173\n",
      "Valence RMSE: 0.20645807941252522\n",
      "Arousal RMSE: 0.23153055757828808\n",
      "Test R^2 score: tensor([0.4834, 0.4692], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.3625, 0.3450], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.47631756228068106\n",
      "Num of epochs: 235\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Epoch 121, Loss: 0.49295912913079615\n",
      "Epoch 122, Loss: 0.4924773619139838\n",
      "Epoch 123, Loss: 0.49197607191949805\n",
      "Epoch 124, Loss: 0.4914699197831528\n",
      "Epoch 125, Loss: 0.49091483381312667\n",
      "Epoch 126, Loss: 0.49035147677946317\n",
      "Epoch 127, Loss: 0.4898087222825037\n",
      "Epoch 128, Loss: 0.48924014731550836\n",
      "Epoch 129, Loss: 0.48873550647212044\n",
      "Epoch 130, Loss: 0.48832307254971563\n",
      "Epoch 131, Loss: 0.48791418392821295\n",
      "Epoch 132, Loss: 0.4875902245243796\n",
      "Epoch 133, Loss: 0.48728851108037585\n",
      "Epoch 134, Loss: 0.48697062262181345\n",
      "Epoch 135, Loss: 0.4866339553030548\n",
      "Epoch 136, Loss: 0.48633446746734893\n",
      "Epoch 137, Loss: 0.4860310087358514\n",
      "Epoch 138, Loss: 0.4856786412545615\n",
      "Epoch 139, Loss: 0.48531372107004\n",
      "Epoch 140, Loss: 0.48495712985488587\n",
      "Epoch 141, Loss: 0.4846131600652309\n",
      "Epoch 142, Loss: 0.48425966858652364\n",
      "Epoch 143, Loss: 0.4838914149236435\n",
      "Epoch 144, Loss: 0.48352437546282767\n",
      "Epoch 145, Loss: 0.4832299911937133\n",
      "Epoch 146, Loss: 0.4832586529465723\n",
      "Epoch 147, Loss: 0.48356744143113517\n",
      "Epoch 148, Loss: 0.4828108019011922\n",
      "Epoch 149, Loss: 0.4818912939738211\n",
      "Epoch 150, Loss: 0.48239416745575797\n",
      "Epoch 151, Loss: 0.4814952491721961\n",
      "Epoch 152, Loss: 0.4811822818820502\n",
      "Epoch 153, Loss: 0.4812753156716337\n",
      "Epoch 154, Loss: 0.4804387742282827\n",
      "Epoch 155, Loss: 0.48058189052417905\n",
      "Epoch 156, Loss: 0.4801288115497346\n",
      "Epoch 157, Loss: 0.4797465937246525\n",
      "Epoch 158, Loss: 0.4797622479529733\n",
      "Epoch 159, Loss: 0.4791664679844762\n",
      "Epoch 160, Loss: 0.47904359923301915\n",
      "Epoch 161, Loss: 0.4788483533353555\n",
      "Epoch 162, Loss: 0.4783709093142749\n",
      "Epoch 163, Loss: 0.4783027642591491\n",
      "Epoch 164, Loss: 0.4780003946284736\n",
      "Epoch 165, Loss: 0.47766356610156796\n",
      "Epoch 166, Loss: 0.47756940749162413\n",
      "Epoch 167, Loss: 0.4772720526560437\n",
      "Epoch 168, Loss: 0.477001191512605\n",
      "Epoch 169, Loss: 0.47690651155802954\n",
      "Epoch 170, Loss: 0.47663605231839845\n",
      "Epoch 171, Loss: 0.4763715079887003\n",
      "Epoch 172, Loss: 0.4762479807342815\n",
      "Epoch 173, Loss: 0.47603155482909787\n",
      "Epoch 174, Loss: 0.47580085926956445\n",
      "Epoch 175, Loss: 0.4756396378860013\n",
      "Epoch 176, Loss: 0.4754627232666358\n",
      "Epoch 177, Loss: 0.4752513954316692\n",
      "Epoch 178, Loss: 0.47507140350730925\n",
      "Epoch 179, Loss: 0.47492471273973497\n",
      "Epoch 180, Loss: 0.47475493910732763\n",
      "Epoch 181, Loss: 0.47455157018218247\n",
      "Epoch 182, Loss: 0.4743853224635176\n",
      "Epoch 183, Loss: 0.47424550491599726\n",
      "Epoch 184, Loss: 0.47408413178207975\n",
      "Epoch 185, Loss: 0.47390385374022453\n",
      "Epoch 186, Loss: 0.4737383066259127\n",
      "Epoch 187, Loss: 0.47358797785338436\n",
      "Epoch 188, Loss: 0.4734560449826861\n",
      "Epoch 189, Loss: 0.47335738206045025\n",
      "Epoch 190, Loss: 0.4733162834925985\n",
      "Epoch 191, Loss: 0.4732413020737725\n",
      "Epoch 192, Loss: 0.47313239020014447\n",
      "Epoch 193, Loss: 0.4729457158004841\n",
      "Epoch 194, Loss: 0.47274703738102797\n",
      "Epoch 195, Loss: 0.47253750657270627\n",
      "Epoch 196, Loss: 0.4723544299968521\n",
      "Epoch 197, Loss: 0.47221102907746554\n",
      "Epoch 198, Loss: 0.47210844467503205\n",
      "Epoch 199, Loss: 0.47205337980155637\n",
      "Epoch 200, Loss: 0.47204973383515075\n",
      "Epoch 201, Loss: 0.4720427890593303\n",
      "Epoch 202, Loss: 0.4719159657967507\n",
      "Epoch 203, Loss: 0.471735017109987\n",
      "Epoch 204, Loss: 0.4714985533244086\n",
      "Epoch 205, Loss: 0.4713143302002759\n",
      "Epoch 206, Loss: 0.47118574544243713\n",
      "Epoch 207, Loss: 0.47112002449718005\n",
      "Epoch 208, Loss: 0.4710910670518786\n",
      "Epoch 209, Loss: 0.47102495326923743\n",
      "Epoch 210, Loss: 0.4709626586360175\n",
      "Epoch 211, Loss: 0.47085840979895394\n",
      "Epoch 212, Loss: 0.47078064719388485\n",
      "Epoch 213, Loss: 0.47066614789276784\n",
      "Epoch 214, Loss: 0.47055654499036753\n",
      "Epoch 215, Loss: 0.47041652390832933\n",
      "Epoch 216, Loss: 0.4702446473560784\n",
      "Epoch 217, Loss: 0.4701045016774931\n",
      "Epoch 218, Loss: 0.47002059077245106\n",
      "Epoch 219, Loss: 0.4699643617672827\n",
      "Epoch 220, Loss: 0.4698888296176042\n",
      "Epoch 221, Loss: 0.4698141575467262\n",
      "Epoch 222, Loss: 0.4697265783615268\n",
      "Epoch 223, Loss: 0.46966066909468496\n",
      "Epoch 224, Loss: 0.4696348739248034\n",
      "Epoch 225, Loss: 0.4697176006517021\n",
      "Epoch 226, Loss: 0.4699424675720642\n",
      "Epoch 227, Loss: 0.46999269112017344\n",
      "Epoch 228, Loss: 0.4698759384724347\n",
      "Epoch 229, Loss: 0.46938041574678274\n",
      "Epoch 230, Loss: 0.4691415581864629\n",
      "Epoch 231, Loss: 0.46924839544758656\n",
      "Epoch 232, Loss: 0.46933531775018905\n",
      "Epoch 233, Loss: 0.4692253881106229\n",
      "Epoch 234, Loss: 0.46894320638835874\n",
      "Epoch 235, Loss: 0.4688628060980674\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.22026019118237972\n",
      "Valence RMSE: 0.2073598610789855\n",
      "Arousal RMSE: 0.2324456746267841\n",
      "Test R^2 score: tensor([0.4789, 0.4650], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.3569, 0.3398], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4719542287617482\n",
      "Num of epochs: 236\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Epoch 121, Loss: 0.49295912913079615\n",
      "Epoch 122, Loss: 0.4924773619139838\n",
      "Epoch 123, Loss: 0.49197607191949805\n",
      "Epoch 124, Loss: 0.4914699197831528\n",
      "Epoch 125, Loss: 0.49091483381312667\n",
      "Epoch 126, Loss: 0.49035147677946317\n",
      "Epoch 127, Loss: 0.4898087222825037\n",
      "Epoch 128, Loss: 0.48924014731550836\n",
      "Epoch 129, Loss: 0.48873550647212044\n",
      "Epoch 130, Loss: 0.48832307254971563\n",
      "Epoch 131, Loss: 0.48791418392821295\n",
      "Epoch 132, Loss: 0.4875902245243796\n",
      "Epoch 133, Loss: 0.48728851108037585\n",
      "Epoch 134, Loss: 0.48697062262181345\n",
      "Epoch 135, Loss: 0.4866339553030548\n",
      "Epoch 136, Loss: 0.48633446746734893\n",
      "Epoch 137, Loss: 0.4860310087358514\n",
      "Epoch 138, Loss: 0.4856786412545615\n",
      "Epoch 139, Loss: 0.48531372107004\n",
      "Epoch 140, Loss: 0.48495712985488587\n",
      "Epoch 141, Loss: 0.4846131600652309\n",
      "Epoch 142, Loss: 0.48425966858652364\n",
      "Epoch 143, Loss: 0.4838914149236435\n",
      "Epoch 144, Loss: 0.48352437546282767\n",
      "Epoch 145, Loss: 0.4832299911937133\n",
      "Epoch 146, Loss: 0.4832586529465723\n",
      "Epoch 147, Loss: 0.48356744143113517\n",
      "Epoch 148, Loss: 0.4828108019011922\n",
      "Epoch 149, Loss: 0.4818912939738211\n",
      "Epoch 150, Loss: 0.48239416745575797\n",
      "Epoch 151, Loss: 0.4814952491721961\n",
      "Epoch 152, Loss: 0.4811822818820502\n",
      "Epoch 153, Loss: 0.4812753156716337\n",
      "Epoch 154, Loss: 0.4804387742282827\n",
      "Epoch 155, Loss: 0.48058189052417905\n",
      "Epoch 156, Loss: 0.4801288115497346\n",
      "Epoch 157, Loss: 0.4797465937246525\n",
      "Epoch 158, Loss: 0.4797622479529733\n",
      "Epoch 159, Loss: 0.4791664679844762\n",
      "Epoch 160, Loss: 0.47904359923301915\n",
      "Epoch 161, Loss: 0.4788483533353555\n",
      "Epoch 162, Loss: 0.4783709093142749\n",
      "Epoch 163, Loss: 0.4783027642591491\n",
      "Epoch 164, Loss: 0.4780003946284736\n",
      "Epoch 165, Loss: 0.47766356610156796\n",
      "Epoch 166, Loss: 0.47756940749162413\n",
      "Epoch 167, Loss: 0.4772720526560437\n",
      "Epoch 168, Loss: 0.477001191512605\n",
      "Epoch 169, Loss: 0.47690651155802954\n",
      "Epoch 170, Loss: 0.47663605231839845\n",
      "Epoch 171, Loss: 0.4763715079887003\n",
      "Epoch 172, Loss: 0.4762479807342815\n",
      "Epoch 173, Loss: 0.47603155482909787\n",
      "Epoch 174, Loss: 0.47580085926956445\n",
      "Epoch 175, Loss: 0.4756396378860013\n",
      "Epoch 176, Loss: 0.4754627232666358\n",
      "Epoch 177, Loss: 0.4752513954316692\n",
      "Epoch 178, Loss: 0.47507140350730925\n",
      "Epoch 179, Loss: 0.47492471273973497\n",
      "Epoch 180, Loss: 0.47475493910732763\n",
      "Epoch 181, Loss: 0.47455157018218247\n",
      "Epoch 182, Loss: 0.4743853224635176\n",
      "Epoch 183, Loss: 0.47424550491599726\n",
      "Epoch 184, Loss: 0.47408413178207975\n",
      "Epoch 185, Loss: 0.47390385374022453\n",
      "Epoch 186, Loss: 0.4737383066259127\n",
      "Epoch 187, Loss: 0.47358797785338436\n",
      "Epoch 188, Loss: 0.4734560449826861\n",
      "Epoch 189, Loss: 0.47335738206045025\n",
      "Epoch 190, Loss: 0.4733162834925985\n",
      "Epoch 191, Loss: 0.4732413020737725\n",
      "Epoch 192, Loss: 0.47313239020014447\n",
      "Epoch 193, Loss: 0.4729457158004841\n",
      "Epoch 194, Loss: 0.47274703738102797\n",
      "Epoch 195, Loss: 0.47253750657270627\n",
      "Epoch 196, Loss: 0.4723544299968521\n",
      "Epoch 197, Loss: 0.47221102907746554\n",
      "Epoch 198, Loss: 0.47210844467503205\n",
      "Epoch 199, Loss: 0.47205337980155637\n",
      "Epoch 200, Loss: 0.47204973383515075\n",
      "Epoch 201, Loss: 0.4720427890593303\n",
      "Epoch 202, Loss: 0.4719159657967507\n",
      "Epoch 203, Loss: 0.471735017109987\n",
      "Epoch 204, Loss: 0.4714985533244086\n",
      "Epoch 205, Loss: 0.4713143302002759\n",
      "Epoch 206, Loss: 0.47118574544243713\n",
      "Epoch 207, Loss: 0.47112002449718005\n",
      "Epoch 208, Loss: 0.4710910670518786\n",
      "Epoch 209, Loss: 0.47102495326923743\n",
      "Epoch 210, Loss: 0.4709626586360175\n",
      "Epoch 211, Loss: 0.47085840979895394\n",
      "Epoch 212, Loss: 0.47078064719388485\n",
      "Epoch 213, Loss: 0.47066614789276784\n",
      "Epoch 214, Loss: 0.47055654499036753\n",
      "Epoch 215, Loss: 0.47041652390832933\n",
      "Epoch 216, Loss: 0.4702446473560784\n",
      "Epoch 217, Loss: 0.4701045016774931\n",
      "Epoch 218, Loss: 0.47002059077245106\n",
      "Epoch 219, Loss: 0.4699643617672827\n",
      "Epoch 220, Loss: 0.4698888296176042\n",
      "Epoch 221, Loss: 0.4698141575467262\n",
      "Epoch 222, Loss: 0.4697265783615268\n",
      "Epoch 223, Loss: 0.46966066909468496\n",
      "Epoch 224, Loss: 0.4696348739248034\n",
      "Epoch 225, Loss: 0.4697176006517021\n",
      "Epoch 226, Loss: 0.4699424675720642\n",
      "Epoch 227, Loss: 0.46999269112017344\n",
      "Epoch 228, Loss: 0.4698759384724347\n",
      "Epoch 229, Loss: 0.46938041574678274\n",
      "Epoch 230, Loss: 0.4691415581864629\n",
      "Epoch 231, Loss: 0.46924839544758656\n",
      "Epoch 232, Loss: 0.46933531775018905\n",
      "Epoch 233, Loss: 0.4692253881106229\n",
      "Epoch 234, Loss: 0.46894320638835874\n",
      "Epoch 235, Loss: 0.4688628060980674\n",
      "Epoch 236, Loss: 0.46897798398181567\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.21900129258822362\n",
      "Valence RMSE: 0.2061263715801495\n",
      "Arousal RMSE: 0.2311602285208846\n",
      "Test R^2 score: tensor([0.4851, 0.4709], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.3646, 0.3471], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4779951657541188\n",
      "Num of epochs: 237\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Epoch 121, Loss: 0.49295912913079615\n",
      "Epoch 122, Loss: 0.4924773619139838\n",
      "Epoch 123, Loss: 0.49197607191949805\n",
      "Epoch 124, Loss: 0.4914699197831528\n",
      "Epoch 125, Loss: 0.49091483381312667\n",
      "Epoch 126, Loss: 0.49035147677946317\n",
      "Epoch 127, Loss: 0.4898087222825037\n",
      "Epoch 128, Loss: 0.48924014731550836\n",
      "Epoch 129, Loss: 0.48873550647212044\n",
      "Epoch 130, Loss: 0.48832307254971563\n",
      "Epoch 131, Loss: 0.48791418392821295\n",
      "Epoch 132, Loss: 0.4875902245243796\n",
      "Epoch 133, Loss: 0.48728851108037585\n",
      "Epoch 134, Loss: 0.48697062262181345\n",
      "Epoch 135, Loss: 0.4866339553030548\n",
      "Epoch 136, Loss: 0.48633446746734893\n",
      "Epoch 137, Loss: 0.4860310087358514\n",
      "Epoch 138, Loss: 0.4856786412545615\n",
      "Epoch 139, Loss: 0.48531372107004\n",
      "Epoch 140, Loss: 0.48495712985488587\n",
      "Epoch 141, Loss: 0.4846131600652309\n",
      "Epoch 142, Loss: 0.48425966858652364\n",
      "Epoch 143, Loss: 0.4838914149236435\n",
      "Epoch 144, Loss: 0.48352437546282767\n",
      "Epoch 145, Loss: 0.4832299911937133\n",
      "Epoch 146, Loss: 0.4832586529465723\n",
      "Epoch 147, Loss: 0.48356744143113517\n",
      "Epoch 148, Loss: 0.4828108019011922\n",
      "Epoch 149, Loss: 0.4818912939738211\n",
      "Epoch 150, Loss: 0.48239416745575797\n",
      "Epoch 151, Loss: 0.4814952491721961\n",
      "Epoch 152, Loss: 0.4811822818820502\n",
      "Epoch 153, Loss: 0.4812753156716337\n",
      "Epoch 154, Loss: 0.4804387742282827\n",
      "Epoch 155, Loss: 0.48058189052417905\n",
      "Epoch 156, Loss: 0.4801288115497346\n",
      "Epoch 157, Loss: 0.4797465937246525\n",
      "Epoch 158, Loss: 0.4797622479529733\n",
      "Epoch 159, Loss: 0.4791664679844762\n",
      "Epoch 160, Loss: 0.47904359923301915\n",
      "Epoch 161, Loss: 0.4788483533353555\n",
      "Epoch 162, Loss: 0.4783709093142749\n",
      "Epoch 163, Loss: 0.4783027642591491\n",
      "Epoch 164, Loss: 0.4780003946284736\n",
      "Epoch 165, Loss: 0.47766356610156796\n",
      "Epoch 166, Loss: 0.47756940749162413\n",
      "Epoch 167, Loss: 0.4772720526560437\n",
      "Epoch 168, Loss: 0.477001191512605\n",
      "Epoch 169, Loss: 0.47690651155802954\n",
      "Epoch 170, Loss: 0.47663605231839845\n",
      "Epoch 171, Loss: 0.4763715079887003\n",
      "Epoch 172, Loss: 0.4762479807342815\n",
      "Epoch 173, Loss: 0.47603155482909787\n",
      "Epoch 174, Loss: 0.47580085926956445\n",
      "Epoch 175, Loss: 0.4756396378860013\n",
      "Epoch 176, Loss: 0.4754627232666358\n",
      "Epoch 177, Loss: 0.4752513954316692\n",
      "Epoch 178, Loss: 0.47507140350730925\n",
      "Epoch 179, Loss: 0.47492471273973497\n",
      "Epoch 180, Loss: 0.47475493910732763\n",
      "Epoch 181, Loss: 0.47455157018218247\n",
      "Epoch 182, Loss: 0.4743853224635176\n",
      "Epoch 183, Loss: 0.47424550491599726\n",
      "Epoch 184, Loss: 0.47408413178207975\n",
      "Epoch 185, Loss: 0.47390385374022453\n",
      "Epoch 186, Loss: 0.4737383066259127\n",
      "Epoch 187, Loss: 0.47358797785338436\n",
      "Epoch 188, Loss: 0.4734560449826861\n",
      "Epoch 189, Loss: 0.47335738206045025\n",
      "Epoch 190, Loss: 0.4733162834925985\n",
      "Epoch 191, Loss: 0.4732413020737725\n",
      "Epoch 192, Loss: 0.47313239020014447\n",
      "Epoch 193, Loss: 0.4729457158004841\n",
      "Epoch 194, Loss: 0.47274703738102797\n",
      "Epoch 195, Loss: 0.47253750657270627\n",
      "Epoch 196, Loss: 0.4723544299968521\n",
      "Epoch 197, Loss: 0.47221102907746554\n",
      "Epoch 198, Loss: 0.47210844467503205\n",
      "Epoch 199, Loss: 0.47205337980155637\n",
      "Epoch 200, Loss: 0.47204973383515075\n",
      "Epoch 201, Loss: 0.4720427890593303\n",
      "Epoch 202, Loss: 0.4719159657967507\n",
      "Epoch 203, Loss: 0.471735017109987\n",
      "Epoch 204, Loss: 0.4714985533244086\n",
      "Epoch 205, Loss: 0.4713143302002759\n",
      "Epoch 206, Loss: 0.47118574544243713\n",
      "Epoch 207, Loss: 0.47112002449718005\n",
      "Epoch 208, Loss: 0.4710910670518786\n",
      "Epoch 209, Loss: 0.47102495326923743\n",
      "Epoch 210, Loss: 0.4709626586360175\n",
      "Epoch 211, Loss: 0.47085840979895394\n",
      "Epoch 212, Loss: 0.47078064719388485\n",
      "Epoch 213, Loss: 0.47066614789276784\n",
      "Epoch 214, Loss: 0.47055654499036753\n",
      "Epoch 215, Loss: 0.47041652390832933\n",
      "Epoch 216, Loss: 0.4702446473560784\n",
      "Epoch 217, Loss: 0.4701045016774931\n",
      "Epoch 218, Loss: 0.47002059077245106\n",
      "Epoch 219, Loss: 0.4699643617672827\n",
      "Epoch 220, Loss: 0.4698888296176042\n",
      "Epoch 221, Loss: 0.4698141575467262\n",
      "Epoch 222, Loss: 0.4697265783615268\n",
      "Epoch 223, Loss: 0.46966066909468496\n",
      "Epoch 224, Loss: 0.4696348739248034\n",
      "Epoch 225, Loss: 0.4697176006517021\n",
      "Epoch 226, Loss: 0.4699424675720642\n",
      "Epoch 227, Loss: 0.46999269112017344\n",
      "Epoch 228, Loss: 0.4698759384724347\n",
      "Epoch 229, Loss: 0.46938041574678274\n",
      "Epoch 230, Loss: 0.4691415581864629\n",
      "Epoch 231, Loss: 0.46924839544758656\n",
      "Epoch 232, Loss: 0.46933531775018905\n",
      "Epoch 233, Loss: 0.4692253881106229\n",
      "Epoch 234, Loss: 0.46894320638835874\n",
      "Epoch 235, Loss: 0.4688628060980674\n",
      "Epoch 236, Loss: 0.46897798398181567\n",
      "Epoch 237, Loss: 0.4689811295666005\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2200178741531186\n",
      "Valence RMSE: 0.20716213716787144\n",
      "Arousal RMSE: 0.23216282824292758\n",
      "Test R^2 score: tensor([0.4799, 0.4663], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.3582, 0.3414], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4731014746630892\n",
      "Num of epochs: 238\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Epoch 121, Loss: 0.49295912913079615\n",
      "Epoch 122, Loss: 0.4924773619139838\n",
      "Epoch 123, Loss: 0.49197607191949805\n",
      "Epoch 124, Loss: 0.4914699197831528\n",
      "Epoch 125, Loss: 0.49091483381312667\n",
      "Epoch 126, Loss: 0.49035147677946317\n",
      "Epoch 127, Loss: 0.4898087222825037\n",
      "Epoch 128, Loss: 0.48924014731550836\n",
      "Epoch 129, Loss: 0.48873550647212044\n",
      "Epoch 130, Loss: 0.48832307254971563\n",
      "Epoch 131, Loss: 0.48791418392821295\n",
      "Epoch 132, Loss: 0.4875902245243796\n",
      "Epoch 133, Loss: 0.48728851108037585\n",
      "Epoch 134, Loss: 0.48697062262181345\n",
      "Epoch 135, Loss: 0.4866339553030548\n",
      "Epoch 136, Loss: 0.48633446746734893\n",
      "Epoch 137, Loss: 0.4860310087358514\n",
      "Epoch 138, Loss: 0.4856786412545615\n",
      "Epoch 139, Loss: 0.48531372107004\n",
      "Epoch 140, Loss: 0.48495712985488587\n",
      "Epoch 141, Loss: 0.4846131600652309\n",
      "Epoch 142, Loss: 0.48425966858652364\n",
      "Epoch 143, Loss: 0.4838914149236435\n",
      "Epoch 144, Loss: 0.48352437546282767\n",
      "Epoch 145, Loss: 0.4832299911937133\n",
      "Epoch 146, Loss: 0.4832586529465723\n",
      "Epoch 147, Loss: 0.48356744143113517\n",
      "Epoch 148, Loss: 0.4828108019011922\n",
      "Epoch 149, Loss: 0.4818912939738211\n",
      "Epoch 150, Loss: 0.48239416745575797\n",
      "Epoch 151, Loss: 0.4814952491721961\n",
      "Epoch 152, Loss: 0.4811822818820502\n",
      "Epoch 153, Loss: 0.4812753156716337\n",
      "Epoch 154, Loss: 0.4804387742282827\n",
      "Epoch 155, Loss: 0.48058189052417905\n",
      "Epoch 156, Loss: 0.4801288115497346\n",
      "Epoch 157, Loss: 0.4797465937246525\n",
      "Epoch 158, Loss: 0.4797622479529733\n",
      "Epoch 159, Loss: 0.4791664679844762\n",
      "Epoch 160, Loss: 0.47904359923301915\n",
      "Epoch 161, Loss: 0.4788483533353555\n",
      "Epoch 162, Loss: 0.4783709093142749\n",
      "Epoch 163, Loss: 0.4783027642591491\n",
      "Epoch 164, Loss: 0.4780003946284736\n",
      "Epoch 165, Loss: 0.47766356610156796\n",
      "Epoch 166, Loss: 0.47756940749162413\n",
      "Epoch 167, Loss: 0.4772720526560437\n",
      "Epoch 168, Loss: 0.477001191512605\n",
      "Epoch 169, Loss: 0.47690651155802954\n",
      "Epoch 170, Loss: 0.47663605231839845\n",
      "Epoch 171, Loss: 0.4763715079887003\n",
      "Epoch 172, Loss: 0.4762479807342815\n",
      "Epoch 173, Loss: 0.47603155482909787\n",
      "Epoch 174, Loss: 0.47580085926956445\n",
      "Epoch 175, Loss: 0.4756396378860013\n",
      "Epoch 176, Loss: 0.4754627232666358\n",
      "Epoch 177, Loss: 0.4752513954316692\n",
      "Epoch 178, Loss: 0.47507140350730925\n",
      "Epoch 179, Loss: 0.47492471273973497\n",
      "Epoch 180, Loss: 0.47475493910732763\n",
      "Epoch 181, Loss: 0.47455157018218247\n",
      "Epoch 182, Loss: 0.4743853224635176\n",
      "Epoch 183, Loss: 0.47424550491599726\n",
      "Epoch 184, Loss: 0.47408413178207975\n",
      "Epoch 185, Loss: 0.47390385374022453\n",
      "Epoch 186, Loss: 0.4737383066259127\n",
      "Epoch 187, Loss: 0.47358797785338436\n",
      "Epoch 188, Loss: 0.4734560449826861\n",
      "Epoch 189, Loss: 0.47335738206045025\n",
      "Epoch 190, Loss: 0.4733162834925985\n",
      "Epoch 191, Loss: 0.4732413020737725\n",
      "Epoch 192, Loss: 0.47313239020014447\n",
      "Epoch 193, Loss: 0.4729457158004841\n",
      "Epoch 194, Loss: 0.47274703738102797\n",
      "Epoch 195, Loss: 0.47253750657270627\n",
      "Epoch 196, Loss: 0.4723544299968521\n",
      "Epoch 197, Loss: 0.47221102907746554\n",
      "Epoch 198, Loss: 0.47210844467503205\n",
      "Epoch 199, Loss: 0.47205337980155637\n",
      "Epoch 200, Loss: 0.47204973383515075\n",
      "Epoch 201, Loss: 0.4720427890593303\n",
      "Epoch 202, Loss: 0.4719159657967507\n",
      "Epoch 203, Loss: 0.471735017109987\n",
      "Epoch 204, Loss: 0.4714985533244086\n",
      "Epoch 205, Loss: 0.4713143302002759\n",
      "Epoch 206, Loss: 0.47118574544243713\n",
      "Epoch 207, Loss: 0.47112002449718005\n",
      "Epoch 208, Loss: 0.4710910670518786\n",
      "Epoch 209, Loss: 0.47102495326923743\n",
      "Epoch 210, Loss: 0.4709626586360175\n",
      "Epoch 211, Loss: 0.47085840979895394\n",
      "Epoch 212, Loss: 0.47078064719388485\n",
      "Epoch 213, Loss: 0.47066614789276784\n",
      "Epoch 214, Loss: 0.47055654499036753\n",
      "Epoch 215, Loss: 0.47041652390832933\n",
      "Epoch 216, Loss: 0.4702446473560784\n",
      "Epoch 217, Loss: 0.4701045016774931\n",
      "Epoch 218, Loss: 0.47002059077245106\n",
      "Epoch 219, Loss: 0.4699643617672827\n",
      "Epoch 220, Loss: 0.4698888296176042\n",
      "Epoch 221, Loss: 0.4698141575467262\n",
      "Epoch 222, Loss: 0.4697265783615268\n",
      "Epoch 223, Loss: 0.46966066909468496\n",
      "Epoch 224, Loss: 0.4696348739248034\n",
      "Epoch 225, Loss: 0.4697176006517021\n",
      "Epoch 226, Loss: 0.4699424675720642\n",
      "Epoch 227, Loss: 0.46999269112017344\n",
      "Epoch 228, Loss: 0.4698759384724347\n",
      "Epoch 229, Loss: 0.46938041574678274\n",
      "Epoch 230, Loss: 0.4691415581864629\n",
      "Epoch 231, Loss: 0.46924839544758656\n",
      "Epoch 232, Loss: 0.46933531775018905\n",
      "Epoch 233, Loss: 0.4692253881106229\n",
      "Epoch 234, Loss: 0.46894320638835874\n",
      "Epoch 235, Loss: 0.4688628060980674\n",
      "Epoch 236, Loss: 0.46897798398181567\n",
      "Epoch 237, Loss: 0.4689811295666005\n",
      "Epoch 238, Loss: 0.4688031484354944\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2192248824650164\n",
      "Valence RMSE: 0.20642026631608223\n",
      "Arousal RMSE: 0.23132179282894683\n",
      "Test R^2 score: tensor([0.4836, 0.4702], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.3628, 0.3462], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4768905505114116\n",
      "Num of epochs: 239\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Epoch 121, Loss: 0.49295912913079615\n",
      "Epoch 122, Loss: 0.4924773619139838\n",
      "Epoch 123, Loss: 0.49197607191949805\n",
      "Epoch 124, Loss: 0.4914699197831528\n",
      "Epoch 125, Loss: 0.49091483381312667\n",
      "Epoch 126, Loss: 0.49035147677946317\n",
      "Epoch 127, Loss: 0.4898087222825037\n",
      "Epoch 128, Loss: 0.48924014731550836\n",
      "Epoch 129, Loss: 0.48873550647212044\n",
      "Epoch 130, Loss: 0.48832307254971563\n",
      "Epoch 131, Loss: 0.48791418392821295\n",
      "Epoch 132, Loss: 0.4875902245243796\n",
      "Epoch 133, Loss: 0.48728851108037585\n",
      "Epoch 134, Loss: 0.48697062262181345\n",
      "Epoch 135, Loss: 0.4866339553030548\n",
      "Epoch 136, Loss: 0.48633446746734893\n",
      "Epoch 137, Loss: 0.4860310087358514\n",
      "Epoch 138, Loss: 0.4856786412545615\n",
      "Epoch 139, Loss: 0.48531372107004\n",
      "Epoch 140, Loss: 0.48495712985488587\n",
      "Epoch 141, Loss: 0.4846131600652309\n",
      "Epoch 142, Loss: 0.48425966858652364\n",
      "Epoch 143, Loss: 0.4838914149236435\n",
      "Epoch 144, Loss: 0.48352437546282767\n",
      "Epoch 145, Loss: 0.4832299911937133\n",
      "Epoch 146, Loss: 0.4832586529465723\n",
      "Epoch 147, Loss: 0.48356744143113517\n",
      "Epoch 148, Loss: 0.4828108019011922\n",
      "Epoch 149, Loss: 0.4818912939738211\n",
      "Epoch 150, Loss: 0.48239416745575797\n",
      "Epoch 151, Loss: 0.4814952491721961\n",
      "Epoch 152, Loss: 0.4811822818820502\n",
      "Epoch 153, Loss: 0.4812753156716337\n",
      "Epoch 154, Loss: 0.4804387742282827\n",
      "Epoch 155, Loss: 0.48058189052417905\n",
      "Epoch 156, Loss: 0.4801288115497346\n",
      "Epoch 157, Loss: 0.4797465937246525\n",
      "Epoch 158, Loss: 0.4797622479529733\n",
      "Epoch 159, Loss: 0.4791664679844762\n",
      "Epoch 160, Loss: 0.47904359923301915\n",
      "Epoch 161, Loss: 0.4788483533353555\n",
      "Epoch 162, Loss: 0.4783709093142749\n",
      "Epoch 163, Loss: 0.4783027642591491\n",
      "Epoch 164, Loss: 0.4780003946284736\n",
      "Epoch 165, Loss: 0.47766356610156796\n",
      "Epoch 166, Loss: 0.47756940749162413\n",
      "Epoch 167, Loss: 0.4772720526560437\n",
      "Epoch 168, Loss: 0.477001191512605\n",
      "Epoch 169, Loss: 0.47690651155802954\n",
      "Epoch 170, Loss: 0.47663605231839845\n",
      "Epoch 171, Loss: 0.4763715079887003\n",
      "Epoch 172, Loss: 0.4762479807342815\n",
      "Epoch 173, Loss: 0.47603155482909787\n",
      "Epoch 174, Loss: 0.47580085926956445\n",
      "Epoch 175, Loss: 0.4756396378860013\n",
      "Epoch 176, Loss: 0.4754627232666358\n",
      "Epoch 177, Loss: 0.4752513954316692\n",
      "Epoch 178, Loss: 0.47507140350730925\n",
      "Epoch 179, Loss: 0.47492471273973497\n",
      "Epoch 180, Loss: 0.47475493910732763\n",
      "Epoch 181, Loss: 0.47455157018218247\n",
      "Epoch 182, Loss: 0.4743853224635176\n",
      "Epoch 183, Loss: 0.47424550491599726\n",
      "Epoch 184, Loss: 0.47408413178207975\n",
      "Epoch 185, Loss: 0.47390385374022453\n",
      "Epoch 186, Loss: 0.4737383066259127\n",
      "Epoch 187, Loss: 0.47358797785338436\n",
      "Epoch 188, Loss: 0.4734560449826861\n",
      "Epoch 189, Loss: 0.47335738206045025\n",
      "Epoch 190, Loss: 0.4733162834925985\n",
      "Epoch 191, Loss: 0.4732413020737725\n",
      "Epoch 192, Loss: 0.47313239020014447\n",
      "Epoch 193, Loss: 0.4729457158004841\n",
      "Epoch 194, Loss: 0.47274703738102797\n",
      "Epoch 195, Loss: 0.47253750657270627\n",
      "Epoch 196, Loss: 0.4723544299968521\n",
      "Epoch 197, Loss: 0.47221102907746554\n",
      "Epoch 198, Loss: 0.47210844467503205\n",
      "Epoch 199, Loss: 0.47205337980155637\n",
      "Epoch 200, Loss: 0.47204973383515075\n",
      "Epoch 201, Loss: 0.4720427890593303\n",
      "Epoch 202, Loss: 0.4719159657967507\n",
      "Epoch 203, Loss: 0.471735017109987\n",
      "Epoch 204, Loss: 0.4714985533244086\n",
      "Epoch 205, Loss: 0.4713143302002759\n",
      "Epoch 206, Loss: 0.47118574544243713\n",
      "Epoch 207, Loss: 0.47112002449718005\n",
      "Epoch 208, Loss: 0.4710910670518786\n",
      "Epoch 209, Loss: 0.47102495326923743\n",
      "Epoch 210, Loss: 0.4709626586360175\n",
      "Epoch 211, Loss: 0.47085840979895394\n",
      "Epoch 212, Loss: 0.47078064719388485\n",
      "Epoch 213, Loss: 0.47066614789276784\n",
      "Epoch 214, Loss: 0.47055654499036753\n",
      "Epoch 215, Loss: 0.47041652390832933\n",
      "Epoch 216, Loss: 0.4702446473560784\n",
      "Epoch 217, Loss: 0.4701045016774931\n",
      "Epoch 218, Loss: 0.47002059077245106\n",
      "Epoch 219, Loss: 0.4699643617672827\n",
      "Epoch 220, Loss: 0.4698888296176042\n",
      "Epoch 221, Loss: 0.4698141575467262\n",
      "Epoch 222, Loss: 0.4697265783615268\n",
      "Epoch 223, Loss: 0.46966066909468496\n",
      "Epoch 224, Loss: 0.4696348739248034\n",
      "Epoch 225, Loss: 0.4697176006517021\n",
      "Epoch 226, Loss: 0.4699424675720642\n",
      "Epoch 227, Loss: 0.46999269112017344\n",
      "Epoch 228, Loss: 0.4698759384724347\n",
      "Epoch 229, Loss: 0.46938041574678274\n",
      "Epoch 230, Loss: 0.4691415581864629\n",
      "Epoch 231, Loss: 0.46924839544758656\n",
      "Epoch 232, Loss: 0.46933531775018905\n",
      "Epoch 233, Loss: 0.4692253881106229\n",
      "Epoch 234, Loss: 0.46894320638835874\n",
      "Epoch 235, Loss: 0.4688628060980674\n",
      "Epoch 236, Loss: 0.46897798398181567\n",
      "Epoch 237, Loss: 0.4689811295666005\n",
      "Epoch 238, Loss: 0.4688031484354944\n",
      "Epoch 239, Loss: 0.46862983752908405\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.21910899451785656\n",
      "Valence RMSE: 0.20634064296056862\n",
      "Arousal RMSE: 0.2311731862043506\n",
      "Test R^2 score: tensor([0.4840, 0.4708], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.3633, 0.3470], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4774299687235128\n",
      "Num of epochs: 240\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Epoch 121, Loss: 0.49295912913079615\n",
      "Epoch 122, Loss: 0.4924773619139838\n",
      "Epoch 123, Loss: 0.49197607191949805\n",
      "Epoch 124, Loss: 0.4914699197831528\n",
      "Epoch 125, Loss: 0.49091483381312667\n",
      "Epoch 126, Loss: 0.49035147677946317\n",
      "Epoch 127, Loss: 0.4898087222825037\n",
      "Epoch 128, Loss: 0.48924014731550836\n",
      "Epoch 129, Loss: 0.48873550647212044\n",
      "Epoch 130, Loss: 0.48832307254971563\n",
      "Epoch 131, Loss: 0.48791418392821295\n",
      "Epoch 132, Loss: 0.4875902245243796\n",
      "Epoch 133, Loss: 0.48728851108037585\n",
      "Epoch 134, Loss: 0.48697062262181345\n",
      "Epoch 135, Loss: 0.4866339553030548\n",
      "Epoch 136, Loss: 0.48633446746734893\n",
      "Epoch 137, Loss: 0.4860310087358514\n",
      "Epoch 138, Loss: 0.4856786412545615\n",
      "Epoch 139, Loss: 0.48531372107004\n",
      "Epoch 140, Loss: 0.48495712985488587\n",
      "Epoch 141, Loss: 0.4846131600652309\n",
      "Epoch 142, Loss: 0.48425966858652364\n",
      "Epoch 143, Loss: 0.4838914149236435\n",
      "Epoch 144, Loss: 0.48352437546282767\n",
      "Epoch 145, Loss: 0.4832299911937133\n",
      "Epoch 146, Loss: 0.4832586529465723\n",
      "Epoch 147, Loss: 0.48356744143113517\n",
      "Epoch 148, Loss: 0.4828108019011922\n",
      "Epoch 149, Loss: 0.4818912939738211\n",
      "Epoch 150, Loss: 0.48239416745575797\n",
      "Epoch 151, Loss: 0.4814952491721961\n",
      "Epoch 152, Loss: 0.4811822818820502\n",
      "Epoch 153, Loss: 0.4812753156716337\n",
      "Epoch 154, Loss: 0.4804387742282827\n",
      "Epoch 155, Loss: 0.48058189052417905\n",
      "Epoch 156, Loss: 0.4801288115497346\n",
      "Epoch 157, Loss: 0.4797465937246525\n",
      "Epoch 158, Loss: 0.4797622479529733\n",
      "Epoch 159, Loss: 0.4791664679844762\n",
      "Epoch 160, Loss: 0.47904359923301915\n",
      "Epoch 161, Loss: 0.4788483533353555\n",
      "Epoch 162, Loss: 0.4783709093142749\n",
      "Epoch 163, Loss: 0.4783027642591491\n",
      "Epoch 164, Loss: 0.4780003946284736\n",
      "Epoch 165, Loss: 0.47766356610156796\n",
      "Epoch 166, Loss: 0.47756940749162413\n",
      "Epoch 167, Loss: 0.4772720526560437\n",
      "Epoch 168, Loss: 0.477001191512605\n",
      "Epoch 169, Loss: 0.47690651155802954\n",
      "Epoch 170, Loss: 0.47663605231839845\n",
      "Epoch 171, Loss: 0.4763715079887003\n",
      "Epoch 172, Loss: 0.4762479807342815\n",
      "Epoch 173, Loss: 0.47603155482909787\n",
      "Epoch 174, Loss: 0.47580085926956445\n",
      "Epoch 175, Loss: 0.4756396378860013\n",
      "Epoch 176, Loss: 0.4754627232666358\n",
      "Epoch 177, Loss: 0.4752513954316692\n",
      "Epoch 178, Loss: 0.47507140350730925\n",
      "Epoch 179, Loss: 0.47492471273973497\n",
      "Epoch 180, Loss: 0.47475493910732763\n",
      "Epoch 181, Loss: 0.47455157018218247\n",
      "Epoch 182, Loss: 0.4743853224635176\n",
      "Epoch 183, Loss: 0.47424550491599726\n",
      "Epoch 184, Loss: 0.47408413178207975\n",
      "Epoch 185, Loss: 0.47390385374022453\n",
      "Epoch 186, Loss: 0.4737383066259127\n",
      "Epoch 187, Loss: 0.47358797785338436\n",
      "Epoch 188, Loss: 0.4734560449826861\n",
      "Epoch 189, Loss: 0.47335738206045025\n",
      "Epoch 190, Loss: 0.4733162834925985\n",
      "Epoch 191, Loss: 0.4732413020737725\n",
      "Epoch 192, Loss: 0.47313239020014447\n",
      "Epoch 193, Loss: 0.4729457158004841\n",
      "Epoch 194, Loss: 0.47274703738102797\n",
      "Epoch 195, Loss: 0.47253750657270627\n",
      "Epoch 196, Loss: 0.4723544299968521\n",
      "Epoch 197, Loss: 0.47221102907746554\n",
      "Epoch 198, Loss: 0.47210844467503205\n",
      "Epoch 199, Loss: 0.47205337980155637\n",
      "Epoch 200, Loss: 0.47204973383515075\n",
      "Epoch 201, Loss: 0.4720427890593303\n",
      "Epoch 202, Loss: 0.4719159657967507\n",
      "Epoch 203, Loss: 0.471735017109987\n",
      "Epoch 204, Loss: 0.4714985533244086\n",
      "Epoch 205, Loss: 0.4713143302002759\n",
      "Epoch 206, Loss: 0.47118574544243713\n",
      "Epoch 207, Loss: 0.47112002449718005\n",
      "Epoch 208, Loss: 0.4710910670518786\n",
      "Epoch 209, Loss: 0.47102495326923743\n",
      "Epoch 210, Loss: 0.4709626586360175\n",
      "Epoch 211, Loss: 0.47085840979895394\n",
      "Epoch 212, Loss: 0.47078064719388485\n",
      "Epoch 213, Loss: 0.47066614789276784\n",
      "Epoch 214, Loss: 0.47055654499036753\n",
      "Epoch 215, Loss: 0.47041652390832933\n",
      "Epoch 216, Loss: 0.4702446473560784\n",
      "Epoch 217, Loss: 0.4701045016774931\n",
      "Epoch 218, Loss: 0.47002059077245106\n",
      "Epoch 219, Loss: 0.4699643617672827\n",
      "Epoch 220, Loss: 0.4698888296176042\n",
      "Epoch 221, Loss: 0.4698141575467262\n",
      "Epoch 222, Loss: 0.4697265783615268\n",
      "Epoch 223, Loss: 0.46966066909468496\n",
      "Epoch 224, Loss: 0.4696348739248034\n",
      "Epoch 225, Loss: 0.4697176006517021\n",
      "Epoch 226, Loss: 0.4699424675720642\n",
      "Epoch 227, Loss: 0.46999269112017344\n",
      "Epoch 228, Loss: 0.4698759384724347\n",
      "Epoch 229, Loss: 0.46938041574678274\n",
      "Epoch 230, Loss: 0.4691415581864629\n",
      "Epoch 231, Loss: 0.46924839544758656\n",
      "Epoch 232, Loss: 0.46933531775018905\n",
      "Epoch 233, Loss: 0.4692253881106229\n",
      "Epoch 234, Loss: 0.46894320638835874\n",
      "Epoch 235, Loss: 0.4688628060980674\n",
      "Epoch 236, Loss: 0.46897798398181567\n",
      "Epoch 237, Loss: 0.4689811295666005\n",
      "Epoch 238, Loss: 0.4688031484354944\n",
      "Epoch 239, Loss: 0.46862983752908405\n",
      "Epoch 240, Loss: 0.4685963696810109\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2200217324598582\n",
      "Valence RMSE: 0.20726981768906444\n",
      "Arousal RMSE: 0.23207401445318432\n",
      "Test R^2 score: tensor([0.4794, 0.4667], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.3575, 0.3419], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.47303518826728796\n",
      "Num of epochs: 241\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Epoch 121, Loss: 0.49295912913079615\n",
      "Epoch 122, Loss: 0.4924773619139838\n",
      "Epoch 123, Loss: 0.49197607191949805\n",
      "Epoch 124, Loss: 0.4914699197831528\n",
      "Epoch 125, Loss: 0.49091483381312667\n",
      "Epoch 126, Loss: 0.49035147677946317\n",
      "Epoch 127, Loss: 0.4898087222825037\n",
      "Epoch 128, Loss: 0.48924014731550836\n",
      "Epoch 129, Loss: 0.48873550647212044\n",
      "Epoch 130, Loss: 0.48832307254971563\n",
      "Epoch 131, Loss: 0.48791418392821295\n",
      "Epoch 132, Loss: 0.4875902245243796\n",
      "Epoch 133, Loss: 0.48728851108037585\n",
      "Epoch 134, Loss: 0.48697062262181345\n",
      "Epoch 135, Loss: 0.4866339553030548\n",
      "Epoch 136, Loss: 0.48633446746734893\n",
      "Epoch 137, Loss: 0.4860310087358514\n",
      "Epoch 138, Loss: 0.4856786412545615\n",
      "Epoch 139, Loss: 0.48531372107004\n",
      "Epoch 140, Loss: 0.48495712985488587\n",
      "Epoch 141, Loss: 0.4846131600652309\n",
      "Epoch 142, Loss: 0.48425966858652364\n",
      "Epoch 143, Loss: 0.4838914149236435\n",
      "Epoch 144, Loss: 0.48352437546282767\n",
      "Epoch 145, Loss: 0.4832299911937133\n",
      "Epoch 146, Loss: 0.4832586529465723\n",
      "Epoch 147, Loss: 0.48356744143113517\n",
      "Epoch 148, Loss: 0.4828108019011922\n",
      "Epoch 149, Loss: 0.4818912939738211\n",
      "Epoch 150, Loss: 0.48239416745575797\n",
      "Epoch 151, Loss: 0.4814952491721961\n",
      "Epoch 152, Loss: 0.4811822818820502\n",
      "Epoch 153, Loss: 0.4812753156716337\n",
      "Epoch 154, Loss: 0.4804387742282827\n",
      "Epoch 155, Loss: 0.48058189052417905\n",
      "Epoch 156, Loss: 0.4801288115497346\n",
      "Epoch 157, Loss: 0.4797465937246525\n",
      "Epoch 158, Loss: 0.4797622479529733\n",
      "Epoch 159, Loss: 0.4791664679844762\n",
      "Epoch 160, Loss: 0.47904359923301915\n",
      "Epoch 161, Loss: 0.4788483533353555\n",
      "Epoch 162, Loss: 0.4783709093142749\n",
      "Epoch 163, Loss: 0.4783027642591491\n",
      "Epoch 164, Loss: 0.4780003946284736\n",
      "Epoch 165, Loss: 0.47766356610156796\n",
      "Epoch 166, Loss: 0.47756940749162413\n",
      "Epoch 167, Loss: 0.4772720526560437\n",
      "Epoch 168, Loss: 0.477001191512605\n",
      "Epoch 169, Loss: 0.47690651155802954\n",
      "Epoch 170, Loss: 0.47663605231839845\n",
      "Epoch 171, Loss: 0.4763715079887003\n",
      "Epoch 172, Loss: 0.4762479807342815\n",
      "Epoch 173, Loss: 0.47603155482909787\n",
      "Epoch 174, Loss: 0.47580085926956445\n",
      "Epoch 175, Loss: 0.4756396378860013\n",
      "Epoch 176, Loss: 0.4754627232666358\n",
      "Epoch 177, Loss: 0.4752513954316692\n",
      "Epoch 178, Loss: 0.47507140350730925\n",
      "Epoch 179, Loss: 0.47492471273973497\n",
      "Epoch 180, Loss: 0.47475493910732763\n",
      "Epoch 181, Loss: 0.47455157018218247\n",
      "Epoch 182, Loss: 0.4743853224635176\n",
      "Epoch 183, Loss: 0.47424550491599726\n",
      "Epoch 184, Loss: 0.47408413178207975\n",
      "Epoch 185, Loss: 0.47390385374022453\n",
      "Epoch 186, Loss: 0.4737383066259127\n",
      "Epoch 187, Loss: 0.47358797785338436\n",
      "Epoch 188, Loss: 0.4734560449826861\n",
      "Epoch 189, Loss: 0.47335738206045025\n",
      "Epoch 190, Loss: 0.4733162834925985\n",
      "Epoch 191, Loss: 0.4732413020737725\n",
      "Epoch 192, Loss: 0.47313239020014447\n",
      "Epoch 193, Loss: 0.4729457158004841\n",
      "Epoch 194, Loss: 0.47274703738102797\n",
      "Epoch 195, Loss: 0.47253750657270627\n",
      "Epoch 196, Loss: 0.4723544299968521\n",
      "Epoch 197, Loss: 0.47221102907746554\n",
      "Epoch 198, Loss: 0.47210844467503205\n",
      "Epoch 199, Loss: 0.47205337980155637\n",
      "Epoch 200, Loss: 0.47204973383515075\n",
      "Epoch 201, Loss: 0.4720427890593303\n",
      "Epoch 202, Loss: 0.4719159657967507\n",
      "Epoch 203, Loss: 0.471735017109987\n",
      "Epoch 204, Loss: 0.4714985533244086\n",
      "Epoch 205, Loss: 0.4713143302002759\n",
      "Epoch 206, Loss: 0.47118574544243713\n",
      "Epoch 207, Loss: 0.47112002449718005\n",
      "Epoch 208, Loss: 0.4710910670518786\n",
      "Epoch 209, Loss: 0.47102495326923743\n",
      "Epoch 210, Loss: 0.4709626586360175\n",
      "Epoch 211, Loss: 0.47085840979895394\n",
      "Epoch 212, Loss: 0.47078064719388485\n",
      "Epoch 213, Loss: 0.47066614789276784\n",
      "Epoch 214, Loss: 0.47055654499036753\n",
      "Epoch 215, Loss: 0.47041652390832933\n",
      "Epoch 216, Loss: 0.4702446473560784\n",
      "Epoch 217, Loss: 0.4701045016774931\n",
      "Epoch 218, Loss: 0.47002059077245106\n",
      "Epoch 219, Loss: 0.4699643617672827\n",
      "Epoch 220, Loss: 0.4698888296176042\n",
      "Epoch 221, Loss: 0.4698141575467262\n",
      "Epoch 222, Loss: 0.4697265783615268\n",
      "Epoch 223, Loss: 0.46966066909468496\n",
      "Epoch 224, Loss: 0.4696348739248034\n",
      "Epoch 225, Loss: 0.4697176006517021\n",
      "Epoch 226, Loss: 0.4699424675720642\n",
      "Epoch 227, Loss: 0.46999269112017344\n",
      "Epoch 228, Loss: 0.4698759384724347\n",
      "Epoch 229, Loss: 0.46938041574678274\n",
      "Epoch 230, Loss: 0.4691415581864629\n",
      "Epoch 231, Loss: 0.46924839544758656\n",
      "Epoch 232, Loss: 0.46933531775018905\n",
      "Epoch 233, Loss: 0.4692253881106229\n",
      "Epoch 234, Loss: 0.46894320638835874\n",
      "Epoch 235, Loss: 0.4688628060980674\n",
      "Epoch 236, Loss: 0.46897798398181567\n",
      "Epoch 237, Loss: 0.4689811295666005\n",
      "Epoch 238, Loss: 0.4688031484354944\n",
      "Epoch 239, Loss: 0.46862983752908405\n",
      "Epoch 240, Loss: 0.4685963696810109\n",
      "Epoch 241, Loss: 0.46867106090894\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.21882773243615902\n",
      "Valence RMSE: 0.2060922650072601\n",
      "Arousal RMSE: 0.23086171460525817\n",
      "Test R^2 score: tensor([0.4853, 0.4723], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.3648, 0.3488], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.47876317820506403\n",
      "Num of epochs: 242\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Epoch 121, Loss: 0.49295912913079615\n",
      "Epoch 122, Loss: 0.4924773619139838\n",
      "Epoch 123, Loss: 0.49197607191949805\n",
      "Epoch 124, Loss: 0.4914699197831528\n",
      "Epoch 125, Loss: 0.49091483381312667\n",
      "Epoch 126, Loss: 0.49035147677946317\n",
      "Epoch 127, Loss: 0.4898087222825037\n",
      "Epoch 128, Loss: 0.48924014731550836\n",
      "Epoch 129, Loss: 0.48873550647212044\n",
      "Epoch 130, Loss: 0.48832307254971563\n",
      "Epoch 131, Loss: 0.48791418392821295\n",
      "Epoch 132, Loss: 0.4875902245243796\n",
      "Epoch 133, Loss: 0.48728851108037585\n",
      "Epoch 134, Loss: 0.48697062262181345\n",
      "Epoch 135, Loss: 0.4866339553030548\n",
      "Epoch 136, Loss: 0.48633446746734893\n",
      "Epoch 137, Loss: 0.4860310087358514\n",
      "Epoch 138, Loss: 0.4856786412545615\n",
      "Epoch 139, Loss: 0.48531372107004\n",
      "Epoch 140, Loss: 0.48495712985488587\n",
      "Epoch 141, Loss: 0.4846131600652309\n",
      "Epoch 142, Loss: 0.48425966858652364\n",
      "Epoch 143, Loss: 0.4838914149236435\n",
      "Epoch 144, Loss: 0.48352437546282767\n",
      "Epoch 145, Loss: 0.4832299911937133\n",
      "Epoch 146, Loss: 0.4832586529465723\n",
      "Epoch 147, Loss: 0.48356744143113517\n",
      "Epoch 148, Loss: 0.4828108019011922\n",
      "Epoch 149, Loss: 0.4818912939738211\n",
      "Epoch 150, Loss: 0.48239416745575797\n",
      "Epoch 151, Loss: 0.4814952491721961\n",
      "Epoch 152, Loss: 0.4811822818820502\n",
      "Epoch 153, Loss: 0.4812753156716337\n",
      "Epoch 154, Loss: 0.4804387742282827\n",
      "Epoch 155, Loss: 0.48058189052417905\n",
      "Epoch 156, Loss: 0.4801288115497346\n",
      "Epoch 157, Loss: 0.4797465937246525\n",
      "Epoch 158, Loss: 0.4797622479529733\n",
      "Epoch 159, Loss: 0.4791664679844762\n",
      "Epoch 160, Loss: 0.47904359923301915\n",
      "Epoch 161, Loss: 0.4788483533353555\n",
      "Epoch 162, Loss: 0.4783709093142749\n",
      "Epoch 163, Loss: 0.4783027642591491\n",
      "Epoch 164, Loss: 0.4780003946284736\n",
      "Epoch 165, Loss: 0.47766356610156796\n",
      "Epoch 166, Loss: 0.47756940749162413\n",
      "Epoch 167, Loss: 0.4772720526560437\n",
      "Epoch 168, Loss: 0.477001191512605\n",
      "Epoch 169, Loss: 0.47690651155802954\n",
      "Epoch 170, Loss: 0.47663605231839845\n",
      "Epoch 171, Loss: 0.4763715079887003\n",
      "Epoch 172, Loss: 0.4762479807342815\n",
      "Epoch 173, Loss: 0.47603155482909787\n",
      "Epoch 174, Loss: 0.47580085926956445\n",
      "Epoch 175, Loss: 0.4756396378860013\n",
      "Epoch 176, Loss: 0.4754627232666358\n",
      "Epoch 177, Loss: 0.4752513954316692\n",
      "Epoch 178, Loss: 0.47507140350730925\n",
      "Epoch 179, Loss: 0.47492471273973497\n",
      "Epoch 180, Loss: 0.47475493910732763\n",
      "Epoch 181, Loss: 0.47455157018218247\n",
      "Epoch 182, Loss: 0.4743853224635176\n",
      "Epoch 183, Loss: 0.47424550491599726\n",
      "Epoch 184, Loss: 0.47408413178207975\n",
      "Epoch 185, Loss: 0.47390385374022453\n",
      "Epoch 186, Loss: 0.4737383066259127\n",
      "Epoch 187, Loss: 0.47358797785338436\n",
      "Epoch 188, Loss: 0.4734560449826861\n",
      "Epoch 189, Loss: 0.47335738206045025\n",
      "Epoch 190, Loss: 0.4733162834925985\n",
      "Epoch 191, Loss: 0.4732413020737725\n",
      "Epoch 192, Loss: 0.47313239020014447\n",
      "Epoch 193, Loss: 0.4729457158004841\n",
      "Epoch 194, Loss: 0.47274703738102797\n",
      "Epoch 195, Loss: 0.47253750657270627\n",
      "Epoch 196, Loss: 0.4723544299968521\n",
      "Epoch 197, Loss: 0.47221102907746554\n",
      "Epoch 198, Loss: 0.47210844467503205\n",
      "Epoch 199, Loss: 0.47205337980155637\n",
      "Epoch 200, Loss: 0.47204973383515075\n",
      "Epoch 201, Loss: 0.4720427890593303\n",
      "Epoch 202, Loss: 0.4719159657967507\n",
      "Epoch 203, Loss: 0.471735017109987\n",
      "Epoch 204, Loss: 0.4714985533244086\n",
      "Epoch 205, Loss: 0.4713143302002759\n",
      "Epoch 206, Loss: 0.47118574544243713\n",
      "Epoch 207, Loss: 0.47112002449718005\n",
      "Epoch 208, Loss: 0.4710910670518786\n",
      "Epoch 209, Loss: 0.47102495326923743\n",
      "Epoch 210, Loss: 0.4709626586360175\n",
      "Epoch 211, Loss: 0.47085840979895394\n",
      "Epoch 212, Loss: 0.47078064719388485\n",
      "Epoch 213, Loss: 0.47066614789276784\n",
      "Epoch 214, Loss: 0.47055654499036753\n",
      "Epoch 215, Loss: 0.47041652390832933\n",
      "Epoch 216, Loss: 0.4702446473560784\n",
      "Epoch 217, Loss: 0.4701045016774931\n",
      "Epoch 218, Loss: 0.47002059077245106\n",
      "Epoch 219, Loss: 0.4699643617672827\n",
      "Epoch 220, Loss: 0.4698888296176042\n",
      "Epoch 221, Loss: 0.4698141575467262\n",
      "Epoch 222, Loss: 0.4697265783615268\n",
      "Epoch 223, Loss: 0.46966066909468496\n",
      "Epoch 224, Loss: 0.4696348739248034\n",
      "Epoch 225, Loss: 0.4697176006517021\n",
      "Epoch 226, Loss: 0.4699424675720642\n",
      "Epoch 227, Loss: 0.46999269112017344\n",
      "Epoch 228, Loss: 0.4698759384724347\n",
      "Epoch 229, Loss: 0.46938041574678274\n",
      "Epoch 230, Loss: 0.4691415581864629\n",
      "Epoch 231, Loss: 0.46924839544758656\n",
      "Epoch 232, Loss: 0.46933531775018905\n",
      "Epoch 233, Loss: 0.4692253881106229\n",
      "Epoch 234, Loss: 0.46894320638835874\n",
      "Epoch 235, Loss: 0.4688628060980674\n",
      "Epoch 236, Loss: 0.46897798398181567\n",
      "Epoch 237, Loss: 0.4689811295666005\n",
      "Epoch 238, Loss: 0.4688031484354944\n",
      "Epoch 239, Loss: 0.46862983752908405\n",
      "Epoch 240, Loss: 0.4685963696810109\n",
      "Epoch 241, Loss: 0.46867106090894\n",
      "Epoch 242, Loss: 0.46866360503981047\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2198330016845729\n",
      "Valence RMSE: 0.20711508102933845\n",
      "Arousal RMSE: 0.2318543518450957\n",
      "Test R^2 score: tensor([0.4801, 0.4677], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.3585, 0.3431], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4739282553179197\n",
      "Num of epochs: 243\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Epoch 121, Loss: 0.49295912913079615\n",
      "Epoch 122, Loss: 0.4924773619139838\n",
      "Epoch 123, Loss: 0.49197607191949805\n",
      "Epoch 124, Loss: 0.4914699197831528\n",
      "Epoch 125, Loss: 0.49091483381312667\n",
      "Epoch 126, Loss: 0.49035147677946317\n",
      "Epoch 127, Loss: 0.4898087222825037\n",
      "Epoch 128, Loss: 0.48924014731550836\n",
      "Epoch 129, Loss: 0.48873550647212044\n",
      "Epoch 130, Loss: 0.48832307254971563\n",
      "Epoch 131, Loss: 0.48791418392821295\n",
      "Epoch 132, Loss: 0.4875902245243796\n",
      "Epoch 133, Loss: 0.48728851108037585\n",
      "Epoch 134, Loss: 0.48697062262181345\n",
      "Epoch 135, Loss: 0.4866339553030548\n",
      "Epoch 136, Loss: 0.48633446746734893\n",
      "Epoch 137, Loss: 0.4860310087358514\n",
      "Epoch 138, Loss: 0.4856786412545615\n",
      "Epoch 139, Loss: 0.48531372107004\n",
      "Epoch 140, Loss: 0.48495712985488587\n",
      "Epoch 141, Loss: 0.4846131600652309\n",
      "Epoch 142, Loss: 0.48425966858652364\n",
      "Epoch 143, Loss: 0.4838914149236435\n",
      "Epoch 144, Loss: 0.48352437546282767\n",
      "Epoch 145, Loss: 0.4832299911937133\n",
      "Epoch 146, Loss: 0.4832586529465723\n",
      "Epoch 147, Loss: 0.48356744143113517\n",
      "Epoch 148, Loss: 0.4828108019011922\n",
      "Epoch 149, Loss: 0.4818912939738211\n",
      "Epoch 150, Loss: 0.48239416745575797\n",
      "Epoch 151, Loss: 0.4814952491721961\n",
      "Epoch 152, Loss: 0.4811822818820502\n",
      "Epoch 153, Loss: 0.4812753156716337\n",
      "Epoch 154, Loss: 0.4804387742282827\n",
      "Epoch 155, Loss: 0.48058189052417905\n",
      "Epoch 156, Loss: 0.4801288115497346\n",
      "Epoch 157, Loss: 0.4797465937246525\n",
      "Epoch 158, Loss: 0.4797622479529733\n",
      "Epoch 159, Loss: 0.4791664679844762\n",
      "Epoch 160, Loss: 0.47904359923301915\n",
      "Epoch 161, Loss: 0.4788483533353555\n",
      "Epoch 162, Loss: 0.4783709093142749\n",
      "Epoch 163, Loss: 0.4783027642591491\n",
      "Epoch 164, Loss: 0.4780003946284736\n",
      "Epoch 165, Loss: 0.47766356610156796\n",
      "Epoch 166, Loss: 0.47756940749162413\n",
      "Epoch 167, Loss: 0.4772720526560437\n",
      "Epoch 168, Loss: 0.477001191512605\n",
      "Epoch 169, Loss: 0.47690651155802954\n",
      "Epoch 170, Loss: 0.47663605231839845\n",
      "Epoch 171, Loss: 0.4763715079887003\n",
      "Epoch 172, Loss: 0.4762479807342815\n",
      "Epoch 173, Loss: 0.47603155482909787\n",
      "Epoch 174, Loss: 0.47580085926956445\n",
      "Epoch 175, Loss: 0.4756396378860013\n",
      "Epoch 176, Loss: 0.4754627232666358\n",
      "Epoch 177, Loss: 0.4752513954316692\n",
      "Epoch 178, Loss: 0.47507140350730925\n",
      "Epoch 179, Loss: 0.47492471273973497\n",
      "Epoch 180, Loss: 0.47475493910732763\n",
      "Epoch 181, Loss: 0.47455157018218247\n",
      "Epoch 182, Loss: 0.4743853224635176\n",
      "Epoch 183, Loss: 0.47424550491599726\n",
      "Epoch 184, Loss: 0.47408413178207975\n",
      "Epoch 185, Loss: 0.47390385374022453\n",
      "Epoch 186, Loss: 0.4737383066259127\n",
      "Epoch 187, Loss: 0.47358797785338436\n",
      "Epoch 188, Loss: 0.4734560449826861\n",
      "Epoch 189, Loss: 0.47335738206045025\n",
      "Epoch 190, Loss: 0.4733162834925985\n",
      "Epoch 191, Loss: 0.4732413020737725\n",
      "Epoch 192, Loss: 0.47313239020014447\n",
      "Epoch 193, Loss: 0.4729457158004841\n",
      "Epoch 194, Loss: 0.47274703738102797\n",
      "Epoch 195, Loss: 0.47253750657270627\n",
      "Epoch 196, Loss: 0.4723544299968521\n",
      "Epoch 197, Loss: 0.47221102907746554\n",
      "Epoch 198, Loss: 0.47210844467503205\n",
      "Epoch 199, Loss: 0.47205337980155637\n",
      "Epoch 200, Loss: 0.47204973383515075\n",
      "Epoch 201, Loss: 0.4720427890593303\n",
      "Epoch 202, Loss: 0.4719159657967507\n",
      "Epoch 203, Loss: 0.471735017109987\n",
      "Epoch 204, Loss: 0.4714985533244086\n",
      "Epoch 205, Loss: 0.4713143302002759\n",
      "Epoch 206, Loss: 0.47118574544243713\n",
      "Epoch 207, Loss: 0.47112002449718005\n",
      "Epoch 208, Loss: 0.4710910670518786\n",
      "Epoch 209, Loss: 0.47102495326923743\n",
      "Epoch 210, Loss: 0.4709626586360175\n",
      "Epoch 211, Loss: 0.47085840979895394\n",
      "Epoch 212, Loss: 0.47078064719388485\n",
      "Epoch 213, Loss: 0.47066614789276784\n",
      "Epoch 214, Loss: 0.47055654499036753\n",
      "Epoch 215, Loss: 0.47041652390832933\n",
      "Epoch 216, Loss: 0.4702446473560784\n",
      "Epoch 217, Loss: 0.4701045016774931\n",
      "Epoch 218, Loss: 0.47002059077245106\n",
      "Epoch 219, Loss: 0.4699643617672827\n",
      "Epoch 220, Loss: 0.4698888296176042\n",
      "Epoch 221, Loss: 0.4698141575467262\n",
      "Epoch 222, Loss: 0.4697265783615268\n",
      "Epoch 223, Loss: 0.46966066909468496\n",
      "Epoch 224, Loss: 0.4696348739248034\n",
      "Epoch 225, Loss: 0.4697176006517021\n",
      "Epoch 226, Loss: 0.4699424675720642\n",
      "Epoch 227, Loss: 0.46999269112017344\n",
      "Epoch 228, Loss: 0.4698759384724347\n",
      "Epoch 229, Loss: 0.46938041574678274\n",
      "Epoch 230, Loss: 0.4691415581864629\n",
      "Epoch 231, Loss: 0.46924839544758656\n",
      "Epoch 232, Loss: 0.46933531775018905\n",
      "Epoch 233, Loss: 0.4692253881106229\n",
      "Epoch 234, Loss: 0.46894320638835874\n",
      "Epoch 235, Loss: 0.4688628060980674\n",
      "Epoch 236, Loss: 0.46897798398181567\n",
      "Epoch 237, Loss: 0.4689811295666005\n",
      "Epoch 238, Loss: 0.4688031484354944\n",
      "Epoch 239, Loss: 0.46862983752908405\n",
      "Epoch 240, Loss: 0.4685963696810109\n",
      "Epoch 241, Loss: 0.46867106090894\n",
      "Epoch 242, Loss: 0.46866360503981047\n",
      "Epoch 243, Loss: 0.468513635806399\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.21912167928667348\n",
      "Valence RMSE: 0.20641800988462997\n",
      "Arousal RMSE: 0.23112815895535616\n",
      "Test R^2 score: tensor([0.4836, 0.4711], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.3628, 0.3473], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4773395229853463\n",
      "Num of epochs: 244\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Epoch 121, Loss: 0.49295912913079615\n",
      "Epoch 122, Loss: 0.4924773619139838\n",
      "Epoch 123, Loss: 0.49197607191949805\n",
      "Epoch 124, Loss: 0.4914699197831528\n",
      "Epoch 125, Loss: 0.49091483381312667\n",
      "Epoch 126, Loss: 0.49035147677946317\n",
      "Epoch 127, Loss: 0.4898087222825037\n",
      "Epoch 128, Loss: 0.48924014731550836\n",
      "Epoch 129, Loss: 0.48873550647212044\n",
      "Epoch 130, Loss: 0.48832307254971563\n",
      "Epoch 131, Loss: 0.48791418392821295\n",
      "Epoch 132, Loss: 0.4875902245243796\n",
      "Epoch 133, Loss: 0.48728851108037585\n",
      "Epoch 134, Loss: 0.48697062262181345\n",
      "Epoch 135, Loss: 0.4866339553030548\n",
      "Epoch 136, Loss: 0.48633446746734893\n",
      "Epoch 137, Loss: 0.4860310087358514\n",
      "Epoch 138, Loss: 0.4856786412545615\n",
      "Epoch 139, Loss: 0.48531372107004\n",
      "Epoch 140, Loss: 0.48495712985488587\n",
      "Epoch 141, Loss: 0.4846131600652309\n",
      "Epoch 142, Loss: 0.48425966858652364\n",
      "Epoch 143, Loss: 0.4838914149236435\n",
      "Epoch 144, Loss: 0.48352437546282767\n",
      "Epoch 145, Loss: 0.4832299911937133\n",
      "Epoch 146, Loss: 0.4832586529465723\n",
      "Epoch 147, Loss: 0.48356744143113517\n",
      "Epoch 148, Loss: 0.4828108019011922\n",
      "Epoch 149, Loss: 0.4818912939738211\n",
      "Epoch 150, Loss: 0.48239416745575797\n",
      "Epoch 151, Loss: 0.4814952491721961\n",
      "Epoch 152, Loss: 0.4811822818820502\n",
      "Epoch 153, Loss: 0.4812753156716337\n",
      "Epoch 154, Loss: 0.4804387742282827\n",
      "Epoch 155, Loss: 0.48058189052417905\n",
      "Epoch 156, Loss: 0.4801288115497346\n",
      "Epoch 157, Loss: 0.4797465937246525\n",
      "Epoch 158, Loss: 0.4797622479529733\n",
      "Epoch 159, Loss: 0.4791664679844762\n",
      "Epoch 160, Loss: 0.47904359923301915\n",
      "Epoch 161, Loss: 0.4788483533353555\n",
      "Epoch 162, Loss: 0.4783709093142749\n",
      "Epoch 163, Loss: 0.4783027642591491\n",
      "Epoch 164, Loss: 0.4780003946284736\n",
      "Epoch 165, Loss: 0.47766356610156796\n",
      "Epoch 166, Loss: 0.47756940749162413\n",
      "Epoch 167, Loss: 0.4772720526560437\n",
      "Epoch 168, Loss: 0.477001191512605\n",
      "Epoch 169, Loss: 0.47690651155802954\n",
      "Epoch 170, Loss: 0.47663605231839845\n",
      "Epoch 171, Loss: 0.4763715079887003\n",
      "Epoch 172, Loss: 0.4762479807342815\n",
      "Epoch 173, Loss: 0.47603155482909787\n",
      "Epoch 174, Loss: 0.47580085926956445\n",
      "Epoch 175, Loss: 0.4756396378860013\n",
      "Epoch 176, Loss: 0.4754627232666358\n",
      "Epoch 177, Loss: 0.4752513954316692\n",
      "Epoch 178, Loss: 0.47507140350730925\n",
      "Epoch 179, Loss: 0.47492471273973497\n",
      "Epoch 180, Loss: 0.47475493910732763\n",
      "Epoch 181, Loss: 0.47455157018218247\n",
      "Epoch 182, Loss: 0.4743853224635176\n",
      "Epoch 183, Loss: 0.47424550491599726\n",
      "Epoch 184, Loss: 0.47408413178207975\n",
      "Epoch 185, Loss: 0.47390385374022453\n",
      "Epoch 186, Loss: 0.4737383066259127\n",
      "Epoch 187, Loss: 0.47358797785338436\n",
      "Epoch 188, Loss: 0.4734560449826861\n",
      "Epoch 189, Loss: 0.47335738206045025\n",
      "Epoch 190, Loss: 0.4733162834925985\n",
      "Epoch 191, Loss: 0.4732413020737725\n",
      "Epoch 192, Loss: 0.47313239020014447\n",
      "Epoch 193, Loss: 0.4729457158004841\n",
      "Epoch 194, Loss: 0.47274703738102797\n",
      "Epoch 195, Loss: 0.47253750657270627\n",
      "Epoch 196, Loss: 0.4723544299968521\n",
      "Epoch 197, Loss: 0.47221102907746554\n",
      "Epoch 198, Loss: 0.47210844467503205\n",
      "Epoch 199, Loss: 0.47205337980155637\n",
      "Epoch 200, Loss: 0.47204973383515075\n",
      "Epoch 201, Loss: 0.4720427890593303\n",
      "Epoch 202, Loss: 0.4719159657967507\n",
      "Epoch 203, Loss: 0.471735017109987\n",
      "Epoch 204, Loss: 0.4714985533244086\n",
      "Epoch 205, Loss: 0.4713143302002759\n",
      "Epoch 206, Loss: 0.47118574544243713\n",
      "Epoch 207, Loss: 0.47112002449718005\n",
      "Epoch 208, Loss: 0.4710910670518786\n",
      "Epoch 209, Loss: 0.47102495326923743\n",
      "Epoch 210, Loss: 0.4709626586360175\n",
      "Epoch 211, Loss: 0.47085840979895394\n",
      "Epoch 212, Loss: 0.47078064719388485\n",
      "Epoch 213, Loss: 0.47066614789276784\n",
      "Epoch 214, Loss: 0.47055654499036753\n",
      "Epoch 215, Loss: 0.47041652390832933\n",
      "Epoch 216, Loss: 0.4702446473560784\n",
      "Epoch 217, Loss: 0.4701045016774931\n",
      "Epoch 218, Loss: 0.47002059077245106\n",
      "Epoch 219, Loss: 0.4699643617672827\n",
      "Epoch 220, Loss: 0.4698888296176042\n",
      "Epoch 221, Loss: 0.4698141575467262\n",
      "Epoch 222, Loss: 0.4697265783615268\n",
      "Epoch 223, Loss: 0.46966066909468496\n",
      "Epoch 224, Loss: 0.4696348739248034\n",
      "Epoch 225, Loss: 0.4697176006517021\n",
      "Epoch 226, Loss: 0.4699424675720642\n",
      "Epoch 227, Loss: 0.46999269112017344\n",
      "Epoch 228, Loss: 0.4698759384724347\n",
      "Epoch 229, Loss: 0.46938041574678274\n",
      "Epoch 230, Loss: 0.4691415581864629\n",
      "Epoch 231, Loss: 0.46924839544758656\n",
      "Epoch 232, Loss: 0.46933531775018905\n",
      "Epoch 233, Loss: 0.4692253881106229\n",
      "Epoch 234, Loss: 0.46894320638835874\n",
      "Epoch 235, Loss: 0.4688628060980674\n",
      "Epoch 236, Loss: 0.46897798398181567\n",
      "Epoch 237, Loss: 0.4689811295666005\n",
      "Epoch 238, Loss: 0.4688031484354944\n",
      "Epoch 239, Loss: 0.46862983752908405\n",
      "Epoch 240, Loss: 0.4685963696810109\n",
      "Epoch 241, Loss: 0.46867106090894\n",
      "Epoch 242, Loss: 0.46866360503981047\n",
      "Epoch 243, Loss: 0.468513635806399\n",
      "Epoch 244, Loss: 0.46835730315983837\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2189099643850402\n",
      "Valence RMSE: 0.20620753973265596\n",
      "Arousal RMSE: 0.23091469327767838\n",
      "Test R^2 score: tensor([0.4847, 0.4720], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.3641, 0.3485], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.47835406507673056\n",
      "Num of epochs: 245\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Epoch 121, Loss: 0.49295912913079615\n",
      "Epoch 122, Loss: 0.4924773619139838\n",
      "Epoch 123, Loss: 0.49197607191949805\n",
      "Epoch 124, Loss: 0.4914699197831528\n",
      "Epoch 125, Loss: 0.49091483381312667\n",
      "Epoch 126, Loss: 0.49035147677946317\n",
      "Epoch 127, Loss: 0.4898087222825037\n",
      "Epoch 128, Loss: 0.48924014731550836\n",
      "Epoch 129, Loss: 0.48873550647212044\n",
      "Epoch 130, Loss: 0.48832307254971563\n",
      "Epoch 131, Loss: 0.48791418392821295\n",
      "Epoch 132, Loss: 0.4875902245243796\n",
      "Epoch 133, Loss: 0.48728851108037585\n",
      "Epoch 134, Loss: 0.48697062262181345\n",
      "Epoch 135, Loss: 0.4866339553030548\n",
      "Epoch 136, Loss: 0.48633446746734893\n",
      "Epoch 137, Loss: 0.4860310087358514\n",
      "Epoch 138, Loss: 0.4856786412545615\n",
      "Epoch 139, Loss: 0.48531372107004\n",
      "Epoch 140, Loss: 0.48495712985488587\n",
      "Epoch 141, Loss: 0.4846131600652309\n",
      "Epoch 142, Loss: 0.48425966858652364\n",
      "Epoch 143, Loss: 0.4838914149236435\n",
      "Epoch 144, Loss: 0.48352437546282767\n",
      "Epoch 145, Loss: 0.4832299911937133\n",
      "Epoch 146, Loss: 0.4832586529465723\n",
      "Epoch 147, Loss: 0.48356744143113517\n",
      "Epoch 148, Loss: 0.4828108019011922\n",
      "Epoch 149, Loss: 0.4818912939738211\n",
      "Epoch 150, Loss: 0.48239416745575797\n",
      "Epoch 151, Loss: 0.4814952491721961\n",
      "Epoch 152, Loss: 0.4811822818820502\n",
      "Epoch 153, Loss: 0.4812753156716337\n",
      "Epoch 154, Loss: 0.4804387742282827\n",
      "Epoch 155, Loss: 0.48058189052417905\n",
      "Epoch 156, Loss: 0.4801288115497346\n",
      "Epoch 157, Loss: 0.4797465937246525\n",
      "Epoch 158, Loss: 0.4797622479529733\n",
      "Epoch 159, Loss: 0.4791664679844762\n",
      "Epoch 160, Loss: 0.47904359923301915\n",
      "Epoch 161, Loss: 0.4788483533353555\n",
      "Epoch 162, Loss: 0.4783709093142749\n",
      "Epoch 163, Loss: 0.4783027642591491\n",
      "Epoch 164, Loss: 0.4780003946284736\n",
      "Epoch 165, Loss: 0.47766356610156796\n",
      "Epoch 166, Loss: 0.47756940749162413\n",
      "Epoch 167, Loss: 0.4772720526560437\n",
      "Epoch 168, Loss: 0.477001191512605\n",
      "Epoch 169, Loss: 0.47690651155802954\n",
      "Epoch 170, Loss: 0.47663605231839845\n",
      "Epoch 171, Loss: 0.4763715079887003\n",
      "Epoch 172, Loss: 0.4762479807342815\n",
      "Epoch 173, Loss: 0.47603155482909787\n",
      "Epoch 174, Loss: 0.47580085926956445\n",
      "Epoch 175, Loss: 0.4756396378860013\n",
      "Epoch 176, Loss: 0.4754627232666358\n",
      "Epoch 177, Loss: 0.4752513954316692\n",
      "Epoch 178, Loss: 0.47507140350730925\n",
      "Epoch 179, Loss: 0.47492471273973497\n",
      "Epoch 180, Loss: 0.47475493910732763\n",
      "Epoch 181, Loss: 0.47455157018218247\n",
      "Epoch 182, Loss: 0.4743853224635176\n",
      "Epoch 183, Loss: 0.47424550491599726\n",
      "Epoch 184, Loss: 0.47408413178207975\n",
      "Epoch 185, Loss: 0.47390385374022453\n",
      "Epoch 186, Loss: 0.4737383066259127\n",
      "Epoch 187, Loss: 0.47358797785338436\n",
      "Epoch 188, Loss: 0.4734560449826861\n",
      "Epoch 189, Loss: 0.47335738206045025\n",
      "Epoch 190, Loss: 0.4733162834925985\n",
      "Epoch 191, Loss: 0.4732413020737725\n",
      "Epoch 192, Loss: 0.47313239020014447\n",
      "Epoch 193, Loss: 0.4729457158004841\n",
      "Epoch 194, Loss: 0.47274703738102797\n",
      "Epoch 195, Loss: 0.47253750657270627\n",
      "Epoch 196, Loss: 0.4723544299968521\n",
      "Epoch 197, Loss: 0.47221102907746554\n",
      "Epoch 198, Loss: 0.47210844467503205\n",
      "Epoch 199, Loss: 0.47205337980155637\n",
      "Epoch 200, Loss: 0.47204973383515075\n",
      "Epoch 201, Loss: 0.4720427890593303\n",
      "Epoch 202, Loss: 0.4719159657967507\n",
      "Epoch 203, Loss: 0.471735017109987\n",
      "Epoch 204, Loss: 0.4714985533244086\n",
      "Epoch 205, Loss: 0.4713143302002759\n",
      "Epoch 206, Loss: 0.47118574544243713\n",
      "Epoch 207, Loss: 0.47112002449718005\n",
      "Epoch 208, Loss: 0.4710910670518786\n",
      "Epoch 209, Loss: 0.47102495326923743\n",
      "Epoch 210, Loss: 0.4709626586360175\n",
      "Epoch 211, Loss: 0.47085840979895394\n",
      "Epoch 212, Loss: 0.47078064719388485\n",
      "Epoch 213, Loss: 0.47066614789276784\n",
      "Epoch 214, Loss: 0.47055654499036753\n",
      "Epoch 215, Loss: 0.47041652390832933\n",
      "Epoch 216, Loss: 0.4702446473560784\n",
      "Epoch 217, Loss: 0.4701045016774931\n",
      "Epoch 218, Loss: 0.47002059077245106\n",
      "Epoch 219, Loss: 0.4699643617672827\n",
      "Epoch 220, Loss: 0.4698888296176042\n",
      "Epoch 221, Loss: 0.4698141575467262\n",
      "Epoch 222, Loss: 0.4697265783615268\n",
      "Epoch 223, Loss: 0.46966066909468496\n",
      "Epoch 224, Loss: 0.4696348739248034\n",
      "Epoch 225, Loss: 0.4697176006517021\n",
      "Epoch 226, Loss: 0.4699424675720642\n",
      "Epoch 227, Loss: 0.46999269112017344\n",
      "Epoch 228, Loss: 0.4698759384724347\n",
      "Epoch 229, Loss: 0.46938041574678274\n",
      "Epoch 230, Loss: 0.4691415581864629\n",
      "Epoch 231, Loss: 0.46924839544758656\n",
      "Epoch 232, Loss: 0.46933531775018905\n",
      "Epoch 233, Loss: 0.4692253881106229\n",
      "Epoch 234, Loss: 0.46894320638835874\n",
      "Epoch 235, Loss: 0.4688628060980674\n",
      "Epoch 236, Loss: 0.46897798398181567\n",
      "Epoch 237, Loss: 0.4689811295666005\n",
      "Epoch 238, Loss: 0.4688031484354944\n",
      "Epoch 239, Loss: 0.46862983752908405\n",
      "Epoch 240, Loss: 0.4685963696810109\n",
      "Epoch 241, Loss: 0.46867106090894\n",
      "Epoch 242, Loss: 0.46866360503981047\n",
      "Epoch 243, Loss: 0.468513635806399\n",
      "Epoch 244, Loss: 0.46835730315983837\n",
      "Epoch 245, Loss: 0.468344910742699\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.21991049922017106\n",
      "Valence RMSE: 0.20720275498270993\n",
      "Arousal RMSE: 0.23192299080105183\n",
      "Test R^2 score: tensor([0.4797, 0.4674], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.3579, 0.3428], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4735505437191591\n",
      "Num of epochs: 246\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Epoch 121, Loss: 0.49295912913079615\n",
      "Epoch 122, Loss: 0.4924773619139838\n",
      "Epoch 123, Loss: 0.49197607191949805\n",
      "Epoch 124, Loss: 0.4914699197831528\n",
      "Epoch 125, Loss: 0.49091483381312667\n",
      "Epoch 126, Loss: 0.49035147677946317\n",
      "Epoch 127, Loss: 0.4898087222825037\n",
      "Epoch 128, Loss: 0.48924014731550836\n",
      "Epoch 129, Loss: 0.48873550647212044\n",
      "Epoch 130, Loss: 0.48832307254971563\n",
      "Epoch 131, Loss: 0.48791418392821295\n",
      "Epoch 132, Loss: 0.4875902245243796\n",
      "Epoch 133, Loss: 0.48728851108037585\n",
      "Epoch 134, Loss: 0.48697062262181345\n",
      "Epoch 135, Loss: 0.4866339553030548\n",
      "Epoch 136, Loss: 0.48633446746734893\n",
      "Epoch 137, Loss: 0.4860310087358514\n",
      "Epoch 138, Loss: 0.4856786412545615\n",
      "Epoch 139, Loss: 0.48531372107004\n",
      "Epoch 140, Loss: 0.48495712985488587\n",
      "Epoch 141, Loss: 0.4846131600652309\n",
      "Epoch 142, Loss: 0.48425966858652364\n",
      "Epoch 143, Loss: 0.4838914149236435\n",
      "Epoch 144, Loss: 0.48352437546282767\n",
      "Epoch 145, Loss: 0.4832299911937133\n",
      "Epoch 146, Loss: 0.4832586529465723\n",
      "Epoch 147, Loss: 0.48356744143113517\n",
      "Epoch 148, Loss: 0.4828108019011922\n",
      "Epoch 149, Loss: 0.4818912939738211\n",
      "Epoch 150, Loss: 0.48239416745575797\n",
      "Epoch 151, Loss: 0.4814952491721961\n",
      "Epoch 152, Loss: 0.4811822818820502\n",
      "Epoch 153, Loss: 0.4812753156716337\n",
      "Epoch 154, Loss: 0.4804387742282827\n",
      "Epoch 155, Loss: 0.48058189052417905\n",
      "Epoch 156, Loss: 0.4801288115497346\n",
      "Epoch 157, Loss: 0.4797465937246525\n",
      "Epoch 158, Loss: 0.4797622479529733\n",
      "Epoch 159, Loss: 0.4791664679844762\n",
      "Epoch 160, Loss: 0.47904359923301915\n",
      "Epoch 161, Loss: 0.4788483533353555\n",
      "Epoch 162, Loss: 0.4783709093142749\n",
      "Epoch 163, Loss: 0.4783027642591491\n",
      "Epoch 164, Loss: 0.4780003946284736\n",
      "Epoch 165, Loss: 0.47766356610156796\n",
      "Epoch 166, Loss: 0.47756940749162413\n",
      "Epoch 167, Loss: 0.4772720526560437\n",
      "Epoch 168, Loss: 0.477001191512605\n",
      "Epoch 169, Loss: 0.47690651155802954\n",
      "Epoch 170, Loss: 0.47663605231839845\n",
      "Epoch 171, Loss: 0.4763715079887003\n",
      "Epoch 172, Loss: 0.4762479807342815\n",
      "Epoch 173, Loss: 0.47603155482909787\n",
      "Epoch 174, Loss: 0.47580085926956445\n",
      "Epoch 175, Loss: 0.4756396378860013\n",
      "Epoch 176, Loss: 0.4754627232666358\n",
      "Epoch 177, Loss: 0.4752513954316692\n",
      "Epoch 178, Loss: 0.47507140350730925\n",
      "Epoch 179, Loss: 0.47492471273973497\n",
      "Epoch 180, Loss: 0.47475493910732763\n",
      "Epoch 181, Loss: 0.47455157018218247\n",
      "Epoch 182, Loss: 0.4743853224635176\n",
      "Epoch 183, Loss: 0.47424550491599726\n",
      "Epoch 184, Loss: 0.47408413178207975\n",
      "Epoch 185, Loss: 0.47390385374022453\n",
      "Epoch 186, Loss: 0.4737383066259127\n",
      "Epoch 187, Loss: 0.47358797785338436\n",
      "Epoch 188, Loss: 0.4734560449826861\n",
      "Epoch 189, Loss: 0.47335738206045025\n",
      "Epoch 190, Loss: 0.4733162834925985\n",
      "Epoch 191, Loss: 0.4732413020737725\n",
      "Epoch 192, Loss: 0.47313239020014447\n",
      "Epoch 193, Loss: 0.4729457158004841\n",
      "Epoch 194, Loss: 0.47274703738102797\n",
      "Epoch 195, Loss: 0.47253750657270627\n",
      "Epoch 196, Loss: 0.4723544299968521\n",
      "Epoch 197, Loss: 0.47221102907746554\n",
      "Epoch 198, Loss: 0.47210844467503205\n",
      "Epoch 199, Loss: 0.47205337980155637\n",
      "Epoch 200, Loss: 0.47204973383515075\n",
      "Epoch 201, Loss: 0.4720427890593303\n",
      "Epoch 202, Loss: 0.4719159657967507\n",
      "Epoch 203, Loss: 0.471735017109987\n",
      "Epoch 204, Loss: 0.4714985533244086\n",
      "Epoch 205, Loss: 0.4713143302002759\n",
      "Epoch 206, Loss: 0.47118574544243713\n",
      "Epoch 207, Loss: 0.47112002449718005\n",
      "Epoch 208, Loss: 0.4710910670518786\n",
      "Epoch 209, Loss: 0.47102495326923743\n",
      "Epoch 210, Loss: 0.4709626586360175\n",
      "Epoch 211, Loss: 0.47085840979895394\n",
      "Epoch 212, Loss: 0.47078064719388485\n",
      "Epoch 213, Loss: 0.47066614789276784\n",
      "Epoch 214, Loss: 0.47055654499036753\n",
      "Epoch 215, Loss: 0.47041652390832933\n",
      "Epoch 216, Loss: 0.4702446473560784\n",
      "Epoch 217, Loss: 0.4701045016774931\n",
      "Epoch 218, Loss: 0.47002059077245106\n",
      "Epoch 219, Loss: 0.4699643617672827\n",
      "Epoch 220, Loss: 0.4698888296176042\n",
      "Epoch 221, Loss: 0.4698141575467262\n",
      "Epoch 222, Loss: 0.4697265783615268\n",
      "Epoch 223, Loss: 0.46966066909468496\n",
      "Epoch 224, Loss: 0.4696348739248034\n",
      "Epoch 225, Loss: 0.4697176006517021\n",
      "Epoch 226, Loss: 0.4699424675720642\n",
      "Epoch 227, Loss: 0.46999269112017344\n",
      "Epoch 228, Loss: 0.4698759384724347\n",
      "Epoch 229, Loss: 0.46938041574678274\n",
      "Epoch 230, Loss: 0.4691415581864629\n",
      "Epoch 231, Loss: 0.46924839544758656\n",
      "Epoch 232, Loss: 0.46933531775018905\n",
      "Epoch 233, Loss: 0.4692253881106229\n",
      "Epoch 234, Loss: 0.46894320638835874\n",
      "Epoch 235, Loss: 0.4688628060980674\n",
      "Epoch 236, Loss: 0.46897798398181567\n",
      "Epoch 237, Loss: 0.4689811295666005\n",
      "Epoch 238, Loss: 0.4688031484354944\n",
      "Epoch 239, Loss: 0.46862983752908405\n",
      "Epoch 240, Loss: 0.4685963696810109\n",
      "Epoch 241, Loss: 0.46867106090894\n",
      "Epoch 242, Loss: 0.46866360503981047\n",
      "Epoch 243, Loss: 0.468513635806399\n",
      "Epoch 244, Loss: 0.46835730315983837\n",
      "Epoch 245, Loss: 0.468344910742699\n",
      "Epoch 246, Loss: 0.46842993299443175\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.21865334353750016\n",
      "Valence RMSE: 0.20596364521891836\n",
      "Arousal RMSE: 0.23064593239073533\n",
      "Test R^2 score: tensor([0.4859, 0.4733], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.3656, 0.3500], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4795773528861611\n",
      "Num of epochs: 247\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Epoch 121, Loss: 0.49295912913079615\n",
      "Epoch 122, Loss: 0.4924773619139838\n",
      "Epoch 123, Loss: 0.49197607191949805\n",
      "Epoch 124, Loss: 0.4914699197831528\n",
      "Epoch 125, Loss: 0.49091483381312667\n",
      "Epoch 126, Loss: 0.49035147677946317\n",
      "Epoch 127, Loss: 0.4898087222825037\n",
      "Epoch 128, Loss: 0.48924014731550836\n",
      "Epoch 129, Loss: 0.48873550647212044\n",
      "Epoch 130, Loss: 0.48832307254971563\n",
      "Epoch 131, Loss: 0.48791418392821295\n",
      "Epoch 132, Loss: 0.4875902245243796\n",
      "Epoch 133, Loss: 0.48728851108037585\n",
      "Epoch 134, Loss: 0.48697062262181345\n",
      "Epoch 135, Loss: 0.4866339553030548\n",
      "Epoch 136, Loss: 0.48633446746734893\n",
      "Epoch 137, Loss: 0.4860310087358514\n",
      "Epoch 138, Loss: 0.4856786412545615\n",
      "Epoch 139, Loss: 0.48531372107004\n",
      "Epoch 140, Loss: 0.48495712985488587\n",
      "Epoch 141, Loss: 0.4846131600652309\n",
      "Epoch 142, Loss: 0.48425966858652364\n",
      "Epoch 143, Loss: 0.4838914149236435\n",
      "Epoch 144, Loss: 0.48352437546282767\n",
      "Epoch 145, Loss: 0.4832299911937133\n",
      "Epoch 146, Loss: 0.4832586529465723\n",
      "Epoch 147, Loss: 0.48356744143113517\n",
      "Epoch 148, Loss: 0.4828108019011922\n",
      "Epoch 149, Loss: 0.4818912939738211\n",
      "Epoch 150, Loss: 0.48239416745575797\n",
      "Epoch 151, Loss: 0.4814952491721961\n",
      "Epoch 152, Loss: 0.4811822818820502\n",
      "Epoch 153, Loss: 0.4812753156716337\n",
      "Epoch 154, Loss: 0.4804387742282827\n",
      "Epoch 155, Loss: 0.48058189052417905\n",
      "Epoch 156, Loss: 0.4801288115497346\n",
      "Epoch 157, Loss: 0.4797465937246525\n",
      "Epoch 158, Loss: 0.4797622479529733\n",
      "Epoch 159, Loss: 0.4791664679844762\n",
      "Epoch 160, Loss: 0.47904359923301915\n",
      "Epoch 161, Loss: 0.4788483533353555\n",
      "Epoch 162, Loss: 0.4783709093142749\n",
      "Epoch 163, Loss: 0.4783027642591491\n",
      "Epoch 164, Loss: 0.4780003946284736\n",
      "Epoch 165, Loss: 0.47766356610156796\n",
      "Epoch 166, Loss: 0.47756940749162413\n",
      "Epoch 167, Loss: 0.4772720526560437\n",
      "Epoch 168, Loss: 0.477001191512605\n",
      "Epoch 169, Loss: 0.47690651155802954\n",
      "Epoch 170, Loss: 0.47663605231839845\n",
      "Epoch 171, Loss: 0.4763715079887003\n",
      "Epoch 172, Loss: 0.4762479807342815\n",
      "Epoch 173, Loss: 0.47603155482909787\n",
      "Epoch 174, Loss: 0.47580085926956445\n",
      "Epoch 175, Loss: 0.4756396378860013\n",
      "Epoch 176, Loss: 0.4754627232666358\n",
      "Epoch 177, Loss: 0.4752513954316692\n",
      "Epoch 178, Loss: 0.47507140350730925\n",
      "Epoch 179, Loss: 0.47492471273973497\n",
      "Epoch 180, Loss: 0.47475493910732763\n",
      "Epoch 181, Loss: 0.47455157018218247\n",
      "Epoch 182, Loss: 0.4743853224635176\n",
      "Epoch 183, Loss: 0.47424550491599726\n",
      "Epoch 184, Loss: 0.47408413178207975\n",
      "Epoch 185, Loss: 0.47390385374022453\n",
      "Epoch 186, Loss: 0.4737383066259127\n",
      "Epoch 187, Loss: 0.47358797785338436\n",
      "Epoch 188, Loss: 0.4734560449826861\n",
      "Epoch 189, Loss: 0.47335738206045025\n",
      "Epoch 190, Loss: 0.4733162834925985\n",
      "Epoch 191, Loss: 0.4732413020737725\n",
      "Epoch 192, Loss: 0.47313239020014447\n",
      "Epoch 193, Loss: 0.4729457158004841\n",
      "Epoch 194, Loss: 0.47274703738102797\n",
      "Epoch 195, Loss: 0.47253750657270627\n",
      "Epoch 196, Loss: 0.4723544299968521\n",
      "Epoch 197, Loss: 0.47221102907746554\n",
      "Epoch 198, Loss: 0.47210844467503205\n",
      "Epoch 199, Loss: 0.47205337980155637\n",
      "Epoch 200, Loss: 0.47204973383515075\n",
      "Epoch 201, Loss: 0.4720427890593303\n",
      "Epoch 202, Loss: 0.4719159657967507\n",
      "Epoch 203, Loss: 0.471735017109987\n",
      "Epoch 204, Loss: 0.4714985533244086\n",
      "Epoch 205, Loss: 0.4713143302002759\n",
      "Epoch 206, Loss: 0.47118574544243713\n",
      "Epoch 207, Loss: 0.47112002449718005\n",
      "Epoch 208, Loss: 0.4710910670518786\n",
      "Epoch 209, Loss: 0.47102495326923743\n",
      "Epoch 210, Loss: 0.4709626586360175\n",
      "Epoch 211, Loss: 0.47085840979895394\n",
      "Epoch 212, Loss: 0.47078064719388485\n",
      "Epoch 213, Loss: 0.47066614789276784\n",
      "Epoch 214, Loss: 0.47055654499036753\n",
      "Epoch 215, Loss: 0.47041652390832933\n",
      "Epoch 216, Loss: 0.4702446473560784\n",
      "Epoch 217, Loss: 0.4701045016774931\n",
      "Epoch 218, Loss: 0.47002059077245106\n",
      "Epoch 219, Loss: 0.4699643617672827\n",
      "Epoch 220, Loss: 0.4698888296176042\n",
      "Epoch 221, Loss: 0.4698141575467262\n",
      "Epoch 222, Loss: 0.4697265783615268\n",
      "Epoch 223, Loss: 0.46966066909468496\n",
      "Epoch 224, Loss: 0.4696348739248034\n",
      "Epoch 225, Loss: 0.4697176006517021\n",
      "Epoch 226, Loss: 0.4699424675720642\n",
      "Epoch 227, Loss: 0.46999269112017344\n",
      "Epoch 228, Loss: 0.4698759384724347\n",
      "Epoch 229, Loss: 0.46938041574678274\n",
      "Epoch 230, Loss: 0.4691415581864629\n",
      "Epoch 231, Loss: 0.46924839544758656\n",
      "Epoch 232, Loss: 0.46933531775018905\n",
      "Epoch 233, Loss: 0.4692253881106229\n",
      "Epoch 234, Loss: 0.46894320638835874\n",
      "Epoch 235, Loss: 0.4688628060980674\n",
      "Epoch 236, Loss: 0.46897798398181567\n",
      "Epoch 237, Loss: 0.4689811295666005\n",
      "Epoch 238, Loss: 0.4688031484354944\n",
      "Epoch 239, Loss: 0.46862983752908405\n",
      "Epoch 240, Loss: 0.4685963696810109\n",
      "Epoch 241, Loss: 0.46867106090894\n",
      "Epoch 242, Loss: 0.46866360503981047\n",
      "Epoch 243, Loss: 0.468513635806399\n",
      "Epoch 244, Loss: 0.46835730315983837\n",
      "Epoch 245, Loss: 0.468344910742699\n",
      "Epoch 246, Loss: 0.46842993299443175\n",
      "Epoch 247, Loss: 0.4683803372309361\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.21957419590083768\n",
      "Valence RMSE: 0.206886473233931\n",
      "Arousal RMSE: 0.23156779181014106\n",
      "Test R^2 score: tensor([0.4813, 0.4690], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.3599, 0.3448], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4751592133140691\n",
      "Num of epochs: 248\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Epoch 121, Loss: 0.49295912913079615\n",
      "Epoch 122, Loss: 0.4924773619139838\n",
      "Epoch 123, Loss: 0.49197607191949805\n",
      "Epoch 124, Loss: 0.4914699197831528\n",
      "Epoch 125, Loss: 0.49091483381312667\n",
      "Epoch 126, Loss: 0.49035147677946317\n",
      "Epoch 127, Loss: 0.4898087222825037\n",
      "Epoch 128, Loss: 0.48924014731550836\n",
      "Epoch 129, Loss: 0.48873550647212044\n",
      "Epoch 130, Loss: 0.48832307254971563\n",
      "Epoch 131, Loss: 0.48791418392821295\n",
      "Epoch 132, Loss: 0.4875902245243796\n",
      "Epoch 133, Loss: 0.48728851108037585\n",
      "Epoch 134, Loss: 0.48697062262181345\n",
      "Epoch 135, Loss: 0.4866339553030548\n",
      "Epoch 136, Loss: 0.48633446746734893\n",
      "Epoch 137, Loss: 0.4860310087358514\n",
      "Epoch 138, Loss: 0.4856786412545615\n",
      "Epoch 139, Loss: 0.48531372107004\n",
      "Epoch 140, Loss: 0.48495712985488587\n",
      "Epoch 141, Loss: 0.4846131600652309\n",
      "Epoch 142, Loss: 0.48425966858652364\n",
      "Epoch 143, Loss: 0.4838914149236435\n",
      "Epoch 144, Loss: 0.48352437546282767\n",
      "Epoch 145, Loss: 0.4832299911937133\n",
      "Epoch 146, Loss: 0.4832586529465723\n",
      "Epoch 147, Loss: 0.48356744143113517\n",
      "Epoch 148, Loss: 0.4828108019011922\n",
      "Epoch 149, Loss: 0.4818912939738211\n",
      "Epoch 150, Loss: 0.48239416745575797\n",
      "Epoch 151, Loss: 0.4814952491721961\n",
      "Epoch 152, Loss: 0.4811822818820502\n",
      "Epoch 153, Loss: 0.4812753156716337\n",
      "Epoch 154, Loss: 0.4804387742282827\n",
      "Epoch 155, Loss: 0.48058189052417905\n",
      "Epoch 156, Loss: 0.4801288115497346\n",
      "Epoch 157, Loss: 0.4797465937246525\n",
      "Epoch 158, Loss: 0.4797622479529733\n",
      "Epoch 159, Loss: 0.4791664679844762\n",
      "Epoch 160, Loss: 0.47904359923301915\n",
      "Epoch 161, Loss: 0.4788483533353555\n",
      "Epoch 162, Loss: 0.4783709093142749\n",
      "Epoch 163, Loss: 0.4783027642591491\n",
      "Epoch 164, Loss: 0.4780003946284736\n",
      "Epoch 165, Loss: 0.47766356610156796\n",
      "Epoch 166, Loss: 0.47756940749162413\n",
      "Epoch 167, Loss: 0.4772720526560437\n",
      "Epoch 168, Loss: 0.477001191512605\n",
      "Epoch 169, Loss: 0.47690651155802954\n",
      "Epoch 170, Loss: 0.47663605231839845\n",
      "Epoch 171, Loss: 0.4763715079887003\n",
      "Epoch 172, Loss: 0.4762479807342815\n",
      "Epoch 173, Loss: 0.47603155482909787\n",
      "Epoch 174, Loss: 0.47580085926956445\n",
      "Epoch 175, Loss: 0.4756396378860013\n",
      "Epoch 176, Loss: 0.4754627232666358\n",
      "Epoch 177, Loss: 0.4752513954316692\n",
      "Epoch 178, Loss: 0.47507140350730925\n",
      "Epoch 179, Loss: 0.47492471273973497\n",
      "Epoch 180, Loss: 0.47475493910732763\n",
      "Epoch 181, Loss: 0.47455157018218247\n",
      "Epoch 182, Loss: 0.4743853224635176\n",
      "Epoch 183, Loss: 0.47424550491599726\n",
      "Epoch 184, Loss: 0.47408413178207975\n",
      "Epoch 185, Loss: 0.47390385374022453\n",
      "Epoch 186, Loss: 0.4737383066259127\n",
      "Epoch 187, Loss: 0.47358797785338436\n",
      "Epoch 188, Loss: 0.4734560449826861\n",
      "Epoch 189, Loss: 0.47335738206045025\n",
      "Epoch 190, Loss: 0.4733162834925985\n",
      "Epoch 191, Loss: 0.4732413020737725\n",
      "Epoch 192, Loss: 0.47313239020014447\n",
      "Epoch 193, Loss: 0.4729457158004841\n",
      "Epoch 194, Loss: 0.47274703738102797\n",
      "Epoch 195, Loss: 0.47253750657270627\n",
      "Epoch 196, Loss: 0.4723544299968521\n",
      "Epoch 197, Loss: 0.47221102907746554\n",
      "Epoch 198, Loss: 0.47210844467503205\n",
      "Epoch 199, Loss: 0.47205337980155637\n",
      "Epoch 200, Loss: 0.47204973383515075\n",
      "Epoch 201, Loss: 0.4720427890593303\n",
      "Epoch 202, Loss: 0.4719159657967507\n",
      "Epoch 203, Loss: 0.471735017109987\n",
      "Epoch 204, Loss: 0.4714985533244086\n",
      "Epoch 205, Loss: 0.4713143302002759\n",
      "Epoch 206, Loss: 0.47118574544243713\n",
      "Epoch 207, Loss: 0.47112002449718005\n",
      "Epoch 208, Loss: 0.4710910670518786\n",
      "Epoch 209, Loss: 0.47102495326923743\n",
      "Epoch 210, Loss: 0.4709626586360175\n",
      "Epoch 211, Loss: 0.47085840979895394\n",
      "Epoch 212, Loss: 0.47078064719388485\n",
      "Epoch 213, Loss: 0.47066614789276784\n",
      "Epoch 214, Loss: 0.47055654499036753\n",
      "Epoch 215, Loss: 0.47041652390832933\n",
      "Epoch 216, Loss: 0.4702446473560784\n",
      "Epoch 217, Loss: 0.4701045016774931\n",
      "Epoch 218, Loss: 0.47002059077245106\n",
      "Epoch 219, Loss: 0.4699643617672827\n",
      "Epoch 220, Loss: 0.4698888296176042\n",
      "Epoch 221, Loss: 0.4698141575467262\n",
      "Epoch 222, Loss: 0.4697265783615268\n",
      "Epoch 223, Loss: 0.46966066909468496\n",
      "Epoch 224, Loss: 0.4696348739248034\n",
      "Epoch 225, Loss: 0.4697176006517021\n",
      "Epoch 226, Loss: 0.4699424675720642\n",
      "Epoch 227, Loss: 0.46999269112017344\n",
      "Epoch 228, Loss: 0.4698759384724347\n",
      "Epoch 229, Loss: 0.46938041574678274\n",
      "Epoch 230, Loss: 0.4691415581864629\n",
      "Epoch 231, Loss: 0.46924839544758656\n",
      "Epoch 232, Loss: 0.46933531775018905\n",
      "Epoch 233, Loss: 0.4692253881106229\n",
      "Epoch 234, Loss: 0.46894320638835874\n",
      "Epoch 235, Loss: 0.4688628060980674\n",
      "Epoch 236, Loss: 0.46897798398181567\n",
      "Epoch 237, Loss: 0.4689811295666005\n",
      "Epoch 238, Loss: 0.4688031484354944\n",
      "Epoch 239, Loss: 0.46862983752908405\n",
      "Epoch 240, Loss: 0.4685963696810109\n",
      "Epoch 241, Loss: 0.46867106090894\n",
      "Epoch 242, Loss: 0.46866360503981047\n",
      "Epoch 243, Loss: 0.468513635806399\n",
      "Epoch 244, Loss: 0.46835730315983837\n",
      "Epoch 245, Loss: 0.468344910742699\n",
      "Epoch 246, Loss: 0.46842993299443175\n",
      "Epoch 247, Loss: 0.4683803372309361\n",
      "Epoch 248, Loss: 0.46823408027364943\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2190062405423068\n",
      "Valence RMSE: 0.2063289812592524\n",
      "Arousal RMSE: 0.2309887838953843\n",
      "Test R^2 score: tensor([0.4841, 0.4717], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.3633, 0.3480], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4778810587100354\n",
      "Num of epochs: 249\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Epoch 121, Loss: 0.49295912913079615\n",
      "Epoch 122, Loss: 0.4924773619139838\n",
      "Epoch 123, Loss: 0.49197607191949805\n",
      "Epoch 124, Loss: 0.4914699197831528\n",
      "Epoch 125, Loss: 0.49091483381312667\n",
      "Epoch 126, Loss: 0.49035147677946317\n",
      "Epoch 127, Loss: 0.4898087222825037\n",
      "Epoch 128, Loss: 0.48924014731550836\n",
      "Epoch 129, Loss: 0.48873550647212044\n",
      "Epoch 130, Loss: 0.48832307254971563\n",
      "Epoch 131, Loss: 0.48791418392821295\n",
      "Epoch 132, Loss: 0.4875902245243796\n",
      "Epoch 133, Loss: 0.48728851108037585\n",
      "Epoch 134, Loss: 0.48697062262181345\n",
      "Epoch 135, Loss: 0.4866339553030548\n",
      "Epoch 136, Loss: 0.48633446746734893\n",
      "Epoch 137, Loss: 0.4860310087358514\n",
      "Epoch 138, Loss: 0.4856786412545615\n",
      "Epoch 139, Loss: 0.48531372107004\n",
      "Epoch 140, Loss: 0.48495712985488587\n",
      "Epoch 141, Loss: 0.4846131600652309\n",
      "Epoch 142, Loss: 0.48425966858652364\n",
      "Epoch 143, Loss: 0.4838914149236435\n",
      "Epoch 144, Loss: 0.48352437546282767\n",
      "Epoch 145, Loss: 0.4832299911937133\n",
      "Epoch 146, Loss: 0.4832586529465723\n",
      "Epoch 147, Loss: 0.48356744143113517\n",
      "Epoch 148, Loss: 0.4828108019011922\n",
      "Epoch 149, Loss: 0.4818912939738211\n",
      "Epoch 150, Loss: 0.48239416745575797\n",
      "Epoch 151, Loss: 0.4814952491721961\n",
      "Epoch 152, Loss: 0.4811822818820502\n",
      "Epoch 153, Loss: 0.4812753156716337\n",
      "Epoch 154, Loss: 0.4804387742282827\n",
      "Epoch 155, Loss: 0.48058189052417905\n",
      "Epoch 156, Loss: 0.4801288115497346\n",
      "Epoch 157, Loss: 0.4797465937246525\n",
      "Epoch 158, Loss: 0.4797622479529733\n",
      "Epoch 159, Loss: 0.4791664679844762\n",
      "Epoch 160, Loss: 0.47904359923301915\n",
      "Epoch 161, Loss: 0.4788483533353555\n",
      "Epoch 162, Loss: 0.4783709093142749\n",
      "Epoch 163, Loss: 0.4783027642591491\n",
      "Epoch 164, Loss: 0.4780003946284736\n",
      "Epoch 165, Loss: 0.47766356610156796\n",
      "Epoch 166, Loss: 0.47756940749162413\n",
      "Epoch 167, Loss: 0.4772720526560437\n",
      "Epoch 168, Loss: 0.477001191512605\n",
      "Epoch 169, Loss: 0.47690651155802954\n",
      "Epoch 170, Loss: 0.47663605231839845\n",
      "Epoch 171, Loss: 0.4763715079887003\n",
      "Epoch 172, Loss: 0.4762479807342815\n",
      "Epoch 173, Loss: 0.47603155482909787\n",
      "Epoch 174, Loss: 0.47580085926956445\n",
      "Epoch 175, Loss: 0.4756396378860013\n",
      "Epoch 176, Loss: 0.4754627232666358\n",
      "Epoch 177, Loss: 0.4752513954316692\n",
      "Epoch 178, Loss: 0.47507140350730925\n",
      "Epoch 179, Loss: 0.47492471273973497\n",
      "Epoch 180, Loss: 0.47475493910732763\n",
      "Epoch 181, Loss: 0.47455157018218247\n",
      "Epoch 182, Loss: 0.4743853224635176\n",
      "Epoch 183, Loss: 0.47424550491599726\n",
      "Epoch 184, Loss: 0.47408413178207975\n",
      "Epoch 185, Loss: 0.47390385374022453\n",
      "Epoch 186, Loss: 0.4737383066259127\n",
      "Epoch 187, Loss: 0.47358797785338436\n",
      "Epoch 188, Loss: 0.4734560449826861\n",
      "Epoch 189, Loss: 0.47335738206045025\n",
      "Epoch 190, Loss: 0.4733162834925985\n",
      "Epoch 191, Loss: 0.4732413020737725\n",
      "Epoch 192, Loss: 0.47313239020014447\n",
      "Epoch 193, Loss: 0.4729457158004841\n",
      "Epoch 194, Loss: 0.47274703738102797\n",
      "Epoch 195, Loss: 0.47253750657270627\n",
      "Epoch 196, Loss: 0.4723544299968521\n",
      "Epoch 197, Loss: 0.47221102907746554\n",
      "Epoch 198, Loss: 0.47210844467503205\n",
      "Epoch 199, Loss: 0.47205337980155637\n",
      "Epoch 200, Loss: 0.47204973383515075\n",
      "Epoch 201, Loss: 0.4720427890593303\n",
      "Epoch 202, Loss: 0.4719159657967507\n",
      "Epoch 203, Loss: 0.471735017109987\n",
      "Epoch 204, Loss: 0.4714985533244086\n",
      "Epoch 205, Loss: 0.4713143302002759\n",
      "Epoch 206, Loss: 0.47118574544243713\n",
      "Epoch 207, Loss: 0.47112002449718005\n",
      "Epoch 208, Loss: 0.4710910670518786\n",
      "Epoch 209, Loss: 0.47102495326923743\n",
      "Epoch 210, Loss: 0.4709626586360175\n",
      "Epoch 211, Loss: 0.47085840979895394\n",
      "Epoch 212, Loss: 0.47078064719388485\n",
      "Epoch 213, Loss: 0.47066614789276784\n",
      "Epoch 214, Loss: 0.47055654499036753\n",
      "Epoch 215, Loss: 0.47041652390832933\n",
      "Epoch 216, Loss: 0.4702446473560784\n",
      "Epoch 217, Loss: 0.4701045016774931\n",
      "Epoch 218, Loss: 0.47002059077245106\n",
      "Epoch 219, Loss: 0.4699643617672827\n",
      "Epoch 220, Loss: 0.4698888296176042\n",
      "Epoch 221, Loss: 0.4698141575467262\n",
      "Epoch 222, Loss: 0.4697265783615268\n",
      "Epoch 223, Loss: 0.46966066909468496\n",
      "Epoch 224, Loss: 0.4696348739248034\n",
      "Epoch 225, Loss: 0.4697176006517021\n",
      "Epoch 226, Loss: 0.4699424675720642\n",
      "Epoch 227, Loss: 0.46999269112017344\n",
      "Epoch 228, Loss: 0.4698759384724347\n",
      "Epoch 229, Loss: 0.46938041574678274\n",
      "Epoch 230, Loss: 0.4691415581864629\n",
      "Epoch 231, Loss: 0.46924839544758656\n",
      "Epoch 232, Loss: 0.46933531775018905\n",
      "Epoch 233, Loss: 0.4692253881106229\n",
      "Epoch 234, Loss: 0.46894320638835874\n",
      "Epoch 235, Loss: 0.4688628060980674\n",
      "Epoch 236, Loss: 0.46897798398181567\n",
      "Epoch 237, Loss: 0.4689811295666005\n",
      "Epoch 238, Loss: 0.4688031484354944\n",
      "Epoch 239, Loss: 0.46862983752908405\n",
      "Epoch 240, Loss: 0.4685963696810109\n",
      "Epoch 241, Loss: 0.46867106090894\n",
      "Epoch 242, Loss: 0.46866360503981047\n",
      "Epoch 243, Loss: 0.468513635806399\n",
      "Epoch 244, Loss: 0.46835730315983837\n",
      "Epoch 245, Loss: 0.468344910742699\n",
      "Epoch 246, Loss: 0.46842993299443175\n",
      "Epoch 247, Loss: 0.4683803372309361\n",
      "Epoch 248, Loss: 0.46823408027364943\n",
      "Epoch 249, Loss: 0.46809559282800506\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2186870122277647\n",
      "Valence RMSE: 0.20600090153318806\n",
      "Arousal RMSE: 0.23067649902347395\n",
      "Test R^2 score: tensor([0.4857, 0.4731], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.3653, 0.3498], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4794145378517074\n",
      "Num of epochs: 250\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Epoch 121, Loss: 0.49295912913079615\n",
      "Epoch 122, Loss: 0.4924773619139838\n",
      "Epoch 123, Loss: 0.49197607191949805\n",
      "Epoch 124, Loss: 0.4914699197831528\n",
      "Epoch 125, Loss: 0.49091483381312667\n",
      "Epoch 126, Loss: 0.49035147677946317\n",
      "Epoch 127, Loss: 0.4898087222825037\n",
      "Epoch 128, Loss: 0.48924014731550836\n",
      "Epoch 129, Loss: 0.48873550647212044\n",
      "Epoch 130, Loss: 0.48832307254971563\n",
      "Epoch 131, Loss: 0.48791418392821295\n",
      "Epoch 132, Loss: 0.4875902245243796\n",
      "Epoch 133, Loss: 0.48728851108037585\n",
      "Epoch 134, Loss: 0.48697062262181345\n",
      "Epoch 135, Loss: 0.4866339553030548\n",
      "Epoch 136, Loss: 0.48633446746734893\n",
      "Epoch 137, Loss: 0.4860310087358514\n",
      "Epoch 138, Loss: 0.4856786412545615\n",
      "Epoch 139, Loss: 0.48531372107004\n",
      "Epoch 140, Loss: 0.48495712985488587\n",
      "Epoch 141, Loss: 0.4846131600652309\n",
      "Epoch 142, Loss: 0.48425966858652364\n",
      "Epoch 143, Loss: 0.4838914149236435\n",
      "Epoch 144, Loss: 0.48352437546282767\n",
      "Epoch 145, Loss: 0.4832299911937133\n",
      "Epoch 146, Loss: 0.4832586529465723\n",
      "Epoch 147, Loss: 0.48356744143113517\n",
      "Epoch 148, Loss: 0.4828108019011922\n",
      "Epoch 149, Loss: 0.4818912939738211\n",
      "Epoch 150, Loss: 0.48239416745575797\n",
      "Epoch 151, Loss: 0.4814952491721961\n",
      "Epoch 152, Loss: 0.4811822818820502\n",
      "Epoch 153, Loss: 0.4812753156716337\n",
      "Epoch 154, Loss: 0.4804387742282827\n",
      "Epoch 155, Loss: 0.48058189052417905\n",
      "Epoch 156, Loss: 0.4801288115497346\n",
      "Epoch 157, Loss: 0.4797465937246525\n",
      "Epoch 158, Loss: 0.4797622479529733\n",
      "Epoch 159, Loss: 0.4791664679844762\n",
      "Epoch 160, Loss: 0.47904359923301915\n",
      "Epoch 161, Loss: 0.4788483533353555\n",
      "Epoch 162, Loss: 0.4783709093142749\n",
      "Epoch 163, Loss: 0.4783027642591491\n",
      "Epoch 164, Loss: 0.4780003946284736\n",
      "Epoch 165, Loss: 0.47766356610156796\n",
      "Epoch 166, Loss: 0.47756940749162413\n",
      "Epoch 167, Loss: 0.4772720526560437\n",
      "Epoch 168, Loss: 0.477001191512605\n",
      "Epoch 169, Loss: 0.47690651155802954\n",
      "Epoch 170, Loss: 0.47663605231839845\n",
      "Epoch 171, Loss: 0.4763715079887003\n",
      "Epoch 172, Loss: 0.4762479807342815\n",
      "Epoch 173, Loss: 0.47603155482909787\n",
      "Epoch 174, Loss: 0.47580085926956445\n",
      "Epoch 175, Loss: 0.4756396378860013\n",
      "Epoch 176, Loss: 0.4754627232666358\n",
      "Epoch 177, Loss: 0.4752513954316692\n",
      "Epoch 178, Loss: 0.47507140350730925\n",
      "Epoch 179, Loss: 0.47492471273973497\n",
      "Epoch 180, Loss: 0.47475493910732763\n",
      "Epoch 181, Loss: 0.47455157018218247\n",
      "Epoch 182, Loss: 0.4743853224635176\n",
      "Epoch 183, Loss: 0.47424550491599726\n",
      "Epoch 184, Loss: 0.47408413178207975\n",
      "Epoch 185, Loss: 0.47390385374022453\n",
      "Epoch 186, Loss: 0.4737383066259127\n",
      "Epoch 187, Loss: 0.47358797785338436\n",
      "Epoch 188, Loss: 0.4734560449826861\n",
      "Epoch 189, Loss: 0.47335738206045025\n",
      "Epoch 190, Loss: 0.4733162834925985\n",
      "Epoch 191, Loss: 0.4732413020737725\n",
      "Epoch 192, Loss: 0.47313239020014447\n",
      "Epoch 193, Loss: 0.4729457158004841\n",
      "Epoch 194, Loss: 0.47274703738102797\n",
      "Epoch 195, Loss: 0.47253750657270627\n",
      "Epoch 196, Loss: 0.4723544299968521\n",
      "Epoch 197, Loss: 0.47221102907746554\n",
      "Epoch 198, Loss: 0.47210844467503205\n",
      "Epoch 199, Loss: 0.47205337980155637\n",
      "Epoch 200, Loss: 0.47204973383515075\n",
      "Epoch 201, Loss: 0.4720427890593303\n",
      "Epoch 202, Loss: 0.4719159657967507\n",
      "Epoch 203, Loss: 0.471735017109987\n",
      "Epoch 204, Loss: 0.4714985533244086\n",
      "Epoch 205, Loss: 0.4713143302002759\n",
      "Epoch 206, Loss: 0.47118574544243713\n",
      "Epoch 207, Loss: 0.47112002449718005\n",
      "Epoch 208, Loss: 0.4710910670518786\n",
      "Epoch 209, Loss: 0.47102495326923743\n",
      "Epoch 210, Loss: 0.4709626586360175\n",
      "Epoch 211, Loss: 0.47085840979895394\n",
      "Epoch 212, Loss: 0.47078064719388485\n",
      "Epoch 213, Loss: 0.47066614789276784\n",
      "Epoch 214, Loss: 0.47055654499036753\n",
      "Epoch 215, Loss: 0.47041652390832933\n",
      "Epoch 216, Loss: 0.4702446473560784\n",
      "Epoch 217, Loss: 0.4701045016774931\n",
      "Epoch 218, Loss: 0.47002059077245106\n",
      "Epoch 219, Loss: 0.4699643617672827\n",
      "Epoch 220, Loss: 0.4698888296176042\n",
      "Epoch 221, Loss: 0.4698141575467262\n",
      "Epoch 222, Loss: 0.4697265783615268\n",
      "Epoch 223, Loss: 0.46966066909468496\n",
      "Epoch 224, Loss: 0.4696348739248034\n",
      "Epoch 225, Loss: 0.4697176006517021\n",
      "Epoch 226, Loss: 0.4699424675720642\n",
      "Epoch 227, Loss: 0.46999269112017344\n",
      "Epoch 228, Loss: 0.4698759384724347\n",
      "Epoch 229, Loss: 0.46938041574678274\n",
      "Epoch 230, Loss: 0.4691415581864629\n",
      "Epoch 231, Loss: 0.46924839544758656\n",
      "Epoch 232, Loss: 0.46933531775018905\n",
      "Epoch 233, Loss: 0.4692253881106229\n",
      "Epoch 234, Loss: 0.46894320638835874\n",
      "Epoch 235, Loss: 0.4688628060980674\n",
      "Epoch 236, Loss: 0.46897798398181567\n",
      "Epoch 237, Loss: 0.4689811295666005\n",
      "Epoch 238, Loss: 0.4688031484354944\n",
      "Epoch 239, Loss: 0.46862983752908405\n",
      "Epoch 240, Loss: 0.4685963696810109\n",
      "Epoch 241, Loss: 0.46867106090894\n",
      "Epoch 242, Loss: 0.46866360503981047\n",
      "Epoch 243, Loss: 0.468513635806399\n",
      "Epoch 244, Loss: 0.46835730315983837\n",
      "Epoch 245, Loss: 0.468344910742699\n",
      "Epoch 246, Loss: 0.46842993299443175\n",
      "Epoch 247, Loss: 0.4683803372309361\n",
      "Epoch 248, Loss: 0.46823408027364943\n",
      "Epoch 249, Loss: 0.46809559282800506\n",
      "Epoch 250, Loss: 0.4681194355765096\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.21988951966346648\n",
      "Valence RMSE: 0.20719048479577423\n",
      "Arousal RMSE: 0.23189416708005414\n",
      "Test R^2 score: tensor([0.4798, 0.4675], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.3580, 0.3429], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4736475418082193\n",
      "Num of epochs: 251\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Epoch 121, Loss: 0.49295912913079615\n",
      "Epoch 122, Loss: 0.4924773619139838\n",
      "Epoch 123, Loss: 0.49197607191949805\n",
      "Epoch 124, Loss: 0.4914699197831528\n",
      "Epoch 125, Loss: 0.49091483381312667\n",
      "Epoch 126, Loss: 0.49035147677946317\n",
      "Epoch 127, Loss: 0.4898087222825037\n",
      "Epoch 128, Loss: 0.48924014731550836\n",
      "Epoch 129, Loss: 0.48873550647212044\n",
      "Epoch 130, Loss: 0.48832307254971563\n",
      "Epoch 131, Loss: 0.48791418392821295\n",
      "Epoch 132, Loss: 0.4875902245243796\n",
      "Epoch 133, Loss: 0.48728851108037585\n",
      "Epoch 134, Loss: 0.48697062262181345\n",
      "Epoch 135, Loss: 0.4866339553030548\n",
      "Epoch 136, Loss: 0.48633446746734893\n",
      "Epoch 137, Loss: 0.4860310087358514\n",
      "Epoch 138, Loss: 0.4856786412545615\n",
      "Epoch 139, Loss: 0.48531372107004\n",
      "Epoch 140, Loss: 0.48495712985488587\n",
      "Epoch 141, Loss: 0.4846131600652309\n",
      "Epoch 142, Loss: 0.48425966858652364\n",
      "Epoch 143, Loss: 0.4838914149236435\n",
      "Epoch 144, Loss: 0.48352437546282767\n",
      "Epoch 145, Loss: 0.4832299911937133\n",
      "Epoch 146, Loss: 0.4832586529465723\n",
      "Epoch 147, Loss: 0.48356744143113517\n",
      "Epoch 148, Loss: 0.4828108019011922\n",
      "Epoch 149, Loss: 0.4818912939738211\n",
      "Epoch 150, Loss: 0.48239416745575797\n",
      "Epoch 151, Loss: 0.4814952491721961\n",
      "Epoch 152, Loss: 0.4811822818820502\n",
      "Epoch 153, Loss: 0.4812753156716337\n",
      "Epoch 154, Loss: 0.4804387742282827\n",
      "Epoch 155, Loss: 0.48058189052417905\n",
      "Epoch 156, Loss: 0.4801288115497346\n",
      "Epoch 157, Loss: 0.4797465937246525\n",
      "Epoch 158, Loss: 0.4797622479529733\n",
      "Epoch 159, Loss: 0.4791664679844762\n",
      "Epoch 160, Loss: 0.47904359923301915\n",
      "Epoch 161, Loss: 0.4788483533353555\n",
      "Epoch 162, Loss: 0.4783709093142749\n",
      "Epoch 163, Loss: 0.4783027642591491\n",
      "Epoch 164, Loss: 0.4780003946284736\n",
      "Epoch 165, Loss: 0.47766356610156796\n",
      "Epoch 166, Loss: 0.47756940749162413\n",
      "Epoch 167, Loss: 0.4772720526560437\n",
      "Epoch 168, Loss: 0.477001191512605\n",
      "Epoch 169, Loss: 0.47690651155802954\n",
      "Epoch 170, Loss: 0.47663605231839845\n",
      "Epoch 171, Loss: 0.4763715079887003\n",
      "Epoch 172, Loss: 0.4762479807342815\n",
      "Epoch 173, Loss: 0.47603155482909787\n",
      "Epoch 174, Loss: 0.47580085926956445\n",
      "Epoch 175, Loss: 0.4756396378860013\n",
      "Epoch 176, Loss: 0.4754627232666358\n",
      "Epoch 177, Loss: 0.4752513954316692\n",
      "Epoch 178, Loss: 0.47507140350730925\n",
      "Epoch 179, Loss: 0.47492471273973497\n",
      "Epoch 180, Loss: 0.47475493910732763\n",
      "Epoch 181, Loss: 0.47455157018218247\n",
      "Epoch 182, Loss: 0.4743853224635176\n",
      "Epoch 183, Loss: 0.47424550491599726\n",
      "Epoch 184, Loss: 0.47408413178207975\n",
      "Epoch 185, Loss: 0.47390385374022453\n",
      "Epoch 186, Loss: 0.4737383066259127\n",
      "Epoch 187, Loss: 0.47358797785338436\n",
      "Epoch 188, Loss: 0.4734560449826861\n",
      "Epoch 189, Loss: 0.47335738206045025\n",
      "Epoch 190, Loss: 0.4733162834925985\n",
      "Epoch 191, Loss: 0.4732413020737725\n",
      "Epoch 192, Loss: 0.47313239020014447\n",
      "Epoch 193, Loss: 0.4729457158004841\n",
      "Epoch 194, Loss: 0.47274703738102797\n",
      "Epoch 195, Loss: 0.47253750657270627\n",
      "Epoch 196, Loss: 0.4723544299968521\n",
      "Epoch 197, Loss: 0.47221102907746554\n",
      "Epoch 198, Loss: 0.47210844467503205\n",
      "Epoch 199, Loss: 0.47205337980155637\n",
      "Epoch 200, Loss: 0.47204973383515075\n",
      "Epoch 201, Loss: 0.4720427890593303\n",
      "Epoch 202, Loss: 0.4719159657967507\n",
      "Epoch 203, Loss: 0.471735017109987\n",
      "Epoch 204, Loss: 0.4714985533244086\n",
      "Epoch 205, Loss: 0.4713143302002759\n",
      "Epoch 206, Loss: 0.47118574544243713\n",
      "Epoch 207, Loss: 0.47112002449718005\n",
      "Epoch 208, Loss: 0.4710910670518786\n",
      "Epoch 209, Loss: 0.47102495326923743\n",
      "Epoch 210, Loss: 0.4709626586360175\n",
      "Epoch 211, Loss: 0.47085840979895394\n",
      "Epoch 212, Loss: 0.47078064719388485\n",
      "Epoch 213, Loss: 0.47066614789276784\n",
      "Epoch 214, Loss: 0.47055654499036753\n",
      "Epoch 215, Loss: 0.47041652390832933\n",
      "Epoch 216, Loss: 0.4702446473560784\n",
      "Epoch 217, Loss: 0.4701045016774931\n",
      "Epoch 218, Loss: 0.47002059077245106\n",
      "Epoch 219, Loss: 0.4699643617672827\n",
      "Epoch 220, Loss: 0.4698888296176042\n",
      "Epoch 221, Loss: 0.4698141575467262\n",
      "Epoch 222, Loss: 0.4697265783615268\n",
      "Epoch 223, Loss: 0.46966066909468496\n",
      "Epoch 224, Loss: 0.4696348739248034\n",
      "Epoch 225, Loss: 0.4697176006517021\n",
      "Epoch 226, Loss: 0.4699424675720642\n",
      "Epoch 227, Loss: 0.46999269112017344\n",
      "Epoch 228, Loss: 0.4698759384724347\n",
      "Epoch 229, Loss: 0.46938041574678274\n",
      "Epoch 230, Loss: 0.4691415581864629\n",
      "Epoch 231, Loss: 0.46924839544758656\n",
      "Epoch 232, Loss: 0.46933531775018905\n",
      "Epoch 233, Loss: 0.4692253881106229\n",
      "Epoch 234, Loss: 0.46894320638835874\n",
      "Epoch 235, Loss: 0.4688628060980674\n",
      "Epoch 236, Loss: 0.46897798398181567\n",
      "Epoch 237, Loss: 0.4689811295666005\n",
      "Epoch 238, Loss: 0.4688031484354944\n",
      "Epoch 239, Loss: 0.46862983752908405\n",
      "Epoch 240, Loss: 0.4685963696810109\n",
      "Epoch 241, Loss: 0.46867106090894\n",
      "Epoch 242, Loss: 0.46866360503981047\n",
      "Epoch 243, Loss: 0.468513635806399\n",
      "Epoch 244, Loss: 0.46835730315983837\n",
      "Epoch 245, Loss: 0.468344910742699\n",
      "Epoch 246, Loss: 0.46842993299443175\n",
      "Epoch 247, Loss: 0.4683803372309361\n",
      "Epoch 248, Loss: 0.46823408027364943\n",
      "Epoch 249, Loss: 0.46809559282800506\n",
      "Epoch 250, Loss: 0.4681194355765096\n",
      "Epoch 251, Loss: 0.4682059468617626\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2184931237437703\n",
      "Valence RMSE: 0.20580223226913405\n",
      "Arousal RMSE: 0.23048629338783347\n",
      "Test R^2 score: tensor([0.4867, 0.4740], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.3666, 0.3509], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4803445480008265\n",
      "Num of epochs: 252\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Epoch 121, Loss: 0.49295912913079615\n",
      "Epoch 122, Loss: 0.4924773619139838\n",
      "Epoch 123, Loss: 0.49197607191949805\n",
      "Epoch 124, Loss: 0.4914699197831528\n",
      "Epoch 125, Loss: 0.49091483381312667\n",
      "Epoch 126, Loss: 0.49035147677946317\n",
      "Epoch 127, Loss: 0.4898087222825037\n",
      "Epoch 128, Loss: 0.48924014731550836\n",
      "Epoch 129, Loss: 0.48873550647212044\n",
      "Epoch 130, Loss: 0.48832307254971563\n",
      "Epoch 131, Loss: 0.48791418392821295\n",
      "Epoch 132, Loss: 0.4875902245243796\n",
      "Epoch 133, Loss: 0.48728851108037585\n",
      "Epoch 134, Loss: 0.48697062262181345\n",
      "Epoch 135, Loss: 0.4866339553030548\n",
      "Epoch 136, Loss: 0.48633446746734893\n",
      "Epoch 137, Loss: 0.4860310087358514\n",
      "Epoch 138, Loss: 0.4856786412545615\n",
      "Epoch 139, Loss: 0.48531372107004\n",
      "Epoch 140, Loss: 0.48495712985488587\n",
      "Epoch 141, Loss: 0.4846131600652309\n",
      "Epoch 142, Loss: 0.48425966858652364\n",
      "Epoch 143, Loss: 0.4838914149236435\n",
      "Epoch 144, Loss: 0.48352437546282767\n",
      "Epoch 145, Loss: 0.4832299911937133\n",
      "Epoch 146, Loss: 0.4832586529465723\n",
      "Epoch 147, Loss: 0.48356744143113517\n",
      "Epoch 148, Loss: 0.4828108019011922\n",
      "Epoch 149, Loss: 0.4818912939738211\n",
      "Epoch 150, Loss: 0.48239416745575797\n",
      "Epoch 151, Loss: 0.4814952491721961\n",
      "Epoch 152, Loss: 0.4811822818820502\n",
      "Epoch 153, Loss: 0.4812753156716337\n",
      "Epoch 154, Loss: 0.4804387742282827\n",
      "Epoch 155, Loss: 0.48058189052417905\n",
      "Epoch 156, Loss: 0.4801288115497346\n",
      "Epoch 157, Loss: 0.4797465937246525\n",
      "Epoch 158, Loss: 0.4797622479529733\n",
      "Epoch 159, Loss: 0.4791664679844762\n",
      "Epoch 160, Loss: 0.47904359923301915\n",
      "Epoch 161, Loss: 0.4788483533353555\n",
      "Epoch 162, Loss: 0.4783709093142749\n",
      "Epoch 163, Loss: 0.4783027642591491\n",
      "Epoch 164, Loss: 0.4780003946284736\n",
      "Epoch 165, Loss: 0.47766356610156796\n",
      "Epoch 166, Loss: 0.47756940749162413\n",
      "Epoch 167, Loss: 0.4772720526560437\n",
      "Epoch 168, Loss: 0.477001191512605\n",
      "Epoch 169, Loss: 0.47690651155802954\n",
      "Epoch 170, Loss: 0.47663605231839845\n",
      "Epoch 171, Loss: 0.4763715079887003\n",
      "Epoch 172, Loss: 0.4762479807342815\n",
      "Epoch 173, Loss: 0.47603155482909787\n",
      "Epoch 174, Loss: 0.47580085926956445\n",
      "Epoch 175, Loss: 0.4756396378860013\n",
      "Epoch 176, Loss: 0.4754627232666358\n",
      "Epoch 177, Loss: 0.4752513954316692\n",
      "Epoch 178, Loss: 0.47507140350730925\n",
      "Epoch 179, Loss: 0.47492471273973497\n",
      "Epoch 180, Loss: 0.47475493910732763\n",
      "Epoch 181, Loss: 0.47455157018218247\n",
      "Epoch 182, Loss: 0.4743853224635176\n",
      "Epoch 183, Loss: 0.47424550491599726\n",
      "Epoch 184, Loss: 0.47408413178207975\n",
      "Epoch 185, Loss: 0.47390385374022453\n",
      "Epoch 186, Loss: 0.4737383066259127\n",
      "Epoch 187, Loss: 0.47358797785338436\n",
      "Epoch 188, Loss: 0.4734560449826861\n",
      "Epoch 189, Loss: 0.47335738206045025\n",
      "Epoch 190, Loss: 0.4733162834925985\n",
      "Epoch 191, Loss: 0.4732413020737725\n",
      "Epoch 192, Loss: 0.47313239020014447\n",
      "Epoch 193, Loss: 0.4729457158004841\n",
      "Epoch 194, Loss: 0.47274703738102797\n",
      "Epoch 195, Loss: 0.47253750657270627\n",
      "Epoch 196, Loss: 0.4723544299968521\n",
      "Epoch 197, Loss: 0.47221102907746554\n",
      "Epoch 198, Loss: 0.47210844467503205\n",
      "Epoch 199, Loss: 0.47205337980155637\n",
      "Epoch 200, Loss: 0.47204973383515075\n",
      "Epoch 201, Loss: 0.4720427890593303\n",
      "Epoch 202, Loss: 0.4719159657967507\n",
      "Epoch 203, Loss: 0.471735017109987\n",
      "Epoch 204, Loss: 0.4714985533244086\n",
      "Epoch 205, Loss: 0.4713143302002759\n",
      "Epoch 206, Loss: 0.47118574544243713\n",
      "Epoch 207, Loss: 0.47112002449718005\n",
      "Epoch 208, Loss: 0.4710910670518786\n",
      "Epoch 209, Loss: 0.47102495326923743\n",
      "Epoch 210, Loss: 0.4709626586360175\n",
      "Epoch 211, Loss: 0.47085840979895394\n",
      "Epoch 212, Loss: 0.47078064719388485\n",
      "Epoch 213, Loss: 0.47066614789276784\n",
      "Epoch 214, Loss: 0.47055654499036753\n",
      "Epoch 215, Loss: 0.47041652390832933\n",
      "Epoch 216, Loss: 0.4702446473560784\n",
      "Epoch 217, Loss: 0.4701045016774931\n",
      "Epoch 218, Loss: 0.47002059077245106\n",
      "Epoch 219, Loss: 0.4699643617672827\n",
      "Epoch 220, Loss: 0.4698888296176042\n",
      "Epoch 221, Loss: 0.4698141575467262\n",
      "Epoch 222, Loss: 0.4697265783615268\n",
      "Epoch 223, Loss: 0.46966066909468496\n",
      "Epoch 224, Loss: 0.4696348739248034\n",
      "Epoch 225, Loss: 0.4697176006517021\n",
      "Epoch 226, Loss: 0.4699424675720642\n",
      "Epoch 227, Loss: 0.46999269112017344\n",
      "Epoch 228, Loss: 0.4698759384724347\n",
      "Epoch 229, Loss: 0.46938041574678274\n",
      "Epoch 230, Loss: 0.4691415581864629\n",
      "Epoch 231, Loss: 0.46924839544758656\n",
      "Epoch 232, Loss: 0.46933531775018905\n",
      "Epoch 233, Loss: 0.4692253881106229\n",
      "Epoch 234, Loss: 0.46894320638835874\n",
      "Epoch 235, Loss: 0.4688628060980674\n",
      "Epoch 236, Loss: 0.46897798398181567\n",
      "Epoch 237, Loss: 0.4689811295666005\n",
      "Epoch 238, Loss: 0.4688031484354944\n",
      "Epoch 239, Loss: 0.46862983752908405\n",
      "Epoch 240, Loss: 0.4685963696810109\n",
      "Epoch 241, Loss: 0.46867106090894\n",
      "Epoch 242, Loss: 0.46866360503981047\n",
      "Epoch 243, Loss: 0.468513635806399\n",
      "Epoch 244, Loss: 0.46835730315983837\n",
      "Epoch 245, Loss: 0.468344910742699\n",
      "Epoch 246, Loss: 0.46842993299443175\n",
      "Epoch 247, Loss: 0.4683803372309361\n",
      "Epoch 248, Loss: 0.46823408027364943\n",
      "Epoch 249, Loss: 0.46809559282800506\n",
      "Epoch 250, Loss: 0.4681194355765096\n",
      "Epoch 251, Loss: 0.4682059468617626\n",
      "Epoch 252, Loss: 0.46815125054377077\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.21954488557978183\n",
      "Valence RMSE: 0.20685335507819036\n",
      "Arousal RMSE: 0.23154179549570547\n",
      "Test R^2 score: tensor([0.4814, 0.4692], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.3601, 0.3449], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4753018462426452\n",
      "Num of epochs: 253\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Epoch 121, Loss: 0.49295912913079615\n",
      "Epoch 122, Loss: 0.4924773619139838\n",
      "Epoch 123, Loss: 0.49197607191949805\n",
      "Epoch 124, Loss: 0.4914699197831528\n",
      "Epoch 125, Loss: 0.49091483381312667\n",
      "Epoch 126, Loss: 0.49035147677946317\n",
      "Epoch 127, Loss: 0.4898087222825037\n",
      "Epoch 128, Loss: 0.48924014731550836\n",
      "Epoch 129, Loss: 0.48873550647212044\n",
      "Epoch 130, Loss: 0.48832307254971563\n",
      "Epoch 131, Loss: 0.48791418392821295\n",
      "Epoch 132, Loss: 0.4875902245243796\n",
      "Epoch 133, Loss: 0.48728851108037585\n",
      "Epoch 134, Loss: 0.48697062262181345\n",
      "Epoch 135, Loss: 0.4866339553030548\n",
      "Epoch 136, Loss: 0.48633446746734893\n",
      "Epoch 137, Loss: 0.4860310087358514\n",
      "Epoch 138, Loss: 0.4856786412545615\n",
      "Epoch 139, Loss: 0.48531372107004\n",
      "Epoch 140, Loss: 0.48495712985488587\n",
      "Epoch 141, Loss: 0.4846131600652309\n",
      "Epoch 142, Loss: 0.48425966858652364\n",
      "Epoch 143, Loss: 0.4838914149236435\n",
      "Epoch 144, Loss: 0.48352437546282767\n",
      "Epoch 145, Loss: 0.4832299911937133\n",
      "Epoch 146, Loss: 0.4832586529465723\n",
      "Epoch 147, Loss: 0.48356744143113517\n",
      "Epoch 148, Loss: 0.4828108019011922\n",
      "Epoch 149, Loss: 0.4818912939738211\n",
      "Epoch 150, Loss: 0.48239416745575797\n",
      "Epoch 151, Loss: 0.4814952491721961\n",
      "Epoch 152, Loss: 0.4811822818820502\n",
      "Epoch 153, Loss: 0.4812753156716337\n",
      "Epoch 154, Loss: 0.4804387742282827\n",
      "Epoch 155, Loss: 0.48058189052417905\n",
      "Epoch 156, Loss: 0.4801288115497346\n",
      "Epoch 157, Loss: 0.4797465937246525\n",
      "Epoch 158, Loss: 0.4797622479529733\n",
      "Epoch 159, Loss: 0.4791664679844762\n",
      "Epoch 160, Loss: 0.47904359923301915\n",
      "Epoch 161, Loss: 0.4788483533353555\n",
      "Epoch 162, Loss: 0.4783709093142749\n",
      "Epoch 163, Loss: 0.4783027642591491\n",
      "Epoch 164, Loss: 0.4780003946284736\n",
      "Epoch 165, Loss: 0.47766356610156796\n",
      "Epoch 166, Loss: 0.47756940749162413\n",
      "Epoch 167, Loss: 0.4772720526560437\n",
      "Epoch 168, Loss: 0.477001191512605\n",
      "Epoch 169, Loss: 0.47690651155802954\n",
      "Epoch 170, Loss: 0.47663605231839845\n",
      "Epoch 171, Loss: 0.4763715079887003\n",
      "Epoch 172, Loss: 0.4762479807342815\n",
      "Epoch 173, Loss: 0.47603155482909787\n",
      "Epoch 174, Loss: 0.47580085926956445\n",
      "Epoch 175, Loss: 0.4756396378860013\n",
      "Epoch 176, Loss: 0.4754627232666358\n",
      "Epoch 177, Loss: 0.4752513954316692\n",
      "Epoch 178, Loss: 0.47507140350730925\n",
      "Epoch 179, Loss: 0.47492471273973497\n",
      "Epoch 180, Loss: 0.47475493910732763\n",
      "Epoch 181, Loss: 0.47455157018218247\n",
      "Epoch 182, Loss: 0.4743853224635176\n",
      "Epoch 183, Loss: 0.47424550491599726\n",
      "Epoch 184, Loss: 0.47408413178207975\n",
      "Epoch 185, Loss: 0.47390385374022453\n",
      "Epoch 186, Loss: 0.4737383066259127\n",
      "Epoch 187, Loss: 0.47358797785338436\n",
      "Epoch 188, Loss: 0.4734560449826861\n",
      "Epoch 189, Loss: 0.47335738206045025\n",
      "Epoch 190, Loss: 0.4733162834925985\n",
      "Epoch 191, Loss: 0.4732413020737725\n",
      "Epoch 192, Loss: 0.47313239020014447\n",
      "Epoch 193, Loss: 0.4729457158004841\n",
      "Epoch 194, Loss: 0.47274703738102797\n",
      "Epoch 195, Loss: 0.47253750657270627\n",
      "Epoch 196, Loss: 0.4723544299968521\n",
      "Epoch 197, Loss: 0.47221102907746554\n",
      "Epoch 198, Loss: 0.47210844467503205\n",
      "Epoch 199, Loss: 0.47205337980155637\n",
      "Epoch 200, Loss: 0.47204973383515075\n",
      "Epoch 201, Loss: 0.4720427890593303\n",
      "Epoch 202, Loss: 0.4719159657967507\n",
      "Epoch 203, Loss: 0.471735017109987\n",
      "Epoch 204, Loss: 0.4714985533244086\n",
      "Epoch 205, Loss: 0.4713143302002759\n",
      "Epoch 206, Loss: 0.47118574544243713\n",
      "Epoch 207, Loss: 0.47112002449718005\n",
      "Epoch 208, Loss: 0.4710910670518786\n",
      "Epoch 209, Loss: 0.47102495326923743\n",
      "Epoch 210, Loss: 0.4709626586360175\n",
      "Epoch 211, Loss: 0.47085840979895394\n",
      "Epoch 212, Loss: 0.47078064719388485\n",
      "Epoch 213, Loss: 0.47066614789276784\n",
      "Epoch 214, Loss: 0.47055654499036753\n",
      "Epoch 215, Loss: 0.47041652390832933\n",
      "Epoch 216, Loss: 0.4702446473560784\n",
      "Epoch 217, Loss: 0.4701045016774931\n",
      "Epoch 218, Loss: 0.47002059077245106\n",
      "Epoch 219, Loss: 0.4699643617672827\n",
      "Epoch 220, Loss: 0.4698888296176042\n",
      "Epoch 221, Loss: 0.4698141575467262\n",
      "Epoch 222, Loss: 0.4697265783615268\n",
      "Epoch 223, Loss: 0.46966066909468496\n",
      "Epoch 224, Loss: 0.4696348739248034\n",
      "Epoch 225, Loss: 0.4697176006517021\n",
      "Epoch 226, Loss: 0.4699424675720642\n",
      "Epoch 227, Loss: 0.46999269112017344\n",
      "Epoch 228, Loss: 0.4698759384724347\n",
      "Epoch 229, Loss: 0.46938041574678274\n",
      "Epoch 230, Loss: 0.4691415581864629\n",
      "Epoch 231, Loss: 0.46924839544758656\n",
      "Epoch 232, Loss: 0.46933531775018905\n",
      "Epoch 233, Loss: 0.4692253881106229\n",
      "Epoch 234, Loss: 0.46894320638835874\n",
      "Epoch 235, Loss: 0.4688628060980674\n",
      "Epoch 236, Loss: 0.46897798398181567\n",
      "Epoch 237, Loss: 0.4689811295666005\n",
      "Epoch 238, Loss: 0.4688031484354944\n",
      "Epoch 239, Loss: 0.46862983752908405\n",
      "Epoch 240, Loss: 0.4685963696810109\n",
      "Epoch 241, Loss: 0.46867106090894\n",
      "Epoch 242, Loss: 0.46866360503981047\n",
      "Epoch 243, Loss: 0.468513635806399\n",
      "Epoch 244, Loss: 0.46835730315983837\n",
      "Epoch 245, Loss: 0.468344910742699\n",
      "Epoch 246, Loss: 0.46842993299443175\n",
      "Epoch 247, Loss: 0.4683803372309361\n",
      "Epoch 248, Loss: 0.46823408027364943\n",
      "Epoch 249, Loss: 0.46809559282800506\n",
      "Epoch 250, Loss: 0.4681194355765096\n",
      "Epoch 251, Loss: 0.4682059468617626\n",
      "Epoch 252, Loss: 0.46815125054377077\n",
      "Epoch 253, Loss: 0.46800465135626707\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.21872481096443092\n",
      "Valence RMSE: 0.20604356378424715\n",
      "Arousal RMSE: 0.23071006845376069\n",
      "Test R^2 score: tensor([0.4855, 0.4730], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.3651, 0.3496], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.47923133847693355\n",
      "Num of epochs: 254\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Epoch 121, Loss: 0.49295912913079615\n",
      "Epoch 122, Loss: 0.4924773619139838\n",
      "Epoch 123, Loss: 0.49197607191949805\n",
      "Epoch 124, Loss: 0.4914699197831528\n",
      "Epoch 125, Loss: 0.49091483381312667\n",
      "Epoch 126, Loss: 0.49035147677946317\n",
      "Epoch 127, Loss: 0.4898087222825037\n",
      "Epoch 128, Loss: 0.48924014731550836\n",
      "Epoch 129, Loss: 0.48873550647212044\n",
      "Epoch 130, Loss: 0.48832307254971563\n",
      "Epoch 131, Loss: 0.48791418392821295\n",
      "Epoch 132, Loss: 0.4875902245243796\n",
      "Epoch 133, Loss: 0.48728851108037585\n",
      "Epoch 134, Loss: 0.48697062262181345\n",
      "Epoch 135, Loss: 0.4866339553030548\n",
      "Epoch 136, Loss: 0.48633446746734893\n",
      "Epoch 137, Loss: 0.4860310087358514\n",
      "Epoch 138, Loss: 0.4856786412545615\n",
      "Epoch 139, Loss: 0.48531372107004\n",
      "Epoch 140, Loss: 0.48495712985488587\n",
      "Epoch 141, Loss: 0.4846131600652309\n",
      "Epoch 142, Loss: 0.48425966858652364\n",
      "Epoch 143, Loss: 0.4838914149236435\n",
      "Epoch 144, Loss: 0.48352437546282767\n",
      "Epoch 145, Loss: 0.4832299911937133\n",
      "Epoch 146, Loss: 0.4832586529465723\n",
      "Epoch 147, Loss: 0.48356744143113517\n",
      "Epoch 148, Loss: 0.4828108019011922\n",
      "Epoch 149, Loss: 0.4818912939738211\n",
      "Epoch 150, Loss: 0.48239416745575797\n",
      "Epoch 151, Loss: 0.4814952491721961\n",
      "Epoch 152, Loss: 0.4811822818820502\n",
      "Epoch 153, Loss: 0.4812753156716337\n",
      "Epoch 154, Loss: 0.4804387742282827\n",
      "Epoch 155, Loss: 0.48058189052417905\n",
      "Epoch 156, Loss: 0.4801288115497346\n",
      "Epoch 157, Loss: 0.4797465937246525\n",
      "Epoch 158, Loss: 0.4797622479529733\n",
      "Epoch 159, Loss: 0.4791664679844762\n",
      "Epoch 160, Loss: 0.47904359923301915\n",
      "Epoch 161, Loss: 0.4788483533353555\n",
      "Epoch 162, Loss: 0.4783709093142749\n",
      "Epoch 163, Loss: 0.4783027642591491\n",
      "Epoch 164, Loss: 0.4780003946284736\n",
      "Epoch 165, Loss: 0.47766356610156796\n",
      "Epoch 166, Loss: 0.47756940749162413\n",
      "Epoch 167, Loss: 0.4772720526560437\n",
      "Epoch 168, Loss: 0.477001191512605\n",
      "Epoch 169, Loss: 0.47690651155802954\n",
      "Epoch 170, Loss: 0.47663605231839845\n",
      "Epoch 171, Loss: 0.4763715079887003\n",
      "Epoch 172, Loss: 0.4762479807342815\n",
      "Epoch 173, Loss: 0.47603155482909787\n",
      "Epoch 174, Loss: 0.47580085926956445\n",
      "Epoch 175, Loss: 0.4756396378860013\n",
      "Epoch 176, Loss: 0.4754627232666358\n",
      "Epoch 177, Loss: 0.4752513954316692\n",
      "Epoch 178, Loss: 0.47507140350730925\n",
      "Epoch 179, Loss: 0.47492471273973497\n",
      "Epoch 180, Loss: 0.47475493910732763\n",
      "Epoch 181, Loss: 0.47455157018218247\n",
      "Epoch 182, Loss: 0.4743853224635176\n",
      "Epoch 183, Loss: 0.47424550491599726\n",
      "Epoch 184, Loss: 0.47408413178207975\n",
      "Epoch 185, Loss: 0.47390385374022453\n",
      "Epoch 186, Loss: 0.4737383066259127\n",
      "Epoch 187, Loss: 0.47358797785338436\n",
      "Epoch 188, Loss: 0.4734560449826861\n",
      "Epoch 189, Loss: 0.47335738206045025\n",
      "Epoch 190, Loss: 0.4733162834925985\n",
      "Epoch 191, Loss: 0.4732413020737725\n",
      "Epoch 192, Loss: 0.47313239020014447\n",
      "Epoch 193, Loss: 0.4729457158004841\n",
      "Epoch 194, Loss: 0.47274703738102797\n",
      "Epoch 195, Loss: 0.47253750657270627\n",
      "Epoch 196, Loss: 0.4723544299968521\n",
      "Epoch 197, Loss: 0.47221102907746554\n",
      "Epoch 198, Loss: 0.47210844467503205\n",
      "Epoch 199, Loss: 0.47205337980155637\n",
      "Epoch 200, Loss: 0.47204973383515075\n",
      "Epoch 201, Loss: 0.4720427890593303\n",
      "Epoch 202, Loss: 0.4719159657967507\n",
      "Epoch 203, Loss: 0.471735017109987\n",
      "Epoch 204, Loss: 0.4714985533244086\n",
      "Epoch 205, Loss: 0.4713143302002759\n",
      "Epoch 206, Loss: 0.47118574544243713\n",
      "Epoch 207, Loss: 0.47112002449718005\n",
      "Epoch 208, Loss: 0.4710910670518786\n",
      "Epoch 209, Loss: 0.47102495326923743\n",
      "Epoch 210, Loss: 0.4709626586360175\n",
      "Epoch 211, Loss: 0.47085840979895394\n",
      "Epoch 212, Loss: 0.47078064719388485\n",
      "Epoch 213, Loss: 0.47066614789276784\n",
      "Epoch 214, Loss: 0.47055654499036753\n",
      "Epoch 215, Loss: 0.47041652390832933\n",
      "Epoch 216, Loss: 0.4702446473560784\n",
      "Epoch 217, Loss: 0.4701045016774931\n",
      "Epoch 218, Loss: 0.47002059077245106\n",
      "Epoch 219, Loss: 0.4699643617672827\n",
      "Epoch 220, Loss: 0.4698888296176042\n",
      "Epoch 221, Loss: 0.4698141575467262\n",
      "Epoch 222, Loss: 0.4697265783615268\n",
      "Epoch 223, Loss: 0.46966066909468496\n",
      "Epoch 224, Loss: 0.4696348739248034\n",
      "Epoch 225, Loss: 0.4697176006517021\n",
      "Epoch 226, Loss: 0.4699424675720642\n",
      "Epoch 227, Loss: 0.46999269112017344\n",
      "Epoch 228, Loss: 0.4698759384724347\n",
      "Epoch 229, Loss: 0.46938041574678274\n",
      "Epoch 230, Loss: 0.4691415581864629\n",
      "Epoch 231, Loss: 0.46924839544758656\n",
      "Epoch 232, Loss: 0.46933531775018905\n",
      "Epoch 233, Loss: 0.4692253881106229\n",
      "Epoch 234, Loss: 0.46894320638835874\n",
      "Epoch 235, Loss: 0.4688628060980674\n",
      "Epoch 236, Loss: 0.46897798398181567\n",
      "Epoch 237, Loss: 0.4689811295666005\n",
      "Epoch 238, Loss: 0.4688031484354944\n",
      "Epoch 239, Loss: 0.46862983752908405\n",
      "Epoch 240, Loss: 0.4685963696810109\n",
      "Epoch 241, Loss: 0.46867106090894\n",
      "Epoch 242, Loss: 0.46866360503981047\n",
      "Epoch 243, Loss: 0.468513635806399\n",
      "Epoch 244, Loss: 0.46835730315983837\n",
      "Epoch 245, Loss: 0.468344910742699\n",
      "Epoch 246, Loss: 0.46842993299443175\n",
      "Epoch 247, Loss: 0.4683803372309361\n",
      "Epoch 248, Loss: 0.46823408027364943\n",
      "Epoch 249, Loss: 0.46809559282800506\n",
      "Epoch 250, Loss: 0.4681194355765096\n",
      "Epoch 251, Loss: 0.4682059468617626\n",
      "Epoch 252, Loss: 0.46815125054377077\n",
      "Epoch 253, Loss: 0.46800465135626707\n",
      "Epoch 254, Loss: 0.4678516840148294\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.21862035945446712\n",
      "Valence RMSE: 0.2059393667954795\n",
      "Arousal RMSE: 0.23060507440183914\n",
      "Test R^2 score: tensor([0.4860, 0.4734], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.3657, 0.3502], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.47973125250635107\n",
      "Num of epochs: 255\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Epoch 121, Loss: 0.49295912913079615\n",
      "Epoch 122, Loss: 0.4924773619139838\n",
      "Epoch 123, Loss: 0.49197607191949805\n",
      "Epoch 124, Loss: 0.4914699197831528\n",
      "Epoch 125, Loss: 0.49091483381312667\n",
      "Epoch 126, Loss: 0.49035147677946317\n",
      "Epoch 127, Loss: 0.4898087222825037\n",
      "Epoch 128, Loss: 0.48924014731550836\n",
      "Epoch 129, Loss: 0.48873550647212044\n",
      "Epoch 130, Loss: 0.48832307254971563\n",
      "Epoch 131, Loss: 0.48791418392821295\n",
      "Epoch 132, Loss: 0.4875902245243796\n",
      "Epoch 133, Loss: 0.48728851108037585\n",
      "Epoch 134, Loss: 0.48697062262181345\n",
      "Epoch 135, Loss: 0.4866339553030548\n",
      "Epoch 136, Loss: 0.48633446746734893\n",
      "Epoch 137, Loss: 0.4860310087358514\n",
      "Epoch 138, Loss: 0.4856786412545615\n",
      "Epoch 139, Loss: 0.48531372107004\n",
      "Epoch 140, Loss: 0.48495712985488587\n",
      "Epoch 141, Loss: 0.4846131600652309\n",
      "Epoch 142, Loss: 0.48425966858652364\n",
      "Epoch 143, Loss: 0.4838914149236435\n",
      "Epoch 144, Loss: 0.48352437546282767\n",
      "Epoch 145, Loss: 0.4832299911937133\n",
      "Epoch 146, Loss: 0.4832586529465723\n",
      "Epoch 147, Loss: 0.48356744143113517\n",
      "Epoch 148, Loss: 0.4828108019011922\n",
      "Epoch 149, Loss: 0.4818912939738211\n",
      "Epoch 150, Loss: 0.48239416745575797\n",
      "Epoch 151, Loss: 0.4814952491721961\n",
      "Epoch 152, Loss: 0.4811822818820502\n",
      "Epoch 153, Loss: 0.4812753156716337\n",
      "Epoch 154, Loss: 0.4804387742282827\n",
      "Epoch 155, Loss: 0.48058189052417905\n",
      "Epoch 156, Loss: 0.4801288115497346\n",
      "Epoch 157, Loss: 0.4797465937246525\n",
      "Epoch 158, Loss: 0.4797622479529733\n",
      "Epoch 159, Loss: 0.4791664679844762\n",
      "Epoch 160, Loss: 0.47904359923301915\n",
      "Epoch 161, Loss: 0.4788483533353555\n",
      "Epoch 162, Loss: 0.4783709093142749\n",
      "Epoch 163, Loss: 0.4783027642591491\n",
      "Epoch 164, Loss: 0.4780003946284736\n",
      "Epoch 165, Loss: 0.47766356610156796\n",
      "Epoch 166, Loss: 0.47756940749162413\n",
      "Epoch 167, Loss: 0.4772720526560437\n",
      "Epoch 168, Loss: 0.477001191512605\n",
      "Epoch 169, Loss: 0.47690651155802954\n",
      "Epoch 170, Loss: 0.47663605231839845\n",
      "Epoch 171, Loss: 0.4763715079887003\n",
      "Epoch 172, Loss: 0.4762479807342815\n",
      "Epoch 173, Loss: 0.47603155482909787\n",
      "Epoch 174, Loss: 0.47580085926956445\n",
      "Epoch 175, Loss: 0.4756396378860013\n",
      "Epoch 176, Loss: 0.4754627232666358\n",
      "Epoch 177, Loss: 0.4752513954316692\n",
      "Epoch 178, Loss: 0.47507140350730925\n",
      "Epoch 179, Loss: 0.47492471273973497\n",
      "Epoch 180, Loss: 0.47475493910732763\n",
      "Epoch 181, Loss: 0.47455157018218247\n",
      "Epoch 182, Loss: 0.4743853224635176\n",
      "Epoch 183, Loss: 0.47424550491599726\n",
      "Epoch 184, Loss: 0.47408413178207975\n",
      "Epoch 185, Loss: 0.47390385374022453\n",
      "Epoch 186, Loss: 0.4737383066259127\n",
      "Epoch 187, Loss: 0.47358797785338436\n",
      "Epoch 188, Loss: 0.4734560449826861\n",
      "Epoch 189, Loss: 0.47335738206045025\n",
      "Epoch 190, Loss: 0.4733162834925985\n",
      "Epoch 191, Loss: 0.4732413020737725\n",
      "Epoch 192, Loss: 0.47313239020014447\n",
      "Epoch 193, Loss: 0.4729457158004841\n",
      "Epoch 194, Loss: 0.47274703738102797\n",
      "Epoch 195, Loss: 0.47253750657270627\n",
      "Epoch 196, Loss: 0.4723544299968521\n",
      "Epoch 197, Loss: 0.47221102907746554\n",
      "Epoch 198, Loss: 0.47210844467503205\n",
      "Epoch 199, Loss: 0.47205337980155637\n",
      "Epoch 200, Loss: 0.47204973383515075\n",
      "Epoch 201, Loss: 0.4720427890593303\n",
      "Epoch 202, Loss: 0.4719159657967507\n",
      "Epoch 203, Loss: 0.471735017109987\n",
      "Epoch 204, Loss: 0.4714985533244086\n",
      "Epoch 205, Loss: 0.4713143302002759\n",
      "Epoch 206, Loss: 0.47118574544243713\n",
      "Epoch 207, Loss: 0.47112002449718005\n",
      "Epoch 208, Loss: 0.4710910670518786\n",
      "Epoch 209, Loss: 0.47102495326923743\n",
      "Epoch 210, Loss: 0.4709626586360175\n",
      "Epoch 211, Loss: 0.47085840979895394\n",
      "Epoch 212, Loss: 0.47078064719388485\n",
      "Epoch 213, Loss: 0.47066614789276784\n",
      "Epoch 214, Loss: 0.47055654499036753\n",
      "Epoch 215, Loss: 0.47041652390832933\n",
      "Epoch 216, Loss: 0.4702446473560784\n",
      "Epoch 217, Loss: 0.4701045016774931\n",
      "Epoch 218, Loss: 0.47002059077245106\n",
      "Epoch 219, Loss: 0.4699643617672827\n",
      "Epoch 220, Loss: 0.4698888296176042\n",
      "Epoch 221, Loss: 0.4698141575467262\n",
      "Epoch 222, Loss: 0.4697265783615268\n",
      "Epoch 223, Loss: 0.46966066909468496\n",
      "Epoch 224, Loss: 0.4696348739248034\n",
      "Epoch 225, Loss: 0.4697176006517021\n",
      "Epoch 226, Loss: 0.4699424675720642\n",
      "Epoch 227, Loss: 0.46999269112017344\n",
      "Epoch 228, Loss: 0.4698759384724347\n",
      "Epoch 229, Loss: 0.46938041574678274\n",
      "Epoch 230, Loss: 0.4691415581864629\n",
      "Epoch 231, Loss: 0.46924839544758656\n",
      "Epoch 232, Loss: 0.46933531775018905\n",
      "Epoch 233, Loss: 0.4692253881106229\n",
      "Epoch 234, Loss: 0.46894320638835874\n",
      "Epoch 235, Loss: 0.4688628060980674\n",
      "Epoch 236, Loss: 0.46897798398181567\n",
      "Epoch 237, Loss: 0.4689811295666005\n",
      "Epoch 238, Loss: 0.4688031484354944\n",
      "Epoch 239, Loss: 0.46862983752908405\n",
      "Epoch 240, Loss: 0.4685963696810109\n",
      "Epoch 241, Loss: 0.46867106090894\n",
      "Epoch 242, Loss: 0.46866360503981047\n",
      "Epoch 243, Loss: 0.468513635806399\n",
      "Epoch 244, Loss: 0.46835730315983837\n",
      "Epoch 245, Loss: 0.468344910742699\n",
      "Epoch 246, Loss: 0.46842993299443175\n",
      "Epoch 247, Loss: 0.4683803372309361\n",
      "Epoch 248, Loss: 0.46823408027364943\n",
      "Epoch 249, Loss: 0.46809559282800506\n",
      "Epoch 250, Loss: 0.4681194355765096\n",
      "Epoch 251, Loss: 0.4682059468617626\n",
      "Epoch 252, Loss: 0.46815125054377077\n",
      "Epoch 253, Loss: 0.46800465135626707\n",
      "Epoch 254, Loss: 0.4678516840148294\n",
      "Epoch 255, Loss: 0.46781997608436393\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.21943922455379855\n",
      "Valence RMSE: 0.20674297463448513\n",
      "Arousal RMSE: 0.23144003323718548\n",
      "Test R^2 score: tensor([0.4820, 0.4696], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.3608, 0.3455], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4758117344373568\n",
      "Num of epochs: 256\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Epoch 121, Loss: 0.49295912913079615\n",
      "Epoch 122, Loss: 0.4924773619139838\n",
      "Epoch 123, Loss: 0.49197607191949805\n",
      "Epoch 124, Loss: 0.4914699197831528\n",
      "Epoch 125, Loss: 0.49091483381312667\n",
      "Epoch 126, Loss: 0.49035147677946317\n",
      "Epoch 127, Loss: 0.4898087222825037\n",
      "Epoch 128, Loss: 0.48924014731550836\n",
      "Epoch 129, Loss: 0.48873550647212044\n",
      "Epoch 130, Loss: 0.48832307254971563\n",
      "Epoch 131, Loss: 0.48791418392821295\n",
      "Epoch 132, Loss: 0.4875902245243796\n",
      "Epoch 133, Loss: 0.48728851108037585\n",
      "Epoch 134, Loss: 0.48697062262181345\n",
      "Epoch 135, Loss: 0.4866339553030548\n",
      "Epoch 136, Loss: 0.48633446746734893\n",
      "Epoch 137, Loss: 0.4860310087358514\n",
      "Epoch 138, Loss: 0.4856786412545615\n",
      "Epoch 139, Loss: 0.48531372107004\n",
      "Epoch 140, Loss: 0.48495712985488587\n",
      "Epoch 141, Loss: 0.4846131600652309\n",
      "Epoch 142, Loss: 0.48425966858652364\n",
      "Epoch 143, Loss: 0.4838914149236435\n",
      "Epoch 144, Loss: 0.48352437546282767\n",
      "Epoch 145, Loss: 0.4832299911937133\n",
      "Epoch 146, Loss: 0.4832586529465723\n",
      "Epoch 147, Loss: 0.48356744143113517\n",
      "Epoch 148, Loss: 0.4828108019011922\n",
      "Epoch 149, Loss: 0.4818912939738211\n",
      "Epoch 150, Loss: 0.48239416745575797\n",
      "Epoch 151, Loss: 0.4814952491721961\n",
      "Epoch 152, Loss: 0.4811822818820502\n",
      "Epoch 153, Loss: 0.4812753156716337\n",
      "Epoch 154, Loss: 0.4804387742282827\n",
      "Epoch 155, Loss: 0.48058189052417905\n",
      "Epoch 156, Loss: 0.4801288115497346\n",
      "Epoch 157, Loss: 0.4797465937246525\n",
      "Epoch 158, Loss: 0.4797622479529733\n",
      "Epoch 159, Loss: 0.4791664679844762\n",
      "Epoch 160, Loss: 0.47904359923301915\n",
      "Epoch 161, Loss: 0.4788483533353555\n",
      "Epoch 162, Loss: 0.4783709093142749\n",
      "Epoch 163, Loss: 0.4783027642591491\n",
      "Epoch 164, Loss: 0.4780003946284736\n",
      "Epoch 165, Loss: 0.47766356610156796\n",
      "Epoch 166, Loss: 0.47756940749162413\n",
      "Epoch 167, Loss: 0.4772720526560437\n",
      "Epoch 168, Loss: 0.477001191512605\n",
      "Epoch 169, Loss: 0.47690651155802954\n",
      "Epoch 170, Loss: 0.47663605231839845\n",
      "Epoch 171, Loss: 0.4763715079887003\n",
      "Epoch 172, Loss: 0.4762479807342815\n",
      "Epoch 173, Loss: 0.47603155482909787\n",
      "Epoch 174, Loss: 0.47580085926956445\n",
      "Epoch 175, Loss: 0.4756396378860013\n",
      "Epoch 176, Loss: 0.4754627232666358\n",
      "Epoch 177, Loss: 0.4752513954316692\n",
      "Epoch 178, Loss: 0.47507140350730925\n",
      "Epoch 179, Loss: 0.47492471273973497\n",
      "Epoch 180, Loss: 0.47475493910732763\n",
      "Epoch 181, Loss: 0.47455157018218247\n",
      "Epoch 182, Loss: 0.4743853224635176\n",
      "Epoch 183, Loss: 0.47424550491599726\n",
      "Epoch 184, Loss: 0.47408413178207975\n",
      "Epoch 185, Loss: 0.47390385374022453\n",
      "Epoch 186, Loss: 0.4737383066259127\n",
      "Epoch 187, Loss: 0.47358797785338436\n",
      "Epoch 188, Loss: 0.4734560449826861\n",
      "Epoch 189, Loss: 0.47335738206045025\n",
      "Epoch 190, Loss: 0.4733162834925985\n",
      "Epoch 191, Loss: 0.4732413020737725\n",
      "Epoch 192, Loss: 0.47313239020014447\n",
      "Epoch 193, Loss: 0.4729457158004841\n",
      "Epoch 194, Loss: 0.47274703738102797\n",
      "Epoch 195, Loss: 0.47253750657270627\n",
      "Epoch 196, Loss: 0.4723544299968521\n",
      "Epoch 197, Loss: 0.47221102907746554\n",
      "Epoch 198, Loss: 0.47210844467503205\n",
      "Epoch 199, Loss: 0.47205337980155637\n",
      "Epoch 200, Loss: 0.47204973383515075\n",
      "Epoch 201, Loss: 0.4720427890593303\n",
      "Epoch 202, Loss: 0.4719159657967507\n",
      "Epoch 203, Loss: 0.471735017109987\n",
      "Epoch 204, Loss: 0.4714985533244086\n",
      "Epoch 205, Loss: 0.4713143302002759\n",
      "Epoch 206, Loss: 0.47118574544243713\n",
      "Epoch 207, Loss: 0.47112002449718005\n",
      "Epoch 208, Loss: 0.4710910670518786\n",
      "Epoch 209, Loss: 0.47102495326923743\n",
      "Epoch 210, Loss: 0.4709626586360175\n",
      "Epoch 211, Loss: 0.47085840979895394\n",
      "Epoch 212, Loss: 0.47078064719388485\n",
      "Epoch 213, Loss: 0.47066614789276784\n",
      "Epoch 214, Loss: 0.47055654499036753\n",
      "Epoch 215, Loss: 0.47041652390832933\n",
      "Epoch 216, Loss: 0.4702446473560784\n",
      "Epoch 217, Loss: 0.4701045016774931\n",
      "Epoch 218, Loss: 0.47002059077245106\n",
      "Epoch 219, Loss: 0.4699643617672827\n",
      "Epoch 220, Loss: 0.4698888296176042\n",
      "Epoch 221, Loss: 0.4698141575467262\n",
      "Epoch 222, Loss: 0.4697265783615268\n",
      "Epoch 223, Loss: 0.46966066909468496\n",
      "Epoch 224, Loss: 0.4696348739248034\n",
      "Epoch 225, Loss: 0.4697176006517021\n",
      "Epoch 226, Loss: 0.4699424675720642\n",
      "Epoch 227, Loss: 0.46999269112017344\n",
      "Epoch 228, Loss: 0.4698759384724347\n",
      "Epoch 229, Loss: 0.46938041574678274\n",
      "Epoch 230, Loss: 0.4691415581864629\n",
      "Epoch 231, Loss: 0.46924839544758656\n",
      "Epoch 232, Loss: 0.46933531775018905\n",
      "Epoch 233, Loss: 0.4692253881106229\n",
      "Epoch 234, Loss: 0.46894320638835874\n",
      "Epoch 235, Loss: 0.4688628060980674\n",
      "Epoch 236, Loss: 0.46897798398181567\n",
      "Epoch 237, Loss: 0.4689811295666005\n",
      "Epoch 238, Loss: 0.4688031484354944\n",
      "Epoch 239, Loss: 0.46862983752908405\n",
      "Epoch 240, Loss: 0.4685963696810109\n",
      "Epoch 241, Loss: 0.46867106090894\n",
      "Epoch 242, Loss: 0.46866360503981047\n",
      "Epoch 243, Loss: 0.468513635806399\n",
      "Epoch 244, Loss: 0.46835730315983837\n",
      "Epoch 245, Loss: 0.468344910742699\n",
      "Epoch 246, Loss: 0.46842993299443175\n",
      "Epoch 247, Loss: 0.4683803372309361\n",
      "Epoch 248, Loss: 0.46823408027364943\n",
      "Epoch 249, Loss: 0.46809559282800506\n",
      "Epoch 250, Loss: 0.4681194355765096\n",
      "Epoch 251, Loss: 0.4682059468617626\n",
      "Epoch 252, Loss: 0.46815125054377077\n",
      "Epoch 253, Loss: 0.46800465135626707\n",
      "Epoch 254, Loss: 0.4678516840148294\n",
      "Epoch 255, Loss: 0.46781997608436393\n",
      "Epoch 256, Loss: 0.4678506648079015\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.21842115473708457\n",
      "Valence RMSE: 0.2057340765111022\n",
      "Arousal RMSE: 0.2304107016513921\n",
      "Test R^2 score: tensor([0.4870, 0.4743], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.3670, 0.3513], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.48068699553725547\n",
      "Num of epochs: 257\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Epoch 121, Loss: 0.49295912913079615\n",
      "Epoch 122, Loss: 0.4924773619139838\n",
      "Epoch 123, Loss: 0.49197607191949805\n",
      "Epoch 124, Loss: 0.4914699197831528\n",
      "Epoch 125, Loss: 0.49091483381312667\n",
      "Epoch 126, Loss: 0.49035147677946317\n",
      "Epoch 127, Loss: 0.4898087222825037\n",
      "Epoch 128, Loss: 0.48924014731550836\n",
      "Epoch 129, Loss: 0.48873550647212044\n",
      "Epoch 130, Loss: 0.48832307254971563\n",
      "Epoch 131, Loss: 0.48791418392821295\n",
      "Epoch 132, Loss: 0.4875902245243796\n",
      "Epoch 133, Loss: 0.48728851108037585\n",
      "Epoch 134, Loss: 0.48697062262181345\n",
      "Epoch 135, Loss: 0.4866339553030548\n",
      "Epoch 136, Loss: 0.48633446746734893\n",
      "Epoch 137, Loss: 0.4860310087358514\n",
      "Epoch 138, Loss: 0.4856786412545615\n",
      "Epoch 139, Loss: 0.48531372107004\n",
      "Epoch 140, Loss: 0.48495712985488587\n",
      "Epoch 141, Loss: 0.4846131600652309\n",
      "Epoch 142, Loss: 0.48425966858652364\n",
      "Epoch 143, Loss: 0.4838914149236435\n",
      "Epoch 144, Loss: 0.48352437546282767\n",
      "Epoch 145, Loss: 0.4832299911937133\n",
      "Epoch 146, Loss: 0.4832586529465723\n",
      "Epoch 147, Loss: 0.48356744143113517\n",
      "Epoch 148, Loss: 0.4828108019011922\n",
      "Epoch 149, Loss: 0.4818912939738211\n",
      "Epoch 150, Loss: 0.48239416745575797\n",
      "Epoch 151, Loss: 0.4814952491721961\n",
      "Epoch 152, Loss: 0.4811822818820502\n",
      "Epoch 153, Loss: 0.4812753156716337\n",
      "Epoch 154, Loss: 0.4804387742282827\n",
      "Epoch 155, Loss: 0.48058189052417905\n",
      "Epoch 156, Loss: 0.4801288115497346\n",
      "Epoch 157, Loss: 0.4797465937246525\n",
      "Epoch 158, Loss: 0.4797622479529733\n",
      "Epoch 159, Loss: 0.4791664679844762\n",
      "Epoch 160, Loss: 0.47904359923301915\n",
      "Epoch 161, Loss: 0.4788483533353555\n",
      "Epoch 162, Loss: 0.4783709093142749\n",
      "Epoch 163, Loss: 0.4783027642591491\n",
      "Epoch 164, Loss: 0.4780003946284736\n",
      "Epoch 165, Loss: 0.47766356610156796\n",
      "Epoch 166, Loss: 0.47756940749162413\n",
      "Epoch 167, Loss: 0.4772720526560437\n",
      "Epoch 168, Loss: 0.477001191512605\n",
      "Epoch 169, Loss: 0.47690651155802954\n",
      "Epoch 170, Loss: 0.47663605231839845\n",
      "Epoch 171, Loss: 0.4763715079887003\n",
      "Epoch 172, Loss: 0.4762479807342815\n",
      "Epoch 173, Loss: 0.47603155482909787\n",
      "Epoch 174, Loss: 0.47580085926956445\n",
      "Epoch 175, Loss: 0.4756396378860013\n",
      "Epoch 176, Loss: 0.4754627232666358\n",
      "Epoch 177, Loss: 0.4752513954316692\n",
      "Epoch 178, Loss: 0.47507140350730925\n",
      "Epoch 179, Loss: 0.47492471273973497\n",
      "Epoch 180, Loss: 0.47475493910732763\n",
      "Epoch 181, Loss: 0.47455157018218247\n",
      "Epoch 182, Loss: 0.4743853224635176\n",
      "Epoch 183, Loss: 0.47424550491599726\n",
      "Epoch 184, Loss: 0.47408413178207975\n",
      "Epoch 185, Loss: 0.47390385374022453\n",
      "Epoch 186, Loss: 0.4737383066259127\n",
      "Epoch 187, Loss: 0.47358797785338436\n",
      "Epoch 188, Loss: 0.4734560449826861\n",
      "Epoch 189, Loss: 0.47335738206045025\n",
      "Epoch 190, Loss: 0.4733162834925985\n",
      "Epoch 191, Loss: 0.4732413020737725\n",
      "Epoch 192, Loss: 0.47313239020014447\n",
      "Epoch 193, Loss: 0.4729457158004841\n",
      "Epoch 194, Loss: 0.47274703738102797\n",
      "Epoch 195, Loss: 0.47253750657270627\n",
      "Epoch 196, Loss: 0.4723544299968521\n",
      "Epoch 197, Loss: 0.47221102907746554\n",
      "Epoch 198, Loss: 0.47210844467503205\n",
      "Epoch 199, Loss: 0.47205337980155637\n",
      "Epoch 200, Loss: 0.47204973383515075\n",
      "Epoch 201, Loss: 0.4720427890593303\n",
      "Epoch 202, Loss: 0.4719159657967507\n",
      "Epoch 203, Loss: 0.471735017109987\n",
      "Epoch 204, Loss: 0.4714985533244086\n",
      "Epoch 205, Loss: 0.4713143302002759\n",
      "Epoch 206, Loss: 0.47118574544243713\n",
      "Epoch 207, Loss: 0.47112002449718005\n",
      "Epoch 208, Loss: 0.4710910670518786\n",
      "Epoch 209, Loss: 0.47102495326923743\n",
      "Epoch 210, Loss: 0.4709626586360175\n",
      "Epoch 211, Loss: 0.47085840979895394\n",
      "Epoch 212, Loss: 0.47078064719388485\n",
      "Epoch 213, Loss: 0.47066614789276784\n",
      "Epoch 214, Loss: 0.47055654499036753\n",
      "Epoch 215, Loss: 0.47041652390832933\n",
      "Epoch 216, Loss: 0.4702446473560784\n",
      "Epoch 217, Loss: 0.4701045016774931\n",
      "Epoch 218, Loss: 0.47002059077245106\n",
      "Epoch 219, Loss: 0.4699643617672827\n",
      "Epoch 220, Loss: 0.4698888296176042\n",
      "Epoch 221, Loss: 0.4698141575467262\n",
      "Epoch 222, Loss: 0.4697265783615268\n",
      "Epoch 223, Loss: 0.46966066909468496\n",
      "Epoch 224, Loss: 0.4696348739248034\n",
      "Epoch 225, Loss: 0.4697176006517021\n",
      "Epoch 226, Loss: 0.4699424675720642\n",
      "Epoch 227, Loss: 0.46999269112017344\n",
      "Epoch 228, Loss: 0.4698759384724347\n",
      "Epoch 229, Loss: 0.46938041574678274\n",
      "Epoch 230, Loss: 0.4691415581864629\n",
      "Epoch 231, Loss: 0.46924839544758656\n",
      "Epoch 232, Loss: 0.46933531775018905\n",
      "Epoch 233, Loss: 0.4692253881106229\n",
      "Epoch 234, Loss: 0.46894320638835874\n",
      "Epoch 235, Loss: 0.4688628060980674\n",
      "Epoch 236, Loss: 0.46897798398181567\n",
      "Epoch 237, Loss: 0.4689811295666005\n",
      "Epoch 238, Loss: 0.4688031484354944\n",
      "Epoch 239, Loss: 0.46862983752908405\n",
      "Epoch 240, Loss: 0.4685963696810109\n",
      "Epoch 241, Loss: 0.46867106090894\n",
      "Epoch 242, Loss: 0.46866360503981047\n",
      "Epoch 243, Loss: 0.468513635806399\n",
      "Epoch 244, Loss: 0.46835730315983837\n",
      "Epoch 245, Loss: 0.468344910742699\n",
      "Epoch 246, Loss: 0.46842993299443175\n",
      "Epoch 247, Loss: 0.4683803372309361\n",
      "Epoch 248, Loss: 0.46823408027364943\n",
      "Epoch 249, Loss: 0.46809559282800506\n",
      "Epoch 250, Loss: 0.4681194355765096\n",
      "Epoch 251, Loss: 0.4682059468617626\n",
      "Epoch 252, Loss: 0.46815125054377077\n",
      "Epoch 253, Loss: 0.46800465135626707\n",
      "Epoch 254, Loss: 0.4678516840148294\n",
      "Epoch 255, Loss: 0.46781997608436393\n",
      "Epoch 256, Loss: 0.4678506648079015\n",
      "Epoch 257, Loss: 0.4677995105079106\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.219157089557255\n",
      "Valence RMSE: 0.20646647762228998\n",
      "Arousal RMSE: 0.2311520136720807\n",
      "Test R^2 score: tensor([0.4834, 0.4709], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.3625, 0.3471], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4771636671758155\n",
      "Num of epochs: 258\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Epoch 121, Loss: 0.49295912913079615\n",
      "Epoch 122, Loss: 0.4924773619139838\n",
      "Epoch 123, Loss: 0.49197607191949805\n",
      "Epoch 124, Loss: 0.4914699197831528\n",
      "Epoch 125, Loss: 0.49091483381312667\n",
      "Epoch 126, Loss: 0.49035147677946317\n",
      "Epoch 127, Loss: 0.4898087222825037\n",
      "Epoch 128, Loss: 0.48924014731550836\n",
      "Epoch 129, Loss: 0.48873550647212044\n",
      "Epoch 130, Loss: 0.48832307254971563\n",
      "Epoch 131, Loss: 0.48791418392821295\n",
      "Epoch 132, Loss: 0.4875902245243796\n",
      "Epoch 133, Loss: 0.48728851108037585\n",
      "Epoch 134, Loss: 0.48697062262181345\n",
      "Epoch 135, Loss: 0.4866339553030548\n",
      "Epoch 136, Loss: 0.48633446746734893\n",
      "Epoch 137, Loss: 0.4860310087358514\n",
      "Epoch 138, Loss: 0.4856786412545615\n",
      "Epoch 139, Loss: 0.48531372107004\n",
      "Epoch 140, Loss: 0.48495712985488587\n",
      "Epoch 141, Loss: 0.4846131600652309\n",
      "Epoch 142, Loss: 0.48425966858652364\n",
      "Epoch 143, Loss: 0.4838914149236435\n",
      "Epoch 144, Loss: 0.48352437546282767\n",
      "Epoch 145, Loss: 0.4832299911937133\n",
      "Epoch 146, Loss: 0.4832586529465723\n",
      "Epoch 147, Loss: 0.48356744143113517\n",
      "Epoch 148, Loss: 0.4828108019011922\n",
      "Epoch 149, Loss: 0.4818912939738211\n",
      "Epoch 150, Loss: 0.48239416745575797\n",
      "Epoch 151, Loss: 0.4814952491721961\n",
      "Epoch 152, Loss: 0.4811822818820502\n",
      "Epoch 153, Loss: 0.4812753156716337\n",
      "Epoch 154, Loss: 0.4804387742282827\n",
      "Epoch 155, Loss: 0.48058189052417905\n",
      "Epoch 156, Loss: 0.4801288115497346\n",
      "Epoch 157, Loss: 0.4797465937246525\n",
      "Epoch 158, Loss: 0.4797622479529733\n",
      "Epoch 159, Loss: 0.4791664679844762\n",
      "Epoch 160, Loss: 0.47904359923301915\n",
      "Epoch 161, Loss: 0.4788483533353555\n",
      "Epoch 162, Loss: 0.4783709093142749\n",
      "Epoch 163, Loss: 0.4783027642591491\n",
      "Epoch 164, Loss: 0.4780003946284736\n",
      "Epoch 165, Loss: 0.47766356610156796\n",
      "Epoch 166, Loss: 0.47756940749162413\n",
      "Epoch 167, Loss: 0.4772720526560437\n",
      "Epoch 168, Loss: 0.477001191512605\n",
      "Epoch 169, Loss: 0.47690651155802954\n",
      "Epoch 170, Loss: 0.47663605231839845\n",
      "Epoch 171, Loss: 0.4763715079887003\n",
      "Epoch 172, Loss: 0.4762479807342815\n",
      "Epoch 173, Loss: 0.47603155482909787\n",
      "Epoch 174, Loss: 0.47580085926956445\n",
      "Epoch 175, Loss: 0.4756396378860013\n",
      "Epoch 176, Loss: 0.4754627232666358\n",
      "Epoch 177, Loss: 0.4752513954316692\n",
      "Epoch 178, Loss: 0.47507140350730925\n",
      "Epoch 179, Loss: 0.47492471273973497\n",
      "Epoch 180, Loss: 0.47475493910732763\n",
      "Epoch 181, Loss: 0.47455157018218247\n",
      "Epoch 182, Loss: 0.4743853224635176\n",
      "Epoch 183, Loss: 0.47424550491599726\n",
      "Epoch 184, Loss: 0.47408413178207975\n",
      "Epoch 185, Loss: 0.47390385374022453\n",
      "Epoch 186, Loss: 0.4737383066259127\n",
      "Epoch 187, Loss: 0.47358797785338436\n",
      "Epoch 188, Loss: 0.4734560449826861\n",
      "Epoch 189, Loss: 0.47335738206045025\n",
      "Epoch 190, Loss: 0.4733162834925985\n",
      "Epoch 191, Loss: 0.4732413020737725\n",
      "Epoch 192, Loss: 0.47313239020014447\n",
      "Epoch 193, Loss: 0.4729457158004841\n",
      "Epoch 194, Loss: 0.47274703738102797\n",
      "Epoch 195, Loss: 0.47253750657270627\n",
      "Epoch 196, Loss: 0.4723544299968521\n",
      "Epoch 197, Loss: 0.47221102907746554\n",
      "Epoch 198, Loss: 0.47210844467503205\n",
      "Epoch 199, Loss: 0.47205337980155637\n",
      "Epoch 200, Loss: 0.47204973383515075\n",
      "Epoch 201, Loss: 0.4720427890593303\n",
      "Epoch 202, Loss: 0.4719159657967507\n",
      "Epoch 203, Loss: 0.471735017109987\n",
      "Epoch 204, Loss: 0.4714985533244086\n",
      "Epoch 205, Loss: 0.4713143302002759\n",
      "Epoch 206, Loss: 0.47118574544243713\n",
      "Epoch 207, Loss: 0.47112002449718005\n",
      "Epoch 208, Loss: 0.4710910670518786\n",
      "Epoch 209, Loss: 0.47102495326923743\n",
      "Epoch 210, Loss: 0.4709626586360175\n",
      "Epoch 211, Loss: 0.47085840979895394\n",
      "Epoch 212, Loss: 0.47078064719388485\n",
      "Epoch 213, Loss: 0.47066614789276784\n",
      "Epoch 214, Loss: 0.47055654499036753\n",
      "Epoch 215, Loss: 0.47041652390832933\n",
      "Epoch 216, Loss: 0.4702446473560784\n",
      "Epoch 217, Loss: 0.4701045016774931\n",
      "Epoch 218, Loss: 0.47002059077245106\n",
      "Epoch 219, Loss: 0.4699643617672827\n",
      "Epoch 220, Loss: 0.4698888296176042\n",
      "Epoch 221, Loss: 0.4698141575467262\n",
      "Epoch 222, Loss: 0.4697265783615268\n",
      "Epoch 223, Loss: 0.46966066909468496\n",
      "Epoch 224, Loss: 0.4696348739248034\n",
      "Epoch 225, Loss: 0.4697176006517021\n",
      "Epoch 226, Loss: 0.4699424675720642\n",
      "Epoch 227, Loss: 0.46999269112017344\n",
      "Epoch 228, Loss: 0.4698759384724347\n",
      "Epoch 229, Loss: 0.46938041574678274\n",
      "Epoch 230, Loss: 0.4691415581864629\n",
      "Epoch 231, Loss: 0.46924839544758656\n",
      "Epoch 232, Loss: 0.46933531775018905\n",
      "Epoch 233, Loss: 0.4692253881106229\n",
      "Epoch 234, Loss: 0.46894320638835874\n",
      "Epoch 235, Loss: 0.4688628060980674\n",
      "Epoch 236, Loss: 0.46897798398181567\n",
      "Epoch 237, Loss: 0.4689811295666005\n",
      "Epoch 238, Loss: 0.4688031484354944\n",
      "Epoch 239, Loss: 0.46862983752908405\n",
      "Epoch 240, Loss: 0.4685963696810109\n",
      "Epoch 241, Loss: 0.46867106090894\n",
      "Epoch 242, Loss: 0.46866360503981047\n",
      "Epoch 243, Loss: 0.468513635806399\n",
      "Epoch 244, Loss: 0.46835730315983837\n",
      "Epoch 245, Loss: 0.468344910742699\n",
      "Epoch 246, Loss: 0.46842993299443175\n",
      "Epoch 247, Loss: 0.4683803372309361\n",
      "Epoch 248, Loss: 0.46823408027364943\n",
      "Epoch 249, Loss: 0.46809559282800506\n",
      "Epoch 250, Loss: 0.4681194355765096\n",
      "Epoch 251, Loss: 0.4682059468617626\n",
      "Epoch 252, Loss: 0.46815125054377077\n",
      "Epoch 253, Loss: 0.46800465135626707\n",
      "Epoch 254, Loss: 0.4678516840148294\n",
      "Epoch 255, Loss: 0.46781997608436393\n",
      "Epoch 256, Loss: 0.4678506648079015\n",
      "Epoch 257, Loss: 0.4677995105079106\n",
      "Epoch 258, Loss: 0.4676942061377158\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.21858226582096615\n",
      "Valence RMSE: 0.2059032127104112\n",
      "Arousal RMSE: 0.23056513365725137\n",
      "Test R^2 score: tensor([0.4862, 0.4736], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.3660, 0.3504], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.47991266907585184\n",
      "Num of epochs: 259\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Epoch 121, Loss: 0.49295912913079615\n",
      "Epoch 122, Loss: 0.4924773619139838\n",
      "Epoch 123, Loss: 0.49197607191949805\n",
      "Epoch 124, Loss: 0.4914699197831528\n",
      "Epoch 125, Loss: 0.49091483381312667\n",
      "Epoch 126, Loss: 0.49035147677946317\n",
      "Epoch 127, Loss: 0.4898087222825037\n",
      "Epoch 128, Loss: 0.48924014731550836\n",
      "Epoch 129, Loss: 0.48873550647212044\n",
      "Epoch 130, Loss: 0.48832307254971563\n",
      "Epoch 131, Loss: 0.48791418392821295\n",
      "Epoch 132, Loss: 0.4875902245243796\n",
      "Epoch 133, Loss: 0.48728851108037585\n",
      "Epoch 134, Loss: 0.48697062262181345\n",
      "Epoch 135, Loss: 0.4866339553030548\n",
      "Epoch 136, Loss: 0.48633446746734893\n",
      "Epoch 137, Loss: 0.4860310087358514\n",
      "Epoch 138, Loss: 0.4856786412545615\n",
      "Epoch 139, Loss: 0.48531372107004\n",
      "Epoch 140, Loss: 0.48495712985488587\n",
      "Epoch 141, Loss: 0.4846131600652309\n",
      "Epoch 142, Loss: 0.48425966858652364\n",
      "Epoch 143, Loss: 0.4838914149236435\n",
      "Epoch 144, Loss: 0.48352437546282767\n",
      "Epoch 145, Loss: 0.4832299911937133\n",
      "Epoch 146, Loss: 0.4832586529465723\n",
      "Epoch 147, Loss: 0.48356744143113517\n",
      "Epoch 148, Loss: 0.4828108019011922\n",
      "Epoch 149, Loss: 0.4818912939738211\n",
      "Epoch 150, Loss: 0.48239416745575797\n",
      "Epoch 151, Loss: 0.4814952491721961\n",
      "Epoch 152, Loss: 0.4811822818820502\n",
      "Epoch 153, Loss: 0.4812753156716337\n",
      "Epoch 154, Loss: 0.4804387742282827\n",
      "Epoch 155, Loss: 0.48058189052417905\n",
      "Epoch 156, Loss: 0.4801288115497346\n",
      "Epoch 157, Loss: 0.4797465937246525\n",
      "Epoch 158, Loss: 0.4797622479529733\n",
      "Epoch 159, Loss: 0.4791664679844762\n",
      "Epoch 160, Loss: 0.47904359923301915\n",
      "Epoch 161, Loss: 0.4788483533353555\n",
      "Epoch 162, Loss: 0.4783709093142749\n",
      "Epoch 163, Loss: 0.4783027642591491\n",
      "Epoch 164, Loss: 0.4780003946284736\n",
      "Epoch 165, Loss: 0.47766356610156796\n",
      "Epoch 166, Loss: 0.47756940749162413\n",
      "Epoch 167, Loss: 0.4772720526560437\n",
      "Epoch 168, Loss: 0.477001191512605\n",
      "Epoch 169, Loss: 0.47690651155802954\n",
      "Epoch 170, Loss: 0.47663605231839845\n",
      "Epoch 171, Loss: 0.4763715079887003\n",
      "Epoch 172, Loss: 0.4762479807342815\n",
      "Epoch 173, Loss: 0.47603155482909787\n",
      "Epoch 174, Loss: 0.47580085926956445\n",
      "Epoch 175, Loss: 0.4756396378860013\n",
      "Epoch 176, Loss: 0.4754627232666358\n",
      "Epoch 177, Loss: 0.4752513954316692\n",
      "Epoch 178, Loss: 0.47507140350730925\n",
      "Epoch 179, Loss: 0.47492471273973497\n",
      "Epoch 180, Loss: 0.47475493910732763\n",
      "Epoch 181, Loss: 0.47455157018218247\n",
      "Epoch 182, Loss: 0.4743853224635176\n",
      "Epoch 183, Loss: 0.47424550491599726\n",
      "Epoch 184, Loss: 0.47408413178207975\n",
      "Epoch 185, Loss: 0.47390385374022453\n",
      "Epoch 186, Loss: 0.4737383066259127\n",
      "Epoch 187, Loss: 0.47358797785338436\n",
      "Epoch 188, Loss: 0.4734560449826861\n",
      "Epoch 189, Loss: 0.47335738206045025\n",
      "Epoch 190, Loss: 0.4733162834925985\n",
      "Epoch 191, Loss: 0.4732413020737725\n",
      "Epoch 192, Loss: 0.47313239020014447\n",
      "Epoch 193, Loss: 0.4729457158004841\n",
      "Epoch 194, Loss: 0.47274703738102797\n",
      "Epoch 195, Loss: 0.47253750657270627\n",
      "Epoch 196, Loss: 0.4723544299968521\n",
      "Epoch 197, Loss: 0.47221102907746554\n",
      "Epoch 198, Loss: 0.47210844467503205\n",
      "Epoch 199, Loss: 0.47205337980155637\n",
      "Epoch 200, Loss: 0.47204973383515075\n",
      "Epoch 201, Loss: 0.4720427890593303\n",
      "Epoch 202, Loss: 0.4719159657967507\n",
      "Epoch 203, Loss: 0.471735017109987\n",
      "Epoch 204, Loss: 0.4714985533244086\n",
      "Epoch 205, Loss: 0.4713143302002759\n",
      "Epoch 206, Loss: 0.47118574544243713\n",
      "Epoch 207, Loss: 0.47112002449718005\n",
      "Epoch 208, Loss: 0.4710910670518786\n",
      "Epoch 209, Loss: 0.47102495326923743\n",
      "Epoch 210, Loss: 0.4709626586360175\n",
      "Epoch 211, Loss: 0.47085840979895394\n",
      "Epoch 212, Loss: 0.47078064719388485\n",
      "Epoch 213, Loss: 0.47066614789276784\n",
      "Epoch 214, Loss: 0.47055654499036753\n",
      "Epoch 215, Loss: 0.47041652390832933\n",
      "Epoch 216, Loss: 0.4702446473560784\n",
      "Epoch 217, Loss: 0.4701045016774931\n",
      "Epoch 218, Loss: 0.47002059077245106\n",
      "Epoch 219, Loss: 0.4699643617672827\n",
      "Epoch 220, Loss: 0.4698888296176042\n",
      "Epoch 221, Loss: 0.4698141575467262\n",
      "Epoch 222, Loss: 0.4697265783615268\n",
      "Epoch 223, Loss: 0.46966066909468496\n",
      "Epoch 224, Loss: 0.4696348739248034\n",
      "Epoch 225, Loss: 0.4697176006517021\n",
      "Epoch 226, Loss: 0.4699424675720642\n",
      "Epoch 227, Loss: 0.46999269112017344\n",
      "Epoch 228, Loss: 0.4698759384724347\n",
      "Epoch 229, Loss: 0.46938041574678274\n",
      "Epoch 230, Loss: 0.4691415581864629\n",
      "Epoch 231, Loss: 0.46924839544758656\n",
      "Epoch 232, Loss: 0.46933531775018905\n",
      "Epoch 233, Loss: 0.4692253881106229\n",
      "Epoch 234, Loss: 0.46894320638835874\n",
      "Epoch 235, Loss: 0.4688628060980674\n",
      "Epoch 236, Loss: 0.46897798398181567\n",
      "Epoch 237, Loss: 0.4689811295666005\n",
      "Epoch 238, Loss: 0.4688031484354944\n",
      "Epoch 239, Loss: 0.46862983752908405\n",
      "Epoch 240, Loss: 0.4685963696810109\n",
      "Epoch 241, Loss: 0.46867106090894\n",
      "Epoch 242, Loss: 0.46866360503981047\n",
      "Epoch 243, Loss: 0.468513635806399\n",
      "Epoch 244, Loss: 0.46835730315983837\n",
      "Epoch 245, Loss: 0.468344910742699\n",
      "Epoch 246, Loss: 0.46842993299443175\n",
      "Epoch 247, Loss: 0.4683803372309361\n",
      "Epoch 248, Loss: 0.46823408027364943\n",
      "Epoch 249, Loss: 0.46809559282800506\n",
      "Epoch 250, Loss: 0.4681194355765096\n",
      "Epoch 251, Loss: 0.4682059468617626\n",
      "Epoch 252, Loss: 0.46815125054377077\n",
      "Epoch 253, Loss: 0.46800465135626707\n",
      "Epoch 254, Loss: 0.4678516840148294\n",
      "Epoch 255, Loss: 0.46781997608436393\n",
      "Epoch 256, Loss: 0.4678506648079015\n",
      "Epoch 257, Loss: 0.4677995105079106\n",
      "Epoch 258, Loss: 0.4676942061377158\n",
      "Epoch 259, Loss: 0.46760336187164103\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.21865911231155877\n",
      "Valence RMSE: 0.20597656650770946\n",
      "Arousal RMSE: 0.23064533128475181\n",
      "Test R^2 score: tensor([0.4858, 0.4733], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.3655, 0.3500], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.47954647219297736\n",
      "Num of epochs: 260\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Epoch 121, Loss: 0.49295912913079615\n",
      "Epoch 122, Loss: 0.4924773619139838\n",
      "Epoch 123, Loss: 0.49197607191949805\n",
      "Epoch 124, Loss: 0.4914699197831528\n",
      "Epoch 125, Loss: 0.49091483381312667\n",
      "Epoch 126, Loss: 0.49035147677946317\n",
      "Epoch 127, Loss: 0.4898087222825037\n",
      "Epoch 128, Loss: 0.48924014731550836\n",
      "Epoch 129, Loss: 0.48873550647212044\n",
      "Epoch 130, Loss: 0.48832307254971563\n",
      "Epoch 131, Loss: 0.48791418392821295\n",
      "Epoch 132, Loss: 0.4875902245243796\n",
      "Epoch 133, Loss: 0.48728851108037585\n",
      "Epoch 134, Loss: 0.48697062262181345\n",
      "Epoch 135, Loss: 0.4866339553030548\n",
      "Epoch 136, Loss: 0.48633446746734893\n",
      "Epoch 137, Loss: 0.4860310087358514\n",
      "Epoch 138, Loss: 0.4856786412545615\n",
      "Epoch 139, Loss: 0.48531372107004\n",
      "Epoch 140, Loss: 0.48495712985488587\n",
      "Epoch 141, Loss: 0.4846131600652309\n",
      "Epoch 142, Loss: 0.48425966858652364\n",
      "Epoch 143, Loss: 0.4838914149236435\n",
      "Epoch 144, Loss: 0.48352437546282767\n",
      "Epoch 145, Loss: 0.4832299911937133\n",
      "Epoch 146, Loss: 0.4832586529465723\n",
      "Epoch 147, Loss: 0.48356744143113517\n",
      "Epoch 148, Loss: 0.4828108019011922\n",
      "Epoch 149, Loss: 0.4818912939738211\n",
      "Epoch 150, Loss: 0.48239416745575797\n",
      "Epoch 151, Loss: 0.4814952491721961\n",
      "Epoch 152, Loss: 0.4811822818820502\n",
      "Epoch 153, Loss: 0.4812753156716337\n",
      "Epoch 154, Loss: 0.4804387742282827\n",
      "Epoch 155, Loss: 0.48058189052417905\n",
      "Epoch 156, Loss: 0.4801288115497346\n",
      "Epoch 157, Loss: 0.4797465937246525\n",
      "Epoch 158, Loss: 0.4797622479529733\n",
      "Epoch 159, Loss: 0.4791664679844762\n",
      "Epoch 160, Loss: 0.47904359923301915\n",
      "Epoch 161, Loss: 0.4788483533353555\n",
      "Epoch 162, Loss: 0.4783709093142749\n",
      "Epoch 163, Loss: 0.4783027642591491\n",
      "Epoch 164, Loss: 0.4780003946284736\n",
      "Epoch 165, Loss: 0.47766356610156796\n",
      "Epoch 166, Loss: 0.47756940749162413\n",
      "Epoch 167, Loss: 0.4772720526560437\n",
      "Epoch 168, Loss: 0.477001191512605\n",
      "Epoch 169, Loss: 0.47690651155802954\n",
      "Epoch 170, Loss: 0.47663605231839845\n",
      "Epoch 171, Loss: 0.4763715079887003\n",
      "Epoch 172, Loss: 0.4762479807342815\n",
      "Epoch 173, Loss: 0.47603155482909787\n",
      "Epoch 174, Loss: 0.47580085926956445\n",
      "Epoch 175, Loss: 0.4756396378860013\n",
      "Epoch 176, Loss: 0.4754627232666358\n",
      "Epoch 177, Loss: 0.4752513954316692\n",
      "Epoch 178, Loss: 0.47507140350730925\n",
      "Epoch 179, Loss: 0.47492471273973497\n",
      "Epoch 180, Loss: 0.47475493910732763\n",
      "Epoch 181, Loss: 0.47455157018218247\n",
      "Epoch 182, Loss: 0.4743853224635176\n",
      "Epoch 183, Loss: 0.47424550491599726\n",
      "Epoch 184, Loss: 0.47408413178207975\n",
      "Epoch 185, Loss: 0.47390385374022453\n",
      "Epoch 186, Loss: 0.4737383066259127\n",
      "Epoch 187, Loss: 0.47358797785338436\n",
      "Epoch 188, Loss: 0.4734560449826861\n",
      "Epoch 189, Loss: 0.47335738206045025\n",
      "Epoch 190, Loss: 0.4733162834925985\n",
      "Epoch 191, Loss: 0.4732413020737725\n",
      "Epoch 192, Loss: 0.47313239020014447\n",
      "Epoch 193, Loss: 0.4729457158004841\n",
      "Epoch 194, Loss: 0.47274703738102797\n",
      "Epoch 195, Loss: 0.47253750657270627\n",
      "Epoch 196, Loss: 0.4723544299968521\n",
      "Epoch 197, Loss: 0.47221102907746554\n",
      "Epoch 198, Loss: 0.47210844467503205\n",
      "Epoch 199, Loss: 0.47205337980155637\n",
      "Epoch 200, Loss: 0.47204973383515075\n",
      "Epoch 201, Loss: 0.4720427890593303\n",
      "Epoch 202, Loss: 0.4719159657967507\n",
      "Epoch 203, Loss: 0.471735017109987\n",
      "Epoch 204, Loss: 0.4714985533244086\n",
      "Epoch 205, Loss: 0.4713143302002759\n",
      "Epoch 206, Loss: 0.47118574544243713\n",
      "Epoch 207, Loss: 0.47112002449718005\n",
      "Epoch 208, Loss: 0.4710910670518786\n",
      "Epoch 209, Loss: 0.47102495326923743\n",
      "Epoch 210, Loss: 0.4709626586360175\n",
      "Epoch 211, Loss: 0.47085840979895394\n",
      "Epoch 212, Loss: 0.47078064719388485\n",
      "Epoch 213, Loss: 0.47066614789276784\n",
      "Epoch 214, Loss: 0.47055654499036753\n",
      "Epoch 215, Loss: 0.47041652390832933\n",
      "Epoch 216, Loss: 0.4702446473560784\n",
      "Epoch 217, Loss: 0.4701045016774931\n",
      "Epoch 218, Loss: 0.47002059077245106\n",
      "Epoch 219, Loss: 0.4699643617672827\n",
      "Epoch 220, Loss: 0.4698888296176042\n",
      "Epoch 221, Loss: 0.4698141575467262\n",
      "Epoch 222, Loss: 0.4697265783615268\n",
      "Epoch 223, Loss: 0.46966066909468496\n",
      "Epoch 224, Loss: 0.4696348739248034\n",
      "Epoch 225, Loss: 0.4697176006517021\n",
      "Epoch 226, Loss: 0.4699424675720642\n",
      "Epoch 227, Loss: 0.46999269112017344\n",
      "Epoch 228, Loss: 0.4698759384724347\n",
      "Epoch 229, Loss: 0.46938041574678274\n",
      "Epoch 230, Loss: 0.4691415581864629\n",
      "Epoch 231, Loss: 0.46924839544758656\n",
      "Epoch 232, Loss: 0.46933531775018905\n",
      "Epoch 233, Loss: 0.4692253881106229\n",
      "Epoch 234, Loss: 0.46894320638835874\n",
      "Epoch 235, Loss: 0.4688628060980674\n",
      "Epoch 236, Loss: 0.46897798398181567\n",
      "Epoch 237, Loss: 0.4689811295666005\n",
      "Epoch 238, Loss: 0.4688031484354944\n",
      "Epoch 239, Loss: 0.46862983752908405\n",
      "Epoch 240, Loss: 0.4685963696810109\n",
      "Epoch 241, Loss: 0.46867106090894\n",
      "Epoch 242, Loss: 0.46866360503981047\n",
      "Epoch 243, Loss: 0.468513635806399\n",
      "Epoch 244, Loss: 0.46835730315983837\n",
      "Epoch 245, Loss: 0.468344910742699\n",
      "Epoch 246, Loss: 0.46842993299443175\n",
      "Epoch 247, Loss: 0.4683803372309361\n",
      "Epoch 248, Loss: 0.46823408027364943\n",
      "Epoch 249, Loss: 0.46809559282800506\n",
      "Epoch 250, Loss: 0.4681194355765096\n",
      "Epoch 251, Loss: 0.4682059468617626\n",
      "Epoch 252, Loss: 0.46815125054377077\n",
      "Epoch 253, Loss: 0.46800465135626707\n",
      "Epoch 254, Loss: 0.4678516840148294\n",
      "Epoch 255, Loss: 0.46781997608436393\n",
      "Epoch 256, Loss: 0.4678506648079015\n",
      "Epoch 257, Loss: 0.4677995105079106\n",
      "Epoch 258, Loss: 0.4676942061377158\n",
      "Epoch 259, Loss: 0.46760336187164103\n",
      "Epoch 260, Loss: 0.46754277859764004\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.21895089758636585\n",
      "Valence RMSE: 0.20625858607652253\n",
      "Arousal RMSE: 0.23094671848169712\n",
      "Test R^2 score: tensor([0.4844, 0.4719], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.3638, 0.3483], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.47815325413195214\n",
      "Num of epochs: 261\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Epoch 121, Loss: 0.49295912913079615\n",
      "Epoch 122, Loss: 0.4924773619139838\n",
      "Epoch 123, Loss: 0.49197607191949805\n",
      "Epoch 124, Loss: 0.4914699197831528\n",
      "Epoch 125, Loss: 0.49091483381312667\n",
      "Epoch 126, Loss: 0.49035147677946317\n",
      "Epoch 127, Loss: 0.4898087222825037\n",
      "Epoch 128, Loss: 0.48924014731550836\n",
      "Epoch 129, Loss: 0.48873550647212044\n",
      "Epoch 130, Loss: 0.48832307254971563\n",
      "Epoch 131, Loss: 0.48791418392821295\n",
      "Epoch 132, Loss: 0.4875902245243796\n",
      "Epoch 133, Loss: 0.48728851108037585\n",
      "Epoch 134, Loss: 0.48697062262181345\n",
      "Epoch 135, Loss: 0.4866339553030548\n",
      "Epoch 136, Loss: 0.48633446746734893\n",
      "Epoch 137, Loss: 0.4860310087358514\n",
      "Epoch 138, Loss: 0.4856786412545615\n",
      "Epoch 139, Loss: 0.48531372107004\n",
      "Epoch 140, Loss: 0.48495712985488587\n",
      "Epoch 141, Loss: 0.4846131600652309\n",
      "Epoch 142, Loss: 0.48425966858652364\n",
      "Epoch 143, Loss: 0.4838914149236435\n",
      "Epoch 144, Loss: 0.48352437546282767\n",
      "Epoch 145, Loss: 0.4832299911937133\n",
      "Epoch 146, Loss: 0.4832586529465723\n",
      "Epoch 147, Loss: 0.48356744143113517\n",
      "Epoch 148, Loss: 0.4828108019011922\n",
      "Epoch 149, Loss: 0.4818912939738211\n",
      "Epoch 150, Loss: 0.48239416745575797\n",
      "Epoch 151, Loss: 0.4814952491721961\n",
      "Epoch 152, Loss: 0.4811822818820502\n",
      "Epoch 153, Loss: 0.4812753156716337\n",
      "Epoch 154, Loss: 0.4804387742282827\n",
      "Epoch 155, Loss: 0.48058189052417905\n",
      "Epoch 156, Loss: 0.4801288115497346\n",
      "Epoch 157, Loss: 0.4797465937246525\n",
      "Epoch 158, Loss: 0.4797622479529733\n",
      "Epoch 159, Loss: 0.4791664679844762\n",
      "Epoch 160, Loss: 0.47904359923301915\n",
      "Epoch 161, Loss: 0.4788483533353555\n",
      "Epoch 162, Loss: 0.4783709093142749\n",
      "Epoch 163, Loss: 0.4783027642591491\n",
      "Epoch 164, Loss: 0.4780003946284736\n",
      "Epoch 165, Loss: 0.47766356610156796\n",
      "Epoch 166, Loss: 0.47756940749162413\n",
      "Epoch 167, Loss: 0.4772720526560437\n",
      "Epoch 168, Loss: 0.477001191512605\n",
      "Epoch 169, Loss: 0.47690651155802954\n",
      "Epoch 170, Loss: 0.47663605231839845\n",
      "Epoch 171, Loss: 0.4763715079887003\n",
      "Epoch 172, Loss: 0.4762479807342815\n",
      "Epoch 173, Loss: 0.47603155482909787\n",
      "Epoch 174, Loss: 0.47580085926956445\n",
      "Epoch 175, Loss: 0.4756396378860013\n",
      "Epoch 176, Loss: 0.4754627232666358\n",
      "Epoch 177, Loss: 0.4752513954316692\n",
      "Epoch 178, Loss: 0.47507140350730925\n",
      "Epoch 179, Loss: 0.47492471273973497\n",
      "Epoch 180, Loss: 0.47475493910732763\n",
      "Epoch 181, Loss: 0.47455157018218247\n",
      "Epoch 182, Loss: 0.4743853224635176\n",
      "Epoch 183, Loss: 0.47424550491599726\n",
      "Epoch 184, Loss: 0.47408413178207975\n",
      "Epoch 185, Loss: 0.47390385374022453\n",
      "Epoch 186, Loss: 0.4737383066259127\n",
      "Epoch 187, Loss: 0.47358797785338436\n",
      "Epoch 188, Loss: 0.4734560449826861\n",
      "Epoch 189, Loss: 0.47335738206045025\n",
      "Epoch 190, Loss: 0.4733162834925985\n",
      "Epoch 191, Loss: 0.4732413020737725\n",
      "Epoch 192, Loss: 0.47313239020014447\n",
      "Epoch 193, Loss: 0.4729457158004841\n",
      "Epoch 194, Loss: 0.47274703738102797\n",
      "Epoch 195, Loss: 0.47253750657270627\n",
      "Epoch 196, Loss: 0.4723544299968521\n",
      "Epoch 197, Loss: 0.47221102907746554\n",
      "Epoch 198, Loss: 0.47210844467503205\n",
      "Epoch 199, Loss: 0.47205337980155637\n",
      "Epoch 200, Loss: 0.47204973383515075\n",
      "Epoch 201, Loss: 0.4720427890593303\n",
      "Epoch 202, Loss: 0.4719159657967507\n",
      "Epoch 203, Loss: 0.471735017109987\n",
      "Epoch 204, Loss: 0.4714985533244086\n",
      "Epoch 205, Loss: 0.4713143302002759\n",
      "Epoch 206, Loss: 0.47118574544243713\n",
      "Epoch 207, Loss: 0.47112002449718005\n",
      "Epoch 208, Loss: 0.4710910670518786\n",
      "Epoch 209, Loss: 0.47102495326923743\n",
      "Epoch 210, Loss: 0.4709626586360175\n",
      "Epoch 211, Loss: 0.47085840979895394\n",
      "Epoch 212, Loss: 0.47078064719388485\n",
      "Epoch 213, Loss: 0.47066614789276784\n",
      "Epoch 214, Loss: 0.47055654499036753\n",
      "Epoch 215, Loss: 0.47041652390832933\n",
      "Epoch 216, Loss: 0.4702446473560784\n",
      "Epoch 217, Loss: 0.4701045016774931\n",
      "Epoch 218, Loss: 0.47002059077245106\n",
      "Epoch 219, Loss: 0.4699643617672827\n",
      "Epoch 220, Loss: 0.4698888296176042\n",
      "Epoch 221, Loss: 0.4698141575467262\n",
      "Epoch 222, Loss: 0.4697265783615268\n",
      "Epoch 223, Loss: 0.46966066909468496\n",
      "Epoch 224, Loss: 0.4696348739248034\n",
      "Epoch 225, Loss: 0.4697176006517021\n",
      "Epoch 226, Loss: 0.4699424675720642\n",
      "Epoch 227, Loss: 0.46999269112017344\n",
      "Epoch 228, Loss: 0.4698759384724347\n",
      "Epoch 229, Loss: 0.46938041574678274\n",
      "Epoch 230, Loss: 0.4691415581864629\n",
      "Epoch 231, Loss: 0.46924839544758656\n",
      "Epoch 232, Loss: 0.46933531775018905\n",
      "Epoch 233, Loss: 0.4692253881106229\n",
      "Epoch 234, Loss: 0.46894320638835874\n",
      "Epoch 235, Loss: 0.4688628060980674\n",
      "Epoch 236, Loss: 0.46897798398181567\n",
      "Epoch 237, Loss: 0.4689811295666005\n",
      "Epoch 238, Loss: 0.4688031484354944\n",
      "Epoch 239, Loss: 0.46862983752908405\n",
      "Epoch 240, Loss: 0.4685963696810109\n",
      "Epoch 241, Loss: 0.46867106090894\n",
      "Epoch 242, Loss: 0.46866360503981047\n",
      "Epoch 243, Loss: 0.468513635806399\n",
      "Epoch 244, Loss: 0.46835730315983837\n",
      "Epoch 245, Loss: 0.468344910742699\n",
      "Epoch 246, Loss: 0.46842993299443175\n",
      "Epoch 247, Loss: 0.4683803372309361\n",
      "Epoch 248, Loss: 0.46823408027364943\n",
      "Epoch 249, Loss: 0.46809559282800506\n",
      "Epoch 250, Loss: 0.4681194355765096\n",
      "Epoch 251, Loss: 0.4682059468617626\n",
      "Epoch 252, Loss: 0.46815125054377077\n",
      "Epoch 253, Loss: 0.46800465135626707\n",
      "Epoch 254, Loss: 0.4678516840148294\n",
      "Epoch 255, Loss: 0.46781997608436393\n",
      "Epoch 256, Loss: 0.4678506648079015\n",
      "Epoch 257, Loss: 0.4677995105079106\n",
      "Epoch 258, Loss: 0.4676942061377158\n",
      "Epoch 259, Loss: 0.46760336187164103\n",
      "Epoch 260, Loss: 0.46754277859764004\n",
      "Epoch 261, Loss: 0.4675358146830681\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.21815478398421895\n",
      "Valence RMSE: 0.20545773753265126\n",
      "Arousal RMSE: 0.23015242262111135\n",
      "Test R^2 score: tensor([0.4884, 0.4755], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.3687, 0.3528], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4819644473740379\n",
      "Num of epochs: 262\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Epoch 121, Loss: 0.49295912913079615\n",
      "Epoch 122, Loss: 0.4924773619139838\n",
      "Epoch 123, Loss: 0.49197607191949805\n",
      "Epoch 124, Loss: 0.4914699197831528\n",
      "Epoch 125, Loss: 0.49091483381312667\n",
      "Epoch 126, Loss: 0.49035147677946317\n",
      "Epoch 127, Loss: 0.4898087222825037\n",
      "Epoch 128, Loss: 0.48924014731550836\n",
      "Epoch 129, Loss: 0.48873550647212044\n",
      "Epoch 130, Loss: 0.48832307254971563\n",
      "Epoch 131, Loss: 0.48791418392821295\n",
      "Epoch 132, Loss: 0.4875902245243796\n",
      "Epoch 133, Loss: 0.48728851108037585\n",
      "Epoch 134, Loss: 0.48697062262181345\n",
      "Epoch 135, Loss: 0.4866339553030548\n",
      "Epoch 136, Loss: 0.48633446746734893\n",
      "Epoch 137, Loss: 0.4860310087358514\n",
      "Epoch 138, Loss: 0.4856786412545615\n",
      "Epoch 139, Loss: 0.48531372107004\n",
      "Epoch 140, Loss: 0.48495712985488587\n",
      "Epoch 141, Loss: 0.4846131600652309\n",
      "Epoch 142, Loss: 0.48425966858652364\n",
      "Epoch 143, Loss: 0.4838914149236435\n",
      "Epoch 144, Loss: 0.48352437546282767\n",
      "Epoch 145, Loss: 0.4832299911937133\n",
      "Epoch 146, Loss: 0.4832586529465723\n",
      "Epoch 147, Loss: 0.48356744143113517\n",
      "Epoch 148, Loss: 0.4828108019011922\n",
      "Epoch 149, Loss: 0.4818912939738211\n",
      "Epoch 150, Loss: 0.48239416745575797\n",
      "Epoch 151, Loss: 0.4814952491721961\n",
      "Epoch 152, Loss: 0.4811822818820502\n",
      "Epoch 153, Loss: 0.4812753156716337\n",
      "Epoch 154, Loss: 0.4804387742282827\n",
      "Epoch 155, Loss: 0.48058189052417905\n",
      "Epoch 156, Loss: 0.4801288115497346\n",
      "Epoch 157, Loss: 0.4797465937246525\n",
      "Epoch 158, Loss: 0.4797622479529733\n",
      "Epoch 159, Loss: 0.4791664679844762\n",
      "Epoch 160, Loss: 0.47904359923301915\n",
      "Epoch 161, Loss: 0.4788483533353555\n",
      "Epoch 162, Loss: 0.4783709093142749\n",
      "Epoch 163, Loss: 0.4783027642591491\n",
      "Epoch 164, Loss: 0.4780003946284736\n",
      "Epoch 165, Loss: 0.47766356610156796\n",
      "Epoch 166, Loss: 0.47756940749162413\n",
      "Epoch 167, Loss: 0.4772720526560437\n",
      "Epoch 168, Loss: 0.477001191512605\n",
      "Epoch 169, Loss: 0.47690651155802954\n",
      "Epoch 170, Loss: 0.47663605231839845\n",
      "Epoch 171, Loss: 0.4763715079887003\n",
      "Epoch 172, Loss: 0.4762479807342815\n",
      "Epoch 173, Loss: 0.47603155482909787\n",
      "Epoch 174, Loss: 0.47580085926956445\n",
      "Epoch 175, Loss: 0.4756396378860013\n",
      "Epoch 176, Loss: 0.4754627232666358\n",
      "Epoch 177, Loss: 0.4752513954316692\n",
      "Epoch 178, Loss: 0.47507140350730925\n",
      "Epoch 179, Loss: 0.47492471273973497\n",
      "Epoch 180, Loss: 0.47475493910732763\n",
      "Epoch 181, Loss: 0.47455157018218247\n",
      "Epoch 182, Loss: 0.4743853224635176\n",
      "Epoch 183, Loss: 0.47424550491599726\n",
      "Epoch 184, Loss: 0.47408413178207975\n",
      "Epoch 185, Loss: 0.47390385374022453\n",
      "Epoch 186, Loss: 0.4737383066259127\n",
      "Epoch 187, Loss: 0.47358797785338436\n",
      "Epoch 188, Loss: 0.4734560449826861\n",
      "Epoch 189, Loss: 0.47335738206045025\n",
      "Epoch 190, Loss: 0.4733162834925985\n",
      "Epoch 191, Loss: 0.4732413020737725\n",
      "Epoch 192, Loss: 0.47313239020014447\n",
      "Epoch 193, Loss: 0.4729457158004841\n",
      "Epoch 194, Loss: 0.47274703738102797\n",
      "Epoch 195, Loss: 0.47253750657270627\n",
      "Epoch 196, Loss: 0.4723544299968521\n",
      "Epoch 197, Loss: 0.47221102907746554\n",
      "Epoch 198, Loss: 0.47210844467503205\n",
      "Epoch 199, Loss: 0.47205337980155637\n",
      "Epoch 200, Loss: 0.47204973383515075\n",
      "Epoch 201, Loss: 0.4720427890593303\n",
      "Epoch 202, Loss: 0.4719159657967507\n",
      "Epoch 203, Loss: 0.471735017109987\n",
      "Epoch 204, Loss: 0.4714985533244086\n",
      "Epoch 205, Loss: 0.4713143302002759\n",
      "Epoch 206, Loss: 0.47118574544243713\n",
      "Epoch 207, Loss: 0.47112002449718005\n",
      "Epoch 208, Loss: 0.4710910670518786\n",
      "Epoch 209, Loss: 0.47102495326923743\n",
      "Epoch 210, Loss: 0.4709626586360175\n",
      "Epoch 211, Loss: 0.47085840979895394\n",
      "Epoch 212, Loss: 0.47078064719388485\n",
      "Epoch 213, Loss: 0.47066614789276784\n",
      "Epoch 214, Loss: 0.47055654499036753\n",
      "Epoch 215, Loss: 0.47041652390832933\n",
      "Epoch 216, Loss: 0.4702446473560784\n",
      "Epoch 217, Loss: 0.4701045016774931\n",
      "Epoch 218, Loss: 0.47002059077245106\n",
      "Epoch 219, Loss: 0.4699643617672827\n",
      "Epoch 220, Loss: 0.4698888296176042\n",
      "Epoch 221, Loss: 0.4698141575467262\n",
      "Epoch 222, Loss: 0.4697265783615268\n",
      "Epoch 223, Loss: 0.46966066909468496\n",
      "Epoch 224, Loss: 0.4696348739248034\n",
      "Epoch 225, Loss: 0.4697176006517021\n",
      "Epoch 226, Loss: 0.4699424675720642\n",
      "Epoch 227, Loss: 0.46999269112017344\n",
      "Epoch 228, Loss: 0.4698759384724347\n",
      "Epoch 229, Loss: 0.46938041574678274\n",
      "Epoch 230, Loss: 0.4691415581864629\n",
      "Epoch 231, Loss: 0.46924839544758656\n",
      "Epoch 232, Loss: 0.46933531775018905\n",
      "Epoch 233, Loss: 0.4692253881106229\n",
      "Epoch 234, Loss: 0.46894320638835874\n",
      "Epoch 235, Loss: 0.4688628060980674\n",
      "Epoch 236, Loss: 0.46897798398181567\n",
      "Epoch 237, Loss: 0.4689811295666005\n",
      "Epoch 238, Loss: 0.4688031484354944\n",
      "Epoch 239, Loss: 0.46862983752908405\n",
      "Epoch 240, Loss: 0.4685963696810109\n",
      "Epoch 241, Loss: 0.46867106090894\n",
      "Epoch 242, Loss: 0.46866360503981047\n",
      "Epoch 243, Loss: 0.468513635806399\n",
      "Epoch 244, Loss: 0.46835730315983837\n",
      "Epoch 245, Loss: 0.468344910742699\n",
      "Epoch 246, Loss: 0.46842993299443175\n",
      "Epoch 247, Loss: 0.4683803372309361\n",
      "Epoch 248, Loss: 0.46823408027364943\n",
      "Epoch 249, Loss: 0.46809559282800506\n",
      "Epoch 250, Loss: 0.4681194355765096\n",
      "Epoch 251, Loss: 0.4682059468617626\n",
      "Epoch 252, Loss: 0.46815125054377077\n",
      "Epoch 253, Loss: 0.46800465135626707\n",
      "Epoch 254, Loss: 0.4678516840148294\n",
      "Epoch 255, Loss: 0.46781997608436393\n",
      "Epoch 256, Loss: 0.4678506648079015\n",
      "Epoch 257, Loss: 0.4677995105079106\n",
      "Epoch 258, Loss: 0.4676942061377158\n",
      "Epoch 259, Loss: 0.46760336187164103\n",
      "Epoch 260, Loss: 0.46754277859764004\n",
      "Epoch 261, Loss: 0.4675358146830681\n",
      "Epoch 262, Loss: 0.4675934670336611\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2193551087959114\n",
      "Valence RMSE: 0.2066393399919551\n",
      "Arousal RMSE: 0.23137309843069798\n",
      "Test R^2 score: tensor([0.4825, 0.4699], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.3614, 0.3459], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4762246963907755\n",
      "Num of epochs: 263\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Epoch 121, Loss: 0.49295912913079615\n",
      "Epoch 122, Loss: 0.4924773619139838\n",
      "Epoch 123, Loss: 0.49197607191949805\n",
      "Epoch 124, Loss: 0.4914699197831528\n",
      "Epoch 125, Loss: 0.49091483381312667\n",
      "Epoch 126, Loss: 0.49035147677946317\n",
      "Epoch 127, Loss: 0.4898087222825037\n",
      "Epoch 128, Loss: 0.48924014731550836\n",
      "Epoch 129, Loss: 0.48873550647212044\n",
      "Epoch 130, Loss: 0.48832307254971563\n",
      "Epoch 131, Loss: 0.48791418392821295\n",
      "Epoch 132, Loss: 0.4875902245243796\n",
      "Epoch 133, Loss: 0.48728851108037585\n",
      "Epoch 134, Loss: 0.48697062262181345\n",
      "Epoch 135, Loss: 0.4866339553030548\n",
      "Epoch 136, Loss: 0.48633446746734893\n",
      "Epoch 137, Loss: 0.4860310087358514\n",
      "Epoch 138, Loss: 0.4856786412545615\n",
      "Epoch 139, Loss: 0.48531372107004\n",
      "Epoch 140, Loss: 0.48495712985488587\n",
      "Epoch 141, Loss: 0.4846131600652309\n",
      "Epoch 142, Loss: 0.48425966858652364\n",
      "Epoch 143, Loss: 0.4838914149236435\n",
      "Epoch 144, Loss: 0.48352437546282767\n",
      "Epoch 145, Loss: 0.4832299911937133\n",
      "Epoch 146, Loss: 0.4832586529465723\n",
      "Epoch 147, Loss: 0.48356744143113517\n",
      "Epoch 148, Loss: 0.4828108019011922\n",
      "Epoch 149, Loss: 0.4818912939738211\n",
      "Epoch 150, Loss: 0.48239416745575797\n",
      "Epoch 151, Loss: 0.4814952491721961\n",
      "Epoch 152, Loss: 0.4811822818820502\n",
      "Epoch 153, Loss: 0.4812753156716337\n",
      "Epoch 154, Loss: 0.4804387742282827\n",
      "Epoch 155, Loss: 0.48058189052417905\n",
      "Epoch 156, Loss: 0.4801288115497346\n",
      "Epoch 157, Loss: 0.4797465937246525\n",
      "Epoch 158, Loss: 0.4797622479529733\n",
      "Epoch 159, Loss: 0.4791664679844762\n",
      "Epoch 160, Loss: 0.47904359923301915\n",
      "Epoch 161, Loss: 0.4788483533353555\n",
      "Epoch 162, Loss: 0.4783709093142749\n",
      "Epoch 163, Loss: 0.4783027642591491\n",
      "Epoch 164, Loss: 0.4780003946284736\n",
      "Epoch 165, Loss: 0.47766356610156796\n",
      "Epoch 166, Loss: 0.47756940749162413\n",
      "Epoch 167, Loss: 0.4772720526560437\n",
      "Epoch 168, Loss: 0.477001191512605\n",
      "Epoch 169, Loss: 0.47690651155802954\n",
      "Epoch 170, Loss: 0.47663605231839845\n",
      "Epoch 171, Loss: 0.4763715079887003\n",
      "Epoch 172, Loss: 0.4762479807342815\n",
      "Epoch 173, Loss: 0.47603155482909787\n",
      "Epoch 174, Loss: 0.47580085926956445\n",
      "Epoch 175, Loss: 0.4756396378860013\n",
      "Epoch 176, Loss: 0.4754627232666358\n",
      "Epoch 177, Loss: 0.4752513954316692\n",
      "Epoch 178, Loss: 0.47507140350730925\n",
      "Epoch 179, Loss: 0.47492471273973497\n",
      "Epoch 180, Loss: 0.47475493910732763\n",
      "Epoch 181, Loss: 0.47455157018218247\n",
      "Epoch 182, Loss: 0.4743853224635176\n",
      "Epoch 183, Loss: 0.47424550491599726\n",
      "Epoch 184, Loss: 0.47408413178207975\n",
      "Epoch 185, Loss: 0.47390385374022453\n",
      "Epoch 186, Loss: 0.4737383066259127\n",
      "Epoch 187, Loss: 0.47358797785338436\n",
      "Epoch 188, Loss: 0.4734560449826861\n",
      "Epoch 189, Loss: 0.47335738206045025\n",
      "Epoch 190, Loss: 0.4733162834925985\n",
      "Epoch 191, Loss: 0.4732413020737725\n",
      "Epoch 192, Loss: 0.47313239020014447\n",
      "Epoch 193, Loss: 0.4729457158004841\n",
      "Epoch 194, Loss: 0.47274703738102797\n",
      "Epoch 195, Loss: 0.47253750657270627\n",
      "Epoch 196, Loss: 0.4723544299968521\n",
      "Epoch 197, Loss: 0.47221102907746554\n",
      "Epoch 198, Loss: 0.47210844467503205\n",
      "Epoch 199, Loss: 0.47205337980155637\n",
      "Epoch 200, Loss: 0.47204973383515075\n",
      "Epoch 201, Loss: 0.4720427890593303\n",
      "Epoch 202, Loss: 0.4719159657967507\n",
      "Epoch 203, Loss: 0.471735017109987\n",
      "Epoch 204, Loss: 0.4714985533244086\n",
      "Epoch 205, Loss: 0.4713143302002759\n",
      "Epoch 206, Loss: 0.47118574544243713\n",
      "Epoch 207, Loss: 0.47112002449718005\n",
      "Epoch 208, Loss: 0.4710910670518786\n",
      "Epoch 209, Loss: 0.47102495326923743\n",
      "Epoch 210, Loss: 0.4709626586360175\n",
      "Epoch 211, Loss: 0.47085840979895394\n",
      "Epoch 212, Loss: 0.47078064719388485\n",
      "Epoch 213, Loss: 0.47066614789276784\n",
      "Epoch 214, Loss: 0.47055654499036753\n",
      "Epoch 215, Loss: 0.47041652390832933\n",
      "Epoch 216, Loss: 0.4702446473560784\n",
      "Epoch 217, Loss: 0.4701045016774931\n",
      "Epoch 218, Loss: 0.47002059077245106\n",
      "Epoch 219, Loss: 0.4699643617672827\n",
      "Epoch 220, Loss: 0.4698888296176042\n",
      "Epoch 221, Loss: 0.4698141575467262\n",
      "Epoch 222, Loss: 0.4697265783615268\n",
      "Epoch 223, Loss: 0.46966066909468496\n",
      "Epoch 224, Loss: 0.4696348739248034\n",
      "Epoch 225, Loss: 0.4697176006517021\n",
      "Epoch 226, Loss: 0.4699424675720642\n",
      "Epoch 227, Loss: 0.46999269112017344\n",
      "Epoch 228, Loss: 0.4698759384724347\n",
      "Epoch 229, Loss: 0.46938041574678274\n",
      "Epoch 230, Loss: 0.4691415581864629\n",
      "Epoch 231, Loss: 0.46924839544758656\n",
      "Epoch 232, Loss: 0.46933531775018905\n",
      "Epoch 233, Loss: 0.4692253881106229\n",
      "Epoch 234, Loss: 0.46894320638835874\n",
      "Epoch 235, Loss: 0.4688628060980674\n",
      "Epoch 236, Loss: 0.46897798398181567\n",
      "Epoch 237, Loss: 0.4689811295666005\n",
      "Epoch 238, Loss: 0.4688031484354944\n",
      "Epoch 239, Loss: 0.46862983752908405\n",
      "Epoch 240, Loss: 0.4685963696810109\n",
      "Epoch 241, Loss: 0.46867106090894\n",
      "Epoch 242, Loss: 0.46866360503981047\n",
      "Epoch 243, Loss: 0.468513635806399\n",
      "Epoch 244, Loss: 0.46835730315983837\n",
      "Epoch 245, Loss: 0.468344910742699\n",
      "Epoch 246, Loss: 0.46842993299443175\n",
      "Epoch 247, Loss: 0.4683803372309361\n",
      "Epoch 248, Loss: 0.46823408027364943\n",
      "Epoch 249, Loss: 0.46809559282800506\n",
      "Epoch 250, Loss: 0.4681194355765096\n",
      "Epoch 251, Loss: 0.4682059468617626\n",
      "Epoch 252, Loss: 0.46815125054377077\n",
      "Epoch 253, Loss: 0.46800465135626707\n",
      "Epoch 254, Loss: 0.4678516840148294\n",
      "Epoch 255, Loss: 0.46781997608436393\n",
      "Epoch 256, Loss: 0.4678506648079015\n",
      "Epoch 257, Loss: 0.4677995105079106\n",
      "Epoch 258, Loss: 0.4676942061377158\n",
      "Epoch 259, Loss: 0.46760336187164103\n",
      "Epoch 260, Loss: 0.46754277859764004\n",
      "Epoch 261, Loss: 0.4675358146830681\n",
      "Epoch 262, Loss: 0.4675934670336611\n",
      "Epoch 263, Loss: 0.4675883044263754\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.21807500331695304\n",
      "Valence RMSE: 0.2053753568783323\n",
      "Arousal RMSE: 0.2300747203203251\n",
      "Test R^2 score: tensor([0.4888, 0.4759], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.3692, 0.3532], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4823465748058053\n",
      "Num of epochs: 264\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Epoch 121, Loss: 0.49295912913079615\n",
      "Epoch 122, Loss: 0.4924773619139838\n",
      "Epoch 123, Loss: 0.49197607191949805\n",
      "Epoch 124, Loss: 0.4914699197831528\n",
      "Epoch 125, Loss: 0.49091483381312667\n",
      "Epoch 126, Loss: 0.49035147677946317\n",
      "Epoch 127, Loss: 0.4898087222825037\n",
      "Epoch 128, Loss: 0.48924014731550836\n",
      "Epoch 129, Loss: 0.48873550647212044\n",
      "Epoch 130, Loss: 0.48832307254971563\n",
      "Epoch 131, Loss: 0.48791418392821295\n",
      "Epoch 132, Loss: 0.4875902245243796\n",
      "Epoch 133, Loss: 0.48728851108037585\n",
      "Epoch 134, Loss: 0.48697062262181345\n",
      "Epoch 135, Loss: 0.4866339553030548\n",
      "Epoch 136, Loss: 0.48633446746734893\n",
      "Epoch 137, Loss: 0.4860310087358514\n",
      "Epoch 138, Loss: 0.4856786412545615\n",
      "Epoch 139, Loss: 0.48531372107004\n",
      "Epoch 140, Loss: 0.48495712985488587\n",
      "Epoch 141, Loss: 0.4846131600652309\n",
      "Epoch 142, Loss: 0.48425966858652364\n",
      "Epoch 143, Loss: 0.4838914149236435\n",
      "Epoch 144, Loss: 0.48352437546282767\n",
      "Epoch 145, Loss: 0.4832299911937133\n",
      "Epoch 146, Loss: 0.4832586529465723\n",
      "Epoch 147, Loss: 0.48356744143113517\n",
      "Epoch 148, Loss: 0.4828108019011922\n",
      "Epoch 149, Loss: 0.4818912939738211\n",
      "Epoch 150, Loss: 0.48239416745575797\n",
      "Epoch 151, Loss: 0.4814952491721961\n",
      "Epoch 152, Loss: 0.4811822818820502\n",
      "Epoch 153, Loss: 0.4812753156716337\n",
      "Epoch 154, Loss: 0.4804387742282827\n",
      "Epoch 155, Loss: 0.48058189052417905\n",
      "Epoch 156, Loss: 0.4801288115497346\n",
      "Epoch 157, Loss: 0.4797465937246525\n",
      "Epoch 158, Loss: 0.4797622479529733\n",
      "Epoch 159, Loss: 0.4791664679844762\n",
      "Epoch 160, Loss: 0.47904359923301915\n",
      "Epoch 161, Loss: 0.4788483533353555\n",
      "Epoch 162, Loss: 0.4783709093142749\n",
      "Epoch 163, Loss: 0.4783027642591491\n",
      "Epoch 164, Loss: 0.4780003946284736\n",
      "Epoch 165, Loss: 0.47766356610156796\n",
      "Epoch 166, Loss: 0.47756940749162413\n",
      "Epoch 167, Loss: 0.4772720526560437\n",
      "Epoch 168, Loss: 0.477001191512605\n",
      "Epoch 169, Loss: 0.47690651155802954\n",
      "Epoch 170, Loss: 0.47663605231839845\n",
      "Epoch 171, Loss: 0.4763715079887003\n",
      "Epoch 172, Loss: 0.4762479807342815\n",
      "Epoch 173, Loss: 0.47603155482909787\n",
      "Epoch 174, Loss: 0.47580085926956445\n",
      "Epoch 175, Loss: 0.4756396378860013\n",
      "Epoch 176, Loss: 0.4754627232666358\n",
      "Epoch 177, Loss: 0.4752513954316692\n",
      "Epoch 178, Loss: 0.47507140350730925\n",
      "Epoch 179, Loss: 0.47492471273973497\n",
      "Epoch 180, Loss: 0.47475493910732763\n",
      "Epoch 181, Loss: 0.47455157018218247\n",
      "Epoch 182, Loss: 0.4743853224635176\n",
      "Epoch 183, Loss: 0.47424550491599726\n",
      "Epoch 184, Loss: 0.47408413178207975\n",
      "Epoch 185, Loss: 0.47390385374022453\n",
      "Epoch 186, Loss: 0.4737383066259127\n",
      "Epoch 187, Loss: 0.47358797785338436\n",
      "Epoch 188, Loss: 0.4734560449826861\n",
      "Epoch 189, Loss: 0.47335738206045025\n",
      "Epoch 190, Loss: 0.4733162834925985\n",
      "Epoch 191, Loss: 0.4732413020737725\n",
      "Epoch 192, Loss: 0.47313239020014447\n",
      "Epoch 193, Loss: 0.4729457158004841\n",
      "Epoch 194, Loss: 0.47274703738102797\n",
      "Epoch 195, Loss: 0.47253750657270627\n",
      "Epoch 196, Loss: 0.4723544299968521\n",
      "Epoch 197, Loss: 0.47221102907746554\n",
      "Epoch 198, Loss: 0.47210844467503205\n",
      "Epoch 199, Loss: 0.47205337980155637\n",
      "Epoch 200, Loss: 0.47204973383515075\n",
      "Epoch 201, Loss: 0.4720427890593303\n",
      "Epoch 202, Loss: 0.4719159657967507\n",
      "Epoch 203, Loss: 0.471735017109987\n",
      "Epoch 204, Loss: 0.4714985533244086\n",
      "Epoch 205, Loss: 0.4713143302002759\n",
      "Epoch 206, Loss: 0.47118574544243713\n",
      "Epoch 207, Loss: 0.47112002449718005\n",
      "Epoch 208, Loss: 0.4710910670518786\n",
      "Epoch 209, Loss: 0.47102495326923743\n",
      "Epoch 210, Loss: 0.4709626586360175\n",
      "Epoch 211, Loss: 0.47085840979895394\n",
      "Epoch 212, Loss: 0.47078064719388485\n",
      "Epoch 213, Loss: 0.47066614789276784\n",
      "Epoch 214, Loss: 0.47055654499036753\n",
      "Epoch 215, Loss: 0.47041652390832933\n",
      "Epoch 216, Loss: 0.4702446473560784\n",
      "Epoch 217, Loss: 0.4701045016774931\n",
      "Epoch 218, Loss: 0.47002059077245106\n",
      "Epoch 219, Loss: 0.4699643617672827\n",
      "Epoch 220, Loss: 0.4698888296176042\n",
      "Epoch 221, Loss: 0.4698141575467262\n",
      "Epoch 222, Loss: 0.4697265783615268\n",
      "Epoch 223, Loss: 0.46966066909468496\n",
      "Epoch 224, Loss: 0.4696348739248034\n",
      "Epoch 225, Loss: 0.4697176006517021\n",
      "Epoch 226, Loss: 0.4699424675720642\n",
      "Epoch 227, Loss: 0.46999269112017344\n",
      "Epoch 228, Loss: 0.4698759384724347\n",
      "Epoch 229, Loss: 0.46938041574678274\n",
      "Epoch 230, Loss: 0.4691415581864629\n",
      "Epoch 231, Loss: 0.46924839544758656\n",
      "Epoch 232, Loss: 0.46933531775018905\n",
      "Epoch 233, Loss: 0.4692253881106229\n",
      "Epoch 234, Loss: 0.46894320638835874\n",
      "Epoch 235, Loss: 0.4688628060980674\n",
      "Epoch 236, Loss: 0.46897798398181567\n",
      "Epoch 237, Loss: 0.4689811295666005\n",
      "Epoch 238, Loss: 0.4688031484354944\n",
      "Epoch 239, Loss: 0.46862983752908405\n",
      "Epoch 240, Loss: 0.4685963696810109\n",
      "Epoch 241, Loss: 0.46867106090894\n",
      "Epoch 242, Loss: 0.46866360503981047\n",
      "Epoch 243, Loss: 0.468513635806399\n",
      "Epoch 244, Loss: 0.46835730315983837\n",
      "Epoch 245, Loss: 0.468344910742699\n",
      "Epoch 246, Loss: 0.46842993299443175\n",
      "Epoch 247, Loss: 0.4683803372309361\n",
      "Epoch 248, Loss: 0.46823408027364943\n",
      "Epoch 249, Loss: 0.46809559282800506\n",
      "Epoch 250, Loss: 0.4681194355765096\n",
      "Epoch 251, Loss: 0.4682059468617626\n",
      "Epoch 252, Loss: 0.46815125054377077\n",
      "Epoch 253, Loss: 0.46800465135626707\n",
      "Epoch 254, Loss: 0.4678516840148294\n",
      "Epoch 255, Loss: 0.46781997608436393\n",
      "Epoch 256, Loss: 0.4678506648079015\n",
      "Epoch 257, Loss: 0.4677995105079106\n",
      "Epoch 258, Loss: 0.4676942061377158\n",
      "Epoch 259, Loss: 0.46760336187164103\n",
      "Epoch 260, Loss: 0.46754277859764004\n",
      "Epoch 261, Loss: 0.4675358146830681\n",
      "Epoch 262, Loss: 0.4675934670336611\n",
      "Epoch 263, Loss: 0.4675883044263754\n",
      "Epoch 264, Loss: 0.46749525618568777\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.21882431375802625\n",
      "Valence RMSE: 0.2061380452166804\n",
      "Arousal RMSE: 0.23081435591745175\n",
      "Test R^2 score: tensor([0.4850, 0.4725], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.3645, 0.3490], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4787570704324936\n",
      "Num of epochs: 265\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Epoch 121, Loss: 0.49295912913079615\n",
      "Epoch 122, Loss: 0.4924773619139838\n",
      "Epoch 123, Loss: 0.49197607191949805\n",
      "Epoch 124, Loss: 0.4914699197831528\n",
      "Epoch 125, Loss: 0.49091483381312667\n",
      "Epoch 126, Loss: 0.49035147677946317\n",
      "Epoch 127, Loss: 0.4898087222825037\n",
      "Epoch 128, Loss: 0.48924014731550836\n",
      "Epoch 129, Loss: 0.48873550647212044\n",
      "Epoch 130, Loss: 0.48832307254971563\n",
      "Epoch 131, Loss: 0.48791418392821295\n",
      "Epoch 132, Loss: 0.4875902245243796\n",
      "Epoch 133, Loss: 0.48728851108037585\n",
      "Epoch 134, Loss: 0.48697062262181345\n",
      "Epoch 135, Loss: 0.4866339553030548\n",
      "Epoch 136, Loss: 0.48633446746734893\n",
      "Epoch 137, Loss: 0.4860310087358514\n",
      "Epoch 138, Loss: 0.4856786412545615\n",
      "Epoch 139, Loss: 0.48531372107004\n",
      "Epoch 140, Loss: 0.48495712985488587\n",
      "Epoch 141, Loss: 0.4846131600652309\n",
      "Epoch 142, Loss: 0.48425966858652364\n",
      "Epoch 143, Loss: 0.4838914149236435\n",
      "Epoch 144, Loss: 0.48352437546282767\n",
      "Epoch 145, Loss: 0.4832299911937133\n",
      "Epoch 146, Loss: 0.4832586529465723\n",
      "Epoch 147, Loss: 0.48356744143113517\n",
      "Epoch 148, Loss: 0.4828108019011922\n",
      "Epoch 149, Loss: 0.4818912939738211\n",
      "Epoch 150, Loss: 0.48239416745575797\n",
      "Epoch 151, Loss: 0.4814952491721961\n",
      "Epoch 152, Loss: 0.4811822818820502\n",
      "Epoch 153, Loss: 0.4812753156716337\n",
      "Epoch 154, Loss: 0.4804387742282827\n",
      "Epoch 155, Loss: 0.48058189052417905\n",
      "Epoch 156, Loss: 0.4801288115497346\n",
      "Epoch 157, Loss: 0.4797465937246525\n",
      "Epoch 158, Loss: 0.4797622479529733\n",
      "Epoch 159, Loss: 0.4791664679844762\n",
      "Epoch 160, Loss: 0.47904359923301915\n",
      "Epoch 161, Loss: 0.4788483533353555\n",
      "Epoch 162, Loss: 0.4783709093142749\n",
      "Epoch 163, Loss: 0.4783027642591491\n",
      "Epoch 164, Loss: 0.4780003946284736\n",
      "Epoch 165, Loss: 0.47766356610156796\n",
      "Epoch 166, Loss: 0.47756940749162413\n",
      "Epoch 167, Loss: 0.4772720526560437\n",
      "Epoch 168, Loss: 0.477001191512605\n",
      "Epoch 169, Loss: 0.47690651155802954\n",
      "Epoch 170, Loss: 0.47663605231839845\n",
      "Epoch 171, Loss: 0.4763715079887003\n",
      "Epoch 172, Loss: 0.4762479807342815\n",
      "Epoch 173, Loss: 0.47603155482909787\n",
      "Epoch 174, Loss: 0.47580085926956445\n",
      "Epoch 175, Loss: 0.4756396378860013\n",
      "Epoch 176, Loss: 0.4754627232666358\n",
      "Epoch 177, Loss: 0.4752513954316692\n",
      "Epoch 178, Loss: 0.47507140350730925\n",
      "Epoch 179, Loss: 0.47492471273973497\n",
      "Epoch 180, Loss: 0.47475493910732763\n",
      "Epoch 181, Loss: 0.47455157018218247\n",
      "Epoch 182, Loss: 0.4743853224635176\n",
      "Epoch 183, Loss: 0.47424550491599726\n",
      "Epoch 184, Loss: 0.47408413178207975\n",
      "Epoch 185, Loss: 0.47390385374022453\n",
      "Epoch 186, Loss: 0.4737383066259127\n",
      "Epoch 187, Loss: 0.47358797785338436\n",
      "Epoch 188, Loss: 0.4734560449826861\n",
      "Epoch 189, Loss: 0.47335738206045025\n",
      "Epoch 190, Loss: 0.4733162834925985\n",
      "Epoch 191, Loss: 0.4732413020737725\n",
      "Epoch 192, Loss: 0.47313239020014447\n",
      "Epoch 193, Loss: 0.4729457158004841\n",
      "Epoch 194, Loss: 0.47274703738102797\n",
      "Epoch 195, Loss: 0.47253750657270627\n",
      "Epoch 196, Loss: 0.4723544299968521\n",
      "Epoch 197, Loss: 0.47221102907746554\n",
      "Epoch 198, Loss: 0.47210844467503205\n",
      "Epoch 199, Loss: 0.47205337980155637\n",
      "Epoch 200, Loss: 0.47204973383515075\n",
      "Epoch 201, Loss: 0.4720427890593303\n",
      "Epoch 202, Loss: 0.4719159657967507\n",
      "Epoch 203, Loss: 0.471735017109987\n",
      "Epoch 204, Loss: 0.4714985533244086\n",
      "Epoch 205, Loss: 0.4713143302002759\n",
      "Epoch 206, Loss: 0.47118574544243713\n",
      "Epoch 207, Loss: 0.47112002449718005\n",
      "Epoch 208, Loss: 0.4710910670518786\n",
      "Epoch 209, Loss: 0.47102495326923743\n",
      "Epoch 210, Loss: 0.4709626586360175\n",
      "Epoch 211, Loss: 0.47085840979895394\n",
      "Epoch 212, Loss: 0.47078064719388485\n",
      "Epoch 213, Loss: 0.47066614789276784\n",
      "Epoch 214, Loss: 0.47055654499036753\n",
      "Epoch 215, Loss: 0.47041652390832933\n",
      "Epoch 216, Loss: 0.4702446473560784\n",
      "Epoch 217, Loss: 0.4701045016774931\n",
      "Epoch 218, Loss: 0.47002059077245106\n",
      "Epoch 219, Loss: 0.4699643617672827\n",
      "Epoch 220, Loss: 0.4698888296176042\n",
      "Epoch 221, Loss: 0.4698141575467262\n",
      "Epoch 222, Loss: 0.4697265783615268\n",
      "Epoch 223, Loss: 0.46966066909468496\n",
      "Epoch 224, Loss: 0.4696348739248034\n",
      "Epoch 225, Loss: 0.4697176006517021\n",
      "Epoch 226, Loss: 0.4699424675720642\n",
      "Epoch 227, Loss: 0.46999269112017344\n",
      "Epoch 228, Loss: 0.4698759384724347\n",
      "Epoch 229, Loss: 0.46938041574678274\n",
      "Epoch 230, Loss: 0.4691415581864629\n",
      "Epoch 231, Loss: 0.46924839544758656\n",
      "Epoch 232, Loss: 0.46933531775018905\n",
      "Epoch 233, Loss: 0.4692253881106229\n",
      "Epoch 234, Loss: 0.46894320638835874\n",
      "Epoch 235, Loss: 0.4688628060980674\n",
      "Epoch 236, Loss: 0.46897798398181567\n",
      "Epoch 237, Loss: 0.4689811295666005\n",
      "Epoch 238, Loss: 0.4688031484354944\n",
      "Epoch 239, Loss: 0.46862983752908405\n",
      "Epoch 240, Loss: 0.4685963696810109\n",
      "Epoch 241, Loss: 0.46867106090894\n",
      "Epoch 242, Loss: 0.46866360503981047\n",
      "Epoch 243, Loss: 0.468513635806399\n",
      "Epoch 244, Loss: 0.46835730315983837\n",
      "Epoch 245, Loss: 0.468344910742699\n",
      "Epoch 246, Loss: 0.46842993299443175\n",
      "Epoch 247, Loss: 0.4683803372309361\n",
      "Epoch 248, Loss: 0.46823408027364943\n",
      "Epoch 249, Loss: 0.46809559282800506\n",
      "Epoch 250, Loss: 0.4681194355765096\n",
      "Epoch 251, Loss: 0.4682059468617626\n",
      "Epoch 252, Loss: 0.46815125054377077\n",
      "Epoch 253, Loss: 0.46800465135626707\n",
      "Epoch 254, Loss: 0.4678516840148294\n",
      "Epoch 255, Loss: 0.46781997608436393\n",
      "Epoch 256, Loss: 0.4678506648079015\n",
      "Epoch 257, Loss: 0.4677995105079106\n",
      "Epoch 258, Loss: 0.4676942061377158\n",
      "Epoch 259, Loss: 0.46760336187164103\n",
      "Epoch 260, Loss: 0.46754277859764004\n",
      "Epoch 261, Loss: 0.4675358146830681\n",
      "Epoch 262, Loss: 0.4675934670336611\n",
      "Epoch 263, Loss: 0.4675883044263754\n",
      "Epoch 264, Loss: 0.46749525618568777\n",
      "Epoch 265, Loss: 0.46735425416294335\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.21835626666228675\n",
      "Valence RMSE: 0.20569017320138536\n",
      "Arousal RMSE: 0.23032687865242185\n",
      "Test R^2 score: tensor([0.4873, 0.4747], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.3673, 0.3518], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4809876510118344\n",
      "Num of epochs: 266\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Epoch 121, Loss: 0.49295912913079615\n",
      "Epoch 122, Loss: 0.4924773619139838\n",
      "Epoch 123, Loss: 0.49197607191949805\n",
      "Epoch 124, Loss: 0.4914699197831528\n",
      "Epoch 125, Loss: 0.49091483381312667\n",
      "Epoch 126, Loss: 0.49035147677946317\n",
      "Epoch 127, Loss: 0.4898087222825037\n",
      "Epoch 128, Loss: 0.48924014731550836\n",
      "Epoch 129, Loss: 0.48873550647212044\n",
      "Epoch 130, Loss: 0.48832307254971563\n",
      "Epoch 131, Loss: 0.48791418392821295\n",
      "Epoch 132, Loss: 0.4875902245243796\n",
      "Epoch 133, Loss: 0.48728851108037585\n",
      "Epoch 134, Loss: 0.48697062262181345\n",
      "Epoch 135, Loss: 0.4866339553030548\n",
      "Epoch 136, Loss: 0.48633446746734893\n",
      "Epoch 137, Loss: 0.4860310087358514\n",
      "Epoch 138, Loss: 0.4856786412545615\n",
      "Epoch 139, Loss: 0.48531372107004\n",
      "Epoch 140, Loss: 0.48495712985488587\n",
      "Epoch 141, Loss: 0.4846131600652309\n",
      "Epoch 142, Loss: 0.48425966858652364\n",
      "Epoch 143, Loss: 0.4838914149236435\n",
      "Epoch 144, Loss: 0.48352437546282767\n",
      "Epoch 145, Loss: 0.4832299911937133\n",
      "Epoch 146, Loss: 0.4832586529465723\n",
      "Epoch 147, Loss: 0.48356744143113517\n",
      "Epoch 148, Loss: 0.4828108019011922\n",
      "Epoch 149, Loss: 0.4818912939738211\n",
      "Epoch 150, Loss: 0.48239416745575797\n",
      "Epoch 151, Loss: 0.4814952491721961\n",
      "Epoch 152, Loss: 0.4811822818820502\n",
      "Epoch 153, Loss: 0.4812753156716337\n",
      "Epoch 154, Loss: 0.4804387742282827\n",
      "Epoch 155, Loss: 0.48058189052417905\n",
      "Epoch 156, Loss: 0.4801288115497346\n",
      "Epoch 157, Loss: 0.4797465937246525\n",
      "Epoch 158, Loss: 0.4797622479529733\n",
      "Epoch 159, Loss: 0.4791664679844762\n",
      "Epoch 160, Loss: 0.47904359923301915\n",
      "Epoch 161, Loss: 0.4788483533353555\n",
      "Epoch 162, Loss: 0.4783709093142749\n",
      "Epoch 163, Loss: 0.4783027642591491\n",
      "Epoch 164, Loss: 0.4780003946284736\n",
      "Epoch 165, Loss: 0.47766356610156796\n",
      "Epoch 166, Loss: 0.47756940749162413\n",
      "Epoch 167, Loss: 0.4772720526560437\n",
      "Epoch 168, Loss: 0.477001191512605\n",
      "Epoch 169, Loss: 0.47690651155802954\n",
      "Epoch 170, Loss: 0.47663605231839845\n",
      "Epoch 171, Loss: 0.4763715079887003\n",
      "Epoch 172, Loss: 0.4762479807342815\n",
      "Epoch 173, Loss: 0.47603155482909787\n",
      "Epoch 174, Loss: 0.47580085926956445\n",
      "Epoch 175, Loss: 0.4756396378860013\n",
      "Epoch 176, Loss: 0.4754627232666358\n",
      "Epoch 177, Loss: 0.4752513954316692\n",
      "Epoch 178, Loss: 0.47507140350730925\n",
      "Epoch 179, Loss: 0.47492471273973497\n",
      "Epoch 180, Loss: 0.47475493910732763\n",
      "Epoch 181, Loss: 0.47455157018218247\n",
      "Epoch 182, Loss: 0.4743853224635176\n",
      "Epoch 183, Loss: 0.47424550491599726\n",
      "Epoch 184, Loss: 0.47408413178207975\n",
      "Epoch 185, Loss: 0.47390385374022453\n",
      "Epoch 186, Loss: 0.4737383066259127\n",
      "Epoch 187, Loss: 0.47358797785338436\n",
      "Epoch 188, Loss: 0.4734560449826861\n",
      "Epoch 189, Loss: 0.47335738206045025\n",
      "Epoch 190, Loss: 0.4733162834925985\n",
      "Epoch 191, Loss: 0.4732413020737725\n",
      "Epoch 192, Loss: 0.47313239020014447\n",
      "Epoch 193, Loss: 0.4729457158004841\n",
      "Epoch 194, Loss: 0.47274703738102797\n",
      "Epoch 195, Loss: 0.47253750657270627\n",
      "Epoch 196, Loss: 0.4723544299968521\n",
      "Epoch 197, Loss: 0.47221102907746554\n",
      "Epoch 198, Loss: 0.47210844467503205\n",
      "Epoch 199, Loss: 0.47205337980155637\n",
      "Epoch 200, Loss: 0.47204973383515075\n",
      "Epoch 201, Loss: 0.4720427890593303\n",
      "Epoch 202, Loss: 0.4719159657967507\n",
      "Epoch 203, Loss: 0.471735017109987\n",
      "Epoch 204, Loss: 0.4714985533244086\n",
      "Epoch 205, Loss: 0.4713143302002759\n",
      "Epoch 206, Loss: 0.47118574544243713\n",
      "Epoch 207, Loss: 0.47112002449718005\n",
      "Epoch 208, Loss: 0.4710910670518786\n",
      "Epoch 209, Loss: 0.47102495326923743\n",
      "Epoch 210, Loss: 0.4709626586360175\n",
      "Epoch 211, Loss: 0.47085840979895394\n",
      "Epoch 212, Loss: 0.47078064719388485\n",
      "Epoch 213, Loss: 0.47066614789276784\n",
      "Epoch 214, Loss: 0.47055654499036753\n",
      "Epoch 215, Loss: 0.47041652390832933\n",
      "Epoch 216, Loss: 0.4702446473560784\n",
      "Epoch 217, Loss: 0.4701045016774931\n",
      "Epoch 218, Loss: 0.47002059077245106\n",
      "Epoch 219, Loss: 0.4699643617672827\n",
      "Epoch 220, Loss: 0.4698888296176042\n",
      "Epoch 221, Loss: 0.4698141575467262\n",
      "Epoch 222, Loss: 0.4697265783615268\n",
      "Epoch 223, Loss: 0.46966066909468496\n",
      "Epoch 224, Loss: 0.4696348739248034\n",
      "Epoch 225, Loss: 0.4697176006517021\n",
      "Epoch 226, Loss: 0.4699424675720642\n",
      "Epoch 227, Loss: 0.46999269112017344\n",
      "Epoch 228, Loss: 0.4698759384724347\n",
      "Epoch 229, Loss: 0.46938041574678274\n",
      "Epoch 230, Loss: 0.4691415581864629\n",
      "Epoch 231, Loss: 0.46924839544758656\n",
      "Epoch 232, Loss: 0.46933531775018905\n",
      "Epoch 233, Loss: 0.4692253881106229\n",
      "Epoch 234, Loss: 0.46894320638835874\n",
      "Epoch 235, Loss: 0.4688628060980674\n",
      "Epoch 236, Loss: 0.46897798398181567\n",
      "Epoch 237, Loss: 0.4689811295666005\n",
      "Epoch 238, Loss: 0.4688031484354944\n",
      "Epoch 239, Loss: 0.46862983752908405\n",
      "Epoch 240, Loss: 0.4685963696810109\n",
      "Epoch 241, Loss: 0.46867106090894\n",
      "Epoch 242, Loss: 0.46866360503981047\n",
      "Epoch 243, Loss: 0.468513635806399\n",
      "Epoch 244, Loss: 0.46835730315983837\n",
      "Epoch 245, Loss: 0.468344910742699\n",
      "Epoch 246, Loss: 0.46842993299443175\n",
      "Epoch 247, Loss: 0.4683803372309361\n",
      "Epoch 248, Loss: 0.46823408027364943\n",
      "Epoch 249, Loss: 0.46809559282800506\n",
      "Epoch 250, Loss: 0.4681194355765096\n",
      "Epoch 251, Loss: 0.4682059468617626\n",
      "Epoch 252, Loss: 0.46815125054377077\n",
      "Epoch 253, Loss: 0.46800465135626707\n",
      "Epoch 254, Loss: 0.4678516840148294\n",
      "Epoch 255, Loss: 0.46781997608436393\n",
      "Epoch 256, Loss: 0.4678506648079015\n",
      "Epoch 257, Loss: 0.4677995105079106\n",
      "Epoch 258, Loss: 0.4676942061377158\n",
      "Epoch 259, Loss: 0.46760336187164103\n",
      "Epoch 260, Loss: 0.46754277859764004\n",
      "Epoch 261, Loss: 0.4675358146830681\n",
      "Epoch 262, Loss: 0.4675934670336611\n",
      "Epoch 263, Loss: 0.4675883044263754\n",
      "Epoch 264, Loss: 0.46749525618568777\n",
      "Epoch 265, Loss: 0.46735425416294335\n",
      "Epoch 266, Loss: 0.4672686694514329\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.21819448410476236\n",
      "Valence RMSE: 0.205537040964188\n",
      "Arousal RMSE: 0.23015688253704794\n",
      "Test R^2 score: tensor([0.4880, 0.4755], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.3682, 0.3527], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.48175678457851273\n",
      "Num of epochs: 267\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Epoch 121, Loss: 0.49295912913079615\n",
      "Epoch 122, Loss: 0.4924773619139838\n",
      "Epoch 123, Loss: 0.49197607191949805\n",
      "Epoch 124, Loss: 0.4914699197831528\n",
      "Epoch 125, Loss: 0.49091483381312667\n",
      "Epoch 126, Loss: 0.49035147677946317\n",
      "Epoch 127, Loss: 0.4898087222825037\n",
      "Epoch 128, Loss: 0.48924014731550836\n",
      "Epoch 129, Loss: 0.48873550647212044\n",
      "Epoch 130, Loss: 0.48832307254971563\n",
      "Epoch 131, Loss: 0.48791418392821295\n",
      "Epoch 132, Loss: 0.4875902245243796\n",
      "Epoch 133, Loss: 0.48728851108037585\n",
      "Epoch 134, Loss: 0.48697062262181345\n",
      "Epoch 135, Loss: 0.4866339553030548\n",
      "Epoch 136, Loss: 0.48633446746734893\n",
      "Epoch 137, Loss: 0.4860310087358514\n",
      "Epoch 138, Loss: 0.4856786412545615\n",
      "Epoch 139, Loss: 0.48531372107004\n",
      "Epoch 140, Loss: 0.48495712985488587\n",
      "Epoch 141, Loss: 0.4846131600652309\n",
      "Epoch 142, Loss: 0.48425966858652364\n",
      "Epoch 143, Loss: 0.4838914149236435\n",
      "Epoch 144, Loss: 0.48352437546282767\n",
      "Epoch 145, Loss: 0.4832299911937133\n",
      "Epoch 146, Loss: 0.4832586529465723\n",
      "Epoch 147, Loss: 0.48356744143113517\n",
      "Epoch 148, Loss: 0.4828108019011922\n",
      "Epoch 149, Loss: 0.4818912939738211\n",
      "Epoch 150, Loss: 0.48239416745575797\n",
      "Epoch 151, Loss: 0.4814952491721961\n",
      "Epoch 152, Loss: 0.4811822818820502\n",
      "Epoch 153, Loss: 0.4812753156716337\n",
      "Epoch 154, Loss: 0.4804387742282827\n",
      "Epoch 155, Loss: 0.48058189052417905\n",
      "Epoch 156, Loss: 0.4801288115497346\n",
      "Epoch 157, Loss: 0.4797465937246525\n",
      "Epoch 158, Loss: 0.4797622479529733\n",
      "Epoch 159, Loss: 0.4791664679844762\n",
      "Epoch 160, Loss: 0.47904359923301915\n",
      "Epoch 161, Loss: 0.4788483533353555\n",
      "Epoch 162, Loss: 0.4783709093142749\n",
      "Epoch 163, Loss: 0.4783027642591491\n",
      "Epoch 164, Loss: 0.4780003946284736\n",
      "Epoch 165, Loss: 0.47766356610156796\n",
      "Epoch 166, Loss: 0.47756940749162413\n",
      "Epoch 167, Loss: 0.4772720526560437\n",
      "Epoch 168, Loss: 0.477001191512605\n",
      "Epoch 169, Loss: 0.47690651155802954\n",
      "Epoch 170, Loss: 0.47663605231839845\n",
      "Epoch 171, Loss: 0.4763715079887003\n",
      "Epoch 172, Loss: 0.4762479807342815\n",
      "Epoch 173, Loss: 0.47603155482909787\n",
      "Epoch 174, Loss: 0.47580085926956445\n",
      "Epoch 175, Loss: 0.4756396378860013\n",
      "Epoch 176, Loss: 0.4754627232666358\n",
      "Epoch 177, Loss: 0.4752513954316692\n",
      "Epoch 178, Loss: 0.47507140350730925\n",
      "Epoch 179, Loss: 0.47492471273973497\n",
      "Epoch 180, Loss: 0.47475493910732763\n",
      "Epoch 181, Loss: 0.47455157018218247\n",
      "Epoch 182, Loss: 0.4743853224635176\n",
      "Epoch 183, Loss: 0.47424550491599726\n",
      "Epoch 184, Loss: 0.47408413178207975\n",
      "Epoch 185, Loss: 0.47390385374022453\n",
      "Epoch 186, Loss: 0.4737383066259127\n",
      "Epoch 187, Loss: 0.47358797785338436\n",
      "Epoch 188, Loss: 0.4734560449826861\n",
      "Epoch 189, Loss: 0.47335738206045025\n",
      "Epoch 190, Loss: 0.4733162834925985\n",
      "Epoch 191, Loss: 0.4732413020737725\n",
      "Epoch 192, Loss: 0.47313239020014447\n",
      "Epoch 193, Loss: 0.4729457158004841\n",
      "Epoch 194, Loss: 0.47274703738102797\n",
      "Epoch 195, Loss: 0.47253750657270627\n",
      "Epoch 196, Loss: 0.4723544299968521\n",
      "Epoch 197, Loss: 0.47221102907746554\n",
      "Epoch 198, Loss: 0.47210844467503205\n",
      "Epoch 199, Loss: 0.47205337980155637\n",
      "Epoch 200, Loss: 0.47204973383515075\n",
      "Epoch 201, Loss: 0.4720427890593303\n",
      "Epoch 202, Loss: 0.4719159657967507\n",
      "Epoch 203, Loss: 0.471735017109987\n",
      "Epoch 204, Loss: 0.4714985533244086\n",
      "Epoch 205, Loss: 0.4713143302002759\n",
      "Epoch 206, Loss: 0.47118574544243713\n",
      "Epoch 207, Loss: 0.47112002449718005\n",
      "Epoch 208, Loss: 0.4710910670518786\n",
      "Epoch 209, Loss: 0.47102495326923743\n",
      "Epoch 210, Loss: 0.4709626586360175\n",
      "Epoch 211, Loss: 0.47085840979895394\n",
      "Epoch 212, Loss: 0.47078064719388485\n",
      "Epoch 213, Loss: 0.47066614789276784\n",
      "Epoch 214, Loss: 0.47055654499036753\n",
      "Epoch 215, Loss: 0.47041652390832933\n",
      "Epoch 216, Loss: 0.4702446473560784\n",
      "Epoch 217, Loss: 0.4701045016774931\n",
      "Epoch 218, Loss: 0.47002059077245106\n",
      "Epoch 219, Loss: 0.4699643617672827\n",
      "Epoch 220, Loss: 0.4698888296176042\n",
      "Epoch 221, Loss: 0.4698141575467262\n",
      "Epoch 222, Loss: 0.4697265783615268\n",
      "Epoch 223, Loss: 0.46966066909468496\n",
      "Epoch 224, Loss: 0.4696348739248034\n",
      "Epoch 225, Loss: 0.4697176006517021\n",
      "Epoch 226, Loss: 0.4699424675720642\n",
      "Epoch 227, Loss: 0.46999269112017344\n",
      "Epoch 228, Loss: 0.4698759384724347\n",
      "Epoch 229, Loss: 0.46938041574678274\n",
      "Epoch 230, Loss: 0.4691415581864629\n",
      "Epoch 231, Loss: 0.46924839544758656\n",
      "Epoch 232, Loss: 0.46933531775018905\n",
      "Epoch 233, Loss: 0.4692253881106229\n",
      "Epoch 234, Loss: 0.46894320638835874\n",
      "Epoch 235, Loss: 0.4688628060980674\n",
      "Epoch 236, Loss: 0.46897798398181567\n",
      "Epoch 237, Loss: 0.4689811295666005\n",
      "Epoch 238, Loss: 0.4688031484354944\n",
      "Epoch 239, Loss: 0.46862983752908405\n",
      "Epoch 240, Loss: 0.4685963696810109\n",
      "Epoch 241, Loss: 0.46867106090894\n",
      "Epoch 242, Loss: 0.46866360503981047\n",
      "Epoch 243, Loss: 0.468513635806399\n",
      "Epoch 244, Loss: 0.46835730315983837\n",
      "Epoch 245, Loss: 0.468344910742699\n",
      "Epoch 246, Loss: 0.46842993299443175\n",
      "Epoch 247, Loss: 0.4683803372309361\n",
      "Epoch 248, Loss: 0.46823408027364943\n",
      "Epoch 249, Loss: 0.46809559282800506\n",
      "Epoch 250, Loss: 0.4681194355765096\n",
      "Epoch 251, Loss: 0.4682059468617626\n",
      "Epoch 252, Loss: 0.46815125054377077\n",
      "Epoch 253, Loss: 0.46800465135626707\n",
      "Epoch 254, Loss: 0.4678516840148294\n",
      "Epoch 255, Loss: 0.46781997608436393\n",
      "Epoch 256, Loss: 0.4678506648079015\n",
      "Epoch 257, Loss: 0.4677995105079106\n",
      "Epoch 258, Loss: 0.4676942061377158\n",
      "Epoch 259, Loss: 0.46760336187164103\n",
      "Epoch 260, Loss: 0.46754277859764004\n",
      "Epoch 261, Loss: 0.4675358146830681\n",
      "Epoch 262, Loss: 0.4675934670336611\n",
      "Epoch 263, Loss: 0.4675883044263754\n",
      "Epoch 264, Loss: 0.46749525618568777\n",
      "Epoch 265, Loss: 0.46735425416294335\n",
      "Epoch 266, Loss: 0.4672686694514329\n",
      "Epoch 267, Loss: 0.46723722493080344\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.21876696437883916\n",
      "Valence RMSE: 0.2061064094356015\n",
      "Arousal RMSE: 0.23073386703435791\n",
      "Test R^2 score: tensor([0.4852, 0.4729], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.3647, 0.3495], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.47902001767516866\n",
      "Num of epochs: 268\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Epoch 121, Loss: 0.49295912913079615\n",
      "Epoch 122, Loss: 0.4924773619139838\n",
      "Epoch 123, Loss: 0.49197607191949805\n",
      "Epoch 124, Loss: 0.4914699197831528\n",
      "Epoch 125, Loss: 0.49091483381312667\n",
      "Epoch 126, Loss: 0.49035147677946317\n",
      "Epoch 127, Loss: 0.4898087222825037\n",
      "Epoch 128, Loss: 0.48924014731550836\n",
      "Epoch 129, Loss: 0.48873550647212044\n",
      "Epoch 130, Loss: 0.48832307254971563\n",
      "Epoch 131, Loss: 0.48791418392821295\n",
      "Epoch 132, Loss: 0.4875902245243796\n",
      "Epoch 133, Loss: 0.48728851108037585\n",
      "Epoch 134, Loss: 0.48697062262181345\n",
      "Epoch 135, Loss: 0.4866339553030548\n",
      "Epoch 136, Loss: 0.48633446746734893\n",
      "Epoch 137, Loss: 0.4860310087358514\n",
      "Epoch 138, Loss: 0.4856786412545615\n",
      "Epoch 139, Loss: 0.48531372107004\n",
      "Epoch 140, Loss: 0.48495712985488587\n",
      "Epoch 141, Loss: 0.4846131600652309\n",
      "Epoch 142, Loss: 0.48425966858652364\n",
      "Epoch 143, Loss: 0.4838914149236435\n",
      "Epoch 144, Loss: 0.48352437546282767\n",
      "Epoch 145, Loss: 0.4832299911937133\n",
      "Epoch 146, Loss: 0.4832586529465723\n",
      "Epoch 147, Loss: 0.48356744143113517\n",
      "Epoch 148, Loss: 0.4828108019011922\n",
      "Epoch 149, Loss: 0.4818912939738211\n",
      "Epoch 150, Loss: 0.48239416745575797\n",
      "Epoch 151, Loss: 0.4814952491721961\n",
      "Epoch 152, Loss: 0.4811822818820502\n",
      "Epoch 153, Loss: 0.4812753156716337\n",
      "Epoch 154, Loss: 0.4804387742282827\n",
      "Epoch 155, Loss: 0.48058189052417905\n",
      "Epoch 156, Loss: 0.4801288115497346\n",
      "Epoch 157, Loss: 0.4797465937246525\n",
      "Epoch 158, Loss: 0.4797622479529733\n",
      "Epoch 159, Loss: 0.4791664679844762\n",
      "Epoch 160, Loss: 0.47904359923301915\n",
      "Epoch 161, Loss: 0.4788483533353555\n",
      "Epoch 162, Loss: 0.4783709093142749\n",
      "Epoch 163, Loss: 0.4783027642591491\n",
      "Epoch 164, Loss: 0.4780003946284736\n",
      "Epoch 165, Loss: 0.47766356610156796\n",
      "Epoch 166, Loss: 0.47756940749162413\n",
      "Epoch 167, Loss: 0.4772720526560437\n",
      "Epoch 168, Loss: 0.477001191512605\n",
      "Epoch 169, Loss: 0.47690651155802954\n",
      "Epoch 170, Loss: 0.47663605231839845\n",
      "Epoch 171, Loss: 0.4763715079887003\n",
      "Epoch 172, Loss: 0.4762479807342815\n",
      "Epoch 173, Loss: 0.47603155482909787\n",
      "Epoch 174, Loss: 0.47580085926956445\n",
      "Epoch 175, Loss: 0.4756396378860013\n",
      "Epoch 176, Loss: 0.4754627232666358\n",
      "Epoch 177, Loss: 0.4752513954316692\n",
      "Epoch 178, Loss: 0.47507140350730925\n",
      "Epoch 179, Loss: 0.47492471273973497\n",
      "Epoch 180, Loss: 0.47475493910732763\n",
      "Epoch 181, Loss: 0.47455157018218247\n",
      "Epoch 182, Loss: 0.4743853224635176\n",
      "Epoch 183, Loss: 0.47424550491599726\n",
      "Epoch 184, Loss: 0.47408413178207975\n",
      "Epoch 185, Loss: 0.47390385374022453\n",
      "Epoch 186, Loss: 0.4737383066259127\n",
      "Epoch 187, Loss: 0.47358797785338436\n",
      "Epoch 188, Loss: 0.4734560449826861\n",
      "Epoch 189, Loss: 0.47335738206045025\n",
      "Epoch 190, Loss: 0.4733162834925985\n",
      "Epoch 191, Loss: 0.4732413020737725\n",
      "Epoch 192, Loss: 0.47313239020014447\n",
      "Epoch 193, Loss: 0.4729457158004841\n",
      "Epoch 194, Loss: 0.47274703738102797\n",
      "Epoch 195, Loss: 0.47253750657270627\n",
      "Epoch 196, Loss: 0.4723544299968521\n",
      "Epoch 197, Loss: 0.47221102907746554\n",
      "Epoch 198, Loss: 0.47210844467503205\n",
      "Epoch 199, Loss: 0.47205337980155637\n",
      "Epoch 200, Loss: 0.47204973383515075\n",
      "Epoch 201, Loss: 0.4720427890593303\n",
      "Epoch 202, Loss: 0.4719159657967507\n",
      "Epoch 203, Loss: 0.471735017109987\n",
      "Epoch 204, Loss: 0.4714985533244086\n",
      "Epoch 205, Loss: 0.4713143302002759\n",
      "Epoch 206, Loss: 0.47118574544243713\n",
      "Epoch 207, Loss: 0.47112002449718005\n",
      "Epoch 208, Loss: 0.4710910670518786\n",
      "Epoch 209, Loss: 0.47102495326923743\n",
      "Epoch 210, Loss: 0.4709626586360175\n",
      "Epoch 211, Loss: 0.47085840979895394\n",
      "Epoch 212, Loss: 0.47078064719388485\n",
      "Epoch 213, Loss: 0.47066614789276784\n",
      "Epoch 214, Loss: 0.47055654499036753\n",
      "Epoch 215, Loss: 0.47041652390832933\n",
      "Epoch 216, Loss: 0.4702446473560784\n",
      "Epoch 217, Loss: 0.4701045016774931\n",
      "Epoch 218, Loss: 0.47002059077245106\n",
      "Epoch 219, Loss: 0.4699643617672827\n",
      "Epoch 220, Loss: 0.4698888296176042\n",
      "Epoch 221, Loss: 0.4698141575467262\n",
      "Epoch 222, Loss: 0.4697265783615268\n",
      "Epoch 223, Loss: 0.46966066909468496\n",
      "Epoch 224, Loss: 0.4696348739248034\n",
      "Epoch 225, Loss: 0.4697176006517021\n",
      "Epoch 226, Loss: 0.4699424675720642\n",
      "Epoch 227, Loss: 0.46999269112017344\n",
      "Epoch 228, Loss: 0.4698759384724347\n",
      "Epoch 229, Loss: 0.46938041574678274\n",
      "Epoch 230, Loss: 0.4691415581864629\n",
      "Epoch 231, Loss: 0.46924839544758656\n",
      "Epoch 232, Loss: 0.46933531775018905\n",
      "Epoch 233, Loss: 0.4692253881106229\n",
      "Epoch 234, Loss: 0.46894320638835874\n",
      "Epoch 235, Loss: 0.4688628060980674\n",
      "Epoch 236, Loss: 0.46897798398181567\n",
      "Epoch 237, Loss: 0.4689811295666005\n",
      "Epoch 238, Loss: 0.4688031484354944\n",
      "Epoch 239, Loss: 0.46862983752908405\n",
      "Epoch 240, Loss: 0.4685963696810109\n",
      "Epoch 241, Loss: 0.46867106090894\n",
      "Epoch 242, Loss: 0.46866360503981047\n",
      "Epoch 243, Loss: 0.468513635806399\n",
      "Epoch 244, Loss: 0.46835730315983837\n",
      "Epoch 245, Loss: 0.468344910742699\n",
      "Epoch 246, Loss: 0.46842993299443175\n",
      "Epoch 247, Loss: 0.4683803372309361\n",
      "Epoch 248, Loss: 0.46823408027364943\n",
      "Epoch 249, Loss: 0.46809559282800506\n",
      "Epoch 250, Loss: 0.4681194355765096\n",
      "Epoch 251, Loss: 0.4682059468617626\n",
      "Epoch 252, Loss: 0.46815125054377077\n",
      "Epoch 253, Loss: 0.46800465135626707\n",
      "Epoch 254, Loss: 0.4678516840148294\n",
      "Epoch 255, Loss: 0.46781997608436393\n",
      "Epoch 256, Loss: 0.4678506648079015\n",
      "Epoch 257, Loss: 0.4677995105079106\n",
      "Epoch 258, Loss: 0.4676942061377158\n",
      "Epoch 259, Loss: 0.46760336187164103\n",
      "Epoch 260, Loss: 0.46754277859764004\n",
      "Epoch 261, Loss: 0.4675358146830681\n",
      "Epoch 262, Loss: 0.4675934670336611\n",
      "Epoch 263, Loss: 0.4675883044263754\n",
      "Epoch 264, Loss: 0.46749525618568777\n",
      "Epoch 265, Loss: 0.46735425416294335\n",
      "Epoch 266, Loss: 0.4672686694514329\n",
      "Epoch 267, Loss: 0.46723722493080344\n",
      "Epoch 268, Loss: 0.4672375438513713\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.21794770736882613\n",
      "Valence RMSE: 0.20529468355837086\n",
      "Arousal RMSE: 0.22990541358855235\n",
      "Test R^2 score: tensor([0.4892, 0.4766], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.3697, 0.3541], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4829328868751622\n",
      "Num of epochs: 269\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Epoch 121, Loss: 0.49295912913079615\n",
      "Epoch 122, Loss: 0.4924773619139838\n",
      "Epoch 123, Loss: 0.49197607191949805\n",
      "Epoch 124, Loss: 0.4914699197831528\n",
      "Epoch 125, Loss: 0.49091483381312667\n",
      "Epoch 126, Loss: 0.49035147677946317\n",
      "Epoch 127, Loss: 0.4898087222825037\n",
      "Epoch 128, Loss: 0.48924014731550836\n",
      "Epoch 129, Loss: 0.48873550647212044\n",
      "Epoch 130, Loss: 0.48832307254971563\n",
      "Epoch 131, Loss: 0.48791418392821295\n",
      "Epoch 132, Loss: 0.4875902245243796\n",
      "Epoch 133, Loss: 0.48728851108037585\n",
      "Epoch 134, Loss: 0.48697062262181345\n",
      "Epoch 135, Loss: 0.4866339553030548\n",
      "Epoch 136, Loss: 0.48633446746734893\n",
      "Epoch 137, Loss: 0.4860310087358514\n",
      "Epoch 138, Loss: 0.4856786412545615\n",
      "Epoch 139, Loss: 0.48531372107004\n",
      "Epoch 140, Loss: 0.48495712985488587\n",
      "Epoch 141, Loss: 0.4846131600652309\n",
      "Epoch 142, Loss: 0.48425966858652364\n",
      "Epoch 143, Loss: 0.4838914149236435\n",
      "Epoch 144, Loss: 0.48352437546282767\n",
      "Epoch 145, Loss: 0.4832299911937133\n",
      "Epoch 146, Loss: 0.4832586529465723\n",
      "Epoch 147, Loss: 0.48356744143113517\n",
      "Epoch 148, Loss: 0.4828108019011922\n",
      "Epoch 149, Loss: 0.4818912939738211\n",
      "Epoch 150, Loss: 0.48239416745575797\n",
      "Epoch 151, Loss: 0.4814952491721961\n",
      "Epoch 152, Loss: 0.4811822818820502\n",
      "Epoch 153, Loss: 0.4812753156716337\n",
      "Epoch 154, Loss: 0.4804387742282827\n",
      "Epoch 155, Loss: 0.48058189052417905\n",
      "Epoch 156, Loss: 0.4801288115497346\n",
      "Epoch 157, Loss: 0.4797465937246525\n",
      "Epoch 158, Loss: 0.4797622479529733\n",
      "Epoch 159, Loss: 0.4791664679844762\n",
      "Epoch 160, Loss: 0.47904359923301915\n",
      "Epoch 161, Loss: 0.4788483533353555\n",
      "Epoch 162, Loss: 0.4783709093142749\n",
      "Epoch 163, Loss: 0.4783027642591491\n",
      "Epoch 164, Loss: 0.4780003946284736\n",
      "Epoch 165, Loss: 0.47766356610156796\n",
      "Epoch 166, Loss: 0.47756940749162413\n",
      "Epoch 167, Loss: 0.4772720526560437\n",
      "Epoch 168, Loss: 0.477001191512605\n",
      "Epoch 169, Loss: 0.47690651155802954\n",
      "Epoch 170, Loss: 0.47663605231839845\n",
      "Epoch 171, Loss: 0.4763715079887003\n",
      "Epoch 172, Loss: 0.4762479807342815\n",
      "Epoch 173, Loss: 0.47603155482909787\n",
      "Epoch 174, Loss: 0.47580085926956445\n",
      "Epoch 175, Loss: 0.4756396378860013\n",
      "Epoch 176, Loss: 0.4754627232666358\n",
      "Epoch 177, Loss: 0.4752513954316692\n",
      "Epoch 178, Loss: 0.47507140350730925\n",
      "Epoch 179, Loss: 0.47492471273973497\n",
      "Epoch 180, Loss: 0.47475493910732763\n",
      "Epoch 181, Loss: 0.47455157018218247\n",
      "Epoch 182, Loss: 0.4743853224635176\n",
      "Epoch 183, Loss: 0.47424550491599726\n",
      "Epoch 184, Loss: 0.47408413178207975\n",
      "Epoch 185, Loss: 0.47390385374022453\n",
      "Epoch 186, Loss: 0.4737383066259127\n",
      "Epoch 187, Loss: 0.47358797785338436\n",
      "Epoch 188, Loss: 0.4734560449826861\n",
      "Epoch 189, Loss: 0.47335738206045025\n",
      "Epoch 190, Loss: 0.4733162834925985\n",
      "Epoch 191, Loss: 0.4732413020737725\n",
      "Epoch 192, Loss: 0.47313239020014447\n",
      "Epoch 193, Loss: 0.4729457158004841\n",
      "Epoch 194, Loss: 0.47274703738102797\n",
      "Epoch 195, Loss: 0.47253750657270627\n",
      "Epoch 196, Loss: 0.4723544299968521\n",
      "Epoch 197, Loss: 0.47221102907746554\n",
      "Epoch 198, Loss: 0.47210844467503205\n",
      "Epoch 199, Loss: 0.47205337980155637\n",
      "Epoch 200, Loss: 0.47204973383515075\n",
      "Epoch 201, Loss: 0.4720427890593303\n",
      "Epoch 202, Loss: 0.4719159657967507\n",
      "Epoch 203, Loss: 0.471735017109987\n",
      "Epoch 204, Loss: 0.4714985533244086\n",
      "Epoch 205, Loss: 0.4713143302002759\n",
      "Epoch 206, Loss: 0.47118574544243713\n",
      "Epoch 207, Loss: 0.47112002449718005\n",
      "Epoch 208, Loss: 0.4710910670518786\n",
      "Epoch 209, Loss: 0.47102495326923743\n",
      "Epoch 210, Loss: 0.4709626586360175\n",
      "Epoch 211, Loss: 0.47085840979895394\n",
      "Epoch 212, Loss: 0.47078064719388485\n",
      "Epoch 213, Loss: 0.47066614789276784\n",
      "Epoch 214, Loss: 0.47055654499036753\n",
      "Epoch 215, Loss: 0.47041652390832933\n",
      "Epoch 216, Loss: 0.4702446473560784\n",
      "Epoch 217, Loss: 0.4701045016774931\n",
      "Epoch 218, Loss: 0.47002059077245106\n",
      "Epoch 219, Loss: 0.4699643617672827\n",
      "Epoch 220, Loss: 0.4698888296176042\n",
      "Epoch 221, Loss: 0.4698141575467262\n",
      "Epoch 222, Loss: 0.4697265783615268\n",
      "Epoch 223, Loss: 0.46966066909468496\n",
      "Epoch 224, Loss: 0.4696348739248034\n",
      "Epoch 225, Loss: 0.4697176006517021\n",
      "Epoch 226, Loss: 0.4699424675720642\n",
      "Epoch 227, Loss: 0.46999269112017344\n",
      "Epoch 228, Loss: 0.4698759384724347\n",
      "Epoch 229, Loss: 0.46938041574678274\n",
      "Epoch 230, Loss: 0.4691415581864629\n",
      "Epoch 231, Loss: 0.46924839544758656\n",
      "Epoch 232, Loss: 0.46933531775018905\n",
      "Epoch 233, Loss: 0.4692253881106229\n",
      "Epoch 234, Loss: 0.46894320638835874\n",
      "Epoch 235, Loss: 0.4688628060980674\n",
      "Epoch 236, Loss: 0.46897798398181567\n",
      "Epoch 237, Loss: 0.4689811295666005\n",
      "Epoch 238, Loss: 0.4688031484354944\n",
      "Epoch 239, Loss: 0.46862983752908405\n",
      "Epoch 240, Loss: 0.4685963696810109\n",
      "Epoch 241, Loss: 0.46867106090894\n",
      "Epoch 242, Loss: 0.46866360503981047\n",
      "Epoch 243, Loss: 0.468513635806399\n",
      "Epoch 244, Loss: 0.46835730315983837\n",
      "Epoch 245, Loss: 0.468344910742699\n",
      "Epoch 246, Loss: 0.46842993299443175\n",
      "Epoch 247, Loss: 0.4683803372309361\n",
      "Epoch 248, Loss: 0.46823408027364943\n",
      "Epoch 249, Loss: 0.46809559282800506\n",
      "Epoch 250, Loss: 0.4681194355765096\n",
      "Epoch 251, Loss: 0.4682059468617626\n",
      "Epoch 252, Loss: 0.46815125054377077\n",
      "Epoch 253, Loss: 0.46800465135626707\n",
      "Epoch 254, Loss: 0.4678516840148294\n",
      "Epoch 255, Loss: 0.46781997608436393\n",
      "Epoch 256, Loss: 0.4678506648079015\n",
      "Epoch 257, Loss: 0.4677995105079106\n",
      "Epoch 258, Loss: 0.4676942061377158\n",
      "Epoch 259, Loss: 0.46760336187164103\n",
      "Epoch 260, Loss: 0.46754277859764004\n",
      "Epoch 261, Loss: 0.4675358146830681\n",
      "Epoch 262, Loss: 0.4675934670336611\n",
      "Epoch 263, Loss: 0.4675883044263754\n",
      "Epoch 264, Loss: 0.46749525618568777\n",
      "Epoch 265, Loss: 0.46735425416294335\n",
      "Epoch 266, Loss: 0.4672686694514329\n",
      "Epoch 267, Loss: 0.46723722493080344\n",
      "Epoch 268, Loss: 0.4672375438513713\n",
      "Epoch 269, Loss: 0.46722038561562945\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.21894781553621995\n",
      "Valence RMSE: 0.20629625618206698\n",
      "Arousal RMSE: 0.23090722496584679\n",
      "Test R^2 score: tensor([0.4842, 0.4721], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.3635, 0.3485], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4781493880585917\n",
      "Num of epochs: 270\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Epoch 121, Loss: 0.49295912913079615\n",
      "Epoch 122, Loss: 0.4924773619139838\n",
      "Epoch 123, Loss: 0.49197607191949805\n",
      "Epoch 124, Loss: 0.4914699197831528\n",
      "Epoch 125, Loss: 0.49091483381312667\n",
      "Epoch 126, Loss: 0.49035147677946317\n",
      "Epoch 127, Loss: 0.4898087222825037\n",
      "Epoch 128, Loss: 0.48924014731550836\n",
      "Epoch 129, Loss: 0.48873550647212044\n",
      "Epoch 130, Loss: 0.48832307254971563\n",
      "Epoch 131, Loss: 0.48791418392821295\n",
      "Epoch 132, Loss: 0.4875902245243796\n",
      "Epoch 133, Loss: 0.48728851108037585\n",
      "Epoch 134, Loss: 0.48697062262181345\n",
      "Epoch 135, Loss: 0.4866339553030548\n",
      "Epoch 136, Loss: 0.48633446746734893\n",
      "Epoch 137, Loss: 0.4860310087358514\n",
      "Epoch 138, Loss: 0.4856786412545615\n",
      "Epoch 139, Loss: 0.48531372107004\n",
      "Epoch 140, Loss: 0.48495712985488587\n",
      "Epoch 141, Loss: 0.4846131600652309\n",
      "Epoch 142, Loss: 0.48425966858652364\n",
      "Epoch 143, Loss: 0.4838914149236435\n",
      "Epoch 144, Loss: 0.48352437546282767\n",
      "Epoch 145, Loss: 0.4832299911937133\n",
      "Epoch 146, Loss: 0.4832586529465723\n",
      "Epoch 147, Loss: 0.48356744143113517\n",
      "Epoch 148, Loss: 0.4828108019011922\n",
      "Epoch 149, Loss: 0.4818912939738211\n",
      "Epoch 150, Loss: 0.48239416745575797\n",
      "Epoch 151, Loss: 0.4814952491721961\n",
      "Epoch 152, Loss: 0.4811822818820502\n",
      "Epoch 153, Loss: 0.4812753156716337\n",
      "Epoch 154, Loss: 0.4804387742282827\n",
      "Epoch 155, Loss: 0.48058189052417905\n",
      "Epoch 156, Loss: 0.4801288115497346\n",
      "Epoch 157, Loss: 0.4797465937246525\n",
      "Epoch 158, Loss: 0.4797622479529733\n",
      "Epoch 159, Loss: 0.4791664679844762\n",
      "Epoch 160, Loss: 0.47904359923301915\n",
      "Epoch 161, Loss: 0.4788483533353555\n",
      "Epoch 162, Loss: 0.4783709093142749\n",
      "Epoch 163, Loss: 0.4783027642591491\n",
      "Epoch 164, Loss: 0.4780003946284736\n",
      "Epoch 165, Loss: 0.47766356610156796\n",
      "Epoch 166, Loss: 0.47756940749162413\n",
      "Epoch 167, Loss: 0.4772720526560437\n",
      "Epoch 168, Loss: 0.477001191512605\n",
      "Epoch 169, Loss: 0.47690651155802954\n",
      "Epoch 170, Loss: 0.47663605231839845\n",
      "Epoch 171, Loss: 0.4763715079887003\n",
      "Epoch 172, Loss: 0.4762479807342815\n",
      "Epoch 173, Loss: 0.47603155482909787\n",
      "Epoch 174, Loss: 0.47580085926956445\n",
      "Epoch 175, Loss: 0.4756396378860013\n",
      "Epoch 176, Loss: 0.4754627232666358\n",
      "Epoch 177, Loss: 0.4752513954316692\n",
      "Epoch 178, Loss: 0.47507140350730925\n",
      "Epoch 179, Loss: 0.47492471273973497\n",
      "Epoch 180, Loss: 0.47475493910732763\n",
      "Epoch 181, Loss: 0.47455157018218247\n",
      "Epoch 182, Loss: 0.4743853224635176\n",
      "Epoch 183, Loss: 0.47424550491599726\n",
      "Epoch 184, Loss: 0.47408413178207975\n",
      "Epoch 185, Loss: 0.47390385374022453\n",
      "Epoch 186, Loss: 0.4737383066259127\n",
      "Epoch 187, Loss: 0.47358797785338436\n",
      "Epoch 188, Loss: 0.4734560449826861\n",
      "Epoch 189, Loss: 0.47335738206045025\n",
      "Epoch 190, Loss: 0.4733162834925985\n",
      "Epoch 191, Loss: 0.4732413020737725\n",
      "Epoch 192, Loss: 0.47313239020014447\n",
      "Epoch 193, Loss: 0.4729457158004841\n",
      "Epoch 194, Loss: 0.47274703738102797\n",
      "Epoch 195, Loss: 0.47253750657270627\n",
      "Epoch 196, Loss: 0.4723544299968521\n",
      "Epoch 197, Loss: 0.47221102907746554\n",
      "Epoch 198, Loss: 0.47210844467503205\n",
      "Epoch 199, Loss: 0.47205337980155637\n",
      "Epoch 200, Loss: 0.47204973383515075\n",
      "Epoch 201, Loss: 0.4720427890593303\n",
      "Epoch 202, Loss: 0.4719159657967507\n",
      "Epoch 203, Loss: 0.471735017109987\n",
      "Epoch 204, Loss: 0.4714985533244086\n",
      "Epoch 205, Loss: 0.4713143302002759\n",
      "Epoch 206, Loss: 0.47118574544243713\n",
      "Epoch 207, Loss: 0.47112002449718005\n",
      "Epoch 208, Loss: 0.4710910670518786\n",
      "Epoch 209, Loss: 0.47102495326923743\n",
      "Epoch 210, Loss: 0.4709626586360175\n",
      "Epoch 211, Loss: 0.47085840979895394\n",
      "Epoch 212, Loss: 0.47078064719388485\n",
      "Epoch 213, Loss: 0.47066614789276784\n",
      "Epoch 214, Loss: 0.47055654499036753\n",
      "Epoch 215, Loss: 0.47041652390832933\n",
      "Epoch 216, Loss: 0.4702446473560784\n",
      "Epoch 217, Loss: 0.4701045016774931\n",
      "Epoch 218, Loss: 0.47002059077245106\n",
      "Epoch 219, Loss: 0.4699643617672827\n",
      "Epoch 220, Loss: 0.4698888296176042\n",
      "Epoch 221, Loss: 0.4698141575467262\n",
      "Epoch 222, Loss: 0.4697265783615268\n",
      "Epoch 223, Loss: 0.46966066909468496\n",
      "Epoch 224, Loss: 0.4696348739248034\n",
      "Epoch 225, Loss: 0.4697176006517021\n",
      "Epoch 226, Loss: 0.4699424675720642\n",
      "Epoch 227, Loss: 0.46999269112017344\n",
      "Epoch 228, Loss: 0.4698759384724347\n",
      "Epoch 229, Loss: 0.46938041574678274\n",
      "Epoch 230, Loss: 0.4691415581864629\n",
      "Epoch 231, Loss: 0.46924839544758656\n",
      "Epoch 232, Loss: 0.46933531775018905\n",
      "Epoch 233, Loss: 0.4692253881106229\n",
      "Epoch 234, Loss: 0.46894320638835874\n",
      "Epoch 235, Loss: 0.4688628060980674\n",
      "Epoch 236, Loss: 0.46897798398181567\n",
      "Epoch 237, Loss: 0.4689811295666005\n",
      "Epoch 238, Loss: 0.4688031484354944\n",
      "Epoch 239, Loss: 0.46862983752908405\n",
      "Epoch 240, Loss: 0.4685963696810109\n",
      "Epoch 241, Loss: 0.46867106090894\n",
      "Epoch 242, Loss: 0.46866360503981047\n",
      "Epoch 243, Loss: 0.468513635806399\n",
      "Epoch 244, Loss: 0.46835730315983837\n",
      "Epoch 245, Loss: 0.468344910742699\n",
      "Epoch 246, Loss: 0.46842993299443175\n",
      "Epoch 247, Loss: 0.4683803372309361\n",
      "Epoch 248, Loss: 0.46823408027364943\n",
      "Epoch 249, Loss: 0.46809559282800506\n",
      "Epoch 250, Loss: 0.4681194355765096\n",
      "Epoch 251, Loss: 0.4682059468617626\n",
      "Epoch 252, Loss: 0.46815125054377077\n",
      "Epoch 253, Loss: 0.46800465135626707\n",
      "Epoch 254, Loss: 0.4678516840148294\n",
      "Epoch 255, Loss: 0.46781997608436393\n",
      "Epoch 256, Loss: 0.4678506648079015\n",
      "Epoch 257, Loss: 0.4677995105079106\n",
      "Epoch 258, Loss: 0.4676942061377158\n",
      "Epoch 259, Loss: 0.46760336187164103\n",
      "Epoch 260, Loss: 0.46754277859764004\n",
      "Epoch 261, Loss: 0.4675358146830681\n",
      "Epoch 262, Loss: 0.4675934670336611\n",
      "Epoch 263, Loss: 0.4675883044263754\n",
      "Epoch 264, Loss: 0.46749525618568777\n",
      "Epoch 265, Loss: 0.46735425416294335\n",
      "Epoch 266, Loss: 0.4672686694514329\n",
      "Epoch 267, Loss: 0.46723722493080344\n",
      "Epoch 268, Loss: 0.4672375438513713\n",
      "Epoch 269, Loss: 0.46722038561562945\n",
      "Epoch 270, Loss: 0.46721867932539873\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.21781971073857329\n",
      "Valence RMSE: 0.2052093243920714\n",
      "Arousal RMSE: 0.22973895175834047\n",
      "Test R^2 score: tensor([0.4897, 0.4774], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.3702, 0.3551], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4835240182054158\n",
      "Num of epochs: 271\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Epoch 121, Loss: 0.49295912913079615\n",
      "Epoch 122, Loss: 0.4924773619139838\n",
      "Epoch 123, Loss: 0.49197607191949805\n",
      "Epoch 124, Loss: 0.4914699197831528\n",
      "Epoch 125, Loss: 0.49091483381312667\n",
      "Epoch 126, Loss: 0.49035147677946317\n",
      "Epoch 127, Loss: 0.4898087222825037\n",
      "Epoch 128, Loss: 0.48924014731550836\n",
      "Epoch 129, Loss: 0.48873550647212044\n",
      "Epoch 130, Loss: 0.48832307254971563\n",
      "Epoch 131, Loss: 0.48791418392821295\n",
      "Epoch 132, Loss: 0.4875902245243796\n",
      "Epoch 133, Loss: 0.48728851108037585\n",
      "Epoch 134, Loss: 0.48697062262181345\n",
      "Epoch 135, Loss: 0.4866339553030548\n",
      "Epoch 136, Loss: 0.48633446746734893\n",
      "Epoch 137, Loss: 0.4860310087358514\n",
      "Epoch 138, Loss: 0.4856786412545615\n",
      "Epoch 139, Loss: 0.48531372107004\n",
      "Epoch 140, Loss: 0.48495712985488587\n",
      "Epoch 141, Loss: 0.4846131600652309\n",
      "Epoch 142, Loss: 0.48425966858652364\n",
      "Epoch 143, Loss: 0.4838914149236435\n",
      "Epoch 144, Loss: 0.48352437546282767\n",
      "Epoch 145, Loss: 0.4832299911937133\n",
      "Epoch 146, Loss: 0.4832586529465723\n",
      "Epoch 147, Loss: 0.48356744143113517\n",
      "Epoch 148, Loss: 0.4828108019011922\n",
      "Epoch 149, Loss: 0.4818912939738211\n",
      "Epoch 150, Loss: 0.48239416745575797\n",
      "Epoch 151, Loss: 0.4814952491721961\n",
      "Epoch 152, Loss: 0.4811822818820502\n",
      "Epoch 153, Loss: 0.4812753156716337\n",
      "Epoch 154, Loss: 0.4804387742282827\n",
      "Epoch 155, Loss: 0.48058189052417905\n",
      "Epoch 156, Loss: 0.4801288115497346\n",
      "Epoch 157, Loss: 0.4797465937246525\n",
      "Epoch 158, Loss: 0.4797622479529733\n",
      "Epoch 159, Loss: 0.4791664679844762\n",
      "Epoch 160, Loss: 0.47904359923301915\n",
      "Epoch 161, Loss: 0.4788483533353555\n",
      "Epoch 162, Loss: 0.4783709093142749\n",
      "Epoch 163, Loss: 0.4783027642591491\n",
      "Epoch 164, Loss: 0.4780003946284736\n",
      "Epoch 165, Loss: 0.47766356610156796\n",
      "Epoch 166, Loss: 0.47756940749162413\n",
      "Epoch 167, Loss: 0.4772720526560437\n",
      "Epoch 168, Loss: 0.477001191512605\n",
      "Epoch 169, Loss: 0.47690651155802954\n",
      "Epoch 170, Loss: 0.47663605231839845\n",
      "Epoch 171, Loss: 0.4763715079887003\n",
      "Epoch 172, Loss: 0.4762479807342815\n",
      "Epoch 173, Loss: 0.47603155482909787\n",
      "Epoch 174, Loss: 0.47580085926956445\n",
      "Epoch 175, Loss: 0.4756396378860013\n",
      "Epoch 176, Loss: 0.4754627232666358\n",
      "Epoch 177, Loss: 0.4752513954316692\n",
      "Epoch 178, Loss: 0.47507140350730925\n",
      "Epoch 179, Loss: 0.47492471273973497\n",
      "Epoch 180, Loss: 0.47475493910732763\n",
      "Epoch 181, Loss: 0.47455157018218247\n",
      "Epoch 182, Loss: 0.4743853224635176\n",
      "Epoch 183, Loss: 0.47424550491599726\n",
      "Epoch 184, Loss: 0.47408413178207975\n",
      "Epoch 185, Loss: 0.47390385374022453\n",
      "Epoch 186, Loss: 0.4737383066259127\n",
      "Epoch 187, Loss: 0.47358797785338436\n",
      "Epoch 188, Loss: 0.4734560449826861\n",
      "Epoch 189, Loss: 0.47335738206045025\n",
      "Epoch 190, Loss: 0.4733162834925985\n",
      "Epoch 191, Loss: 0.4732413020737725\n",
      "Epoch 192, Loss: 0.47313239020014447\n",
      "Epoch 193, Loss: 0.4729457158004841\n",
      "Epoch 194, Loss: 0.47274703738102797\n",
      "Epoch 195, Loss: 0.47253750657270627\n",
      "Epoch 196, Loss: 0.4723544299968521\n",
      "Epoch 197, Loss: 0.47221102907746554\n",
      "Epoch 198, Loss: 0.47210844467503205\n",
      "Epoch 199, Loss: 0.47205337980155637\n",
      "Epoch 200, Loss: 0.47204973383515075\n",
      "Epoch 201, Loss: 0.4720427890593303\n",
      "Epoch 202, Loss: 0.4719159657967507\n",
      "Epoch 203, Loss: 0.471735017109987\n",
      "Epoch 204, Loss: 0.4714985533244086\n",
      "Epoch 205, Loss: 0.4713143302002759\n",
      "Epoch 206, Loss: 0.47118574544243713\n",
      "Epoch 207, Loss: 0.47112002449718005\n",
      "Epoch 208, Loss: 0.4710910670518786\n",
      "Epoch 209, Loss: 0.47102495326923743\n",
      "Epoch 210, Loss: 0.4709626586360175\n",
      "Epoch 211, Loss: 0.47085840979895394\n",
      "Epoch 212, Loss: 0.47078064719388485\n",
      "Epoch 213, Loss: 0.47066614789276784\n",
      "Epoch 214, Loss: 0.47055654499036753\n",
      "Epoch 215, Loss: 0.47041652390832933\n",
      "Epoch 216, Loss: 0.4702446473560784\n",
      "Epoch 217, Loss: 0.4701045016774931\n",
      "Epoch 218, Loss: 0.47002059077245106\n",
      "Epoch 219, Loss: 0.4699643617672827\n",
      "Epoch 220, Loss: 0.4698888296176042\n",
      "Epoch 221, Loss: 0.4698141575467262\n",
      "Epoch 222, Loss: 0.4697265783615268\n",
      "Epoch 223, Loss: 0.46966066909468496\n",
      "Epoch 224, Loss: 0.4696348739248034\n",
      "Epoch 225, Loss: 0.4697176006517021\n",
      "Epoch 226, Loss: 0.4699424675720642\n",
      "Epoch 227, Loss: 0.46999269112017344\n",
      "Epoch 228, Loss: 0.4698759384724347\n",
      "Epoch 229, Loss: 0.46938041574678274\n",
      "Epoch 230, Loss: 0.4691415581864629\n",
      "Epoch 231, Loss: 0.46924839544758656\n",
      "Epoch 232, Loss: 0.46933531775018905\n",
      "Epoch 233, Loss: 0.4692253881106229\n",
      "Epoch 234, Loss: 0.46894320638835874\n",
      "Epoch 235, Loss: 0.4688628060980674\n",
      "Epoch 236, Loss: 0.46897798398181567\n",
      "Epoch 237, Loss: 0.4689811295666005\n",
      "Epoch 238, Loss: 0.4688031484354944\n",
      "Epoch 239, Loss: 0.46862983752908405\n",
      "Epoch 240, Loss: 0.4685963696810109\n",
      "Epoch 241, Loss: 0.46867106090894\n",
      "Epoch 242, Loss: 0.46866360503981047\n",
      "Epoch 243, Loss: 0.468513635806399\n",
      "Epoch 244, Loss: 0.46835730315983837\n",
      "Epoch 245, Loss: 0.468344910742699\n",
      "Epoch 246, Loss: 0.46842993299443175\n",
      "Epoch 247, Loss: 0.4683803372309361\n",
      "Epoch 248, Loss: 0.46823408027364943\n",
      "Epoch 249, Loss: 0.46809559282800506\n",
      "Epoch 250, Loss: 0.4681194355765096\n",
      "Epoch 251, Loss: 0.4682059468617626\n",
      "Epoch 252, Loss: 0.46815125054377077\n",
      "Epoch 253, Loss: 0.46800465135626707\n",
      "Epoch 254, Loss: 0.4678516840148294\n",
      "Epoch 255, Loss: 0.46781997608436393\n",
      "Epoch 256, Loss: 0.4678506648079015\n",
      "Epoch 257, Loss: 0.4677995105079106\n",
      "Epoch 258, Loss: 0.4676942061377158\n",
      "Epoch 259, Loss: 0.46760336187164103\n",
      "Epoch 260, Loss: 0.46754277859764004\n",
      "Epoch 261, Loss: 0.4675358146830681\n",
      "Epoch 262, Loss: 0.4675934670336611\n",
      "Epoch 263, Loss: 0.4675883044263754\n",
      "Epoch 264, Loss: 0.46749525618568777\n",
      "Epoch 265, Loss: 0.46735425416294335\n",
      "Epoch 266, Loss: 0.4672686694514329\n",
      "Epoch 267, Loss: 0.46723722493080344\n",
      "Epoch 268, Loss: 0.4672375438513713\n",
      "Epoch 269, Loss: 0.46722038561562945\n",
      "Epoch 270, Loss: 0.46721867932539873\n",
      "Epoch 271, Loss: 0.46717319721795353\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.21881367505680058\n",
      "Valence RMSE: 0.20619647750350523\n",
      "Arousal RMSE: 0.23074198024822598\n",
      "Test R^2 score: tensor([0.4847, 0.4728], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.3641, 0.3494], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.47877646004693836\n",
      "Num of epochs: 272\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Epoch 121, Loss: 0.49295912913079615\n",
      "Epoch 122, Loss: 0.4924773619139838\n",
      "Epoch 123, Loss: 0.49197607191949805\n",
      "Epoch 124, Loss: 0.4914699197831528\n",
      "Epoch 125, Loss: 0.49091483381312667\n",
      "Epoch 126, Loss: 0.49035147677946317\n",
      "Epoch 127, Loss: 0.4898087222825037\n",
      "Epoch 128, Loss: 0.48924014731550836\n",
      "Epoch 129, Loss: 0.48873550647212044\n",
      "Epoch 130, Loss: 0.48832307254971563\n",
      "Epoch 131, Loss: 0.48791418392821295\n",
      "Epoch 132, Loss: 0.4875902245243796\n",
      "Epoch 133, Loss: 0.48728851108037585\n",
      "Epoch 134, Loss: 0.48697062262181345\n",
      "Epoch 135, Loss: 0.4866339553030548\n",
      "Epoch 136, Loss: 0.48633446746734893\n",
      "Epoch 137, Loss: 0.4860310087358514\n",
      "Epoch 138, Loss: 0.4856786412545615\n",
      "Epoch 139, Loss: 0.48531372107004\n",
      "Epoch 140, Loss: 0.48495712985488587\n",
      "Epoch 141, Loss: 0.4846131600652309\n",
      "Epoch 142, Loss: 0.48425966858652364\n",
      "Epoch 143, Loss: 0.4838914149236435\n",
      "Epoch 144, Loss: 0.48352437546282767\n",
      "Epoch 145, Loss: 0.4832299911937133\n",
      "Epoch 146, Loss: 0.4832586529465723\n",
      "Epoch 147, Loss: 0.48356744143113517\n",
      "Epoch 148, Loss: 0.4828108019011922\n",
      "Epoch 149, Loss: 0.4818912939738211\n",
      "Epoch 150, Loss: 0.48239416745575797\n",
      "Epoch 151, Loss: 0.4814952491721961\n",
      "Epoch 152, Loss: 0.4811822818820502\n",
      "Epoch 153, Loss: 0.4812753156716337\n",
      "Epoch 154, Loss: 0.4804387742282827\n",
      "Epoch 155, Loss: 0.48058189052417905\n",
      "Epoch 156, Loss: 0.4801288115497346\n",
      "Epoch 157, Loss: 0.4797465937246525\n",
      "Epoch 158, Loss: 0.4797622479529733\n",
      "Epoch 159, Loss: 0.4791664679844762\n",
      "Epoch 160, Loss: 0.47904359923301915\n",
      "Epoch 161, Loss: 0.4788483533353555\n",
      "Epoch 162, Loss: 0.4783709093142749\n",
      "Epoch 163, Loss: 0.4783027642591491\n",
      "Epoch 164, Loss: 0.4780003946284736\n",
      "Epoch 165, Loss: 0.47766356610156796\n",
      "Epoch 166, Loss: 0.47756940749162413\n",
      "Epoch 167, Loss: 0.4772720526560437\n",
      "Epoch 168, Loss: 0.477001191512605\n",
      "Epoch 169, Loss: 0.47690651155802954\n",
      "Epoch 170, Loss: 0.47663605231839845\n",
      "Epoch 171, Loss: 0.4763715079887003\n",
      "Epoch 172, Loss: 0.4762479807342815\n",
      "Epoch 173, Loss: 0.47603155482909787\n",
      "Epoch 174, Loss: 0.47580085926956445\n",
      "Epoch 175, Loss: 0.4756396378860013\n",
      "Epoch 176, Loss: 0.4754627232666358\n",
      "Epoch 177, Loss: 0.4752513954316692\n",
      "Epoch 178, Loss: 0.47507140350730925\n",
      "Epoch 179, Loss: 0.47492471273973497\n",
      "Epoch 180, Loss: 0.47475493910732763\n",
      "Epoch 181, Loss: 0.47455157018218247\n",
      "Epoch 182, Loss: 0.4743853224635176\n",
      "Epoch 183, Loss: 0.47424550491599726\n",
      "Epoch 184, Loss: 0.47408413178207975\n",
      "Epoch 185, Loss: 0.47390385374022453\n",
      "Epoch 186, Loss: 0.4737383066259127\n",
      "Epoch 187, Loss: 0.47358797785338436\n",
      "Epoch 188, Loss: 0.4734560449826861\n",
      "Epoch 189, Loss: 0.47335738206045025\n",
      "Epoch 190, Loss: 0.4733162834925985\n",
      "Epoch 191, Loss: 0.4732413020737725\n",
      "Epoch 192, Loss: 0.47313239020014447\n",
      "Epoch 193, Loss: 0.4729457158004841\n",
      "Epoch 194, Loss: 0.47274703738102797\n",
      "Epoch 195, Loss: 0.47253750657270627\n",
      "Epoch 196, Loss: 0.4723544299968521\n",
      "Epoch 197, Loss: 0.47221102907746554\n",
      "Epoch 198, Loss: 0.47210844467503205\n",
      "Epoch 199, Loss: 0.47205337980155637\n",
      "Epoch 200, Loss: 0.47204973383515075\n",
      "Epoch 201, Loss: 0.4720427890593303\n",
      "Epoch 202, Loss: 0.4719159657967507\n",
      "Epoch 203, Loss: 0.471735017109987\n",
      "Epoch 204, Loss: 0.4714985533244086\n",
      "Epoch 205, Loss: 0.4713143302002759\n",
      "Epoch 206, Loss: 0.47118574544243713\n",
      "Epoch 207, Loss: 0.47112002449718005\n",
      "Epoch 208, Loss: 0.4710910670518786\n",
      "Epoch 209, Loss: 0.47102495326923743\n",
      "Epoch 210, Loss: 0.4709626586360175\n",
      "Epoch 211, Loss: 0.47085840979895394\n",
      "Epoch 212, Loss: 0.47078064719388485\n",
      "Epoch 213, Loss: 0.47066614789276784\n",
      "Epoch 214, Loss: 0.47055654499036753\n",
      "Epoch 215, Loss: 0.47041652390832933\n",
      "Epoch 216, Loss: 0.4702446473560784\n",
      "Epoch 217, Loss: 0.4701045016774931\n",
      "Epoch 218, Loss: 0.47002059077245106\n",
      "Epoch 219, Loss: 0.4699643617672827\n",
      "Epoch 220, Loss: 0.4698888296176042\n",
      "Epoch 221, Loss: 0.4698141575467262\n",
      "Epoch 222, Loss: 0.4697265783615268\n",
      "Epoch 223, Loss: 0.46966066909468496\n",
      "Epoch 224, Loss: 0.4696348739248034\n",
      "Epoch 225, Loss: 0.4697176006517021\n",
      "Epoch 226, Loss: 0.4699424675720642\n",
      "Epoch 227, Loss: 0.46999269112017344\n",
      "Epoch 228, Loss: 0.4698759384724347\n",
      "Epoch 229, Loss: 0.46938041574678274\n",
      "Epoch 230, Loss: 0.4691415581864629\n",
      "Epoch 231, Loss: 0.46924839544758656\n",
      "Epoch 232, Loss: 0.46933531775018905\n",
      "Epoch 233, Loss: 0.4692253881106229\n",
      "Epoch 234, Loss: 0.46894320638835874\n",
      "Epoch 235, Loss: 0.4688628060980674\n",
      "Epoch 236, Loss: 0.46897798398181567\n",
      "Epoch 237, Loss: 0.4689811295666005\n",
      "Epoch 238, Loss: 0.4688031484354944\n",
      "Epoch 239, Loss: 0.46862983752908405\n",
      "Epoch 240, Loss: 0.4685963696810109\n",
      "Epoch 241, Loss: 0.46867106090894\n",
      "Epoch 242, Loss: 0.46866360503981047\n",
      "Epoch 243, Loss: 0.468513635806399\n",
      "Epoch 244, Loss: 0.46835730315983837\n",
      "Epoch 245, Loss: 0.468344910742699\n",
      "Epoch 246, Loss: 0.46842993299443175\n",
      "Epoch 247, Loss: 0.4683803372309361\n",
      "Epoch 248, Loss: 0.46823408027364943\n",
      "Epoch 249, Loss: 0.46809559282800506\n",
      "Epoch 250, Loss: 0.4681194355765096\n",
      "Epoch 251, Loss: 0.4682059468617626\n",
      "Epoch 252, Loss: 0.46815125054377077\n",
      "Epoch 253, Loss: 0.46800465135626707\n",
      "Epoch 254, Loss: 0.4678516840148294\n",
      "Epoch 255, Loss: 0.46781997608436393\n",
      "Epoch 256, Loss: 0.4678506648079015\n",
      "Epoch 257, Loss: 0.4677995105079106\n",
      "Epoch 258, Loss: 0.4676942061377158\n",
      "Epoch 259, Loss: 0.46760336187164103\n",
      "Epoch 260, Loss: 0.46754277859764004\n",
      "Epoch 261, Loss: 0.4675358146830681\n",
      "Epoch 262, Loss: 0.4675934670336611\n",
      "Epoch 263, Loss: 0.4675883044263754\n",
      "Epoch 264, Loss: 0.46749525618568777\n",
      "Epoch 265, Loss: 0.46735425416294335\n",
      "Epoch 266, Loss: 0.4672686694514329\n",
      "Epoch 267, Loss: 0.46723722493080344\n",
      "Epoch 268, Loss: 0.4672375438513713\n",
      "Epoch 269, Loss: 0.46722038561562945\n",
      "Epoch 270, Loss: 0.46721867932539873\n",
      "Epoch 271, Loss: 0.46717319721795353\n",
      "Epoch 272, Loss: 0.4671238189214812\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2177914395283195\n",
      "Valence RMSE: 0.2052267220902912\n",
      "Arousal RMSE: 0.229669795148839\n",
      "Test R^2 score: tensor([0.4896, 0.4777], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.3701, 0.3555], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4836380433847466\n",
      "Num of epochs: 273\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Epoch 121, Loss: 0.49295912913079615\n",
      "Epoch 122, Loss: 0.4924773619139838\n",
      "Epoch 123, Loss: 0.49197607191949805\n",
      "Epoch 124, Loss: 0.4914699197831528\n",
      "Epoch 125, Loss: 0.49091483381312667\n",
      "Epoch 126, Loss: 0.49035147677946317\n",
      "Epoch 127, Loss: 0.4898087222825037\n",
      "Epoch 128, Loss: 0.48924014731550836\n",
      "Epoch 129, Loss: 0.48873550647212044\n",
      "Epoch 130, Loss: 0.48832307254971563\n",
      "Epoch 131, Loss: 0.48791418392821295\n",
      "Epoch 132, Loss: 0.4875902245243796\n",
      "Epoch 133, Loss: 0.48728851108037585\n",
      "Epoch 134, Loss: 0.48697062262181345\n",
      "Epoch 135, Loss: 0.4866339553030548\n",
      "Epoch 136, Loss: 0.48633446746734893\n",
      "Epoch 137, Loss: 0.4860310087358514\n",
      "Epoch 138, Loss: 0.4856786412545615\n",
      "Epoch 139, Loss: 0.48531372107004\n",
      "Epoch 140, Loss: 0.48495712985488587\n",
      "Epoch 141, Loss: 0.4846131600652309\n",
      "Epoch 142, Loss: 0.48425966858652364\n",
      "Epoch 143, Loss: 0.4838914149236435\n",
      "Epoch 144, Loss: 0.48352437546282767\n",
      "Epoch 145, Loss: 0.4832299911937133\n",
      "Epoch 146, Loss: 0.4832586529465723\n",
      "Epoch 147, Loss: 0.48356744143113517\n",
      "Epoch 148, Loss: 0.4828108019011922\n",
      "Epoch 149, Loss: 0.4818912939738211\n",
      "Epoch 150, Loss: 0.48239416745575797\n",
      "Epoch 151, Loss: 0.4814952491721961\n",
      "Epoch 152, Loss: 0.4811822818820502\n",
      "Epoch 153, Loss: 0.4812753156716337\n",
      "Epoch 154, Loss: 0.4804387742282827\n",
      "Epoch 155, Loss: 0.48058189052417905\n",
      "Epoch 156, Loss: 0.4801288115497346\n",
      "Epoch 157, Loss: 0.4797465937246525\n",
      "Epoch 158, Loss: 0.4797622479529733\n",
      "Epoch 159, Loss: 0.4791664679844762\n",
      "Epoch 160, Loss: 0.47904359923301915\n",
      "Epoch 161, Loss: 0.4788483533353555\n",
      "Epoch 162, Loss: 0.4783709093142749\n",
      "Epoch 163, Loss: 0.4783027642591491\n",
      "Epoch 164, Loss: 0.4780003946284736\n",
      "Epoch 165, Loss: 0.47766356610156796\n",
      "Epoch 166, Loss: 0.47756940749162413\n",
      "Epoch 167, Loss: 0.4772720526560437\n",
      "Epoch 168, Loss: 0.477001191512605\n",
      "Epoch 169, Loss: 0.47690651155802954\n",
      "Epoch 170, Loss: 0.47663605231839845\n",
      "Epoch 171, Loss: 0.4763715079887003\n",
      "Epoch 172, Loss: 0.4762479807342815\n",
      "Epoch 173, Loss: 0.47603155482909787\n",
      "Epoch 174, Loss: 0.47580085926956445\n",
      "Epoch 175, Loss: 0.4756396378860013\n",
      "Epoch 176, Loss: 0.4754627232666358\n",
      "Epoch 177, Loss: 0.4752513954316692\n",
      "Epoch 178, Loss: 0.47507140350730925\n",
      "Epoch 179, Loss: 0.47492471273973497\n",
      "Epoch 180, Loss: 0.47475493910732763\n",
      "Epoch 181, Loss: 0.47455157018218247\n",
      "Epoch 182, Loss: 0.4743853224635176\n",
      "Epoch 183, Loss: 0.47424550491599726\n",
      "Epoch 184, Loss: 0.47408413178207975\n",
      "Epoch 185, Loss: 0.47390385374022453\n",
      "Epoch 186, Loss: 0.4737383066259127\n",
      "Epoch 187, Loss: 0.47358797785338436\n",
      "Epoch 188, Loss: 0.4734560449826861\n",
      "Epoch 189, Loss: 0.47335738206045025\n",
      "Epoch 190, Loss: 0.4733162834925985\n",
      "Epoch 191, Loss: 0.4732413020737725\n",
      "Epoch 192, Loss: 0.47313239020014447\n",
      "Epoch 193, Loss: 0.4729457158004841\n",
      "Epoch 194, Loss: 0.47274703738102797\n",
      "Epoch 195, Loss: 0.47253750657270627\n",
      "Epoch 196, Loss: 0.4723544299968521\n",
      "Epoch 197, Loss: 0.47221102907746554\n",
      "Epoch 198, Loss: 0.47210844467503205\n",
      "Epoch 199, Loss: 0.47205337980155637\n",
      "Epoch 200, Loss: 0.47204973383515075\n",
      "Epoch 201, Loss: 0.4720427890593303\n",
      "Epoch 202, Loss: 0.4719159657967507\n",
      "Epoch 203, Loss: 0.471735017109987\n",
      "Epoch 204, Loss: 0.4714985533244086\n",
      "Epoch 205, Loss: 0.4713143302002759\n",
      "Epoch 206, Loss: 0.47118574544243713\n",
      "Epoch 207, Loss: 0.47112002449718005\n",
      "Epoch 208, Loss: 0.4710910670518786\n",
      "Epoch 209, Loss: 0.47102495326923743\n",
      "Epoch 210, Loss: 0.4709626586360175\n",
      "Epoch 211, Loss: 0.47085840979895394\n",
      "Epoch 212, Loss: 0.47078064719388485\n",
      "Epoch 213, Loss: 0.47066614789276784\n",
      "Epoch 214, Loss: 0.47055654499036753\n",
      "Epoch 215, Loss: 0.47041652390832933\n",
      "Epoch 216, Loss: 0.4702446473560784\n",
      "Epoch 217, Loss: 0.4701045016774931\n",
      "Epoch 218, Loss: 0.47002059077245106\n",
      "Epoch 219, Loss: 0.4699643617672827\n",
      "Epoch 220, Loss: 0.4698888296176042\n",
      "Epoch 221, Loss: 0.4698141575467262\n",
      "Epoch 222, Loss: 0.4697265783615268\n",
      "Epoch 223, Loss: 0.46966066909468496\n",
      "Epoch 224, Loss: 0.4696348739248034\n",
      "Epoch 225, Loss: 0.4697176006517021\n",
      "Epoch 226, Loss: 0.4699424675720642\n",
      "Epoch 227, Loss: 0.46999269112017344\n",
      "Epoch 228, Loss: 0.4698759384724347\n",
      "Epoch 229, Loss: 0.46938041574678274\n",
      "Epoch 230, Loss: 0.4691415581864629\n",
      "Epoch 231, Loss: 0.46924839544758656\n",
      "Epoch 232, Loss: 0.46933531775018905\n",
      "Epoch 233, Loss: 0.4692253881106229\n",
      "Epoch 234, Loss: 0.46894320638835874\n",
      "Epoch 235, Loss: 0.4688628060980674\n",
      "Epoch 236, Loss: 0.46897798398181567\n",
      "Epoch 237, Loss: 0.4689811295666005\n",
      "Epoch 238, Loss: 0.4688031484354944\n",
      "Epoch 239, Loss: 0.46862983752908405\n",
      "Epoch 240, Loss: 0.4685963696810109\n",
      "Epoch 241, Loss: 0.46867106090894\n",
      "Epoch 242, Loss: 0.46866360503981047\n",
      "Epoch 243, Loss: 0.468513635806399\n",
      "Epoch 244, Loss: 0.46835730315983837\n",
      "Epoch 245, Loss: 0.468344910742699\n",
      "Epoch 246, Loss: 0.46842993299443175\n",
      "Epoch 247, Loss: 0.4683803372309361\n",
      "Epoch 248, Loss: 0.46823408027364943\n",
      "Epoch 249, Loss: 0.46809559282800506\n",
      "Epoch 250, Loss: 0.4681194355765096\n",
      "Epoch 251, Loss: 0.4682059468617626\n",
      "Epoch 252, Loss: 0.46815125054377077\n",
      "Epoch 253, Loss: 0.46800465135626707\n",
      "Epoch 254, Loss: 0.4678516840148294\n",
      "Epoch 255, Loss: 0.46781997608436393\n",
      "Epoch 256, Loss: 0.4678506648079015\n",
      "Epoch 257, Loss: 0.4677995105079106\n",
      "Epoch 258, Loss: 0.4676942061377158\n",
      "Epoch 259, Loss: 0.46760336187164103\n",
      "Epoch 260, Loss: 0.46754277859764004\n",
      "Epoch 261, Loss: 0.4675358146830681\n",
      "Epoch 262, Loss: 0.4675934670336611\n",
      "Epoch 263, Loss: 0.4675883044263754\n",
      "Epoch 264, Loss: 0.46749525618568777\n",
      "Epoch 265, Loss: 0.46735425416294335\n",
      "Epoch 266, Loss: 0.4672686694514329\n",
      "Epoch 267, Loss: 0.46723722493080344\n",
      "Epoch 268, Loss: 0.4672375438513713\n",
      "Epoch 269, Loss: 0.46722038561562945\n",
      "Epoch 270, Loss: 0.46721867932539873\n",
      "Epoch 271, Loss: 0.46717319721795353\n",
      "Epoch 272, Loss: 0.4671238189214812\n",
      "Epoch 273, Loss: 0.46705709570299314\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2185496097697394\n",
      "Valence RMSE: 0.20595380160162724\n",
      "Arousal RMSE: 0.23045801237285785\n",
      "Test R^2 score: tensor([0.4859, 0.4741], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.3656, 0.3510], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.48003091624753724\n",
      "Num of epochs: 274\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Epoch 121, Loss: 0.49295912913079615\n",
      "Epoch 122, Loss: 0.4924773619139838\n",
      "Epoch 123, Loss: 0.49197607191949805\n",
      "Epoch 124, Loss: 0.4914699197831528\n",
      "Epoch 125, Loss: 0.49091483381312667\n",
      "Epoch 126, Loss: 0.49035147677946317\n",
      "Epoch 127, Loss: 0.4898087222825037\n",
      "Epoch 128, Loss: 0.48924014731550836\n",
      "Epoch 129, Loss: 0.48873550647212044\n",
      "Epoch 130, Loss: 0.48832307254971563\n",
      "Epoch 131, Loss: 0.48791418392821295\n",
      "Epoch 132, Loss: 0.4875902245243796\n",
      "Epoch 133, Loss: 0.48728851108037585\n",
      "Epoch 134, Loss: 0.48697062262181345\n",
      "Epoch 135, Loss: 0.4866339553030548\n",
      "Epoch 136, Loss: 0.48633446746734893\n",
      "Epoch 137, Loss: 0.4860310087358514\n",
      "Epoch 138, Loss: 0.4856786412545615\n",
      "Epoch 139, Loss: 0.48531372107004\n",
      "Epoch 140, Loss: 0.48495712985488587\n",
      "Epoch 141, Loss: 0.4846131600652309\n",
      "Epoch 142, Loss: 0.48425966858652364\n",
      "Epoch 143, Loss: 0.4838914149236435\n",
      "Epoch 144, Loss: 0.48352437546282767\n",
      "Epoch 145, Loss: 0.4832299911937133\n",
      "Epoch 146, Loss: 0.4832586529465723\n",
      "Epoch 147, Loss: 0.48356744143113517\n",
      "Epoch 148, Loss: 0.4828108019011922\n",
      "Epoch 149, Loss: 0.4818912939738211\n",
      "Epoch 150, Loss: 0.48239416745575797\n",
      "Epoch 151, Loss: 0.4814952491721961\n",
      "Epoch 152, Loss: 0.4811822818820502\n",
      "Epoch 153, Loss: 0.4812753156716337\n",
      "Epoch 154, Loss: 0.4804387742282827\n",
      "Epoch 155, Loss: 0.48058189052417905\n",
      "Epoch 156, Loss: 0.4801288115497346\n",
      "Epoch 157, Loss: 0.4797465937246525\n",
      "Epoch 158, Loss: 0.4797622479529733\n",
      "Epoch 159, Loss: 0.4791664679844762\n",
      "Epoch 160, Loss: 0.47904359923301915\n",
      "Epoch 161, Loss: 0.4788483533353555\n",
      "Epoch 162, Loss: 0.4783709093142749\n",
      "Epoch 163, Loss: 0.4783027642591491\n",
      "Epoch 164, Loss: 0.4780003946284736\n",
      "Epoch 165, Loss: 0.47766356610156796\n",
      "Epoch 166, Loss: 0.47756940749162413\n",
      "Epoch 167, Loss: 0.4772720526560437\n",
      "Epoch 168, Loss: 0.477001191512605\n",
      "Epoch 169, Loss: 0.47690651155802954\n",
      "Epoch 170, Loss: 0.47663605231839845\n",
      "Epoch 171, Loss: 0.4763715079887003\n",
      "Epoch 172, Loss: 0.4762479807342815\n",
      "Epoch 173, Loss: 0.47603155482909787\n",
      "Epoch 174, Loss: 0.47580085926956445\n",
      "Epoch 175, Loss: 0.4756396378860013\n",
      "Epoch 176, Loss: 0.4754627232666358\n",
      "Epoch 177, Loss: 0.4752513954316692\n",
      "Epoch 178, Loss: 0.47507140350730925\n",
      "Epoch 179, Loss: 0.47492471273973497\n",
      "Epoch 180, Loss: 0.47475493910732763\n",
      "Epoch 181, Loss: 0.47455157018218247\n",
      "Epoch 182, Loss: 0.4743853224635176\n",
      "Epoch 183, Loss: 0.47424550491599726\n",
      "Epoch 184, Loss: 0.47408413178207975\n",
      "Epoch 185, Loss: 0.47390385374022453\n",
      "Epoch 186, Loss: 0.4737383066259127\n",
      "Epoch 187, Loss: 0.47358797785338436\n",
      "Epoch 188, Loss: 0.4734560449826861\n",
      "Epoch 189, Loss: 0.47335738206045025\n",
      "Epoch 190, Loss: 0.4733162834925985\n",
      "Epoch 191, Loss: 0.4732413020737725\n",
      "Epoch 192, Loss: 0.47313239020014447\n",
      "Epoch 193, Loss: 0.4729457158004841\n",
      "Epoch 194, Loss: 0.47274703738102797\n",
      "Epoch 195, Loss: 0.47253750657270627\n",
      "Epoch 196, Loss: 0.4723544299968521\n",
      "Epoch 197, Loss: 0.47221102907746554\n",
      "Epoch 198, Loss: 0.47210844467503205\n",
      "Epoch 199, Loss: 0.47205337980155637\n",
      "Epoch 200, Loss: 0.47204973383515075\n",
      "Epoch 201, Loss: 0.4720427890593303\n",
      "Epoch 202, Loss: 0.4719159657967507\n",
      "Epoch 203, Loss: 0.471735017109987\n",
      "Epoch 204, Loss: 0.4714985533244086\n",
      "Epoch 205, Loss: 0.4713143302002759\n",
      "Epoch 206, Loss: 0.47118574544243713\n",
      "Epoch 207, Loss: 0.47112002449718005\n",
      "Epoch 208, Loss: 0.4710910670518786\n",
      "Epoch 209, Loss: 0.47102495326923743\n",
      "Epoch 210, Loss: 0.4709626586360175\n",
      "Epoch 211, Loss: 0.47085840979895394\n",
      "Epoch 212, Loss: 0.47078064719388485\n",
      "Epoch 213, Loss: 0.47066614789276784\n",
      "Epoch 214, Loss: 0.47055654499036753\n",
      "Epoch 215, Loss: 0.47041652390832933\n",
      "Epoch 216, Loss: 0.4702446473560784\n",
      "Epoch 217, Loss: 0.4701045016774931\n",
      "Epoch 218, Loss: 0.47002059077245106\n",
      "Epoch 219, Loss: 0.4699643617672827\n",
      "Epoch 220, Loss: 0.4698888296176042\n",
      "Epoch 221, Loss: 0.4698141575467262\n",
      "Epoch 222, Loss: 0.4697265783615268\n",
      "Epoch 223, Loss: 0.46966066909468496\n",
      "Epoch 224, Loss: 0.4696348739248034\n",
      "Epoch 225, Loss: 0.4697176006517021\n",
      "Epoch 226, Loss: 0.4699424675720642\n",
      "Epoch 227, Loss: 0.46999269112017344\n",
      "Epoch 228, Loss: 0.4698759384724347\n",
      "Epoch 229, Loss: 0.46938041574678274\n",
      "Epoch 230, Loss: 0.4691415581864629\n",
      "Epoch 231, Loss: 0.46924839544758656\n",
      "Epoch 232, Loss: 0.46933531775018905\n",
      "Epoch 233, Loss: 0.4692253881106229\n",
      "Epoch 234, Loss: 0.46894320638835874\n",
      "Epoch 235, Loss: 0.4688628060980674\n",
      "Epoch 236, Loss: 0.46897798398181567\n",
      "Epoch 237, Loss: 0.4689811295666005\n",
      "Epoch 238, Loss: 0.4688031484354944\n",
      "Epoch 239, Loss: 0.46862983752908405\n",
      "Epoch 240, Loss: 0.4685963696810109\n",
      "Epoch 241, Loss: 0.46867106090894\n",
      "Epoch 242, Loss: 0.46866360503981047\n",
      "Epoch 243, Loss: 0.468513635806399\n",
      "Epoch 244, Loss: 0.46835730315983837\n",
      "Epoch 245, Loss: 0.468344910742699\n",
      "Epoch 246, Loss: 0.46842993299443175\n",
      "Epoch 247, Loss: 0.4683803372309361\n",
      "Epoch 248, Loss: 0.46823408027364943\n",
      "Epoch 249, Loss: 0.46809559282800506\n",
      "Epoch 250, Loss: 0.4681194355765096\n",
      "Epoch 251, Loss: 0.4682059468617626\n",
      "Epoch 252, Loss: 0.46815125054377077\n",
      "Epoch 253, Loss: 0.46800465135626707\n",
      "Epoch 254, Loss: 0.4678516840148294\n",
      "Epoch 255, Loss: 0.46781997608436393\n",
      "Epoch 256, Loss: 0.4678506648079015\n",
      "Epoch 257, Loss: 0.4677995105079106\n",
      "Epoch 258, Loss: 0.4676942061377158\n",
      "Epoch 259, Loss: 0.46760336187164103\n",
      "Epoch 260, Loss: 0.46754277859764004\n",
      "Epoch 261, Loss: 0.4675358146830681\n",
      "Epoch 262, Loss: 0.4675934670336611\n",
      "Epoch 263, Loss: 0.4675883044263754\n",
      "Epoch 264, Loss: 0.46749525618568777\n",
      "Epoch 265, Loss: 0.46735425416294335\n",
      "Epoch 266, Loss: 0.4672686694514329\n",
      "Epoch 267, Loss: 0.46723722493080344\n",
      "Epoch 268, Loss: 0.4672375438513713\n",
      "Epoch 269, Loss: 0.46722038561562945\n",
      "Epoch 270, Loss: 0.46721867932539873\n",
      "Epoch 271, Loss: 0.46717319721795353\n",
      "Epoch 272, Loss: 0.4671238189214812\n",
      "Epoch 273, Loss: 0.46705709570299314\n",
      "Epoch 274, Loss: 0.4669974785875546\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.21783180704011562\n",
      "Valence RMSE: 0.20527580678925236\n",
      "Arousal RMSE: 0.22970249337729412\n",
      "Test R^2 score: tensor([0.4893, 0.4776], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.3698, 0.3553], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4834415832924012\n",
      "Num of epochs: 275\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Epoch 121, Loss: 0.49295912913079615\n",
      "Epoch 122, Loss: 0.4924773619139838\n",
      "Epoch 123, Loss: 0.49197607191949805\n",
      "Epoch 124, Loss: 0.4914699197831528\n",
      "Epoch 125, Loss: 0.49091483381312667\n",
      "Epoch 126, Loss: 0.49035147677946317\n",
      "Epoch 127, Loss: 0.4898087222825037\n",
      "Epoch 128, Loss: 0.48924014731550836\n",
      "Epoch 129, Loss: 0.48873550647212044\n",
      "Epoch 130, Loss: 0.48832307254971563\n",
      "Epoch 131, Loss: 0.48791418392821295\n",
      "Epoch 132, Loss: 0.4875902245243796\n",
      "Epoch 133, Loss: 0.48728851108037585\n",
      "Epoch 134, Loss: 0.48697062262181345\n",
      "Epoch 135, Loss: 0.4866339553030548\n",
      "Epoch 136, Loss: 0.48633446746734893\n",
      "Epoch 137, Loss: 0.4860310087358514\n",
      "Epoch 138, Loss: 0.4856786412545615\n",
      "Epoch 139, Loss: 0.48531372107004\n",
      "Epoch 140, Loss: 0.48495712985488587\n",
      "Epoch 141, Loss: 0.4846131600652309\n",
      "Epoch 142, Loss: 0.48425966858652364\n",
      "Epoch 143, Loss: 0.4838914149236435\n",
      "Epoch 144, Loss: 0.48352437546282767\n",
      "Epoch 145, Loss: 0.4832299911937133\n",
      "Epoch 146, Loss: 0.4832586529465723\n",
      "Epoch 147, Loss: 0.48356744143113517\n",
      "Epoch 148, Loss: 0.4828108019011922\n",
      "Epoch 149, Loss: 0.4818912939738211\n",
      "Epoch 150, Loss: 0.48239416745575797\n",
      "Epoch 151, Loss: 0.4814952491721961\n",
      "Epoch 152, Loss: 0.4811822818820502\n",
      "Epoch 153, Loss: 0.4812753156716337\n",
      "Epoch 154, Loss: 0.4804387742282827\n",
      "Epoch 155, Loss: 0.48058189052417905\n",
      "Epoch 156, Loss: 0.4801288115497346\n",
      "Epoch 157, Loss: 0.4797465937246525\n",
      "Epoch 158, Loss: 0.4797622479529733\n",
      "Epoch 159, Loss: 0.4791664679844762\n",
      "Epoch 160, Loss: 0.47904359923301915\n",
      "Epoch 161, Loss: 0.4788483533353555\n",
      "Epoch 162, Loss: 0.4783709093142749\n",
      "Epoch 163, Loss: 0.4783027642591491\n",
      "Epoch 164, Loss: 0.4780003946284736\n",
      "Epoch 165, Loss: 0.47766356610156796\n",
      "Epoch 166, Loss: 0.47756940749162413\n",
      "Epoch 167, Loss: 0.4772720526560437\n",
      "Epoch 168, Loss: 0.477001191512605\n",
      "Epoch 169, Loss: 0.47690651155802954\n",
      "Epoch 170, Loss: 0.47663605231839845\n",
      "Epoch 171, Loss: 0.4763715079887003\n",
      "Epoch 172, Loss: 0.4762479807342815\n",
      "Epoch 173, Loss: 0.47603155482909787\n",
      "Epoch 174, Loss: 0.47580085926956445\n",
      "Epoch 175, Loss: 0.4756396378860013\n",
      "Epoch 176, Loss: 0.4754627232666358\n",
      "Epoch 177, Loss: 0.4752513954316692\n",
      "Epoch 178, Loss: 0.47507140350730925\n",
      "Epoch 179, Loss: 0.47492471273973497\n",
      "Epoch 180, Loss: 0.47475493910732763\n",
      "Epoch 181, Loss: 0.47455157018218247\n",
      "Epoch 182, Loss: 0.4743853224635176\n",
      "Epoch 183, Loss: 0.47424550491599726\n",
      "Epoch 184, Loss: 0.47408413178207975\n",
      "Epoch 185, Loss: 0.47390385374022453\n",
      "Epoch 186, Loss: 0.4737383066259127\n",
      "Epoch 187, Loss: 0.47358797785338436\n",
      "Epoch 188, Loss: 0.4734560449826861\n",
      "Epoch 189, Loss: 0.47335738206045025\n",
      "Epoch 190, Loss: 0.4733162834925985\n",
      "Epoch 191, Loss: 0.4732413020737725\n",
      "Epoch 192, Loss: 0.47313239020014447\n",
      "Epoch 193, Loss: 0.4729457158004841\n",
      "Epoch 194, Loss: 0.47274703738102797\n",
      "Epoch 195, Loss: 0.47253750657270627\n",
      "Epoch 196, Loss: 0.4723544299968521\n",
      "Epoch 197, Loss: 0.47221102907746554\n",
      "Epoch 198, Loss: 0.47210844467503205\n",
      "Epoch 199, Loss: 0.47205337980155637\n",
      "Epoch 200, Loss: 0.47204973383515075\n",
      "Epoch 201, Loss: 0.4720427890593303\n",
      "Epoch 202, Loss: 0.4719159657967507\n",
      "Epoch 203, Loss: 0.471735017109987\n",
      "Epoch 204, Loss: 0.4714985533244086\n",
      "Epoch 205, Loss: 0.4713143302002759\n",
      "Epoch 206, Loss: 0.47118574544243713\n",
      "Epoch 207, Loss: 0.47112002449718005\n",
      "Epoch 208, Loss: 0.4710910670518786\n",
      "Epoch 209, Loss: 0.47102495326923743\n",
      "Epoch 210, Loss: 0.4709626586360175\n",
      "Epoch 211, Loss: 0.47085840979895394\n",
      "Epoch 212, Loss: 0.47078064719388485\n",
      "Epoch 213, Loss: 0.47066614789276784\n",
      "Epoch 214, Loss: 0.47055654499036753\n",
      "Epoch 215, Loss: 0.47041652390832933\n",
      "Epoch 216, Loss: 0.4702446473560784\n",
      "Epoch 217, Loss: 0.4701045016774931\n",
      "Epoch 218, Loss: 0.47002059077245106\n",
      "Epoch 219, Loss: 0.4699643617672827\n",
      "Epoch 220, Loss: 0.4698888296176042\n",
      "Epoch 221, Loss: 0.4698141575467262\n",
      "Epoch 222, Loss: 0.4697265783615268\n",
      "Epoch 223, Loss: 0.46966066909468496\n",
      "Epoch 224, Loss: 0.4696348739248034\n",
      "Epoch 225, Loss: 0.4697176006517021\n",
      "Epoch 226, Loss: 0.4699424675720642\n",
      "Epoch 227, Loss: 0.46999269112017344\n",
      "Epoch 228, Loss: 0.4698759384724347\n",
      "Epoch 229, Loss: 0.46938041574678274\n",
      "Epoch 230, Loss: 0.4691415581864629\n",
      "Epoch 231, Loss: 0.46924839544758656\n",
      "Epoch 232, Loss: 0.46933531775018905\n",
      "Epoch 233, Loss: 0.4692253881106229\n",
      "Epoch 234, Loss: 0.46894320638835874\n",
      "Epoch 235, Loss: 0.4688628060980674\n",
      "Epoch 236, Loss: 0.46897798398181567\n",
      "Epoch 237, Loss: 0.4689811295666005\n",
      "Epoch 238, Loss: 0.4688031484354944\n",
      "Epoch 239, Loss: 0.46862983752908405\n",
      "Epoch 240, Loss: 0.4685963696810109\n",
      "Epoch 241, Loss: 0.46867106090894\n",
      "Epoch 242, Loss: 0.46866360503981047\n",
      "Epoch 243, Loss: 0.468513635806399\n",
      "Epoch 244, Loss: 0.46835730315983837\n",
      "Epoch 245, Loss: 0.468344910742699\n",
      "Epoch 246, Loss: 0.46842993299443175\n",
      "Epoch 247, Loss: 0.4683803372309361\n",
      "Epoch 248, Loss: 0.46823408027364943\n",
      "Epoch 249, Loss: 0.46809559282800506\n",
      "Epoch 250, Loss: 0.4681194355765096\n",
      "Epoch 251, Loss: 0.4682059468617626\n",
      "Epoch 252, Loss: 0.46815125054377077\n",
      "Epoch 253, Loss: 0.46800465135626707\n",
      "Epoch 254, Loss: 0.4678516840148294\n",
      "Epoch 255, Loss: 0.46781997608436393\n",
      "Epoch 256, Loss: 0.4678506648079015\n",
      "Epoch 257, Loss: 0.4677995105079106\n",
      "Epoch 258, Loss: 0.4676942061377158\n",
      "Epoch 259, Loss: 0.46760336187164103\n",
      "Epoch 260, Loss: 0.46754277859764004\n",
      "Epoch 261, Loss: 0.4675358146830681\n",
      "Epoch 262, Loss: 0.4675934670336611\n",
      "Epoch 263, Loss: 0.4675883044263754\n",
      "Epoch 264, Loss: 0.46749525618568777\n",
      "Epoch 265, Loss: 0.46735425416294335\n",
      "Epoch 266, Loss: 0.4672686694514329\n",
      "Epoch 267, Loss: 0.46723722493080344\n",
      "Epoch 268, Loss: 0.4672375438513713\n",
      "Epoch 269, Loss: 0.46722038561562945\n",
      "Epoch 270, Loss: 0.46721867932539873\n",
      "Epoch 271, Loss: 0.46717319721795353\n",
      "Epoch 272, Loss: 0.4671238189214812\n",
      "Epoch 273, Loss: 0.46705709570299314\n",
      "Epoch 274, Loss: 0.4669974785875546\n",
      "Epoch 275, Loss: 0.4669422896788895\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.21836797266970065\n",
      "Valence RMSE: 0.20577951552028179\n",
      "Arousal RMSE: 0.23026926405403816\n",
      "Test R^2 score: tensor([0.4868, 0.4750], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.3667, 0.3521], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4808962745776839\n",
      "Num of epochs: 276\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Epoch 121, Loss: 0.49295912913079615\n",
      "Epoch 122, Loss: 0.4924773619139838\n",
      "Epoch 123, Loss: 0.49197607191949805\n",
      "Epoch 124, Loss: 0.4914699197831528\n",
      "Epoch 125, Loss: 0.49091483381312667\n",
      "Epoch 126, Loss: 0.49035147677946317\n",
      "Epoch 127, Loss: 0.4898087222825037\n",
      "Epoch 128, Loss: 0.48924014731550836\n",
      "Epoch 129, Loss: 0.48873550647212044\n",
      "Epoch 130, Loss: 0.48832307254971563\n",
      "Epoch 131, Loss: 0.48791418392821295\n",
      "Epoch 132, Loss: 0.4875902245243796\n",
      "Epoch 133, Loss: 0.48728851108037585\n",
      "Epoch 134, Loss: 0.48697062262181345\n",
      "Epoch 135, Loss: 0.4866339553030548\n",
      "Epoch 136, Loss: 0.48633446746734893\n",
      "Epoch 137, Loss: 0.4860310087358514\n",
      "Epoch 138, Loss: 0.4856786412545615\n",
      "Epoch 139, Loss: 0.48531372107004\n",
      "Epoch 140, Loss: 0.48495712985488587\n",
      "Epoch 141, Loss: 0.4846131600652309\n",
      "Epoch 142, Loss: 0.48425966858652364\n",
      "Epoch 143, Loss: 0.4838914149236435\n",
      "Epoch 144, Loss: 0.48352437546282767\n",
      "Epoch 145, Loss: 0.4832299911937133\n",
      "Epoch 146, Loss: 0.4832586529465723\n",
      "Epoch 147, Loss: 0.48356744143113517\n",
      "Epoch 148, Loss: 0.4828108019011922\n",
      "Epoch 149, Loss: 0.4818912939738211\n",
      "Epoch 150, Loss: 0.48239416745575797\n",
      "Epoch 151, Loss: 0.4814952491721961\n",
      "Epoch 152, Loss: 0.4811822818820502\n",
      "Epoch 153, Loss: 0.4812753156716337\n",
      "Epoch 154, Loss: 0.4804387742282827\n",
      "Epoch 155, Loss: 0.48058189052417905\n",
      "Epoch 156, Loss: 0.4801288115497346\n",
      "Epoch 157, Loss: 0.4797465937246525\n",
      "Epoch 158, Loss: 0.4797622479529733\n",
      "Epoch 159, Loss: 0.4791664679844762\n",
      "Epoch 160, Loss: 0.47904359923301915\n",
      "Epoch 161, Loss: 0.4788483533353555\n",
      "Epoch 162, Loss: 0.4783709093142749\n",
      "Epoch 163, Loss: 0.4783027642591491\n",
      "Epoch 164, Loss: 0.4780003946284736\n",
      "Epoch 165, Loss: 0.47766356610156796\n",
      "Epoch 166, Loss: 0.47756940749162413\n",
      "Epoch 167, Loss: 0.4772720526560437\n",
      "Epoch 168, Loss: 0.477001191512605\n",
      "Epoch 169, Loss: 0.47690651155802954\n",
      "Epoch 170, Loss: 0.47663605231839845\n",
      "Epoch 171, Loss: 0.4763715079887003\n",
      "Epoch 172, Loss: 0.4762479807342815\n",
      "Epoch 173, Loss: 0.47603155482909787\n",
      "Epoch 174, Loss: 0.47580085926956445\n",
      "Epoch 175, Loss: 0.4756396378860013\n",
      "Epoch 176, Loss: 0.4754627232666358\n",
      "Epoch 177, Loss: 0.4752513954316692\n",
      "Epoch 178, Loss: 0.47507140350730925\n",
      "Epoch 179, Loss: 0.47492471273973497\n",
      "Epoch 180, Loss: 0.47475493910732763\n",
      "Epoch 181, Loss: 0.47455157018218247\n",
      "Epoch 182, Loss: 0.4743853224635176\n",
      "Epoch 183, Loss: 0.47424550491599726\n",
      "Epoch 184, Loss: 0.47408413178207975\n",
      "Epoch 185, Loss: 0.47390385374022453\n",
      "Epoch 186, Loss: 0.4737383066259127\n",
      "Epoch 187, Loss: 0.47358797785338436\n",
      "Epoch 188, Loss: 0.4734560449826861\n",
      "Epoch 189, Loss: 0.47335738206045025\n",
      "Epoch 190, Loss: 0.4733162834925985\n",
      "Epoch 191, Loss: 0.4732413020737725\n",
      "Epoch 192, Loss: 0.47313239020014447\n",
      "Epoch 193, Loss: 0.4729457158004841\n",
      "Epoch 194, Loss: 0.47274703738102797\n",
      "Epoch 195, Loss: 0.47253750657270627\n",
      "Epoch 196, Loss: 0.4723544299968521\n",
      "Epoch 197, Loss: 0.47221102907746554\n",
      "Epoch 198, Loss: 0.47210844467503205\n",
      "Epoch 199, Loss: 0.47205337980155637\n",
      "Epoch 200, Loss: 0.47204973383515075\n",
      "Epoch 201, Loss: 0.4720427890593303\n",
      "Epoch 202, Loss: 0.4719159657967507\n",
      "Epoch 203, Loss: 0.471735017109987\n",
      "Epoch 204, Loss: 0.4714985533244086\n",
      "Epoch 205, Loss: 0.4713143302002759\n",
      "Epoch 206, Loss: 0.47118574544243713\n",
      "Epoch 207, Loss: 0.47112002449718005\n",
      "Epoch 208, Loss: 0.4710910670518786\n",
      "Epoch 209, Loss: 0.47102495326923743\n",
      "Epoch 210, Loss: 0.4709626586360175\n",
      "Epoch 211, Loss: 0.47085840979895394\n",
      "Epoch 212, Loss: 0.47078064719388485\n",
      "Epoch 213, Loss: 0.47066614789276784\n",
      "Epoch 214, Loss: 0.47055654499036753\n",
      "Epoch 215, Loss: 0.47041652390832933\n",
      "Epoch 216, Loss: 0.4702446473560784\n",
      "Epoch 217, Loss: 0.4701045016774931\n",
      "Epoch 218, Loss: 0.47002059077245106\n",
      "Epoch 219, Loss: 0.4699643617672827\n",
      "Epoch 220, Loss: 0.4698888296176042\n",
      "Epoch 221, Loss: 0.4698141575467262\n",
      "Epoch 222, Loss: 0.4697265783615268\n",
      "Epoch 223, Loss: 0.46966066909468496\n",
      "Epoch 224, Loss: 0.4696348739248034\n",
      "Epoch 225, Loss: 0.4697176006517021\n",
      "Epoch 226, Loss: 0.4699424675720642\n",
      "Epoch 227, Loss: 0.46999269112017344\n",
      "Epoch 228, Loss: 0.4698759384724347\n",
      "Epoch 229, Loss: 0.46938041574678274\n",
      "Epoch 230, Loss: 0.4691415581864629\n",
      "Epoch 231, Loss: 0.46924839544758656\n",
      "Epoch 232, Loss: 0.46933531775018905\n",
      "Epoch 233, Loss: 0.4692253881106229\n",
      "Epoch 234, Loss: 0.46894320638835874\n",
      "Epoch 235, Loss: 0.4688628060980674\n",
      "Epoch 236, Loss: 0.46897798398181567\n",
      "Epoch 237, Loss: 0.4689811295666005\n",
      "Epoch 238, Loss: 0.4688031484354944\n",
      "Epoch 239, Loss: 0.46862983752908405\n",
      "Epoch 240, Loss: 0.4685963696810109\n",
      "Epoch 241, Loss: 0.46867106090894\n",
      "Epoch 242, Loss: 0.46866360503981047\n",
      "Epoch 243, Loss: 0.468513635806399\n",
      "Epoch 244, Loss: 0.46835730315983837\n",
      "Epoch 245, Loss: 0.468344910742699\n",
      "Epoch 246, Loss: 0.46842993299443175\n",
      "Epoch 247, Loss: 0.4683803372309361\n",
      "Epoch 248, Loss: 0.46823408027364943\n",
      "Epoch 249, Loss: 0.46809559282800506\n",
      "Epoch 250, Loss: 0.4681194355765096\n",
      "Epoch 251, Loss: 0.4682059468617626\n",
      "Epoch 252, Loss: 0.46815125054377077\n",
      "Epoch 253, Loss: 0.46800465135626707\n",
      "Epoch 254, Loss: 0.4678516840148294\n",
      "Epoch 255, Loss: 0.46781997608436393\n",
      "Epoch 256, Loss: 0.4678506648079015\n",
      "Epoch 257, Loss: 0.4677995105079106\n",
      "Epoch 258, Loss: 0.4676942061377158\n",
      "Epoch 259, Loss: 0.46760336187164103\n",
      "Epoch 260, Loss: 0.46754277859764004\n",
      "Epoch 261, Loss: 0.4675358146830681\n",
      "Epoch 262, Loss: 0.4675934670336611\n",
      "Epoch 263, Loss: 0.4675883044263754\n",
      "Epoch 264, Loss: 0.46749525618568777\n",
      "Epoch 265, Loss: 0.46735425416294335\n",
      "Epoch 266, Loss: 0.4672686694514329\n",
      "Epoch 267, Loss: 0.46723722493080344\n",
      "Epoch 268, Loss: 0.4672375438513713\n",
      "Epoch 269, Loss: 0.46722038561562945\n",
      "Epoch 270, Loss: 0.46721867932539873\n",
      "Epoch 271, Loss: 0.46717319721795353\n",
      "Epoch 272, Loss: 0.4671238189214812\n",
      "Epoch 273, Loss: 0.46705709570299314\n",
      "Epoch 274, Loss: 0.4669974785875546\n",
      "Epoch 275, Loss: 0.4669422896788895\n",
      "Epoch 276, Loss: 0.4669034987743203\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.21783561381512442\n",
      "Valence RMSE: 0.20529245829429782\n",
      "Arousal RMSE: 0.22969483202699195\n",
      "Test R^2 score: tensor([0.4892, 0.4776], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.3697, 0.3553], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4834175820479159\n",
      "Num of epochs: 277\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Epoch 121, Loss: 0.49295912913079615\n",
      "Epoch 122, Loss: 0.4924773619139838\n",
      "Epoch 123, Loss: 0.49197607191949805\n",
      "Epoch 124, Loss: 0.4914699197831528\n",
      "Epoch 125, Loss: 0.49091483381312667\n",
      "Epoch 126, Loss: 0.49035147677946317\n",
      "Epoch 127, Loss: 0.4898087222825037\n",
      "Epoch 128, Loss: 0.48924014731550836\n",
      "Epoch 129, Loss: 0.48873550647212044\n",
      "Epoch 130, Loss: 0.48832307254971563\n",
      "Epoch 131, Loss: 0.48791418392821295\n",
      "Epoch 132, Loss: 0.4875902245243796\n",
      "Epoch 133, Loss: 0.48728851108037585\n",
      "Epoch 134, Loss: 0.48697062262181345\n",
      "Epoch 135, Loss: 0.4866339553030548\n",
      "Epoch 136, Loss: 0.48633446746734893\n",
      "Epoch 137, Loss: 0.4860310087358514\n",
      "Epoch 138, Loss: 0.4856786412545615\n",
      "Epoch 139, Loss: 0.48531372107004\n",
      "Epoch 140, Loss: 0.48495712985488587\n",
      "Epoch 141, Loss: 0.4846131600652309\n",
      "Epoch 142, Loss: 0.48425966858652364\n",
      "Epoch 143, Loss: 0.4838914149236435\n",
      "Epoch 144, Loss: 0.48352437546282767\n",
      "Epoch 145, Loss: 0.4832299911937133\n",
      "Epoch 146, Loss: 0.4832586529465723\n",
      "Epoch 147, Loss: 0.48356744143113517\n",
      "Epoch 148, Loss: 0.4828108019011922\n",
      "Epoch 149, Loss: 0.4818912939738211\n",
      "Epoch 150, Loss: 0.48239416745575797\n",
      "Epoch 151, Loss: 0.4814952491721961\n",
      "Epoch 152, Loss: 0.4811822818820502\n",
      "Epoch 153, Loss: 0.4812753156716337\n",
      "Epoch 154, Loss: 0.4804387742282827\n",
      "Epoch 155, Loss: 0.48058189052417905\n",
      "Epoch 156, Loss: 0.4801288115497346\n",
      "Epoch 157, Loss: 0.4797465937246525\n",
      "Epoch 158, Loss: 0.4797622479529733\n",
      "Epoch 159, Loss: 0.4791664679844762\n",
      "Epoch 160, Loss: 0.47904359923301915\n",
      "Epoch 161, Loss: 0.4788483533353555\n",
      "Epoch 162, Loss: 0.4783709093142749\n",
      "Epoch 163, Loss: 0.4783027642591491\n",
      "Epoch 164, Loss: 0.4780003946284736\n",
      "Epoch 165, Loss: 0.47766356610156796\n",
      "Epoch 166, Loss: 0.47756940749162413\n",
      "Epoch 167, Loss: 0.4772720526560437\n",
      "Epoch 168, Loss: 0.477001191512605\n",
      "Epoch 169, Loss: 0.47690651155802954\n",
      "Epoch 170, Loss: 0.47663605231839845\n",
      "Epoch 171, Loss: 0.4763715079887003\n",
      "Epoch 172, Loss: 0.4762479807342815\n",
      "Epoch 173, Loss: 0.47603155482909787\n",
      "Epoch 174, Loss: 0.47580085926956445\n",
      "Epoch 175, Loss: 0.4756396378860013\n",
      "Epoch 176, Loss: 0.4754627232666358\n",
      "Epoch 177, Loss: 0.4752513954316692\n",
      "Epoch 178, Loss: 0.47507140350730925\n",
      "Epoch 179, Loss: 0.47492471273973497\n",
      "Epoch 180, Loss: 0.47475493910732763\n",
      "Epoch 181, Loss: 0.47455157018218247\n",
      "Epoch 182, Loss: 0.4743853224635176\n",
      "Epoch 183, Loss: 0.47424550491599726\n",
      "Epoch 184, Loss: 0.47408413178207975\n",
      "Epoch 185, Loss: 0.47390385374022453\n",
      "Epoch 186, Loss: 0.4737383066259127\n",
      "Epoch 187, Loss: 0.47358797785338436\n",
      "Epoch 188, Loss: 0.4734560449826861\n",
      "Epoch 189, Loss: 0.47335738206045025\n",
      "Epoch 190, Loss: 0.4733162834925985\n",
      "Epoch 191, Loss: 0.4732413020737725\n",
      "Epoch 192, Loss: 0.47313239020014447\n",
      "Epoch 193, Loss: 0.4729457158004841\n",
      "Epoch 194, Loss: 0.47274703738102797\n",
      "Epoch 195, Loss: 0.47253750657270627\n",
      "Epoch 196, Loss: 0.4723544299968521\n",
      "Epoch 197, Loss: 0.47221102907746554\n",
      "Epoch 198, Loss: 0.47210844467503205\n",
      "Epoch 199, Loss: 0.47205337980155637\n",
      "Epoch 200, Loss: 0.47204973383515075\n",
      "Epoch 201, Loss: 0.4720427890593303\n",
      "Epoch 202, Loss: 0.4719159657967507\n",
      "Epoch 203, Loss: 0.471735017109987\n",
      "Epoch 204, Loss: 0.4714985533244086\n",
      "Epoch 205, Loss: 0.4713143302002759\n",
      "Epoch 206, Loss: 0.47118574544243713\n",
      "Epoch 207, Loss: 0.47112002449718005\n",
      "Epoch 208, Loss: 0.4710910670518786\n",
      "Epoch 209, Loss: 0.47102495326923743\n",
      "Epoch 210, Loss: 0.4709626586360175\n",
      "Epoch 211, Loss: 0.47085840979895394\n",
      "Epoch 212, Loss: 0.47078064719388485\n",
      "Epoch 213, Loss: 0.47066614789276784\n",
      "Epoch 214, Loss: 0.47055654499036753\n",
      "Epoch 215, Loss: 0.47041652390832933\n",
      "Epoch 216, Loss: 0.4702446473560784\n",
      "Epoch 217, Loss: 0.4701045016774931\n",
      "Epoch 218, Loss: 0.47002059077245106\n",
      "Epoch 219, Loss: 0.4699643617672827\n",
      "Epoch 220, Loss: 0.4698888296176042\n",
      "Epoch 221, Loss: 0.4698141575467262\n",
      "Epoch 222, Loss: 0.4697265783615268\n",
      "Epoch 223, Loss: 0.46966066909468496\n",
      "Epoch 224, Loss: 0.4696348739248034\n",
      "Epoch 225, Loss: 0.4697176006517021\n",
      "Epoch 226, Loss: 0.4699424675720642\n",
      "Epoch 227, Loss: 0.46999269112017344\n",
      "Epoch 228, Loss: 0.4698759384724347\n",
      "Epoch 229, Loss: 0.46938041574678274\n",
      "Epoch 230, Loss: 0.4691415581864629\n",
      "Epoch 231, Loss: 0.46924839544758656\n",
      "Epoch 232, Loss: 0.46933531775018905\n",
      "Epoch 233, Loss: 0.4692253881106229\n",
      "Epoch 234, Loss: 0.46894320638835874\n",
      "Epoch 235, Loss: 0.4688628060980674\n",
      "Epoch 236, Loss: 0.46897798398181567\n",
      "Epoch 237, Loss: 0.4689811295666005\n",
      "Epoch 238, Loss: 0.4688031484354944\n",
      "Epoch 239, Loss: 0.46862983752908405\n",
      "Epoch 240, Loss: 0.4685963696810109\n",
      "Epoch 241, Loss: 0.46867106090894\n",
      "Epoch 242, Loss: 0.46866360503981047\n",
      "Epoch 243, Loss: 0.468513635806399\n",
      "Epoch 244, Loss: 0.46835730315983837\n",
      "Epoch 245, Loss: 0.468344910742699\n",
      "Epoch 246, Loss: 0.46842993299443175\n",
      "Epoch 247, Loss: 0.4683803372309361\n",
      "Epoch 248, Loss: 0.46823408027364943\n",
      "Epoch 249, Loss: 0.46809559282800506\n",
      "Epoch 250, Loss: 0.4681194355765096\n",
      "Epoch 251, Loss: 0.4682059468617626\n",
      "Epoch 252, Loss: 0.46815125054377077\n",
      "Epoch 253, Loss: 0.46800465135626707\n",
      "Epoch 254, Loss: 0.4678516840148294\n",
      "Epoch 255, Loss: 0.46781997608436393\n",
      "Epoch 256, Loss: 0.4678506648079015\n",
      "Epoch 257, Loss: 0.4677995105079106\n",
      "Epoch 258, Loss: 0.4676942061377158\n",
      "Epoch 259, Loss: 0.46760336187164103\n",
      "Epoch 260, Loss: 0.46754277859764004\n",
      "Epoch 261, Loss: 0.4675358146830681\n",
      "Epoch 262, Loss: 0.4675934670336611\n",
      "Epoch 263, Loss: 0.4675883044263754\n",
      "Epoch 264, Loss: 0.46749525618568777\n",
      "Epoch 265, Loss: 0.46735425416294335\n",
      "Epoch 266, Loss: 0.4672686694514329\n",
      "Epoch 267, Loss: 0.46723722493080344\n",
      "Epoch 268, Loss: 0.4672375438513713\n",
      "Epoch 269, Loss: 0.46722038561562945\n",
      "Epoch 270, Loss: 0.46721867932539873\n",
      "Epoch 271, Loss: 0.46717319721795353\n",
      "Epoch 272, Loss: 0.4671238189214812\n",
      "Epoch 273, Loss: 0.46705709570299314\n",
      "Epoch 274, Loss: 0.4669974785875546\n",
      "Epoch 275, Loss: 0.4669422896788895\n",
      "Epoch 276, Loss: 0.4669034987743203\n",
      "Epoch 277, Loss: 0.46684804340658115\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.21813905724277693\n",
      "Valence RMSE: 0.20557651210157293\n",
      "Arousal RMSE: 0.23001650867206658\n",
      "Test R^2 score: tensor([0.4878, 0.4761], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.3680, 0.3535], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.48197826241537767\n",
      "Num of epochs: 278\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Epoch 121, Loss: 0.49295912913079615\n",
      "Epoch 122, Loss: 0.4924773619139838\n",
      "Epoch 123, Loss: 0.49197607191949805\n",
      "Epoch 124, Loss: 0.4914699197831528\n",
      "Epoch 125, Loss: 0.49091483381312667\n",
      "Epoch 126, Loss: 0.49035147677946317\n",
      "Epoch 127, Loss: 0.4898087222825037\n",
      "Epoch 128, Loss: 0.48924014731550836\n",
      "Epoch 129, Loss: 0.48873550647212044\n",
      "Epoch 130, Loss: 0.48832307254971563\n",
      "Epoch 131, Loss: 0.48791418392821295\n",
      "Epoch 132, Loss: 0.4875902245243796\n",
      "Epoch 133, Loss: 0.48728851108037585\n",
      "Epoch 134, Loss: 0.48697062262181345\n",
      "Epoch 135, Loss: 0.4866339553030548\n",
      "Epoch 136, Loss: 0.48633446746734893\n",
      "Epoch 137, Loss: 0.4860310087358514\n",
      "Epoch 138, Loss: 0.4856786412545615\n",
      "Epoch 139, Loss: 0.48531372107004\n",
      "Epoch 140, Loss: 0.48495712985488587\n",
      "Epoch 141, Loss: 0.4846131600652309\n",
      "Epoch 142, Loss: 0.48425966858652364\n",
      "Epoch 143, Loss: 0.4838914149236435\n",
      "Epoch 144, Loss: 0.48352437546282767\n",
      "Epoch 145, Loss: 0.4832299911937133\n",
      "Epoch 146, Loss: 0.4832586529465723\n",
      "Epoch 147, Loss: 0.48356744143113517\n",
      "Epoch 148, Loss: 0.4828108019011922\n",
      "Epoch 149, Loss: 0.4818912939738211\n",
      "Epoch 150, Loss: 0.48239416745575797\n",
      "Epoch 151, Loss: 0.4814952491721961\n",
      "Epoch 152, Loss: 0.4811822818820502\n",
      "Epoch 153, Loss: 0.4812753156716337\n",
      "Epoch 154, Loss: 0.4804387742282827\n",
      "Epoch 155, Loss: 0.48058189052417905\n",
      "Epoch 156, Loss: 0.4801288115497346\n",
      "Epoch 157, Loss: 0.4797465937246525\n",
      "Epoch 158, Loss: 0.4797622479529733\n",
      "Epoch 159, Loss: 0.4791664679844762\n",
      "Epoch 160, Loss: 0.47904359923301915\n",
      "Epoch 161, Loss: 0.4788483533353555\n",
      "Epoch 162, Loss: 0.4783709093142749\n",
      "Epoch 163, Loss: 0.4783027642591491\n",
      "Epoch 164, Loss: 0.4780003946284736\n",
      "Epoch 165, Loss: 0.47766356610156796\n",
      "Epoch 166, Loss: 0.47756940749162413\n",
      "Epoch 167, Loss: 0.4772720526560437\n",
      "Epoch 168, Loss: 0.477001191512605\n",
      "Epoch 169, Loss: 0.47690651155802954\n",
      "Epoch 170, Loss: 0.47663605231839845\n",
      "Epoch 171, Loss: 0.4763715079887003\n",
      "Epoch 172, Loss: 0.4762479807342815\n",
      "Epoch 173, Loss: 0.47603155482909787\n",
      "Epoch 174, Loss: 0.47580085926956445\n",
      "Epoch 175, Loss: 0.4756396378860013\n",
      "Epoch 176, Loss: 0.4754627232666358\n",
      "Epoch 177, Loss: 0.4752513954316692\n",
      "Epoch 178, Loss: 0.47507140350730925\n",
      "Epoch 179, Loss: 0.47492471273973497\n",
      "Epoch 180, Loss: 0.47475493910732763\n",
      "Epoch 181, Loss: 0.47455157018218247\n",
      "Epoch 182, Loss: 0.4743853224635176\n",
      "Epoch 183, Loss: 0.47424550491599726\n",
      "Epoch 184, Loss: 0.47408413178207975\n",
      "Epoch 185, Loss: 0.47390385374022453\n",
      "Epoch 186, Loss: 0.4737383066259127\n",
      "Epoch 187, Loss: 0.47358797785338436\n",
      "Epoch 188, Loss: 0.4734560449826861\n",
      "Epoch 189, Loss: 0.47335738206045025\n",
      "Epoch 190, Loss: 0.4733162834925985\n",
      "Epoch 191, Loss: 0.4732413020737725\n",
      "Epoch 192, Loss: 0.47313239020014447\n",
      "Epoch 193, Loss: 0.4729457158004841\n",
      "Epoch 194, Loss: 0.47274703738102797\n",
      "Epoch 195, Loss: 0.47253750657270627\n",
      "Epoch 196, Loss: 0.4723544299968521\n",
      "Epoch 197, Loss: 0.47221102907746554\n",
      "Epoch 198, Loss: 0.47210844467503205\n",
      "Epoch 199, Loss: 0.47205337980155637\n",
      "Epoch 200, Loss: 0.47204973383515075\n",
      "Epoch 201, Loss: 0.4720427890593303\n",
      "Epoch 202, Loss: 0.4719159657967507\n",
      "Epoch 203, Loss: 0.471735017109987\n",
      "Epoch 204, Loss: 0.4714985533244086\n",
      "Epoch 205, Loss: 0.4713143302002759\n",
      "Epoch 206, Loss: 0.47118574544243713\n",
      "Epoch 207, Loss: 0.47112002449718005\n",
      "Epoch 208, Loss: 0.4710910670518786\n",
      "Epoch 209, Loss: 0.47102495326923743\n",
      "Epoch 210, Loss: 0.4709626586360175\n",
      "Epoch 211, Loss: 0.47085840979895394\n",
      "Epoch 212, Loss: 0.47078064719388485\n",
      "Epoch 213, Loss: 0.47066614789276784\n",
      "Epoch 214, Loss: 0.47055654499036753\n",
      "Epoch 215, Loss: 0.47041652390832933\n",
      "Epoch 216, Loss: 0.4702446473560784\n",
      "Epoch 217, Loss: 0.4701045016774931\n",
      "Epoch 218, Loss: 0.47002059077245106\n",
      "Epoch 219, Loss: 0.4699643617672827\n",
      "Epoch 220, Loss: 0.4698888296176042\n",
      "Epoch 221, Loss: 0.4698141575467262\n",
      "Epoch 222, Loss: 0.4697265783615268\n",
      "Epoch 223, Loss: 0.46966066909468496\n",
      "Epoch 224, Loss: 0.4696348739248034\n",
      "Epoch 225, Loss: 0.4697176006517021\n",
      "Epoch 226, Loss: 0.4699424675720642\n",
      "Epoch 227, Loss: 0.46999269112017344\n",
      "Epoch 228, Loss: 0.4698759384724347\n",
      "Epoch 229, Loss: 0.46938041574678274\n",
      "Epoch 230, Loss: 0.4691415581864629\n",
      "Epoch 231, Loss: 0.46924839544758656\n",
      "Epoch 232, Loss: 0.46933531775018905\n",
      "Epoch 233, Loss: 0.4692253881106229\n",
      "Epoch 234, Loss: 0.46894320638835874\n",
      "Epoch 235, Loss: 0.4688628060980674\n",
      "Epoch 236, Loss: 0.46897798398181567\n",
      "Epoch 237, Loss: 0.4689811295666005\n",
      "Epoch 238, Loss: 0.4688031484354944\n",
      "Epoch 239, Loss: 0.46862983752908405\n",
      "Epoch 240, Loss: 0.4685963696810109\n",
      "Epoch 241, Loss: 0.46867106090894\n",
      "Epoch 242, Loss: 0.46866360503981047\n",
      "Epoch 243, Loss: 0.468513635806399\n",
      "Epoch 244, Loss: 0.46835730315983837\n",
      "Epoch 245, Loss: 0.468344910742699\n",
      "Epoch 246, Loss: 0.46842993299443175\n",
      "Epoch 247, Loss: 0.4683803372309361\n",
      "Epoch 248, Loss: 0.46823408027364943\n",
      "Epoch 249, Loss: 0.46809559282800506\n",
      "Epoch 250, Loss: 0.4681194355765096\n",
      "Epoch 251, Loss: 0.4682059468617626\n",
      "Epoch 252, Loss: 0.46815125054377077\n",
      "Epoch 253, Loss: 0.46800465135626707\n",
      "Epoch 254, Loss: 0.4678516840148294\n",
      "Epoch 255, Loss: 0.46781997608436393\n",
      "Epoch 256, Loss: 0.4678506648079015\n",
      "Epoch 257, Loss: 0.4677995105079106\n",
      "Epoch 258, Loss: 0.4676942061377158\n",
      "Epoch 259, Loss: 0.46760336187164103\n",
      "Epoch 260, Loss: 0.46754277859764004\n",
      "Epoch 261, Loss: 0.4675358146830681\n",
      "Epoch 262, Loss: 0.4675934670336611\n",
      "Epoch 263, Loss: 0.4675883044263754\n",
      "Epoch 264, Loss: 0.46749525618568777\n",
      "Epoch 265, Loss: 0.46735425416294335\n",
      "Epoch 266, Loss: 0.4672686694514329\n",
      "Epoch 267, Loss: 0.46723722493080344\n",
      "Epoch 268, Loss: 0.4672375438513713\n",
      "Epoch 269, Loss: 0.46722038561562945\n",
      "Epoch 270, Loss: 0.46721867932539873\n",
      "Epoch 271, Loss: 0.46717319721795353\n",
      "Epoch 272, Loss: 0.4671238189214812\n",
      "Epoch 273, Loss: 0.46705709570299314\n",
      "Epoch 274, Loss: 0.4669974785875546\n",
      "Epoch 275, Loss: 0.4669422896788895\n",
      "Epoch 276, Loss: 0.4669034987743203\n",
      "Epoch 277, Loss: 0.46684804340658115\n",
      "Epoch 278, Loss: 0.4668104417178138\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2178077670831\n",
      "Valence RMSE: 0.2052880043689999\n",
      "Arousal RMSE: 0.22964599292311935\n",
      "Test R^2 score: tensor([0.4893, 0.4778], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.3697, 0.3556], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4835397291468809\n",
      "Num of epochs: 279\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Epoch 121, Loss: 0.49295912913079615\n",
      "Epoch 122, Loss: 0.4924773619139838\n",
      "Epoch 123, Loss: 0.49197607191949805\n",
      "Epoch 124, Loss: 0.4914699197831528\n",
      "Epoch 125, Loss: 0.49091483381312667\n",
      "Epoch 126, Loss: 0.49035147677946317\n",
      "Epoch 127, Loss: 0.4898087222825037\n",
      "Epoch 128, Loss: 0.48924014731550836\n",
      "Epoch 129, Loss: 0.48873550647212044\n",
      "Epoch 130, Loss: 0.48832307254971563\n",
      "Epoch 131, Loss: 0.48791418392821295\n",
      "Epoch 132, Loss: 0.4875902245243796\n",
      "Epoch 133, Loss: 0.48728851108037585\n",
      "Epoch 134, Loss: 0.48697062262181345\n",
      "Epoch 135, Loss: 0.4866339553030548\n",
      "Epoch 136, Loss: 0.48633446746734893\n",
      "Epoch 137, Loss: 0.4860310087358514\n",
      "Epoch 138, Loss: 0.4856786412545615\n",
      "Epoch 139, Loss: 0.48531372107004\n",
      "Epoch 140, Loss: 0.48495712985488587\n",
      "Epoch 141, Loss: 0.4846131600652309\n",
      "Epoch 142, Loss: 0.48425966858652364\n",
      "Epoch 143, Loss: 0.4838914149236435\n",
      "Epoch 144, Loss: 0.48352437546282767\n",
      "Epoch 145, Loss: 0.4832299911937133\n",
      "Epoch 146, Loss: 0.4832586529465723\n",
      "Epoch 147, Loss: 0.48356744143113517\n",
      "Epoch 148, Loss: 0.4828108019011922\n",
      "Epoch 149, Loss: 0.4818912939738211\n",
      "Epoch 150, Loss: 0.48239416745575797\n",
      "Epoch 151, Loss: 0.4814952491721961\n",
      "Epoch 152, Loss: 0.4811822818820502\n",
      "Epoch 153, Loss: 0.4812753156716337\n",
      "Epoch 154, Loss: 0.4804387742282827\n",
      "Epoch 155, Loss: 0.48058189052417905\n",
      "Epoch 156, Loss: 0.4801288115497346\n",
      "Epoch 157, Loss: 0.4797465937246525\n",
      "Epoch 158, Loss: 0.4797622479529733\n",
      "Epoch 159, Loss: 0.4791664679844762\n",
      "Epoch 160, Loss: 0.47904359923301915\n",
      "Epoch 161, Loss: 0.4788483533353555\n",
      "Epoch 162, Loss: 0.4783709093142749\n",
      "Epoch 163, Loss: 0.4783027642591491\n",
      "Epoch 164, Loss: 0.4780003946284736\n",
      "Epoch 165, Loss: 0.47766356610156796\n",
      "Epoch 166, Loss: 0.47756940749162413\n",
      "Epoch 167, Loss: 0.4772720526560437\n",
      "Epoch 168, Loss: 0.477001191512605\n",
      "Epoch 169, Loss: 0.47690651155802954\n",
      "Epoch 170, Loss: 0.47663605231839845\n",
      "Epoch 171, Loss: 0.4763715079887003\n",
      "Epoch 172, Loss: 0.4762479807342815\n",
      "Epoch 173, Loss: 0.47603155482909787\n",
      "Epoch 174, Loss: 0.47580085926956445\n",
      "Epoch 175, Loss: 0.4756396378860013\n",
      "Epoch 176, Loss: 0.4754627232666358\n",
      "Epoch 177, Loss: 0.4752513954316692\n",
      "Epoch 178, Loss: 0.47507140350730925\n",
      "Epoch 179, Loss: 0.47492471273973497\n",
      "Epoch 180, Loss: 0.47475493910732763\n",
      "Epoch 181, Loss: 0.47455157018218247\n",
      "Epoch 182, Loss: 0.4743853224635176\n",
      "Epoch 183, Loss: 0.47424550491599726\n",
      "Epoch 184, Loss: 0.47408413178207975\n",
      "Epoch 185, Loss: 0.47390385374022453\n",
      "Epoch 186, Loss: 0.4737383066259127\n",
      "Epoch 187, Loss: 0.47358797785338436\n",
      "Epoch 188, Loss: 0.4734560449826861\n",
      "Epoch 189, Loss: 0.47335738206045025\n",
      "Epoch 190, Loss: 0.4733162834925985\n",
      "Epoch 191, Loss: 0.4732413020737725\n",
      "Epoch 192, Loss: 0.47313239020014447\n",
      "Epoch 193, Loss: 0.4729457158004841\n",
      "Epoch 194, Loss: 0.47274703738102797\n",
      "Epoch 195, Loss: 0.47253750657270627\n",
      "Epoch 196, Loss: 0.4723544299968521\n",
      "Epoch 197, Loss: 0.47221102907746554\n",
      "Epoch 198, Loss: 0.47210844467503205\n",
      "Epoch 199, Loss: 0.47205337980155637\n",
      "Epoch 200, Loss: 0.47204973383515075\n",
      "Epoch 201, Loss: 0.4720427890593303\n",
      "Epoch 202, Loss: 0.4719159657967507\n",
      "Epoch 203, Loss: 0.471735017109987\n",
      "Epoch 204, Loss: 0.4714985533244086\n",
      "Epoch 205, Loss: 0.4713143302002759\n",
      "Epoch 206, Loss: 0.47118574544243713\n",
      "Epoch 207, Loss: 0.47112002449718005\n",
      "Epoch 208, Loss: 0.4710910670518786\n",
      "Epoch 209, Loss: 0.47102495326923743\n",
      "Epoch 210, Loss: 0.4709626586360175\n",
      "Epoch 211, Loss: 0.47085840979895394\n",
      "Epoch 212, Loss: 0.47078064719388485\n",
      "Epoch 213, Loss: 0.47066614789276784\n",
      "Epoch 214, Loss: 0.47055654499036753\n",
      "Epoch 215, Loss: 0.47041652390832933\n",
      "Epoch 216, Loss: 0.4702446473560784\n",
      "Epoch 217, Loss: 0.4701045016774931\n",
      "Epoch 218, Loss: 0.47002059077245106\n",
      "Epoch 219, Loss: 0.4699643617672827\n",
      "Epoch 220, Loss: 0.4698888296176042\n",
      "Epoch 221, Loss: 0.4698141575467262\n",
      "Epoch 222, Loss: 0.4697265783615268\n",
      "Epoch 223, Loss: 0.46966066909468496\n",
      "Epoch 224, Loss: 0.4696348739248034\n",
      "Epoch 225, Loss: 0.4697176006517021\n",
      "Epoch 226, Loss: 0.4699424675720642\n",
      "Epoch 227, Loss: 0.46999269112017344\n",
      "Epoch 228, Loss: 0.4698759384724347\n",
      "Epoch 229, Loss: 0.46938041574678274\n",
      "Epoch 230, Loss: 0.4691415581864629\n",
      "Epoch 231, Loss: 0.46924839544758656\n",
      "Epoch 232, Loss: 0.46933531775018905\n",
      "Epoch 233, Loss: 0.4692253881106229\n",
      "Epoch 234, Loss: 0.46894320638835874\n",
      "Epoch 235, Loss: 0.4688628060980674\n",
      "Epoch 236, Loss: 0.46897798398181567\n",
      "Epoch 237, Loss: 0.4689811295666005\n",
      "Epoch 238, Loss: 0.4688031484354944\n",
      "Epoch 239, Loss: 0.46862983752908405\n",
      "Epoch 240, Loss: 0.4685963696810109\n",
      "Epoch 241, Loss: 0.46867106090894\n",
      "Epoch 242, Loss: 0.46866360503981047\n",
      "Epoch 243, Loss: 0.468513635806399\n",
      "Epoch 244, Loss: 0.46835730315983837\n",
      "Epoch 245, Loss: 0.468344910742699\n",
      "Epoch 246, Loss: 0.46842993299443175\n",
      "Epoch 247, Loss: 0.4683803372309361\n",
      "Epoch 248, Loss: 0.46823408027364943\n",
      "Epoch 249, Loss: 0.46809559282800506\n",
      "Epoch 250, Loss: 0.4681194355765096\n",
      "Epoch 251, Loss: 0.4682059468617626\n",
      "Epoch 252, Loss: 0.46815125054377077\n",
      "Epoch 253, Loss: 0.46800465135626707\n",
      "Epoch 254, Loss: 0.4678516840148294\n",
      "Epoch 255, Loss: 0.46781997608436393\n",
      "Epoch 256, Loss: 0.4678506648079015\n",
      "Epoch 257, Loss: 0.4677995105079106\n",
      "Epoch 258, Loss: 0.4676942061377158\n",
      "Epoch 259, Loss: 0.46760336187164103\n",
      "Epoch 260, Loss: 0.46754277859764004\n",
      "Epoch 261, Loss: 0.4675358146830681\n",
      "Epoch 262, Loss: 0.4675934670336611\n",
      "Epoch 263, Loss: 0.4675883044263754\n",
      "Epoch 264, Loss: 0.46749525618568777\n",
      "Epoch 265, Loss: 0.46735425416294335\n",
      "Epoch 266, Loss: 0.4672686694514329\n",
      "Epoch 267, Loss: 0.46723722493080344\n",
      "Epoch 268, Loss: 0.4672375438513713\n",
      "Epoch 269, Loss: 0.46722038561562945\n",
      "Epoch 270, Loss: 0.46721867932539873\n",
      "Epoch 271, Loss: 0.46717319721795353\n",
      "Epoch 272, Loss: 0.4671238189214812\n",
      "Epoch 273, Loss: 0.46705709570299314\n",
      "Epoch 274, Loss: 0.4669974785875546\n",
      "Epoch 275, Loss: 0.4669422896788895\n",
      "Epoch 276, Loss: 0.4669034987743203\n",
      "Epoch 277, Loss: 0.46684804340658115\n",
      "Epoch 278, Loss: 0.4668104417178138\n",
      "Epoch 279, Loss: 0.46677111311176783\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.21799671816593497\n",
      "Valence RMSE: 0.205462416703562\n",
      "Arousal RMSE: 0.22984850137551044\n",
      "Test R^2 score: tensor([0.4884, 0.4769], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.3687, 0.3545], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.48264494388759444\n",
      "Num of epochs: 280\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Epoch 121, Loss: 0.49295912913079615\n",
      "Epoch 122, Loss: 0.4924773619139838\n",
      "Epoch 123, Loss: 0.49197607191949805\n",
      "Epoch 124, Loss: 0.4914699197831528\n",
      "Epoch 125, Loss: 0.49091483381312667\n",
      "Epoch 126, Loss: 0.49035147677946317\n",
      "Epoch 127, Loss: 0.4898087222825037\n",
      "Epoch 128, Loss: 0.48924014731550836\n",
      "Epoch 129, Loss: 0.48873550647212044\n",
      "Epoch 130, Loss: 0.48832307254971563\n",
      "Epoch 131, Loss: 0.48791418392821295\n",
      "Epoch 132, Loss: 0.4875902245243796\n",
      "Epoch 133, Loss: 0.48728851108037585\n",
      "Epoch 134, Loss: 0.48697062262181345\n",
      "Epoch 135, Loss: 0.4866339553030548\n",
      "Epoch 136, Loss: 0.48633446746734893\n",
      "Epoch 137, Loss: 0.4860310087358514\n",
      "Epoch 138, Loss: 0.4856786412545615\n",
      "Epoch 139, Loss: 0.48531372107004\n",
      "Epoch 140, Loss: 0.48495712985488587\n",
      "Epoch 141, Loss: 0.4846131600652309\n",
      "Epoch 142, Loss: 0.48425966858652364\n",
      "Epoch 143, Loss: 0.4838914149236435\n",
      "Epoch 144, Loss: 0.48352437546282767\n",
      "Epoch 145, Loss: 0.4832299911937133\n",
      "Epoch 146, Loss: 0.4832586529465723\n",
      "Epoch 147, Loss: 0.48356744143113517\n",
      "Epoch 148, Loss: 0.4828108019011922\n",
      "Epoch 149, Loss: 0.4818912939738211\n",
      "Epoch 150, Loss: 0.48239416745575797\n",
      "Epoch 151, Loss: 0.4814952491721961\n",
      "Epoch 152, Loss: 0.4811822818820502\n",
      "Epoch 153, Loss: 0.4812753156716337\n",
      "Epoch 154, Loss: 0.4804387742282827\n",
      "Epoch 155, Loss: 0.48058189052417905\n",
      "Epoch 156, Loss: 0.4801288115497346\n",
      "Epoch 157, Loss: 0.4797465937246525\n",
      "Epoch 158, Loss: 0.4797622479529733\n",
      "Epoch 159, Loss: 0.4791664679844762\n",
      "Epoch 160, Loss: 0.47904359923301915\n",
      "Epoch 161, Loss: 0.4788483533353555\n",
      "Epoch 162, Loss: 0.4783709093142749\n",
      "Epoch 163, Loss: 0.4783027642591491\n",
      "Epoch 164, Loss: 0.4780003946284736\n",
      "Epoch 165, Loss: 0.47766356610156796\n",
      "Epoch 166, Loss: 0.47756940749162413\n",
      "Epoch 167, Loss: 0.4772720526560437\n",
      "Epoch 168, Loss: 0.477001191512605\n",
      "Epoch 169, Loss: 0.47690651155802954\n",
      "Epoch 170, Loss: 0.47663605231839845\n",
      "Epoch 171, Loss: 0.4763715079887003\n",
      "Epoch 172, Loss: 0.4762479807342815\n",
      "Epoch 173, Loss: 0.47603155482909787\n",
      "Epoch 174, Loss: 0.47580085926956445\n",
      "Epoch 175, Loss: 0.4756396378860013\n",
      "Epoch 176, Loss: 0.4754627232666358\n",
      "Epoch 177, Loss: 0.4752513954316692\n",
      "Epoch 178, Loss: 0.47507140350730925\n",
      "Epoch 179, Loss: 0.47492471273973497\n",
      "Epoch 180, Loss: 0.47475493910732763\n",
      "Epoch 181, Loss: 0.47455157018218247\n",
      "Epoch 182, Loss: 0.4743853224635176\n",
      "Epoch 183, Loss: 0.47424550491599726\n",
      "Epoch 184, Loss: 0.47408413178207975\n",
      "Epoch 185, Loss: 0.47390385374022453\n",
      "Epoch 186, Loss: 0.4737383066259127\n",
      "Epoch 187, Loss: 0.47358797785338436\n",
      "Epoch 188, Loss: 0.4734560449826861\n",
      "Epoch 189, Loss: 0.47335738206045025\n",
      "Epoch 190, Loss: 0.4733162834925985\n",
      "Epoch 191, Loss: 0.4732413020737725\n",
      "Epoch 192, Loss: 0.47313239020014447\n",
      "Epoch 193, Loss: 0.4729457158004841\n",
      "Epoch 194, Loss: 0.47274703738102797\n",
      "Epoch 195, Loss: 0.47253750657270627\n",
      "Epoch 196, Loss: 0.4723544299968521\n",
      "Epoch 197, Loss: 0.47221102907746554\n",
      "Epoch 198, Loss: 0.47210844467503205\n",
      "Epoch 199, Loss: 0.47205337980155637\n",
      "Epoch 200, Loss: 0.47204973383515075\n",
      "Epoch 201, Loss: 0.4720427890593303\n",
      "Epoch 202, Loss: 0.4719159657967507\n",
      "Epoch 203, Loss: 0.471735017109987\n",
      "Epoch 204, Loss: 0.4714985533244086\n",
      "Epoch 205, Loss: 0.4713143302002759\n",
      "Epoch 206, Loss: 0.47118574544243713\n",
      "Epoch 207, Loss: 0.47112002449718005\n",
      "Epoch 208, Loss: 0.4710910670518786\n",
      "Epoch 209, Loss: 0.47102495326923743\n",
      "Epoch 210, Loss: 0.4709626586360175\n",
      "Epoch 211, Loss: 0.47085840979895394\n",
      "Epoch 212, Loss: 0.47078064719388485\n",
      "Epoch 213, Loss: 0.47066614789276784\n",
      "Epoch 214, Loss: 0.47055654499036753\n",
      "Epoch 215, Loss: 0.47041652390832933\n",
      "Epoch 216, Loss: 0.4702446473560784\n",
      "Epoch 217, Loss: 0.4701045016774931\n",
      "Epoch 218, Loss: 0.47002059077245106\n",
      "Epoch 219, Loss: 0.4699643617672827\n",
      "Epoch 220, Loss: 0.4698888296176042\n",
      "Epoch 221, Loss: 0.4698141575467262\n",
      "Epoch 222, Loss: 0.4697265783615268\n",
      "Epoch 223, Loss: 0.46966066909468496\n",
      "Epoch 224, Loss: 0.4696348739248034\n",
      "Epoch 225, Loss: 0.4697176006517021\n",
      "Epoch 226, Loss: 0.4699424675720642\n",
      "Epoch 227, Loss: 0.46999269112017344\n",
      "Epoch 228, Loss: 0.4698759384724347\n",
      "Epoch 229, Loss: 0.46938041574678274\n",
      "Epoch 230, Loss: 0.4691415581864629\n",
      "Epoch 231, Loss: 0.46924839544758656\n",
      "Epoch 232, Loss: 0.46933531775018905\n",
      "Epoch 233, Loss: 0.4692253881106229\n",
      "Epoch 234, Loss: 0.46894320638835874\n",
      "Epoch 235, Loss: 0.4688628060980674\n",
      "Epoch 236, Loss: 0.46897798398181567\n",
      "Epoch 237, Loss: 0.4689811295666005\n",
      "Epoch 238, Loss: 0.4688031484354944\n",
      "Epoch 239, Loss: 0.46862983752908405\n",
      "Epoch 240, Loss: 0.4685963696810109\n",
      "Epoch 241, Loss: 0.46867106090894\n",
      "Epoch 242, Loss: 0.46866360503981047\n",
      "Epoch 243, Loss: 0.468513635806399\n",
      "Epoch 244, Loss: 0.46835730315983837\n",
      "Epoch 245, Loss: 0.468344910742699\n",
      "Epoch 246, Loss: 0.46842993299443175\n",
      "Epoch 247, Loss: 0.4683803372309361\n",
      "Epoch 248, Loss: 0.46823408027364943\n",
      "Epoch 249, Loss: 0.46809559282800506\n",
      "Epoch 250, Loss: 0.4681194355765096\n",
      "Epoch 251, Loss: 0.4682059468617626\n",
      "Epoch 252, Loss: 0.46815125054377077\n",
      "Epoch 253, Loss: 0.46800465135626707\n",
      "Epoch 254, Loss: 0.4678516840148294\n",
      "Epoch 255, Loss: 0.46781997608436393\n",
      "Epoch 256, Loss: 0.4678506648079015\n",
      "Epoch 257, Loss: 0.4677995105079106\n",
      "Epoch 258, Loss: 0.4676942061377158\n",
      "Epoch 259, Loss: 0.46760336187164103\n",
      "Epoch 260, Loss: 0.46754277859764004\n",
      "Epoch 261, Loss: 0.4675358146830681\n",
      "Epoch 262, Loss: 0.4675934670336611\n",
      "Epoch 263, Loss: 0.4675883044263754\n",
      "Epoch 264, Loss: 0.46749525618568777\n",
      "Epoch 265, Loss: 0.46735425416294335\n",
      "Epoch 266, Loss: 0.4672686694514329\n",
      "Epoch 267, Loss: 0.46723722493080344\n",
      "Epoch 268, Loss: 0.4672375438513713\n",
      "Epoch 269, Loss: 0.46722038561562945\n",
      "Epoch 270, Loss: 0.46721867932539873\n",
      "Epoch 271, Loss: 0.46717319721795353\n",
      "Epoch 272, Loss: 0.4671238189214812\n",
      "Epoch 273, Loss: 0.46705709570299314\n",
      "Epoch 274, Loss: 0.4669974785875546\n",
      "Epoch 275, Loss: 0.4669422896788895\n",
      "Epoch 276, Loss: 0.4669034987743203\n",
      "Epoch 277, Loss: 0.46684804340658115\n",
      "Epoch 278, Loss: 0.4668104417178138\n",
      "Epoch 279, Loss: 0.46677111311176783\n",
      "Epoch 280, Loss: 0.46673031256560493\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.21794259598453725\n",
      "Valence RMSE: 0.20541727375474278\n",
      "Arousal RMSE: 0.22978619177863482\n",
      "Test R^2 score: tensor([0.4886, 0.4772], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.3689, 0.3548], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.48289912773900956\n",
      "Num of epochs: 281\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Epoch 121, Loss: 0.49295912913079615\n",
      "Epoch 122, Loss: 0.4924773619139838\n",
      "Epoch 123, Loss: 0.49197607191949805\n",
      "Epoch 124, Loss: 0.4914699197831528\n",
      "Epoch 125, Loss: 0.49091483381312667\n",
      "Epoch 126, Loss: 0.49035147677946317\n",
      "Epoch 127, Loss: 0.4898087222825037\n",
      "Epoch 128, Loss: 0.48924014731550836\n",
      "Epoch 129, Loss: 0.48873550647212044\n",
      "Epoch 130, Loss: 0.48832307254971563\n",
      "Epoch 131, Loss: 0.48791418392821295\n",
      "Epoch 132, Loss: 0.4875902245243796\n",
      "Epoch 133, Loss: 0.48728851108037585\n",
      "Epoch 134, Loss: 0.48697062262181345\n",
      "Epoch 135, Loss: 0.4866339553030548\n",
      "Epoch 136, Loss: 0.48633446746734893\n",
      "Epoch 137, Loss: 0.4860310087358514\n",
      "Epoch 138, Loss: 0.4856786412545615\n",
      "Epoch 139, Loss: 0.48531372107004\n",
      "Epoch 140, Loss: 0.48495712985488587\n",
      "Epoch 141, Loss: 0.4846131600652309\n",
      "Epoch 142, Loss: 0.48425966858652364\n",
      "Epoch 143, Loss: 0.4838914149236435\n",
      "Epoch 144, Loss: 0.48352437546282767\n",
      "Epoch 145, Loss: 0.4832299911937133\n",
      "Epoch 146, Loss: 0.4832586529465723\n",
      "Epoch 147, Loss: 0.48356744143113517\n",
      "Epoch 148, Loss: 0.4828108019011922\n",
      "Epoch 149, Loss: 0.4818912939738211\n",
      "Epoch 150, Loss: 0.48239416745575797\n",
      "Epoch 151, Loss: 0.4814952491721961\n",
      "Epoch 152, Loss: 0.4811822818820502\n",
      "Epoch 153, Loss: 0.4812753156716337\n",
      "Epoch 154, Loss: 0.4804387742282827\n",
      "Epoch 155, Loss: 0.48058189052417905\n",
      "Epoch 156, Loss: 0.4801288115497346\n",
      "Epoch 157, Loss: 0.4797465937246525\n",
      "Epoch 158, Loss: 0.4797622479529733\n",
      "Epoch 159, Loss: 0.4791664679844762\n",
      "Epoch 160, Loss: 0.47904359923301915\n",
      "Epoch 161, Loss: 0.4788483533353555\n",
      "Epoch 162, Loss: 0.4783709093142749\n",
      "Epoch 163, Loss: 0.4783027642591491\n",
      "Epoch 164, Loss: 0.4780003946284736\n",
      "Epoch 165, Loss: 0.47766356610156796\n",
      "Epoch 166, Loss: 0.47756940749162413\n",
      "Epoch 167, Loss: 0.4772720526560437\n",
      "Epoch 168, Loss: 0.477001191512605\n",
      "Epoch 169, Loss: 0.47690651155802954\n",
      "Epoch 170, Loss: 0.47663605231839845\n",
      "Epoch 171, Loss: 0.4763715079887003\n",
      "Epoch 172, Loss: 0.4762479807342815\n",
      "Epoch 173, Loss: 0.47603155482909787\n",
      "Epoch 174, Loss: 0.47580085926956445\n",
      "Epoch 175, Loss: 0.4756396378860013\n",
      "Epoch 176, Loss: 0.4754627232666358\n",
      "Epoch 177, Loss: 0.4752513954316692\n",
      "Epoch 178, Loss: 0.47507140350730925\n",
      "Epoch 179, Loss: 0.47492471273973497\n",
      "Epoch 180, Loss: 0.47475493910732763\n",
      "Epoch 181, Loss: 0.47455157018218247\n",
      "Epoch 182, Loss: 0.4743853224635176\n",
      "Epoch 183, Loss: 0.47424550491599726\n",
      "Epoch 184, Loss: 0.47408413178207975\n",
      "Epoch 185, Loss: 0.47390385374022453\n",
      "Epoch 186, Loss: 0.4737383066259127\n",
      "Epoch 187, Loss: 0.47358797785338436\n",
      "Epoch 188, Loss: 0.4734560449826861\n",
      "Epoch 189, Loss: 0.47335738206045025\n",
      "Epoch 190, Loss: 0.4733162834925985\n",
      "Epoch 191, Loss: 0.4732413020737725\n",
      "Epoch 192, Loss: 0.47313239020014447\n",
      "Epoch 193, Loss: 0.4729457158004841\n",
      "Epoch 194, Loss: 0.47274703738102797\n",
      "Epoch 195, Loss: 0.47253750657270627\n",
      "Epoch 196, Loss: 0.4723544299968521\n",
      "Epoch 197, Loss: 0.47221102907746554\n",
      "Epoch 198, Loss: 0.47210844467503205\n",
      "Epoch 199, Loss: 0.47205337980155637\n",
      "Epoch 200, Loss: 0.47204973383515075\n",
      "Epoch 201, Loss: 0.4720427890593303\n",
      "Epoch 202, Loss: 0.4719159657967507\n",
      "Epoch 203, Loss: 0.471735017109987\n",
      "Epoch 204, Loss: 0.4714985533244086\n",
      "Epoch 205, Loss: 0.4713143302002759\n",
      "Epoch 206, Loss: 0.47118574544243713\n",
      "Epoch 207, Loss: 0.47112002449718005\n",
      "Epoch 208, Loss: 0.4710910670518786\n",
      "Epoch 209, Loss: 0.47102495326923743\n",
      "Epoch 210, Loss: 0.4709626586360175\n",
      "Epoch 211, Loss: 0.47085840979895394\n",
      "Epoch 212, Loss: 0.47078064719388485\n",
      "Epoch 213, Loss: 0.47066614789276784\n",
      "Epoch 214, Loss: 0.47055654499036753\n",
      "Epoch 215, Loss: 0.47041652390832933\n",
      "Epoch 216, Loss: 0.4702446473560784\n",
      "Epoch 217, Loss: 0.4701045016774931\n",
      "Epoch 218, Loss: 0.47002059077245106\n",
      "Epoch 219, Loss: 0.4699643617672827\n",
      "Epoch 220, Loss: 0.4698888296176042\n",
      "Epoch 221, Loss: 0.4698141575467262\n",
      "Epoch 222, Loss: 0.4697265783615268\n",
      "Epoch 223, Loss: 0.46966066909468496\n",
      "Epoch 224, Loss: 0.4696348739248034\n",
      "Epoch 225, Loss: 0.4697176006517021\n",
      "Epoch 226, Loss: 0.4699424675720642\n",
      "Epoch 227, Loss: 0.46999269112017344\n",
      "Epoch 228, Loss: 0.4698759384724347\n",
      "Epoch 229, Loss: 0.46938041574678274\n",
      "Epoch 230, Loss: 0.4691415581864629\n",
      "Epoch 231, Loss: 0.46924839544758656\n",
      "Epoch 232, Loss: 0.46933531775018905\n",
      "Epoch 233, Loss: 0.4692253881106229\n",
      "Epoch 234, Loss: 0.46894320638835874\n",
      "Epoch 235, Loss: 0.4688628060980674\n",
      "Epoch 236, Loss: 0.46897798398181567\n",
      "Epoch 237, Loss: 0.4689811295666005\n",
      "Epoch 238, Loss: 0.4688031484354944\n",
      "Epoch 239, Loss: 0.46862983752908405\n",
      "Epoch 240, Loss: 0.4685963696810109\n",
      "Epoch 241, Loss: 0.46867106090894\n",
      "Epoch 242, Loss: 0.46866360503981047\n",
      "Epoch 243, Loss: 0.468513635806399\n",
      "Epoch 244, Loss: 0.46835730315983837\n",
      "Epoch 245, Loss: 0.468344910742699\n",
      "Epoch 246, Loss: 0.46842993299443175\n",
      "Epoch 247, Loss: 0.4683803372309361\n",
      "Epoch 248, Loss: 0.46823408027364943\n",
      "Epoch 249, Loss: 0.46809559282800506\n",
      "Epoch 250, Loss: 0.4681194355765096\n",
      "Epoch 251, Loss: 0.4682059468617626\n",
      "Epoch 252, Loss: 0.46815125054377077\n",
      "Epoch 253, Loss: 0.46800465135626707\n",
      "Epoch 254, Loss: 0.4678516840148294\n",
      "Epoch 255, Loss: 0.46781997608436393\n",
      "Epoch 256, Loss: 0.4678506648079015\n",
      "Epoch 257, Loss: 0.4677995105079106\n",
      "Epoch 258, Loss: 0.4676942061377158\n",
      "Epoch 259, Loss: 0.46760336187164103\n",
      "Epoch 260, Loss: 0.46754277859764004\n",
      "Epoch 261, Loss: 0.4675358146830681\n",
      "Epoch 262, Loss: 0.4675934670336611\n",
      "Epoch 263, Loss: 0.4675883044263754\n",
      "Epoch 264, Loss: 0.46749525618568777\n",
      "Epoch 265, Loss: 0.46735425416294335\n",
      "Epoch 266, Loss: 0.4672686694514329\n",
      "Epoch 267, Loss: 0.46723722493080344\n",
      "Epoch 268, Loss: 0.4672375438513713\n",
      "Epoch 269, Loss: 0.46722038561562945\n",
      "Epoch 270, Loss: 0.46721867932539873\n",
      "Epoch 271, Loss: 0.46717319721795353\n",
      "Epoch 272, Loss: 0.4671238189214812\n",
      "Epoch 273, Loss: 0.46705709570299314\n",
      "Epoch 274, Loss: 0.4669974785875546\n",
      "Epoch 275, Loss: 0.4669422896788895\n",
      "Epoch 276, Loss: 0.4669034987743203\n",
      "Epoch 277, Loss: 0.46684804340658115\n",
      "Epoch 278, Loss: 0.4668104417178138\n",
      "Epoch 279, Loss: 0.46677111311176783\n",
      "Epoch 280, Loss: 0.46673031256560493\n",
      "Epoch 281, Loss: 0.46669448942736436\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.21779581491742656\n",
      "Valence RMSE: 0.20529626688790323\n",
      "Arousal RMSE: 0.22961593322977972\n",
      "Test R^2 score: tensor([0.4892, 0.4779], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.3697, 0.3558], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.48358752023716334\n",
      "Num of epochs: 282\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Epoch 121, Loss: 0.49295912913079615\n",
      "Epoch 122, Loss: 0.4924773619139838\n",
      "Epoch 123, Loss: 0.49197607191949805\n",
      "Epoch 124, Loss: 0.4914699197831528\n",
      "Epoch 125, Loss: 0.49091483381312667\n",
      "Epoch 126, Loss: 0.49035147677946317\n",
      "Epoch 127, Loss: 0.4898087222825037\n",
      "Epoch 128, Loss: 0.48924014731550836\n",
      "Epoch 129, Loss: 0.48873550647212044\n",
      "Epoch 130, Loss: 0.48832307254971563\n",
      "Epoch 131, Loss: 0.48791418392821295\n",
      "Epoch 132, Loss: 0.4875902245243796\n",
      "Epoch 133, Loss: 0.48728851108037585\n",
      "Epoch 134, Loss: 0.48697062262181345\n",
      "Epoch 135, Loss: 0.4866339553030548\n",
      "Epoch 136, Loss: 0.48633446746734893\n",
      "Epoch 137, Loss: 0.4860310087358514\n",
      "Epoch 138, Loss: 0.4856786412545615\n",
      "Epoch 139, Loss: 0.48531372107004\n",
      "Epoch 140, Loss: 0.48495712985488587\n",
      "Epoch 141, Loss: 0.4846131600652309\n",
      "Epoch 142, Loss: 0.48425966858652364\n",
      "Epoch 143, Loss: 0.4838914149236435\n",
      "Epoch 144, Loss: 0.48352437546282767\n",
      "Epoch 145, Loss: 0.4832299911937133\n",
      "Epoch 146, Loss: 0.4832586529465723\n",
      "Epoch 147, Loss: 0.48356744143113517\n",
      "Epoch 148, Loss: 0.4828108019011922\n",
      "Epoch 149, Loss: 0.4818912939738211\n",
      "Epoch 150, Loss: 0.48239416745575797\n",
      "Epoch 151, Loss: 0.4814952491721961\n",
      "Epoch 152, Loss: 0.4811822818820502\n",
      "Epoch 153, Loss: 0.4812753156716337\n",
      "Epoch 154, Loss: 0.4804387742282827\n",
      "Epoch 155, Loss: 0.48058189052417905\n",
      "Epoch 156, Loss: 0.4801288115497346\n",
      "Epoch 157, Loss: 0.4797465937246525\n",
      "Epoch 158, Loss: 0.4797622479529733\n",
      "Epoch 159, Loss: 0.4791664679844762\n",
      "Epoch 160, Loss: 0.47904359923301915\n",
      "Epoch 161, Loss: 0.4788483533353555\n",
      "Epoch 162, Loss: 0.4783709093142749\n",
      "Epoch 163, Loss: 0.4783027642591491\n",
      "Epoch 164, Loss: 0.4780003946284736\n",
      "Epoch 165, Loss: 0.47766356610156796\n",
      "Epoch 166, Loss: 0.47756940749162413\n",
      "Epoch 167, Loss: 0.4772720526560437\n",
      "Epoch 168, Loss: 0.477001191512605\n",
      "Epoch 169, Loss: 0.47690651155802954\n",
      "Epoch 170, Loss: 0.47663605231839845\n",
      "Epoch 171, Loss: 0.4763715079887003\n",
      "Epoch 172, Loss: 0.4762479807342815\n",
      "Epoch 173, Loss: 0.47603155482909787\n",
      "Epoch 174, Loss: 0.47580085926956445\n",
      "Epoch 175, Loss: 0.4756396378860013\n",
      "Epoch 176, Loss: 0.4754627232666358\n",
      "Epoch 177, Loss: 0.4752513954316692\n",
      "Epoch 178, Loss: 0.47507140350730925\n",
      "Epoch 179, Loss: 0.47492471273973497\n",
      "Epoch 180, Loss: 0.47475493910732763\n",
      "Epoch 181, Loss: 0.47455157018218247\n",
      "Epoch 182, Loss: 0.4743853224635176\n",
      "Epoch 183, Loss: 0.47424550491599726\n",
      "Epoch 184, Loss: 0.47408413178207975\n",
      "Epoch 185, Loss: 0.47390385374022453\n",
      "Epoch 186, Loss: 0.4737383066259127\n",
      "Epoch 187, Loss: 0.47358797785338436\n",
      "Epoch 188, Loss: 0.4734560449826861\n",
      "Epoch 189, Loss: 0.47335738206045025\n",
      "Epoch 190, Loss: 0.4733162834925985\n",
      "Epoch 191, Loss: 0.4732413020737725\n",
      "Epoch 192, Loss: 0.47313239020014447\n",
      "Epoch 193, Loss: 0.4729457158004841\n",
      "Epoch 194, Loss: 0.47274703738102797\n",
      "Epoch 195, Loss: 0.47253750657270627\n",
      "Epoch 196, Loss: 0.4723544299968521\n",
      "Epoch 197, Loss: 0.47221102907746554\n",
      "Epoch 198, Loss: 0.47210844467503205\n",
      "Epoch 199, Loss: 0.47205337980155637\n",
      "Epoch 200, Loss: 0.47204973383515075\n",
      "Epoch 201, Loss: 0.4720427890593303\n",
      "Epoch 202, Loss: 0.4719159657967507\n",
      "Epoch 203, Loss: 0.471735017109987\n",
      "Epoch 204, Loss: 0.4714985533244086\n",
      "Epoch 205, Loss: 0.4713143302002759\n",
      "Epoch 206, Loss: 0.47118574544243713\n",
      "Epoch 207, Loss: 0.47112002449718005\n",
      "Epoch 208, Loss: 0.4710910670518786\n",
      "Epoch 209, Loss: 0.47102495326923743\n",
      "Epoch 210, Loss: 0.4709626586360175\n",
      "Epoch 211, Loss: 0.47085840979895394\n",
      "Epoch 212, Loss: 0.47078064719388485\n",
      "Epoch 213, Loss: 0.47066614789276784\n",
      "Epoch 214, Loss: 0.47055654499036753\n",
      "Epoch 215, Loss: 0.47041652390832933\n",
      "Epoch 216, Loss: 0.4702446473560784\n",
      "Epoch 217, Loss: 0.4701045016774931\n",
      "Epoch 218, Loss: 0.47002059077245106\n",
      "Epoch 219, Loss: 0.4699643617672827\n",
      "Epoch 220, Loss: 0.4698888296176042\n",
      "Epoch 221, Loss: 0.4698141575467262\n",
      "Epoch 222, Loss: 0.4697265783615268\n",
      "Epoch 223, Loss: 0.46966066909468496\n",
      "Epoch 224, Loss: 0.4696348739248034\n",
      "Epoch 225, Loss: 0.4697176006517021\n",
      "Epoch 226, Loss: 0.4699424675720642\n",
      "Epoch 227, Loss: 0.46999269112017344\n",
      "Epoch 228, Loss: 0.4698759384724347\n",
      "Epoch 229, Loss: 0.46938041574678274\n",
      "Epoch 230, Loss: 0.4691415581864629\n",
      "Epoch 231, Loss: 0.46924839544758656\n",
      "Epoch 232, Loss: 0.46933531775018905\n",
      "Epoch 233, Loss: 0.4692253881106229\n",
      "Epoch 234, Loss: 0.46894320638835874\n",
      "Epoch 235, Loss: 0.4688628060980674\n",
      "Epoch 236, Loss: 0.46897798398181567\n",
      "Epoch 237, Loss: 0.4689811295666005\n",
      "Epoch 238, Loss: 0.4688031484354944\n",
      "Epoch 239, Loss: 0.46862983752908405\n",
      "Epoch 240, Loss: 0.4685963696810109\n",
      "Epoch 241, Loss: 0.46867106090894\n",
      "Epoch 242, Loss: 0.46866360503981047\n",
      "Epoch 243, Loss: 0.468513635806399\n",
      "Epoch 244, Loss: 0.46835730315983837\n",
      "Epoch 245, Loss: 0.468344910742699\n",
      "Epoch 246, Loss: 0.46842993299443175\n",
      "Epoch 247, Loss: 0.4683803372309361\n",
      "Epoch 248, Loss: 0.46823408027364943\n",
      "Epoch 249, Loss: 0.46809559282800506\n",
      "Epoch 250, Loss: 0.4681194355765096\n",
      "Epoch 251, Loss: 0.4682059468617626\n",
      "Epoch 252, Loss: 0.46815125054377077\n",
      "Epoch 253, Loss: 0.46800465135626707\n",
      "Epoch 254, Loss: 0.4678516840148294\n",
      "Epoch 255, Loss: 0.46781997608436393\n",
      "Epoch 256, Loss: 0.4678506648079015\n",
      "Epoch 257, Loss: 0.4677995105079106\n",
      "Epoch 258, Loss: 0.4676942061377158\n",
      "Epoch 259, Loss: 0.46760336187164103\n",
      "Epoch 260, Loss: 0.46754277859764004\n",
      "Epoch 261, Loss: 0.4675358146830681\n",
      "Epoch 262, Loss: 0.4675934670336611\n",
      "Epoch 263, Loss: 0.4675883044263754\n",
      "Epoch 264, Loss: 0.46749525618568777\n",
      "Epoch 265, Loss: 0.46735425416294335\n",
      "Epoch 266, Loss: 0.4672686694514329\n",
      "Epoch 267, Loss: 0.46723722493080344\n",
      "Epoch 268, Loss: 0.4672375438513713\n",
      "Epoch 269, Loss: 0.46722038561562945\n",
      "Epoch 270, Loss: 0.46721867932539873\n",
      "Epoch 271, Loss: 0.46717319721795353\n",
      "Epoch 272, Loss: 0.4671238189214812\n",
      "Epoch 273, Loss: 0.46705709570299314\n",
      "Epoch 274, Loss: 0.4669974785875546\n",
      "Epoch 275, Loss: 0.4669422896788895\n",
      "Epoch 276, Loss: 0.4669034987743203\n",
      "Epoch 277, Loss: 0.46684804340658115\n",
      "Epoch 278, Loss: 0.4668104417178138\n",
      "Epoch 279, Loss: 0.46677111311176783\n",
      "Epoch 280, Loss: 0.46673031256560493\n",
      "Epoch 281, Loss: 0.46669448942736436\n",
      "Epoch 282, Loss: 0.46666225583115023\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.21803255601783794\n",
      "Valence RMSE: 0.20552260714936688\n",
      "Arousal RMSE: 0.22986267404228614\n",
      "Test R^2 score: tensor([0.4881, 0.4768], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.3683, 0.3544], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4824627914593218\n",
      "Num of epochs: 283\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Epoch 121, Loss: 0.49295912913079615\n",
      "Epoch 122, Loss: 0.4924773619139838\n",
      "Epoch 123, Loss: 0.49197607191949805\n",
      "Epoch 124, Loss: 0.4914699197831528\n",
      "Epoch 125, Loss: 0.49091483381312667\n",
      "Epoch 126, Loss: 0.49035147677946317\n",
      "Epoch 127, Loss: 0.4898087222825037\n",
      "Epoch 128, Loss: 0.48924014731550836\n",
      "Epoch 129, Loss: 0.48873550647212044\n",
      "Epoch 130, Loss: 0.48832307254971563\n",
      "Epoch 131, Loss: 0.48791418392821295\n",
      "Epoch 132, Loss: 0.4875902245243796\n",
      "Epoch 133, Loss: 0.48728851108037585\n",
      "Epoch 134, Loss: 0.48697062262181345\n",
      "Epoch 135, Loss: 0.4866339553030548\n",
      "Epoch 136, Loss: 0.48633446746734893\n",
      "Epoch 137, Loss: 0.4860310087358514\n",
      "Epoch 138, Loss: 0.4856786412545615\n",
      "Epoch 139, Loss: 0.48531372107004\n",
      "Epoch 140, Loss: 0.48495712985488587\n",
      "Epoch 141, Loss: 0.4846131600652309\n",
      "Epoch 142, Loss: 0.48425966858652364\n",
      "Epoch 143, Loss: 0.4838914149236435\n",
      "Epoch 144, Loss: 0.48352437546282767\n",
      "Epoch 145, Loss: 0.4832299911937133\n",
      "Epoch 146, Loss: 0.4832586529465723\n",
      "Epoch 147, Loss: 0.48356744143113517\n",
      "Epoch 148, Loss: 0.4828108019011922\n",
      "Epoch 149, Loss: 0.4818912939738211\n",
      "Epoch 150, Loss: 0.48239416745575797\n",
      "Epoch 151, Loss: 0.4814952491721961\n",
      "Epoch 152, Loss: 0.4811822818820502\n",
      "Epoch 153, Loss: 0.4812753156716337\n",
      "Epoch 154, Loss: 0.4804387742282827\n",
      "Epoch 155, Loss: 0.48058189052417905\n",
      "Epoch 156, Loss: 0.4801288115497346\n",
      "Epoch 157, Loss: 0.4797465937246525\n",
      "Epoch 158, Loss: 0.4797622479529733\n",
      "Epoch 159, Loss: 0.4791664679844762\n",
      "Epoch 160, Loss: 0.47904359923301915\n",
      "Epoch 161, Loss: 0.4788483533353555\n",
      "Epoch 162, Loss: 0.4783709093142749\n",
      "Epoch 163, Loss: 0.4783027642591491\n",
      "Epoch 164, Loss: 0.4780003946284736\n",
      "Epoch 165, Loss: 0.47766356610156796\n",
      "Epoch 166, Loss: 0.47756940749162413\n",
      "Epoch 167, Loss: 0.4772720526560437\n",
      "Epoch 168, Loss: 0.477001191512605\n",
      "Epoch 169, Loss: 0.47690651155802954\n",
      "Epoch 170, Loss: 0.47663605231839845\n",
      "Epoch 171, Loss: 0.4763715079887003\n",
      "Epoch 172, Loss: 0.4762479807342815\n",
      "Epoch 173, Loss: 0.47603155482909787\n",
      "Epoch 174, Loss: 0.47580085926956445\n",
      "Epoch 175, Loss: 0.4756396378860013\n",
      "Epoch 176, Loss: 0.4754627232666358\n",
      "Epoch 177, Loss: 0.4752513954316692\n",
      "Epoch 178, Loss: 0.47507140350730925\n",
      "Epoch 179, Loss: 0.47492471273973497\n",
      "Epoch 180, Loss: 0.47475493910732763\n",
      "Epoch 181, Loss: 0.47455157018218247\n",
      "Epoch 182, Loss: 0.4743853224635176\n",
      "Epoch 183, Loss: 0.47424550491599726\n",
      "Epoch 184, Loss: 0.47408413178207975\n",
      "Epoch 185, Loss: 0.47390385374022453\n",
      "Epoch 186, Loss: 0.4737383066259127\n",
      "Epoch 187, Loss: 0.47358797785338436\n",
      "Epoch 188, Loss: 0.4734560449826861\n",
      "Epoch 189, Loss: 0.47335738206045025\n",
      "Epoch 190, Loss: 0.4733162834925985\n",
      "Epoch 191, Loss: 0.4732413020737725\n",
      "Epoch 192, Loss: 0.47313239020014447\n",
      "Epoch 193, Loss: 0.4729457158004841\n",
      "Epoch 194, Loss: 0.47274703738102797\n",
      "Epoch 195, Loss: 0.47253750657270627\n",
      "Epoch 196, Loss: 0.4723544299968521\n",
      "Epoch 197, Loss: 0.47221102907746554\n",
      "Epoch 198, Loss: 0.47210844467503205\n",
      "Epoch 199, Loss: 0.47205337980155637\n",
      "Epoch 200, Loss: 0.47204973383515075\n",
      "Epoch 201, Loss: 0.4720427890593303\n",
      "Epoch 202, Loss: 0.4719159657967507\n",
      "Epoch 203, Loss: 0.471735017109987\n",
      "Epoch 204, Loss: 0.4714985533244086\n",
      "Epoch 205, Loss: 0.4713143302002759\n",
      "Epoch 206, Loss: 0.47118574544243713\n",
      "Epoch 207, Loss: 0.47112002449718005\n",
      "Epoch 208, Loss: 0.4710910670518786\n",
      "Epoch 209, Loss: 0.47102495326923743\n",
      "Epoch 210, Loss: 0.4709626586360175\n",
      "Epoch 211, Loss: 0.47085840979895394\n",
      "Epoch 212, Loss: 0.47078064719388485\n",
      "Epoch 213, Loss: 0.47066614789276784\n",
      "Epoch 214, Loss: 0.47055654499036753\n",
      "Epoch 215, Loss: 0.47041652390832933\n",
      "Epoch 216, Loss: 0.4702446473560784\n",
      "Epoch 217, Loss: 0.4701045016774931\n",
      "Epoch 218, Loss: 0.47002059077245106\n",
      "Epoch 219, Loss: 0.4699643617672827\n",
      "Epoch 220, Loss: 0.4698888296176042\n",
      "Epoch 221, Loss: 0.4698141575467262\n",
      "Epoch 222, Loss: 0.4697265783615268\n",
      "Epoch 223, Loss: 0.46966066909468496\n",
      "Epoch 224, Loss: 0.4696348739248034\n",
      "Epoch 225, Loss: 0.4697176006517021\n",
      "Epoch 226, Loss: 0.4699424675720642\n",
      "Epoch 227, Loss: 0.46999269112017344\n",
      "Epoch 228, Loss: 0.4698759384724347\n",
      "Epoch 229, Loss: 0.46938041574678274\n",
      "Epoch 230, Loss: 0.4691415581864629\n",
      "Epoch 231, Loss: 0.46924839544758656\n",
      "Epoch 232, Loss: 0.46933531775018905\n",
      "Epoch 233, Loss: 0.4692253881106229\n",
      "Epoch 234, Loss: 0.46894320638835874\n",
      "Epoch 235, Loss: 0.4688628060980674\n",
      "Epoch 236, Loss: 0.46897798398181567\n",
      "Epoch 237, Loss: 0.4689811295666005\n",
      "Epoch 238, Loss: 0.4688031484354944\n",
      "Epoch 239, Loss: 0.46862983752908405\n",
      "Epoch 240, Loss: 0.4685963696810109\n",
      "Epoch 241, Loss: 0.46867106090894\n",
      "Epoch 242, Loss: 0.46866360503981047\n",
      "Epoch 243, Loss: 0.468513635806399\n",
      "Epoch 244, Loss: 0.46835730315983837\n",
      "Epoch 245, Loss: 0.468344910742699\n",
      "Epoch 246, Loss: 0.46842993299443175\n",
      "Epoch 247, Loss: 0.4683803372309361\n",
      "Epoch 248, Loss: 0.46823408027364943\n",
      "Epoch 249, Loss: 0.46809559282800506\n",
      "Epoch 250, Loss: 0.4681194355765096\n",
      "Epoch 251, Loss: 0.4682059468617626\n",
      "Epoch 252, Loss: 0.46815125054377077\n",
      "Epoch 253, Loss: 0.46800465135626707\n",
      "Epoch 254, Loss: 0.4678516840148294\n",
      "Epoch 255, Loss: 0.46781997608436393\n",
      "Epoch 256, Loss: 0.4678506648079015\n",
      "Epoch 257, Loss: 0.4677995105079106\n",
      "Epoch 258, Loss: 0.4676942061377158\n",
      "Epoch 259, Loss: 0.46760336187164103\n",
      "Epoch 260, Loss: 0.46754277859764004\n",
      "Epoch 261, Loss: 0.4675358146830681\n",
      "Epoch 262, Loss: 0.4675934670336611\n",
      "Epoch 263, Loss: 0.4675883044263754\n",
      "Epoch 264, Loss: 0.46749525618568777\n",
      "Epoch 265, Loss: 0.46735425416294335\n",
      "Epoch 266, Loss: 0.4672686694514329\n",
      "Epoch 267, Loss: 0.46723722493080344\n",
      "Epoch 268, Loss: 0.4672375438513713\n",
      "Epoch 269, Loss: 0.46722038561562945\n",
      "Epoch 270, Loss: 0.46721867932539873\n",
      "Epoch 271, Loss: 0.46717319721795353\n",
      "Epoch 272, Loss: 0.4671238189214812\n",
      "Epoch 273, Loss: 0.46705709570299314\n",
      "Epoch 274, Loss: 0.4669974785875546\n",
      "Epoch 275, Loss: 0.4669422896788895\n",
      "Epoch 276, Loss: 0.4669034987743203\n",
      "Epoch 277, Loss: 0.46684804340658115\n",
      "Epoch 278, Loss: 0.4668104417178138\n",
      "Epoch 279, Loss: 0.46677111311176783\n",
      "Epoch 280, Loss: 0.46673031256560493\n",
      "Epoch 281, Loss: 0.46669448942736436\n",
      "Epoch 282, Loss: 0.46666225583115023\n",
      "Epoch 283, Loss: 0.4666367100427352\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.217597380207128\n",
      "Valence RMSE: 0.20513054573126205\n",
      "Arousal RMSE: 0.22938766085821927\n",
      "Test R^2 score: tensor([0.4901, 0.4790], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.3707, 0.3570], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.484518403835765\n",
      "Num of epochs: 284\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Epoch 121, Loss: 0.49295912913079615\n",
      "Epoch 122, Loss: 0.4924773619139838\n",
      "Epoch 123, Loss: 0.49197607191949805\n",
      "Epoch 124, Loss: 0.4914699197831528\n",
      "Epoch 125, Loss: 0.49091483381312667\n",
      "Epoch 126, Loss: 0.49035147677946317\n",
      "Epoch 127, Loss: 0.4898087222825037\n",
      "Epoch 128, Loss: 0.48924014731550836\n",
      "Epoch 129, Loss: 0.48873550647212044\n",
      "Epoch 130, Loss: 0.48832307254971563\n",
      "Epoch 131, Loss: 0.48791418392821295\n",
      "Epoch 132, Loss: 0.4875902245243796\n",
      "Epoch 133, Loss: 0.48728851108037585\n",
      "Epoch 134, Loss: 0.48697062262181345\n",
      "Epoch 135, Loss: 0.4866339553030548\n",
      "Epoch 136, Loss: 0.48633446746734893\n",
      "Epoch 137, Loss: 0.4860310087358514\n",
      "Epoch 138, Loss: 0.4856786412545615\n",
      "Epoch 139, Loss: 0.48531372107004\n",
      "Epoch 140, Loss: 0.48495712985488587\n",
      "Epoch 141, Loss: 0.4846131600652309\n",
      "Epoch 142, Loss: 0.48425966858652364\n",
      "Epoch 143, Loss: 0.4838914149236435\n",
      "Epoch 144, Loss: 0.48352437546282767\n",
      "Epoch 145, Loss: 0.4832299911937133\n",
      "Epoch 146, Loss: 0.4832586529465723\n",
      "Epoch 147, Loss: 0.48356744143113517\n",
      "Epoch 148, Loss: 0.4828108019011922\n",
      "Epoch 149, Loss: 0.4818912939738211\n",
      "Epoch 150, Loss: 0.48239416745575797\n",
      "Epoch 151, Loss: 0.4814952491721961\n",
      "Epoch 152, Loss: 0.4811822818820502\n",
      "Epoch 153, Loss: 0.4812753156716337\n",
      "Epoch 154, Loss: 0.4804387742282827\n",
      "Epoch 155, Loss: 0.48058189052417905\n",
      "Epoch 156, Loss: 0.4801288115497346\n",
      "Epoch 157, Loss: 0.4797465937246525\n",
      "Epoch 158, Loss: 0.4797622479529733\n",
      "Epoch 159, Loss: 0.4791664679844762\n",
      "Epoch 160, Loss: 0.47904359923301915\n",
      "Epoch 161, Loss: 0.4788483533353555\n",
      "Epoch 162, Loss: 0.4783709093142749\n",
      "Epoch 163, Loss: 0.4783027642591491\n",
      "Epoch 164, Loss: 0.4780003946284736\n",
      "Epoch 165, Loss: 0.47766356610156796\n",
      "Epoch 166, Loss: 0.47756940749162413\n",
      "Epoch 167, Loss: 0.4772720526560437\n",
      "Epoch 168, Loss: 0.477001191512605\n",
      "Epoch 169, Loss: 0.47690651155802954\n",
      "Epoch 170, Loss: 0.47663605231839845\n",
      "Epoch 171, Loss: 0.4763715079887003\n",
      "Epoch 172, Loss: 0.4762479807342815\n",
      "Epoch 173, Loss: 0.47603155482909787\n",
      "Epoch 174, Loss: 0.47580085926956445\n",
      "Epoch 175, Loss: 0.4756396378860013\n",
      "Epoch 176, Loss: 0.4754627232666358\n",
      "Epoch 177, Loss: 0.4752513954316692\n",
      "Epoch 178, Loss: 0.47507140350730925\n",
      "Epoch 179, Loss: 0.47492471273973497\n",
      "Epoch 180, Loss: 0.47475493910732763\n",
      "Epoch 181, Loss: 0.47455157018218247\n",
      "Epoch 182, Loss: 0.4743853224635176\n",
      "Epoch 183, Loss: 0.47424550491599726\n",
      "Epoch 184, Loss: 0.47408413178207975\n",
      "Epoch 185, Loss: 0.47390385374022453\n",
      "Epoch 186, Loss: 0.4737383066259127\n",
      "Epoch 187, Loss: 0.47358797785338436\n",
      "Epoch 188, Loss: 0.4734560449826861\n",
      "Epoch 189, Loss: 0.47335738206045025\n",
      "Epoch 190, Loss: 0.4733162834925985\n",
      "Epoch 191, Loss: 0.4732413020737725\n",
      "Epoch 192, Loss: 0.47313239020014447\n",
      "Epoch 193, Loss: 0.4729457158004841\n",
      "Epoch 194, Loss: 0.47274703738102797\n",
      "Epoch 195, Loss: 0.47253750657270627\n",
      "Epoch 196, Loss: 0.4723544299968521\n",
      "Epoch 197, Loss: 0.47221102907746554\n",
      "Epoch 198, Loss: 0.47210844467503205\n",
      "Epoch 199, Loss: 0.47205337980155637\n",
      "Epoch 200, Loss: 0.47204973383515075\n",
      "Epoch 201, Loss: 0.4720427890593303\n",
      "Epoch 202, Loss: 0.4719159657967507\n",
      "Epoch 203, Loss: 0.471735017109987\n",
      "Epoch 204, Loss: 0.4714985533244086\n",
      "Epoch 205, Loss: 0.4713143302002759\n",
      "Epoch 206, Loss: 0.47118574544243713\n",
      "Epoch 207, Loss: 0.47112002449718005\n",
      "Epoch 208, Loss: 0.4710910670518786\n",
      "Epoch 209, Loss: 0.47102495326923743\n",
      "Epoch 210, Loss: 0.4709626586360175\n",
      "Epoch 211, Loss: 0.47085840979895394\n",
      "Epoch 212, Loss: 0.47078064719388485\n",
      "Epoch 213, Loss: 0.47066614789276784\n",
      "Epoch 214, Loss: 0.47055654499036753\n",
      "Epoch 215, Loss: 0.47041652390832933\n",
      "Epoch 216, Loss: 0.4702446473560784\n",
      "Epoch 217, Loss: 0.4701045016774931\n",
      "Epoch 218, Loss: 0.47002059077245106\n",
      "Epoch 219, Loss: 0.4699643617672827\n",
      "Epoch 220, Loss: 0.4698888296176042\n",
      "Epoch 221, Loss: 0.4698141575467262\n",
      "Epoch 222, Loss: 0.4697265783615268\n",
      "Epoch 223, Loss: 0.46966066909468496\n",
      "Epoch 224, Loss: 0.4696348739248034\n",
      "Epoch 225, Loss: 0.4697176006517021\n",
      "Epoch 226, Loss: 0.4699424675720642\n",
      "Epoch 227, Loss: 0.46999269112017344\n",
      "Epoch 228, Loss: 0.4698759384724347\n",
      "Epoch 229, Loss: 0.46938041574678274\n",
      "Epoch 230, Loss: 0.4691415581864629\n",
      "Epoch 231, Loss: 0.46924839544758656\n",
      "Epoch 232, Loss: 0.46933531775018905\n",
      "Epoch 233, Loss: 0.4692253881106229\n",
      "Epoch 234, Loss: 0.46894320638835874\n",
      "Epoch 235, Loss: 0.4688628060980674\n",
      "Epoch 236, Loss: 0.46897798398181567\n",
      "Epoch 237, Loss: 0.4689811295666005\n",
      "Epoch 238, Loss: 0.4688031484354944\n",
      "Epoch 239, Loss: 0.46862983752908405\n",
      "Epoch 240, Loss: 0.4685963696810109\n",
      "Epoch 241, Loss: 0.46867106090894\n",
      "Epoch 242, Loss: 0.46866360503981047\n",
      "Epoch 243, Loss: 0.468513635806399\n",
      "Epoch 244, Loss: 0.46835730315983837\n",
      "Epoch 245, Loss: 0.468344910742699\n",
      "Epoch 246, Loss: 0.46842993299443175\n",
      "Epoch 247, Loss: 0.4683803372309361\n",
      "Epoch 248, Loss: 0.46823408027364943\n",
      "Epoch 249, Loss: 0.46809559282800506\n",
      "Epoch 250, Loss: 0.4681194355765096\n",
      "Epoch 251, Loss: 0.4682059468617626\n",
      "Epoch 252, Loss: 0.46815125054377077\n",
      "Epoch 253, Loss: 0.46800465135626707\n",
      "Epoch 254, Loss: 0.4678516840148294\n",
      "Epoch 255, Loss: 0.46781997608436393\n",
      "Epoch 256, Loss: 0.4678506648079015\n",
      "Epoch 257, Loss: 0.4677995105079106\n",
      "Epoch 258, Loss: 0.4676942061377158\n",
      "Epoch 259, Loss: 0.46760336187164103\n",
      "Epoch 260, Loss: 0.46754277859764004\n",
      "Epoch 261, Loss: 0.4675358146830681\n",
      "Epoch 262, Loss: 0.4675934670336611\n",
      "Epoch 263, Loss: 0.4675883044263754\n",
      "Epoch 264, Loss: 0.46749525618568777\n",
      "Epoch 265, Loss: 0.46735425416294335\n",
      "Epoch 266, Loss: 0.4672686694514329\n",
      "Epoch 267, Loss: 0.46723722493080344\n",
      "Epoch 268, Loss: 0.4672375438513713\n",
      "Epoch 269, Loss: 0.46722038561562945\n",
      "Epoch 270, Loss: 0.46721867932539873\n",
      "Epoch 271, Loss: 0.46717319721795353\n",
      "Epoch 272, Loss: 0.4671238189214812\n",
      "Epoch 273, Loss: 0.46705709570299314\n",
      "Epoch 274, Loss: 0.4669974785875546\n",
      "Epoch 275, Loss: 0.4669422896788895\n",
      "Epoch 276, Loss: 0.4669034987743203\n",
      "Epoch 277, Loss: 0.46684804340658115\n",
      "Epoch 278, Loss: 0.4668104417178138\n",
      "Epoch 279, Loss: 0.46677111311176783\n",
      "Epoch 280, Loss: 0.46673031256560493\n",
      "Epoch 281, Loss: 0.46669448942736436\n",
      "Epoch 282, Loss: 0.46666225583115023\n",
      "Epoch 283, Loss: 0.4666367100427352\n",
      "Epoch 284, Loss: 0.4666213020629852\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2182960514411706\n",
      "Valence RMSE: 0.20578417765328272\n",
      "Arousal RMSE: 0.2301286691769945\n",
      "Test R^2 score: tensor([0.4868, 0.4756], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.3667, 0.3529], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.48120511367728164\n",
      "Num of epochs: 285\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Epoch 121, Loss: 0.49295912913079615\n",
      "Epoch 122, Loss: 0.4924773619139838\n",
      "Epoch 123, Loss: 0.49197607191949805\n",
      "Epoch 124, Loss: 0.4914699197831528\n",
      "Epoch 125, Loss: 0.49091483381312667\n",
      "Epoch 126, Loss: 0.49035147677946317\n",
      "Epoch 127, Loss: 0.4898087222825037\n",
      "Epoch 128, Loss: 0.48924014731550836\n",
      "Epoch 129, Loss: 0.48873550647212044\n",
      "Epoch 130, Loss: 0.48832307254971563\n",
      "Epoch 131, Loss: 0.48791418392821295\n",
      "Epoch 132, Loss: 0.4875902245243796\n",
      "Epoch 133, Loss: 0.48728851108037585\n",
      "Epoch 134, Loss: 0.48697062262181345\n",
      "Epoch 135, Loss: 0.4866339553030548\n",
      "Epoch 136, Loss: 0.48633446746734893\n",
      "Epoch 137, Loss: 0.4860310087358514\n",
      "Epoch 138, Loss: 0.4856786412545615\n",
      "Epoch 139, Loss: 0.48531372107004\n",
      "Epoch 140, Loss: 0.48495712985488587\n",
      "Epoch 141, Loss: 0.4846131600652309\n",
      "Epoch 142, Loss: 0.48425966858652364\n",
      "Epoch 143, Loss: 0.4838914149236435\n",
      "Epoch 144, Loss: 0.48352437546282767\n",
      "Epoch 145, Loss: 0.4832299911937133\n",
      "Epoch 146, Loss: 0.4832586529465723\n",
      "Epoch 147, Loss: 0.48356744143113517\n",
      "Epoch 148, Loss: 0.4828108019011922\n",
      "Epoch 149, Loss: 0.4818912939738211\n",
      "Epoch 150, Loss: 0.48239416745575797\n",
      "Epoch 151, Loss: 0.4814952491721961\n",
      "Epoch 152, Loss: 0.4811822818820502\n",
      "Epoch 153, Loss: 0.4812753156716337\n",
      "Epoch 154, Loss: 0.4804387742282827\n",
      "Epoch 155, Loss: 0.48058189052417905\n",
      "Epoch 156, Loss: 0.4801288115497346\n",
      "Epoch 157, Loss: 0.4797465937246525\n",
      "Epoch 158, Loss: 0.4797622479529733\n",
      "Epoch 159, Loss: 0.4791664679844762\n",
      "Epoch 160, Loss: 0.47904359923301915\n",
      "Epoch 161, Loss: 0.4788483533353555\n",
      "Epoch 162, Loss: 0.4783709093142749\n",
      "Epoch 163, Loss: 0.4783027642591491\n",
      "Epoch 164, Loss: 0.4780003946284736\n",
      "Epoch 165, Loss: 0.47766356610156796\n",
      "Epoch 166, Loss: 0.47756940749162413\n",
      "Epoch 167, Loss: 0.4772720526560437\n",
      "Epoch 168, Loss: 0.477001191512605\n",
      "Epoch 169, Loss: 0.47690651155802954\n",
      "Epoch 170, Loss: 0.47663605231839845\n",
      "Epoch 171, Loss: 0.4763715079887003\n",
      "Epoch 172, Loss: 0.4762479807342815\n",
      "Epoch 173, Loss: 0.47603155482909787\n",
      "Epoch 174, Loss: 0.47580085926956445\n",
      "Epoch 175, Loss: 0.4756396378860013\n",
      "Epoch 176, Loss: 0.4754627232666358\n",
      "Epoch 177, Loss: 0.4752513954316692\n",
      "Epoch 178, Loss: 0.47507140350730925\n",
      "Epoch 179, Loss: 0.47492471273973497\n",
      "Epoch 180, Loss: 0.47475493910732763\n",
      "Epoch 181, Loss: 0.47455157018218247\n",
      "Epoch 182, Loss: 0.4743853224635176\n",
      "Epoch 183, Loss: 0.47424550491599726\n",
      "Epoch 184, Loss: 0.47408413178207975\n",
      "Epoch 185, Loss: 0.47390385374022453\n",
      "Epoch 186, Loss: 0.4737383066259127\n",
      "Epoch 187, Loss: 0.47358797785338436\n",
      "Epoch 188, Loss: 0.4734560449826861\n",
      "Epoch 189, Loss: 0.47335738206045025\n",
      "Epoch 190, Loss: 0.4733162834925985\n",
      "Epoch 191, Loss: 0.4732413020737725\n",
      "Epoch 192, Loss: 0.47313239020014447\n",
      "Epoch 193, Loss: 0.4729457158004841\n",
      "Epoch 194, Loss: 0.47274703738102797\n",
      "Epoch 195, Loss: 0.47253750657270627\n",
      "Epoch 196, Loss: 0.4723544299968521\n",
      "Epoch 197, Loss: 0.47221102907746554\n",
      "Epoch 198, Loss: 0.47210844467503205\n",
      "Epoch 199, Loss: 0.47205337980155637\n",
      "Epoch 200, Loss: 0.47204973383515075\n",
      "Epoch 201, Loss: 0.4720427890593303\n",
      "Epoch 202, Loss: 0.4719159657967507\n",
      "Epoch 203, Loss: 0.471735017109987\n",
      "Epoch 204, Loss: 0.4714985533244086\n",
      "Epoch 205, Loss: 0.4713143302002759\n",
      "Epoch 206, Loss: 0.47118574544243713\n",
      "Epoch 207, Loss: 0.47112002449718005\n",
      "Epoch 208, Loss: 0.4710910670518786\n",
      "Epoch 209, Loss: 0.47102495326923743\n",
      "Epoch 210, Loss: 0.4709626586360175\n",
      "Epoch 211, Loss: 0.47085840979895394\n",
      "Epoch 212, Loss: 0.47078064719388485\n",
      "Epoch 213, Loss: 0.47066614789276784\n",
      "Epoch 214, Loss: 0.47055654499036753\n",
      "Epoch 215, Loss: 0.47041652390832933\n",
      "Epoch 216, Loss: 0.4702446473560784\n",
      "Epoch 217, Loss: 0.4701045016774931\n",
      "Epoch 218, Loss: 0.47002059077245106\n",
      "Epoch 219, Loss: 0.4699643617672827\n",
      "Epoch 220, Loss: 0.4698888296176042\n",
      "Epoch 221, Loss: 0.4698141575467262\n",
      "Epoch 222, Loss: 0.4697265783615268\n",
      "Epoch 223, Loss: 0.46966066909468496\n",
      "Epoch 224, Loss: 0.4696348739248034\n",
      "Epoch 225, Loss: 0.4697176006517021\n",
      "Epoch 226, Loss: 0.4699424675720642\n",
      "Epoch 227, Loss: 0.46999269112017344\n",
      "Epoch 228, Loss: 0.4698759384724347\n",
      "Epoch 229, Loss: 0.46938041574678274\n",
      "Epoch 230, Loss: 0.4691415581864629\n",
      "Epoch 231, Loss: 0.46924839544758656\n",
      "Epoch 232, Loss: 0.46933531775018905\n",
      "Epoch 233, Loss: 0.4692253881106229\n",
      "Epoch 234, Loss: 0.46894320638835874\n",
      "Epoch 235, Loss: 0.4688628060980674\n",
      "Epoch 236, Loss: 0.46897798398181567\n",
      "Epoch 237, Loss: 0.4689811295666005\n",
      "Epoch 238, Loss: 0.4688031484354944\n",
      "Epoch 239, Loss: 0.46862983752908405\n",
      "Epoch 240, Loss: 0.4685963696810109\n",
      "Epoch 241, Loss: 0.46867106090894\n",
      "Epoch 242, Loss: 0.46866360503981047\n",
      "Epoch 243, Loss: 0.468513635806399\n",
      "Epoch 244, Loss: 0.46835730315983837\n",
      "Epoch 245, Loss: 0.468344910742699\n",
      "Epoch 246, Loss: 0.46842993299443175\n",
      "Epoch 247, Loss: 0.4683803372309361\n",
      "Epoch 248, Loss: 0.46823408027364943\n",
      "Epoch 249, Loss: 0.46809559282800506\n",
      "Epoch 250, Loss: 0.4681194355765096\n",
      "Epoch 251, Loss: 0.4682059468617626\n",
      "Epoch 252, Loss: 0.46815125054377077\n",
      "Epoch 253, Loss: 0.46800465135626707\n",
      "Epoch 254, Loss: 0.4678516840148294\n",
      "Epoch 255, Loss: 0.46781997608436393\n",
      "Epoch 256, Loss: 0.4678506648079015\n",
      "Epoch 257, Loss: 0.4677995105079106\n",
      "Epoch 258, Loss: 0.4676942061377158\n",
      "Epoch 259, Loss: 0.46760336187164103\n",
      "Epoch 260, Loss: 0.46754277859764004\n",
      "Epoch 261, Loss: 0.4675358146830681\n",
      "Epoch 262, Loss: 0.4675934670336611\n",
      "Epoch 263, Loss: 0.4675883044263754\n",
      "Epoch 264, Loss: 0.46749525618568777\n",
      "Epoch 265, Loss: 0.46735425416294335\n",
      "Epoch 266, Loss: 0.4672686694514329\n",
      "Epoch 267, Loss: 0.46723722493080344\n",
      "Epoch 268, Loss: 0.4672375438513713\n",
      "Epoch 269, Loss: 0.46722038561562945\n",
      "Epoch 270, Loss: 0.46721867932539873\n",
      "Epoch 271, Loss: 0.46717319721795353\n",
      "Epoch 272, Loss: 0.4671238189214812\n",
      "Epoch 273, Loss: 0.46705709570299314\n",
      "Epoch 274, Loss: 0.4669974785875546\n",
      "Epoch 275, Loss: 0.4669422896788895\n",
      "Epoch 276, Loss: 0.4669034987743203\n",
      "Epoch 277, Loss: 0.46684804340658115\n",
      "Epoch 278, Loss: 0.4668104417178138\n",
      "Epoch 279, Loss: 0.46677111311176783\n",
      "Epoch 280, Loss: 0.46673031256560493\n",
      "Epoch 281, Loss: 0.46669448942736436\n",
      "Epoch 282, Loss: 0.46666225583115023\n",
      "Epoch 283, Loss: 0.4666367100427352\n",
      "Epoch 284, Loss: 0.4666213020629852\n",
      "Epoch 285, Loss: 0.46662540558501797\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.21733620262863185\n",
      "Valence RMSE: 0.20491173233019133\n",
      "Arousal RMSE: 0.2290878257339474\n",
      "Test R^2 score: tensor([0.4911, 0.4803], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.3720, 0.3587], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4857426560316928\n",
      "Num of epochs: 286\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Epoch 121, Loss: 0.49295912913079615\n",
      "Epoch 122, Loss: 0.4924773619139838\n",
      "Epoch 123, Loss: 0.49197607191949805\n",
      "Epoch 124, Loss: 0.4914699197831528\n",
      "Epoch 125, Loss: 0.49091483381312667\n",
      "Epoch 126, Loss: 0.49035147677946317\n",
      "Epoch 127, Loss: 0.4898087222825037\n",
      "Epoch 128, Loss: 0.48924014731550836\n",
      "Epoch 129, Loss: 0.48873550647212044\n",
      "Epoch 130, Loss: 0.48832307254971563\n",
      "Epoch 131, Loss: 0.48791418392821295\n",
      "Epoch 132, Loss: 0.4875902245243796\n",
      "Epoch 133, Loss: 0.48728851108037585\n",
      "Epoch 134, Loss: 0.48697062262181345\n",
      "Epoch 135, Loss: 0.4866339553030548\n",
      "Epoch 136, Loss: 0.48633446746734893\n",
      "Epoch 137, Loss: 0.4860310087358514\n",
      "Epoch 138, Loss: 0.4856786412545615\n",
      "Epoch 139, Loss: 0.48531372107004\n",
      "Epoch 140, Loss: 0.48495712985488587\n",
      "Epoch 141, Loss: 0.4846131600652309\n",
      "Epoch 142, Loss: 0.48425966858652364\n",
      "Epoch 143, Loss: 0.4838914149236435\n",
      "Epoch 144, Loss: 0.48352437546282767\n",
      "Epoch 145, Loss: 0.4832299911937133\n",
      "Epoch 146, Loss: 0.4832586529465723\n",
      "Epoch 147, Loss: 0.48356744143113517\n",
      "Epoch 148, Loss: 0.4828108019011922\n",
      "Epoch 149, Loss: 0.4818912939738211\n",
      "Epoch 150, Loss: 0.48239416745575797\n",
      "Epoch 151, Loss: 0.4814952491721961\n",
      "Epoch 152, Loss: 0.4811822818820502\n",
      "Epoch 153, Loss: 0.4812753156716337\n",
      "Epoch 154, Loss: 0.4804387742282827\n",
      "Epoch 155, Loss: 0.48058189052417905\n",
      "Epoch 156, Loss: 0.4801288115497346\n",
      "Epoch 157, Loss: 0.4797465937246525\n",
      "Epoch 158, Loss: 0.4797622479529733\n",
      "Epoch 159, Loss: 0.4791664679844762\n",
      "Epoch 160, Loss: 0.47904359923301915\n",
      "Epoch 161, Loss: 0.4788483533353555\n",
      "Epoch 162, Loss: 0.4783709093142749\n",
      "Epoch 163, Loss: 0.4783027642591491\n",
      "Epoch 164, Loss: 0.4780003946284736\n",
      "Epoch 165, Loss: 0.47766356610156796\n",
      "Epoch 166, Loss: 0.47756940749162413\n",
      "Epoch 167, Loss: 0.4772720526560437\n",
      "Epoch 168, Loss: 0.477001191512605\n",
      "Epoch 169, Loss: 0.47690651155802954\n",
      "Epoch 170, Loss: 0.47663605231839845\n",
      "Epoch 171, Loss: 0.4763715079887003\n",
      "Epoch 172, Loss: 0.4762479807342815\n",
      "Epoch 173, Loss: 0.47603155482909787\n",
      "Epoch 174, Loss: 0.47580085926956445\n",
      "Epoch 175, Loss: 0.4756396378860013\n",
      "Epoch 176, Loss: 0.4754627232666358\n",
      "Epoch 177, Loss: 0.4752513954316692\n",
      "Epoch 178, Loss: 0.47507140350730925\n",
      "Epoch 179, Loss: 0.47492471273973497\n",
      "Epoch 180, Loss: 0.47475493910732763\n",
      "Epoch 181, Loss: 0.47455157018218247\n",
      "Epoch 182, Loss: 0.4743853224635176\n",
      "Epoch 183, Loss: 0.47424550491599726\n",
      "Epoch 184, Loss: 0.47408413178207975\n",
      "Epoch 185, Loss: 0.47390385374022453\n",
      "Epoch 186, Loss: 0.4737383066259127\n",
      "Epoch 187, Loss: 0.47358797785338436\n",
      "Epoch 188, Loss: 0.4734560449826861\n",
      "Epoch 189, Loss: 0.47335738206045025\n",
      "Epoch 190, Loss: 0.4733162834925985\n",
      "Epoch 191, Loss: 0.4732413020737725\n",
      "Epoch 192, Loss: 0.47313239020014447\n",
      "Epoch 193, Loss: 0.4729457158004841\n",
      "Epoch 194, Loss: 0.47274703738102797\n",
      "Epoch 195, Loss: 0.47253750657270627\n",
      "Epoch 196, Loss: 0.4723544299968521\n",
      "Epoch 197, Loss: 0.47221102907746554\n",
      "Epoch 198, Loss: 0.47210844467503205\n",
      "Epoch 199, Loss: 0.47205337980155637\n",
      "Epoch 200, Loss: 0.47204973383515075\n",
      "Epoch 201, Loss: 0.4720427890593303\n",
      "Epoch 202, Loss: 0.4719159657967507\n",
      "Epoch 203, Loss: 0.471735017109987\n",
      "Epoch 204, Loss: 0.4714985533244086\n",
      "Epoch 205, Loss: 0.4713143302002759\n",
      "Epoch 206, Loss: 0.47118574544243713\n",
      "Epoch 207, Loss: 0.47112002449718005\n",
      "Epoch 208, Loss: 0.4710910670518786\n",
      "Epoch 209, Loss: 0.47102495326923743\n",
      "Epoch 210, Loss: 0.4709626586360175\n",
      "Epoch 211, Loss: 0.47085840979895394\n",
      "Epoch 212, Loss: 0.47078064719388485\n",
      "Epoch 213, Loss: 0.47066614789276784\n",
      "Epoch 214, Loss: 0.47055654499036753\n",
      "Epoch 215, Loss: 0.47041652390832933\n",
      "Epoch 216, Loss: 0.4702446473560784\n",
      "Epoch 217, Loss: 0.4701045016774931\n",
      "Epoch 218, Loss: 0.47002059077245106\n",
      "Epoch 219, Loss: 0.4699643617672827\n",
      "Epoch 220, Loss: 0.4698888296176042\n",
      "Epoch 221, Loss: 0.4698141575467262\n",
      "Epoch 222, Loss: 0.4697265783615268\n",
      "Epoch 223, Loss: 0.46966066909468496\n",
      "Epoch 224, Loss: 0.4696348739248034\n",
      "Epoch 225, Loss: 0.4697176006517021\n",
      "Epoch 226, Loss: 0.4699424675720642\n",
      "Epoch 227, Loss: 0.46999269112017344\n",
      "Epoch 228, Loss: 0.4698759384724347\n",
      "Epoch 229, Loss: 0.46938041574678274\n",
      "Epoch 230, Loss: 0.4691415581864629\n",
      "Epoch 231, Loss: 0.46924839544758656\n",
      "Epoch 232, Loss: 0.46933531775018905\n",
      "Epoch 233, Loss: 0.4692253881106229\n",
      "Epoch 234, Loss: 0.46894320638835874\n",
      "Epoch 235, Loss: 0.4688628060980674\n",
      "Epoch 236, Loss: 0.46897798398181567\n",
      "Epoch 237, Loss: 0.4689811295666005\n",
      "Epoch 238, Loss: 0.4688031484354944\n",
      "Epoch 239, Loss: 0.46862983752908405\n",
      "Epoch 240, Loss: 0.4685963696810109\n",
      "Epoch 241, Loss: 0.46867106090894\n",
      "Epoch 242, Loss: 0.46866360503981047\n",
      "Epoch 243, Loss: 0.468513635806399\n",
      "Epoch 244, Loss: 0.46835730315983837\n",
      "Epoch 245, Loss: 0.468344910742699\n",
      "Epoch 246, Loss: 0.46842993299443175\n",
      "Epoch 247, Loss: 0.4683803372309361\n",
      "Epoch 248, Loss: 0.46823408027364943\n",
      "Epoch 249, Loss: 0.46809559282800506\n",
      "Epoch 250, Loss: 0.4681194355765096\n",
      "Epoch 251, Loss: 0.4682059468617626\n",
      "Epoch 252, Loss: 0.46815125054377077\n",
      "Epoch 253, Loss: 0.46800465135626707\n",
      "Epoch 254, Loss: 0.4678516840148294\n",
      "Epoch 255, Loss: 0.46781997608436393\n",
      "Epoch 256, Loss: 0.4678506648079015\n",
      "Epoch 257, Loss: 0.4677995105079106\n",
      "Epoch 258, Loss: 0.4676942061377158\n",
      "Epoch 259, Loss: 0.46760336187164103\n",
      "Epoch 260, Loss: 0.46754277859764004\n",
      "Epoch 261, Loss: 0.4675358146830681\n",
      "Epoch 262, Loss: 0.4675934670336611\n",
      "Epoch 263, Loss: 0.4675883044263754\n",
      "Epoch 264, Loss: 0.46749525618568777\n",
      "Epoch 265, Loss: 0.46735425416294335\n",
      "Epoch 266, Loss: 0.4672686694514329\n",
      "Epoch 267, Loss: 0.46723722493080344\n",
      "Epoch 268, Loss: 0.4672375438513713\n",
      "Epoch 269, Loss: 0.46722038561562945\n",
      "Epoch 270, Loss: 0.46721867932539873\n",
      "Epoch 271, Loss: 0.46717319721795353\n",
      "Epoch 272, Loss: 0.4671238189214812\n",
      "Epoch 273, Loss: 0.46705709570299314\n",
      "Epoch 274, Loss: 0.4669974785875546\n",
      "Epoch 275, Loss: 0.4669422896788895\n",
      "Epoch 276, Loss: 0.4669034987743203\n",
      "Epoch 277, Loss: 0.46684804340658115\n",
      "Epoch 278, Loss: 0.4668104417178138\n",
      "Epoch 279, Loss: 0.46677111311176783\n",
      "Epoch 280, Loss: 0.46673031256560493\n",
      "Epoch 281, Loss: 0.46669448942736436\n",
      "Epoch 282, Loss: 0.46666225583115023\n",
      "Epoch 283, Loss: 0.4666367100427352\n",
      "Epoch 284, Loss: 0.4666213020629852\n",
      "Epoch 285, Loss: 0.46662540558501797\n",
      "Epoch 286, Loss: 0.4666499301628323\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.21868796277040248\n",
      "Valence RMSE: 0.2062021302271728\n",
      "Arousal RMSE: 0.23049844166743072\n",
      "Test R^2 score: tensor([0.4847, 0.4739], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.3641, 0.3508], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4793184613535083\n",
      "Num of epochs: 287\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Epoch 121, Loss: 0.49295912913079615\n",
      "Epoch 122, Loss: 0.4924773619139838\n",
      "Epoch 123, Loss: 0.49197607191949805\n",
      "Epoch 124, Loss: 0.4914699197831528\n",
      "Epoch 125, Loss: 0.49091483381312667\n",
      "Epoch 126, Loss: 0.49035147677946317\n",
      "Epoch 127, Loss: 0.4898087222825037\n",
      "Epoch 128, Loss: 0.48924014731550836\n",
      "Epoch 129, Loss: 0.48873550647212044\n",
      "Epoch 130, Loss: 0.48832307254971563\n",
      "Epoch 131, Loss: 0.48791418392821295\n",
      "Epoch 132, Loss: 0.4875902245243796\n",
      "Epoch 133, Loss: 0.48728851108037585\n",
      "Epoch 134, Loss: 0.48697062262181345\n",
      "Epoch 135, Loss: 0.4866339553030548\n",
      "Epoch 136, Loss: 0.48633446746734893\n",
      "Epoch 137, Loss: 0.4860310087358514\n",
      "Epoch 138, Loss: 0.4856786412545615\n",
      "Epoch 139, Loss: 0.48531372107004\n",
      "Epoch 140, Loss: 0.48495712985488587\n",
      "Epoch 141, Loss: 0.4846131600652309\n",
      "Epoch 142, Loss: 0.48425966858652364\n",
      "Epoch 143, Loss: 0.4838914149236435\n",
      "Epoch 144, Loss: 0.48352437546282767\n",
      "Epoch 145, Loss: 0.4832299911937133\n",
      "Epoch 146, Loss: 0.4832586529465723\n",
      "Epoch 147, Loss: 0.48356744143113517\n",
      "Epoch 148, Loss: 0.4828108019011922\n",
      "Epoch 149, Loss: 0.4818912939738211\n",
      "Epoch 150, Loss: 0.48239416745575797\n",
      "Epoch 151, Loss: 0.4814952491721961\n",
      "Epoch 152, Loss: 0.4811822818820502\n",
      "Epoch 153, Loss: 0.4812753156716337\n",
      "Epoch 154, Loss: 0.4804387742282827\n",
      "Epoch 155, Loss: 0.48058189052417905\n",
      "Epoch 156, Loss: 0.4801288115497346\n",
      "Epoch 157, Loss: 0.4797465937246525\n",
      "Epoch 158, Loss: 0.4797622479529733\n",
      "Epoch 159, Loss: 0.4791664679844762\n",
      "Epoch 160, Loss: 0.47904359923301915\n",
      "Epoch 161, Loss: 0.4788483533353555\n",
      "Epoch 162, Loss: 0.4783709093142749\n",
      "Epoch 163, Loss: 0.4783027642591491\n",
      "Epoch 164, Loss: 0.4780003946284736\n",
      "Epoch 165, Loss: 0.47766356610156796\n",
      "Epoch 166, Loss: 0.47756940749162413\n",
      "Epoch 167, Loss: 0.4772720526560437\n",
      "Epoch 168, Loss: 0.477001191512605\n",
      "Epoch 169, Loss: 0.47690651155802954\n",
      "Epoch 170, Loss: 0.47663605231839845\n",
      "Epoch 171, Loss: 0.4763715079887003\n",
      "Epoch 172, Loss: 0.4762479807342815\n",
      "Epoch 173, Loss: 0.47603155482909787\n",
      "Epoch 174, Loss: 0.47580085926956445\n",
      "Epoch 175, Loss: 0.4756396378860013\n",
      "Epoch 176, Loss: 0.4754627232666358\n",
      "Epoch 177, Loss: 0.4752513954316692\n",
      "Epoch 178, Loss: 0.47507140350730925\n",
      "Epoch 179, Loss: 0.47492471273973497\n",
      "Epoch 180, Loss: 0.47475493910732763\n",
      "Epoch 181, Loss: 0.47455157018218247\n",
      "Epoch 182, Loss: 0.4743853224635176\n",
      "Epoch 183, Loss: 0.47424550491599726\n",
      "Epoch 184, Loss: 0.47408413178207975\n",
      "Epoch 185, Loss: 0.47390385374022453\n",
      "Epoch 186, Loss: 0.4737383066259127\n",
      "Epoch 187, Loss: 0.47358797785338436\n",
      "Epoch 188, Loss: 0.4734560449826861\n",
      "Epoch 189, Loss: 0.47335738206045025\n",
      "Epoch 190, Loss: 0.4733162834925985\n",
      "Epoch 191, Loss: 0.4732413020737725\n",
      "Epoch 192, Loss: 0.47313239020014447\n",
      "Epoch 193, Loss: 0.4729457158004841\n",
      "Epoch 194, Loss: 0.47274703738102797\n",
      "Epoch 195, Loss: 0.47253750657270627\n",
      "Epoch 196, Loss: 0.4723544299968521\n",
      "Epoch 197, Loss: 0.47221102907746554\n",
      "Epoch 198, Loss: 0.47210844467503205\n",
      "Epoch 199, Loss: 0.47205337980155637\n",
      "Epoch 200, Loss: 0.47204973383515075\n",
      "Epoch 201, Loss: 0.4720427890593303\n",
      "Epoch 202, Loss: 0.4719159657967507\n",
      "Epoch 203, Loss: 0.471735017109987\n",
      "Epoch 204, Loss: 0.4714985533244086\n",
      "Epoch 205, Loss: 0.4713143302002759\n",
      "Epoch 206, Loss: 0.47118574544243713\n",
      "Epoch 207, Loss: 0.47112002449718005\n",
      "Epoch 208, Loss: 0.4710910670518786\n",
      "Epoch 209, Loss: 0.47102495326923743\n",
      "Epoch 210, Loss: 0.4709626586360175\n",
      "Epoch 211, Loss: 0.47085840979895394\n",
      "Epoch 212, Loss: 0.47078064719388485\n",
      "Epoch 213, Loss: 0.47066614789276784\n",
      "Epoch 214, Loss: 0.47055654499036753\n",
      "Epoch 215, Loss: 0.47041652390832933\n",
      "Epoch 216, Loss: 0.4702446473560784\n",
      "Epoch 217, Loss: 0.4701045016774931\n",
      "Epoch 218, Loss: 0.47002059077245106\n",
      "Epoch 219, Loss: 0.4699643617672827\n",
      "Epoch 220, Loss: 0.4698888296176042\n",
      "Epoch 221, Loss: 0.4698141575467262\n",
      "Epoch 222, Loss: 0.4697265783615268\n",
      "Epoch 223, Loss: 0.46966066909468496\n",
      "Epoch 224, Loss: 0.4696348739248034\n",
      "Epoch 225, Loss: 0.4697176006517021\n",
      "Epoch 226, Loss: 0.4699424675720642\n",
      "Epoch 227, Loss: 0.46999269112017344\n",
      "Epoch 228, Loss: 0.4698759384724347\n",
      "Epoch 229, Loss: 0.46938041574678274\n",
      "Epoch 230, Loss: 0.4691415581864629\n",
      "Epoch 231, Loss: 0.46924839544758656\n",
      "Epoch 232, Loss: 0.46933531775018905\n",
      "Epoch 233, Loss: 0.4692253881106229\n",
      "Epoch 234, Loss: 0.46894320638835874\n",
      "Epoch 235, Loss: 0.4688628060980674\n",
      "Epoch 236, Loss: 0.46897798398181567\n",
      "Epoch 237, Loss: 0.4689811295666005\n",
      "Epoch 238, Loss: 0.4688031484354944\n",
      "Epoch 239, Loss: 0.46862983752908405\n",
      "Epoch 240, Loss: 0.4685963696810109\n",
      "Epoch 241, Loss: 0.46867106090894\n",
      "Epoch 242, Loss: 0.46866360503981047\n",
      "Epoch 243, Loss: 0.468513635806399\n",
      "Epoch 244, Loss: 0.46835730315983837\n",
      "Epoch 245, Loss: 0.468344910742699\n",
      "Epoch 246, Loss: 0.46842993299443175\n",
      "Epoch 247, Loss: 0.4683803372309361\n",
      "Epoch 248, Loss: 0.46823408027364943\n",
      "Epoch 249, Loss: 0.46809559282800506\n",
      "Epoch 250, Loss: 0.4681194355765096\n",
      "Epoch 251, Loss: 0.4682059468617626\n",
      "Epoch 252, Loss: 0.46815125054377077\n",
      "Epoch 253, Loss: 0.46800465135626707\n",
      "Epoch 254, Loss: 0.4678516840148294\n",
      "Epoch 255, Loss: 0.46781997608436393\n",
      "Epoch 256, Loss: 0.4678506648079015\n",
      "Epoch 257, Loss: 0.4677995105079106\n",
      "Epoch 258, Loss: 0.4676942061377158\n",
      "Epoch 259, Loss: 0.46760336187164103\n",
      "Epoch 260, Loss: 0.46754277859764004\n",
      "Epoch 261, Loss: 0.4675358146830681\n",
      "Epoch 262, Loss: 0.4675934670336611\n",
      "Epoch 263, Loss: 0.4675883044263754\n",
      "Epoch 264, Loss: 0.46749525618568777\n",
      "Epoch 265, Loss: 0.46735425416294335\n",
      "Epoch 266, Loss: 0.4672686694514329\n",
      "Epoch 267, Loss: 0.46723722493080344\n",
      "Epoch 268, Loss: 0.4672375438513713\n",
      "Epoch 269, Loss: 0.46722038561562945\n",
      "Epoch 270, Loss: 0.46721867932539873\n",
      "Epoch 271, Loss: 0.46717319721795353\n",
      "Epoch 272, Loss: 0.4671238189214812\n",
      "Epoch 273, Loss: 0.46705709570299314\n",
      "Epoch 274, Loss: 0.4669974785875546\n",
      "Epoch 275, Loss: 0.4669422896788895\n",
      "Epoch 276, Loss: 0.4669034987743203\n",
      "Epoch 277, Loss: 0.46684804340658115\n",
      "Epoch 278, Loss: 0.4668104417178138\n",
      "Epoch 279, Loss: 0.46677111311176783\n",
      "Epoch 280, Loss: 0.46673031256560493\n",
      "Epoch 281, Loss: 0.46669448942736436\n",
      "Epoch 282, Loss: 0.46666225583115023\n",
      "Epoch 283, Loss: 0.4666367100427352\n",
      "Epoch 284, Loss: 0.4666213020629852\n",
      "Epoch 285, Loss: 0.46662540558501797\n",
      "Epoch 286, Loss: 0.4666499301628323\n",
      "Epoch 287, Loss: 0.466693164365506\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.21711187948645422\n",
      "Valence RMSE: 0.20475178921497483\n",
      "Arousal RMSE: 0.22880524740824396\n",
      "Test R^2 score: tensor([0.4919, 0.4816], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.3730, 0.3603], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4867802841214398\n",
      "Num of epochs: 288\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Epoch 121, Loss: 0.49295912913079615\n",
      "Epoch 122, Loss: 0.4924773619139838\n",
      "Epoch 123, Loss: 0.49197607191949805\n",
      "Epoch 124, Loss: 0.4914699197831528\n",
      "Epoch 125, Loss: 0.49091483381312667\n",
      "Epoch 126, Loss: 0.49035147677946317\n",
      "Epoch 127, Loss: 0.4898087222825037\n",
      "Epoch 128, Loss: 0.48924014731550836\n",
      "Epoch 129, Loss: 0.48873550647212044\n",
      "Epoch 130, Loss: 0.48832307254971563\n",
      "Epoch 131, Loss: 0.48791418392821295\n",
      "Epoch 132, Loss: 0.4875902245243796\n",
      "Epoch 133, Loss: 0.48728851108037585\n",
      "Epoch 134, Loss: 0.48697062262181345\n",
      "Epoch 135, Loss: 0.4866339553030548\n",
      "Epoch 136, Loss: 0.48633446746734893\n",
      "Epoch 137, Loss: 0.4860310087358514\n",
      "Epoch 138, Loss: 0.4856786412545615\n",
      "Epoch 139, Loss: 0.48531372107004\n",
      "Epoch 140, Loss: 0.48495712985488587\n",
      "Epoch 141, Loss: 0.4846131600652309\n",
      "Epoch 142, Loss: 0.48425966858652364\n",
      "Epoch 143, Loss: 0.4838914149236435\n",
      "Epoch 144, Loss: 0.48352437546282767\n",
      "Epoch 145, Loss: 0.4832299911937133\n",
      "Epoch 146, Loss: 0.4832586529465723\n",
      "Epoch 147, Loss: 0.48356744143113517\n",
      "Epoch 148, Loss: 0.4828108019011922\n",
      "Epoch 149, Loss: 0.4818912939738211\n",
      "Epoch 150, Loss: 0.48239416745575797\n",
      "Epoch 151, Loss: 0.4814952491721961\n",
      "Epoch 152, Loss: 0.4811822818820502\n",
      "Epoch 153, Loss: 0.4812753156716337\n",
      "Epoch 154, Loss: 0.4804387742282827\n",
      "Epoch 155, Loss: 0.48058189052417905\n",
      "Epoch 156, Loss: 0.4801288115497346\n",
      "Epoch 157, Loss: 0.4797465937246525\n",
      "Epoch 158, Loss: 0.4797622479529733\n",
      "Epoch 159, Loss: 0.4791664679844762\n",
      "Epoch 160, Loss: 0.47904359923301915\n",
      "Epoch 161, Loss: 0.4788483533353555\n",
      "Epoch 162, Loss: 0.4783709093142749\n",
      "Epoch 163, Loss: 0.4783027642591491\n",
      "Epoch 164, Loss: 0.4780003946284736\n",
      "Epoch 165, Loss: 0.47766356610156796\n",
      "Epoch 166, Loss: 0.47756940749162413\n",
      "Epoch 167, Loss: 0.4772720526560437\n",
      "Epoch 168, Loss: 0.477001191512605\n",
      "Epoch 169, Loss: 0.47690651155802954\n",
      "Epoch 170, Loss: 0.47663605231839845\n",
      "Epoch 171, Loss: 0.4763715079887003\n",
      "Epoch 172, Loss: 0.4762479807342815\n",
      "Epoch 173, Loss: 0.47603155482909787\n",
      "Epoch 174, Loss: 0.47580085926956445\n",
      "Epoch 175, Loss: 0.4756396378860013\n",
      "Epoch 176, Loss: 0.4754627232666358\n",
      "Epoch 177, Loss: 0.4752513954316692\n",
      "Epoch 178, Loss: 0.47507140350730925\n",
      "Epoch 179, Loss: 0.47492471273973497\n",
      "Epoch 180, Loss: 0.47475493910732763\n",
      "Epoch 181, Loss: 0.47455157018218247\n",
      "Epoch 182, Loss: 0.4743853224635176\n",
      "Epoch 183, Loss: 0.47424550491599726\n",
      "Epoch 184, Loss: 0.47408413178207975\n",
      "Epoch 185, Loss: 0.47390385374022453\n",
      "Epoch 186, Loss: 0.4737383066259127\n",
      "Epoch 187, Loss: 0.47358797785338436\n",
      "Epoch 188, Loss: 0.4734560449826861\n",
      "Epoch 189, Loss: 0.47335738206045025\n",
      "Epoch 190, Loss: 0.4733162834925985\n",
      "Epoch 191, Loss: 0.4732413020737725\n",
      "Epoch 192, Loss: 0.47313239020014447\n",
      "Epoch 193, Loss: 0.4729457158004841\n",
      "Epoch 194, Loss: 0.47274703738102797\n",
      "Epoch 195, Loss: 0.47253750657270627\n",
      "Epoch 196, Loss: 0.4723544299968521\n",
      "Epoch 197, Loss: 0.47221102907746554\n",
      "Epoch 198, Loss: 0.47210844467503205\n",
      "Epoch 199, Loss: 0.47205337980155637\n",
      "Epoch 200, Loss: 0.47204973383515075\n",
      "Epoch 201, Loss: 0.4720427890593303\n",
      "Epoch 202, Loss: 0.4719159657967507\n",
      "Epoch 203, Loss: 0.471735017109987\n",
      "Epoch 204, Loss: 0.4714985533244086\n",
      "Epoch 205, Loss: 0.4713143302002759\n",
      "Epoch 206, Loss: 0.47118574544243713\n",
      "Epoch 207, Loss: 0.47112002449718005\n",
      "Epoch 208, Loss: 0.4710910670518786\n",
      "Epoch 209, Loss: 0.47102495326923743\n",
      "Epoch 210, Loss: 0.4709626586360175\n",
      "Epoch 211, Loss: 0.47085840979895394\n",
      "Epoch 212, Loss: 0.47078064719388485\n",
      "Epoch 213, Loss: 0.47066614789276784\n",
      "Epoch 214, Loss: 0.47055654499036753\n",
      "Epoch 215, Loss: 0.47041652390832933\n",
      "Epoch 216, Loss: 0.4702446473560784\n",
      "Epoch 217, Loss: 0.4701045016774931\n",
      "Epoch 218, Loss: 0.47002059077245106\n",
      "Epoch 219, Loss: 0.4699643617672827\n",
      "Epoch 220, Loss: 0.4698888296176042\n",
      "Epoch 221, Loss: 0.4698141575467262\n",
      "Epoch 222, Loss: 0.4697265783615268\n",
      "Epoch 223, Loss: 0.46966066909468496\n",
      "Epoch 224, Loss: 0.4696348739248034\n",
      "Epoch 225, Loss: 0.4697176006517021\n",
      "Epoch 226, Loss: 0.4699424675720642\n",
      "Epoch 227, Loss: 0.46999269112017344\n",
      "Epoch 228, Loss: 0.4698759384724347\n",
      "Epoch 229, Loss: 0.46938041574678274\n",
      "Epoch 230, Loss: 0.4691415581864629\n",
      "Epoch 231, Loss: 0.46924839544758656\n",
      "Epoch 232, Loss: 0.46933531775018905\n",
      "Epoch 233, Loss: 0.4692253881106229\n",
      "Epoch 234, Loss: 0.46894320638835874\n",
      "Epoch 235, Loss: 0.4688628060980674\n",
      "Epoch 236, Loss: 0.46897798398181567\n",
      "Epoch 237, Loss: 0.4689811295666005\n",
      "Epoch 238, Loss: 0.4688031484354944\n",
      "Epoch 239, Loss: 0.46862983752908405\n",
      "Epoch 240, Loss: 0.4685963696810109\n",
      "Epoch 241, Loss: 0.46867106090894\n",
      "Epoch 242, Loss: 0.46866360503981047\n",
      "Epoch 243, Loss: 0.468513635806399\n",
      "Epoch 244, Loss: 0.46835730315983837\n",
      "Epoch 245, Loss: 0.468344910742699\n",
      "Epoch 246, Loss: 0.46842993299443175\n",
      "Epoch 247, Loss: 0.4683803372309361\n",
      "Epoch 248, Loss: 0.46823408027364943\n",
      "Epoch 249, Loss: 0.46809559282800506\n",
      "Epoch 250, Loss: 0.4681194355765096\n",
      "Epoch 251, Loss: 0.4682059468617626\n",
      "Epoch 252, Loss: 0.46815125054377077\n",
      "Epoch 253, Loss: 0.46800465135626707\n",
      "Epoch 254, Loss: 0.4678516840148294\n",
      "Epoch 255, Loss: 0.46781997608436393\n",
      "Epoch 256, Loss: 0.4678506648079015\n",
      "Epoch 257, Loss: 0.4677995105079106\n",
      "Epoch 258, Loss: 0.4676942061377158\n",
      "Epoch 259, Loss: 0.46760336187164103\n",
      "Epoch 260, Loss: 0.46754277859764004\n",
      "Epoch 261, Loss: 0.4675358146830681\n",
      "Epoch 262, Loss: 0.4675934670336611\n",
      "Epoch 263, Loss: 0.4675883044263754\n",
      "Epoch 264, Loss: 0.46749525618568777\n",
      "Epoch 265, Loss: 0.46735425416294335\n",
      "Epoch 266, Loss: 0.4672686694514329\n",
      "Epoch 267, Loss: 0.46723722493080344\n",
      "Epoch 268, Loss: 0.4672375438513713\n",
      "Epoch 269, Loss: 0.46722038561562945\n",
      "Epoch 270, Loss: 0.46721867932539873\n",
      "Epoch 271, Loss: 0.46717319721795353\n",
      "Epoch 272, Loss: 0.4671238189214812\n",
      "Epoch 273, Loss: 0.46705709570299314\n",
      "Epoch 274, Loss: 0.4669974785875546\n",
      "Epoch 275, Loss: 0.4669422896788895\n",
      "Epoch 276, Loss: 0.4669034987743203\n",
      "Epoch 277, Loss: 0.46684804340658115\n",
      "Epoch 278, Loss: 0.4668104417178138\n",
      "Epoch 279, Loss: 0.46677111311176783\n",
      "Epoch 280, Loss: 0.46673031256560493\n",
      "Epoch 281, Loss: 0.46669448942736436\n",
      "Epoch 282, Loss: 0.46666225583115023\n",
      "Epoch 283, Loss: 0.4666367100427352\n",
      "Epoch 284, Loss: 0.4666213020629852\n",
      "Epoch 285, Loss: 0.46662540558501797\n",
      "Epoch 286, Loss: 0.4666499301628323\n",
      "Epoch 287, Loss: 0.466693164365506\n",
      "Epoch 288, Loss: 0.46673154174214454\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.21861476046011144\n",
      "Valence RMSE: 0.20619079015779493\n",
      "Arousal RMSE: 0.23036967039123876\n",
      "Test R^2 score: tensor([0.4848, 0.4745], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.3642, 0.3515], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.47964061438556377\n",
      "Num of epochs: 289\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Epoch 121, Loss: 0.49295912913079615\n",
      "Epoch 122, Loss: 0.4924773619139838\n",
      "Epoch 123, Loss: 0.49197607191949805\n",
      "Epoch 124, Loss: 0.4914699197831528\n",
      "Epoch 125, Loss: 0.49091483381312667\n",
      "Epoch 126, Loss: 0.49035147677946317\n",
      "Epoch 127, Loss: 0.4898087222825037\n",
      "Epoch 128, Loss: 0.48924014731550836\n",
      "Epoch 129, Loss: 0.48873550647212044\n",
      "Epoch 130, Loss: 0.48832307254971563\n",
      "Epoch 131, Loss: 0.48791418392821295\n",
      "Epoch 132, Loss: 0.4875902245243796\n",
      "Epoch 133, Loss: 0.48728851108037585\n",
      "Epoch 134, Loss: 0.48697062262181345\n",
      "Epoch 135, Loss: 0.4866339553030548\n",
      "Epoch 136, Loss: 0.48633446746734893\n",
      "Epoch 137, Loss: 0.4860310087358514\n",
      "Epoch 138, Loss: 0.4856786412545615\n",
      "Epoch 139, Loss: 0.48531372107004\n",
      "Epoch 140, Loss: 0.48495712985488587\n",
      "Epoch 141, Loss: 0.4846131600652309\n",
      "Epoch 142, Loss: 0.48425966858652364\n",
      "Epoch 143, Loss: 0.4838914149236435\n",
      "Epoch 144, Loss: 0.48352437546282767\n",
      "Epoch 145, Loss: 0.4832299911937133\n",
      "Epoch 146, Loss: 0.4832586529465723\n",
      "Epoch 147, Loss: 0.48356744143113517\n",
      "Epoch 148, Loss: 0.4828108019011922\n",
      "Epoch 149, Loss: 0.4818912939738211\n",
      "Epoch 150, Loss: 0.48239416745575797\n",
      "Epoch 151, Loss: 0.4814952491721961\n",
      "Epoch 152, Loss: 0.4811822818820502\n",
      "Epoch 153, Loss: 0.4812753156716337\n",
      "Epoch 154, Loss: 0.4804387742282827\n",
      "Epoch 155, Loss: 0.48058189052417905\n",
      "Epoch 156, Loss: 0.4801288115497346\n",
      "Epoch 157, Loss: 0.4797465937246525\n",
      "Epoch 158, Loss: 0.4797622479529733\n",
      "Epoch 159, Loss: 0.4791664679844762\n",
      "Epoch 160, Loss: 0.47904359923301915\n",
      "Epoch 161, Loss: 0.4788483533353555\n",
      "Epoch 162, Loss: 0.4783709093142749\n",
      "Epoch 163, Loss: 0.4783027642591491\n",
      "Epoch 164, Loss: 0.4780003946284736\n",
      "Epoch 165, Loss: 0.47766356610156796\n",
      "Epoch 166, Loss: 0.47756940749162413\n",
      "Epoch 167, Loss: 0.4772720526560437\n",
      "Epoch 168, Loss: 0.477001191512605\n",
      "Epoch 169, Loss: 0.47690651155802954\n",
      "Epoch 170, Loss: 0.47663605231839845\n",
      "Epoch 171, Loss: 0.4763715079887003\n",
      "Epoch 172, Loss: 0.4762479807342815\n",
      "Epoch 173, Loss: 0.47603155482909787\n",
      "Epoch 174, Loss: 0.47580085926956445\n",
      "Epoch 175, Loss: 0.4756396378860013\n",
      "Epoch 176, Loss: 0.4754627232666358\n",
      "Epoch 177, Loss: 0.4752513954316692\n",
      "Epoch 178, Loss: 0.47507140350730925\n",
      "Epoch 179, Loss: 0.47492471273973497\n",
      "Epoch 180, Loss: 0.47475493910732763\n",
      "Epoch 181, Loss: 0.47455157018218247\n",
      "Epoch 182, Loss: 0.4743853224635176\n",
      "Epoch 183, Loss: 0.47424550491599726\n",
      "Epoch 184, Loss: 0.47408413178207975\n",
      "Epoch 185, Loss: 0.47390385374022453\n",
      "Epoch 186, Loss: 0.4737383066259127\n",
      "Epoch 187, Loss: 0.47358797785338436\n",
      "Epoch 188, Loss: 0.4734560449826861\n",
      "Epoch 189, Loss: 0.47335738206045025\n",
      "Epoch 190, Loss: 0.4733162834925985\n",
      "Epoch 191, Loss: 0.4732413020737725\n",
      "Epoch 192, Loss: 0.47313239020014447\n",
      "Epoch 193, Loss: 0.4729457158004841\n",
      "Epoch 194, Loss: 0.47274703738102797\n",
      "Epoch 195, Loss: 0.47253750657270627\n",
      "Epoch 196, Loss: 0.4723544299968521\n",
      "Epoch 197, Loss: 0.47221102907746554\n",
      "Epoch 198, Loss: 0.47210844467503205\n",
      "Epoch 199, Loss: 0.47205337980155637\n",
      "Epoch 200, Loss: 0.47204973383515075\n",
      "Epoch 201, Loss: 0.4720427890593303\n",
      "Epoch 202, Loss: 0.4719159657967507\n",
      "Epoch 203, Loss: 0.471735017109987\n",
      "Epoch 204, Loss: 0.4714985533244086\n",
      "Epoch 205, Loss: 0.4713143302002759\n",
      "Epoch 206, Loss: 0.47118574544243713\n",
      "Epoch 207, Loss: 0.47112002449718005\n",
      "Epoch 208, Loss: 0.4710910670518786\n",
      "Epoch 209, Loss: 0.47102495326923743\n",
      "Epoch 210, Loss: 0.4709626586360175\n",
      "Epoch 211, Loss: 0.47085840979895394\n",
      "Epoch 212, Loss: 0.47078064719388485\n",
      "Epoch 213, Loss: 0.47066614789276784\n",
      "Epoch 214, Loss: 0.47055654499036753\n",
      "Epoch 215, Loss: 0.47041652390832933\n",
      "Epoch 216, Loss: 0.4702446473560784\n",
      "Epoch 217, Loss: 0.4701045016774931\n",
      "Epoch 218, Loss: 0.47002059077245106\n",
      "Epoch 219, Loss: 0.4699643617672827\n",
      "Epoch 220, Loss: 0.4698888296176042\n",
      "Epoch 221, Loss: 0.4698141575467262\n",
      "Epoch 222, Loss: 0.4697265783615268\n",
      "Epoch 223, Loss: 0.46966066909468496\n",
      "Epoch 224, Loss: 0.4696348739248034\n",
      "Epoch 225, Loss: 0.4697176006517021\n",
      "Epoch 226, Loss: 0.4699424675720642\n",
      "Epoch 227, Loss: 0.46999269112017344\n",
      "Epoch 228, Loss: 0.4698759384724347\n",
      "Epoch 229, Loss: 0.46938041574678274\n",
      "Epoch 230, Loss: 0.4691415581864629\n",
      "Epoch 231, Loss: 0.46924839544758656\n",
      "Epoch 232, Loss: 0.46933531775018905\n",
      "Epoch 233, Loss: 0.4692253881106229\n",
      "Epoch 234, Loss: 0.46894320638835874\n",
      "Epoch 235, Loss: 0.4688628060980674\n",
      "Epoch 236, Loss: 0.46897798398181567\n",
      "Epoch 237, Loss: 0.4689811295666005\n",
      "Epoch 238, Loss: 0.4688031484354944\n",
      "Epoch 239, Loss: 0.46862983752908405\n",
      "Epoch 240, Loss: 0.4685963696810109\n",
      "Epoch 241, Loss: 0.46867106090894\n",
      "Epoch 242, Loss: 0.46866360503981047\n",
      "Epoch 243, Loss: 0.468513635806399\n",
      "Epoch 244, Loss: 0.46835730315983837\n",
      "Epoch 245, Loss: 0.468344910742699\n",
      "Epoch 246, Loss: 0.46842993299443175\n",
      "Epoch 247, Loss: 0.4683803372309361\n",
      "Epoch 248, Loss: 0.46823408027364943\n",
      "Epoch 249, Loss: 0.46809559282800506\n",
      "Epoch 250, Loss: 0.4681194355765096\n",
      "Epoch 251, Loss: 0.4682059468617626\n",
      "Epoch 252, Loss: 0.46815125054377077\n",
      "Epoch 253, Loss: 0.46800465135626707\n",
      "Epoch 254, Loss: 0.4678516840148294\n",
      "Epoch 255, Loss: 0.46781997608436393\n",
      "Epoch 256, Loss: 0.4678506648079015\n",
      "Epoch 257, Loss: 0.4677995105079106\n",
      "Epoch 258, Loss: 0.4676942061377158\n",
      "Epoch 259, Loss: 0.46760336187164103\n",
      "Epoch 260, Loss: 0.46754277859764004\n",
      "Epoch 261, Loss: 0.4675358146830681\n",
      "Epoch 262, Loss: 0.4675934670336611\n",
      "Epoch 263, Loss: 0.4675883044263754\n",
      "Epoch 264, Loss: 0.46749525618568777\n",
      "Epoch 265, Loss: 0.46735425416294335\n",
      "Epoch 266, Loss: 0.4672686694514329\n",
      "Epoch 267, Loss: 0.46723722493080344\n",
      "Epoch 268, Loss: 0.4672375438513713\n",
      "Epoch 269, Loss: 0.46722038561562945\n",
      "Epoch 270, Loss: 0.46721867932539873\n",
      "Epoch 271, Loss: 0.46717319721795353\n",
      "Epoch 272, Loss: 0.4671238189214812\n",
      "Epoch 273, Loss: 0.46705709570299314\n",
      "Epoch 274, Loss: 0.4669974785875546\n",
      "Epoch 275, Loss: 0.4669422896788895\n",
      "Epoch 276, Loss: 0.4669034987743203\n",
      "Epoch 277, Loss: 0.46684804340658115\n",
      "Epoch 278, Loss: 0.4668104417178138\n",
      "Epoch 279, Loss: 0.46677111311176783\n",
      "Epoch 280, Loss: 0.46673031256560493\n",
      "Epoch 281, Loss: 0.46669448942736436\n",
      "Epoch 282, Loss: 0.46666225583115023\n",
      "Epoch 283, Loss: 0.4666367100427352\n",
      "Epoch 284, Loss: 0.4666213020629852\n",
      "Epoch 285, Loss: 0.46662540558501797\n",
      "Epoch 286, Loss: 0.4666499301628323\n",
      "Epoch 287, Loss: 0.466693164365506\n",
      "Epoch 288, Loss: 0.46673154174214454\n",
      "Epoch 289, Loss: 0.4666228668343984\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.21725438994643706\n",
      "Valence RMSE: 0.2049084875208102\n",
      "Arousal RMSE: 0.2289354748481131\n",
      "Test R^2 score: tensor([0.4912, 0.4810], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.3721, 0.3596], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4860961847661258\n",
      "Num of epochs: 290\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Epoch 121, Loss: 0.49295912913079615\n",
      "Epoch 122, Loss: 0.4924773619139838\n",
      "Epoch 123, Loss: 0.49197607191949805\n",
      "Epoch 124, Loss: 0.4914699197831528\n",
      "Epoch 125, Loss: 0.49091483381312667\n",
      "Epoch 126, Loss: 0.49035147677946317\n",
      "Epoch 127, Loss: 0.4898087222825037\n",
      "Epoch 128, Loss: 0.48924014731550836\n",
      "Epoch 129, Loss: 0.48873550647212044\n",
      "Epoch 130, Loss: 0.48832307254971563\n",
      "Epoch 131, Loss: 0.48791418392821295\n",
      "Epoch 132, Loss: 0.4875902245243796\n",
      "Epoch 133, Loss: 0.48728851108037585\n",
      "Epoch 134, Loss: 0.48697062262181345\n",
      "Epoch 135, Loss: 0.4866339553030548\n",
      "Epoch 136, Loss: 0.48633446746734893\n",
      "Epoch 137, Loss: 0.4860310087358514\n",
      "Epoch 138, Loss: 0.4856786412545615\n",
      "Epoch 139, Loss: 0.48531372107004\n",
      "Epoch 140, Loss: 0.48495712985488587\n",
      "Epoch 141, Loss: 0.4846131600652309\n",
      "Epoch 142, Loss: 0.48425966858652364\n",
      "Epoch 143, Loss: 0.4838914149236435\n",
      "Epoch 144, Loss: 0.48352437546282767\n",
      "Epoch 145, Loss: 0.4832299911937133\n",
      "Epoch 146, Loss: 0.4832586529465723\n",
      "Epoch 147, Loss: 0.48356744143113517\n",
      "Epoch 148, Loss: 0.4828108019011922\n",
      "Epoch 149, Loss: 0.4818912939738211\n",
      "Epoch 150, Loss: 0.48239416745575797\n",
      "Epoch 151, Loss: 0.4814952491721961\n",
      "Epoch 152, Loss: 0.4811822818820502\n",
      "Epoch 153, Loss: 0.4812753156716337\n",
      "Epoch 154, Loss: 0.4804387742282827\n",
      "Epoch 155, Loss: 0.48058189052417905\n",
      "Epoch 156, Loss: 0.4801288115497346\n",
      "Epoch 157, Loss: 0.4797465937246525\n",
      "Epoch 158, Loss: 0.4797622479529733\n",
      "Epoch 159, Loss: 0.4791664679844762\n",
      "Epoch 160, Loss: 0.47904359923301915\n",
      "Epoch 161, Loss: 0.4788483533353555\n",
      "Epoch 162, Loss: 0.4783709093142749\n",
      "Epoch 163, Loss: 0.4783027642591491\n",
      "Epoch 164, Loss: 0.4780003946284736\n",
      "Epoch 165, Loss: 0.47766356610156796\n",
      "Epoch 166, Loss: 0.47756940749162413\n",
      "Epoch 167, Loss: 0.4772720526560437\n",
      "Epoch 168, Loss: 0.477001191512605\n",
      "Epoch 169, Loss: 0.47690651155802954\n",
      "Epoch 170, Loss: 0.47663605231839845\n",
      "Epoch 171, Loss: 0.4763715079887003\n",
      "Epoch 172, Loss: 0.4762479807342815\n",
      "Epoch 173, Loss: 0.47603155482909787\n",
      "Epoch 174, Loss: 0.47580085926956445\n",
      "Epoch 175, Loss: 0.4756396378860013\n",
      "Epoch 176, Loss: 0.4754627232666358\n",
      "Epoch 177, Loss: 0.4752513954316692\n",
      "Epoch 178, Loss: 0.47507140350730925\n",
      "Epoch 179, Loss: 0.47492471273973497\n",
      "Epoch 180, Loss: 0.47475493910732763\n",
      "Epoch 181, Loss: 0.47455157018218247\n",
      "Epoch 182, Loss: 0.4743853224635176\n",
      "Epoch 183, Loss: 0.47424550491599726\n",
      "Epoch 184, Loss: 0.47408413178207975\n",
      "Epoch 185, Loss: 0.47390385374022453\n",
      "Epoch 186, Loss: 0.4737383066259127\n",
      "Epoch 187, Loss: 0.47358797785338436\n",
      "Epoch 188, Loss: 0.4734560449826861\n",
      "Epoch 189, Loss: 0.47335738206045025\n",
      "Epoch 190, Loss: 0.4733162834925985\n",
      "Epoch 191, Loss: 0.4732413020737725\n",
      "Epoch 192, Loss: 0.47313239020014447\n",
      "Epoch 193, Loss: 0.4729457158004841\n",
      "Epoch 194, Loss: 0.47274703738102797\n",
      "Epoch 195, Loss: 0.47253750657270627\n",
      "Epoch 196, Loss: 0.4723544299968521\n",
      "Epoch 197, Loss: 0.47221102907746554\n",
      "Epoch 198, Loss: 0.47210844467503205\n",
      "Epoch 199, Loss: 0.47205337980155637\n",
      "Epoch 200, Loss: 0.47204973383515075\n",
      "Epoch 201, Loss: 0.4720427890593303\n",
      "Epoch 202, Loss: 0.4719159657967507\n",
      "Epoch 203, Loss: 0.471735017109987\n",
      "Epoch 204, Loss: 0.4714985533244086\n",
      "Epoch 205, Loss: 0.4713143302002759\n",
      "Epoch 206, Loss: 0.47118574544243713\n",
      "Epoch 207, Loss: 0.47112002449718005\n",
      "Epoch 208, Loss: 0.4710910670518786\n",
      "Epoch 209, Loss: 0.47102495326923743\n",
      "Epoch 210, Loss: 0.4709626586360175\n",
      "Epoch 211, Loss: 0.47085840979895394\n",
      "Epoch 212, Loss: 0.47078064719388485\n",
      "Epoch 213, Loss: 0.47066614789276784\n",
      "Epoch 214, Loss: 0.47055654499036753\n",
      "Epoch 215, Loss: 0.47041652390832933\n",
      "Epoch 216, Loss: 0.4702446473560784\n",
      "Epoch 217, Loss: 0.4701045016774931\n",
      "Epoch 218, Loss: 0.47002059077245106\n",
      "Epoch 219, Loss: 0.4699643617672827\n",
      "Epoch 220, Loss: 0.4698888296176042\n",
      "Epoch 221, Loss: 0.4698141575467262\n",
      "Epoch 222, Loss: 0.4697265783615268\n",
      "Epoch 223, Loss: 0.46966066909468496\n",
      "Epoch 224, Loss: 0.4696348739248034\n",
      "Epoch 225, Loss: 0.4697176006517021\n",
      "Epoch 226, Loss: 0.4699424675720642\n",
      "Epoch 227, Loss: 0.46999269112017344\n",
      "Epoch 228, Loss: 0.4698759384724347\n",
      "Epoch 229, Loss: 0.46938041574678274\n",
      "Epoch 230, Loss: 0.4691415581864629\n",
      "Epoch 231, Loss: 0.46924839544758656\n",
      "Epoch 232, Loss: 0.46933531775018905\n",
      "Epoch 233, Loss: 0.4692253881106229\n",
      "Epoch 234, Loss: 0.46894320638835874\n",
      "Epoch 235, Loss: 0.4688628060980674\n",
      "Epoch 236, Loss: 0.46897798398181567\n",
      "Epoch 237, Loss: 0.4689811295666005\n",
      "Epoch 238, Loss: 0.4688031484354944\n",
      "Epoch 239, Loss: 0.46862983752908405\n",
      "Epoch 240, Loss: 0.4685963696810109\n",
      "Epoch 241, Loss: 0.46867106090894\n",
      "Epoch 242, Loss: 0.46866360503981047\n",
      "Epoch 243, Loss: 0.468513635806399\n",
      "Epoch 244, Loss: 0.46835730315983837\n",
      "Epoch 245, Loss: 0.468344910742699\n",
      "Epoch 246, Loss: 0.46842993299443175\n",
      "Epoch 247, Loss: 0.4683803372309361\n",
      "Epoch 248, Loss: 0.46823408027364943\n",
      "Epoch 249, Loss: 0.46809559282800506\n",
      "Epoch 250, Loss: 0.4681194355765096\n",
      "Epoch 251, Loss: 0.4682059468617626\n",
      "Epoch 252, Loss: 0.46815125054377077\n",
      "Epoch 253, Loss: 0.46800465135626707\n",
      "Epoch 254, Loss: 0.4678516840148294\n",
      "Epoch 255, Loss: 0.46781997608436393\n",
      "Epoch 256, Loss: 0.4678506648079015\n",
      "Epoch 257, Loss: 0.4677995105079106\n",
      "Epoch 258, Loss: 0.4676942061377158\n",
      "Epoch 259, Loss: 0.46760336187164103\n",
      "Epoch 260, Loss: 0.46754277859764004\n",
      "Epoch 261, Loss: 0.4675358146830681\n",
      "Epoch 262, Loss: 0.4675934670336611\n",
      "Epoch 263, Loss: 0.4675883044263754\n",
      "Epoch 264, Loss: 0.46749525618568777\n",
      "Epoch 265, Loss: 0.46735425416294335\n",
      "Epoch 266, Loss: 0.4672686694514329\n",
      "Epoch 267, Loss: 0.46723722493080344\n",
      "Epoch 268, Loss: 0.4672375438513713\n",
      "Epoch 269, Loss: 0.46722038561562945\n",
      "Epoch 270, Loss: 0.46721867932539873\n",
      "Epoch 271, Loss: 0.46717319721795353\n",
      "Epoch 272, Loss: 0.4671238189214812\n",
      "Epoch 273, Loss: 0.46705709570299314\n",
      "Epoch 274, Loss: 0.4669974785875546\n",
      "Epoch 275, Loss: 0.4669422896788895\n",
      "Epoch 276, Loss: 0.4669034987743203\n",
      "Epoch 277, Loss: 0.46684804340658115\n",
      "Epoch 278, Loss: 0.4668104417178138\n",
      "Epoch 279, Loss: 0.46677111311176783\n",
      "Epoch 280, Loss: 0.46673031256560493\n",
      "Epoch 281, Loss: 0.46669448942736436\n",
      "Epoch 282, Loss: 0.46666225583115023\n",
      "Epoch 283, Loss: 0.4666367100427352\n",
      "Epoch 284, Loss: 0.4666213020629852\n",
      "Epoch 285, Loss: 0.46662540558501797\n",
      "Epoch 286, Loss: 0.4666499301628323\n",
      "Epoch 287, Loss: 0.466693164365506\n",
      "Epoch 288, Loss: 0.46673154174214454\n",
      "Epoch 289, Loss: 0.4666228668343984\n",
      "Epoch 290, Loss: 0.46649923372154634\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2180289973237679\n",
      "Valence RMSE: 0.20563618995345959\n",
      "Arousal RMSE: 0.22975431384293935\n",
      "Test R^2 score: tensor([0.4875, 0.4773], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.3676, 0.3550], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.48242638202425164\n",
      "Num of epochs: 291\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Epoch 121, Loss: 0.49295912913079615\n",
      "Epoch 122, Loss: 0.4924773619139838\n",
      "Epoch 123, Loss: 0.49197607191949805\n",
      "Epoch 124, Loss: 0.4914699197831528\n",
      "Epoch 125, Loss: 0.49091483381312667\n",
      "Epoch 126, Loss: 0.49035147677946317\n",
      "Epoch 127, Loss: 0.4898087222825037\n",
      "Epoch 128, Loss: 0.48924014731550836\n",
      "Epoch 129, Loss: 0.48873550647212044\n",
      "Epoch 130, Loss: 0.48832307254971563\n",
      "Epoch 131, Loss: 0.48791418392821295\n",
      "Epoch 132, Loss: 0.4875902245243796\n",
      "Epoch 133, Loss: 0.48728851108037585\n",
      "Epoch 134, Loss: 0.48697062262181345\n",
      "Epoch 135, Loss: 0.4866339553030548\n",
      "Epoch 136, Loss: 0.48633446746734893\n",
      "Epoch 137, Loss: 0.4860310087358514\n",
      "Epoch 138, Loss: 0.4856786412545615\n",
      "Epoch 139, Loss: 0.48531372107004\n",
      "Epoch 140, Loss: 0.48495712985488587\n",
      "Epoch 141, Loss: 0.4846131600652309\n",
      "Epoch 142, Loss: 0.48425966858652364\n",
      "Epoch 143, Loss: 0.4838914149236435\n",
      "Epoch 144, Loss: 0.48352437546282767\n",
      "Epoch 145, Loss: 0.4832299911937133\n",
      "Epoch 146, Loss: 0.4832586529465723\n",
      "Epoch 147, Loss: 0.48356744143113517\n",
      "Epoch 148, Loss: 0.4828108019011922\n",
      "Epoch 149, Loss: 0.4818912939738211\n",
      "Epoch 150, Loss: 0.48239416745575797\n",
      "Epoch 151, Loss: 0.4814952491721961\n",
      "Epoch 152, Loss: 0.4811822818820502\n",
      "Epoch 153, Loss: 0.4812753156716337\n",
      "Epoch 154, Loss: 0.4804387742282827\n",
      "Epoch 155, Loss: 0.48058189052417905\n",
      "Epoch 156, Loss: 0.4801288115497346\n",
      "Epoch 157, Loss: 0.4797465937246525\n",
      "Epoch 158, Loss: 0.4797622479529733\n",
      "Epoch 159, Loss: 0.4791664679844762\n",
      "Epoch 160, Loss: 0.47904359923301915\n",
      "Epoch 161, Loss: 0.4788483533353555\n",
      "Epoch 162, Loss: 0.4783709093142749\n",
      "Epoch 163, Loss: 0.4783027642591491\n",
      "Epoch 164, Loss: 0.4780003946284736\n",
      "Epoch 165, Loss: 0.47766356610156796\n",
      "Epoch 166, Loss: 0.47756940749162413\n",
      "Epoch 167, Loss: 0.4772720526560437\n",
      "Epoch 168, Loss: 0.477001191512605\n",
      "Epoch 169, Loss: 0.47690651155802954\n",
      "Epoch 170, Loss: 0.47663605231839845\n",
      "Epoch 171, Loss: 0.4763715079887003\n",
      "Epoch 172, Loss: 0.4762479807342815\n",
      "Epoch 173, Loss: 0.47603155482909787\n",
      "Epoch 174, Loss: 0.47580085926956445\n",
      "Epoch 175, Loss: 0.4756396378860013\n",
      "Epoch 176, Loss: 0.4754627232666358\n",
      "Epoch 177, Loss: 0.4752513954316692\n",
      "Epoch 178, Loss: 0.47507140350730925\n",
      "Epoch 179, Loss: 0.47492471273973497\n",
      "Epoch 180, Loss: 0.47475493910732763\n",
      "Epoch 181, Loss: 0.47455157018218247\n",
      "Epoch 182, Loss: 0.4743853224635176\n",
      "Epoch 183, Loss: 0.47424550491599726\n",
      "Epoch 184, Loss: 0.47408413178207975\n",
      "Epoch 185, Loss: 0.47390385374022453\n",
      "Epoch 186, Loss: 0.4737383066259127\n",
      "Epoch 187, Loss: 0.47358797785338436\n",
      "Epoch 188, Loss: 0.4734560449826861\n",
      "Epoch 189, Loss: 0.47335738206045025\n",
      "Epoch 190, Loss: 0.4733162834925985\n",
      "Epoch 191, Loss: 0.4732413020737725\n",
      "Epoch 192, Loss: 0.47313239020014447\n",
      "Epoch 193, Loss: 0.4729457158004841\n",
      "Epoch 194, Loss: 0.47274703738102797\n",
      "Epoch 195, Loss: 0.47253750657270627\n",
      "Epoch 196, Loss: 0.4723544299968521\n",
      "Epoch 197, Loss: 0.47221102907746554\n",
      "Epoch 198, Loss: 0.47210844467503205\n",
      "Epoch 199, Loss: 0.47205337980155637\n",
      "Epoch 200, Loss: 0.47204973383515075\n",
      "Epoch 201, Loss: 0.4720427890593303\n",
      "Epoch 202, Loss: 0.4719159657967507\n",
      "Epoch 203, Loss: 0.471735017109987\n",
      "Epoch 204, Loss: 0.4714985533244086\n",
      "Epoch 205, Loss: 0.4713143302002759\n",
      "Epoch 206, Loss: 0.47118574544243713\n",
      "Epoch 207, Loss: 0.47112002449718005\n",
      "Epoch 208, Loss: 0.4710910670518786\n",
      "Epoch 209, Loss: 0.47102495326923743\n",
      "Epoch 210, Loss: 0.4709626586360175\n",
      "Epoch 211, Loss: 0.47085840979895394\n",
      "Epoch 212, Loss: 0.47078064719388485\n",
      "Epoch 213, Loss: 0.47066614789276784\n",
      "Epoch 214, Loss: 0.47055654499036753\n",
      "Epoch 215, Loss: 0.47041652390832933\n",
      "Epoch 216, Loss: 0.4702446473560784\n",
      "Epoch 217, Loss: 0.4701045016774931\n",
      "Epoch 218, Loss: 0.47002059077245106\n",
      "Epoch 219, Loss: 0.4699643617672827\n",
      "Epoch 220, Loss: 0.4698888296176042\n",
      "Epoch 221, Loss: 0.4698141575467262\n",
      "Epoch 222, Loss: 0.4697265783615268\n",
      "Epoch 223, Loss: 0.46966066909468496\n",
      "Epoch 224, Loss: 0.4696348739248034\n",
      "Epoch 225, Loss: 0.4697176006517021\n",
      "Epoch 226, Loss: 0.4699424675720642\n",
      "Epoch 227, Loss: 0.46999269112017344\n",
      "Epoch 228, Loss: 0.4698759384724347\n",
      "Epoch 229, Loss: 0.46938041574678274\n",
      "Epoch 230, Loss: 0.4691415581864629\n",
      "Epoch 231, Loss: 0.46924839544758656\n",
      "Epoch 232, Loss: 0.46933531775018905\n",
      "Epoch 233, Loss: 0.4692253881106229\n",
      "Epoch 234, Loss: 0.46894320638835874\n",
      "Epoch 235, Loss: 0.4688628060980674\n",
      "Epoch 236, Loss: 0.46897798398181567\n",
      "Epoch 237, Loss: 0.4689811295666005\n",
      "Epoch 238, Loss: 0.4688031484354944\n",
      "Epoch 239, Loss: 0.46862983752908405\n",
      "Epoch 240, Loss: 0.4685963696810109\n",
      "Epoch 241, Loss: 0.46867106090894\n",
      "Epoch 242, Loss: 0.46866360503981047\n",
      "Epoch 243, Loss: 0.468513635806399\n",
      "Epoch 244, Loss: 0.46835730315983837\n",
      "Epoch 245, Loss: 0.468344910742699\n",
      "Epoch 246, Loss: 0.46842993299443175\n",
      "Epoch 247, Loss: 0.4683803372309361\n",
      "Epoch 248, Loss: 0.46823408027364943\n",
      "Epoch 249, Loss: 0.46809559282800506\n",
      "Epoch 250, Loss: 0.4681194355765096\n",
      "Epoch 251, Loss: 0.4682059468617626\n",
      "Epoch 252, Loss: 0.46815125054377077\n",
      "Epoch 253, Loss: 0.46800465135626707\n",
      "Epoch 254, Loss: 0.4678516840148294\n",
      "Epoch 255, Loss: 0.46781997608436393\n",
      "Epoch 256, Loss: 0.4678506648079015\n",
      "Epoch 257, Loss: 0.4677995105079106\n",
      "Epoch 258, Loss: 0.4676942061377158\n",
      "Epoch 259, Loss: 0.46760336187164103\n",
      "Epoch 260, Loss: 0.46754277859764004\n",
      "Epoch 261, Loss: 0.4675358146830681\n",
      "Epoch 262, Loss: 0.4675934670336611\n",
      "Epoch 263, Loss: 0.4675883044263754\n",
      "Epoch 264, Loss: 0.46749525618568777\n",
      "Epoch 265, Loss: 0.46735425416294335\n",
      "Epoch 266, Loss: 0.4672686694514329\n",
      "Epoch 267, Loss: 0.46723722493080344\n",
      "Epoch 268, Loss: 0.4672375438513713\n",
      "Epoch 269, Loss: 0.46722038561562945\n",
      "Epoch 270, Loss: 0.46721867932539873\n",
      "Epoch 271, Loss: 0.46717319721795353\n",
      "Epoch 272, Loss: 0.4671238189214812\n",
      "Epoch 273, Loss: 0.46705709570299314\n",
      "Epoch 274, Loss: 0.4669974785875546\n",
      "Epoch 275, Loss: 0.4669422896788895\n",
      "Epoch 276, Loss: 0.4669034987743203\n",
      "Epoch 277, Loss: 0.46684804340658115\n",
      "Epoch 278, Loss: 0.4668104417178138\n",
      "Epoch 279, Loss: 0.46677111311176783\n",
      "Epoch 280, Loss: 0.46673031256560493\n",
      "Epoch 281, Loss: 0.46669448942736436\n",
      "Epoch 282, Loss: 0.46666225583115023\n",
      "Epoch 283, Loss: 0.4666367100427352\n",
      "Epoch 284, Loss: 0.4666213020629852\n",
      "Epoch 285, Loss: 0.46662540558501797\n",
      "Epoch 286, Loss: 0.4666499301628323\n",
      "Epoch 287, Loss: 0.466693164365506\n",
      "Epoch 288, Loss: 0.46673154174214454\n",
      "Epoch 289, Loss: 0.4666228668343984\n",
      "Epoch 290, Loss: 0.46649923372154634\n",
      "Epoch 291, Loss: 0.46640938673130766\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.21751889490819099\n",
      "Valence RMSE: 0.20516464765478176\n",
      "Arousal RMSE: 0.22920821677429132\n",
      "Test R^2 score: tensor([0.4899, 0.4798], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.3705, 0.3581], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4848410363155861\n",
      "Num of epochs: 292\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Epoch 121, Loss: 0.49295912913079615\n",
      "Epoch 122, Loss: 0.4924773619139838\n",
      "Epoch 123, Loss: 0.49197607191949805\n",
      "Epoch 124, Loss: 0.4914699197831528\n",
      "Epoch 125, Loss: 0.49091483381312667\n",
      "Epoch 126, Loss: 0.49035147677946317\n",
      "Epoch 127, Loss: 0.4898087222825037\n",
      "Epoch 128, Loss: 0.48924014731550836\n",
      "Epoch 129, Loss: 0.48873550647212044\n",
      "Epoch 130, Loss: 0.48832307254971563\n",
      "Epoch 131, Loss: 0.48791418392821295\n",
      "Epoch 132, Loss: 0.4875902245243796\n",
      "Epoch 133, Loss: 0.48728851108037585\n",
      "Epoch 134, Loss: 0.48697062262181345\n",
      "Epoch 135, Loss: 0.4866339553030548\n",
      "Epoch 136, Loss: 0.48633446746734893\n",
      "Epoch 137, Loss: 0.4860310087358514\n",
      "Epoch 138, Loss: 0.4856786412545615\n",
      "Epoch 139, Loss: 0.48531372107004\n",
      "Epoch 140, Loss: 0.48495712985488587\n",
      "Epoch 141, Loss: 0.4846131600652309\n",
      "Epoch 142, Loss: 0.48425966858652364\n",
      "Epoch 143, Loss: 0.4838914149236435\n",
      "Epoch 144, Loss: 0.48352437546282767\n",
      "Epoch 145, Loss: 0.4832299911937133\n",
      "Epoch 146, Loss: 0.4832586529465723\n",
      "Epoch 147, Loss: 0.48356744143113517\n",
      "Epoch 148, Loss: 0.4828108019011922\n",
      "Epoch 149, Loss: 0.4818912939738211\n",
      "Epoch 150, Loss: 0.48239416745575797\n",
      "Epoch 151, Loss: 0.4814952491721961\n",
      "Epoch 152, Loss: 0.4811822818820502\n",
      "Epoch 153, Loss: 0.4812753156716337\n",
      "Epoch 154, Loss: 0.4804387742282827\n",
      "Epoch 155, Loss: 0.48058189052417905\n",
      "Epoch 156, Loss: 0.4801288115497346\n",
      "Epoch 157, Loss: 0.4797465937246525\n",
      "Epoch 158, Loss: 0.4797622479529733\n",
      "Epoch 159, Loss: 0.4791664679844762\n",
      "Epoch 160, Loss: 0.47904359923301915\n",
      "Epoch 161, Loss: 0.4788483533353555\n",
      "Epoch 162, Loss: 0.4783709093142749\n",
      "Epoch 163, Loss: 0.4783027642591491\n",
      "Epoch 164, Loss: 0.4780003946284736\n",
      "Epoch 165, Loss: 0.47766356610156796\n",
      "Epoch 166, Loss: 0.47756940749162413\n",
      "Epoch 167, Loss: 0.4772720526560437\n",
      "Epoch 168, Loss: 0.477001191512605\n",
      "Epoch 169, Loss: 0.47690651155802954\n",
      "Epoch 170, Loss: 0.47663605231839845\n",
      "Epoch 171, Loss: 0.4763715079887003\n",
      "Epoch 172, Loss: 0.4762479807342815\n",
      "Epoch 173, Loss: 0.47603155482909787\n",
      "Epoch 174, Loss: 0.47580085926956445\n",
      "Epoch 175, Loss: 0.4756396378860013\n",
      "Epoch 176, Loss: 0.4754627232666358\n",
      "Epoch 177, Loss: 0.4752513954316692\n",
      "Epoch 178, Loss: 0.47507140350730925\n",
      "Epoch 179, Loss: 0.47492471273973497\n",
      "Epoch 180, Loss: 0.47475493910732763\n",
      "Epoch 181, Loss: 0.47455157018218247\n",
      "Epoch 182, Loss: 0.4743853224635176\n",
      "Epoch 183, Loss: 0.47424550491599726\n",
      "Epoch 184, Loss: 0.47408413178207975\n",
      "Epoch 185, Loss: 0.47390385374022453\n",
      "Epoch 186, Loss: 0.4737383066259127\n",
      "Epoch 187, Loss: 0.47358797785338436\n",
      "Epoch 188, Loss: 0.4734560449826861\n",
      "Epoch 189, Loss: 0.47335738206045025\n",
      "Epoch 190, Loss: 0.4733162834925985\n",
      "Epoch 191, Loss: 0.4732413020737725\n",
      "Epoch 192, Loss: 0.47313239020014447\n",
      "Epoch 193, Loss: 0.4729457158004841\n",
      "Epoch 194, Loss: 0.47274703738102797\n",
      "Epoch 195, Loss: 0.47253750657270627\n",
      "Epoch 196, Loss: 0.4723544299968521\n",
      "Epoch 197, Loss: 0.47221102907746554\n",
      "Epoch 198, Loss: 0.47210844467503205\n",
      "Epoch 199, Loss: 0.47205337980155637\n",
      "Epoch 200, Loss: 0.47204973383515075\n",
      "Epoch 201, Loss: 0.4720427890593303\n",
      "Epoch 202, Loss: 0.4719159657967507\n",
      "Epoch 203, Loss: 0.471735017109987\n",
      "Epoch 204, Loss: 0.4714985533244086\n",
      "Epoch 205, Loss: 0.4713143302002759\n",
      "Epoch 206, Loss: 0.47118574544243713\n",
      "Epoch 207, Loss: 0.47112002449718005\n",
      "Epoch 208, Loss: 0.4710910670518786\n",
      "Epoch 209, Loss: 0.47102495326923743\n",
      "Epoch 210, Loss: 0.4709626586360175\n",
      "Epoch 211, Loss: 0.47085840979895394\n",
      "Epoch 212, Loss: 0.47078064719388485\n",
      "Epoch 213, Loss: 0.47066614789276784\n",
      "Epoch 214, Loss: 0.47055654499036753\n",
      "Epoch 215, Loss: 0.47041652390832933\n",
      "Epoch 216, Loss: 0.4702446473560784\n",
      "Epoch 217, Loss: 0.4701045016774931\n",
      "Epoch 218, Loss: 0.47002059077245106\n",
      "Epoch 219, Loss: 0.4699643617672827\n",
      "Epoch 220, Loss: 0.4698888296176042\n",
      "Epoch 221, Loss: 0.4698141575467262\n",
      "Epoch 222, Loss: 0.4697265783615268\n",
      "Epoch 223, Loss: 0.46966066909468496\n",
      "Epoch 224, Loss: 0.4696348739248034\n",
      "Epoch 225, Loss: 0.4697176006517021\n",
      "Epoch 226, Loss: 0.4699424675720642\n",
      "Epoch 227, Loss: 0.46999269112017344\n",
      "Epoch 228, Loss: 0.4698759384724347\n",
      "Epoch 229, Loss: 0.46938041574678274\n",
      "Epoch 230, Loss: 0.4691415581864629\n",
      "Epoch 231, Loss: 0.46924839544758656\n",
      "Epoch 232, Loss: 0.46933531775018905\n",
      "Epoch 233, Loss: 0.4692253881106229\n",
      "Epoch 234, Loss: 0.46894320638835874\n",
      "Epoch 235, Loss: 0.4688628060980674\n",
      "Epoch 236, Loss: 0.46897798398181567\n",
      "Epoch 237, Loss: 0.4689811295666005\n",
      "Epoch 238, Loss: 0.4688031484354944\n",
      "Epoch 239, Loss: 0.46862983752908405\n",
      "Epoch 240, Loss: 0.4685963696810109\n",
      "Epoch 241, Loss: 0.46867106090894\n",
      "Epoch 242, Loss: 0.46866360503981047\n",
      "Epoch 243, Loss: 0.468513635806399\n",
      "Epoch 244, Loss: 0.46835730315983837\n",
      "Epoch 245, Loss: 0.468344910742699\n",
      "Epoch 246, Loss: 0.46842993299443175\n",
      "Epoch 247, Loss: 0.4683803372309361\n",
      "Epoch 248, Loss: 0.46823408027364943\n",
      "Epoch 249, Loss: 0.46809559282800506\n",
      "Epoch 250, Loss: 0.4681194355765096\n",
      "Epoch 251, Loss: 0.4682059468617626\n",
      "Epoch 252, Loss: 0.46815125054377077\n",
      "Epoch 253, Loss: 0.46800465135626707\n",
      "Epoch 254, Loss: 0.4678516840148294\n",
      "Epoch 255, Loss: 0.46781997608436393\n",
      "Epoch 256, Loss: 0.4678506648079015\n",
      "Epoch 257, Loss: 0.4677995105079106\n",
      "Epoch 258, Loss: 0.4676942061377158\n",
      "Epoch 259, Loss: 0.46760336187164103\n",
      "Epoch 260, Loss: 0.46754277859764004\n",
      "Epoch 261, Loss: 0.4675358146830681\n",
      "Epoch 262, Loss: 0.4675934670336611\n",
      "Epoch 263, Loss: 0.4675883044263754\n",
      "Epoch 264, Loss: 0.46749525618568777\n",
      "Epoch 265, Loss: 0.46735425416294335\n",
      "Epoch 266, Loss: 0.4672686694514329\n",
      "Epoch 267, Loss: 0.46723722493080344\n",
      "Epoch 268, Loss: 0.4672375438513713\n",
      "Epoch 269, Loss: 0.46722038561562945\n",
      "Epoch 270, Loss: 0.46721867932539873\n",
      "Epoch 271, Loss: 0.46717319721795353\n",
      "Epoch 272, Loss: 0.4671238189214812\n",
      "Epoch 273, Loss: 0.46705709570299314\n",
      "Epoch 274, Loss: 0.4669974785875546\n",
      "Epoch 275, Loss: 0.4669422896788895\n",
      "Epoch 276, Loss: 0.4669034987743203\n",
      "Epoch 277, Loss: 0.46684804340658115\n",
      "Epoch 278, Loss: 0.4668104417178138\n",
      "Epoch 279, Loss: 0.46677111311176783\n",
      "Epoch 280, Loss: 0.46673031256560493\n",
      "Epoch 281, Loss: 0.46669448942736436\n",
      "Epoch 282, Loss: 0.46666225583115023\n",
      "Epoch 283, Loss: 0.4666367100427352\n",
      "Epoch 284, Loss: 0.4666213020629852\n",
      "Epoch 285, Loss: 0.46662540558501797\n",
      "Epoch 286, Loss: 0.4666499301628323\n",
      "Epoch 287, Loss: 0.466693164365506\n",
      "Epoch 288, Loss: 0.46673154174214454\n",
      "Epoch 289, Loss: 0.4666228668343984\n",
      "Epoch 290, Loss: 0.46649923372154634\n",
      "Epoch 291, Loss: 0.46640938673130766\n",
      "Epoch 292, Loss: 0.4663389345853611\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.21756900706825105\n",
      "Valence RMSE: 0.20522929498298792\n",
      "Arousal RMSE: 0.22924546266854465\n",
      "Test R^2 score: tensor([0.4896, 0.4796], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.3701, 0.3578], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.48459573439538334\n",
      "Num of epochs: 293\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Epoch 121, Loss: 0.49295912913079615\n",
      "Epoch 122, Loss: 0.4924773619139838\n",
      "Epoch 123, Loss: 0.49197607191949805\n",
      "Epoch 124, Loss: 0.4914699197831528\n",
      "Epoch 125, Loss: 0.49091483381312667\n",
      "Epoch 126, Loss: 0.49035147677946317\n",
      "Epoch 127, Loss: 0.4898087222825037\n",
      "Epoch 128, Loss: 0.48924014731550836\n",
      "Epoch 129, Loss: 0.48873550647212044\n",
      "Epoch 130, Loss: 0.48832307254971563\n",
      "Epoch 131, Loss: 0.48791418392821295\n",
      "Epoch 132, Loss: 0.4875902245243796\n",
      "Epoch 133, Loss: 0.48728851108037585\n",
      "Epoch 134, Loss: 0.48697062262181345\n",
      "Epoch 135, Loss: 0.4866339553030548\n",
      "Epoch 136, Loss: 0.48633446746734893\n",
      "Epoch 137, Loss: 0.4860310087358514\n",
      "Epoch 138, Loss: 0.4856786412545615\n",
      "Epoch 139, Loss: 0.48531372107004\n",
      "Epoch 140, Loss: 0.48495712985488587\n",
      "Epoch 141, Loss: 0.4846131600652309\n",
      "Epoch 142, Loss: 0.48425966858652364\n",
      "Epoch 143, Loss: 0.4838914149236435\n",
      "Epoch 144, Loss: 0.48352437546282767\n",
      "Epoch 145, Loss: 0.4832299911937133\n",
      "Epoch 146, Loss: 0.4832586529465723\n",
      "Epoch 147, Loss: 0.48356744143113517\n",
      "Epoch 148, Loss: 0.4828108019011922\n",
      "Epoch 149, Loss: 0.4818912939738211\n",
      "Epoch 150, Loss: 0.48239416745575797\n",
      "Epoch 151, Loss: 0.4814952491721961\n",
      "Epoch 152, Loss: 0.4811822818820502\n",
      "Epoch 153, Loss: 0.4812753156716337\n",
      "Epoch 154, Loss: 0.4804387742282827\n",
      "Epoch 155, Loss: 0.48058189052417905\n",
      "Epoch 156, Loss: 0.4801288115497346\n",
      "Epoch 157, Loss: 0.4797465937246525\n",
      "Epoch 158, Loss: 0.4797622479529733\n",
      "Epoch 159, Loss: 0.4791664679844762\n",
      "Epoch 160, Loss: 0.47904359923301915\n",
      "Epoch 161, Loss: 0.4788483533353555\n",
      "Epoch 162, Loss: 0.4783709093142749\n",
      "Epoch 163, Loss: 0.4783027642591491\n",
      "Epoch 164, Loss: 0.4780003946284736\n",
      "Epoch 165, Loss: 0.47766356610156796\n",
      "Epoch 166, Loss: 0.47756940749162413\n",
      "Epoch 167, Loss: 0.4772720526560437\n",
      "Epoch 168, Loss: 0.477001191512605\n",
      "Epoch 169, Loss: 0.47690651155802954\n",
      "Epoch 170, Loss: 0.47663605231839845\n",
      "Epoch 171, Loss: 0.4763715079887003\n",
      "Epoch 172, Loss: 0.4762479807342815\n",
      "Epoch 173, Loss: 0.47603155482909787\n",
      "Epoch 174, Loss: 0.47580085926956445\n",
      "Epoch 175, Loss: 0.4756396378860013\n",
      "Epoch 176, Loss: 0.4754627232666358\n",
      "Epoch 177, Loss: 0.4752513954316692\n",
      "Epoch 178, Loss: 0.47507140350730925\n",
      "Epoch 179, Loss: 0.47492471273973497\n",
      "Epoch 180, Loss: 0.47475493910732763\n",
      "Epoch 181, Loss: 0.47455157018218247\n",
      "Epoch 182, Loss: 0.4743853224635176\n",
      "Epoch 183, Loss: 0.47424550491599726\n",
      "Epoch 184, Loss: 0.47408413178207975\n",
      "Epoch 185, Loss: 0.47390385374022453\n",
      "Epoch 186, Loss: 0.4737383066259127\n",
      "Epoch 187, Loss: 0.47358797785338436\n",
      "Epoch 188, Loss: 0.4734560449826861\n",
      "Epoch 189, Loss: 0.47335738206045025\n",
      "Epoch 190, Loss: 0.4733162834925985\n",
      "Epoch 191, Loss: 0.4732413020737725\n",
      "Epoch 192, Loss: 0.47313239020014447\n",
      "Epoch 193, Loss: 0.4729457158004841\n",
      "Epoch 194, Loss: 0.47274703738102797\n",
      "Epoch 195, Loss: 0.47253750657270627\n",
      "Epoch 196, Loss: 0.4723544299968521\n",
      "Epoch 197, Loss: 0.47221102907746554\n",
      "Epoch 198, Loss: 0.47210844467503205\n",
      "Epoch 199, Loss: 0.47205337980155637\n",
      "Epoch 200, Loss: 0.47204973383515075\n",
      "Epoch 201, Loss: 0.4720427890593303\n",
      "Epoch 202, Loss: 0.4719159657967507\n",
      "Epoch 203, Loss: 0.471735017109987\n",
      "Epoch 204, Loss: 0.4714985533244086\n",
      "Epoch 205, Loss: 0.4713143302002759\n",
      "Epoch 206, Loss: 0.47118574544243713\n",
      "Epoch 207, Loss: 0.47112002449718005\n",
      "Epoch 208, Loss: 0.4710910670518786\n",
      "Epoch 209, Loss: 0.47102495326923743\n",
      "Epoch 210, Loss: 0.4709626586360175\n",
      "Epoch 211, Loss: 0.47085840979895394\n",
      "Epoch 212, Loss: 0.47078064719388485\n",
      "Epoch 213, Loss: 0.47066614789276784\n",
      "Epoch 214, Loss: 0.47055654499036753\n",
      "Epoch 215, Loss: 0.47041652390832933\n",
      "Epoch 216, Loss: 0.4702446473560784\n",
      "Epoch 217, Loss: 0.4701045016774931\n",
      "Epoch 218, Loss: 0.47002059077245106\n",
      "Epoch 219, Loss: 0.4699643617672827\n",
      "Epoch 220, Loss: 0.4698888296176042\n",
      "Epoch 221, Loss: 0.4698141575467262\n",
      "Epoch 222, Loss: 0.4697265783615268\n",
      "Epoch 223, Loss: 0.46966066909468496\n",
      "Epoch 224, Loss: 0.4696348739248034\n",
      "Epoch 225, Loss: 0.4697176006517021\n",
      "Epoch 226, Loss: 0.4699424675720642\n",
      "Epoch 227, Loss: 0.46999269112017344\n",
      "Epoch 228, Loss: 0.4698759384724347\n",
      "Epoch 229, Loss: 0.46938041574678274\n",
      "Epoch 230, Loss: 0.4691415581864629\n",
      "Epoch 231, Loss: 0.46924839544758656\n",
      "Epoch 232, Loss: 0.46933531775018905\n",
      "Epoch 233, Loss: 0.4692253881106229\n",
      "Epoch 234, Loss: 0.46894320638835874\n",
      "Epoch 235, Loss: 0.4688628060980674\n",
      "Epoch 236, Loss: 0.46897798398181567\n",
      "Epoch 237, Loss: 0.4689811295666005\n",
      "Epoch 238, Loss: 0.4688031484354944\n",
      "Epoch 239, Loss: 0.46862983752908405\n",
      "Epoch 240, Loss: 0.4685963696810109\n",
      "Epoch 241, Loss: 0.46867106090894\n",
      "Epoch 242, Loss: 0.46866360503981047\n",
      "Epoch 243, Loss: 0.468513635806399\n",
      "Epoch 244, Loss: 0.46835730315983837\n",
      "Epoch 245, Loss: 0.468344910742699\n",
      "Epoch 246, Loss: 0.46842993299443175\n",
      "Epoch 247, Loss: 0.4683803372309361\n",
      "Epoch 248, Loss: 0.46823408027364943\n",
      "Epoch 249, Loss: 0.46809559282800506\n",
      "Epoch 250, Loss: 0.4681194355765096\n",
      "Epoch 251, Loss: 0.4682059468617626\n",
      "Epoch 252, Loss: 0.46815125054377077\n",
      "Epoch 253, Loss: 0.46800465135626707\n",
      "Epoch 254, Loss: 0.4678516840148294\n",
      "Epoch 255, Loss: 0.46781997608436393\n",
      "Epoch 256, Loss: 0.4678506648079015\n",
      "Epoch 257, Loss: 0.4677995105079106\n",
      "Epoch 258, Loss: 0.4676942061377158\n",
      "Epoch 259, Loss: 0.46760336187164103\n",
      "Epoch 260, Loss: 0.46754277859764004\n",
      "Epoch 261, Loss: 0.4675358146830681\n",
      "Epoch 262, Loss: 0.4675934670336611\n",
      "Epoch 263, Loss: 0.4675883044263754\n",
      "Epoch 264, Loss: 0.46749525618568777\n",
      "Epoch 265, Loss: 0.46735425416294335\n",
      "Epoch 266, Loss: 0.4672686694514329\n",
      "Epoch 267, Loss: 0.46723722493080344\n",
      "Epoch 268, Loss: 0.4672375438513713\n",
      "Epoch 269, Loss: 0.46722038561562945\n",
      "Epoch 270, Loss: 0.46721867932539873\n",
      "Epoch 271, Loss: 0.46717319721795353\n",
      "Epoch 272, Loss: 0.4671238189214812\n",
      "Epoch 273, Loss: 0.46705709570299314\n",
      "Epoch 274, Loss: 0.4669974785875546\n",
      "Epoch 275, Loss: 0.4669422896788895\n",
      "Epoch 276, Loss: 0.4669034987743203\n",
      "Epoch 277, Loss: 0.46684804340658115\n",
      "Epoch 278, Loss: 0.4668104417178138\n",
      "Epoch 279, Loss: 0.46677111311176783\n",
      "Epoch 280, Loss: 0.46673031256560493\n",
      "Epoch 281, Loss: 0.46669448942736436\n",
      "Epoch 282, Loss: 0.46666225583115023\n",
      "Epoch 283, Loss: 0.4666367100427352\n",
      "Epoch 284, Loss: 0.4666213020629852\n",
      "Epoch 285, Loss: 0.46662540558501797\n",
      "Epoch 286, Loss: 0.4666499301628323\n",
      "Epoch 287, Loss: 0.466693164365506\n",
      "Epoch 288, Loss: 0.46673154174214454\n",
      "Epoch 289, Loss: 0.4666228668343984\n",
      "Epoch 290, Loss: 0.46649923372154634\n",
      "Epoch 291, Loss: 0.46640938673130766\n",
      "Epoch 292, Loss: 0.4663389345853611\n",
      "Epoch 293, Loss: 0.4663027458423396\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.21771715348858842\n",
      "Valence RMSE: 0.20537753106269868\n",
      "Arousal RMSE: 0.22939395715872155\n",
      "Test R^2 score: tensor([0.4888, 0.4790], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.3692, 0.3570], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4838897333934406\n",
      "Num of epochs: 294\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Epoch 121, Loss: 0.49295912913079615\n",
      "Epoch 122, Loss: 0.4924773619139838\n",
      "Epoch 123, Loss: 0.49197607191949805\n",
      "Epoch 124, Loss: 0.4914699197831528\n",
      "Epoch 125, Loss: 0.49091483381312667\n",
      "Epoch 126, Loss: 0.49035147677946317\n",
      "Epoch 127, Loss: 0.4898087222825037\n",
      "Epoch 128, Loss: 0.48924014731550836\n",
      "Epoch 129, Loss: 0.48873550647212044\n",
      "Epoch 130, Loss: 0.48832307254971563\n",
      "Epoch 131, Loss: 0.48791418392821295\n",
      "Epoch 132, Loss: 0.4875902245243796\n",
      "Epoch 133, Loss: 0.48728851108037585\n",
      "Epoch 134, Loss: 0.48697062262181345\n",
      "Epoch 135, Loss: 0.4866339553030548\n",
      "Epoch 136, Loss: 0.48633446746734893\n",
      "Epoch 137, Loss: 0.4860310087358514\n",
      "Epoch 138, Loss: 0.4856786412545615\n",
      "Epoch 139, Loss: 0.48531372107004\n",
      "Epoch 140, Loss: 0.48495712985488587\n",
      "Epoch 141, Loss: 0.4846131600652309\n",
      "Epoch 142, Loss: 0.48425966858652364\n",
      "Epoch 143, Loss: 0.4838914149236435\n",
      "Epoch 144, Loss: 0.48352437546282767\n",
      "Epoch 145, Loss: 0.4832299911937133\n",
      "Epoch 146, Loss: 0.4832586529465723\n",
      "Epoch 147, Loss: 0.48356744143113517\n",
      "Epoch 148, Loss: 0.4828108019011922\n",
      "Epoch 149, Loss: 0.4818912939738211\n",
      "Epoch 150, Loss: 0.48239416745575797\n",
      "Epoch 151, Loss: 0.4814952491721961\n",
      "Epoch 152, Loss: 0.4811822818820502\n",
      "Epoch 153, Loss: 0.4812753156716337\n",
      "Epoch 154, Loss: 0.4804387742282827\n",
      "Epoch 155, Loss: 0.48058189052417905\n",
      "Epoch 156, Loss: 0.4801288115497346\n",
      "Epoch 157, Loss: 0.4797465937246525\n",
      "Epoch 158, Loss: 0.4797622479529733\n",
      "Epoch 159, Loss: 0.4791664679844762\n",
      "Epoch 160, Loss: 0.47904359923301915\n",
      "Epoch 161, Loss: 0.4788483533353555\n",
      "Epoch 162, Loss: 0.4783709093142749\n",
      "Epoch 163, Loss: 0.4783027642591491\n",
      "Epoch 164, Loss: 0.4780003946284736\n",
      "Epoch 165, Loss: 0.47766356610156796\n",
      "Epoch 166, Loss: 0.47756940749162413\n",
      "Epoch 167, Loss: 0.4772720526560437\n",
      "Epoch 168, Loss: 0.477001191512605\n",
      "Epoch 169, Loss: 0.47690651155802954\n",
      "Epoch 170, Loss: 0.47663605231839845\n",
      "Epoch 171, Loss: 0.4763715079887003\n",
      "Epoch 172, Loss: 0.4762479807342815\n",
      "Epoch 173, Loss: 0.47603155482909787\n",
      "Epoch 174, Loss: 0.47580085926956445\n",
      "Epoch 175, Loss: 0.4756396378860013\n",
      "Epoch 176, Loss: 0.4754627232666358\n",
      "Epoch 177, Loss: 0.4752513954316692\n",
      "Epoch 178, Loss: 0.47507140350730925\n",
      "Epoch 179, Loss: 0.47492471273973497\n",
      "Epoch 180, Loss: 0.47475493910732763\n",
      "Epoch 181, Loss: 0.47455157018218247\n",
      "Epoch 182, Loss: 0.4743853224635176\n",
      "Epoch 183, Loss: 0.47424550491599726\n",
      "Epoch 184, Loss: 0.47408413178207975\n",
      "Epoch 185, Loss: 0.47390385374022453\n",
      "Epoch 186, Loss: 0.4737383066259127\n",
      "Epoch 187, Loss: 0.47358797785338436\n",
      "Epoch 188, Loss: 0.4734560449826861\n",
      "Epoch 189, Loss: 0.47335738206045025\n",
      "Epoch 190, Loss: 0.4733162834925985\n",
      "Epoch 191, Loss: 0.4732413020737725\n",
      "Epoch 192, Loss: 0.47313239020014447\n",
      "Epoch 193, Loss: 0.4729457158004841\n",
      "Epoch 194, Loss: 0.47274703738102797\n",
      "Epoch 195, Loss: 0.47253750657270627\n",
      "Epoch 196, Loss: 0.4723544299968521\n",
      "Epoch 197, Loss: 0.47221102907746554\n",
      "Epoch 198, Loss: 0.47210844467503205\n",
      "Epoch 199, Loss: 0.47205337980155637\n",
      "Epoch 200, Loss: 0.47204973383515075\n",
      "Epoch 201, Loss: 0.4720427890593303\n",
      "Epoch 202, Loss: 0.4719159657967507\n",
      "Epoch 203, Loss: 0.471735017109987\n",
      "Epoch 204, Loss: 0.4714985533244086\n",
      "Epoch 205, Loss: 0.4713143302002759\n",
      "Epoch 206, Loss: 0.47118574544243713\n",
      "Epoch 207, Loss: 0.47112002449718005\n",
      "Epoch 208, Loss: 0.4710910670518786\n",
      "Epoch 209, Loss: 0.47102495326923743\n",
      "Epoch 210, Loss: 0.4709626586360175\n",
      "Epoch 211, Loss: 0.47085840979895394\n",
      "Epoch 212, Loss: 0.47078064719388485\n",
      "Epoch 213, Loss: 0.47066614789276784\n",
      "Epoch 214, Loss: 0.47055654499036753\n",
      "Epoch 215, Loss: 0.47041652390832933\n",
      "Epoch 216, Loss: 0.4702446473560784\n",
      "Epoch 217, Loss: 0.4701045016774931\n",
      "Epoch 218, Loss: 0.47002059077245106\n",
      "Epoch 219, Loss: 0.4699643617672827\n",
      "Epoch 220, Loss: 0.4698888296176042\n",
      "Epoch 221, Loss: 0.4698141575467262\n",
      "Epoch 222, Loss: 0.4697265783615268\n",
      "Epoch 223, Loss: 0.46966066909468496\n",
      "Epoch 224, Loss: 0.4696348739248034\n",
      "Epoch 225, Loss: 0.4697176006517021\n",
      "Epoch 226, Loss: 0.4699424675720642\n",
      "Epoch 227, Loss: 0.46999269112017344\n",
      "Epoch 228, Loss: 0.4698759384724347\n",
      "Epoch 229, Loss: 0.46938041574678274\n",
      "Epoch 230, Loss: 0.4691415581864629\n",
      "Epoch 231, Loss: 0.46924839544758656\n",
      "Epoch 232, Loss: 0.46933531775018905\n",
      "Epoch 233, Loss: 0.4692253881106229\n",
      "Epoch 234, Loss: 0.46894320638835874\n",
      "Epoch 235, Loss: 0.4688628060980674\n",
      "Epoch 236, Loss: 0.46897798398181567\n",
      "Epoch 237, Loss: 0.4689811295666005\n",
      "Epoch 238, Loss: 0.4688031484354944\n",
      "Epoch 239, Loss: 0.46862983752908405\n",
      "Epoch 240, Loss: 0.4685963696810109\n",
      "Epoch 241, Loss: 0.46867106090894\n",
      "Epoch 242, Loss: 0.46866360503981047\n",
      "Epoch 243, Loss: 0.468513635806399\n",
      "Epoch 244, Loss: 0.46835730315983837\n",
      "Epoch 245, Loss: 0.468344910742699\n",
      "Epoch 246, Loss: 0.46842993299443175\n",
      "Epoch 247, Loss: 0.4683803372309361\n",
      "Epoch 248, Loss: 0.46823408027364943\n",
      "Epoch 249, Loss: 0.46809559282800506\n",
      "Epoch 250, Loss: 0.4681194355765096\n",
      "Epoch 251, Loss: 0.4682059468617626\n",
      "Epoch 252, Loss: 0.46815125054377077\n",
      "Epoch 253, Loss: 0.46800465135626707\n",
      "Epoch 254, Loss: 0.4678516840148294\n",
      "Epoch 255, Loss: 0.46781997608436393\n",
      "Epoch 256, Loss: 0.4678506648079015\n",
      "Epoch 257, Loss: 0.4677995105079106\n",
      "Epoch 258, Loss: 0.4676942061377158\n",
      "Epoch 259, Loss: 0.46760336187164103\n",
      "Epoch 260, Loss: 0.46754277859764004\n",
      "Epoch 261, Loss: 0.4675358146830681\n",
      "Epoch 262, Loss: 0.4675934670336611\n",
      "Epoch 263, Loss: 0.4675883044263754\n",
      "Epoch 264, Loss: 0.46749525618568777\n",
      "Epoch 265, Loss: 0.46735425416294335\n",
      "Epoch 266, Loss: 0.4672686694514329\n",
      "Epoch 267, Loss: 0.46723722493080344\n",
      "Epoch 268, Loss: 0.4672375438513713\n",
      "Epoch 269, Loss: 0.46722038561562945\n",
      "Epoch 270, Loss: 0.46721867932539873\n",
      "Epoch 271, Loss: 0.46717319721795353\n",
      "Epoch 272, Loss: 0.4671238189214812\n",
      "Epoch 273, Loss: 0.46705709570299314\n",
      "Epoch 274, Loss: 0.4669974785875546\n",
      "Epoch 275, Loss: 0.4669422896788895\n",
      "Epoch 276, Loss: 0.4669034987743203\n",
      "Epoch 277, Loss: 0.46684804340658115\n",
      "Epoch 278, Loss: 0.4668104417178138\n",
      "Epoch 279, Loss: 0.46677111311176783\n",
      "Epoch 280, Loss: 0.46673031256560493\n",
      "Epoch 281, Loss: 0.46669448942736436\n",
      "Epoch 282, Loss: 0.46666225583115023\n",
      "Epoch 283, Loss: 0.4666367100427352\n",
      "Epoch 284, Loss: 0.4666213020629852\n",
      "Epoch 285, Loss: 0.46662540558501797\n",
      "Epoch 286, Loss: 0.4666499301628323\n",
      "Epoch 287, Loss: 0.466693164365506\n",
      "Epoch 288, Loss: 0.46673154174214454\n",
      "Epoch 289, Loss: 0.4666228668343984\n",
      "Epoch 290, Loss: 0.46649923372154634\n",
      "Epoch 291, Loss: 0.46640938673130766\n",
      "Epoch 292, Loss: 0.4663389345853611\n",
      "Epoch 293, Loss: 0.4663027458423396\n",
      "Epoch 294, Loss: 0.46628067971608783\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.21728237722451196\n",
      "Valence RMSE: 0.2049846979661786\n",
      "Arousal RMSE: 0.22892037153644662\n",
      "Test R^2 score: tensor([0.4908, 0.4811], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.3716, 0.3597], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4859411330552654\n",
      "Num of epochs: 295\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Epoch 121, Loss: 0.49295912913079615\n",
      "Epoch 122, Loss: 0.4924773619139838\n",
      "Epoch 123, Loss: 0.49197607191949805\n",
      "Epoch 124, Loss: 0.4914699197831528\n",
      "Epoch 125, Loss: 0.49091483381312667\n",
      "Epoch 126, Loss: 0.49035147677946317\n",
      "Epoch 127, Loss: 0.4898087222825037\n",
      "Epoch 128, Loss: 0.48924014731550836\n",
      "Epoch 129, Loss: 0.48873550647212044\n",
      "Epoch 130, Loss: 0.48832307254971563\n",
      "Epoch 131, Loss: 0.48791418392821295\n",
      "Epoch 132, Loss: 0.4875902245243796\n",
      "Epoch 133, Loss: 0.48728851108037585\n",
      "Epoch 134, Loss: 0.48697062262181345\n",
      "Epoch 135, Loss: 0.4866339553030548\n",
      "Epoch 136, Loss: 0.48633446746734893\n",
      "Epoch 137, Loss: 0.4860310087358514\n",
      "Epoch 138, Loss: 0.4856786412545615\n",
      "Epoch 139, Loss: 0.48531372107004\n",
      "Epoch 140, Loss: 0.48495712985488587\n",
      "Epoch 141, Loss: 0.4846131600652309\n",
      "Epoch 142, Loss: 0.48425966858652364\n",
      "Epoch 143, Loss: 0.4838914149236435\n",
      "Epoch 144, Loss: 0.48352437546282767\n",
      "Epoch 145, Loss: 0.4832299911937133\n",
      "Epoch 146, Loss: 0.4832586529465723\n",
      "Epoch 147, Loss: 0.48356744143113517\n",
      "Epoch 148, Loss: 0.4828108019011922\n",
      "Epoch 149, Loss: 0.4818912939738211\n",
      "Epoch 150, Loss: 0.48239416745575797\n",
      "Epoch 151, Loss: 0.4814952491721961\n",
      "Epoch 152, Loss: 0.4811822818820502\n",
      "Epoch 153, Loss: 0.4812753156716337\n",
      "Epoch 154, Loss: 0.4804387742282827\n",
      "Epoch 155, Loss: 0.48058189052417905\n",
      "Epoch 156, Loss: 0.4801288115497346\n",
      "Epoch 157, Loss: 0.4797465937246525\n",
      "Epoch 158, Loss: 0.4797622479529733\n",
      "Epoch 159, Loss: 0.4791664679844762\n",
      "Epoch 160, Loss: 0.47904359923301915\n",
      "Epoch 161, Loss: 0.4788483533353555\n",
      "Epoch 162, Loss: 0.4783709093142749\n",
      "Epoch 163, Loss: 0.4783027642591491\n",
      "Epoch 164, Loss: 0.4780003946284736\n",
      "Epoch 165, Loss: 0.47766356610156796\n",
      "Epoch 166, Loss: 0.47756940749162413\n",
      "Epoch 167, Loss: 0.4772720526560437\n",
      "Epoch 168, Loss: 0.477001191512605\n",
      "Epoch 169, Loss: 0.47690651155802954\n",
      "Epoch 170, Loss: 0.47663605231839845\n",
      "Epoch 171, Loss: 0.4763715079887003\n",
      "Epoch 172, Loss: 0.4762479807342815\n",
      "Epoch 173, Loss: 0.47603155482909787\n",
      "Epoch 174, Loss: 0.47580085926956445\n",
      "Epoch 175, Loss: 0.4756396378860013\n",
      "Epoch 176, Loss: 0.4754627232666358\n",
      "Epoch 177, Loss: 0.4752513954316692\n",
      "Epoch 178, Loss: 0.47507140350730925\n",
      "Epoch 179, Loss: 0.47492471273973497\n",
      "Epoch 180, Loss: 0.47475493910732763\n",
      "Epoch 181, Loss: 0.47455157018218247\n",
      "Epoch 182, Loss: 0.4743853224635176\n",
      "Epoch 183, Loss: 0.47424550491599726\n",
      "Epoch 184, Loss: 0.47408413178207975\n",
      "Epoch 185, Loss: 0.47390385374022453\n",
      "Epoch 186, Loss: 0.4737383066259127\n",
      "Epoch 187, Loss: 0.47358797785338436\n",
      "Epoch 188, Loss: 0.4734560449826861\n",
      "Epoch 189, Loss: 0.47335738206045025\n",
      "Epoch 190, Loss: 0.4733162834925985\n",
      "Epoch 191, Loss: 0.4732413020737725\n",
      "Epoch 192, Loss: 0.47313239020014447\n",
      "Epoch 193, Loss: 0.4729457158004841\n",
      "Epoch 194, Loss: 0.47274703738102797\n",
      "Epoch 195, Loss: 0.47253750657270627\n",
      "Epoch 196, Loss: 0.4723544299968521\n",
      "Epoch 197, Loss: 0.47221102907746554\n",
      "Epoch 198, Loss: 0.47210844467503205\n",
      "Epoch 199, Loss: 0.47205337980155637\n",
      "Epoch 200, Loss: 0.47204973383515075\n",
      "Epoch 201, Loss: 0.4720427890593303\n",
      "Epoch 202, Loss: 0.4719159657967507\n",
      "Epoch 203, Loss: 0.471735017109987\n",
      "Epoch 204, Loss: 0.4714985533244086\n",
      "Epoch 205, Loss: 0.4713143302002759\n",
      "Epoch 206, Loss: 0.47118574544243713\n",
      "Epoch 207, Loss: 0.47112002449718005\n",
      "Epoch 208, Loss: 0.4710910670518786\n",
      "Epoch 209, Loss: 0.47102495326923743\n",
      "Epoch 210, Loss: 0.4709626586360175\n",
      "Epoch 211, Loss: 0.47085840979895394\n",
      "Epoch 212, Loss: 0.47078064719388485\n",
      "Epoch 213, Loss: 0.47066614789276784\n",
      "Epoch 214, Loss: 0.47055654499036753\n",
      "Epoch 215, Loss: 0.47041652390832933\n",
      "Epoch 216, Loss: 0.4702446473560784\n",
      "Epoch 217, Loss: 0.4701045016774931\n",
      "Epoch 218, Loss: 0.47002059077245106\n",
      "Epoch 219, Loss: 0.4699643617672827\n",
      "Epoch 220, Loss: 0.4698888296176042\n",
      "Epoch 221, Loss: 0.4698141575467262\n",
      "Epoch 222, Loss: 0.4697265783615268\n",
      "Epoch 223, Loss: 0.46966066909468496\n",
      "Epoch 224, Loss: 0.4696348739248034\n",
      "Epoch 225, Loss: 0.4697176006517021\n",
      "Epoch 226, Loss: 0.4699424675720642\n",
      "Epoch 227, Loss: 0.46999269112017344\n",
      "Epoch 228, Loss: 0.4698759384724347\n",
      "Epoch 229, Loss: 0.46938041574678274\n",
      "Epoch 230, Loss: 0.4691415581864629\n",
      "Epoch 231, Loss: 0.46924839544758656\n",
      "Epoch 232, Loss: 0.46933531775018905\n",
      "Epoch 233, Loss: 0.4692253881106229\n",
      "Epoch 234, Loss: 0.46894320638835874\n",
      "Epoch 235, Loss: 0.4688628060980674\n",
      "Epoch 236, Loss: 0.46897798398181567\n",
      "Epoch 237, Loss: 0.4689811295666005\n",
      "Epoch 238, Loss: 0.4688031484354944\n",
      "Epoch 239, Loss: 0.46862983752908405\n",
      "Epoch 240, Loss: 0.4685963696810109\n",
      "Epoch 241, Loss: 0.46867106090894\n",
      "Epoch 242, Loss: 0.46866360503981047\n",
      "Epoch 243, Loss: 0.468513635806399\n",
      "Epoch 244, Loss: 0.46835730315983837\n",
      "Epoch 245, Loss: 0.468344910742699\n",
      "Epoch 246, Loss: 0.46842993299443175\n",
      "Epoch 247, Loss: 0.4683803372309361\n",
      "Epoch 248, Loss: 0.46823408027364943\n",
      "Epoch 249, Loss: 0.46809559282800506\n",
      "Epoch 250, Loss: 0.4681194355765096\n",
      "Epoch 251, Loss: 0.4682059468617626\n",
      "Epoch 252, Loss: 0.46815125054377077\n",
      "Epoch 253, Loss: 0.46800465135626707\n",
      "Epoch 254, Loss: 0.4678516840148294\n",
      "Epoch 255, Loss: 0.46781997608436393\n",
      "Epoch 256, Loss: 0.4678506648079015\n",
      "Epoch 257, Loss: 0.4677995105079106\n",
      "Epoch 258, Loss: 0.4676942061377158\n",
      "Epoch 259, Loss: 0.46760336187164103\n",
      "Epoch 260, Loss: 0.46754277859764004\n",
      "Epoch 261, Loss: 0.4675358146830681\n",
      "Epoch 262, Loss: 0.4675934670336611\n",
      "Epoch 263, Loss: 0.4675883044263754\n",
      "Epoch 264, Loss: 0.46749525618568777\n",
      "Epoch 265, Loss: 0.46735425416294335\n",
      "Epoch 266, Loss: 0.4672686694514329\n",
      "Epoch 267, Loss: 0.46723722493080344\n",
      "Epoch 268, Loss: 0.4672375438513713\n",
      "Epoch 269, Loss: 0.46722038561562945\n",
      "Epoch 270, Loss: 0.46721867932539873\n",
      "Epoch 271, Loss: 0.46717319721795353\n",
      "Epoch 272, Loss: 0.4671238189214812\n",
      "Epoch 273, Loss: 0.46705709570299314\n",
      "Epoch 274, Loss: 0.4669974785875546\n",
      "Epoch 275, Loss: 0.4669422896788895\n",
      "Epoch 276, Loss: 0.4669034987743203\n",
      "Epoch 277, Loss: 0.46684804340658115\n",
      "Epoch 278, Loss: 0.4668104417178138\n",
      "Epoch 279, Loss: 0.46677111311176783\n",
      "Epoch 280, Loss: 0.46673031256560493\n",
      "Epoch 281, Loss: 0.46669448942736436\n",
      "Epoch 282, Loss: 0.46666225583115023\n",
      "Epoch 283, Loss: 0.4666367100427352\n",
      "Epoch 284, Loss: 0.4666213020629852\n",
      "Epoch 285, Loss: 0.46662540558501797\n",
      "Epoch 286, Loss: 0.4666499301628323\n",
      "Epoch 287, Loss: 0.466693164365506\n",
      "Epoch 288, Loss: 0.46673154174214454\n",
      "Epoch 289, Loss: 0.4666228668343984\n",
      "Epoch 290, Loss: 0.46649923372154634\n",
      "Epoch 291, Loss: 0.46640938673130766\n",
      "Epoch 292, Loss: 0.4663389345853611\n",
      "Epoch 293, Loss: 0.4663027458423396\n",
      "Epoch 294, Loss: 0.46628067971608783\n",
      "Epoch 295, Loss: 0.46627999262949893\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.21813901315730547\n",
      "Valence RMSE: 0.2058034131722952\n",
      "Arousal RMSE: 0.22981343139407606\n",
      "Test R^2 score: tensor([0.4867, 0.4771], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.3666, 0.3547], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4818749679029802\n",
      "Num of epochs: 296\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Epoch 121, Loss: 0.49295912913079615\n",
      "Epoch 122, Loss: 0.4924773619139838\n",
      "Epoch 123, Loss: 0.49197607191949805\n",
      "Epoch 124, Loss: 0.4914699197831528\n",
      "Epoch 125, Loss: 0.49091483381312667\n",
      "Epoch 126, Loss: 0.49035147677946317\n",
      "Epoch 127, Loss: 0.4898087222825037\n",
      "Epoch 128, Loss: 0.48924014731550836\n",
      "Epoch 129, Loss: 0.48873550647212044\n",
      "Epoch 130, Loss: 0.48832307254971563\n",
      "Epoch 131, Loss: 0.48791418392821295\n",
      "Epoch 132, Loss: 0.4875902245243796\n",
      "Epoch 133, Loss: 0.48728851108037585\n",
      "Epoch 134, Loss: 0.48697062262181345\n",
      "Epoch 135, Loss: 0.4866339553030548\n",
      "Epoch 136, Loss: 0.48633446746734893\n",
      "Epoch 137, Loss: 0.4860310087358514\n",
      "Epoch 138, Loss: 0.4856786412545615\n",
      "Epoch 139, Loss: 0.48531372107004\n",
      "Epoch 140, Loss: 0.48495712985488587\n",
      "Epoch 141, Loss: 0.4846131600652309\n",
      "Epoch 142, Loss: 0.48425966858652364\n",
      "Epoch 143, Loss: 0.4838914149236435\n",
      "Epoch 144, Loss: 0.48352437546282767\n",
      "Epoch 145, Loss: 0.4832299911937133\n",
      "Epoch 146, Loss: 0.4832586529465723\n",
      "Epoch 147, Loss: 0.48356744143113517\n",
      "Epoch 148, Loss: 0.4828108019011922\n",
      "Epoch 149, Loss: 0.4818912939738211\n",
      "Epoch 150, Loss: 0.48239416745575797\n",
      "Epoch 151, Loss: 0.4814952491721961\n",
      "Epoch 152, Loss: 0.4811822818820502\n",
      "Epoch 153, Loss: 0.4812753156716337\n",
      "Epoch 154, Loss: 0.4804387742282827\n",
      "Epoch 155, Loss: 0.48058189052417905\n",
      "Epoch 156, Loss: 0.4801288115497346\n",
      "Epoch 157, Loss: 0.4797465937246525\n",
      "Epoch 158, Loss: 0.4797622479529733\n",
      "Epoch 159, Loss: 0.4791664679844762\n",
      "Epoch 160, Loss: 0.47904359923301915\n",
      "Epoch 161, Loss: 0.4788483533353555\n",
      "Epoch 162, Loss: 0.4783709093142749\n",
      "Epoch 163, Loss: 0.4783027642591491\n",
      "Epoch 164, Loss: 0.4780003946284736\n",
      "Epoch 165, Loss: 0.47766356610156796\n",
      "Epoch 166, Loss: 0.47756940749162413\n",
      "Epoch 167, Loss: 0.4772720526560437\n",
      "Epoch 168, Loss: 0.477001191512605\n",
      "Epoch 169, Loss: 0.47690651155802954\n",
      "Epoch 170, Loss: 0.47663605231839845\n",
      "Epoch 171, Loss: 0.4763715079887003\n",
      "Epoch 172, Loss: 0.4762479807342815\n",
      "Epoch 173, Loss: 0.47603155482909787\n",
      "Epoch 174, Loss: 0.47580085926956445\n",
      "Epoch 175, Loss: 0.4756396378860013\n",
      "Epoch 176, Loss: 0.4754627232666358\n",
      "Epoch 177, Loss: 0.4752513954316692\n",
      "Epoch 178, Loss: 0.47507140350730925\n",
      "Epoch 179, Loss: 0.47492471273973497\n",
      "Epoch 180, Loss: 0.47475493910732763\n",
      "Epoch 181, Loss: 0.47455157018218247\n",
      "Epoch 182, Loss: 0.4743853224635176\n",
      "Epoch 183, Loss: 0.47424550491599726\n",
      "Epoch 184, Loss: 0.47408413178207975\n",
      "Epoch 185, Loss: 0.47390385374022453\n",
      "Epoch 186, Loss: 0.4737383066259127\n",
      "Epoch 187, Loss: 0.47358797785338436\n",
      "Epoch 188, Loss: 0.4734560449826861\n",
      "Epoch 189, Loss: 0.47335738206045025\n",
      "Epoch 190, Loss: 0.4733162834925985\n",
      "Epoch 191, Loss: 0.4732413020737725\n",
      "Epoch 192, Loss: 0.47313239020014447\n",
      "Epoch 193, Loss: 0.4729457158004841\n",
      "Epoch 194, Loss: 0.47274703738102797\n",
      "Epoch 195, Loss: 0.47253750657270627\n",
      "Epoch 196, Loss: 0.4723544299968521\n",
      "Epoch 197, Loss: 0.47221102907746554\n",
      "Epoch 198, Loss: 0.47210844467503205\n",
      "Epoch 199, Loss: 0.47205337980155637\n",
      "Epoch 200, Loss: 0.47204973383515075\n",
      "Epoch 201, Loss: 0.4720427890593303\n",
      "Epoch 202, Loss: 0.4719159657967507\n",
      "Epoch 203, Loss: 0.471735017109987\n",
      "Epoch 204, Loss: 0.4714985533244086\n",
      "Epoch 205, Loss: 0.4713143302002759\n",
      "Epoch 206, Loss: 0.47118574544243713\n",
      "Epoch 207, Loss: 0.47112002449718005\n",
      "Epoch 208, Loss: 0.4710910670518786\n",
      "Epoch 209, Loss: 0.47102495326923743\n",
      "Epoch 210, Loss: 0.4709626586360175\n",
      "Epoch 211, Loss: 0.47085840979895394\n",
      "Epoch 212, Loss: 0.47078064719388485\n",
      "Epoch 213, Loss: 0.47066614789276784\n",
      "Epoch 214, Loss: 0.47055654499036753\n",
      "Epoch 215, Loss: 0.47041652390832933\n",
      "Epoch 216, Loss: 0.4702446473560784\n",
      "Epoch 217, Loss: 0.4701045016774931\n",
      "Epoch 218, Loss: 0.47002059077245106\n",
      "Epoch 219, Loss: 0.4699643617672827\n",
      "Epoch 220, Loss: 0.4698888296176042\n",
      "Epoch 221, Loss: 0.4698141575467262\n",
      "Epoch 222, Loss: 0.4697265783615268\n",
      "Epoch 223, Loss: 0.46966066909468496\n",
      "Epoch 224, Loss: 0.4696348739248034\n",
      "Epoch 225, Loss: 0.4697176006517021\n",
      "Epoch 226, Loss: 0.4699424675720642\n",
      "Epoch 227, Loss: 0.46999269112017344\n",
      "Epoch 228, Loss: 0.4698759384724347\n",
      "Epoch 229, Loss: 0.46938041574678274\n",
      "Epoch 230, Loss: 0.4691415581864629\n",
      "Epoch 231, Loss: 0.46924839544758656\n",
      "Epoch 232, Loss: 0.46933531775018905\n",
      "Epoch 233, Loss: 0.4692253881106229\n",
      "Epoch 234, Loss: 0.46894320638835874\n",
      "Epoch 235, Loss: 0.4688628060980674\n",
      "Epoch 236, Loss: 0.46897798398181567\n",
      "Epoch 237, Loss: 0.4689811295666005\n",
      "Epoch 238, Loss: 0.4688031484354944\n",
      "Epoch 239, Loss: 0.46862983752908405\n",
      "Epoch 240, Loss: 0.4685963696810109\n",
      "Epoch 241, Loss: 0.46867106090894\n",
      "Epoch 242, Loss: 0.46866360503981047\n",
      "Epoch 243, Loss: 0.468513635806399\n",
      "Epoch 244, Loss: 0.46835730315983837\n",
      "Epoch 245, Loss: 0.468344910742699\n",
      "Epoch 246, Loss: 0.46842993299443175\n",
      "Epoch 247, Loss: 0.4683803372309361\n",
      "Epoch 248, Loss: 0.46823408027364943\n",
      "Epoch 249, Loss: 0.46809559282800506\n",
      "Epoch 250, Loss: 0.4681194355765096\n",
      "Epoch 251, Loss: 0.4682059468617626\n",
      "Epoch 252, Loss: 0.46815125054377077\n",
      "Epoch 253, Loss: 0.46800465135626707\n",
      "Epoch 254, Loss: 0.4678516840148294\n",
      "Epoch 255, Loss: 0.46781997608436393\n",
      "Epoch 256, Loss: 0.4678506648079015\n",
      "Epoch 257, Loss: 0.4677995105079106\n",
      "Epoch 258, Loss: 0.4676942061377158\n",
      "Epoch 259, Loss: 0.46760336187164103\n",
      "Epoch 260, Loss: 0.46754277859764004\n",
      "Epoch 261, Loss: 0.4675358146830681\n",
      "Epoch 262, Loss: 0.4675934670336611\n",
      "Epoch 263, Loss: 0.4675883044263754\n",
      "Epoch 264, Loss: 0.46749525618568777\n",
      "Epoch 265, Loss: 0.46735425416294335\n",
      "Epoch 266, Loss: 0.4672686694514329\n",
      "Epoch 267, Loss: 0.46723722493080344\n",
      "Epoch 268, Loss: 0.4672375438513713\n",
      "Epoch 269, Loss: 0.46722038561562945\n",
      "Epoch 270, Loss: 0.46721867932539873\n",
      "Epoch 271, Loss: 0.46717319721795353\n",
      "Epoch 272, Loss: 0.4671238189214812\n",
      "Epoch 273, Loss: 0.46705709570299314\n",
      "Epoch 274, Loss: 0.4669974785875546\n",
      "Epoch 275, Loss: 0.4669422896788895\n",
      "Epoch 276, Loss: 0.4669034987743203\n",
      "Epoch 277, Loss: 0.46684804340658115\n",
      "Epoch 278, Loss: 0.4668104417178138\n",
      "Epoch 279, Loss: 0.46677111311176783\n",
      "Epoch 280, Loss: 0.46673031256560493\n",
      "Epoch 281, Loss: 0.46669448942736436\n",
      "Epoch 282, Loss: 0.46666225583115023\n",
      "Epoch 283, Loss: 0.4666367100427352\n",
      "Epoch 284, Loss: 0.4666213020629852\n",
      "Epoch 285, Loss: 0.46662540558501797\n",
      "Epoch 286, Loss: 0.4666499301628323\n",
      "Epoch 287, Loss: 0.466693164365506\n",
      "Epoch 288, Loss: 0.46673154174214454\n",
      "Epoch 289, Loss: 0.4666228668343984\n",
      "Epoch 290, Loss: 0.46649923372154634\n",
      "Epoch 291, Loss: 0.46640938673130766\n",
      "Epoch 292, Loss: 0.4663389345853611\n",
      "Epoch 293, Loss: 0.4663027458423396\n",
      "Epoch 294, Loss: 0.46628067971608783\n",
      "Epoch 295, Loss: 0.46627999262949893\n",
      "Epoch 296, Loss: 0.46630041304996855\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2170539833494879\n",
      "Valence RMSE: 0.2047965036497352\n",
      "Arousal RMSE: 0.228655320228075\n",
      "Test R^2 score: tensor([0.4917, 0.4823], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.3727, 0.3611], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.48700887541971283\n",
      "Num of epochs: 297\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Epoch 121, Loss: 0.49295912913079615\n",
      "Epoch 122, Loss: 0.4924773619139838\n",
      "Epoch 123, Loss: 0.49197607191949805\n",
      "Epoch 124, Loss: 0.4914699197831528\n",
      "Epoch 125, Loss: 0.49091483381312667\n",
      "Epoch 126, Loss: 0.49035147677946317\n",
      "Epoch 127, Loss: 0.4898087222825037\n",
      "Epoch 128, Loss: 0.48924014731550836\n",
      "Epoch 129, Loss: 0.48873550647212044\n",
      "Epoch 130, Loss: 0.48832307254971563\n",
      "Epoch 131, Loss: 0.48791418392821295\n",
      "Epoch 132, Loss: 0.4875902245243796\n",
      "Epoch 133, Loss: 0.48728851108037585\n",
      "Epoch 134, Loss: 0.48697062262181345\n",
      "Epoch 135, Loss: 0.4866339553030548\n",
      "Epoch 136, Loss: 0.48633446746734893\n",
      "Epoch 137, Loss: 0.4860310087358514\n",
      "Epoch 138, Loss: 0.4856786412545615\n",
      "Epoch 139, Loss: 0.48531372107004\n",
      "Epoch 140, Loss: 0.48495712985488587\n",
      "Epoch 141, Loss: 0.4846131600652309\n",
      "Epoch 142, Loss: 0.48425966858652364\n",
      "Epoch 143, Loss: 0.4838914149236435\n",
      "Epoch 144, Loss: 0.48352437546282767\n",
      "Epoch 145, Loss: 0.4832299911937133\n",
      "Epoch 146, Loss: 0.4832586529465723\n",
      "Epoch 147, Loss: 0.48356744143113517\n",
      "Epoch 148, Loss: 0.4828108019011922\n",
      "Epoch 149, Loss: 0.4818912939738211\n",
      "Epoch 150, Loss: 0.48239416745575797\n",
      "Epoch 151, Loss: 0.4814952491721961\n",
      "Epoch 152, Loss: 0.4811822818820502\n",
      "Epoch 153, Loss: 0.4812753156716337\n",
      "Epoch 154, Loss: 0.4804387742282827\n",
      "Epoch 155, Loss: 0.48058189052417905\n",
      "Epoch 156, Loss: 0.4801288115497346\n",
      "Epoch 157, Loss: 0.4797465937246525\n",
      "Epoch 158, Loss: 0.4797622479529733\n",
      "Epoch 159, Loss: 0.4791664679844762\n",
      "Epoch 160, Loss: 0.47904359923301915\n",
      "Epoch 161, Loss: 0.4788483533353555\n",
      "Epoch 162, Loss: 0.4783709093142749\n",
      "Epoch 163, Loss: 0.4783027642591491\n",
      "Epoch 164, Loss: 0.4780003946284736\n",
      "Epoch 165, Loss: 0.47766356610156796\n",
      "Epoch 166, Loss: 0.47756940749162413\n",
      "Epoch 167, Loss: 0.4772720526560437\n",
      "Epoch 168, Loss: 0.477001191512605\n",
      "Epoch 169, Loss: 0.47690651155802954\n",
      "Epoch 170, Loss: 0.47663605231839845\n",
      "Epoch 171, Loss: 0.4763715079887003\n",
      "Epoch 172, Loss: 0.4762479807342815\n",
      "Epoch 173, Loss: 0.47603155482909787\n",
      "Epoch 174, Loss: 0.47580085926956445\n",
      "Epoch 175, Loss: 0.4756396378860013\n",
      "Epoch 176, Loss: 0.4754627232666358\n",
      "Epoch 177, Loss: 0.4752513954316692\n",
      "Epoch 178, Loss: 0.47507140350730925\n",
      "Epoch 179, Loss: 0.47492471273973497\n",
      "Epoch 180, Loss: 0.47475493910732763\n",
      "Epoch 181, Loss: 0.47455157018218247\n",
      "Epoch 182, Loss: 0.4743853224635176\n",
      "Epoch 183, Loss: 0.47424550491599726\n",
      "Epoch 184, Loss: 0.47408413178207975\n",
      "Epoch 185, Loss: 0.47390385374022453\n",
      "Epoch 186, Loss: 0.4737383066259127\n",
      "Epoch 187, Loss: 0.47358797785338436\n",
      "Epoch 188, Loss: 0.4734560449826861\n",
      "Epoch 189, Loss: 0.47335738206045025\n",
      "Epoch 190, Loss: 0.4733162834925985\n",
      "Epoch 191, Loss: 0.4732413020737725\n",
      "Epoch 192, Loss: 0.47313239020014447\n",
      "Epoch 193, Loss: 0.4729457158004841\n",
      "Epoch 194, Loss: 0.47274703738102797\n",
      "Epoch 195, Loss: 0.47253750657270627\n",
      "Epoch 196, Loss: 0.4723544299968521\n",
      "Epoch 197, Loss: 0.47221102907746554\n",
      "Epoch 198, Loss: 0.47210844467503205\n",
      "Epoch 199, Loss: 0.47205337980155637\n",
      "Epoch 200, Loss: 0.47204973383515075\n",
      "Epoch 201, Loss: 0.4720427890593303\n",
      "Epoch 202, Loss: 0.4719159657967507\n",
      "Epoch 203, Loss: 0.471735017109987\n",
      "Epoch 204, Loss: 0.4714985533244086\n",
      "Epoch 205, Loss: 0.4713143302002759\n",
      "Epoch 206, Loss: 0.47118574544243713\n",
      "Epoch 207, Loss: 0.47112002449718005\n",
      "Epoch 208, Loss: 0.4710910670518786\n",
      "Epoch 209, Loss: 0.47102495326923743\n",
      "Epoch 210, Loss: 0.4709626586360175\n",
      "Epoch 211, Loss: 0.47085840979895394\n",
      "Epoch 212, Loss: 0.47078064719388485\n",
      "Epoch 213, Loss: 0.47066614789276784\n",
      "Epoch 214, Loss: 0.47055654499036753\n",
      "Epoch 215, Loss: 0.47041652390832933\n",
      "Epoch 216, Loss: 0.4702446473560784\n",
      "Epoch 217, Loss: 0.4701045016774931\n",
      "Epoch 218, Loss: 0.47002059077245106\n",
      "Epoch 219, Loss: 0.4699643617672827\n",
      "Epoch 220, Loss: 0.4698888296176042\n",
      "Epoch 221, Loss: 0.4698141575467262\n",
      "Epoch 222, Loss: 0.4697265783615268\n",
      "Epoch 223, Loss: 0.46966066909468496\n",
      "Epoch 224, Loss: 0.4696348739248034\n",
      "Epoch 225, Loss: 0.4697176006517021\n",
      "Epoch 226, Loss: 0.4699424675720642\n",
      "Epoch 227, Loss: 0.46999269112017344\n",
      "Epoch 228, Loss: 0.4698759384724347\n",
      "Epoch 229, Loss: 0.46938041574678274\n",
      "Epoch 230, Loss: 0.4691415581864629\n",
      "Epoch 231, Loss: 0.46924839544758656\n",
      "Epoch 232, Loss: 0.46933531775018905\n",
      "Epoch 233, Loss: 0.4692253881106229\n",
      "Epoch 234, Loss: 0.46894320638835874\n",
      "Epoch 235, Loss: 0.4688628060980674\n",
      "Epoch 236, Loss: 0.46897798398181567\n",
      "Epoch 237, Loss: 0.4689811295666005\n",
      "Epoch 238, Loss: 0.4688031484354944\n",
      "Epoch 239, Loss: 0.46862983752908405\n",
      "Epoch 240, Loss: 0.4685963696810109\n",
      "Epoch 241, Loss: 0.46867106090894\n",
      "Epoch 242, Loss: 0.46866360503981047\n",
      "Epoch 243, Loss: 0.468513635806399\n",
      "Epoch 244, Loss: 0.46835730315983837\n",
      "Epoch 245, Loss: 0.468344910742699\n",
      "Epoch 246, Loss: 0.46842993299443175\n",
      "Epoch 247, Loss: 0.4683803372309361\n",
      "Epoch 248, Loss: 0.46823408027364943\n",
      "Epoch 249, Loss: 0.46809559282800506\n",
      "Epoch 250, Loss: 0.4681194355765096\n",
      "Epoch 251, Loss: 0.4682059468617626\n",
      "Epoch 252, Loss: 0.46815125054377077\n",
      "Epoch 253, Loss: 0.46800465135626707\n",
      "Epoch 254, Loss: 0.4678516840148294\n",
      "Epoch 255, Loss: 0.46781997608436393\n",
      "Epoch 256, Loss: 0.4678506648079015\n",
      "Epoch 257, Loss: 0.4677995105079106\n",
      "Epoch 258, Loss: 0.4676942061377158\n",
      "Epoch 259, Loss: 0.46760336187164103\n",
      "Epoch 260, Loss: 0.46754277859764004\n",
      "Epoch 261, Loss: 0.4675358146830681\n",
      "Epoch 262, Loss: 0.4675934670336611\n",
      "Epoch 263, Loss: 0.4675883044263754\n",
      "Epoch 264, Loss: 0.46749525618568777\n",
      "Epoch 265, Loss: 0.46735425416294335\n",
      "Epoch 266, Loss: 0.4672686694514329\n",
      "Epoch 267, Loss: 0.46723722493080344\n",
      "Epoch 268, Loss: 0.4672375438513713\n",
      "Epoch 269, Loss: 0.46722038561562945\n",
      "Epoch 270, Loss: 0.46721867932539873\n",
      "Epoch 271, Loss: 0.46717319721795353\n",
      "Epoch 272, Loss: 0.4671238189214812\n",
      "Epoch 273, Loss: 0.46705709570299314\n",
      "Epoch 274, Loss: 0.4669974785875546\n",
      "Epoch 275, Loss: 0.4669422896788895\n",
      "Epoch 276, Loss: 0.4669034987743203\n",
      "Epoch 277, Loss: 0.46684804340658115\n",
      "Epoch 278, Loss: 0.4668104417178138\n",
      "Epoch 279, Loss: 0.46677111311176783\n",
      "Epoch 280, Loss: 0.46673031256560493\n",
      "Epoch 281, Loss: 0.46669448942736436\n",
      "Epoch 282, Loss: 0.46666225583115023\n",
      "Epoch 283, Loss: 0.4666367100427352\n",
      "Epoch 284, Loss: 0.4666213020629852\n",
      "Epoch 285, Loss: 0.46662540558501797\n",
      "Epoch 286, Loss: 0.4666499301628323\n",
      "Epoch 287, Loss: 0.466693164365506\n",
      "Epoch 288, Loss: 0.46673154174214454\n",
      "Epoch 289, Loss: 0.4666228668343984\n",
      "Epoch 290, Loss: 0.46649923372154634\n",
      "Epoch 291, Loss: 0.46640938673130766\n",
      "Epoch 292, Loss: 0.4663389345853611\n",
      "Epoch 293, Loss: 0.4663027458423396\n",
      "Epoch 294, Loss: 0.46628067971608783\n",
      "Epoch 295, Loss: 0.46627999262949893\n",
      "Epoch 296, Loss: 0.46630041304996855\n",
      "Epoch 297, Loss: 0.46631715776658106\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2183568215983475\n",
      "Valence RMSE: 0.20605056554521547\n",
      "Arousal RMSE: 0.2300055814878456\n",
      "Test R^2 score: tensor([0.4855, 0.4762], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.3650, 0.3536], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.48082073959017396\n",
      "Num of epochs: 298\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Epoch 121, Loss: 0.49295912913079615\n",
      "Epoch 122, Loss: 0.4924773619139838\n",
      "Epoch 123, Loss: 0.49197607191949805\n",
      "Epoch 124, Loss: 0.4914699197831528\n",
      "Epoch 125, Loss: 0.49091483381312667\n",
      "Epoch 126, Loss: 0.49035147677946317\n",
      "Epoch 127, Loss: 0.4898087222825037\n",
      "Epoch 128, Loss: 0.48924014731550836\n",
      "Epoch 129, Loss: 0.48873550647212044\n",
      "Epoch 130, Loss: 0.48832307254971563\n",
      "Epoch 131, Loss: 0.48791418392821295\n",
      "Epoch 132, Loss: 0.4875902245243796\n",
      "Epoch 133, Loss: 0.48728851108037585\n",
      "Epoch 134, Loss: 0.48697062262181345\n",
      "Epoch 135, Loss: 0.4866339553030548\n",
      "Epoch 136, Loss: 0.48633446746734893\n",
      "Epoch 137, Loss: 0.4860310087358514\n",
      "Epoch 138, Loss: 0.4856786412545615\n",
      "Epoch 139, Loss: 0.48531372107004\n",
      "Epoch 140, Loss: 0.48495712985488587\n",
      "Epoch 141, Loss: 0.4846131600652309\n",
      "Epoch 142, Loss: 0.48425966858652364\n",
      "Epoch 143, Loss: 0.4838914149236435\n",
      "Epoch 144, Loss: 0.48352437546282767\n",
      "Epoch 145, Loss: 0.4832299911937133\n",
      "Epoch 146, Loss: 0.4832586529465723\n",
      "Epoch 147, Loss: 0.48356744143113517\n",
      "Epoch 148, Loss: 0.4828108019011922\n",
      "Epoch 149, Loss: 0.4818912939738211\n",
      "Epoch 150, Loss: 0.48239416745575797\n",
      "Epoch 151, Loss: 0.4814952491721961\n",
      "Epoch 152, Loss: 0.4811822818820502\n",
      "Epoch 153, Loss: 0.4812753156716337\n",
      "Epoch 154, Loss: 0.4804387742282827\n",
      "Epoch 155, Loss: 0.48058189052417905\n",
      "Epoch 156, Loss: 0.4801288115497346\n",
      "Epoch 157, Loss: 0.4797465937246525\n",
      "Epoch 158, Loss: 0.4797622479529733\n",
      "Epoch 159, Loss: 0.4791664679844762\n",
      "Epoch 160, Loss: 0.47904359923301915\n",
      "Epoch 161, Loss: 0.4788483533353555\n",
      "Epoch 162, Loss: 0.4783709093142749\n",
      "Epoch 163, Loss: 0.4783027642591491\n",
      "Epoch 164, Loss: 0.4780003946284736\n",
      "Epoch 165, Loss: 0.47766356610156796\n",
      "Epoch 166, Loss: 0.47756940749162413\n",
      "Epoch 167, Loss: 0.4772720526560437\n",
      "Epoch 168, Loss: 0.477001191512605\n",
      "Epoch 169, Loss: 0.47690651155802954\n",
      "Epoch 170, Loss: 0.47663605231839845\n",
      "Epoch 171, Loss: 0.4763715079887003\n",
      "Epoch 172, Loss: 0.4762479807342815\n",
      "Epoch 173, Loss: 0.47603155482909787\n",
      "Epoch 174, Loss: 0.47580085926956445\n",
      "Epoch 175, Loss: 0.4756396378860013\n",
      "Epoch 176, Loss: 0.4754627232666358\n",
      "Epoch 177, Loss: 0.4752513954316692\n",
      "Epoch 178, Loss: 0.47507140350730925\n",
      "Epoch 179, Loss: 0.47492471273973497\n",
      "Epoch 180, Loss: 0.47475493910732763\n",
      "Epoch 181, Loss: 0.47455157018218247\n",
      "Epoch 182, Loss: 0.4743853224635176\n",
      "Epoch 183, Loss: 0.47424550491599726\n",
      "Epoch 184, Loss: 0.47408413178207975\n",
      "Epoch 185, Loss: 0.47390385374022453\n",
      "Epoch 186, Loss: 0.4737383066259127\n",
      "Epoch 187, Loss: 0.47358797785338436\n",
      "Epoch 188, Loss: 0.4734560449826861\n",
      "Epoch 189, Loss: 0.47335738206045025\n",
      "Epoch 190, Loss: 0.4733162834925985\n",
      "Epoch 191, Loss: 0.4732413020737725\n",
      "Epoch 192, Loss: 0.47313239020014447\n",
      "Epoch 193, Loss: 0.4729457158004841\n",
      "Epoch 194, Loss: 0.47274703738102797\n",
      "Epoch 195, Loss: 0.47253750657270627\n",
      "Epoch 196, Loss: 0.4723544299968521\n",
      "Epoch 197, Loss: 0.47221102907746554\n",
      "Epoch 198, Loss: 0.47210844467503205\n",
      "Epoch 199, Loss: 0.47205337980155637\n",
      "Epoch 200, Loss: 0.47204973383515075\n",
      "Epoch 201, Loss: 0.4720427890593303\n",
      "Epoch 202, Loss: 0.4719159657967507\n",
      "Epoch 203, Loss: 0.471735017109987\n",
      "Epoch 204, Loss: 0.4714985533244086\n",
      "Epoch 205, Loss: 0.4713143302002759\n",
      "Epoch 206, Loss: 0.47118574544243713\n",
      "Epoch 207, Loss: 0.47112002449718005\n",
      "Epoch 208, Loss: 0.4710910670518786\n",
      "Epoch 209, Loss: 0.47102495326923743\n",
      "Epoch 210, Loss: 0.4709626586360175\n",
      "Epoch 211, Loss: 0.47085840979895394\n",
      "Epoch 212, Loss: 0.47078064719388485\n",
      "Epoch 213, Loss: 0.47066614789276784\n",
      "Epoch 214, Loss: 0.47055654499036753\n",
      "Epoch 215, Loss: 0.47041652390832933\n",
      "Epoch 216, Loss: 0.4702446473560784\n",
      "Epoch 217, Loss: 0.4701045016774931\n",
      "Epoch 218, Loss: 0.47002059077245106\n",
      "Epoch 219, Loss: 0.4699643617672827\n",
      "Epoch 220, Loss: 0.4698888296176042\n",
      "Epoch 221, Loss: 0.4698141575467262\n",
      "Epoch 222, Loss: 0.4697265783615268\n",
      "Epoch 223, Loss: 0.46966066909468496\n",
      "Epoch 224, Loss: 0.4696348739248034\n",
      "Epoch 225, Loss: 0.4697176006517021\n",
      "Epoch 226, Loss: 0.4699424675720642\n",
      "Epoch 227, Loss: 0.46999269112017344\n",
      "Epoch 228, Loss: 0.4698759384724347\n",
      "Epoch 229, Loss: 0.46938041574678274\n",
      "Epoch 230, Loss: 0.4691415581864629\n",
      "Epoch 231, Loss: 0.46924839544758656\n",
      "Epoch 232, Loss: 0.46933531775018905\n",
      "Epoch 233, Loss: 0.4692253881106229\n",
      "Epoch 234, Loss: 0.46894320638835874\n",
      "Epoch 235, Loss: 0.4688628060980674\n",
      "Epoch 236, Loss: 0.46897798398181567\n",
      "Epoch 237, Loss: 0.4689811295666005\n",
      "Epoch 238, Loss: 0.4688031484354944\n",
      "Epoch 239, Loss: 0.46862983752908405\n",
      "Epoch 240, Loss: 0.4685963696810109\n",
      "Epoch 241, Loss: 0.46867106090894\n",
      "Epoch 242, Loss: 0.46866360503981047\n",
      "Epoch 243, Loss: 0.468513635806399\n",
      "Epoch 244, Loss: 0.46835730315983837\n",
      "Epoch 245, Loss: 0.468344910742699\n",
      "Epoch 246, Loss: 0.46842993299443175\n",
      "Epoch 247, Loss: 0.4683803372309361\n",
      "Epoch 248, Loss: 0.46823408027364943\n",
      "Epoch 249, Loss: 0.46809559282800506\n",
      "Epoch 250, Loss: 0.4681194355765096\n",
      "Epoch 251, Loss: 0.4682059468617626\n",
      "Epoch 252, Loss: 0.46815125054377077\n",
      "Epoch 253, Loss: 0.46800465135626707\n",
      "Epoch 254, Loss: 0.4678516840148294\n",
      "Epoch 255, Loss: 0.46781997608436393\n",
      "Epoch 256, Loss: 0.4678506648079015\n",
      "Epoch 257, Loss: 0.4677995105079106\n",
      "Epoch 258, Loss: 0.4676942061377158\n",
      "Epoch 259, Loss: 0.46760336187164103\n",
      "Epoch 260, Loss: 0.46754277859764004\n",
      "Epoch 261, Loss: 0.4675358146830681\n",
      "Epoch 262, Loss: 0.4675934670336611\n",
      "Epoch 263, Loss: 0.4675883044263754\n",
      "Epoch 264, Loss: 0.46749525618568777\n",
      "Epoch 265, Loss: 0.46735425416294335\n",
      "Epoch 266, Loss: 0.4672686694514329\n",
      "Epoch 267, Loss: 0.46723722493080344\n",
      "Epoch 268, Loss: 0.4672375438513713\n",
      "Epoch 269, Loss: 0.46722038561562945\n",
      "Epoch 270, Loss: 0.46721867932539873\n",
      "Epoch 271, Loss: 0.46717319721795353\n",
      "Epoch 272, Loss: 0.4671238189214812\n",
      "Epoch 273, Loss: 0.46705709570299314\n",
      "Epoch 274, Loss: 0.4669974785875546\n",
      "Epoch 275, Loss: 0.4669422896788895\n",
      "Epoch 276, Loss: 0.4669034987743203\n",
      "Epoch 277, Loss: 0.46684804340658115\n",
      "Epoch 278, Loss: 0.4668104417178138\n",
      "Epoch 279, Loss: 0.46677111311176783\n",
      "Epoch 280, Loss: 0.46673031256560493\n",
      "Epoch 281, Loss: 0.46669448942736436\n",
      "Epoch 282, Loss: 0.46666225583115023\n",
      "Epoch 283, Loss: 0.4666367100427352\n",
      "Epoch 284, Loss: 0.4666213020629852\n",
      "Epoch 285, Loss: 0.46662540558501797\n",
      "Epoch 286, Loss: 0.4666499301628323\n",
      "Epoch 287, Loss: 0.466693164365506\n",
      "Epoch 288, Loss: 0.46673154174214454\n",
      "Epoch 289, Loss: 0.4666228668343984\n",
      "Epoch 290, Loss: 0.46649923372154634\n",
      "Epoch 291, Loss: 0.46640938673130766\n",
      "Epoch 292, Loss: 0.4663389345853611\n",
      "Epoch 293, Loss: 0.4663027458423396\n",
      "Epoch 294, Loss: 0.46628067971608783\n",
      "Epoch 295, Loss: 0.46627999262949893\n",
      "Epoch 296, Loss: 0.46630041304996855\n",
      "Epoch 297, Loss: 0.46631715776658106\n",
      "Epoch 298, Loss: 0.4663076031259929\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2170219585144529\n",
      "Valence RMSE: 0.20481310343588993\n",
      "Arousal RMSE: 0.22857964392276336\n",
      "Test R^2 score: tensor([0.4916, 0.4827], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.3726, 0.3616], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4871389825139597\n",
      "Num of epochs: 299\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Epoch 121, Loss: 0.49295912913079615\n",
      "Epoch 122, Loss: 0.4924773619139838\n",
      "Epoch 123, Loss: 0.49197607191949805\n",
      "Epoch 124, Loss: 0.4914699197831528\n",
      "Epoch 125, Loss: 0.49091483381312667\n",
      "Epoch 126, Loss: 0.49035147677946317\n",
      "Epoch 127, Loss: 0.4898087222825037\n",
      "Epoch 128, Loss: 0.48924014731550836\n",
      "Epoch 129, Loss: 0.48873550647212044\n",
      "Epoch 130, Loss: 0.48832307254971563\n",
      "Epoch 131, Loss: 0.48791418392821295\n",
      "Epoch 132, Loss: 0.4875902245243796\n",
      "Epoch 133, Loss: 0.48728851108037585\n",
      "Epoch 134, Loss: 0.48697062262181345\n",
      "Epoch 135, Loss: 0.4866339553030548\n",
      "Epoch 136, Loss: 0.48633446746734893\n",
      "Epoch 137, Loss: 0.4860310087358514\n",
      "Epoch 138, Loss: 0.4856786412545615\n",
      "Epoch 139, Loss: 0.48531372107004\n",
      "Epoch 140, Loss: 0.48495712985488587\n",
      "Epoch 141, Loss: 0.4846131600652309\n",
      "Epoch 142, Loss: 0.48425966858652364\n",
      "Epoch 143, Loss: 0.4838914149236435\n",
      "Epoch 144, Loss: 0.48352437546282767\n",
      "Epoch 145, Loss: 0.4832299911937133\n",
      "Epoch 146, Loss: 0.4832586529465723\n",
      "Epoch 147, Loss: 0.48356744143113517\n",
      "Epoch 148, Loss: 0.4828108019011922\n",
      "Epoch 149, Loss: 0.4818912939738211\n",
      "Epoch 150, Loss: 0.48239416745575797\n",
      "Epoch 151, Loss: 0.4814952491721961\n",
      "Epoch 152, Loss: 0.4811822818820502\n",
      "Epoch 153, Loss: 0.4812753156716337\n",
      "Epoch 154, Loss: 0.4804387742282827\n",
      "Epoch 155, Loss: 0.48058189052417905\n",
      "Epoch 156, Loss: 0.4801288115497346\n",
      "Epoch 157, Loss: 0.4797465937246525\n",
      "Epoch 158, Loss: 0.4797622479529733\n",
      "Epoch 159, Loss: 0.4791664679844762\n",
      "Epoch 160, Loss: 0.47904359923301915\n",
      "Epoch 161, Loss: 0.4788483533353555\n",
      "Epoch 162, Loss: 0.4783709093142749\n",
      "Epoch 163, Loss: 0.4783027642591491\n",
      "Epoch 164, Loss: 0.4780003946284736\n",
      "Epoch 165, Loss: 0.47766356610156796\n",
      "Epoch 166, Loss: 0.47756940749162413\n",
      "Epoch 167, Loss: 0.4772720526560437\n",
      "Epoch 168, Loss: 0.477001191512605\n",
      "Epoch 169, Loss: 0.47690651155802954\n",
      "Epoch 170, Loss: 0.47663605231839845\n",
      "Epoch 171, Loss: 0.4763715079887003\n",
      "Epoch 172, Loss: 0.4762479807342815\n",
      "Epoch 173, Loss: 0.47603155482909787\n",
      "Epoch 174, Loss: 0.47580085926956445\n",
      "Epoch 175, Loss: 0.4756396378860013\n",
      "Epoch 176, Loss: 0.4754627232666358\n",
      "Epoch 177, Loss: 0.4752513954316692\n",
      "Epoch 178, Loss: 0.47507140350730925\n",
      "Epoch 179, Loss: 0.47492471273973497\n",
      "Epoch 180, Loss: 0.47475493910732763\n",
      "Epoch 181, Loss: 0.47455157018218247\n",
      "Epoch 182, Loss: 0.4743853224635176\n",
      "Epoch 183, Loss: 0.47424550491599726\n",
      "Epoch 184, Loss: 0.47408413178207975\n",
      "Epoch 185, Loss: 0.47390385374022453\n",
      "Epoch 186, Loss: 0.4737383066259127\n",
      "Epoch 187, Loss: 0.47358797785338436\n",
      "Epoch 188, Loss: 0.4734560449826861\n",
      "Epoch 189, Loss: 0.47335738206045025\n",
      "Epoch 190, Loss: 0.4733162834925985\n",
      "Epoch 191, Loss: 0.4732413020737725\n",
      "Epoch 192, Loss: 0.47313239020014447\n",
      "Epoch 193, Loss: 0.4729457158004841\n",
      "Epoch 194, Loss: 0.47274703738102797\n",
      "Epoch 195, Loss: 0.47253750657270627\n",
      "Epoch 196, Loss: 0.4723544299968521\n",
      "Epoch 197, Loss: 0.47221102907746554\n",
      "Epoch 198, Loss: 0.47210844467503205\n",
      "Epoch 199, Loss: 0.47205337980155637\n",
      "Epoch 200, Loss: 0.47204973383515075\n",
      "Epoch 201, Loss: 0.4720427890593303\n",
      "Epoch 202, Loss: 0.4719159657967507\n",
      "Epoch 203, Loss: 0.471735017109987\n",
      "Epoch 204, Loss: 0.4714985533244086\n",
      "Epoch 205, Loss: 0.4713143302002759\n",
      "Epoch 206, Loss: 0.47118574544243713\n",
      "Epoch 207, Loss: 0.47112002449718005\n",
      "Epoch 208, Loss: 0.4710910670518786\n",
      "Epoch 209, Loss: 0.47102495326923743\n",
      "Epoch 210, Loss: 0.4709626586360175\n",
      "Epoch 211, Loss: 0.47085840979895394\n",
      "Epoch 212, Loss: 0.47078064719388485\n",
      "Epoch 213, Loss: 0.47066614789276784\n",
      "Epoch 214, Loss: 0.47055654499036753\n",
      "Epoch 215, Loss: 0.47041652390832933\n",
      "Epoch 216, Loss: 0.4702446473560784\n",
      "Epoch 217, Loss: 0.4701045016774931\n",
      "Epoch 218, Loss: 0.47002059077245106\n",
      "Epoch 219, Loss: 0.4699643617672827\n",
      "Epoch 220, Loss: 0.4698888296176042\n",
      "Epoch 221, Loss: 0.4698141575467262\n",
      "Epoch 222, Loss: 0.4697265783615268\n",
      "Epoch 223, Loss: 0.46966066909468496\n",
      "Epoch 224, Loss: 0.4696348739248034\n",
      "Epoch 225, Loss: 0.4697176006517021\n",
      "Epoch 226, Loss: 0.4699424675720642\n",
      "Epoch 227, Loss: 0.46999269112017344\n",
      "Epoch 228, Loss: 0.4698759384724347\n",
      "Epoch 229, Loss: 0.46938041574678274\n",
      "Epoch 230, Loss: 0.4691415581864629\n",
      "Epoch 231, Loss: 0.46924839544758656\n",
      "Epoch 232, Loss: 0.46933531775018905\n",
      "Epoch 233, Loss: 0.4692253881106229\n",
      "Epoch 234, Loss: 0.46894320638835874\n",
      "Epoch 235, Loss: 0.4688628060980674\n",
      "Epoch 236, Loss: 0.46897798398181567\n",
      "Epoch 237, Loss: 0.4689811295666005\n",
      "Epoch 238, Loss: 0.4688031484354944\n",
      "Epoch 239, Loss: 0.46862983752908405\n",
      "Epoch 240, Loss: 0.4685963696810109\n",
      "Epoch 241, Loss: 0.46867106090894\n",
      "Epoch 242, Loss: 0.46866360503981047\n",
      "Epoch 243, Loss: 0.468513635806399\n",
      "Epoch 244, Loss: 0.46835730315983837\n",
      "Epoch 245, Loss: 0.468344910742699\n",
      "Epoch 246, Loss: 0.46842993299443175\n",
      "Epoch 247, Loss: 0.4683803372309361\n",
      "Epoch 248, Loss: 0.46823408027364943\n",
      "Epoch 249, Loss: 0.46809559282800506\n",
      "Epoch 250, Loss: 0.4681194355765096\n",
      "Epoch 251, Loss: 0.4682059468617626\n",
      "Epoch 252, Loss: 0.46815125054377077\n",
      "Epoch 253, Loss: 0.46800465135626707\n",
      "Epoch 254, Loss: 0.4678516840148294\n",
      "Epoch 255, Loss: 0.46781997608436393\n",
      "Epoch 256, Loss: 0.4678506648079015\n",
      "Epoch 257, Loss: 0.4677995105079106\n",
      "Epoch 258, Loss: 0.4676942061377158\n",
      "Epoch 259, Loss: 0.46760336187164103\n",
      "Epoch 260, Loss: 0.46754277859764004\n",
      "Epoch 261, Loss: 0.4675358146830681\n",
      "Epoch 262, Loss: 0.4675934670336611\n",
      "Epoch 263, Loss: 0.4675883044263754\n",
      "Epoch 264, Loss: 0.46749525618568777\n",
      "Epoch 265, Loss: 0.46735425416294335\n",
      "Epoch 266, Loss: 0.4672686694514329\n",
      "Epoch 267, Loss: 0.46723722493080344\n",
      "Epoch 268, Loss: 0.4672375438513713\n",
      "Epoch 269, Loss: 0.46722038561562945\n",
      "Epoch 270, Loss: 0.46721867932539873\n",
      "Epoch 271, Loss: 0.46717319721795353\n",
      "Epoch 272, Loss: 0.4671238189214812\n",
      "Epoch 273, Loss: 0.46705709570299314\n",
      "Epoch 274, Loss: 0.4669974785875546\n",
      "Epoch 275, Loss: 0.4669422896788895\n",
      "Epoch 276, Loss: 0.4669034987743203\n",
      "Epoch 277, Loss: 0.46684804340658115\n",
      "Epoch 278, Loss: 0.4668104417178138\n",
      "Epoch 279, Loss: 0.46677111311176783\n",
      "Epoch 280, Loss: 0.46673031256560493\n",
      "Epoch 281, Loss: 0.46669448942736436\n",
      "Epoch 282, Loss: 0.46666225583115023\n",
      "Epoch 283, Loss: 0.4666367100427352\n",
      "Epoch 284, Loss: 0.4666213020629852\n",
      "Epoch 285, Loss: 0.46662540558501797\n",
      "Epoch 286, Loss: 0.4666499301628323\n",
      "Epoch 287, Loss: 0.466693164365506\n",
      "Epoch 288, Loss: 0.46673154174214454\n",
      "Epoch 289, Loss: 0.4666228668343984\n",
      "Epoch 290, Loss: 0.46649923372154634\n",
      "Epoch 291, Loss: 0.46640938673130766\n",
      "Epoch 292, Loss: 0.4663389345853611\n",
      "Epoch 293, Loss: 0.4663027458423396\n",
      "Epoch 294, Loss: 0.46628067971608783\n",
      "Epoch 295, Loss: 0.46627999262949893\n",
      "Epoch 296, Loss: 0.46630041304996855\n",
      "Epoch 297, Loss: 0.46631715776658106\n",
      "Epoch 298, Loss: 0.4663076031259929\n",
      "Epoch 299, Loss: 0.4662523804979687\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.21810515202795894\n",
      "Valence RMSE: 0.2058643657999131\n",
      "Arousal RMSE: 0.2296945310103785\n",
      "Test R^2 score: tensor([0.4864, 0.4776], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.3662, 0.3553], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.48199341404604407\n",
      "Num of epochs: 300\n",
      "Epoch 1, Loss: 0.5635314068474343\n",
      "Epoch 2, Loss: 0.5622419189103066\n",
      "Epoch 3, Loss: 0.5610729234697547\n",
      "Epoch 4, Loss: 0.5600306296912679\n",
      "Epoch 5, Loss: 0.5591160926271372\n",
      "Epoch 6, Loss: 0.5583312745080063\n",
      "Epoch 7, Loss: 0.5576761351524713\n",
      "Epoch 8, Loss: 0.5571489656466321\n",
      "Epoch 9, Loss: 0.5567461683366715\n",
      "Epoch 10, Loss: 0.5564591764555323\n",
      "Epoch 11, Loss: 0.5562776687255635\n",
      "Epoch 12, Loss: 0.5561884867484396\n",
      "Epoch 13, Loss: 0.5561754658943873\n",
      "Epoch 14, Loss: 0.5562198587888023\n",
      "Epoch 15, Loss: 0.556301294597715\n",
      "Epoch 16, Loss: 0.556399644589122\n",
      "Epoch 17, Loss: 0.5564974684422056\n",
      "Epoch 18, Loss: 0.556581085768642\n",
      "Epoch 19, Loss: 0.556640491150957\n",
      "Epoch 20, Loss: 0.5566708472898202\n",
      "Epoch 21, Loss: 0.5566718109496585\n",
      "Epoch 22, Loss: 0.5566464607869802\n",
      "Epoch 23, Loss: 0.5565999869520355\n",
      "Epoch 24, Loss: 0.5565389172175111\n",
      "Epoch 25, Loss: 0.5564700216562759\n",
      "Epoch 26, Loss: 0.5563996178077218\n",
      "Epoch 27, Loss: 0.5563330888305745\n",
      "Epoch 28, Loss: 0.556274534605916\n",
      "Epoch 29, Loss: 0.556226690211144\n",
      "Epoch 30, Loss: 0.5561908711928052\n",
      "Epoch 31, Loss: 0.556167133459517\n",
      "Epoch 32, Loss: 0.5561544604173257\n",
      "Epoch 33, Loss: 0.5561509772895993\n",
      "Epoch 34, Loss: 0.5561538977597088\n",
      "Epoch 35, Loss: 0.5561600333763147\n",
      "Epoch 36, Loss: 0.5561695179954298\n",
      "Epoch 37, Loss: 0.5561783594438537\n",
      "Epoch 38, Loss: 0.5561847091246419\n",
      "Epoch 39, Loss: 0.5561872811266109\n",
      "Epoch 40, Loss: 0.5561854860848238\n",
      "Epoch 41, Loss: 0.5561793775410118\n",
      "Epoch 42, Loss: 0.5561702145994972\n",
      "Epoch 43, Loss: 0.5561563895248338\n",
      "Epoch 44, Loss: 0.556139777546453\n",
      "Epoch 45, Loss: 0.5561212358490961\n",
      "Epoch 46, Loss: 0.5561013001553491\n",
      "Epoch 47, Loss: 0.5560808278120776\n",
      "Epoch 48, Loss: 0.5560601939286132\n",
      "Epoch 49, Loss: 0.5560399076630016\n",
      "Epoch 50, Loss: 0.5560200762520293\n",
      "Epoch 51, Loss: 0.5560000297285662\n",
      "Epoch 52, Loss: 0.5559790712266599\n",
      "Epoch 53, Loss: 0.5559565305749463\n",
      "Epoch 54, Loss: 0.5559317910907483\n",
      "Epoch 55, Loss: 0.5559042091544464\n",
      "Epoch 56, Loss: 0.5558734358552578\n",
      "Epoch 57, Loss: 0.5558388272613985\n",
      "Epoch 58, Loss: 0.5557991761927283\n",
      "Epoch 59, Loss: 0.5557541598199932\n",
      "Epoch 60, Loss: 0.5557033478001566\n",
      "Epoch 61, Loss: 0.555645531745945\n",
      "Epoch 62, Loss: 0.5555804949037033\n",
      "Epoch 63, Loss: 0.5555075373030949\n",
      "Epoch 64, Loss: 0.5554248583228432\n",
      "Epoch 65, Loss: 0.5553323999552002\n",
      "Epoch 66, Loss: 0.5552285202059055\n",
      "Epoch 67, Loss: 0.5551124073585372\n",
      "Epoch 68, Loss: 0.5549855841663196\n",
      "Epoch 69, Loss: 0.5548432897008836\n",
      "Epoch 70, Loss: 0.5546820465679463\n",
      "Epoch 71, Loss: 0.5545011664103975\n",
      "Epoch 72, Loss: 0.5543092862030777\n",
      "Epoch 73, Loss: 0.5540950189848831\n",
      "Epoch 74, Loss: 0.5538426802599625\n",
      "Epoch 75, Loss: 0.5535552598200458\n",
      "Epoch 76, Loss: 0.5532257268539288\n",
      "Epoch 77, Loss: 0.5528448419075513\n",
      "Epoch 78, Loss: 0.5524105026289456\n",
      "Epoch 79, Loss: 0.5519178580364809\n",
      "Epoch 80, Loss: 0.5513923987888926\n",
      "Epoch 81, Loss: 0.5508045976167801\n",
      "Epoch 82, Loss: 0.5501423098946724\n",
      "Epoch 83, Loss: 0.5493983771305396\n",
      "Epoch 84, Loss: 0.5485618189572455\n",
      "Epoch 85, Loss: 0.5476116396247932\n",
      "Epoch 86, Loss: 0.5465305606066203\n",
      "Epoch 87, Loss: 0.5453207818744112\n",
      "Epoch 88, Loss: 0.5440664379102254\n",
      "Epoch 89, Loss: 0.5426986351781876\n",
      "Epoch 90, Loss: 0.5412137877087413\n",
      "Epoch 91, Loss: 0.5396359475699604\n",
      "Epoch 92, Loss: 0.537823704897069\n",
      "Epoch 93, Loss: 0.5358039667359309\n",
      "Epoch 94, Loss: 0.5336399454922224\n",
      "Epoch 95, Loss: 0.531426596741333\n",
      "Epoch 96, Loss: 0.5290638033946026\n",
      "Epoch 97, Loss: 0.5265011024336761\n",
      "Epoch 98, Loss: 0.5237075621407302\n",
      "Epoch 99, Loss: 0.5206573157109828\n",
      "Epoch 100, Loss: 0.5174917760831325\n",
      "Epoch 101, Loss: 0.5141860940880854\n",
      "Epoch 102, Loss: 0.5107167750030202\n",
      "Epoch 103, Loss: 0.5075355361373228\n",
      "Epoch 104, Loss: 0.5047156760083354\n",
      "Epoch 105, Loss: 0.5023874563422681\n",
      "Epoch 106, Loss: 0.500285841775467\n",
      "Epoch 107, Loss: 0.4986438091065128\n",
      "Epoch 108, Loss: 0.49771073909905683\n",
      "Epoch 109, Loss: 0.4968712788568465\n",
      "Epoch 110, Loss: 0.496503600581925\n",
      "Epoch 111, Loss: 0.49642335650320907\n",
      "Epoch 112, Loss: 0.4961499760521734\n",
      "Epoch 113, Loss: 0.4959126840402773\n",
      "Epoch 114, Loss: 0.4958854598472803\n",
      "Epoch 115, Loss: 0.49547592499541393\n",
      "Epoch 116, Loss: 0.49500396758233045\n",
      "Epoch 117, Loss: 0.4946452191779925\n",
      "Epoch 118, Loss: 0.49417409670402695\n",
      "Epoch 119, Loss: 0.49382416011291036\n",
      "Epoch 120, Loss: 0.4933596081706725\n",
      "Epoch 121, Loss: 0.49295912913079615\n",
      "Epoch 122, Loss: 0.4924773619139838\n",
      "Epoch 123, Loss: 0.49197607191949805\n",
      "Epoch 124, Loss: 0.4914699197831528\n",
      "Epoch 125, Loss: 0.49091483381312667\n",
      "Epoch 126, Loss: 0.49035147677946317\n",
      "Epoch 127, Loss: 0.4898087222825037\n",
      "Epoch 128, Loss: 0.48924014731550836\n",
      "Epoch 129, Loss: 0.48873550647212044\n",
      "Epoch 130, Loss: 0.48832307254971563\n",
      "Epoch 131, Loss: 0.48791418392821295\n",
      "Epoch 132, Loss: 0.4875902245243796\n",
      "Epoch 133, Loss: 0.48728851108037585\n",
      "Epoch 134, Loss: 0.48697062262181345\n",
      "Epoch 135, Loss: 0.4866339553030548\n",
      "Epoch 136, Loss: 0.48633446746734893\n",
      "Epoch 137, Loss: 0.4860310087358514\n",
      "Epoch 138, Loss: 0.4856786412545615\n",
      "Epoch 139, Loss: 0.48531372107004\n",
      "Epoch 140, Loss: 0.48495712985488587\n",
      "Epoch 141, Loss: 0.4846131600652309\n",
      "Epoch 142, Loss: 0.48425966858652364\n",
      "Epoch 143, Loss: 0.4838914149236435\n",
      "Epoch 144, Loss: 0.48352437546282767\n",
      "Epoch 145, Loss: 0.4832299911937133\n",
      "Epoch 146, Loss: 0.4832586529465723\n",
      "Epoch 147, Loss: 0.48356744143113517\n",
      "Epoch 148, Loss: 0.4828108019011922\n",
      "Epoch 149, Loss: 0.4818912939738211\n",
      "Epoch 150, Loss: 0.48239416745575797\n",
      "Epoch 151, Loss: 0.4814952491721961\n",
      "Epoch 152, Loss: 0.4811822818820502\n",
      "Epoch 153, Loss: 0.4812753156716337\n",
      "Epoch 154, Loss: 0.4804387742282827\n",
      "Epoch 155, Loss: 0.48058189052417905\n",
      "Epoch 156, Loss: 0.4801288115497346\n",
      "Epoch 157, Loss: 0.4797465937246525\n",
      "Epoch 158, Loss: 0.4797622479529733\n",
      "Epoch 159, Loss: 0.4791664679844762\n",
      "Epoch 160, Loss: 0.47904359923301915\n",
      "Epoch 161, Loss: 0.4788483533353555\n",
      "Epoch 162, Loss: 0.4783709093142749\n",
      "Epoch 163, Loss: 0.4783027642591491\n",
      "Epoch 164, Loss: 0.4780003946284736\n",
      "Epoch 165, Loss: 0.47766356610156796\n",
      "Epoch 166, Loss: 0.47756940749162413\n",
      "Epoch 167, Loss: 0.4772720526560437\n",
      "Epoch 168, Loss: 0.477001191512605\n",
      "Epoch 169, Loss: 0.47690651155802954\n",
      "Epoch 170, Loss: 0.47663605231839845\n",
      "Epoch 171, Loss: 0.4763715079887003\n",
      "Epoch 172, Loss: 0.4762479807342815\n",
      "Epoch 173, Loss: 0.47603155482909787\n",
      "Epoch 174, Loss: 0.47580085926956445\n",
      "Epoch 175, Loss: 0.4756396378860013\n",
      "Epoch 176, Loss: 0.4754627232666358\n",
      "Epoch 177, Loss: 0.4752513954316692\n",
      "Epoch 178, Loss: 0.47507140350730925\n",
      "Epoch 179, Loss: 0.47492471273973497\n",
      "Epoch 180, Loss: 0.47475493910732763\n",
      "Epoch 181, Loss: 0.47455157018218247\n",
      "Epoch 182, Loss: 0.4743853224635176\n",
      "Epoch 183, Loss: 0.47424550491599726\n",
      "Epoch 184, Loss: 0.47408413178207975\n",
      "Epoch 185, Loss: 0.47390385374022453\n",
      "Epoch 186, Loss: 0.4737383066259127\n",
      "Epoch 187, Loss: 0.47358797785338436\n",
      "Epoch 188, Loss: 0.4734560449826861\n",
      "Epoch 189, Loss: 0.47335738206045025\n",
      "Epoch 190, Loss: 0.4733162834925985\n",
      "Epoch 191, Loss: 0.4732413020737725\n",
      "Epoch 192, Loss: 0.47313239020014447\n",
      "Epoch 193, Loss: 0.4729457158004841\n",
      "Epoch 194, Loss: 0.47274703738102797\n",
      "Epoch 195, Loss: 0.47253750657270627\n",
      "Epoch 196, Loss: 0.4723544299968521\n",
      "Epoch 197, Loss: 0.47221102907746554\n",
      "Epoch 198, Loss: 0.47210844467503205\n",
      "Epoch 199, Loss: 0.47205337980155637\n",
      "Epoch 200, Loss: 0.47204973383515075\n",
      "Epoch 201, Loss: 0.4720427890593303\n",
      "Epoch 202, Loss: 0.4719159657967507\n",
      "Epoch 203, Loss: 0.471735017109987\n",
      "Epoch 204, Loss: 0.4714985533244086\n",
      "Epoch 205, Loss: 0.4713143302002759\n",
      "Epoch 206, Loss: 0.47118574544243713\n",
      "Epoch 207, Loss: 0.47112002449718005\n",
      "Epoch 208, Loss: 0.4710910670518786\n",
      "Epoch 209, Loss: 0.47102495326923743\n",
      "Epoch 210, Loss: 0.4709626586360175\n",
      "Epoch 211, Loss: 0.47085840979895394\n",
      "Epoch 212, Loss: 0.47078064719388485\n",
      "Epoch 213, Loss: 0.47066614789276784\n",
      "Epoch 214, Loss: 0.47055654499036753\n",
      "Epoch 215, Loss: 0.47041652390832933\n",
      "Epoch 216, Loss: 0.4702446473560784\n",
      "Epoch 217, Loss: 0.4701045016774931\n",
      "Epoch 218, Loss: 0.47002059077245106\n",
      "Epoch 219, Loss: 0.4699643617672827\n",
      "Epoch 220, Loss: 0.4698888296176042\n",
      "Epoch 221, Loss: 0.4698141575467262\n",
      "Epoch 222, Loss: 0.4697265783615268\n",
      "Epoch 223, Loss: 0.46966066909468496\n",
      "Epoch 224, Loss: 0.4696348739248034\n",
      "Epoch 225, Loss: 0.4697176006517021\n",
      "Epoch 226, Loss: 0.4699424675720642\n",
      "Epoch 227, Loss: 0.46999269112017344\n",
      "Epoch 228, Loss: 0.4698759384724347\n",
      "Epoch 229, Loss: 0.46938041574678274\n",
      "Epoch 230, Loss: 0.4691415581864629\n",
      "Epoch 231, Loss: 0.46924839544758656\n",
      "Epoch 232, Loss: 0.46933531775018905\n",
      "Epoch 233, Loss: 0.4692253881106229\n",
      "Epoch 234, Loss: 0.46894320638835874\n",
      "Epoch 235, Loss: 0.4688628060980674\n",
      "Epoch 236, Loss: 0.46897798398181567\n",
      "Epoch 237, Loss: 0.4689811295666005\n",
      "Epoch 238, Loss: 0.4688031484354944\n",
      "Epoch 239, Loss: 0.46862983752908405\n",
      "Epoch 240, Loss: 0.4685963696810109\n",
      "Epoch 241, Loss: 0.46867106090894\n",
      "Epoch 242, Loss: 0.46866360503981047\n",
      "Epoch 243, Loss: 0.468513635806399\n",
      "Epoch 244, Loss: 0.46835730315983837\n",
      "Epoch 245, Loss: 0.468344910742699\n",
      "Epoch 246, Loss: 0.46842993299443175\n",
      "Epoch 247, Loss: 0.4683803372309361\n",
      "Epoch 248, Loss: 0.46823408027364943\n",
      "Epoch 249, Loss: 0.46809559282800506\n",
      "Epoch 250, Loss: 0.4681194355765096\n",
      "Epoch 251, Loss: 0.4682059468617626\n",
      "Epoch 252, Loss: 0.46815125054377077\n",
      "Epoch 253, Loss: 0.46800465135626707\n",
      "Epoch 254, Loss: 0.4678516840148294\n",
      "Epoch 255, Loss: 0.46781997608436393\n",
      "Epoch 256, Loss: 0.4678506648079015\n",
      "Epoch 257, Loss: 0.4677995105079106\n",
      "Epoch 258, Loss: 0.4676942061377158\n",
      "Epoch 259, Loss: 0.46760336187164103\n",
      "Epoch 260, Loss: 0.46754277859764004\n",
      "Epoch 261, Loss: 0.4675358146830681\n",
      "Epoch 262, Loss: 0.4675934670336611\n",
      "Epoch 263, Loss: 0.4675883044263754\n",
      "Epoch 264, Loss: 0.46749525618568777\n",
      "Epoch 265, Loss: 0.46735425416294335\n",
      "Epoch 266, Loss: 0.4672686694514329\n",
      "Epoch 267, Loss: 0.46723722493080344\n",
      "Epoch 268, Loss: 0.4672375438513713\n",
      "Epoch 269, Loss: 0.46722038561562945\n",
      "Epoch 270, Loss: 0.46721867932539873\n",
      "Epoch 271, Loss: 0.46717319721795353\n",
      "Epoch 272, Loss: 0.4671238189214812\n",
      "Epoch 273, Loss: 0.46705709570299314\n",
      "Epoch 274, Loss: 0.4669974785875546\n",
      "Epoch 275, Loss: 0.4669422896788895\n",
      "Epoch 276, Loss: 0.4669034987743203\n",
      "Epoch 277, Loss: 0.46684804340658115\n",
      "Epoch 278, Loss: 0.4668104417178138\n",
      "Epoch 279, Loss: 0.46677111311176783\n",
      "Epoch 280, Loss: 0.46673031256560493\n",
      "Epoch 281, Loss: 0.46669448942736436\n",
      "Epoch 282, Loss: 0.46666225583115023\n",
      "Epoch 283, Loss: 0.4666367100427352\n",
      "Epoch 284, Loss: 0.4666213020629852\n",
      "Epoch 285, Loss: 0.46662540558501797\n",
      "Epoch 286, Loss: 0.4666499301628323\n",
      "Epoch 287, Loss: 0.466693164365506\n",
      "Epoch 288, Loss: 0.46673154174214454\n",
      "Epoch 289, Loss: 0.4666228668343984\n",
      "Epoch 290, Loss: 0.46649923372154634\n",
      "Epoch 291, Loss: 0.46640938673130766\n",
      "Epoch 292, Loss: 0.4663389345853611\n",
      "Epoch 293, Loss: 0.4663027458423396\n",
      "Epoch 294, Loss: 0.46628067971608783\n",
      "Epoch 295, Loss: 0.46627999262949893\n",
      "Epoch 296, Loss: 0.46630041304996855\n",
      "Epoch 297, Loss: 0.46631715776658106\n",
      "Epoch 298, Loss: 0.4663076031259929\n",
      "Epoch 299, Loss: 0.4662523804979687\n",
      "Epoch 300, Loss: 0.466189735803077\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.21718510114104672\n",
      "Valence RMSE: 0.2050082642460063\n",
      "Arousal RMSE: 0.2287145555187362\n",
      "Test R^2 score: tensor([0.4907, 0.4820], dtype=torch.float64)\n",
      "Test Adjusted R^2 score: tensor([0.3715, 0.3608], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.48634889881576765\n",
      "Completed.\n"
     ]
    }
   ],
   "source": [
    "for num_epochs in num_epochs_list:\n",
    "  # Set the seed\n",
    "  torch.manual_seed(seed)\n",
    "\n",
    "  print(f'Num of epochs: {num_epochs}')\n",
    "  \n",
    "  model = train_model(num_epochs)\n",
    "\n",
    "  print(\"Training completed.\")\n",
    "  print(\"Testing model...\")\n",
    "\n",
    "  test_pred, rmse, adjusted_r2_score, r2_score = test_model(model)\n",
    "  adjusted_r2_scores_valence_list.append(adjusted_r2_score[0])\n",
    "  adjusted_r2_scores_arousal_list.append(adjusted_r2_score[1])\n",
    "  r2_scores_list.append(r2_score)\n",
    "  rmse_list.append(rmse)\n",
    "\n",
    "print(\"Completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the graph to visualise the relationship the evaluation metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import relevant libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RMSE vs. num_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(num_epochs_list, rmse_list, color='b', linestyle='-')\n",
    "plt.title('num_epochs vs. Test RMSE')\n",
    "plt.xlabel('num_epochs')\n",
    "plt.ylabel('Tets RMSE') \n",
    "plt.grid(True)\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min RMSE score: 0.2170219585144529\n",
      "Corresponding R^2 SCore: 0.4871389825139597\n",
      "Corresponding num_epochs: 298\n"
     ]
    }
   ],
   "source": [
    "min_rmse = min(rmse_list)\n",
    "corresponding_r2_score = r2_scores_list[rmse_list.index(min_rmse)]\n",
    "corresponding_num_epochs = num_epochs_list[rmse_list.index(min_rmse)]\n",
    "\n",
    "print(f'Min RMSE score: {min_rmse}')\n",
    "print(f'Corresponding R^2 SCore: {corresponding_r2_score}')\n",
    "print(f'Corresponding num_epochs: {corresponding_num_epochs}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test R^2 Score vs. num_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(num_epochs_list, r2_scores_list, color='b', linestyle='-')\n",
    "plt.title('num_epochs vs. Test R^2 Score')\n",
    "plt.xlabel('num_epochs')\n",
    "plt.ylabel('Test R^2 SCore') \n",
    "plt.grid(True)\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max R^2 score: 0.4871389825139597\n",
      "Corresponding RMSE: 0.2170219585144529\n",
      "Corresponding num_epochs: 298\n"
     ]
    }
   ],
   "source": [
    "max_r2_score = max(r2_scores_list)\n",
    "corresponding_rmse = rmse_list[r2_scores_list.index(max_r2_score)]\n",
    "corresponding_num_epochs = num_epochs_list[r2_scores_list.index(max_r2_score)]\n",
    "\n",
    "print(f'Max R^2 score: {max_r2_score}')\n",
    "print(f'Corresponding RMSE: {corresponding_rmse}')\n",
    "print(f'Corresponding num_epochs: {corresponding_num_epochs}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Adjusted R^2 Score (Valence) vs. num_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(num_epochs_list, adjusted_r2_scores_valence_list, color='b', linestyle='-')\n",
    "plt.title('num_epochs vs. Test Adjusted R^2 Score (Valence)')\n",
    "plt.xlabel('num_epochs')\n",
    "plt.ylabel('Test Adjusted R^2 Score (Valence)') \n",
    "plt.grid(True)\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max R^2 score: 0.37302216253674314\n",
      "Corresponding RMSE: 0.21711187948645422\n",
      "Corresponding num_epochs: 287\n"
     ]
    }
   ],
   "source": [
    "max_r2_score_valence = max(adjusted_r2_scores_valence_list)\n",
    "corresponding_rmse = rmse_list[adjusted_r2_scores_valence_list.index(max_r2_score_valence)]\n",
    "corresponding_num_epochs = num_epochs_list[adjusted_r2_scores_valence_list.index(max_r2_score_valence)]\n",
    "\n",
    "print(f'Max R^2 score: {max_r2_score_valence}')\n",
    "print(f'Corresponding RMSE: {corresponding_rmse}')\n",
    "print(f'Corresponding num_epochs: {corresponding_num_epochs}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Adjusted R^2 Score (Arousal) vs. num_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(num_epochs_list, adjusted_r2_scores_arousal_list, color='b', linestyle='-')\n",
    "plt.title('num_epochs vs. Test Adjusted R^2 Score (Arousal)')\n",
    "plt.xlabel('num_epochs')\n",
    "plt.ylabel('Test Adjusted R^2 SCore (Arousal)') \n",
    "plt.grid(True)\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max R^2 score: 0.36156875975913716\n",
      "Corresponding RMSE: 0.2170219585144529\n",
      "Corresponding num_epochs: 298\n"
     ]
    }
   ],
   "source": [
    "max_r2_score_arousal = max(adjusted_r2_scores_arousal_list)\n",
    "corresponding_rmse = rmse_list[adjusted_r2_scores_arousal_list.index(max_r2_score_arousal)]\n",
    "corresponding_num_epochs = num_epochs_list[adjusted_r2_scores_arousal_list.index(max_r2_score_arousal)]\n",
    "\n",
    "print(f'Max R^2 score: {max_r2_score_arousal}')\n",
    "print(f'Corresponding RMSE: {corresponding_rmse}')\n",
    "print(f'Corresponding num_epochs: {corresponding_num_epochs}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
