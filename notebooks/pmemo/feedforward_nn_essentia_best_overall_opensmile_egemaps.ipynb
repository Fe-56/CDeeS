{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PMEmo Dataset - Feed Forward Neural Network\n",
    "## Essentia Best Overall openSMILE eGeMAPS Featureset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import relevant libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torcheval.metrics import R2Score\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import math\n",
    "\n",
    "import sys\n",
    "sys.path.insert(1, '../../utils')\n",
    "from paths import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import annotations dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>song_id</th>\n",
       "      <th>valence_mean_mapped</th>\n",
       "      <th>arousal_mean_mapped</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.150</td>\n",
       "      <td>-0.200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>-0.425</td>\n",
       "      <td>-0.475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>-0.600</td>\n",
       "      <td>-0.700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>-0.300</td>\n",
       "      <td>0.025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>0.450</td>\n",
       "      <td>0.400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>762</th>\n",
       "      <td>993</td>\n",
       "      <td>0.525</td>\n",
       "      <td>0.725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>763</th>\n",
       "      <td>996</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>764</th>\n",
       "      <td>997</td>\n",
       "      <td>0.325</td>\n",
       "      <td>0.425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>765</th>\n",
       "      <td>999</td>\n",
       "      <td>0.550</td>\n",
       "      <td>0.750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>766</th>\n",
       "      <td>1000</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.325</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>767 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     song_id  valence_mean_mapped  arousal_mean_mapped\n",
       "0          1                0.150               -0.200\n",
       "1          4               -0.425               -0.475\n",
       "2          5               -0.600               -0.700\n",
       "3          6               -0.300                0.025\n",
       "4          7                0.450                0.400\n",
       "..       ...                  ...                  ...\n",
       "762      993                0.525                0.725\n",
       "763      996                0.125                0.750\n",
       "764      997                0.325                0.425\n",
       "765      999                0.550                0.750\n",
       "766     1000                0.150                0.325\n",
       "\n",
       "[767 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_annotations = pd.read_csv(get_pmemo_path('processed/annotations/pmemo_static_annotations.csv'))\n",
    "df_annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the featureset\n",
    "\n",
    "This is where you should change between normalised and standardised, and untouched featuresets!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>song_id</th>\n",
       "      <th>lowlevel.melbands_kurtosis.dmean</th>\n",
       "      <th>lowlevel.melbands_kurtosis.dmean2</th>\n",
       "      <th>lowlevel.melbands_kurtosis.dvar</th>\n",
       "      <th>lowlevel.melbands_kurtosis.dvar2</th>\n",
       "      <th>lowlevel.melbands_kurtosis.max</th>\n",
       "      <th>lowlevel.melbands_kurtosis.mean</th>\n",
       "      <th>lowlevel.melbands_kurtosis.median</th>\n",
       "      <th>lowlevel.melbands_kurtosis.min</th>\n",
       "      <th>lowlevel.melbands_kurtosis.stdev</th>\n",
       "      <th>...</th>\n",
       "      <th>slopeUV0-500_sma3nz_amean</th>\n",
       "      <th>slopeUV500-1500_sma3nz_amean</th>\n",
       "      <th>spectralFluxUV_sma3nz_amean</th>\n",
       "      <th>loudnessPeaksPerSec</th>\n",
       "      <th>VoicedSegmentsPerSec</th>\n",
       "      <th>MeanVoicedSegmentLengthSec</th>\n",
       "      <th>StddevVoicedSegmentLengthSec</th>\n",
       "      <th>MeanUnvoicedSegmentLength</th>\n",
       "      <th>StddevUnvoicedSegmentLength</th>\n",
       "      <th>equivalentSoundLevel_dBp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.091959</td>\n",
       "      <td>0.086915</td>\n",
       "      <td>0.011661</td>\n",
       "      <td>0.009166</td>\n",
       "      <td>0.084971</td>\n",
       "      <td>0.064362</td>\n",
       "      <td>0.039249</td>\n",
       "      <td>0.200273</td>\n",
       "      <td>0.112646</td>\n",
       "      <td>...</td>\n",
       "      <td>0.360946</td>\n",
       "      <td>0.477083</td>\n",
       "      <td>0.354845</td>\n",
       "      <td>0.618233</td>\n",
       "      <td>0.568024</td>\n",
       "      <td>0.012333</td>\n",
       "      <td>0.010856</td>\n",
       "      <td>0.092155</td>\n",
       "      <td>0.066829</td>\n",
       "      <td>0.644904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>0.367830</td>\n",
       "      <td>0.350220</td>\n",
       "      <td>0.067970</td>\n",
       "      <td>0.045142</td>\n",
       "      <td>0.166129</td>\n",
       "      <td>0.345452</td>\n",
       "      <td>0.276910</td>\n",
       "      <td>0.408428</td>\n",
       "      <td>0.378375</td>\n",
       "      <td>...</td>\n",
       "      <td>0.194518</td>\n",
       "      <td>0.197515</td>\n",
       "      <td>0.997077</td>\n",
       "      <td>0.719396</td>\n",
       "      <td>0.075804</td>\n",
       "      <td>0.038816</td>\n",
       "      <td>0.059588</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.893336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>0.793184</td>\n",
       "      <td>0.761592</td>\n",
       "      <td>0.292918</td>\n",
       "      <td>0.223284</td>\n",
       "      <td>0.284663</td>\n",
       "      <td>0.566046</td>\n",
       "      <td>0.407319</td>\n",
       "      <td>0.019133</td>\n",
       "      <td>0.656534</td>\n",
       "      <td>...</td>\n",
       "      <td>0.227733</td>\n",
       "      <td>0.398985</td>\n",
       "      <td>0.482067</td>\n",
       "      <td>0.719487</td>\n",
       "      <td>0.395234</td>\n",
       "      <td>0.023807</td>\n",
       "      <td>0.022551</td>\n",
       "      <td>0.108542</td>\n",
       "      <td>0.109709</td>\n",
       "      <td>0.827230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>0.174975</td>\n",
       "      <td>0.166081</td>\n",
       "      <td>0.013457</td>\n",
       "      <td>0.010707</td>\n",
       "      <td>0.045539</td>\n",
       "      <td>0.168166</td>\n",
       "      <td>0.194476</td>\n",
       "      <td>0.250194</td>\n",
       "      <td>0.142053</td>\n",
       "      <td>...</td>\n",
       "      <td>0.155392</td>\n",
       "      <td>0.542070</td>\n",
       "      <td>0.367544</td>\n",
       "      <td>0.607535</td>\n",
       "      <td>0.304105</td>\n",
       "      <td>0.005747</td>\n",
       "      <td>0.007682</td>\n",
       "      <td>0.347203</td>\n",
       "      <td>0.335536</td>\n",
       "      <td>0.827628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>0.080461</td>\n",
       "      <td>0.077460</td>\n",
       "      <td>0.006162</td>\n",
       "      <td>0.004753</td>\n",
       "      <td>0.063129</td>\n",
       "      <td>0.050890</td>\n",
       "      <td>0.049963</td>\n",
       "      <td>0.095697</td>\n",
       "      <td>0.066929</td>\n",
       "      <td>...</td>\n",
       "      <td>0.612921</td>\n",
       "      <td>0.377957</td>\n",
       "      <td>0.459692</td>\n",
       "      <td>0.572874</td>\n",
       "      <td>0.442982</td>\n",
       "      <td>0.027635</td>\n",
       "      <td>0.023468</td>\n",
       "      <td>0.038398</td>\n",
       "      <td>0.029670</td>\n",
       "      <td>0.822717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>762</th>\n",
       "      <td>993</td>\n",
       "      <td>0.066183</td>\n",
       "      <td>0.066978</td>\n",
       "      <td>0.008151</td>\n",
       "      <td>0.007277</td>\n",
       "      <td>0.126425</td>\n",
       "      <td>0.058467</td>\n",
       "      <td>0.047070</td>\n",
       "      <td>0.022686</td>\n",
       "      <td>0.083308</td>\n",
       "      <td>...</td>\n",
       "      <td>0.487869</td>\n",
       "      <td>0.487722</td>\n",
       "      <td>0.581421</td>\n",
       "      <td>0.471706</td>\n",
       "      <td>0.561126</td>\n",
       "      <td>0.014875</td>\n",
       "      <td>0.047375</td>\n",
       "      <td>0.075363</td>\n",
       "      <td>0.064431</td>\n",
       "      <td>0.822125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>763</th>\n",
       "      <td>996</td>\n",
       "      <td>0.143177</td>\n",
       "      <td>0.121697</td>\n",
       "      <td>0.034041</td>\n",
       "      <td>0.022859</td>\n",
       "      <td>0.178212</td>\n",
       "      <td>0.121857</td>\n",
       "      <td>0.032984</td>\n",
       "      <td>0.306449</td>\n",
       "      <td>0.274032</td>\n",
       "      <td>...</td>\n",
       "      <td>0.504947</td>\n",
       "      <td>0.673577</td>\n",
       "      <td>0.573298</td>\n",
       "      <td>0.598725</td>\n",
       "      <td>0.638633</td>\n",
       "      <td>0.009723</td>\n",
       "      <td>0.009633</td>\n",
       "      <td>0.088765</td>\n",
       "      <td>0.082178</td>\n",
       "      <td>0.852123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>764</th>\n",
       "      <td>997</td>\n",
       "      <td>0.057246</td>\n",
       "      <td>0.055671</td>\n",
       "      <td>0.003049</td>\n",
       "      <td>0.002654</td>\n",
       "      <td>0.047044</td>\n",
       "      <td>0.046206</td>\n",
       "      <td>0.045261</td>\n",
       "      <td>0.055447</td>\n",
       "      <td>0.051686</td>\n",
       "      <td>...</td>\n",
       "      <td>0.446266</td>\n",
       "      <td>0.590639</td>\n",
       "      <td>0.423028</td>\n",
       "      <td>0.510169</td>\n",
       "      <td>0.532011</td>\n",
       "      <td>0.016340</td>\n",
       "      <td>0.019502</td>\n",
       "      <td>0.080065</td>\n",
       "      <td>0.163099</td>\n",
       "      <td>0.822882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>765</th>\n",
       "      <td>999</td>\n",
       "      <td>0.064311</td>\n",
       "      <td>0.060727</td>\n",
       "      <td>0.002448</td>\n",
       "      <td>0.001986</td>\n",
       "      <td>0.036530</td>\n",
       "      <td>0.063658</td>\n",
       "      <td>0.057241</td>\n",
       "      <td>0.558007</td>\n",
       "      <td>0.069792</td>\n",
       "      <td>...</td>\n",
       "      <td>0.384011</td>\n",
       "      <td>0.610316</td>\n",
       "      <td>0.595691</td>\n",
       "      <td>0.377443</td>\n",
       "      <td>0.643245</td>\n",
       "      <td>0.008016</td>\n",
       "      <td>0.018335</td>\n",
       "      <td>0.099848</td>\n",
       "      <td>0.240641</td>\n",
       "      <td>0.889240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>766</th>\n",
       "      <td>1000</td>\n",
       "      <td>0.027510</td>\n",
       "      <td>0.027913</td>\n",
       "      <td>0.000619</td>\n",
       "      <td>0.000547</td>\n",
       "      <td>0.022764</td>\n",
       "      <td>0.028755</td>\n",
       "      <td>0.036564</td>\n",
       "      <td>0.272612</td>\n",
       "      <td>0.031357</td>\n",
       "      <td>...</td>\n",
       "      <td>0.432009</td>\n",
       "      <td>0.621859</td>\n",
       "      <td>0.597951</td>\n",
       "      <td>0.344933</td>\n",
       "      <td>0.656199</td>\n",
       "      <td>0.008606</td>\n",
       "      <td>0.012944</td>\n",
       "      <td>0.100924</td>\n",
       "      <td>0.120431</td>\n",
       "      <td>0.834901</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>767 rows × 225 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     song_id  lowlevel.melbands_kurtosis.dmean  \\\n",
       "0          1                          0.091959   \n",
       "1          4                          0.367830   \n",
       "2          5                          0.793184   \n",
       "3          6                          0.174975   \n",
       "4          7                          0.080461   \n",
       "..       ...                               ...   \n",
       "762      993                          0.066183   \n",
       "763      996                          0.143177   \n",
       "764      997                          0.057246   \n",
       "765      999                          0.064311   \n",
       "766     1000                          0.027510   \n",
       "\n",
       "     lowlevel.melbands_kurtosis.dmean2  lowlevel.melbands_kurtosis.dvar  \\\n",
       "0                             0.086915                         0.011661   \n",
       "1                             0.350220                         0.067970   \n",
       "2                             0.761592                         0.292918   \n",
       "3                             0.166081                         0.013457   \n",
       "4                             0.077460                         0.006162   \n",
       "..                                 ...                              ...   \n",
       "762                           0.066978                         0.008151   \n",
       "763                           0.121697                         0.034041   \n",
       "764                           0.055671                         0.003049   \n",
       "765                           0.060727                         0.002448   \n",
       "766                           0.027913                         0.000619   \n",
       "\n",
       "     lowlevel.melbands_kurtosis.dvar2  lowlevel.melbands_kurtosis.max  \\\n",
       "0                            0.009166                        0.084971   \n",
       "1                            0.045142                        0.166129   \n",
       "2                            0.223284                        0.284663   \n",
       "3                            0.010707                        0.045539   \n",
       "4                            0.004753                        0.063129   \n",
       "..                                ...                             ...   \n",
       "762                          0.007277                        0.126425   \n",
       "763                          0.022859                        0.178212   \n",
       "764                          0.002654                        0.047044   \n",
       "765                          0.001986                        0.036530   \n",
       "766                          0.000547                        0.022764   \n",
       "\n",
       "     lowlevel.melbands_kurtosis.mean  lowlevel.melbands_kurtosis.median  \\\n",
       "0                           0.064362                           0.039249   \n",
       "1                           0.345452                           0.276910   \n",
       "2                           0.566046                           0.407319   \n",
       "3                           0.168166                           0.194476   \n",
       "4                           0.050890                           0.049963   \n",
       "..                               ...                                ...   \n",
       "762                         0.058467                           0.047070   \n",
       "763                         0.121857                           0.032984   \n",
       "764                         0.046206                           0.045261   \n",
       "765                         0.063658                           0.057241   \n",
       "766                         0.028755                           0.036564   \n",
       "\n",
       "     lowlevel.melbands_kurtosis.min  lowlevel.melbands_kurtosis.stdev  ...  \\\n",
       "0                          0.200273                          0.112646  ...   \n",
       "1                          0.408428                          0.378375  ...   \n",
       "2                          0.019133                          0.656534  ...   \n",
       "3                          0.250194                          0.142053  ...   \n",
       "4                          0.095697                          0.066929  ...   \n",
       "..                              ...                               ...  ...   \n",
       "762                        0.022686                          0.083308  ...   \n",
       "763                        0.306449                          0.274032  ...   \n",
       "764                        0.055447                          0.051686  ...   \n",
       "765                        0.558007                          0.069792  ...   \n",
       "766                        0.272612                          0.031357  ...   \n",
       "\n",
       "     slopeUV0-500_sma3nz_amean  slopeUV500-1500_sma3nz_amean  \\\n",
       "0                     0.360946                      0.477083   \n",
       "1                     0.194518                      0.197515   \n",
       "2                     0.227733                      0.398985   \n",
       "3                     0.155392                      0.542070   \n",
       "4                     0.612921                      0.377957   \n",
       "..                         ...                           ...   \n",
       "762                   0.487869                      0.487722   \n",
       "763                   0.504947                      0.673577   \n",
       "764                   0.446266                      0.590639   \n",
       "765                   0.384011                      0.610316   \n",
       "766                   0.432009                      0.621859   \n",
       "\n",
       "     spectralFluxUV_sma3nz_amean  loudnessPeaksPerSec  VoicedSegmentsPerSec  \\\n",
       "0                       0.354845             0.618233              0.568024   \n",
       "1                       0.997077             0.719396              0.075804   \n",
       "2                       0.482067             0.719487              0.395234   \n",
       "3                       0.367544             0.607535              0.304105   \n",
       "4                       0.459692             0.572874              0.442982   \n",
       "..                           ...                  ...                   ...   \n",
       "762                     0.581421             0.471706              0.561126   \n",
       "763                     0.573298             0.598725              0.638633   \n",
       "764                     0.423028             0.510169              0.532011   \n",
       "765                     0.595691             0.377443              0.643245   \n",
       "766                     0.597951             0.344933              0.656199   \n",
       "\n",
       "     MeanVoicedSegmentLengthSec  StddevVoicedSegmentLengthSec  \\\n",
       "0                      0.012333                      0.010856   \n",
       "1                      0.038816                      0.059588   \n",
       "2                      0.023807                      0.022551   \n",
       "3                      0.005747                      0.007682   \n",
       "4                      0.027635                      0.023468   \n",
       "..                          ...                           ...   \n",
       "762                    0.014875                      0.047375   \n",
       "763                    0.009723                      0.009633   \n",
       "764                    0.016340                      0.019502   \n",
       "765                    0.008016                      0.018335   \n",
       "766                    0.008606                      0.012944   \n",
       "\n",
       "     MeanUnvoicedSegmentLength  StddevUnvoicedSegmentLength  \\\n",
       "0                     0.092155                     0.066829   \n",
       "1                     1.000000                     1.000000   \n",
       "2                     0.108542                     0.109709   \n",
       "3                     0.347203                     0.335536   \n",
       "4                     0.038398                     0.029670   \n",
       "..                         ...                          ...   \n",
       "762                   0.075363                     0.064431   \n",
       "763                   0.088765                     0.082178   \n",
       "764                   0.080065                     0.163099   \n",
       "765                   0.099848                     0.240641   \n",
       "766                   0.100924                     0.120431   \n",
       "\n",
       "     equivalentSoundLevel_dBp  \n",
       "0                    0.644904  \n",
       "1                    0.893336  \n",
       "2                    0.827230  \n",
       "3                    0.827628  \n",
       "4                    0.822717  \n",
       "..                        ...  \n",
       "762                  0.822125  \n",
       "763                  0.852123  \n",
       "764                  0.822882  \n",
       "765                  0.889240  \n",
       "766                  0.834901  \n",
       "\n",
       "[767 rows x 225 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_essentia_best_overall_opensmile_egemaps = pd.read_csv(get_pmemo_path('processed/features/integrated/normalised_essentia_best_overall_opensmile_egemaps_features.csv'))\n",
    "\n",
    "# drop Unnamed:0 column\n",
    "df_essentia_best_overall_opensmile_egemaps = df_essentia_best_overall_opensmile_egemaps[df_essentia_best_overall_opensmile_egemaps.columns[1:]]\n",
    "\n",
    "df_essentia_best_overall_opensmile_egemaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 767 entries, 0 to 766\n",
      "Data columns (total 225 columns):\n",
      " #    Column                                          Dtype  \n",
      "---   ------                                          -----  \n",
      " 0    song_id                                         int64  \n",
      " 1    lowlevel.melbands_kurtosis.dmean                float64\n",
      " 2    lowlevel.melbands_kurtosis.dmean2               float64\n",
      " 3    lowlevel.melbands_kurtosis.dvar                 float64\n",
      " 4    lowlevel.melbands_kurtosis.dvar2                float64\n",
      " 5    lowlevel.melbands_kurtosis.max                  float64\n",
      " 6    lowlevel.melbands_kurtosis.mean                 float64\n",
      " 7    lowlevel.melbands_kurtosis.median               float64\n",
      " 8    lowlevel.melbands_kurtosis.min                  float64\n",
      " 9    lowlevel.melbands_kurtosis.stdev                float64\n",
      " 10   lowlevel.melbands_kurtosis.var                  float64\n",
      " 11   lowlevel.melbands_skewness.dmean                float64\n",
      " 12   lowlevel.melbands_skewness.dmean2               float64\n",
      " 13   lowlevel.melbands_skewness.dvar                 float64\n",
      " 14   lowlevel.melbands_skewness.dvar2                float64\n",
      " 15   lowlevel.melbands_skewness.max                  float64\n",
      " 16   lowlevel.melbands_skewness.mean                 float64\n",
      " 17   lowlevel.melbands_skewness.median               float64\n",
      " 18   lowlevel.melbands_skewness.min                  float64\n",
      " 19   lowlevel.melbands_skewness.stdev                float64\n",
      " 20   lowlevel.melbands_skewness.var                  float64\n",
      " 21   lowlevel.spectral_energy.dmean                  float64\n",
      " 22   lowlevel.spectral_energy.dmean2                 float64\n",
      " 23   lowlevel.spectral_energy.dvar                   float64\n",
      " 24   lowlevel.spectral_energy.dvar2                  float64\n",
      " 25   lowlevel.spectral_energy.max                    float64\n",
      " 26   lowlevel.spectral_energy.mean                   float64\n",
      " 27   lowlevel.spectral_energy.median                 float64\n",
      " 28   lowlevel.spectral_energy.min                    float64\n",
      " 29   lowlevel.spectral_energy.stdev                  float64\n",
      " 30   lowlevel.spectral_energy.var                    float64\n",
      " 31   tonal.chords_strength.dmean                     float64\n",
      " 32   tonal.chords_strength.dmean2                    float64\n",
      " 33   tonal.chords_strength.dvar                      float64\n",
      " 34   tonal.chords_strength.dvar2                     float64\n",
      " 35   tonal.chords_strength.max                       float64\n",
      " 36   tonal.chords_strength.mean                      float64\n",
      " 37   tonal.chords_strength.median                    float64\n",
      " 38   tonal.chords_strength.min                       float64\n",
      " 39   tonal.chords_strength.stdev                     float64\n",
      " 40   tonal.chords_strength.var                       float64\n",
      " 41   tonal.hpcp_entropy.dmean                        float64\n",
      " 42   tonal.hpcp_entropy.dmean2                       float64\n",
      " 43   tonal.hpcp_entropy.dvar                         float64\n",
      " 44   tonal.hpcp_entropy.dvar2                        float64\n",
      " 45   tonal.hpcp_entropy.max                          float64\n",
      " 46   tonal.hpcp_entropy.mean                         float64\n",
      " 47   tonal.hpcp_entropy.median                       float64\n",
      " 48   tonal.hpcp_entropy.min                          float64\n",
      " 49   tonal.hpcp_entropy.stdev                        float64\n",
      " 50   tonal.hpcp_entropy.var                          float64\n",
      " 51   tonal.key_edma.strength                         float64\n",
      " 52   tonal.key_temperley.strength                    float64\n",
      " 53   rhythm.beats_loudness_band_ratio.dmean_0        float64\n",
      " 54   rhythm.beats_loudness_band_ratio.dmean_1        float64\n",
      " 55   rhythm.beats_loudness_band_ratio.dmean_2        float64\n",
      " 56   rhythm.beats_loudness_band_ratio.dmean_3        float64\n",
      " 57   rhythm.beats_loudness_band_ratio.dmean_4        float64\n",
      " 58   rhythm.beats_loudness_band_ratio.dmean_5        float64\n",
      " 59   rhythm.beats_loudness_band_ratio.dmean2_0       float64\n",
      " 60   rhythm.beats_loudness_band_ratio.dmean2_1       float64\n",
      " 61   rhythm.beats_loudness_band_ratio.dmean2_2       float64\n",
      " 62   rhythm.beats_loudness_band_ratio.dmean2_3       float64\n",
      " 63   rhythm.beats_loudness_band_ratio.dmean2_4       float64\n",
      " 64   rhythm.beats_loudness_band_ratio.dmean2_5       float64\n",
      " 65   rhythm.beats_loudness_band_ratio.dvar_0         float64\n",
      " 66   rhythm.beats_loudness_band_ratio.dvar_1         float64\n",
      " 67   rhythm.beats_loudness_band_ratio.dvar_2         float64\n",
      " 68   rhythm.beats_loudness_band_ratio.dvar_3         float64\n",
      " 69   rhythm.beats_loudness_band_ratio.dvar_4         float64\n",
      " 70   rhythm.beats_loudness_band_ratio.dvar_5         float64\n",
      " 71   rhythm.beats_loudness_band_ratio.dvar2_0        float64\n",
      " 72   rhythm.beats_loudness_band_ratio.dvar2_1        float64\n",
      " 73   rhythm.beats_loudness_band_ratio.dvar2_2        float64\n",
      " 74   rhythm.beats_loudness_band_ratio.dvar2_3        float64\n",
      " 75   rhythm.beats_loudness_band_ratio.dvar2_4        float64\n",
      " 76   rhythm.beats_loudness_band_ratio.dvar2_5        float64\n",
      " 77   rhythm.beats_loudness_band_ratio.max_0          float64\n",
      " 78   rhythm.beats_loudness_band_ratio.max_1          float64\n",
      " 79   rhythm.beats_loudness_band_ratio.max_2          float64\n",
      " 80   rhythm.beats_loudness_band_ratio.max_3          float64\n",
      " 81   rhythm.beats_loudness_band_ratio.max_4          float64\n",
      " 82   rhythm.beats_loudness_band_ratio.max_5          float64\n",
      " 83   rhythm.beats_loudness_band_ratio.mean_0         float64\n",
      " 84   rhythm.beats_loudness_band_ratio.mean_1         float64\n",
      " 85   rhythm.beats_loudness_band_ratio.mean_2         float64\n",
      " 86   rhythm.beats_loudness_band_ratio.mean_3         float64\n",
      " 87   rhythm.beats_loudness_band_ratio.mean_4         float64\n",
      " 88   rhythm.beats_loudness_band_ratio.mean_5         float64\n",
      " 89   rhythm.beats_loudness_band_ratio.median_0       float64\n",
      " 90   rhythm.beats_loudness_band_ratio.median_1       float64\n",
      " 91   rhythm.beats_loudness_band_ratio.median_2       float64\n",
      " 92   rhythm.beats_loudness_band_ratio.median_3       float64\n",
      " 93   rhythm.beats_loudness_band_ratio.median_4       float64\n",
      " 94   rhythm.beats_loudness_band_ratio.median_5       float64\n",
      " 95   rhythm.beats_loudness_band_ratio.min_0          float64\n",
      " 96   rhythm.beats_loudness_band_ratio.min_1          float64\n",
      " 97   rhythm.beats_loudness_band_ratio.min_2          float64\n",
      " 98   rhythm.beats_loudness_band_ratio.min_3          float64\n",
      " 99   rhythm.beats_loudness_band_ratio.min_4          float64\n",
      " 100  rhythm.beats_loudness_band_ratio.min_5          float64\n",
      " 101  rhythm.beats_loudness_band_ratio.stdev_0        float64\n",
      " 102  rhythm.beats_loudness_band_ratio.stdev_1        float64\n",
      " 103  rhythm.beats_loudness_band_ratio.stdev_2        float64\n",
      " 104  rhythm.beats_loudness_band_ratio.stdev_3        float64\n",
      " 105  rhythm.beats_loudness_band_ratio.stdev_4        float64\n",
      " 106  rhythm.beats_loudness_band_ratio.stdev_5        float64\n",
      " 107  rhythm.beats_loudness_band_ratio.var_0          float64\n",
      " 108  rhythm.beats_loudness_band_ratio.var_1          float64\n",
      " 109  rhythm.beats_loudness_band_ratio.var_2          float64\n",
      " 110  rhythm.beats_loudness_band_ratio.var_3          float64\n",
      " 111  rhythm.beats_loudness_band_ratio.var_4          float64\n",
      " 112  rhythm.beats_loudness_band_ratio.var_5          float64\n",
      " 113  tonal.chords_histogram_0                        float64\n",
      " 114  tonal.chords_histogram_1                        float64\n",
      " 115  tonal.chords_histogram_2                        float64\n",
      " 116  tonal.chords_histogram_3                        float64\n",
      " 117  tonal.chords_histogram_4                        float64\n",
      " 118  tonal.chords_histogram_5                        float64\n",
      " 119  tonal.chords_histogram_6                        float64\n",
      " 120  tonal.chords_histogram_7                        float64\n",
      " 121  tonal.chords_histogram_8                        float64\n",
      " 122  tonal.chords_histogram_9                        float64\n",
      " 123  tonal.chords_histogram_10                       float64\n",
      " 124  tonal.chords_histogram_11                       float64\n",
      " 125  tonal.chords_histogram_12                       float64\n",
      " 126  tonal.chords_histogram_13                       float64\n",
      " 127  tonal.chords_histogram_14                       float64\n",
      " 128  tonal.chords_histogram_15                       float64\n",
      " 129  tonal.chords_histogram_16                       float64\n",
      " 130  tonal.chords_histogram_17                       float64\n",
      " 131  tonal.chords_histogram_18                       float64\n",
      " 132  tonal.chords_histogram_19                       float64\n",
      " 133  tonal.chords_histogram_20                       float64\n",
      " 134  tonal.chords_histogram_21                       float64\n",
      " 135  tonal.chords_histogram_22                       float64\n",
      " 136  tonal.chords_histogram_23                       float64\n",
      " 137  F0semitoneFrom27.5Hz_sma3nz_amean               float64\n",
      " 138  F0semitoneFrom27.5Hz_sma3nz_stddevNorm          float64\n",
      " 139  F0semitoneFrom27.5Hz_sma3nz_percentile20.0      float64\n",
      " 140  F0semitoneFrom27.5Hz_sma3nz_percentile50.0      float64\n",
      " 141  F0semitoneFrom27.5Hz_sma3nz_percentile80.0      float64\n",
      " 142  F0semitoneFrom27.5Hz_sma3nz_pctlrange0-2        float64\n",
      " 143  F0semitoneFrom27.5Hz_sma3nz_meanRisingSlope     float64\n",
      " 144  F0semitoneFrom27.5Hz_sma3nz_stddevRisingSlope   float64\n",
      " 145  F0semitoneFrom27.5Hz_sma3nz_meanFallingSlope    float64\n",
      " 146  F0semitoneFrom27.5Hz_sma3nz_stddevFallingSlope  float64\n",
      " 147  loudness_sma3_amean                             float64\n",
      " 148  loudness_sma3_stddevNorm                        float64\n",
      " 149  loudness_sma3_percentile20.0                    float64\n",
      " 150  loudness_sma3_percentile50.0                    float64\n",
      " 151  loudness_sma3_percentile80.0                    float64\n",
      " 152  loudness_sma3_pctlrange0-2                      float64\n",
      " 153  loudness_sma3_meanRisingSlope                   float64\n",
      " 154  loudness_sma3_stddevRisingSlope                 float64\n",
      " 155  loudness_sma3_meanFallingSlope                  float64\n",
      " 156  loudness_sma3_stddevFallingSlope                float64\n",
      " 157  spectralFlux_sma3_amean                         float64\n",
      " 158  spectralFlux_sma3_stddevNorm                    float64\n",
      " 159  mfcc1_sma3_amean                                float64\n",
      " 160  mfcc1_sma3_stddevNorm                           float64\n",
      " 161  mfcc2_sma3_amean                                float64\n",
      " 162  mfcc2_sma3_stddevNorm                           float64\n",
      " 163  mfcc3_sma3_amean                                float64\n",
      " 164  mfcc3_sma3_stddevNorm                           float64\n",
      " 165  mfcc4_sma3_amean                                float64\n",
      " 166  mfcc4_sma3_stddevNorm                           float64\n",
      " 167  jitterLocal_sma3nz_amean                        float64\n",
      " 168  jitterLocal_sma3nz_stddevNorm                   float64\n",
      " 169  shimmerLocaldB_sma3nz_amean                     float64\n",
      " 170  shimmerLocaldB_sma3nz_stddevNorm                float64\n",
      " 171  HNRdBACF_sma3nz_amean                           float64\n",
      " 172  HNRdBACF_sma3nz_stddevNorm                      float64\n",
      " 173  logRelF0-H1-H2_sma3nz_amean                     float64\n",
      " 174  logRelF0-H1-H2_sma3nz_stddevNorm                float64\n",
      " 175  logRelF0-H1-A3_sma3nz_amean                     float64\n",
      " 176  logRelF0-H1-A3_sma3nz_stddevNorm                float64\n",
      " 177  F1frequency_sma3nz_amean                        float64\n",
      " 178  F1frequency_sma3nz_stddevNorm                   float64\n",
      " 179  F1bandwidth_sma3nz_amean                        float64\n",
      " 180  F1bandwidth_sma3nz_stddevNorm                   float64\n",
      " 181  F1amplitudeLogRelF0_sma3nz_amean                float64\n",
      " 182  F1amplitudeLogRelF0_sma3nz_stddevNorm           float64\n",
      " 183  F2frequency_sma3nz_amean                        float64\n",
      " 184  F2frequency_sma3nz_stddevNorm                   float64\n",
      " 185  F2bandwidth_sma3nz_amean                        float64\n",
      " 186  F2bandwidth_sma3nz_stddevNorm                   float64\n",
      " 187  F2amplitudeLogRelF0_sma3nz_amean                float64\n",
      " 188  F2amplitudeLogRelF0_sma3nz_stddevNorm           float64\n",
      " 189  F3frequency_sma3nz_amean                        float64\n",
      " 190  F3frequency_sma3nz_stddevNorm                   float64\n",
      " 191  F3bandwidth_sma3nz_amean                        float64\n",
      " 192  F3bandwidth_sma3nz_stddevNorm                   float64\n",
      " 193  F3amplitudeLogRelF0_sma3nz_amean                float64\n",
      " 194  F3amplitudeLogRelF0_sma3nz_stddevNorm           float64\n",
      " 195  alphaRatioV_sma3nz_amean                        float64\n",
      " 196  alphaRatioV_sma3nz_stddevNorm                   float64\n",
      " 197  hammarbergIndexV_sma3nz_amean                   float64\n",
      " 198  hammarbergIndexV_sma3nz_stddevNorm              float64\n",
      " 199  slopeV0-500_sma3nz_amean                        float64\n",
      " 200  slopeV0-500_sma3nz_stddevNorm                   float64\n",
      " 201  slopeV500-1500_sma3nz_amean                     float64\n",
      " 202  slopeV500-1500_sma3nz_stddevNorm                float64\n",
      " 203  spectralFluxV_sma3nz_amean                      float64\n",
      " 204  spectralFluxV_sma3nz_stddevNorm                 float64\n",
      " 205  mfcc1V_sma3nz_amean                             float64\n",
      " 206  mfcc1V_sma3nz_stddevNorm                        float64\n",
      " 207  mfcc2V_sma3nz_amean                             float64\n",
      " 208  mfcc2V_sma3nz_stddevNorm                        float64\n",
      " 209  mfcc3V_sma3nz_amean                             float64\n",
      " 210  mfcc3V_sma3nz_stddevNorm                        float64\n",
      " 211  mfcc4V_sma3nz_amean                             float64\n",
      " 212  mfcc4V_sma3nz_stddevNorm                        float64\n",
      " 213  alphaRatioUV_sma3nz_amean                       float64\n",
      " 214  hammarbergIndexUV_sma3nz_amean                  float64\n",
      " 215  slopeUV0-500_sma3nz_amean                       float64\n",
      " 216  slopeUV500-1500_sma3nz_amean                    float64\n",
      " 217  spectralFluxUV_sma3nz_amean                     float64\n",
      " 218  loudnessPeaksPerSec                             float64\n",
      " 219  VoicedSegmentsPerSec                            float64\n",
      " 220  MeanVoicedSegmentLengthSec                      float64\n",
      " 221  StddevVoicedSegmentLengthSec                    float64\n",
      " 222  MeanUnvoicedSegmentLength                       float64\n",
      " 223  StddevUnvoicedSegmentLength                     float64\n",
      " 224  equivalentSoundLevel_dBp                        float64\n",
      "dtypes: float64(224), int64(1)\n",
      "memory usage: 1.3 MB\n"
     ]
    }
   ],
   "source": [
    "df_essentia_best_overall_opensmile_egemaps.info(verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join both the featureset and annotation set together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lowlevel.melbands_kurtosis.dmean</th>\n",
       "      <th>lowlevel.melbands_kurtosis.dmean2</th>\n",
       "      <th>lowlevel.melbands_kurtosis.dvar</th>\n",
       "      <th>lowlevel.melbands_kurtosis.dvar2</th>\n",
       "      <th>lowlevel.melbands_kurtosis.max</th>\n",
       "      <th>lowlevel.melbands_kurtosis.mean</th>\n",
       "      <th>lowlevel.melbands_kurtosis.median</th>\n",
       "      <th>lowlevel.melbands_kurtosis.min</th>\n",
       "      <th>lowlevel.melbands_kurtosis.stdev</th>\n",
       "      <th>lowlevel.melbands_kurtosis.var</th>\n",
       "      <th>...</th>\n",
       "      <th>spectralFluxUV_sma3nz_amean</th>\n",
       "      <th>loudnessPeaksPerSec</th>\n",
       "      <th>VoicedSegmentsPerSec</th>\n",
       "      <th>MeanVoicedSegmentLengthSec</th>\n",
       "      <th>StddevVoicedSegmentLengthSec</th>\n",
       "      <th>MeanUnvoicedSegmentLength</th>\n",
       "      <th>StddevUnvoicedSegmentLength</th>\n",
       "      <th>equivalentSoundLevel_dBp</th>\n",
       "      <th>valence_mean_mapped</th>\n",
       "      <th>arousal_mean_mapped</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.091959</td>\n",
       "      <td>0.086915</td>\n",
       "      <td>0.011661</td>\n",
       "      <td>0.009166</td>\n",
       "      <td>0.084971</td>\n",
       "      <td>0.064362</td>\n",
       "      <td>0.039249</td>\n",
       "      <td>0.200273</td>\n",
       "      <td>0.112646</td>\n",
       "      <td>0.014056</td>\n",
       "      <td>...</td>\n",
       "      <td>0.354845</td>\n",
       "      <td>0.618233</td>\n",
       "      <td>0.568024</td>\n",
       "      <td>0.012333</td>\n",
       "      <td>0.010856</td>\n",
       "      <td>0.092155</td>\n",
       "      <td>0.066829</td>\n",
       "      <td>0.644904</td>\n",
       "      <td>0.150</td>\n",
       "      <td>-0.200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.367830</td>\n",
       "      <td>0.350220</td>\n",
       "      <td>0.067970</td>\n",
       "      <td>0.045142</td>\n",
       "      <td>0.166129</td>\n",
       "      <td>0.345452</td>\n",
       "      <td>0.276910</td>\n",
       "      <td>0.408428</td>\n",
       "      <td>0.378375</td>\n",
       "      <td>0.146383</td>\n",
       "      <td>...</td>\n",
       "      <td>0.997077</td>\n",
       "      <td>0.719396</td>\n",
       "      <td>0.075804</td>\n",
       "      <td>0.038816</td>\n",
       "      <td>0.059588</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.893336</td>\n",
       "      <td>-0.425</td>\n",
       "      <td>-0.475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.793184</td>\n",
       "      <td>0.761592</td>\n",
       "      <td>0.292918</td>\n",
       "      <td>0.223284</td>\n",
       "      <td>0.284663</td>\n",
       "      <td>0.566046</td>\n",
       "      <td>0.407319</td>\n",
       "      <td>0.019133</td>\n",
       "      <td>0.656534</td>\n",
       "      <td>0.434120</td>\n",
       "      <td>...</td>\n",
       "      <td>0.482067</td>\n",
       "      <td>0.719487</td>\n",
       "      <td>0.395234</td>\n",
       "      <td>0.023807</td>\n",
       "      <td>0.022551</td>\n",
       "      <td>0.108542</td>\n",
       "      <td>0.109709</td>\n",
       "      <td>0.827230</td>\n",
       "      <td>-0.600</td>\n",
       "      <td>-0.700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.174975</td>\n",
       "      <td>0.166081</td>\n",
       "      <td>0.013457</td>\n",
       "      <td>0.010707</td>\n",
       "      <td>0.045539</td>\n",
       "      <td>0.168166</td>\n",
       "      <td>0.194476</td>\n",
       "      <td>0.250194</td>\n",
       "      <td>0.142053</td>\n",
       "      <td>0.021845</td>\n",
       "      <td>...</td>\n",
       "      <td>0.367544</td>\n",
       "      <td>0.607535</td>\n",
       "      <td>0.304105</td>\n",
       "      <td>0.005747</td>\n",
       "      <td>0.007682</td>\n",
       "      <td>0.347203</td>\n",
       "      <td>0.335536</td>\n",
       "      <td>0.827628</td>\n",
       "      <td>-0.300</td>\n",
       "      <td>0.025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.080461</td>\n",
       "      <td>0.077460</td>\n",
       "      <td>0.006162</td>\n",
       "      <td>0.004753</td>\n",
       "      <td>0.063129</td>\n",
       "      <td>0.050890</td>\n",
       "      <td>0.049963</td>\n",
       "      <td>0.095697</td>\n",
       "      <td>0.066929</td>\n",
       "      <td>0.005333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.459692</td>\n",
       "      <td>0.572874</td>\n",
       "      <td>0.442982</td>\n",
       "      <td>0.027635</td>\n",
       "      <td>0.023468</td>\n",
       "      <td>0.038398</td>\n",
       "      <td>0.029670</td>\n",
       "      <td>0.822717</td>\n",
       "      <td>0.450</td>\n",
       "      <td>0.400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>762</th>\n",
       "      <td>0.066183</td>\n",
       "      <td>0.066978</td>\n",
       "      <td>0.008151</td>\n",
       "      <td>0.007277</td>\n",
       "      <td>0.126425</td>\n",
       "      <td>0.058467</td>\n",
       "      <td>0.047070</td>\n",
       "      <td>0.022686</td>\n",
       "      <td>0.083308</td>\n",
       "      <td>0.007984</td>\n",
       "      <td>...</td>\n",
       "      <td>0.581421</td>\n",
       "      <td>0.471706</td>\n",
       "      <td>0.561126</td>\n",
       "      <td>0.014875</td>\n",
       "      <td>0.047375</td>\n",
       "      <td>0.075363</td>\n",
       "      <td>0.064431</td>\n",
       "      <td>0.822125</td>\n",
       "      <td>0.525</td>\n",
       "      <td>0.725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>763</th>\n",
       "      <td>0.143177</td>\n",
       "      <td>0.121697</td>\n",
       "      <td>0.034041</td>\n",
       "      <td>0.022859</td>\n",
       "      <td>0.178212</td>\n",
       "      <td>0.121857</td>\n",
       "      <td>0.032984</td>\n",
       "      <td>0.306449</td>\n",
       "      <td>0.274032</td>\n",
       "      <td>0.077814</td>\n",
       "      <td>...</td>\n",
       "      <td>0.573298</td>\n",
       "      <td>0.598725</td>\n",
       "      <td>0.638633</td>\n",
       "      <td>0.009723</td>\n",
       "      <td>0.009633</td>\n",
       "      <td>0.088765</td>\n",
       "      <td>0.082178</td>\n",
       "      <td>0.852123</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>764</th>\n",
       "      <td>0.057246</td>\n",
       "      <td>0.055671</td>\n",
       "      <td>0.003049</td>\n",
       "      <td>0.002654</td>\n",
       "      <td>0.047044</td>\n",
       "      <td>0.046206</td>\n",
       "      <td>0.045261</td>\n",
       "      <td>0.055447</td>\n",
       "      <td>0.051686</td>\n",
       "      <td>0.003342</td>\n",
       "      <td>...</td>\n",
       "      <td>0.423028</td>\n",
       "      <td>0.510169</td>\n",
       "      <td>0.532011</td>\n",
       "      <td>0.016340</td>\n",
       "      <td>0.019502</td>\n",
       "      <td>0.080065</td>\n",
       "      <td>0.163099</td>\n",
       "      <td>0.822882</td>\n",
       "      <td>0.325</td>\n",
       "      <td>0.425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>765</th>\n",
       "      <td>0.064311</td>\n",
       "      <td>0.060727</td>\n",
       "      <td>0.002448</td>\n",
       "      <td>0.001986</td>\n",
       "      <td>0.036530</td>\n",
       "      <td>0.063658</td>\n",
       "      <td>0.057241</td>\n",
       "      <td>0.558007</td>\n",
       "      <td>0.069792</td>\n",
       "      <td>0.005759</td>\n",
       "      <td>...</td>\n",
       "      <td>0.595691</td>\n",
       "      <td>0.377443</td>\n",
       "      <td>0.643245</td>\n",
       "      <td>0.008016</td>\n",
       "      <td>0.018335</td>\n",
       "      <td>0.099848</td>\n",
       "      <td>0.240641</td>\n",
       "      <td>0.889240</td>\n",
       "      <td>0.550</td>\n",
       "      <td>0.750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>766</th>\n",
       "      <td>0.027510</td>\n",
       "      <td>0.027913</td>\n",
       "      <td>0.000619</td>\n",
       "      <td>0.000547</td>\n",
       "      <td>0.022764</td>\n",
       "      <td>0.028755</td>\n",
       "      <td>0.036564</td>\n",
       "      <td>0.272612</td>\n",
       "      <td>0.031357</td>\n",
       "      <td>0.001399</td>\n",
       "      <td>...</td>\n",
       "      <td>0.597951</td>\n",
       "      <td>0.344933</td>\n",
       "      <td>0.656199</td>\n",
       "      <td>0.008606</td>\n",
       "      <td>0.012944</td>\n",
       "      <td>0.100924</td>\n",
       "      <td>0.120431</td>\n",
       "      <td>0.834901</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.325</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>767 rows × 226 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     lowlevel.melbands_kurtosis.dmean  lowlevel.melbands_kurtosis.dmean2  \\\n",
       "0                            0.091959                           0.086915   \n",
       "1                            0.367830                           0.350220   \n",
       "2                            0.793184                           0.761592   \n",
       "3                            0.174975                           0.166081   \n",
       "4                            0.080461                           0.077460   \n",
       "..                                ...                                ...   \n",
       "762                          0.066183                           0.066978   \n",
       "763                          0.143177                           0.121697   \n",
       "764                          0.057246                           0.055671   \n",
       "765                          0.064311                           0.060727   \n",
       "766                          0.027510                           0.027913   \n",
       "\n",
       "     lowlevel.melbands_kurtosis.dvar  lowlevel.melbands_kurtosis.dvar2  \\\n",
       "0                           0.011661                          0.009166   \n",
       "1                           0.067970                          0.045142   \n",
       "2                           0.292918                          0.223284   \n",
       "3                           0.013457                          0.010707   \n",
       "4                           0.006162                          0.004753   \n",
       "..                               ...                               ...   \n",
       "762                         0.008151                          0.007277   \n",
       "763                         0.034041                          0.022859   \n",
       "764                         0.003049                          0.002654   \n",
       "765                         0.002448                          0.001986   \n",
       "766                         0.000619                          0.000547   \n",
       "\n",
       "     lowlevel.melbands_kurtosis.max  lowlevel.melbands_kurtosis.mean  \\\n",
       "0                          0.084971                         0.064362   \n",
       "1                          0.166129                         0.345452   \n",
       "2                          0.284663                         0.566046   \n",
       "3                          0.045539                         0.168166   \n",
       "4                          0.063129                         0.050890   \n",
       "..                              ...                              ...   \n",
       "762                        0.126425                         0.058467   \n",
       "763                        0.178212                         0.121857   \n",
       "764                        0.047044                         0.046206   \n",
       "765                        0.036530                         0.063658   \n",
       "766                        0.022764                         0.028755   \n",
       "\n",
       "     lowlevel.melbands_kurtosis.median  lowlevel.melbands_kurtosis.min  \\\n",
       "0                             0.039249                        0.200273   \n",
       "1                             0.276910                        0.408428   \n",
       "2                             0.407319                        0.019133   \n",
       "3                             0.194476                        0.250194   \n",
       "4                             0.049963                        0.095697   \n",
       "..                                 ...                             ...   \n",
       "762                           0.047070                        0.022686   \n",
       "763                           0.032984                        0.306449   \n",
       "764                           0.045261                        0.055447   \n",
       "765                           0.057241                        0.558007   \n",
       "766                           0.036564                        0.272612   \n",
       "\n",
       "     lowlevel.melbands_kurtosis.stdev  lowlevel.melbands_kurtosis.var  ...  \\\n",
       "0                            0.112646                        0.014056  ...   \n",
       "1                            0.378375                        0.146383  ...   \n",
       "2                            0.656534                        0.434120  ...   \n",
       "3                            0.142053                        0.021845  ...   \n",
       "4                            0.066929                        0.005333  ...   \n",
       "..                                ...                             ...  ...   \n",
       "762                          0.083308                        0.007984  ...   \n",
       "763                          0.274032                        0.077814  ...   \n",
       "764                          0.051686                        0.003342  ...   \n",
       "765                          0.069792                        0.005759  ...   \n",
       "766                          0.031357                        0.001399  ...   \n",
       "\n",
       "     spectralFluxUV_sma3nz_amean  loudnessPeaksPerSec  VoicedSegmentsPerSec  \\\n",
       "0                       0.354845             0.618233              0.568024   \n",
       "1                       0.997077             0.719396              0.075804   \n",
       "2                       0.482067             0.719487              0.395234   \n",
       "3                       0.367544             0.607535              0.304105   \n",
       "4                       0.459692             0.572874              0.442982   \n",
       "..                           ...                  ...                   ...   \n",
       "762                     0.581421             0.471706              0.561126   \n",
       "763                     0.573298             0.598725              0.638633   \n",
       "764                     0.423028             0.510169              0.532011   \n",
       "765                     0.595691             0.377443              0.643245   \n",
       "766                     0.597951             0.344933              0.656199   \n",
       "\n",
       "     MeanVoicedSegmentLengthSec  StddevVoicedSegmentLengthSec  \\\n",
       "0                      0.012333                      0.010856   \n",
       "1                      0.038816                      0.059588   \n",
       "2                      0.023807                      0.022551   \n",
       "3                      0.005747                      0.007682   \n",
       "4                      0.027635                      0.023468   \n",
       "..                          ...                           ...   \n",
       "762                    0.014875                      0.047375   \n",
       "763                    0.009723                      0.009633   \n",
       "764                    0.016340                      0.019502   \n",
       "765                    0.008016                      0.018335   \n",
       "766                    0.008606                      0.012944   \n",
       "\n",
       "     MeanUnvoicedSegmentLength  StddevUnvoicedSegmentLength  \\\n",
       "0                     0.092155                     0.066829   \n",
       "1                     1.000000                     1.000000   \n",
       "2                     0.108542                     0.109709   \n",
       "3                     0.347203                     0.335536   \n",
       "4                     0.038398                     0.029670   \n",
       "..                         ...                          ...   \n",
       "762                   0.075363                     0.064431   \n",
       "763                   0.088765                     0.082178   \n",
       "764                   0.080065                     0.163099   \n",
       "765                   0.099848                     0.240641   \n",
       "766                   0.100924                     0.120431   \n",
       "\n",
       "     equivalentSoundLevel_dBp  valence_mean_mapped  arousal_mean_mapped  \n",
       "0                    0.644904                0.150               -0.200  \n",
       "1                    0.893336               -0.425               -0.475  \n",
       "2                    0.827230               -0.600               -0.700  \n",
       "3                    0.827628               -0.300                0.025  \n",
       "4                    0.822717                0.450                0.400  \n",
       "..                        ...                  ...                  ...  \n",
       "762                  0.822125                0.525                0.725  \n",
       "763                  0.852123                0.125                0.750  \n",
       "764                  0.822882                0.325                0.425  \n",
       "765                  0.889240                0.550                0.750  \n",
       "766                  0.834901                0.150                0.325  \n",
       "\n",
       "[767 rows x 226 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_essentia_best_overall_opensmile_egemaps_whole = pd.merge(df_essentia_best_overall_opensmile_egemaps, df_annotations, how='inner', on='song_id')\n",
    "df_essentia_best_overall_opensmile_egemaps_whole = df_essentia_best_overall_opensmile_egemaps_whole.drop('song_id', axis=1)\n",
    "df_essentia_best_overall_opensmile_egemaps_whole"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare dataframes for the neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform splitting of the dataframe into training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lowlevel.melbands_kurtosis.dmean</th>\n",
       "      <th>lowlevel.melbands_kurtosis.dmean2</th>\n",
       "      <th>lowlevel.melbands_kurtosis.dvar</th>\n",
       "      <th>lowlevel.melbands_kurtosis.dvar2</th>\n",
       "      <th>lowlevel.melbands_kurtosis.max</th>\n",
       "      <th>lowlevel.melbands_kurtosis.mean</th>\n",
       "      <th>lowlevel.melbands_kurtosis.median</th>\n",
       "      <th>lowlevel.melbands_kurtosis.min</th>\n",
       "      <th>lowlevel.melbands_kurtosis.stdev</th>\n",
       "      <th>lowlevel.melbands_kurtosis.var</th>\n",
       "      <th>...</th>\n",
       "      <th>slopeUV0-500_sma3nz_amean</th>\n",
       "      <th>slopeUV500-1500_sma3nz_amean</th>\n",
       "      <th>spectralFluxUV_sma3nz_amean</th>\n",
       "      <th>loudnessPeaksPerSec</th>\n",
       "      <th>VoicedSegmentsPerSec</th>\n",
       "      <th>MeanVoicedSegmentLengthSec</th>\n",
       "      <th>StddevVoicedSegmentLengthSec</th>\n",
       "      <th>MeanUnvoicedSegmentLength</th>\n",
       "      <th>StddevUnvoicedSegmentLength</th>\n",
       "      <th>equivalentSoundLevel_dBp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.091959</td>\n",
       "      <td>0.086915</td>\n",
       "      <td>0.011661</td>\n",
       "      <td>0.009166</td>\n",
       "      <td>0.084971</td>\n",
       "      <td>0.064362</td>\n",
       "      <td>0.039249</td>\n",
       "      <td>0.200273</td>\n",
       "      <td>0.112646</td>\n",
       "      <td>0.014056</td>\n",
       "      <td>...</td>\n",
       "      <td>0.360946</td>\n",
       "      <td>0.477083</td>\n",
       "      <td>0.354845</td>\n",
       "      <td>0.618233</td>\n",
       "      <td>0.568024</td>\n",
       "      <td>0.012333</td>\n",
       "      <td>0.010856</td>\n",
       "      <td>0.092155</td>\n",
       "      <td>0.066829</td>\n",
       "      <td>0.644904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.367830</td>\n",
       "      <td>0.350220</td>\n",
       "      <td>0.067970</td>\n",
       "      <td>0.045142</td>\n",
       "      <td>0.166129</td>\n",
       "      <td>0.345452</td>\n",
       "      <td>0.276910</td>\n",
       "      <td>0.408428</td>\n",
       "      <td>0.378375</td>\n",
       "      <td>0.146383</td>\n",
       "      <td>...</td>\n",
       "      <td>0.194518</td>\n",
       "      <td>0.197515</td>\n",
       "      <td>0.997077</td>\n",
       "      <td>0.719396</td>\n",
       "      <td>0.075804</td>\n",
       "      <td>0.038816</td>\n",
       "      <td>0.059588</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.893336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.793184</td>\n",
       "      <td>0.761592</td>\n",
       "      <td>0.292918</td>\n",
       "      <td>0.223284</td>\n",
       "      <td>0.284663</td>\n",
       "      <td>0.566046</td>\n",
       "      <td>0.407319</td>\n",
       "      <td>0.019133</td>\n",
       "      <td>0.656534</td>\n",
       "      <td>0.434120</td>\n",
       "      <td>...</td>\n",
       "      <td>0.227733</td>\n",
       "      <td>0.398985</td>\n",
       "      <td>0.482067</td>\n",
       "      <td>0.719487</td>\n",
       "      <td>0.395234</td>\n",
       "      <td>0.023807</td>\n",
       "      <td>0.022551</td>\n",
       "      <td>0.108542</td>\n",
       "      <td>0.109709</td>\n",
       "      <td>0.827230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.174975</td>\n",
       "      <td>0.166081</td>\n",
       "      <td>0.013457</td>\n",
       "      <td>0.010707</td>\n",
       "      <td>0.045539</td>\n",
       "      <td>0.168166</td>\n",
       "      <td>0.194476</td>\n",
       "      <td>0.250194</td>\n",
       "      <td>0.142053</td>\n",
       "      <td>0.021845</td>\n",
       "      <td>...</td>\n",
       "      <td>0.155392</td>\n",
       "      <td>0.542070</td>\n",
       "      <td>0.367544</td>\n",
       "      <td>0.607535</td>\n",
       "      <td>0.304105</td>\n",
       "      <td>0.005747</td>\n",
       "      <td>0.007682</td>\n",
       "      <td>0.347203</td>\n",
       "      <td>0.335536</td>\n",
       "      <td>0.827628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.080461</td>\n",
       "      <td>0.077460</td>\n",
       "      <td>0.006162</td>\n",
       "      <td>0.004753</td>\n",
       "      <td>0.063129</td>\n",
       "      <td>0.050890</td>\n",
       "      <td>0.049963</td>\n",
       "      <td>0.095697</td>\n",
       "      <td>0.066929</td>\n",
       "      <td>0.005333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.612921</td>\n",
       "      <td>0.377957</td>\n",
       "      <td>0.459692</td>\n",
       "      <td>0.572874</td>\n",
       "      <td>0.442982</td>\n",
       "      <td>0.027635</td>\n",
       "      <td>0.023468</td>\n",
       "      <td>0.038398</td>\n",
       "      <td>0.029670</td>\n",
       "      <td>0.822717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>762</th>\n",
       "      <td>0.066183</td>\n",
       "      <td>0.066978</td>\n",
       "      <td>0.008151</td>\n",
       "      <td>0.007277</td>\n",
       "      <td>0.126425</td>\n",
       "      <td>0.058467</td>\n",
       "      <td>0.047070</td>\n",
       "      <td>0.022686</td>\n",
       "      <td>0.083308</td>\n",
       "      <td>0.007984</td>\n",
       "      <td>...</td>\n",
       "      <td>0.487869</td>\n",
       "      <td>0.487722</td>\n",
       "      <td>0.581421</td>\n",
       "      <td>0.471706</td>\n",
       "      <td>0.561126</td>\n",
       "      <td>0.014875</td>\n",
       "      <td>0.047375</td>\n",
       "      <td>0.075363</td>\n",
       "      <td>0.064431</td>\n",
       "      <td>0.822125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>763</th>\n",
       "      <td>0.143177</td>\n",
       "      <td>0.121697</td>\n",
       "      <td>0.034041</td>\n",
       "      <td>0.022859</td>\n",
       "      <td>0.178212</td>\n",
       "      <td>0.121857</td>\n",
       "      <td>0.032984</td>\n",
       "      <td>0.306449</td>\n",
       "      <td>0.274032</td>\n",
       "      <td>0.077814</td>\n",
       "      <td>...</td>\n",
       "      <td>0.504947</td>\n",
       "      <td>0.673577</td>\n",
       "      <td>0.573298</td>\n",
       "      <td>0.598725</td>\n",
       "      <td>0.638633</td>\n",
       "      <td>0.009723</td>\n",
       "      <td>0.009633</td>\n",
       "      <td>0.088765</td>\n",
       "      <td>0.082178</td>\n",
       "      <td>0.852123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>764</th>\n",
       "      <td>0.057246</td>\n",
       "      <td>0.055671</td>\n",
       "      <td>0.003049</td>\n",
       "      <td>0.002654</td>\n",
       "      <td>0.047044</td>\n",
       "      <td>0.046206</td>\n",
       "      <td>0.045261</td>\n",
       "      <td>0.055447</td>\n",
       "      <td>0.051686</td>\n",
       "      <td>0.003342</td>\n",
       "      <td>...</td>\n",
       "      <td>0.446266</td>\n",
       "      <td>0.590639</td>\n",
       "      <td>0.423028</td>\n",
       "      <td>0.510169</td>\n",
       "      <td>0.532011</td>\n",
       "      <td>0.016340</td>\n",
       "      <td>0.019502</td>\n",
       "      <td>0.080065</td>\n",
       "      <td>0.163099</td>\n",
       "      <td>0.822882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>765</th>\n",
       "      <td>0.064311</td>\n",
       "      <td>0.060727</td>\n",
       "      <td>0.002448</td>\n",
       "      <td>0.001986</td>\n",
       "      <td>0.036530</td>\n",
       "      <td>0.063658</td>\n",
       "      <td>0.057241</td>\n",
       "      <td>0.558007</td>\n",
       "      <td>0.069792</td>\n",
       "      <td>0.005759</td>\n",
       "      <td>...</td>\n",
       "      <td>0.384011</td>\n",
       "      <td>0.610316</td>\n",
       "      <td>0.595691</td>\n",
       "      <td>0.377443</td>\n",
       "      <td>0.643245</td>\n",
       "      <td>0.008016</td>\n",
       "      <td>0.018335</td>\n",
       "      <td>0.099848</td>\n",
       "      <td>0.240641</td>\n",
       "      <td>0.889240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>766</th>\n",
       "      <td>0.027510</td>\n",
       "      <td>0.027913</td>\n",
       "      <td>0.000619</td>\n",
       "      <td>0.000547</td>\n",
       "      <td>0.022764</td>\n",
       "      <td>0.028755</td>\n",
       "      <td>0.036564</td>\n",
       "      <td>0.272612</td>\n",
       "      <td>0.031357</td>\n",
       "      <td>0.001399</td>\n",
       "      <td>...</td>\n",
       "      <td>0.432009</td>\n",
       "      <td>0.621859</td>\n",
       "      <td>0.597951</td>\n",
       "      <td>0.344933</td>\n",
       "      <td>0.656199</td>\n",
       "      <td>0.008606</td>\n",
       "      <td>0.012944</td>\n",
       "      <td>0.100924</td>\n",
       "      <td>0.120431</td>\n",
       "      <td>0.834901</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>767 rows × 224 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     lowlevel.melbands_kurtosis.dmean  lowlevel.melbands_kurtosis.dmean2  \\\n",
       "0                            0.091959                           0.086915   \n",
       "1                            0.367830                           0.350220   \n",
       "2                            0.793184                           0.761592   \n",
       "3                            0.174975                           0.166081   \n",
       "4                            0.080461                           0.077460   \n",
       "..                                ...                                ...   \n",
       "762                          0.066183                           0.066978   \n",
       "763                          0.143177                           0.121697   \n",
       "764                          0.057246                           0.055671   \n",
       "765                          0.064311                           0.060727   \n",
       "766                          0.027510                           0.027913   \n",
       "\n",
       "     lowlevel.melbands_kurtosis.dvar  lowlevel.melbands_kurtosis.dvar2  \\\n",
       "0                           0.011661                          0.009166   \n",
       "1                           0.067970                          0.045142   \n",
       "2                           0.292918                          0.223284   \n",
       "3                           0.013457                          0.010707   \n",
       "4                           0.006162                          0.004753   \n",
       "..                               ...                               ...   \n",
       "762                         0.008151                          0.007277   \n",
       "763                         0.034041                          0.022859   \n",
       "764                         0.003049                          0.002654   \n",
       "765                         0.002448                          0.001986   \n",
       "766                         0.000619                          0.000547   \n",
       "\n",
       "     lowlevel.melbands_kurtosis.max  lowlevel.melbands_kurtosis.mean  \\\n",
       "0                          0.084971                         0.064362   \n",
       "1                          0.166129                         0.345452   \n",
       "2                          0.284663                         0.566046   \n",
       "3                          0.045539                         0.168166   \n",
       "4                          0.063129                         0.050890   \n",
       "..                              ...                              ...   \n",
       "762                        0.126425                         0.058467   \n",
       "763                        0.178212                         0.121857   \n",
       "764                        0.047044                         0.046206   \n",
       "765                        0.036530                         0.063658   \n",
       "766                        0.022764                         0.028755   \n",
       "\n",
       "     lowlevel.melbands_kurtosis.median  lowlevel.melbands_kurtosis.min  \\\n",
       "0                             0.039249                        0.200273   \n",
       "1                             0.276910                        0.408428   \n",
       "2                             0.407319                        0.019133   \n",
       "3                             0.194476                        0.250194   \n",
       "4                             0.049963                        0.095697   \n",
       "..                                 ...                             ...   \n",
       "762                           0.047070                        0.022686   \n",
       "763                           0.032984                        0.306449   \n",
       "764                           0.045261                        0.055447   \n",
       "765                           0.057241                        0.558007   \n",
       "766                           0.036564                        0.272612   \n",
       "\n",
       "     lowlevel.melbands_kurtosis.stdev  lowlevel.melbands_kurtosis.var  ...  \\\n",
       "0                            0.112646                        0.014056  ...   \n",
       "1                            0.378375                        0.146383  ...   \n",
       "2                            0.656534                        0.434120  ...   \n",
       "3                            0.142053                        0.021845  ...   \n",
       "4                            0.066929                        0.005333  ...   \n",
       "..                                ...                             ...  ...   \n",
       "762                          0.083308                        0.007984  ...   \n",
       "763                          0.274032                        0.077814  ...   \n",
       "764                          0.051686                        0.003342  ...   \n",
       "765                          0.069792                        0.005759  ...   \n",
       "766                          0.031357                        0.001399  ...   \n",
       "\n",
       "     slopeUV0-500_sma3nz_amean  slopeUV500-1500_sma3nz_amean  \\\n",
       "0                     0.360946                      0.477083   \n",
       "1                     0.194518                      0.197515   \n",
       "2                     0.227733                      0.398985   \n",
       "3                     0.155392                      0.542070   \n",
       "4                     0.612921                      0.377957   \n",
       "..                         ...                           ...   \n",
       "762                   0.487869                      0.487722   \n",
       "763                   0.504947                      0.673577   \n",
       "764                   0.446266                      0.590639   \n",
       "765                   0.384011                      0.610316   \n",
       "766                   0.432009                      0.621859   \n",
       "\n",
       "     spectralFluxUV_sma3nz_amean  loudnessPeaksPerSec  VoicedSegmentsPerSec  \\\n",
       "0                       0.354845             0.618233              0.568024   \n",
       "1                       0.997077             0.719396              0.075804   \n",
       "2                       0.482067             0.719487              0.395234   \n",
       "3                       0.367544             0.607535              0.304105   \n",
       "4                       0.459692             0.572874              0.442982   \n",
       "..                           ...                  ...                   ...   \n",
       "762                     0.581421             0.471706              0.561126   \n",
       "763                     0.573298             0.598725              0.638633   \n",
       "764                     0.423028             0.510169              0.532011   \n",
       "765                     0.595691             0.377443              0.643245   \n",
       "766                     0.597951             0.344933              0.656199   \n",
       "\n",
       "     MeanVoicedSegmentLengthSec  StddevVoicedSegmentLengthSec  \\\n",
       "0                      0.012333                      0.010856   \n",
       "1                      0.038816                      0.059588   \n",
       "2                      0.023807                      0.022551   \n",
       "3                      0.005747                      0.007682   \n",
       "4                      0.027635                      0.023468   \n",
       "..                          ...                           ...   \n",
       "762                    0.014875                      0.047375   \n",
       "763                    0.009723                      0.009633   \n",
       "764                    0.016340                      0.019502   \n",
       "765                    0.008016                      0.018335   \n",
       "766                    0.008606                      0.012944   \n",
       "\n",
       "     MeanUnvoicedSegmentLength  StddevUnvoicedSegmentLength  \\\n",
       "0                     0.092155                     0.066829   \n",
       "1                     1.000000                     1.000000   \n",
       "2                     0.108542                     0.109709   \n",
       "3                     0.347203                     0.335536   \n",
       "4                     0.038398                     0.029670   \n",
       "..                         ...                          ...   \n",
       "762                   0.075363                     0.064431   \n",
       "763                   0.088765                     0.082178   \n",
       "764                   0.080065                     0.163099   \n",
       "765                   0.099848                     0.240641   \n",
       "766                   0.100924                     0.120431   \n",
       "\n",
       "     equivalentSoundLevel_dBp  \n",
       "0                    0.644904  \n",
       "1                    0.893336  \n",
       "2                    0.827230  \n",
       "3                    0.827628  \n",
       "4                    0.822717  \n",
       "..                        ...  \n",
       "762                  0.822125  \n",
       "763                  0.852123  \n",
       "764                  0.822882  \n",
       "765                  0.889240  \n",
       "766                  0.834901  \n",
       "\n",
       "[767 rows x 224 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = df_essentia_best_overall_opensmile_egemaps.drop('song_id', axis=1)\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>valence_mean_mapped</th>\n",
       "      <th>arousal_mean_mapped</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.150</td>\n",
       "      <td>-0.200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.425</td>\n",
       "      <td>-0.475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.600</td>\n",
       "      <td>-0.700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.300</td>\n",
       "      <td>0.025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.450</td>\n",
       "      <td>0.400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>762</th>\n",
       "      <td>0.525</td>\n",
       "      <td>0.725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>763</th>\n",
       "      <td>0.125</td>\n",
       "      <td>0.750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>764</th>\n",
       "      <td>0.325</td>\n",
       "      <td>0.425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>765</th>\n",
       "      <td>0.550</td>\n",
       "      <td>0.750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>766</th>\n",
       "      <td>0.150</td>\n",
       "      <td>0.325</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>767 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     valence_mean_mapped  arousal_mean_mapped\n",
       "0                  0.150               -0.200\n",
       "1                 -0.425               -0.475\n",
       "2                 -0.600               -0.700\n",
       "3                 -0.300                0.025\n",
       "4                  0.450                0.400\n",
       "..                   ...                  ...\n",
       "762                0.525                0.725\n",
       "763                0.125                0.750\n",
       "764                0.325                0.425\n",
       "765                0.550                0.750\n",
       "766                0.150                0.325\n",
       "\n",
       "[767 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets = df_annotations.drop('song_id', axis=1)\n",
    "targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform 80-20 train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(features, targets, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create tensors for X_train and X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tensor = torch.tensor(X_train.values, dtype=torch.float64)\n",
    "X_test_tensor = torch.tensor(X_test.values, dtype=torch.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create tensors for Y_train and Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float64)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define neural network parameters and instantitate neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 1\n",
    "hidden_size = 20 \n",
    "output_size = 2  # Output size for valence and arousal\n",
    "learning_rate = 0.001\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 126"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set a random seed to ensure consistent initial weights of the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x11347be50>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the seed\n",
    "seed = 42\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_size, math.ceil((input_size**0.5) * 2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(math.ceil((input_size**0.5) * 2), math.ceil((input_size**0.5) * 2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(math.ceil((input_size**0.5) * 2), math.ceil((input_size**0.5) * 2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(math.ceil((input_size**0.5) * 2), math.ceil((input_size**0.5) * 2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(math.ceil((input_size**0.5) * 2), math.ceil((input_size**0.5) * 2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(math.ceil((input_size**0.5) * 2), math.ceil((input_size**0.5) * 2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(math.ceil((input_size**0.5) * 2), math.ceil((input_size**0.5) * 2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(math.ceil((input_size**0.5) * 2), math.ceil((input_size**0.5) * 2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(math.ceil((input_size**0.5) * 2), math.ceil((input_size**0.5) * 2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(math.ceil((input_size**0.5) * 2), math.ceil((input_size**0.5) * 2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(math.ceil((input_size**0.5) * 2), math.ceil((input_size**0.5) * 2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(math.ceil((input_size**0.5) * 2), 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare input_train_data and target_train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([613, 224])\n"
     ]
    }
   ],
   "source": [
    "input_train_data = X_train_tensor.float()\n",
    "\n",
    "# input_train_data = input_train_data.view(input_train_data.shape[1], -1)\n",
    "print(input_train_data.shape)\n",
    "\n",
    "target_train_labels = y_train_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(num_epochs):\n",
    "  model = NeuralNetwork(input_size=input_train_data.shape[1])\n",
    "  optimiser = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "  \n",
    "  for epoch in range(num_epochs):\n",
    "    optimiser.zero_grad()\n",
    "    \n",
    "    # forward pass\n",
    "    output = model(input_train_data)\n",
    "\n",
    "    # calculate loss\n",
    "    loss = torch.sqrt(criterion(output.float(), target_train_labels.float()))\n",
    "\n",
    "    # backward pass\n",
    "    loss.backward()\n",
    "    # update weights\n",
    "    optimiser.step()\n",
    "\n",
    "    print(f'Epoch {epoch + 1}, Loss: {math.sqrt(loss.item())}')\n",
    "\n",
    "  print(\"Training completed.\")\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Epoch 121, Loss: 0.4596922103619757\n",
      "Epoch 122, Loss: 0.45943389956138125\n",
      "Epoch 123, Loss: 0.45877818894117006\n",
      "Epoch 124, Loss: 0.45746538132874076\n",
      "Epoch 125, Loss: 0.4570462803121619\n",
      "Epoch 126, Loss: 0.45696189549769234\n",
      "Training completed.\n"
     ]
    }
   ],
   "source": [
    "model = train_model(num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare input_test_data and target_test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([154, 224])\n"
     ]
    }
   ],
   "source": [
    "input_test_data = X_test_tensor.float()\n",
    "\n",
    "# input_test_data = input_test_data.view(input_test_data.shape[1], -1)\n",
    "print(input_test_data.shape)\n",
    "\n",
    "target_test_labels = y_test_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(trained_model):\n",
    "  with torch.no_grad():\n",
    "    test_pred = trained_model(input_test_data)\n",
    "    test_loss = criterion(test_pred.float(), target_test_labels)\n",
    "\n",
    "    # Separate the output into valence and arousal\n",
    "    valence_pred = test_pred[:, 0]\n",
    "    arousal_pred = test_pred[:, 1]\n",
    "        \n",
    "    valence_target = target_test_labels[:, 0]\n",
    "    arousal_target = target_test_labels[:, 1]\n",
    "\n",
    "     # Calculate RMSE for valence and arousal separately\n",
    "    valence_rmse = math.sqrt(mean_squared_error(valence_pred, valence_target))\n",
    "    arousal_rmse = math.sqrt(mean_squared_error(arousal_pred, arousal_target))\n",
    "\n",
    "  rmse = math.sqrt(test_loss.item())\n",
    "  print(f'Test RMSE: {rmse}')\n",
    "\n",
    "  print(f'Valence RMSE: {valence_rmse}')\n",
    "  print(f'Arousal RMSE: {arousal_rmse}')\n",
    "\n",
    "  metric = R2Score(multioutput=\"raw_values\")\n",
    "  metric.update(test_pred, target_test_labels)\n",
    "  adjusted_r2_score = metric.compute()\n",
    "  print(f'Test R^2 score: {adjusted_r2_score}')\n",
    "\n",
    "  # metric = R2Score(multioutput=\"raw_values\", num_regressors=input_test_data.shape[1])\n",
    "  # metric.update(test_pred, target_test_labels)\n",
    "  # adjusted_r2_score = metric.compute()\n",
    "  # print(f'Test Adjusted R^2 score: {adjusted_r2_score}')\n",
    "\n",
    "  metric = R2Score()\n",
    "  metric.update(test_pred, target_test_labels)\n",
    "  r2_score = metric.compute()\n",
    "  print(f'Test R^2 score (overall): {r2_score}')\n",
    "  return test_pred, rmse, adjusted_r2_score, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE: 0.2313020158721768\n",
      "Valence RMSE: 0.24420265825347615\n",
      "Arousal RMSE: 0.21763801780709502\n",
      "Test R^2 score: tensor([0.3627, 0.6507], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.5067090666334924\n"
     ]
    }
   ],
   "source": [
    "test_pred, rmse, adjusted_r2_score, r2_score = test_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), '../../models/pmemo_feedforward_nn_essentia_best_overall_opensmile_egemaps_normalised.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "True values (test set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5750,  0.3500],\n",
       "        [ 0.1250, -0.0250],\n",
       "        [ 0.2000,  0.4750],\n",
       "        [ 0.3500,  0.3500],\n",
       "        [ 0.3000,  0.4500],\n",
       "        [ 0.3500,  0.0250],\n",
       "        [ 0.3250, -0.0250],\n",
       "        [ 0.3750,  0.3500],\n",
       "        [ 0.1500,  0.1000],\n",
       "        [ 0.2750,  0.6500],\n",
       "        [ 0.5000,  0.5250],\n",
       "        [ 0.0500, -0.3500],\n",
       "        [ 0.0500,  0.2250],\n",
       "        [-0.3250, -0.4500],\n",
       "        [-0.1000,  0.4500],\n",
       "        [ 0.1250, -0.4000],\n",
       "        [ 0.3750,  0.5500],\n",
       "        [ 0.2000, -0.2250],\n",
       "        [-0.4500, -0.3000],\n",
       "        [ 0.0500,  0.0750],\n",
       "        [ 0.2750,  0.4250],\n",
       "        [-0.0250,  0.4000],\n",
       "        [ 0.6500,  0.6750],\n",
       "        [-0.1750, -0.3250],\n",
       "        [-0.6500,  0.6500],\n",
       "        [ 0.0250,  0.3000],\n",
       "        [-0.0500,  0.6750],\n",
       "        [-0.7250, -0.4500],\n",
       "        [ 0.0000, -0.2750],\n",
       "        [ 0.2750,  0.4500],\n",
       "        [ 0.0000, -0.2000],\n",
       "        [ 0.3250,  0.2250],\n",
       "        [-0.3750, -0.1250],\n",
       "        [-0.1000,  0.2250],\n",
       "        [ 0.4000,  0.2250],\n",
       "        [ 0.3500,  0.4000],\n",
       "        [ 0.4500,  0.7000],\n",
       "        [ 0.5250,  0.4500],\n",
       "        [ 0.5750,  0.3250],\n",
       "        [ 0.6000,  0.5250],\n",
       "        [ 0.5750,  0.7000],\n",
       "        [ 0.3000,  0.5000],\n",
       "        [ 0.6750,  0.7750],\n",
       "        [ 0.3500,  0.3500],\n",
       "        [ 0.2000,  0.5250],\n",
       "        [ 0.1818,  0.7500],\n",
       "        [ 0.4250,  0.5750],\n",
       "        [ 0.3000,  0.2250],\n",
       "        [-0.1250, -0.1250],\n",
       "        [ 0.1000,  0.1500],\n",
       "        [ 0.4500,  0.2250],\n",
       "        [-0.1500, -0.3750],\n",
       "        [ 0.1750,  0.1000],\n",
       "        [-0.5500, -0.4750],\n",
       "        [ 0.1500,  0.1500],\n",
       "        [ 0.7000,  0.6250],\n",
       "        [ 0.7000,  0.5250],\n",
       "        [ 0.3750,  0.5250],\n",
       "        [ 0.5750,  0.5750],\n",
       "        [ 0.4000,  0.6000],\n",
       "        [ 0.4500,  0.3750],\n",
       "        [ 0.1250,  0.7500],\n",
       "        [ 0.0500,  0.3500],\n",
       "        [ 0.5500,  0.5750],\n",
       "        [-0.0250, -0.4250],\n",
       "        [-0.0750, -0.2750],\n",
       "        [-0.2250, -0.6000],\n",
       "        [ 0.6500,  0.4750],\n",
       "        [ 0.3000,  0.1250],\n",
       "        [ 0.3000,  0.2250],\n",
       "        [ 0.1750,  0.6000],\n",
       "        [-0.3250,  0.2000],\n",
       "        [ 0.3250,  0.1500],\n",
       "        [ 0.4000,  0.5250],\n",
       "        [ 0.0500,  0.1750],\n",
       "        [ 0.5750,  0.7500],\n",
       "        [-0.2000, -0.1500],\n",
       "        [ 0.4750,  0.3750],\n",
       "        [ 0.2250,  0.4250],\n",
       "        [ 0.1500,  0.1250],\n",
       "        [ 0.3750,  0.2500],\n",
       "        [ 0.1000, -0.2750],\n",
       "        [ 0.5000,  0.6750],\n",
       "        [ 0.7500,  0.7750],\n",
       "        [-0.1500,  0.1000],\n",
       "        [-0.2000, -0.0250],\n",
       "        [ 0.0750,  0.8750],\n",
       "        [ 0.2750, -0.6500],\n",
       "        [ 0.2500,  0.8500],\n",
       "        [-0.3000, -0.5000],\n",
       "        [ 0.2000,  0.3500],\n",
       "        [ 0.0500,  0.4000],\n",
       "        [ 0.3000,  0.4750],\n",
       "        [ 0.3000, -0.1750],\n",
       "        [-0.2500,  0.0250],\n",
       "        [ 0.2000,  0.3000],\n",
       "        [ 0.4750,  0.5250],\n",
       "        [ 0.0250,  0.4250],\n",
       "        [ 0.1000,  0.4000],\n",
       "        [ 0.1500,  0.3000],\n",
       "        [ 0.0909,  0.0909],\n",
       "        [ 0.1750,  0.1250],\n",
       "        [ 0.1750,  0.2500],\n",
       "        [ 0.0000,  0.4750],\n",
       "        [ 0.0750, -0.2750],\n",
       "        [-0.1750, -0.0250],\n",
       "        [ 0.3000,  0.2750],\n",
       "        [-0.0500,  0.0500],\n",
       "        [ 0.0750,  0.8500],\n",
       "        [ 0.5500,  0.7250],\n",
       "        [ 0.4750,  0.3250],\n",
       "        [ 0.4500,  0.7250],\n",
       "        [-0.1818, -0.1591],\n",
       "        [ 0.5909,  0.8182],\n",
       "        [ 0.2250,  0.6750],\n",
       "        [ 0.5000,  0.2750],\n",
       "        [ 0.5750,  0.6500],\n",
       "        [ 0.3000,  0.3000],\n",
       "        [ 0.0750,  0.0000],\n",
       "        [-0.1500, -0.1250],\n",
       "        [-0.1000,  0.0750],\n",
       "        [-0.0750, -0.2000],\n",
       "        [ 0.0750,  0.2250],\n",
       "        [-0.0750,  0.6000],\n",
       "        [ 0.4000,  0.4000],\n",
       "        [ 0.5250,  0.7250],\n",
       "        [-0.2500, -0.4250],\n",
       "        [ 0.5750,  0.4500],\n",
       "        [ 0.1250,  0.0500],\n",
       "        [ 0.0750,  0.3000],\n",
       "        [-0.6000, -0.7000],\n",
       "        [-0.0250, -0.0750],\n",
       "        [ 0.5000,  0.4750],\n",
       "        [-0.1000, -0.0500],\n",
       "        [-0.0500, -0.3250],\n",
       "        [ 0.4750,  0.5250],\n",
       "        [-0.2000, -0.0250],\n",
       "        [ 0.5000,  0.6750],\n",
       "        [ 0.0500, -0.0750],\n",
       "        [ 0.4500,  0.3750],\n",
       "        [ 0.6500,  0.7500],\n",
       "        [-0.1500, -0.3000],\n",
       "        [ 0.7750,  0.6500],\n",
       "        [ 0.5500,  0.7000],\n",
       "        [ 0.2500,  0.4000],\n",
       "        [ 0.3500,  0.4750],\n",
       "        [ 0.7250,  0.9000],\n",
       "        [ 0.1500,  0.3000],\n",
       "        [-0.2273,  0.0227],\n",
       "        [ 0.3000, -0.1750],\n",
       "        [ 0.2250,  0.5500],\n",
       "        [ 0.4750,  0.4000],\n",
       "        [ 0.3500,  0.4250],\n",
       "        [ 0.0000, -0.1250]], dtype=torch.float64)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_test_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicted values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3008,  0.3641],\n",
       "        [-0.0238, -0.0134],\n",
       "        [ 0.4611,  0.5686],\n",
       "        [ 0.2185,  0.2551],\n",
       "        [ 0.2321,  0.2726],\n",
       "        [ 0.0035,  0.0173],\n",
       "        [-0.1502, -0.1944],\n",
       "        [ 0.4095,  0.5073],\n",
       "        [-0.2334, -0.3242],\n",
       "        [ 0.3925,  0.4851],\n",
       "        [ 0.4249,  0.5270],\n",
       "        [-0.1980, -0.2688],\n",
       "        [-0.1515, -0.1964],\n",
       "        [-0.2047, -0.2793],\n",
       "        [ 0.0809,  0.0975],\n",
       "        [ 0.0569,  0.0717],\n",
       "        [ 0.4453,  0.5510],\n",
       "        [ 0.1321,  0.1544],\n",
       "        [-0.1258, -0.1565],\n",
       "        [ 0.1646,  0.1901],\n",
       "        [ 0.3431,  0.4206],\n",
       "        [ 0.0212,  0.0353],\n",
       "        [ 0.3805,  0.4695],\n",
       "        [-0.3204, -0.4574],\n",
       "        [ 0.1537,  0.1782],\n",
       "        [ 0.2313,  0.2716],\n",
       "        [ 0.4090,  0.5066],\n",
       "        [-0.1672, -0.2209],\n",
       "        [-0.1567, -0.2045],\n",
       "        [ 0.4174,  0.5176],\n",
       "        [-0.1875, -0.2524],\n",
       "        [ 0.3716,  0.4580],\n",
       "        [ 0.3443,  0.4222],\n",
       "        [-0.0035,  0.0099],\n",
       "        [ 0.4198,  0.5207],\n",
       "        [ 0.4049,  0.5013],\n",
       "        [ 0.4775,  0.5872],\n",
       "        [ 0.4371,  0.5415],\n",
       "        [ 0.2912,  0.3513],\n",
       "        [ 0.3331,  0.4074],\n",
       "        [ 0.5142,  0.6306],\n",
       "        [ 0.5340,  0.6543],\n",
       "        [ 0.4601,  0.5674],\n",
       "        [ 0.2176,  0.2540],\n",
       "        [ 0.4024,  0.4981],\n",
       "        [ 0.4795,  0.5895],\n",
       "        [ 0.3749,  0.4622],\n",
       "        [ 0.2248,  0.2632],\n",
       "        [-0.2495, -0.3489],\n",
       "        [ 0.3235,  0.3945],\n",
       "        [ 0.2133,  0.2484],\n",
       "        [-0.2771, -0.3913],\n",
       "        [ 0.1931,  0.2224],\n",
       "        [ 0.0171,  0.0311],\n",
       "        [ 0.1629,  0.1882],\n",
       "        [ 0.4627,  0.5704],\n",
       "        [ 0.3830,  0.4727],\n",
       "        [ 0.4343,  0.5382],\n",
       "        [ 0.4108,  0.5089],\n",
       "        [ 0.3747,  0.4619],\n",
       "        [ 0.2497,  0.2958],\n",
       "        [ 0.4695,  0.5780],\n",
       "        [ 0.1046,  0.1243],\n",
       "        [ 0.4835,  0.5941],\n",
       "        [ 0.0043,  0.0180],\n",
       "        [ 0.1347,  0.1573],\n",
       "        [-0.1291, -0.1615],\n",
       "        [ 0.4199,  0.5208],\n",
       "        [ 0.3417,  0.4187],\n",
       "        [ 0.2380,  0.2802],\n",
       "        [ 0.3943,  0.4874],\n",
       "        [-0.0581, -0.0584],\n",
       "        [ 0.3942,  0.4874],\n",
       "        [ 0.4978,  0.6107],\n",
       "        [-0.1046, -0.1233],\n",
       "        [ 0.4343,  0.5383],\n",
       "        [-0.0168, -0.0052],\n",
       "        [ 0.5045,  0.6188],\n",
       "        [-0.0962, -0.1107],\n",
       "        [ 0.2273,  0.2664],\n",
       "        [-0.0070,  0.0061],\n",
       "        [-0.2513, -0.3517],\n",
       "        [ 0.3362,  0.4114],\n",
       "        [ 0.5302,  0.6494],\n",
       "        [ 0.2186,  0.2553],\n",
       "        [ 0.2825,  0.3396],\n",
       "        [ 0.2851,  0.3431],\n",
       "        [-0.2294, -0.3180],\n",
       "        [ 0.5339,  0.6545],\n",
       "        [-0.3238, -0.4625],\n",
       "        [ 0.3057,  0.3707],\n",
       "        [ 0.2614,  0.3114],\n",
       "        [ 0.3359,  0.4111],\n",
       "        [-0.0656, -0.0687],\n",
       "        [-0.0324, -0.0238],\n",
       "        [ 0.2590,  0.3082],\n",
       "        [ 0.4043,  0.5005],\n",
       "        [ 0.3933,  0.4861],\n",
       "        [ 0.4211,  0.5224],\n",
       "        [ 0.2695,  0.3223],\n",
       "        [ 0.0404,  0.0549],\n",
       "        [ 0.1496,  0.1736],\n",
       "        [ 0.0730,  0.0883],\n",
       "        [ 0.3232,  0.3941],\n",
       "        [-0.0582, -0.0585],\n",
       "        [ 0.3227,  0.3934],\n",
       "        [ 0.1019,  0.1214],\n",
       "        [-0.0496, -0.0467],\n",
       "        [ 0.4385,  0.5433],\n",
       "        [ 0.5164,  0.6332],\n",
       "        [ 0.4048,  0.5011],\n",
       "        [ 0.5338,  0.6539],\n",
       "        [-0.0567, -0.0565],\n",
       "        [ 0.4788,  0.5887],\n",
       "        [ 0.4363,  0.5406],\n",
       "        [ 0.4701,  0.5787],\n",
       "        [ 0.4048,  0.5012],\n",
       "        [ 0.3706,  0.4566],\n",
       "        [-0.0403, -0.0339],\n",
       "        [ 0.1957,  0.2258],\n",
       "        [-0.0826, -0.0921],\n",
       "        [-0.1610, -0.2113],\n",
       "        [ 0.2521,  0.2990],\n",
       "        [ 0.2652,  0.3165],\n",
       "        [ 0.2314,  0.2717],\n",
       "        [ 0.3604,  0.4433],\n",
       "        [-0.0937, -0.1073],\n",
       "        [ 0.3101,  0.3766],\n",
       "        [-0.0963, -0.1109],\n",
       "        [ 0.1263,  0.1481],\n",
       "        [-0.1287, -0.1609],\n",
       "        [ 0.3161,  0.3846],\n",
       "        [ 0.4178,  0.5180],\n",
       "        [ 0.0125,  0.0265],\n",
       "        [ 0.1749,  0.2013],\n",
       "        [ 0.0223,  0.0365],\n",
       "        [ 0.1432,  0.1666],\n",
       "        [ 0.4836,  0.5943],\n",
       "        [-0.0919, -0.1048],\n",
       "        [ 0.4492,  0.5553],\n",
       "        [ 0.4455,  0.5512],\n",
       "        [-0.1474, -0.1901],\n",
       "        [ 0.3823,  0.4718],\n",
       "        [ 0.4333,  0.5370],\n",
       "        [ 0.3166,  0.3853],\n",
       "        [ 0.3811,  0.4702],\n",
       "        [ 0.4668,  0.5750],\n",
       "        [ 0.1261,  0.1479],\n",
       "        [-0.0016,  0.0121],\n",
       "        [-0.1149, -0.1396],\n",
       "        [ 0.3809,  0.4700],\n",
       "        [ 0.3958,  0.4894],\n",
       "        [ 0.2537,  0.3011],\n",
       "        [-0.1085, -0.1294]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3525, 0.6457], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "pred_valence = test_pred[:, 0]\n",
    "pred_arousal = test_pred[1]\n",
    "real_valence = target_test_labels[0]\n",
    "real_arousal = target_test_labels[1]\n",
    "\n",
    "\n",
    "metric = R2Score(multioutput='raw_values')\n",
    "metric.update(test_pred, target_test_labels)\n",
    "print(metric.compute())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse relationship between epochs and r^2 score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create lists to store the epochs and R^2 scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs_list = [i for i in range(1, 301)]\n",
    "adjusted_r2_scores_valence_list = []\n",
    "adjusted_r2_scores_arousal_list = []\n",
    "r2_scores_list = []\n",
    "rmse_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conduct training and testing for each num_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of epochs: 1\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3635894387033764\n",
      "Valence RMSE: 0.30878921575726337\n",
      "Arousal RMSE: 0.41114934039262035\n",
      "Test R^2 score: tensor([-0.0189, -0.2467], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.1328033603493014\n",
      "Num of epochs: 2\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.36168605157733685\n",
      "Valence RMSE: 0.30806630900139986\n",
      "Arousal RMSE: 0.40832431848894574\n",
      "Test R^2 score: tensor([-0.0142, -0.2296], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.12188413840832579\n",
      "Num of epochs: 3\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.35993151122679096\n",
      "Valence RMSE: 0.3074771234992677\n",
      "Arousal RMSE: 0.40565897509191035\n",
      "Test R^2 score: tensor([-0.0103, -0.2136], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.1119462489897387\n",
      "Num of epochs: 4\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3583003432791008\n",
      "Valence RMSE: 0.3070090997469473\n",
      "Arousal RMSE: 0.40311745764778284\n",
      "Test R^2 score: tensor([-0.0072, -0.1985], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.10282995188911648\n",
      "Num of epochs: 5\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.35675926554298576\n",
      "Valence RMSE: 0.3066412479631257\n",
      "Arousal RMSE: 0.4006563267304762\n",
      "Test R^2 score: tensor([-0.0048, -0.1839], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.09432932274293127\n",
      "Num of epochs: 6\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.35531002477311935\n",
      "Valence RMSE: 0.3063608987859996\n",
      "Arousal RMSE: 0.3982881207161267\n",
      "Test R^2 score: tensor([-0.0030, -0.1699], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.0864341548089188\n",
      "Num of epochs: 7\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3539471794015457\n",
      "Valence RMSE: 0.3061562588742031\n",
      "Arousal RMSE: 0.3960120664383497\n",
      "Test R^2 score: tensor([-0.0016, -0.1566], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.07909795108516182\n",
      "Num of epochs: 8\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.35272476363451855\n",
      "Valence RMSE: 0.3060218719275353\n",
      "Arousal RMSE: 0.3939290947163222\n",
      "Test R^2 score: tensor([-0.0007, -0.1444], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.07259091908658521\n",
      "Num of epochs: 9\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3515794837172643\n",
      "Valence RMSE: 0.30594213265167775\n",
      "Arousal RMSE: 0.39193836021795503\n",
      "Test R^2 score: tensor([-0.0002, -0.1329], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.06656130444032071\n",
      "Num of epochs: 10\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.35050242097988105\n",
      "Valence RMSE: 0.305908897690072\n",
      "Arousal RMSE: 0.3900303072064536\n",
      "Test R^2 score: tensor([ 3.9943e-06, -1.2191e-01], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.06095079742912285\n",
      "Num of epochs: 11\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.349490689388844\n",
      "Valence RMSE: 0.3059144034757717\n",
      "Arousal RMSE: 0.3882059526656458\n",
      "Test R^2 score: tensor([-3.2002e-05, -1.1143e-01], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.055733390327010524\n",
      "Num of epochs: 12\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.34854024124228355\n",
      "Valence RMSE: 0.3059505354905583\n",
      "Arousal RMSE: 0.38646457711347754\n",
      "Test R^2 score: tensor([-0.0003, -0.1015], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.050877131170535494\n",
      "Num of epochs: 13\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3476506663454704\n",
      "Valence RMSE: 0.3060040330027111\n",
      "Arousal RMSE: 0.3848161943148636\n",
      "Test R^2 score: tensor([-0.0006, -0.0921], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.0463639142967428\n",
      "Num of epochs: 14\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3468247674130945\n",
      "Valence RMSE: 0.30606746948550634\n",
      "Arousal RMSE: 0.38327215226915823\n",
      "Test R^2 score: tensor([-0.0010, -0.0834], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.04219816427818501\n",
      "Num of epochs: 15\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.34604739802799817\n",
      "Valence RMSE: 0.30613656037700304\n",
      "Arousal RMSE: 0.38180886548695164\n",
      "Test R^2 score: tensor([-0.0015, -0.0751], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.03829590573941655\n",
      "Num of epochs: 16\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3453149805676492\n",
      "Valence RMSE: 0.30620269336876976\n",
      "Arousal RMSE: 0.380427104952556\n",
      "Test R^2 score: tensor([-0.0019, -0.0673], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.03462851934471678\n",
      "Num of epochs: 17\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3446276812226948\n",
      "Valence RMSE: 0.3062655492768052\n",
      "Arousal RMSE: 0.37912780253634726\n",
      "Test R^2 score: tensor([-0.0023, -0.0601], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.0311950674075544\n",
      "Num of epochs: 18\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.34398828829986317\n",
      "Valence RMSE: 0.3063104517999906\n",
      "Arousal RMSE: 0.37792828961725683\n",
      "Test R^2 score: tensor([-0.0026, -0.0534], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.027993437930322718\n",
      "Num of epochs: 19\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3433812306250344\n",
      "Valence RMSE: 0.30634396070844927\n",
      "Arousal RMSE: 0.37679532484969375\n",
      "Test R^2 score: tensor([-0.0028, -0.0471], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.024950053682860962\n",
      "Num of epochs: 20\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.34280906289342056\n",
      "Valence RMSE: 0.3063654999334855\n",
      "Arousal RMSE: 0.3757343312158683\n",
      "Test R^2 score: tensor([-0.0030, -0.0412], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.02207637545190766\n",
      "Num of epochs: 21\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.34227684157339855\n",
      "Valence RMSE: 0.3063816731687801\n",
      "Arousal RMSE: 0.3747494401613184\n",
      "Test R^2 score: tensor([-0.0031, -0.0357], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.01940374393284605\n",
      "Num of epochs: 22\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.34177315702784156\n",
      "Valence RMSE: 0.30637570562304617\n",
      "Arousal RMSE: 0.3738337982761001\n",
      "Test R^2 score: tensor([-0.0031, -0.0307], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.016856682070718665\n",
      "Num of epochs: 23\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.34135576061476436\n",
      "Valence RMSE: 0.30638395339657243\n",
      "Arousal RMSE: 0.3730635116315392\n",
      "Test R^2 score: tensor([-0.0031, -0.0264], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.014762185820159646\n",
      "Num of epochs: 24\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.340970755081749\n",
      "Valence RMSE: 0.3063803245038268\n",
      "Arousal RMSE: 0.3723616634388749\n",
      "Test R^2 score: tensor([-0.0031, -0.0226], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.012821106659362203\n",
      "Num of epochs: 25\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.34060995148151624\n",
      "Valence RMSE: 0.30636297144105434\n",
      "Arousal RMSE: 0.3717149550748387\n",
      "Test R^2 score: tensor([-0.0030, -0.0190], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.010989877621024768\n",
      "Num of epochs: 26\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.34027100383508435\n",
      "Valence RMSE: 0.3063296056351651\n",
      "Arousal RMSE: 0.3711211187917985\n",
      "Test R^2 score: tensor([-0.0027, -0.0158], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.009254019018275272\n",
      "Num of epochs: 27\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.33995826808992413\n",
      "Valence RMSE: 0.30627690853510775\n",
      "Arousal RMSE: 0.3705910190271463\n",
      "Test R^2 score: tensor([-0.0024, -0.0129], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.007631685170689018\n",
      "Num of epochs: 28\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.33966863146892246\n",
      "Valence RMSE: 0.3062161574005286\n",
      "Arousal RMSE: 0.3701097450146322\n",
      "Test R^2 score: tensor([-0.0020, -0.0102], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.006118361760150681\n",
      "Num of epochs: 29\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3394081549897471\n",
      "Valence RMSE: 0.3061490188529951\n",
      "Arousal RMSE: 0.3696871239337891\n",
      "Test R^2 score: tensor([-0.0016, -0.0079], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.004745789641726117\n",
      "Num of epochs: 30\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3391741253186821\n",
      "Valence RMSE: 0.30607772223451307\n",
      "Arousal RMSE: 0.3693163989360862\n",
      "Test R^2 score: tensor([-0.0011, -0.0059], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.003502321948704923\n",
      "Num of epochs: 31\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.33899545713293977\n",
      "Valence RMSE: 0.3060476498401832\n",
      "Arousal RMSE: 0.3690131107980341\n",
      "Test R^2 score: tensor([-0.0009, -0.0043], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.0025782431903169645\n",
      "Num of epochs: 32\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3387997114399721\n",
      "Valence RMSE: 0.305914505214846\n",
      "Arousal RMSE: 0.36876388711853647\n",
      "Test R^2 score: tensor([-3.2667e-05, -2.8971e-03], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.001464877131631459\n",
      "Num of epochs: 33\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3386450603756927\n",
      "Valence RMSE: 0.30581554281221474\n",
      "Arousal RMSE: 0.3685618097526974\n",
      "Test R^2 score: tensor([ 0.0006, -0.0018], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.0005919992008543984\n",
      "Num of epochs: 34\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.33852182219855054\n",
      "Valence RMSE: 0.30572452055940924\n",
      "Arousal RMSE: 0.3684108653907665\n",
      "Test R^2 score: tensor([ 0.0012, -0.0010], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.00011561357685307971\n",
      "Num of epochs: 35\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.33842393282840655\n",
      "Valence RMSE: 0.30562045799100274\n",
      "Arousal RMSE: 0.36831732552170465\n",
      "Test R^2 score: tensor([ 0.0019, -0.0005], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.0007096413198814933\n",
      "Num of epochs: 36\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3383809135438806\n",
      "Valence RMSE: 0.3055846875866695\n",
      "Arousal RMSE: 0.3682679513807033\n",
      "Test R^2 score: tensor([ 0.0021, -0.0002], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.000960562551866273\n",
      "Num of epochs: 37\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3383447664107183\n",
      "Valence RMSE: 0.3055209862841762\n",
      "Arousal RMSE: 0.36825438063245264\n",
      "Test R^2 score: tensor([ 0.0025, -0.0001], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.0012054124768058339\n",
      "Num of epochs: 38\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.33830295198921634\n",
      "Valence RMSE: 0.30543314915172304\n",
      "Arousal RMSE: 0.36825041214979964\n",
      "Test R^2 score: tensor([ 0.0031, -0.0001], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.001502918730502678\n",
      "Num of epochs: 39\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3382761575279098\n",
      "Valence RMSE: 0.3053664640767187\n",
      "Arousal RMSE: 0.36825648686883516\n",
      "Test R^2 score: tensor([ 0.0035, -0.0001], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.0017040470155705312\n",
      "Num of epochs: 40\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3382517771426295\n",
      "Valence RMSE: 0.3053040486128824\n",
      "Arousal RMSE: 0.3682634483367538\n",
      "Test R^2 score: tensor([ 0.0040, -0.0002], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.0018887897823857136\n",
      "Num of epochs: 41\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3382164695828674\n",
      "Valence RMSE: 0.3052247311906432\n",
      "Arousal RMSE: 0.3682643399296176\n",
      "Test R^2 score: tensor([ 0.0045, -0.0002], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.0021451054240041456\n",
      "Num of epochs: 42\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3381640275159628\n",
      "Valence RMSE: 0.3051207781259583\n",
      "Arousal RMSE: 0.3682541646301416\n",
      "Test R^2 score: tensor([ 0.0051, -0.0001], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.0025117384614638527\n",
      "Num of epochs: 43\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3380793476442049\n",
      "Valence RMSE: 0.3049626018255209\n",
      "Arousal RMSE: 0.36822968660181526\n",
      "Test R^2 score: tensor([6.1812e-03, 6.4517e-06], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.0030938172113049167\n",
      "Num of epochs: 44\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.33796185447163496\n",
      "Valence RMSE: 0.30474258176833047\n",
      "Arousal RMSE: 0.3681961284600716\n",
      "Test R^2 score: tensor([0.0076, 0.0002], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.0039016937338784974\n",
      "Num of epochs: 45\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.33780567695465546\n",
      "Valence RMSE: 0.30445149458336\n",
      "Arousal RMSE: 0.36815029296680507\n",
      "Test R^2 score: tensor([0.0095, 0.0004], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.004973613320293058\n",
      "Num of epochs: 46\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3376318264845271\n",
      "Valence RMSE: 0.30412235189263953\n",
      "Arousal RMSE: 0.36810337622717504\n",
      "Test R^2 score: tensor([0.0117, 0.0007], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.006171229445223747\n",
      "Num of epochs: 47\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3374424195926585\n",
      "Valence RMSE: 0.30377209832531976\n",
      "Arousal RMSE: 0.3680452218955273\n",
      "Test R^2 score: tensor([0.0139, 0.0010], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.007466704943781877\n",
      "Num of epochs: 48\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.33721619901784955\n",
      "Valence RMSE: 0.3033695914129241\n",
      "Arousal RMSE: 0.36796252630675225\n",
      "Test R^2 score: tensor([0.0165, 0.0015], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.008996854044624691\n",
      "Num of epochs: 49\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3369394094448587\n",
      "Valence RMSE: 0.30289975166188343\n",
      "Arousal RMSE: 0.36784245502289364\n",
      "Test R^2 score: tensor([0.0196, 0.0021], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.010844585952753005\n",
      "Num of epochs: 50\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3365778800790619\n",
      "Valence RMSE: 0.30228906121324073\n",
      "Arousal RMSE: 0.36768282824718423\n",
      "Test R^2 score: tensor([0.0235, 0.0030], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.013252208188640646\n",
      "Num of epochs: 51\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.33613236168641863\n",
      "Valence RMSE: 0.3015156024710829\n",
      "Arousal RMSE: 0.36750274912206937\n",
      "Test R^2 score: tensor([0.0285, 0.0040], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.016235670099825517\n",
      "Num of epochs: 52\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3355882968841079\n",
      "Valence RMSE: 0.30052652519220996\n",
      "Arousal RMSE: 0.3673184145493501\n",
      "Test R^2 score: tensor([0.0349, 0.0049], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.019916717673458484\n",
      "Num of epochs: 53\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.33496409248512493\n",
      "Valence RMSE: 0.2993750380472291\n",
      "Arousal RMSE: 0.36711915382200744\n",
      "Test R^2 score: tensor([0.0423, 0.0060], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.024147182568445724\n",
      "Num of epochs: 54\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.33424160216634335\n",
      "Valence RMSE: 0.298103303915921\n",
      "Arousal RMSE: 0.3668369084918533\n",
      "Test R^2 score: tensor([0.0504, 0.0076], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.028970845274092327\n",
      "Num of epochs: 55\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3333969770514699\n",
      "Valence RMSE: 0.29673619853630334\n",
      "Arousal RMSE: 0.3664078562099075\n",
      "Test R^2 score: tensor([0.0591, 0.0099], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.034475888181121184\n",
      "Num of epochs: 56\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.33237419160931797\n",
      "Valence RMSE: 0.29521307567797\n",
      "Arousal RMSE: 0.36577923183888994\n",
      "Test R^2 score: tensor([0.0687, 0.0133], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.04099042435794825\n",
      "Num of epochs: 57\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3311945185007426\n",
      "Valence RMSE: 0.293515388111187\n",
      "Arousal RMSE: 0.3650045686177321\n",
      "Test R^2 score: tensor([0.0794, 0.0174], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.048418140019351574\n",
      "Num of epochs: 58\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.32977787855493235\n",
      "Valence RMSE: 0.29144687151609205\n",
      "Arousal RMSE: 0.3640956185562605\n",
      "Test R^2 score: tensor([0.0923, 0.0223], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.057326925795136985\n",
      "Num of epochs: 59\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3279865051812932\n",
      "Valence RMSE: 0.28882012139179636\n",
      "Arousal RMSE: 0.36295073032204384\n",
      "Test R^2 score: tensor([0.1086, 0.0285], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.068540202181961\n",
      "Num of epochs: 60\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.325849429270875\n",
      "Valence RMSE: 0.28568052574006964\n",
      "Arousal RMSE: 0.36158310016535244\n",
      "Test R^2 score: tensor([0.1279, 0.0358], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.08183124734829328\n",
      "Num of epochs: 61\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3233234227922019\n",
      "Valence RMSE: 0.2820985129847264\n",
      "Arousal RMSE: 0.3598562218774826\n",
      "Test R^2 score: tensor([0.1496, 0.0450], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.09729177025228503\n",
      "Num of epochs: 62\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3204906415568314\n",
      "Valence RMSE: 0.2781845822147302\n",
      "Arousal RMSE: 0.3578293460143184\n",
      "Test R^2 score: tensor([0.1730, 0.0557], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.11437248124435812\n",
      "Num of epochs: 63\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.31741931764396164\n",
      "Valence RMSE: 0.2741173852984436\n",
      "Arousal RMSE: 0.35548516917624157\n",
      "Test R^2 score: tensor([0.1971, 0.0680], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.13254050732087852\n",
      "Num of epochs: 64\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3139178943979228\n",
      "Valence RMSE: 0.2696985565404349\n",
      "Arousal RMSE: 0.352635190312108\n",
      "Test R^2 score: tensor([0.2227, 0.0829], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.1528216665599772\n",
      "Num of epochs: 65\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3101467236949533\n",
      "Valence RMSE: 0.26518655275048714\n",
      "Arousal RMSE: 0.3493681048374916\n",
      "Test R^2 score: tensor([0.2485, 0.0998], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.17417370052095132\n",
      "Num of epochs: 66\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.306077094209825\n",
      "Valence RMSE: 0.26048474349323997\n",
      "Arousal RMSE: 0.3457080757042293\n",
      "Test R^2 score: tensor([0.2749, 0.1186], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.1967604023622267\n",
      "Num of epochs: 67\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.30190110310261536\n",
      "Valence RMSE: 0.2558215061262033\n",
      "Arousal RMSE: 0.34182438343756716\n",
      "Test R^2 score: tensor([0.3007, 0.1383], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.2194706656208017\n",
      "Num of epochs: 68\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.29752209142925007\n",
      "Valence RMSE: 0.25145427084742406\n",
      "Arousal RMSE: 0.33735669468599605\n",
      "Test R^2 score: tensor([0.3243, 0.1607], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.24249665375310298\n",
      "Num of epochs: 69\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2933098785996361\n",
      "Valence RMSE: 0.24793980574805388\n",
      "Arousal RMSE: 0.3325465719172035\n",
      "Test R^2 score: tensor([0.3431, 0.1844], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.26375637259795215\n",
      "Num of epochs: 70\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.28937863317202667\n",
      "Valence RMSE: 0.24567674823742267\n",
      "Arousal RMSE: 0.32729638257778954\n",
      "Test R^2 score: tensor([0.3550, 0.2100], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.28249946638130163\n",
      "Num of epochs: 71\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2864693606609704\n",
      "Valence RMSE: 0.24562805113566422\n",
      "Arousal RMSE: 0.32217425361177726\n",
      "Test R^2 score: tensor([0.3553, 0.2345], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.2948943338410848\n",
      "Num of epochs: 72\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2851987341821135\n",
      "Valence RMSE: 0.24831386799076796\n",
      "Arousal RMSE: 0.3178314945401464\n",
      "Test R^2 score: tensor([0.3411, 0.2550], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.29805507410678483\n",
      "Num of epochs: 73\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2853046525481005\n",
      "Valence RMSE: 0.2531271573134674\n",
      "Arousal RMSE: 0.3142039652225729\n",
      "Test R^2 score: tensor([0.3153, 0.2719], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.2936137298179519\n",
      "Num of epochs: 74\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2858125948620961\n",
      "Valence RMSE: 0.2583351185938712\n",
      "Arousal RMSE: 0.3108707854795941\n",
      "Test R^2 score: tensor([0.2868, 0.2873], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.2870645607783376\n",
      "Num of epochs: 75\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.28591458127771613\n",
      "Valence RMSE: 0.2625288429878732\n",
      "Arousal RMSE: 0.30752707551347747\n",
      "Test R^2 score: tensor([0.2635, 0.3025], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.28301831609945094\n",
      "Num of epochs: 76\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2852604246039877\n",
      "Valence RMSE: 0.2646432563378871\n",
      "Arousal RMSE: 0.3044847558834006\n",
      "Test R^2 score: tensor([0.2516, 0.3163], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.2839285538347098\n",
      "Num of epochs: 77\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.28300288034571086\n",
      "Valence RMSE: 0.26365988150035885\n",
      "Arousal RMSE: 0.3011058409515734\n",
      "Test R^2 score: tensor([0.2571, 0.3314], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.29424980670260636\n",
      "Num of epochs: 78\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2786593697351068\n",
      "Valence RMSE: 0.260027445887446\n",
      "Arousal RMSE: 0.2961212860764746\n",
      "Test R^2 score: tensor([0.2775, 0.3533], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.31539085776065984\n",
      "Num of epochs: 79\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2730640579112148\n",
      "Valence RMSE: 0.25535396146037653\n",
      "Arousal RMSE: 0.28969348251621235\n",
      "Test R^2 score: tensor([0.3032, 0.3811], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.34214533054356844\n",
      "Num of epochs: 80\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.26722863348403114\n",
      "Valence RMSE: 0.2499949838152139\n",
      "Arousal RMSE: 0.2834162895366395\n",
      "Test R^2 score: tensor([0.3322, 0.4076], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.3698807416929583\n",
      "Num of epochs: 81\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.26196636335538714\n",
      "Valence RMSE: 0.24529171320820342\n",
      "Arousal RMSE: 0.2776413630759616\n",
      "Test R^2 score: tensor([0.3570, 0.4315], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.3942747328772088\n",
      "Num of epochs: 82\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.25824026558621155\n",
      "Valence RMSE: 0.24389862388611136\n",
      "Arousal RMSE: 0.27182628792398894\n",
      "Test R^2 score: tensor([0.3643, 0.4551], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.40969812062557626\n",
      "Num of epochs: 83\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.25591746848754265\n",
      "Valence RMSE: 0.24525231106624512\n",
      "Arousal RMSE: 0.26615560349316436\n",
      "Test R^2 score: tensor([0.3573, 0.4776], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4174097106965105\n",
      "Num of epochs: 84\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.25362073011846015\n",
      "Valence RMSE: 0.24591025569697536\n",
      "Arousal RMSE: 0.26110361091851975\n",
      "Test R^2 score: tensor([0.3538, 0.4972], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.42550545765350556\n",
      "Num of epochs: 85\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2519147492873474\n",
      "Valence RMSE: 0.2472777726350997\n",
      "Arousal RMSE: 0.2564679024315483\n",
      "Test R^2 score: tensor([0.3466, 0.5149], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.43074932679665073\n",
      "Num of epochs: 86\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.25175678942853924\n",
      "Valence RMSE: 0.2512393258215662\n",
      "Arousal RMSE: 0.2522731916146791\n",
      "Test R^2 score: tensor([0.3255, 0.5306], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.42806661406006613\n",
      "Num of epochs: 87\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2521977245724924\n",
      "Valence RMSE: 0.25540442100542365\n",
      "Arousal RMSE: 0.2489497264307992\n",
      "Test R^2 score: tensor([0.3029, 0.5429], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4229343326537269\n",
      "Num of epochs: 88\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2521123186851091\n",
      "Valence RMSE: 0.2579233394824348\n",
      "Arousal RMSE: 0.2461641594866985\n",
      "Test R^2 score: tensor([0.2891, 0.5531], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4211113586500389\n",
      "Num of epochs: 89\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2518925224942791\n",
      "Valence RMSE: 0.2597367588051506\n",
      "Arousal RMSE: 0.2437960251982318\n",
      "Test R^2 score: tensor([0.2791, 0.5617], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4203742625299141\n",
      "Num of epochs: 90\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.25025735279168926\n",
      "Valence RMSE: 0.25888184683257287\n",
      "Arousal RMSE: 0.24132483219338266\n",
      "Test R^2 score: tensor([0.2838, 0.5705], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.427163850597947\n",
      "Num of epochs: 91\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.24698584599452678\n",
      "Valence RMSE: 0.25500287452445114\n",
      "Arousal RMSE: 0.23869970722129344\n",
      "Test R^2 score: tensor([0.3051, 0.5798], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4424609461012539\n",
      "Num of epochs: 92\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.24438377222479932\n",
      "Valence RMSE: 0.251669057092469\n",
      "Arousal RMSE: 0.2368745278746382\n",
      "Test R^2 score: tensor([0.3232, 0.5862], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4546868230413155\n",
      "Num of epochs: 93\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.24226801135666937\n",
      "Valence RMSE: 0.2489520300288672\n",
      "Arousal RMSE: 0.23539427647658845\n",
      "Test R^2 score: tensor([0.3377, 0.5914], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.46453218884638997\n",
      "Num of epochs: 94\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2402292481804058\n",
      "Valence RMSE: 0.24660264018913425\n",
      "Arousal RMSE: 0.23368209433842896\n",
      "Test R^2 score: tensor([0.3502, 0.5973], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.47371434408332197\n",
      "Num of epochs: 95\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23905386274399132\n",
      "Valence RMSE: 0.24572858935871475\n",
      "Arousal RMSE: 0.23218733591094548\n",
      "Test R^2 score: tensor([0.3548, 0.6024], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.47858137285900765\n",
      "Num of epochs: 96\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23842228401773374\n",
      "Valence RMSE: 0.2454979210977711\n",
      "Arousal RMSE: 0.23113014033037388\n",
      "Test R^2 score: tensor([0.3560, 0.6060], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4809929793774264\n",
      "Num of epochs: 97\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23729629303098276\n",
      "Valence RMSE: 0.2444013428001763\n",
      "Arousal RMSE: 0.22997183525371703\n",
      "Test R^2 score: tensor([0.3617, 0.6100], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.48583277129659835\n",
      "Num of epochs: 98\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23767634099372695\n",
      "Valence RMSE: 0.24542748226504368\n",
      "Arousal RMSE: 0.22966374786930213\n",
      "Test R^2 score: tensor([0.3563, 0.6110], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.48366938809333526\n",
      "Num of epochs: 99\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2369988462701729\n",
      "Valence RMSE: 0.24494903650809843\n",
      "Arousal RMSE: 0.22877254157905477\n",
      "Test R^2 score: tensor([0.3588, 0.6140], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.48642951324939837\n",
      "Num of epochs: 100\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23603631480954865\n",
      "Valence RMSE: 0.2441629804350017\n",
      "Arousal RMSE: 0.22761968896130602\n",
      "Test R^2 score: tensor([0.3629, 0.6179], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.49042390660401664\n",
      "Num of epochs: 101\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23610953690814115\n",
      "Valence RMSE: 0.2447873810061177\n",
      "Arousal RMSE: 0.2271003411228603\n",
      "Test R^2 score: tensor([0.3597, 0.6196], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4896635154600743\n",
      "Num of epochs: 102\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23403078329648624\n",
      "Valence RMSE: 0.24277922183516387\n",
      "Arousal RMSE: 0.22494235818503835\n",
      "Test R^2 score: tensor([0.3702, 0.6268], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.49849203162531114\n",
      "Num of epochs: 103\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2346881950872999\n",
      "Valence RMSE: 0.24386041587069623\n",
      "Arousal RMSE: 0.22514261124460683\n",
      "Test R^2 score: tensor([0.3645, 0.6262], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.49534845362089\n",
      "Num of epochs: 104\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2329043355494024\n",
      "Valence RMSE: 0.24241669015580145\n",
      "Arousal RMSE: 0.2229865632035334\n",
      "Test R^2 score: tensor([0.3720, 0.6333], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.5026622996247099\n",
      "Num of epochs: 105\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2331705132608728\n",
      "Valence RMSE: 0.24320180844679973\n",
      "Arousal RMSE: 0.22268780136523836\n",
      "Test R^2 score: tensor([0.3680, 0.6343], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.5011161798515893\n",
      "Num of epochs: 106\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23321062785406707\n",
      "Valence RMSE: 0.24380943005135478\n",
      "Arousal RMSE: 0.22210663138729078\n",
      "Test R^2 score: tensor([0.3648, 0.6362], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.5004883088736554\n",
      "Num of epochs: 107\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23252425904294668\n",
      "Valence RMSE: 0.2434356884082296\n",
      "Arousal RMSE: 0.22107493683399318\n",
      "Test R^2 score: tensor([0.3667, 0.6396], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.5031473057942821\n",
      "Num of epochs: 108\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2341387319611625\n",
      "Valence RMSE: 0.2459656772600443\n",
      "Arousal RMSE: 0.22168170248978586\n",
      "Test R^2 score: tensor([0.3535, 0.6376], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.49554108757377263\n",
      "Num of epochs: 109\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23286924766075642\n",
      "Valence RMSE: 0.24455483585051993\n",
      "Arousal RMSE: 0.22056542175576543\n",
      "Test R^2 score: tensor([0.3609, 0.6412], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.5010590884151025\n",
      "Num of epochs: 110\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23346442974819376\n",
      "Valence RMSE: 0.2458581257485658\n",
      "Arousal RMSE: 0.22037482142636214\n",
      "Test R^2 score: tensor([0.3541, 0.6418], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.497954025555648\n",
      "Num of epochs: 111\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23345941816968202\n",
      "Valence RMSE: 0.24611084269195022\n",
      "Arousal RMSE: 0.22008192332336393\n",
      "Test R^2 score: tensor([0.3527, 0.6428], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4977654558229185\n",
      "Num of epochs: 112\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.232802550570393\n",
      "Valence RMSE: 0.24510429693168015\n",
      "Arousal RMSE: 0.21981341799305054\n",
      "Test R^2 score: tensor([0.3580, 0.6437], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.5008427350857324\n",
      "Num of epochs: 113\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23358700315741213\n",
      "Valence RMSE: 0.24659484972062087\n",
      "Arousal RMSE: 0.21981072808074248\n",
      "Test R^2 score: tensor([0.3502, 0.6437], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.49693120225519744\n",
      "Num of epochs: 114\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23275135635366395\n",
      "Valence RMSE: 0.24535344493162664\n",
      "Arousal RMSE: 0.21942669579867394\n",
      "Test R^2 score: tensor([0.3567, 0.6449], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.5008162101631923\n",
      "Num of epochs: 115\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23305582429650756\n",
      "Valence RMSE: 0.24615387501596028\n",
      "Arousal RMSE: 0.21917642275499483\n",
      "Test R^2 score: tensor([0.3525, 0.6457], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.49911896385261895\n",
      "Num of epochs: 116\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23313690459824907\n",
      "Valence RMSE: 0.24657458960027467\n",
      "Arousal RMSE: 0.21887577375023096\n",
      "Test R^2 score: tensor([0.3503, 0.6467], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.49849701410994796\n",
      "Num of epochs: 117\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23289070228276282\n",
      "Valence RMSE: 0.24638891966507778\n",
      "Arousal RMSE: 0.2185604234206019\n",
      "Test R^2 score: tensor([0.3513, 0.6477], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.49949472137863427\n",
      "Num of epochs: 118\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23330289147751257\n",
      "Valence RMSE: 0.24752638311993996\n",
      "Arousal RMSE: 0.21815400065801385\n",
      "Test R^2 score: tensor([0.3453, 0.6490], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.49714746830760814\n",
      "Num of epochs: 119\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23253495007370553\n",
      "Valence RMSE: 0.2463041705133086\n",
      "Arousal RMSE: 0.21789736482874783\n",
      "Test R^2 score: tensor([0.3517, 0.6498], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.5007849652866242\n",
      "Num of epochs: 120\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23353510015185122\n",
      "Valence RMSE: 0.24811608401033522\n",
      "Arousal RMSE: 0.21798095068433535\n",
      "Test R^2 score: tensor([0.3422, 0.6496], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4958641216657269\n",
      "Num of epochs: 121\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Epoch 121, Loss: 0.4596922103619757\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23204713412828581\n",
      "Valence RMSE: 0.2451375792948869\n",
      "Arousal RMSE: 0.21817266586752884\n",
      "Test R^2 score: tensor([0.3579, 0.6490], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.5034054775368342\n",
      "Num of epochs: 122\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Epoch 121, Loss: 0.4596922103619757\n",
      "Epoch 122, Loss: 0.45943389956138125\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2334946958919583\n",
      "Valence RMSE: 0.24780428410641445\n",
      "Arousal RMSE: 0.21824890102326652\n",
      "Test R^2 score: tensor([0.3438, 0.6487], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4962592746618819\n",
      "Num of epochs: 123\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Epoch 121, Loss: 0.4596922103619757\n",
      "Epoch 122, Loss: 0.45943389956138125\n",
      "Epoch 123, Loss: 0.45877818894117006\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23159849983416872\n",
      "Valence RMSE: 0.24491708530036707\n",
      "Arousal RMSE: 0.21746574805897043\n",
      "Test R^2 score: tensor([0.3590, 0.6512], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.5051184073594553\n",
      "Num of epochs: 124\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Epoch 121, Loss: 0.4596922103619757\n",
      "Epoch 122, Loss: 0.45943389956138125\n",
      "Epoch 123, Loss: 0.45877818894117006\n",
      "Epoch 124, Loss: 0.45746538132874076\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23137344574783747\n",
      "Valence RMSE: 0.2443236930026239\n",
      "Arousal RMSE: 0.2176540278331981\n",
      "Test R^2 score: tensor([0.3621, 0.6506], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.5063674448158497\n",
      "Num of epochs: 125\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Epoch 121, Loss: 0.4596922103619757\n",
      "Epoch 122, Loss: 0.45943389956138125\n",
      "Epoch 123, Loss: 0.45877818894117006\n",
      "Epoch 124, Loss: 0.45746538132874076\n",
      "Epoch 125, Loss: 0.4570462803121619\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2328446458195541\n",
      "Valence RMSE: 0.24658520017973334\n",
      "Arousal RMSE: 0.21824068645875366\n",
      "Test R^2 score: tensor([0.3502, 0.6487], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4994927263085122\n",
      "Num of epochs: 126\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Epoch 121, Loss: 0.4596922103619757\n",
      "Epoch 122, Loss: 0.45943389956138125\n",
      "Epoch 123, Loss: 0.45877818894117006\n",
      "Epoch 124, Loss: 0.45746538132874076\n",
      "Epoch 125, Loss: 0.4570462803121619\n",
      "Epoch 126, Loss: 0.45696189549769234\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2313020158721768\n",
      "Valence RMSE: 0.24420265825347615\n",
      "Arousal RMSE: 0.21763801780709502\n",
      "Test R^2 score: tensor([0.3627, 0.6507], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.5067090666334924\n",
      "Num of epochs: 127\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Epoch 121, Loss: 0.4596922103619757\n",
      "Epoch 122, Loss: 0.45943389956138125\n",
      "Epoch 123, Loss: 0.45877818894117006\n",
      "Epoch 124, Loss: 0.45746538132874076\n",
      "Epoch 125, Loss: 0.4570462803121619\n",
      "Epoch 126, Loss: 0.45696189549769234\n",
      "Epoch 127, Loss: 0.45594920341421274\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23136488826619417\n",
      "Valence RMSE: 0.2444646592705676\n",
      "Arousal RMSE: 0.2174774779433087\n",
      "Test R^2 score: tensor([0.3614, 0.6512], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.5062825798936958\n",
      "Num of epochs: 128\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Epoch 121, Loss: 0.4596922103619757\n",
      "Epoch 122, Loss: 0.45943389956138125\n",
      "Epoch 123, Loss: 0.45877818894117006\n",
      "Epoch 124, Loss: 0.45746538132874076\n",
      "Epoch 125, Loss: 0.4570462803121619\n",
      "Epoch 126, Loss: 0.45696189549769234\n",
      "Epoch 127, Loss: 0.45594920341421274\n",
      "Epoch 128, Loss: 0.45536359747685007\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23263045549307143\n",
      "Valence RMSE: 0.24641972447284732\n",
      "Arousal RMSE: 0.21797058754922372\n",
      "Test R^2 score: tensor([0.3511, 0.6496], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.5003630684541831\n",
      "Num of epochs: 129\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Epoch 121, Loss: 0.4596922103619757\n",
      "Epoch 122, Loss: 0.45943389956138125\n",
      "Epoch 123, Loss: 0.45877818894117006\n",
      "Epoch 124, Loss: 0.45746538132874076\n",
      "Epoch 125, Loss: 0.4570462803121619\n",
      "Epoch 126, Loss: 0.45696189549769234\n",
      "Epoch 127, Loss: 0.45594920341421274\n",
      "Epoch 128, Loss: 0.45536359747685007\n",
      "Epoch 129, Loss: 0.4553290890752543\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23150258213930702\n",
      "Valence RMSE: 0.24454876694821687\n",
      "Arousal RMSE: 0.21767588671793642\n",
      "Test R^2 score: tensor([0.3609, 0.6506], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.5057444535227262\n",
      "Num of epochs: 130\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Epoch 121, Loss: 0.4596922103619757\n",
      "Epoch 122, Loss: 0.45943389956138125\n",
      "Epoch 123, Loss: 0.45877818894117006\n",
      "Epoch 124, Loss: 0.45746538132874076\n",
      "Epoch 125, Loss: 0.4570462803121619\n",
      "Epoch 126, Loss: 0.45696189549769234\n",
      "Epoch 127, Loss: 0.45594920341421274\n",
      "Epoch 128, Loss: 0.45536359747685007\n",
      "Epoch 129, Loss: 0.4553290890752543\n",
      "Epoch 130, Loss: 0.4547195823074053\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23177721399443038\n",
      "Valence RMSE: 0.24535489123606394\n",
      "Arousal RMSE: 0.21735301056249295\n",
      "Test R^2 score: tensor([0.3567, 0.6516], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.5041523291677217\n",
      "Num of epochs: 131\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Epoch 121, Loss: 0.4596922103619757\n",
      "Epoch 122, Loss: 0.45943389956138125\n",
      "Epoch 123, Loss: 0.45877818894117006\n",
      "Epoch 124, Loss: 0.45746538132874076\n",
      "Epoch 125, Loss: 0.4570462803121619\n",
      "Epoch 126, Loss: 0.45696189549769234\n",
      "Epoch 127, Loss: 0.45594920341421274\n",
      "Epoch 128, Loss: 0.45536359747685007\n",
      "Epoch 129, Loss: 0.4553290890752543\n",
      "Epoch 130, Loss: 0.4547195823074053\n",
      "Epoch 131, Loss: 0.45392838323998064\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23251740447345332\n",
      "Valence RMSE: 0.24636204826469507\n",
      "Arousal RMSE: 0.2177944626040043\n",
      "Test R^2 score: tensor([0.3514, 0.6502], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.5007979363649164\n",
      "Num of epochs: 132\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Epoch 121, Loss: 0.4596922103619757\n",
      "Epoch 122, Loss: 0.45943389956138125\n",
      "Epoch 123, Loss: 0.45877818894117006\n",
      "Epoch 124, Loss: 0.45746538132874076\n",
      "Epoch 125, Loss: 0.4570462803121619\n",
      "Epoch 126, Loss: 0.45696189549769234\n",
      "Epoch 127, Loss: 0.45594920341421274\n",
      "Epoch 128, Loss: 0.45536359747685007\n",
      "Epoch 129, Loss: 0.4553290890752543\n",
      "Epoch 130, Loss: 0.4547195823074053\n",
      "Epoch 131, Loss: 0.45392838323998064\n",
      "Epoch 132, Loss: 0.4537853493595631\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23178726151851226\n",
      "Valence RMSE: 0.24484103684729017\n",
      "Arousal RMSE: 0.21795305889123529\n",
      "Test R^2 score: tensor([0.3594, 0.6497], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.5045349834920623\n",
      "Num of epochs: 133\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Epoch 121, Loss: 0.4596922103619757\n",
      "Epoch 122, Loss: 0.45943389956138125\n",
      "Epoch 123, Loss: 0.45877818894117006\n",
      "Epoch 124, Loss: 0.45746538132874076\n",
      "Epoch 125, Loss: 0.4570462803121619\n",
      "Epoch 126, Loss: 0.45696189549769234\n",
      "Epoch 127, Loss: 0.45594920341421274\n",
      "Epoch 128, Loss: 0.45536359747685007\n",
      "Epoch 129, Loss: 0.4553290890752543\n",
      "Epoch 130, Loss: 0.4547195823074053\n",
      "Epoch 131, Loss: 0.45392838323998064\n",
      "Epoch 132, Loss: 0.4537853493595631\n",
      "Epoch 133, Loss: 0.45347437050767975\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23220740748478777\n",
      "Valence RMSE: 0.24587083938087745\n",
      "Arousal RMSE: 0.21768805783449693\n",
      "Test R^2 score: tensor([0.3540, 0.6505], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.5022606808187513\n",
      "Num of epochs: 134\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Epoch 121, Loss: 0.4596922103619757\n",
      "Epoch 122, Loss: 0.45943389956138125\n",
      "Epoch 123, Loss: 0.45877818894117006\n",
      "Epoch 124, Loss: 0.45746538132874076\n",
      "Epoch 125, Loss: 0.4570462803121619\n",
      "Epoch 126, Loss: 0.45696189549769234\n",
      "Epoch 127, Loss: 0.45594920341421274\n",
      "Epoch 128, Loss: 0.45536359747685007\n",
      "Epoch 129, Loss: 0.4553290890752543\n",
      "Epoch 130, Loss: 0.4547195823074053\n",
      "Epoch 131, Loss: 0.45392838323998064\n",
      "Epoch 132, Loss: 0.4537853493595631\n",
      "Epoch 133, Loss: 0.45347437050767975\n",
      "Epoch 134, Loss: 0.45267431273812453\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2323131744407242\n",
      "Valence RMSE: 0.2460223507696824\n",
      "Arousal RMSE: 0.2177425657955101\n",
      "Test R^2 score: tensor([0.3532, 0.6503], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.5017749615986443\n",
      "Num of epochs: 135\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Epoch 121, Loss: 0.4596922103619757\n",
      "Epoch 122, Loss: 0.45943389956138125\n",
      "Epoch 123, Loss: 0.45877818894117006\n",
      "Epoch 124, Loss: 0.45746538132874076\n",
      "Epoch 125, Loss: 0.4570462803121619\n",
      "Epoch 126, Loss: 0.45696189549769234\n",
      "Epoch 127, Loss: 0.45594920341421274\n",
      "Epoch 128, Loss: 0.45536359747685007\n",
      "Epoch 129, Loss: 0.4553290890752543\n",
      "Epoch 130, Loss: 0.4547195823074053\n",
      "Epoch 131, Loss: 0.45392838323998064\n",
      "Epoch 132, Loss: 0.4537853493595631\n",
      "Epoch 133, Loss: 0.45347437050767975\n",
      "Epoch 134, Loss: 0.45267431273812453\n",
      "Epoch 135, Loss: 0.45228737659373064\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2320502108127064\n",
      "Valence RMSE: 0.24526049896317387\n",
      "Arousal RMSE: 0.21804102440774298\n",
      "Test R^2 score: tensor([0.3572, 0.6494], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.503295153738409\n",
      "Num of epochs: 136\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Epoch 121, Loss: 0.4596922103619757\n",
      "Epoch 122, Loss: 0.45943389956138125\n",
      "Epoch 123, Loss: 0.45877818894117006\n",
      "Epoch 124, Loss: 0.45746538132874076\n",
      "Epoch 125, Loss: 0.4570462803121619\n",
      "Epoch 126, Loss: 0.45696189549769234\n",
      "Epoch 127, Loss: 0.45594920341421274\n",
      "Epoch 128, Loss: 0.45536359747685007\n",
      "Epoch 129, Loss: 0.4553290890752543\n",
      "Epoch 130, Loss: 0.4547195823074053\n",
      "Epoch 131, Loss: 0.45392838323998064\n",
      "Epoch 132, Loss: 0.4537853493595631\n",
      "Epoch 133, Loss: 0.45347437050767975\n",
      "Epoch 134, Loss: 0.45267431273812453\n",
      "Epoch 135, Loss: 0.45228737659373064\n",
      "Epoch 136, Loss: 0.4522154657432981\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2328455939080044\n",
      "Valence RMSE: 0.2468564878624286\n",
      "Arousal RMSE: 0.21793580615623773\n",
      "Test R^2 score: tensor([0.3488, 0.6497], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4992678563128201\n",
      "Num of epochs: 137\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Epoch 121, Loss: 0.4596922103619757\n",
      "Epoch 122, Loss: 0.45943389956138125\n",
      "Epoch 123, Loss: 0.45877818894117006\n",
      "Epoch 124, Loss: 0.45746538132874076\n",
      "Epoch 125, Loss: 0.4570462803121619\n",
      "Epoch 126, Loss: 0.45696189549769234\n",
      "Epoch 127, Loss: 0.45594920341421274\n",
      "Epoch 128, Loss: 0.45536359747685007\n",
      "Epoch 129, Loss: 0.4553290890752543\n",
      "Epoch 130, Loss: 0.4547195823074053\n",
      "Epoch 131, Loss: 0.45392838323998064\n",
      "Epoch 132, Loss: 0.4537853493595631\n",
      "Epoch 133, Loss: 0.45347437050767975\n",
      "Epoch 134, Loss: 0.45267431273812453\n",
      "Epoch 135, Loss: 0.45228737659373064\n",
      "Epoch 136, Loss: 0.4522154657432981\n",
      "Epoch 137, Loss: 0.45165964487407706\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23249312379405837\n",
      "Valence RMSE: 0.24636787395308743\n",
      "Arousal RMSE: 0.21773602344783075\n",
      "Test R^2 score: tensor([0.3514, 0.6504], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.5008764532232595\n",
      "Num of epochs: 138\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Epoch 121, Loss: 0.4596922103619757\n",
      "Epoch 122, Loss: 0.45943389956138125\n",
      "Epoch 123, Loss: 0.45877818894117006\n",
      "Epoch 124, Loss: 0.45746538132874076\n",
      "Epoch 125, Loss: 0.4570462803121619\n",
      "Epoch 126, Loss: 0.45696189549769234\n",
      "Epoch 127, Loss: 0.45594920341421274\n",
      "Epoch 128, Loss: 0.45536359747685007\n",
      "Epoch 129, Loss: 0.4553290890752543\n",
      "Epoch 130, Loss: 0.4547195823074053\n",
      "Epoch 131, Loss: 0.45392838323998064\n",
      "Epoch 132, Loss: 0.4537853493595631\n",
      "Epoch 133, Loss: 0.45347437050767975\n",
      "Epoch 134, Loss: 0.45267431273812453\n",
      "Epoch 135, Loss: 0.45228737659373064\n",
      "Epoch 136, Loss: 0.4522154657432981\n",
      "Epoch 137, Loss: 0.45165964487407706\n",
      "Epoch 138, Loss: 0.4510751265462795\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23250472183252638\n",
      "Valence RMSE: 0.2462543045008706\n",
      "Arousal RMSE: 0.2178892123617722\n",
      "Test R^2 score: tensor([0.3520, 0.6499], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.5009292999632626\n",
      "Num of epochs: 139\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Epoch 121, Loss: 0.4596922103619757\n",
      "Epoch 122, Loss: 0.45943389956138125\n",
      "Epoch 123, Loss: 0.45877818894117006\n",
      "Epoch 124, Loss: 0.45746538132874076\n",
      "Epoch 125, Loss: 0.4570462803121619\n",
      "Epoch 126, Loss: 0.45696189549769234\n",
      "Epoch 127, Loss: 0.45594920341421274\n",
      "Epoch 128, Loss: 0.45536359747685007\n",
      "Epoch 129, Loss: 0.4553290890752543\n",
      "Epoch 130, Loss: 0.4547195823074053\n",
      "Epoch 131, Loss: 0.45392838323998064\n",
      "Epoch 132, Loss: 0.4537853493595631\n",
      "Epoch 133, Loss: 0.45347437050767975\n",
      "Epoch 134, Loss: 0.45267431273812453\n",
      "Epoch 135, Loss: 0.45228737659373064\n",
      "Epoch 136, Loss: 0.4522154657432981\n",
      "Epoch 137, Loss: 0.45165964487407706\n",
      "Epoch 138, Loss: 0.4510751265462795\n",
      "Epoch 139, Loss: 0.45082650445054473\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23334038314683667\n",
      "Valence RMSE: 0.24754449453082356\n",
      "Arousal RMSE: 0.21821363853275566\n",
      "Test R^2 score: tensor([0.3452, 0.6488], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4970035976779211\n",
      "Num of epochs: 140\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Epoch 121, Loss: 0.4596922103619757\n",
      "Epoch 122, Loss: 0.45943389956138125\n",
      "Epoch 123, Loss: 0.45877818894117006\n",
      "Epoch 124, Loss: 0.45746538132874076\n",
      "Epoch 125, Loss: 0.4570462803121619\n",
      "Epoch 126, Loss: 0.45696189549769234\n",
      "Epoch 127, Loss: 0.45594920341421274\n",
      "Epoch 128, Loss: 0.45536359747685007\n",
      "Epoch 129, Loss: 0.4553290890752543\n",
      "Epoch 130, Loss: 0.4547195823074053\n",
      "Epoch 131, Loss: 0.45392838323998064\n",
      "Epoch 132, Loss: 0.4537853493595631\n",
      "Epoch 133, Loss: 0.45347437050767975\n",
      "Epoch 134, Loss: 0.45267431273812453\n",
      "Epoch 135, Loss: 0.45228737659373064\n",
      "Epoch 136, Loss: 0.4522154657432981\n",
      "Epoch 137, Loss: 0.45165964487407706\n",
      "Epoch 138, Loss: 0.4510751265462795\n",
      "Epoch 139, Loss: 0.45082650445054473\n",
      "Epoch 140, Loss: 0.4506281098405297\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23272006807017875\n",
      "Valence RMSE: 0.24629745299467182\n",
      "Arousal RMSE: 0.21829985069512686\n",
      "Test R^2 score: tensor([0.3518, 0.6485], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.5001552598114456\n",
      "Num of epochs: 141\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Epoch 121, Loss: 0.4596922103619757\n",
      "Epoch 122, Loss: 0.45943389956138125\n",
      "Epoch 123, Loss: 0.45877818894117006\n",
      "Epoch 124, Loss: 0.45746538132874076\n",
      "Epoch 125, Loss: 0.4570462803121619\n",
      "Epoch 126, Loss: 0.45696189549769234\n",
      "Epoch 127, Loss: 0.45594920341421274\n",
      "Epoch 128, Loss: 0.45536359747685007\n",
      "Epoch 129, Loss: 0.4553290890752543\n",
      "Epoch 130, Loss: 0.4547195823074053\n",
      "Epoch 131, Loss: 0.45392838323998064\n",
      "Epoch 132, Loss: 0.4537853493595631\n",
      "Epoch 133, Loss: 0.45347437050767975\n",
      "Epoch 134, Loss: 0.45267431273812453\n",
      "Epoch 135, Loss: 0.45228737659373064\n",
      "Epoch 136, Loss: 0.4522154657432981\n",
      "Epoch 137, Loss: 0.45165964487407706\n",
      "Epoch 138, Loss: 0.4510751265462795\n",
      "Epoch 139, Loss: 0.45082650445054473\n",
      "Epoch 140, Loss: 0.4506281098405297\n",
      "Epoch 141, Loss: 0.4501911419656675\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23310574772296472\n",
      "Valence RMSE: 0.2469015009830739\n",
      "Arousal RMSE: 0.21844044509950558\n",
      "Test R^2 score: tensor([0.3486, 0.6481], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4983370764239003\n",
      "Num of epochs: 142\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Epoch 121, Loss: 0.4596922103619757\n",
      "Epoch 122, Loss: 0.45943389956138125\n",
      "Epoch 123, Loss: 0.45877818894117006\n",
      "Epoch 124, Loss: 0.45746538132874076\n",
      "Epoch 125, Loss: 0.4570462803121619\n",
      "Epoch 126, Loss: 0.45696189549769234\n",
      "Epoch 127, Loss: 0.45594920341421274\n",
      "Epoch 128, Loss: 0.45536359747685007\n",
      "Epoch 129, Loss: 0.4553290890752543\n",
      "Epoch 130, Loss: 0.4547195823074053\n",
      "Epoch 131, Loss: 0.45392838323998064\n",
      "Epoch 132, Loss: 0.4537853493595631\n",
      "Epoch 133, Loss: 0.45347437050767975\n",
      "Epoch 134, Loss: 0.45267431273812453\n",
      "Epoch 135, Loss: 0.45228737659373064\n",
      "Epoch 136, Loss: 0.4522154657432981\n",
      "Epoch 137, Loss: 0.45165964487407706\n",
      "Epoch 138, Loss: 0.4510751265462795\n",
      "Epoch 139, Loss: 0.45082650445054473\n",
      "Epoch 140, Loss: 0.4506281098405297\n",
      "Epoch 141, Loss: 0.4501911419656675\n",
      "Epoch 142, Loss: 0.4497001887659326\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2333568659315539\n",
      "Valence RMSE: 0.24712783081517103\n",
      "Arousal RMSE: 0.21872057285811647\n",
      "Test R^2 score: tensor([0.3474, 0.6472], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.49728808585097795\n",
      "Num of epochs: 143\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Epoch 121, Loss: 0.4596922103619757\n",
      "Epoch 122, Loss: 0.45943389956138125\n",
      "Epoch 123, Loss: 0.45877818894117006\n",
      "Epoch 124, Loss: 0.45746538132874076\n",
      "Epoch 125, Loss: 0.4570462803121619\n",
      "Epoch 126, Loss: 0.45696189549769234\n",
      "Epoch 127, Loss: 0.45594920341421274\n",
      "Epoch 128, Loss: 0.45536359747685007\n",
      "Epoch 129, Loss: 0.4553290890752543\n",
      "Epoch 130, Loss: 0.4547195823074053\n",
      "Epoch 131, Loss: 0.45392838323998064\n",
      "Epoch 132, Loss: 0.4537853493595631\n",
      "Epoch 133, Loss: 0.45347437050767975\n",
      "Epoch 134, Loss: 0.45267431273812453\n",
      "Epoch 135, Loss: 0.45228737659373064\n",
      "Epoch 136, Loss: 0.4522154657432981\n",
      "Epoch 137, Loss: 0.45165964487407706\n",
      "Epoch 138, Loss: 0.4510751265462795\n",
      "Epoch 139, Loss: 0.45082650445054473\n",
      "Epoch 140, Loss: 0.4506281098405297\n",
      "Epoch 141, Loss: 0.4501911419656675\n",
      "Epoch 142, Loss: 0.4497001887659326\n",
      "Epoch 143, Loss: 0.44943436133718806\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23304830402804816\n",
      "Valence RMSE: 0.24625555737766655\n",
      "Arousal RMSE: 0.21904616974810112\n",
      "Test R^2 score: tensor([0.3520, 0.6461], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4990619246120213\n",
      "Num of epochs: 144\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Epoch 121, Loss: 0.4596922103619757\n",
      "Epoch 122, Loss: 0.45943389956138125\n",
      "Epoch 123, Loss: 0.45877818894117006\n",
      "Epoch 124, Loss: 0.45746538132874076\n",
      "Epoch 125, Loss: 0.4570462803121619\n",
      "Epoch 126, Loss: 0.45696189549769234\n",
      "Epoch 127, Loss: 0.45594920341421274\n",
      "Epoch 128, Loss: 0.45536359747685007\n",
      "Epoch 129, Loss: 0.4553290890752543\n",
      "Epoch 130, Loss: 0.4547195823074053\n",
      "Epoch 131, Loss: 0.45392838323998064\n",
      "Epoch 132, Loss: 0.4537853493595631\n",
      "Epoch 133, Loss: 0.45347437050767975\n",
      "Epoch 134, Loss: 0.45267431273812453\n",
      "Epoch 135, Loss: 0.45228737659373064\n",
      "Epoch 136, Loss: 0.4522154657432981\n",
      "Epoch 137, Loss: 0.45165964487407706\n",
      "Epoch 138, Loss: 0.4510751265462795\n",
      "Epoch 139, Loss: 0.45082650445054473\n",
      "Epoch 140, Loss: 0.4506281098405297\n",
      "Epoch 141, Loss: 0.4501911419656675\n",
      "Epoch 142, Loss: 0.4497001887659326\n",
      "Epoch 143, Loss: 0.44943436133718806\n",
      "Epoch 144, Loss: 0.449226296402944\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23361127292759598\n",
      "Valence RMSE: 0.2472760873326119\n",
      "Arousal RMSE: 0.21909584731614146\n",
      "Test R^2 score: tensor([0.3466, 0.6460], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.49629059469071257\n",
      "Num of epochs: 145\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Epoch 121, Loss: 0.4596922103619757\n",
      "Epoch 122, Loss: 0.45943389956138125\n",
      "Epoch 123, Loss: 0.45877818894117006\n",
      "Epoch 124, Loss: 0.45746538132874076\n",
      "Epoch 125, Loss: 0.4570462803121619\n",
      "Epoch 126, Loss: 0.45696189549769234\n",
      "Epoch 127, Loss: 0.45594920341421274\n",
      "Epoch 128, Loss: 0.45536359747685007\n",
      "Epoch 129, Loss: 0.4553290890752543\n",
      "Epoch 130, Loss: 0.4547195823074053\n",
      "Epoch 131, Loss: 0.45392838323998064\n",
      "Epoch 132, Loss: 0.4537853493595631\n",
      "Epoch 133, Loss: 0.45347437050767975\n",
      "Epoch 134, Loss: 0.45267431273812453\n",
      "Epoch 135, Loss: 0.45228737659373064\n",
      "Epoch 136, Loss: 0.4522154657432981\n",
      "Epoch 137, Loss: 0.45165964487407706\n",
      "Epoch 138, Loss: 0.4510751265462795\n",
      "Epoch 139, Loss: 0.45082650445054473\n",
      "Epoch 140, Loss: 0.4506281098405297\n",
      "Epoch 141, Loss: 0.4501911419656675\n",
      "Epoch 142, Loss: 0.4497001887659326\n",
      "Epoch 143, Loss: 0.44943436133718806\n",
      "Epoch 144, Loss: 0.449226296402944\n",
      "Epoch 145, Loss: 0.44886073897867573\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23338539608307815\n",
      "Valence RMSE: 0.24674482499948178\n",
      "Arousal RMSE: 0.21921331516512782\n",
      "Test R^2 score: tensor([0.3494, 0.6456], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.49750302842309135\n",
      "Num of epochs: 146\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Epoch 121, Loss: 0.4596922103619757\n",
      "Epoch 122, Loss: 0.45943389956138125\n",
      "Epoch 123, Loss: 0.45877818894117006\n",
      "Epoch 124, Loss: 0.45746538132874076\n",
      "Epoch 125, Loss: 0.4570462803121619\n",
      "Epoch 126, Loss: 0.45696189549769234\n",
      "Epoch 127, Loss: 0.45594920341421274\n",
      "Epoch 128, Loss: 0.45536359747685007\n",
      "Epoch 129, Loss: 0.4553290890752543\n",
      "Epoch 130, Loss: 0.4547195823074053\n",
      "Epoch 131, Loss: 0.45392838323998064\n",
      "Epoch 132, Loss: 0.4537853493595631\n",
      "Epoch 133, Loss: 0.45347437050767975\n",
      "Epoch 134, Loss: 0.45267431273812453\n",
      "Epoch 135, Loss: 0.45228737659373064\n",
      "Epoch 136, Loss: 0.4522154657432981\n",
      "Epoch 137, Loss: 0.45165964487407706\n",
      "Epoch 138, Loss: 0.4510751265462795\n",
      "Epoch 139, Loss: 0.45082650445054473\n",
      "Epoch 140, Loss: 0.4506281098405297\n",
      "Epoch 141, Loss: 0.4501911419656675\n",
      "Epoch 142, Loss: 0.4497001887659326\n",
      "Epoch 143, Loss: 0.44943436133718806\n",
      "Epoch 144, Loss: 0.449226296402944\n",
      "Epoch 145, Loss: 0.44886073897867573\n",
      "Epoch 146, Loss: 0.44844710376304864\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23360423610588074\n",
      "Valence RMSE: 0.2470251841814465\n",
      "Arousal RMSE: 0.2193637085603418\n",
      "Test R^2 score: tensor([0.3479, 0.6451], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4965201602734349\n",
      "Num of epochs: 147\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Epoch 121, Loss: 0.4596922103619757\n",
      "Epoch 122, Loss: 0.45943389956138125\n",
      "Epoch 123, Loss: 0.45877818894117006\n",
      "Epoch 124, Loss: 0.45746538132874076\n",
      "Epoch 125, Loss: 0.4570462803121619\n",
      "Epoch 126, Loss: 0.45696189549769234\n",
      "Epoch 127, Loss: 0.45594920341421274\n",
      "Epoch 128, Loss: 0.45536359747685007\n",
      "Epoch 129, Loss: 0.4553290890752543\n",
      "Epoch 130, Loss: 0.4547195823074053\n",
      "Epoch 131, Loss: 0.45392838323998064\n",
      "Epoch 132, Loss: 0.4537853493595631\n",
      "Epoch 133, Loss: 0.45347437050767975\n",
      "Epoch 134, Loss: 0.45267431273812453\n",
      "Epoch 135, Loss: 0.45228737659373064\n",
      "Epoch 136, Loss: 0.4522154657432981\n",
      "Epoch 137, Loss: 0.45165964487407706\n",
      "Epoch 138, Loss: 0.4510751265462795\n",
      "Epoch 139, Loss: 0.45082650445054473\n",
      "Epoch 140, Loss: 0.4506281098405297\n",
      "Epoch 141, Loss: 0.4501911419656675\n",
      "Epoch 142, Loss: 0.4497001887659326\n",
      "Epoch 143, Loss: 0.44943436133718806\n",
      "Epoch 144, Loss: 0.449226296402944\n",
      "Epoch 145, Loss: 0.44886073897867573\n",
      "Epoch 146, Loss: 0.44844710376304864\n",
      "Epoch 147, Loss: 0.44810507013116724\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2340998745015979\n",
      "Valence RMSE: 0.24779210881661468\n",
      "Arousal RMSE: 0.2195553991400411\n",
      "Test R^2 score: tensor([0.3439, 0.6445], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4941823112224359\n",
      "Num of epochs: 148\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Epoch 121, Loss: 0.4596922103619757\n",
      "Epoch 122, Loss: 0.45943389956138125\n",
      "Epoch 123, Loss: 0.45877818894117006\n",
      "Epoch 124, Loss: 0.45746538132874076\n",
      "Epoch 125, Loss: 0.4570462803121619\n",
      "Epoch 126, Loss: 0.45696189549769234\n",
      "Epoch 127, Loss: 0.45594920341421274\n",
      "Epoch 128, Loss: 0.45536359747685007\n",
      "Epoch 129, Loss: 0.4553290890752543\n",
      "Epoch 130, Loss: 0.4547195823074053\n",
      "Epoch 131, Loss: 0.45392838323998064\n",
      "Epoch 132, Loss: 0.4537853493595631\n",
      "Epoch 133, Loss: 0.45347437050767975\n",
      "Epoch 134, Loss: 0.45267431273812453\n",
      "Epoch 135, Loss: 0.45228737659373064\n",
      "Epoch 136, Loss: 0.4522154657432981\n",
      "Epoch 137, Loss: 0.45165964487407706\n",
      "Epoch 138, Loss: 0.4510751265462795\n",
      "Epoch 139, Loss: 0.45082650445054473\n",
      "Epoch 140, Loss: 0.4506281098405297\n",
      "Epoch 141, Loss: 0.4501911419656675\n",
      "Epoch 142, Loss: 0.4497001887659326\n",
      "Epoch 143, Loss: 0.44943436133718806\n",
      "Epoch 144, Loss: 0.449226296402944\n",
      "Epoch 145, Loss: 0.44886073897867573\n",
      "Epoch 146, Loss: 0.44844710376304864\n",
      "Epoch 147, Loss: 0.44810507013116724\n",
      "Epoch 148, Loss: 0.44788545862049767\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23387285719589196\n",
      "Valence RMSE: 0.2470843321805828\n",
      "Arousal RMSE: 0.21986895973924012\n",
      "Test R^2 score: tensor([0.3476, 0.6435], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.495545672508254\n",
      "Num of epochs: 149\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Epoch 121, Loss: 0.4596922103619757\n",
      "Epoch 122, Loss: 0.45943389956138125\n",
      "Epoch 123, Loss: 0.45877818894117006\n",
      "Epoch 124, Loss: 0.45746538132874076\n",
      "Epoch 125, Loss: 0.4570462803121619\n",
      "Epoch 126, Loss: 0.45696189549769234\n",
      "Epoch 127, Loss: 0.45594920341421274\n",
      "Epoch 128, Loss: 0.45536359747685007\n",
      "Epoch 129, Loss: 0.4553290890752543\n",
      "Epoch 130, Loss: 0.4547195823074053\n",
      "Epoch 131, Loss: 0.45392838323998064\n",
      "Epoch 132, Loss: 0.4537853493595631\n",
      "Epoch 133, Loss: 0.45347437050767975\n",
      "Epoch 134, Loss: 0.45267431273812453\n",
      "Epoch 135, Loss: 0.45228737659373064\n",
      "Epoch 136, Loss: 0.4522154657432981\n",
      "Epoch 137, Loss: 0.45165964487407706\n",
      "Epoch 138, Loss: 0.4510751265462795\n",
      "Epoch 139, Loss: 0.45082650445054473\n",
      "Epoch 140, Loss: 0.4506281098405297\n",
      "Epoch 141, Loss: 0.4501911419656675\n",
      "Epoch 142, Loss: 0.4497001887659326\n",
      "Epoch 143, Loss: 0.44943436133718806\n",
      "Epoch 144, Loss: 0.449226296402944\n",
      "Epoch 145, Loss: 0.44886073897867573\n",
      "Epoch 146, Loss: 0.44844710376304864\n",
      "Epoch 147, Loss: 0.44810507013116724\n",
      "Epoch 148, Loss: 0.44788545862049767\n",
      "Epoch 149, Loss: 0.4476741441009651\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23444686616044041\n",
      "Valence RMSE: 0.24823224105944788\n",
      "Arousal RMSE: 0.21979859099527246\n",
      "Test R^2 score: tensor([0.3415, 0.6437], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.49262185293119487\n",
      "Num of epochs: 150\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Epoch 121, Loss: 0.4596922103619757\n",
      "Epoch 122, Loss: 0.45943389956138125\n",
      "Epoch 123, Loss: 0.45877818894117006\n",
      "Epoch 124, Loss: 0.45746538132874076\n",
      "Epoch 125, Loss: 0.4570462803121619\n",
      "Epoch 126, Loss: 0.45696189549769234\n",
      "Epoch 127, Loss: 0.45594920341421274\n",
      "Epoch 128, Loss: 0.45536359747685007\n",
      "Epoch 129, Loss: 0.4553290890752543\n",
      "Epoch 130, Loss: 0.4547195823074053\n",
      "Epoch 131, Loss: 0.45392838323998064\n",
      "Epoch 132, Loss: 0.4537853493595631\n",
      "Epoch 133, Loss: 0.45347437050767975\n",
      "Epoch 134, Loss: 0.45267431273812453\n",
      "Epoch 135, Loss: 0.45228737659373064\n",
      "Epoch 136, Loss: 0.4522154657432981\n",
      "Epoch 137, Loss: 0.45165964487407706\n",
      "Epoch 138, Loss: 0.4510751265462795\n",
      "Epoch 139, Loss: 0.45082650445054473\n",
      "Epoch 140, Loss: 0.4506281098405297\n",
      "Epoch 141, Loss: 0.4501911419656675\n",
      "Epoch 142, Loss: 0.4497001887659326\n",
      "Epoch 143, Loss: 0.44943436133718806\n",
      "Epoch 144, Loss: 0.449226296402944\n",
      "Epoch 145, Loss: 0.44886073897867573\n",
      "Epoch 146, Loss: 0.44844710376304864\n",
      "Epoch 147, Loss: 0.44810507013116724\n",
      "Epoch 148, Loss: 0.44788545862049767\n",
      "Epoch 149, Loss: 0.4476741441009651\n",
      "Epoch 150, Loss: 0.44735861731902055\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23397673607521471\n",
      "Valence RMSE: 0.2473167997173418\n",
      "Arousal RMSE: 0.21982863013355114\n",
      "Test R^2 score: tensor([0.3464, 0.6436], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.49499698072741616\n",
      "Num of epochs: 151\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Epoch 121, Loss: 0.4596922103619757\n",
      "Epoch 122, Loss: 0.45943389956138125\n",
      "Epoch 123, Loss: 0.45877818894117006\n",
      "Epoch 124, Loss: 0.45746538132874076\n",
      "Epoch 125, Loss: 0.4570462803121619\n",
      "Epoch 126, Loss: 0.45696189549769234\n",
      "Epoch 127, Loss: 0.45594920341421274\n",
      "Epoch 128, Loss: 0.45536359747685007\n",
      "Epoch 129, Loss: 0.4553290890752543\n",
      "Epoch 130, Loss: 0.4547195823074053\n",
      "Epoch 131, Loss: 0.45392838323998064\n",
      "Epoch 132, Loss: 0.4537853493595631\n",
      "Epoch 133, Loss: 0.45347437050767975\n",
      "Epoch 134, Loss: 0.45267431273812453\n",
      "Epoch 135, Loss: 0.45228737659373064\n",
      "Epoch 136, Loss: 0.4522154657432981\n",
      "Epoch 137, Loss: 0.45165964487407706\n",
      "Epoch 138, Loss: 0.4510751265462795\n",
      "Epoch 139, Loss: 0.45082650445054473\n",
      "Epoch 140, Loss: 0.4506281098405297\n",
      "Epoch 141, Loss: 0.4501911419656675\n",
      "Epoch 142, Loss: 0.4497001887659326\n",
      "Epoch 143, Loss: 0.44943436133718806\n",
      "Epoch 144, Loss: 0.449226296402944\n",
      "Epoch 145, Loss: 0.44886073897867573\n",
      "Epoch 146, Loss: 0.44844710376304864\n",
      "Epoch 147, Loss: 0.44810507013116724\n",
      "Epoch 148, Loss: 0.44788545862049767\n",
      "Epoch 149, Loss: 0.4476741441009651\n",
      "Epoch 150, Loss: 0.44735861731902055\n",
      "Epoch 151, Loss: 0.4469288720157105\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2340636050592936\n",
      "Valence RMSE: 0.24753653361362118\n",
      "Arousal RMSE: 0.21976625526513058\n",
      "Test R^2 score: tensor([0.3452, 0.6438], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4945171149892738\n",
      "Num of epochs: 152\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Epoch 121, Loss: 0.4596922103619757\n",
      "Epoch 122, Loss: 0.45943389956138125\n",
      "Epoch 123, Loss: 0.45877818894117006\n",
      "Epoch 124, Loss: 0.45746538132874076\n",
      "Epoch 125, Loss: 0.4570462803121619\n",
      "Epoch 126, Loss: 0.45696189549769234\n",
      "Epoch 127, Loss: 0.45594920341421274\n",
      "Epoch 128, Loss: 0.45536359747685007\n",
      "Epoch 129, Loss: 0.4553290890752543\n",
      "Epoch 130, Loss: 0.4547195823074053\n",
      "Epoch 131, Loss: 0.45392838323998064\n",
      "Epoch 132, Loss: 0.4537853493595631\n",
      "Epoch 133, Loss: 0.45347437050767975\n",
      "Epoch 134, Loss: 0.45267431273812453\n",
      "Epoch 135, Loss: 0.45228737659373064\n",
      "Epoch 136, Loss: 0.4522154657432981\n",
      "Epoch 137, Loss: 0.45165964487407706\n",
      "Epoch 138, Loss: 0.4510751265462795\n",
      "Epoch 139, Loss: 0.45082650445054473\n",
      "Epoch 140, Loss: 0.4506281098405297\n",
      "Epoch 141, Loss: 0.4501911419656675\n",
      "Epoch 142, Loss: 0.4497001887659326\n",
      "Epoch 143, Loss: 0.44943436133718806\n",
      "Epoch 144, Loss: 0.449226296402944\n",
      "Epoch 145, Loss: 0.44886073897867573\n",
      "Epoch 146, Loss: 0.44844710376304864\n",
      "Epoch 147, Loss: 0.44810507013116724\n",
      "Epoch 148, Loss: 0.44788545862049767\n",
      "Epoch 149, Loss: 0.4476741441009651\n",
      "Epoch 150, Loss: 0.44735861731902055\n",
      "Epoch 151, Loss: 0.4469288720157105\n",
      "Epoch 152, Loss: 0.44655235754178146\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2341946151256342\n",
      "Valence RMSE: 0.24768449154410582\n",
      "Arousal RMSE: 0.2198786668965992\n",
      "Test R^2 score: tensor([0.3444, 0.6434], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4939433846880066\n",
      "Num of epochs: 153\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Epoch 121, Loss: 0.4596922103619757\n",
      "Epoch 122, Loss: 0.45943389956138125\n",
      "Epoch 123, Loss: 0.45877818894117006\n",
      "Epoch 124, Loss: 0.45746538132874076\n",
      "Epoch 125, Loss: 0.4570462803121619\n",
      "Epoch 126, Loss: 0.45696189549769234\n",
      "Epoch 127, Loss: 0.45594920341421274\n",
      "Epoch 128, Loss: 0.45536359747685007\n",
      "Epoch 129, Loss: 0.4553290890752543\n",
      "Epoch 130, Loss: 0.4547195823074053\n",
      "Epoch 131, Loss: 0.45392838323998064\n",
      "Epoch 132, Loss: 0.4537853493595631\n",
      "Epoch 133, Loss: 0.45347437050767975\n",
      "Epoch 134, Loss: 0.45267431273812453\n",
      "Epoch 135, Loss: 0.45228737659373064\n",
      "Epoch 136, Loss: 0.4522154657432981\n",
      "Epoch 137, Loss: 0.45165964487407706\n",
      "Epoch 138, Loss: 0.4510751265462795\n",
      "Epoch 139, Loss: 0.45082650445054473\n",
      "Epoch 140, Loss: 0.4506281098405297\n",
      "Epoch 141, Loss: 0.4501911419656675\n",
      "Epoch 142, Loss: 0.4497001887659326\n",
      "Epoch 143, Loss: 0.44943436133718806\n",
      "Epoch 144, Loss: 0.449226296402944\n",
      "Epoch 145, Loss: 0.44886073897867573\n",
      "Epoch 146, Loss: 0.44844710376304864\n",
      "Epoch 147, Loss: 0.44810507013116724\n",
      "Epoch 148, Loss: 0.44788545862049767\n",
      "Epoch 149, Loss: 0.4476741441009651\n",
      "Epoch 150, Loss: 0.44735861731902055\n",
      "Epoch 151, Loss: 0.4469288720157105\n",
      "Epoch 152, Loss: 0.44655235754178146\n",
      "Epoch 153, Loss: 0.4462857402786373\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23408093655911968\n",
      "Valence RMSE: 0.2471581185722097\n",
      "Arousal RMSE: 0.22022859520197366\n",
      "Test R^2 score: tensor([0.3472, 0.6423], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.49476718787207896\n",
      "Num of epochs: 154\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Epoch 121, Loss: 0.4596922103619757\n",
      "Epoch 122, Loss: 0.45943389956138125\n",
      "Epoch 123, Loss: 0.45877818894117006\n",
      "Epoch 124, Loss: 0.45746538132874076\n",
      "Epoch 125, Loss: 0.4570462803121619\n",
      "Epoch 126, Loss: 0.45696189549769234\n",
      "Epoch 127, Loss: 0.45594920341421274\n",
      "Epoch 128, Loss: 0.45536359747685007\n",
      "Epoch 129, Loss: 0.4553290890752543\n",
      "Epoch 130, Loss: 0.4547195823074053\n",
      "Epoch 131, Loss: 0.45392838323998064\n",
      "Epoch 132, Loss: 0.4537853493595631\n",
      "Epoch 133, Loss: 0.45347437050767975\n",
      "Epoch 134, Loss: 0.45267431273812453\n",
      "Epoch 135, Loss: 0.45228737659373064\n",
      "Epoch 136, Loss: 0.4522154657432981\n",
      "Epoch 137, Loss: 0.45165964487407706\n",
      "Epoch 138, Loss: 0.4510751265462795\n",
      "Epoch 139, Loss: 0.45082650445054473\n",
      "Epoch 140, Loss: 0.4506281098405297\n",
      "Epoch 141, Loss: 0.4501911419656675\n",
      "Epoch 142, Loss: 0.4497001887659326\n",
      "Epoch 143, Loss: 0.44943436133718806\n",
      "Epoch 144, Loss: 0.449226296402944\n",
      "Epoch 145, Loss: 0.44886073897867573\n",
      "Epoch 146, Loss: 0.44844710376304864\n",
      "Epoch 147, Loss: 0.44810507013116724\n",
      "Epoch 148, Loss: 0.44788545862049767\n",
      "Epoch 149, Loss: 0.4476741441009651\n",
      "Epoch 150, Loss: 0.44735861731902055\n",
      "Epoch 151, Loss: 0.4469288720157105\n",
      "Epoch 152, Loss: 0.44655235754178146\n",
      "Epoch 153, Loss: 0.4462857402786373\n",
      "Epoch 154, Loss: 0.4460517369142754\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23461572036536887\n",
      "Valence RMSE: 0.2481685293822966\n",
      "Arousal RMSE: 0.22023045545371353\n",
      "Test R^2 score: tensor([0.3419, 0.6423], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4920900905903608\n",
      "Num of epochs: 155\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Epoch 121, Loss: 0.4596922103619757\n",
      "Epoch 122, Loss: 0.45943389956138125\n",
      "Epoch 123, Loss: 0.45877818894117006\n",
      "Epoch 124, Loss: 0.45746538132874076\n",
      "Epoch 125, Loss: 0.4570462803121619\n",
      "Epoch 126, Loss: 0.45696189549769234\n",
      "Epoch 127, Loss: 0.45594920341421274\n",
      "Epoch 128, Loss: 0.45536359747685007\n",
      "Epoch 129, Loss: 0.4553290890752543\n",
      "Epoch 130, Loss: 0.4547195823074053\n",
      "Epoch 131, Loss: 0.45392838323998064\n",
      "Epoch 132, Loss: 0.4537853493595631\n",
      "Epoch 133, Loss: 0.45347437050767975\n",
      "Epoch 134, Loss: 0.45267431273812453\n",
      "Epoch 135, Loss: 0.45228737659373064\n",
      "Epoch 136, Loss: 0.4522154657432981\n",
      "Epoch 137, Loss: 0.45165964487407706\n",
      "Epoch 138, Loss: 0.4510751265462795\n",
      "Epoch 139, Loss: 0.45082650445054473\n",
      "Epoch 140, Loss: 0.4506281098405297\n",
      "Epoch 141, Loss: 0.4501911419656675\n",
      "Epoch 142, Loss: 0.4497001887659326\n",
      "Epoch 143, Loss: 0.44943436133718806\n",
      "Epoch 144, Loss: 0.449226296402944\n",
      "Epoch 145, Loss: 0.44886073897867573\n",
      "Epoch 146, Loss: 0.44844710376304864\n",
      "Epoch 147, Loss: 0.44810507013116724\n",
      "Epoch 148, Loss: 0.44788545862049767\n",
      "Epoch 149, Loss: 0.4476741441009651\n",
      "Epoch 150, Loss: 0.44735861731902055\n",
      "Epoch 151, Loss: 0.4469288720157105\n",
      "Epoch 152, Loss: 0.44655235754178146\n",
      "Epoch 153, Loss: 0.4462857402786373\n",
      "Epoch 154, Loss: 0.4460517369142754\n",
      "Epoch 155, Loss: 0.4457881796184935\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2343515637726334\n",
      "Valence RMSE: 0.24746565664543343\n",
      "Arousal RMSE: 0.22045874821925698\n",
      "Test R^2 score: tensor([0.3456, 0.6416], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4935804315690005\n",
      "Num of epochs: 156\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Epoch 121, Loss: 0.4596922103619757\n",
      "Epoch 122, Loss: 0.45943389956138125\n",
      "Epoch 123, Loss: 0.45877818894117006\n",
      "Epoch 124, Loss: 0.45746538132874076\n",
      "Epoch 125, Loss: 0.4570462803121619\n",
      "Epoch 126, Loss: 0.45696189549769234\n",
      "Epoch 127, Loss: 0.45594920341421274\n",
      "Epoch 128, Loss: 0.45536359747685007\n",
      "Epoch 129, Loss: 0.4553290890752543\n",
      "Epoch 130, Loss: 0.4547195823074053\n",
      "Epoch 131, Loss: 0.45392838323998064\n",
      "Epoch 132, Loss: 0.4537853493595631\n",
      "Epoch 133, Loss: 0.45347437050767975\n",
      "Epoch 134, Loss: 0.45267431273812453\n",
      "Epoch 135, Loss: 0.45228737659373064\n",
      "Epoch 136, Loss: 0.4522154657432981\n",
      "Epoch 137, Loss: 0.45165964487407706\n",
      "Epoch 138, Loss: 0.4510751265462795\n",
      "Epoch 139, Loss: 0.45082650445054473\n",
      "Epoch 140, Loss: 0.4506281098405297\n",
      "Epoch 141, Loss: 0.4501911419656675\n",
      "Epoch 142, Loss: 0.4497001887659326\n",
      "Epoch 143, Loss: 0.44943436133718806\n",
      "Epoch 144, Loss: 0.449226296402944\n",
      "Epoch 145, Loss: 0.44886073897867573\n",
      "Epoch 146, Loss: 0.44844710376304864\n",
      "Epoch 147, Loss: 0.44810507013116724\n",
      "Epoch 148, Loss: 0.44788545862049767\n",
      "Epoch 149, Loss: 0.4476741441009651\n",
      "Epoch 150, Loss: 0.44735861731902055\n",
      "Epoch 151, Loss: 0.4469288720157105\n",
      "Epoch 152, Loss: 0.44655235754178146\n",
      "Epoch 153, Loss: 0.4462857402786373\n",
      "Epoch 154, Loss: 0.4460517369142754\n",
      "Epoch 155, Loss: 0.4457881796184935\n",
      "Epoch 156, Loss: 0.44542210886348255\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23460188504377613\n",
      "Valence RMSE: 0.2480343799708568\n",
      "Arousal RMSE: 0.22035207120573863\n",
      "Test R^2 score: tensor([0.3426, 0.6419], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.49224816652571723\n",
      "Num of epochs: 157\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Epoch 121, Loss: 0.4596922103619757\n",
      "Epoch 122, Loss: 0.45943389956138125\n",
      "Epoch 123, Loss: 0.45877818894117006\n",
      "Epoch 124, Loss: 0.45746538132874076\n",
      "Epoch 125, Loss: 0.4570462803121619\n",
      "Epoch 126, Loss: 0.45696189549769234\n",
      "Epoch 127, Loss: 0.45594920341421274\n",
      "Epoch 128, Loss: 0.45536359747685007\n",
      "Epoch 129, Loss: 0.4553290890752543\n",
      "Epoch 130, Loss: 0.4547195823074053\n",
      "Epoch 131, Loss: 0.45392838323998064\n",
      "Epoch 132, Loss: 0.4537853493595631\n",
      "Epoch 133, Loss: 0.45347437050767975\n",
      "Epoch 134, Loss: 0.45267431273812453\n",
      "Epoch 135, Loss: 0.45228737659373064\n",
      "Epoch 136, Loss: 0.4522154657432981\n",
      "Epoch 137, Loss: 0.45165964487407706\n",
      "Epoch 138, Loss: 0.4510751265462795\n",
      "Epoch 139, Loss: 0.45082650445054473\n",
      "Epoch 140, Loss: 0.4506281098405297\n",
      "Epoch 141, Loss: 0.4501911419656675\n",
      "Epoch 142, Loss: 0.4497001887659326\n",
      "Epoch 143, Loss: 0.44943436133718806\n",
      "Epoch 144, Loss: 0.449226296402944\n",
      "Epoch 145, Loss: 0.44886073897867573\n",
      "Epoch 146, Loss: 0.44844710376304864\n",
      "Epoch 147, Loss: 0.44810507013116724\n",
      "Epoch 148, Loss: 0.44788545862049767\n",
      "Epoch 149, Loss: 0.4476741441009651\n",
      "Epoch 150, Loss: 0.44735861731902055\n",
      "Epoch 151, Loss: 0.4469288720157105\n",
      "Epoch 152, Loss: 0.44655235754178146\n",
      "Epoch 153, Loss: 0.4462857402786373\n",
      "Epoch 154, Loss: 0.4460517369142754\n",
      "Epoch 155, Loss: 0.4457881796184935\n",
      "Epoch 156, Loss: 0.44542210886348255\n",
      "Epoch 157, Loss: 0.4450579467827662\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23454503286698766\n",
      "Valence RMSE: 0.2479057309054651\n",
      "Arousal RMSE: 0.22037580055300932\n",
      "Test R^2 score: tensor([0.3433, 0.6418], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.49255049662772554\n",
      "Num of epochs: 158\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Epoch 121, Loss: 0.4596922103619757\n",
      "Epoch 122, Loss: 0.45943389956138125\n",
      "Epoch 123, Loss: 0.45877818894117006\n",
      "Epoch 124, Loss: 0.45746538132874076\n",
      "Epoch 125, Loss: 0.4570462803121619\n",
      "Epoch 126, Loss: 0.45696189549769234\n",
      "Epoch 127, Loss: 0.45594920341421274\n",
      "Epoch 128, Loss: 0.45536359747685007\n",
      "Epoch 129, Loss: 0.4553290890752543\n",
      "Epoch 130, Loss: 0.4547195823074053\n",
      "Epoch 131, Loss: 0.45392838323998064\n",
      "Epoch 132, Loss: 0.4537853493595631\n",
      "Epoch 133, Loss: 0.45347437050767975\n",
      "Epoch 134, Loss: 0.45267431273812453\n",
      "Epoch 135, Loss: 0.45228737659373064\n",
      "Epoch 136, Loss: 0.4522154657432981\n",
      "Epoch 137, Loss: 0.45165964487407706\n",
      "Epoch 138, Loss: 0.4510751265462795\n",
      "Epoch 139, Loss: 0.45082650445054473\n",
      "Epoch 140, Loss: 0.4506281098405297\n",
      "Epoch 141, Loss: 0.4501911419656675\n",
      "Epoch 142, Loss: 0.4497001887659326\n",
      "Epoch 143, Loss: 0.44943436133718806\n",
      "Epoch 144, Loss: 0.449226296402944\n",
      "Epoch 145, Loss: 0.44886073897867573\n",
      "Epoch 146, Loss: 0.44844710376304864\n",
      "Epoch 147, Loss: 0.44810507013116724\n",
      "Epoch 148, Loss: 0.44788545862049767\n",
      "Epoch 149, Loss: 0.4476741441009651\n",
      "Epoch 150, Loss: 0.44735861731902055\n",
      "Epoch 151, Loss: 0.4469288720157105\n",
      "Epoch 152, Loss: 0.44655235754178146\n",
      "Epoch 153, Loss: 0.4462857402786373\n",
      "Epoch 154, Loss: 0.4460517369142754\n",
      "Epoch 155, Loss: 0.4457881796184935\n",
      "Epoch 156, Loss: 0.44542210886348255\n",
      "Epoch 157, Loss: 0.4450579467827662\n",
      "Epoch 158, Loss: 0.44473329320345206\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23455862795901128\n",
      "Valence RMSE: 0.24785267908290473\n",
      "Arousal RMSE: 0.22046439479302402\n",
      "Test R^2 score: tensor([0.3436, 0.6415], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4925470040694183\n",
      "Num of epochs: 159\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Epoch 121, Loss: 0.4596922103619757\n",
      "Epoch 122, Loss: 0.45943389956138125\n",
      "Epoch 123, Loss: 0.45877818894117006\n",
      "Epoch 124, Loss: 0.45746538132874076\n",
      "Epoch 125, Loss: 0.4570462803121619\n",
      "Epoch 126, Loss: 0.45696189549769234\n",
      "Epoch 127, Loss: 0.45594920341421274\n",
      "Epoch 128, Loss: 0.45536359747685007\n",
      "Epoch 129, Loss: 0.4553290890752543\n",
      "Epoch 130, Loss: 0.4547195823074053\n",
      "Epoch 131, Loss: 0.45392838323998064\n",
      "Epoch 132, Loss: 0.4537853493595631\n",
      "Epoch 133, Loss: 0.45347437050767975\n",
      "Epoch 134, Loss: 0.45267431273812453\n",
      "Epoch 135, Loss: 0.45228737659373064\n",
      "Epoch 136, Loss: 0.4522154657432981\n",
      "Epoch 137, Loss: 0.45165964487407706\n",
      "Epoch 138, Loss: 0.4510751265462795\n",
      "Epoch 139, Loss: 0.45082650445054473\n",
      "Epoch 140, Loss: 0.4506281098405297\n",
      "Epoch 141, Loss: 0.4501911419656675\n",
      "Epoch 142, Loss: 0.4497001887659326\n",
      "Epoch 143, Loss: 0.44943436133718806\n",
      "Epoch 144, Loss: 0.449226296402944\n",
      "Epoch 145, Loss: 0.44886073897867573\n",
      "Epoch 146, Loss: 0.44844710376304864\n",
      "Epoch 147, Loss: 0.44810507013116724\n",
      "Epoch 148, Loss: 0.44788545862049767\n",
      "Epoch 149, Loss: 0.4476741441009651\n",
      "Epoch 150, Loss: 0.44735861731902055\n",
      "Epoch 151, Loss: 0.4469288720157105\n",
      "Epoch 152, Loss: 0.44655235754178146\n",
      "Epoch 153, Loss: 0.4462857402786373\n",
      "Epoch 154, Loss: 0.4460517369142754\n",
      "Epoch 155, Loss: 0.4457881796184935\n",
      "Epoch 156, Loss: 0.44542210886348255\n",
      "Epoch 157, Loss: 0.4450579467827662\n",
      "Epoch 158, Loss: 0.44473329320345206\n",
      "Epoch 159, Loss: 0.4444482514678638\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2348388219656708\n",
      "Valence RMSE: 0.24832043698940828\n",
      "Arousal RMSE: 0.22053458952698862\n",
      "Test R^2 score: tensor([0.3411, 0.6413], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4911928073938248\n",
      "Num of epochs: 160\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Epoch 121, Loss: 0.4596922103619757\n",
      "Epoch 122, Loss: 0.45943389956138125\n",
      "Epoch 123, Loss: 0.45877818894117006\n",
      "Epoch 124, Loss: 0.45746538132874076\n",
      "Epoch 125, Loss: 0.4570462803121619\n",
      "Epoch 126, Loss: 0.45696189549769234\n",
      "Epoch 127, Loss: 0.45594920341421274\n",
      "Epoch 128, Loss: 0.45536359747685007\n",
      "Epoch 129, Loss: 0.4553290890752543\n",
      "Epoch 130, Loss: 0.4547195823074053\n",
      "Epoch 131, Loss: 0.45392838323998064\n",
      "Epoch 132, Loss: 0.4537853493595631\n",
      "Epoch 133, Loss: 0.45347437050767975\n",
      "Epoch 134, Loss: 0.45267431273812453\n",
      "Epoch 135, Loss: 0.45228737659373064\n",
      "Epoch 136, Loss: 0.4522154657432981\n",
      "Epoch 137, Loss: 0.45165964487407706\n",
      "Epoch 138, Loss: 0.4510751265462795\n",
      "Epoch 139, Loss: 0.45082650445054473\n",
      "Epoch 140, Loss: 0.4506281098405297\n",
      "Epoch 141, Loss: 0.4501911419656675\n",
      "Epoch 142, Loss: 0.4497001887659326\n",
      "Epoch 143, Loss: 0.44943436133718806\n",
      "Epoch 144, Loss: 0.449226296402944\n",
      "Epoch 145, Loss: 0.44886073897867573\n",
      "Epoch 146, Loss: 0.44844710376304864\n",
      "Epoch 147, Loss: 0.44810507013116724\n",
      "Epoch 148, Loss: 0.44788545862049767\n",
      "Epoch 149, Loss: 0.4476741441009651\n",
      "Epoch 150, Loss: 0.44735861731902055\n",
      "Epoch 151, Loss: 0.4469288720157105\n",
      "Epoch 152, Loss: 0.44655235754178146\n",
      "Epoch 153, Loss: 0.4462857402786373\n",
      "Epoch 154, Loss: 0.4460517369142754\n",
      "Epoch 155, Loss: 0.4457881796184935\n",
      "Epoch 156, Loss: 0.44542210886348255\n",
      "Epoch 157, Loss: 0.4450579467827662\n",
      "Epoch 158, Loss: 0.44473329320345206\n",
      "Epoch 159, Loss: 0.4444482514678638\n",
      "Epoch 160, Loss: 0.4441745674630603\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23470725277571086\n",
      "Valence RMSE: 0.24771904498064456\n",
      "Arousal RMSE: 0.22093045006272963\n",
      "Test R^2 score: tensor([0.3443, 0.6400], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4921422772454463\n",
      "Num of epochs: 161\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Epoch 121, Loss: 0.4596922103619757\n",
      "Epoch 122, Loss: 0.45943389956138125\n",
      "Epoch 123, Loss: 0.45877818894117006\n",
      "Epoch 124, Loss: 0.45746538132874076\n",
      "Epoch 125, Loss: 0.4570462803121619\n",
      "Epoch 126, Loss: 0.45696189549769234\n",
      "Epoch 127, Loss: 0.45594920341421274\n",
      "Epoch 128, Loss: 0.45536359747685007\n",
      "Epoch 129, Loss: 0.4553290890752543\n",
      "Epoch 130, Loss: 0.4547195823074053\n",
      "Epoch 131, Loss: 0.45392838323998064\n",
      "Epoch 132, Loss: 0.4537853493595631\n",
      "Epoch 133, Loss: 0.45347437050767975\n",
      "Epoch 134, Loss: 0.45267431273812453\n",
      "Epoch 135, Loss: 0.45228737659373064\n",
      "Epoch 136, Loss: 0.4522154657432981\n",
      "Epoch 137, Loss: 0.45165964487407706\n",
      "Epoch 138, Loss: 0.4510751265462795\n",
      "Epoch 139, Loss: 0.45082650445054473\n",
      "Epoch 140, Loss: 0.4506281098405297\n",
      "Epoch 141, Loss: 0.4501911419656675\n",
      "Epoch 142, Loss: 0.4497001887659326\n",
      "Epoch 143, Loss: 0.44943436133718806\n",
      "Epoch 144, Loss: 0.449226296402944\n",
      "Epoch 145, Loss: 0.44886073897867573\n",
      "Epoch 146, Loss: 0.44844710376304864\n",
      "Epoch 147, Loss: 0.44810507013116724\n",
      "Epoch 148, Loss: 0.44788545862049767\n",
      "Epoch 149, Loss: 0.4476741441009651\n",
      "Epoch 150, Loss: 0.44735861731902055\n",
      "Epoch 151, Loss: 0.4469288720157105\n",
      "Epoch 152, Loss: 0.44655235754178146\n",
      "Epoch 153, Loss: 0.4462857402786373\n",
      "Epoch 154, Loss: 0.4460517369142754\n",
      "Epoch 155, Loss: 0.4457881796184935\n",
      "Epoch 156, Loss: 0.44542210886348255\n",
      "Epoch 157, Loss: 0.4450579467827662\n",
      "Epoch 158, Loss: 0.44473329320345206\n",
      "Epoch 159, Loss: 0.4444482514678638\n",
      "Epoch 160, Loss: 0.4441745674630603\n",
      "Epoch 161, Loss: 0.4439221813722748\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23517910770091585\n",
      "Valence RMSE: 0.24863329078633378\n",
      "Arousal RMSE: 0.22090702141569957\n",
      "Test R^2 score: tensor([0.3394, 0.6401], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4897558655214769\n",
      "Num of epochs: 162\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Epoch 121, Loss: 0.4596922103619757\n",
      "Epoch 122, Loss: 0.45943389956138125\n",
      "Epoch 123, Loss: 0.45877818894117006\n",
      "Epoch 124, Loss: 0.45746538132874076\n",
      "Epoch 125, Loss: 0.4570462803121619\n",
      "Epoch 126, Loss: 0.45696189549769234\n",
      "Epoch 127, Loss: 0.45594920341421274\n",
      "Epoch 128, Loss: 0.45536359747685007\n",
      "Epoch 129, Loss: 0.4553290890752543\n",
      "Epoch 130, Loss: 0.4547195823074053\n",
      "Epoch 131, Loss: 0.45392838323998064\n",
      "Epoch 132, Loss: 0.4537853493595631\n",
      "Epoch 133, Loss: 0.45347437050767975\n",
      "Epoch 134, Loss: 0.45267431273812453\n",
      "Epoch 135, Loss: 0.45228737659373064\n",
      "Epoch 136, Loss: 0.4522154657432981\n",
      "Epoch 137, Loss: 0.45165964487407706\n",
      "Epoch 138, Loss: 0.4510751265462795\n",
      "Epoch 139, Loss: 0.45082650445054473\n",
      "Epoch 140, Loss: 0.4506281098405297\n",
      "Epoch 141, Loss: 0.4501911419656675\n",
      "Epoch 142, Loss: 0.4497001887659326\n",
      "Epoch 143, Loss: 0.44943436133718806\n",
      "Epoch 144, Loss: 0.449226296402944\n",
      "Epoch 145, Loss: 0.44886073897867573\n",
      "Epoch 146, Loss: 0.44844710376304864\n",
      "Epoch 147, Loss: 0.44810507013116724\n",
      "Epoch 148, Loss: 0.44788545862049767\n",
      "Epoch 149, Loss: 0.4476741441009651\n",
      "Epoch 150, Loss: 0.44735861731902055\n",
      "Epoch 151, Loss: 0.4469288720157105\n",
      "Epoch 152, Loss: 0.44655235754178146\n",
      "Epoch 153, Loss: 0.4462857402786373\n",
      "Epoch 154, Loss: 0.4460517369142754\n",
      "Epoch 155, Loss: 0.4457881796184935\n",
      "Epoch 156, Loss: 0.44542210886348255\n",
      "Epoch 157, Loss: 0.4450579467827662\n",
      "Epoch 158, Loss: 0.44473329320345206\n",
      "Epoch 159, Loss: 0.4444482514678638\n",
      "Epoch 160, Loss: 0.4441745674630603\n",
      "Epoch 161, Loss: 0.4439221813722748\n",
      "Epoch 162, Loss: 0.44363357870363757\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23494339963033345\n",
      "Valence RMSE: 0.247805290562816\n",
      "Arousal RMSE: 0.2213353564815063\n",
      "Test R^2 score: tensor([0.3438, 0.6387], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.49125359515599615\n",
      "Num of epochs: 163\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Epoch 121, Loss: 0.4596922103619757\n",
      "Epoch 122, Loss: 0.45943389956138125\n",
      "Epoch 123, Loss: 0.45877818894117006\n",
      "Epoch 124, Loss: 0.45746538132874076\n",
      "Epoch 125, Loss: 0.4570462803121619\n",
      "Epoch 126, Loss: 0.45696189549769234\n",
      "Epoch 127, Loss: 0.45594920341421274\n",
      "Epoch 128, Loss: 0.45536359747685007\n",
      "Epoch 129, Loss: 0.4553290890752543\n",
      "Epoch 130, Loss: 0.4547195823074053\n",
      "Epoch 131, Loss: 0.45392838323998064\n",
      "Epoch 132, Loss: 0.4537853493595631\n",
      "Epoch 133, Loss: 0.45347437050767975\n",
      "Epoch 134, Loss: 0.45267431273812453\n",
      "Epoch 135, Loss: 0.45228737659373064\n",
      "Epoch 136, Loss: 0.4522154657432981\n",
      "Epoch 137, Loss: 0.45165964487407706\n",
      "Epoch 138, Loss: 0.4510751265462795\n",
      "Epoch 139, Loss: 0.45082650445054473\n",
      "Epoch 140, Loss: 0.4506281098405297\n",
      "Epoch 141, Loss: 0.4501911419656675\n",
      "Epoch 142, Loss: 0.4497001887659326\n",
      "Epoch 143, Loss: 0.44943436133718806\n",
      "Epoch 144, Loss: 0.449226296402944\n",
      "Epoch 145, Loss: 0.44886073897867573\n",
      "Epoch 146, Loss: 0.44844710376304864\n",
      "Epoch 147, Loss: 0.44810507013116724\n",
      "Epoch 148, Loss: 0.44788545862049767\n",
      "Epoch 149, Loss: 0.4476741441009651\n",
      "Epoch 150, Loss: 0.44735861731902055\n",
      "Epoch 151, Loss: 0.4469288720157105\n",
      "Epoch 152, Loss: 0.44655235754178146\n",
      "Epoch 153, Loss: 0.4462857402786373\n",
      "Epoch 154, Loss: 0.4460517369142754\n",
      "Epoch 155, Loss: 0.4457881796184935\n",
      "Epoch 156, Loss: 0.44542210886348255\n",
      "Epoch 157, Loss: 0.4450579467827662\n",
      "Epoch 158, Loss: 0.44473329320345206\n",
      "Epoch 159, Loss: 0.4444482514678638\n",
      "Epoch 160, Loss: 0.4441745674630603\n",
      "Epoch 161, Loss: 0.4439221813722748\n",
      "Epoch 162, Loss: 0.44363357870363757\n",
      "Epoch 163, Loss: 0.4433189071134186\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23538804573034028\n",
      "Valence RMSE: 0.24878598409547495\n",
      "Arousal RMSE: 0.22118001325423506\n",
      "Test R^2 score: tensor([0.3386, 0.6392], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.48890502415106357\n",
      "Num of epochs: 164\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Epoch 121, Loss: 0.4596922103619757\n",
      "Epoch 122, Loss: 0.45943389956138125\n",
      "Epoch 123, Loss: 0.45877818894117006\n",
      "Epoch 124, Loss: 0.45746538132874076\n",
      "Epoch 125, Loss: 0.4570462803121619\n",
      "Epoch 126, Loss: 0.45696189549769234\n",
      "Epoch 127, Loss: 0.45594920341421274\n",
      "Epoch 128, Loss: 0.45536359747685007\n",
      "Epoch 129, Loss: 0.4553290890752543\n",
      "Epoch 130, Loss: 0.4547195823074053\n",
      "Epoch 131, Loss: 0.45392838323998064\n",
      "Epoch 132, Loss: 0.4537853493595631\n",
      "Epoch 133, Loss: 0.45347437050767975\n",
      "Epoch 134, Loss: 0.45267431273812453\n",
      "Epoch 135, Loss: 0.45228737659373064\n",
      "Epoch 136, Loss: 0.4522154657432981\n",
      "Epoch 137, Loss: 0.45165964487407706\n",
      "Epoch 138, Loss: 0.4510751265462795\n",
      "Epoch 139, Loss: 0.45082650445054473\n",
      "Epoch 140, Loss: 0.4506281098405297\n",
      "Epoch 141, Loss: 0.4501911419656675\n",
      "Epoch 142, Loss: 0.4497001887659326\n",
      "Epoch 143, Loss: 0.44943436133718806\n",
      "Epoch 144, Loss: 0.449226296402944\n",
      "Epoch 145, Loss: 0.44886073897867573\n",
      "Epoch 146, Loss: 0.44844710376304864\n",
      "Epoch 147, Loss: 0.44810507013116724\n",
      "Epoch 148, Loss: 0.44788545862049767\n",
      "Epoch 149, Loss: 0.4476741441009651\n",
      "Epoch 150, Loss: 0.44735861731902055\n",
      "Epoch 151, Loss: 0.4469288720157105\n",
      "Epoch 152, Loss: 0.44655235754178146\n",
      "Epoch 153, Loss: 0.4462857402786373\n",
      "Epoch 154, Loss: 0.4460517369142754\n",
      "Epoch 155, Loss: 0.4457881796184935\n",
      "Epoch 156, Loss: 0.44542210886348255\n",
      "Epoch 157, Loss: 0.4450579467827662\n",
      "Epoch 158, Loss: 0.44473329320345206\n",
      "Epoch 159, Loss: 0.4444482514678638\n",
      "Epoch 160, Loss: 0.4441745674630603\n",
      "Epoch 161, Loss: 0.4439221813722748\n",
      "Epoch 162, Loss: 0.44363357870363757\n",
      "Epoch 163, Loss: 0.4433189071134186\n",
      "Epoch 164, Loss: 0.44295452976444744\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23520921372994882\n",
      "Valence RMSE: 0.24824735022451674\n",
      "Arousal RMSE: 0.22140461050625754\n",
      "Test R^2 score: tensor([0.3415, 0.6385], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4899688954229375\n",
      "Num of epochs: 165\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Epoch 121, Loss: 0.4596922103619757\n",
      "Epoch 122, Loss: 0.45943389956138125\n",
      "Epoch 123, Loss: 0.45877818894117006\n",
      "Epoch 124, Loss: 0.45746538132874076\n",
      "Epoch 125, Loss: 0.4570462803121619\n",
      "Epoch 126, Loss: 0.45696189549769234\n",
      "Epoch 127, Loss: 0.45594920341421274\n",
      "Epoch 128, Loss: 0.45536359747685007\n",
      "Epoch 129, Loss: 0.4553290890752543\n",
      "Epoch 130, Loss: 0.4547195823074053\n",
      "Epoch 131, Loss: 0.45392838323998064\n",
      "Epoch 132, Loss: 0.4537853493595631\n",
      "Epoch 133, Loss: 0.45347437050767975\n",
      "Epoch 134, Loss: 0.45267431273812453\n",
      "Epoch 135, Loss: 0.45228737659373064\n",
      "Epoch 136, Loss: 0.4522154657432981\n",
      "Epoch 137, Loss: 0.45165964487407706\n",
      "Epoch 138, Loss: 0.4510751265462795\n",
      "Epoch 139, Loss: 0.45082650445054473\n",
      "Epoch 140, Loss: 0.4506281098405297\n",
      "Epoch 141, Loss: 0.4501911419656675\n",
      "Epoch 142, Loss: 0.4497001887659326\n",
      "Epoch 143, Loss: 0.44943436133718806\n",
      "Epoch 144, Loss: 0.449226296402944\n",
      "Epoch 145, Loss: 0.44886073897867573\n",
      "Epoch 146, Loss: 0.44844710376304864\n",
      "Epoch 147, Loss: 0.44810507013116724\n",
      "Epoch 148, Loss: 0.44788545862049767\n",
      "Epoch 149, Loss: 0.4476741441009651\n",
      "Epoch 150, Loss: 0.44735861731902055\n",
      "Epoch 151, Loss: 0.4469288720157105\n",
      "Epoch 152, Loss: 0.44655235754178146\n",
      "Epoch 153, Loss: 0.4462857402786373\n",
      "Epoch 154, Loss: 0.4460517369142754\n",
      "Epoch 155, Loss: 0.4457881796184935\n",
      "Epoch 156, Loss: 0.44542210886348255\n",
      "Epoch 157, Loss: 0.4450579467827662\n",
      "Epoch 158, Loss: 0.44473329320345206\n",
      "Epoch 159, Loss: 0.4444482514678638\n",
      "Epoch 160, Loss: 0.4441745674630603\n",
      "Epoch 161, Loss: 0.4439221813722748\n",
      "Epoch 162, Loss: 0.44363357870363757\n",
      "Epoch 163, Loss: 0.4433189071134186\n",
      "Epoch 164, Loss: 0.44295452976444744\n",
      "Epoch 165, Loss: 0.4426058108229731\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23552989826490037\n",
      "Valence RMSE: 0.24888523944548038\n",
      "Arousal RMSE: 0.22137028603567038\n",
      "Test R^2 score: tensor([0.3381, 0.6386], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.488330593941551\n",
      "Num of epochs: 166\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Epoch 121, Loss: 0.4596922103619757\n",
      "Epoch 122, Loss: 0.45943389956138125\n",
      "Epoch 123, Loss: 0.45877818894117006\n",
      "Epoch 124, Loss: 0.45746538132874076\n",
      "Epoch 125, Loss: 0.4570462803121619\n",
      "Epoch 126, Loss: 0.45696189549769234\n",
      "Epoch 127, Loss: 0.45594920341421274\n",
      "Epoch 128, Loss: 0.45536359747685007\n",
      "Epoch 129, Loss: 0.4553290890752543\n",
      "Epoch 130, Loss: 0.4547195823074053\n",
      "Epoch 131, Loss: 0.45392838323998064\n",
      "Epoch 132, Loss: 0.4537853493595631\n",
      "Epoch 133, Loss: 0.45347437050767975\n",
      "Epoch 134, Loss: 0.45267431273812453\n",
      "Epoch 135, Loss: 0.45228737659373064\n",
      "Epoch 136, Loss: 0.4522154657432981\n",
      "Epoch 137, Loss: 0.45165964487407706\n",
      "Epoch 138, Loss: 0.4510751265462795\n",
      "Epoch 139, Loss: 0.45082650445054473\n",
      "Epoch 140, Loss: 0.4506281098405297\n",
      "Epoch 141, Loss: 0.4501911419656675\n",
      "Epoch 142, Loss: 0.4497001887659326\n",
      "Epoch 143, Loss: 0.44943436133718806\n",
      "Epoch 144, Loss: 0.449226296402944\n",
      "Epoch 145, Loss: 0.44886073897867573\n",
      "Epoch 146, Loss: 0.44844710376304864\n",
      "Epoch 147, Loss: 0.44810507013116724\n",
      "Epoch 148, Loss: 0.44788545862049767\n",
      "Epoch 149, Loss: 0.4476741441009651\n",
      "Epoch 150, Loss: 0.44735861731902055\n",
      "Epoch 151, Loss: 0.4469288720157105\n",
      "Epoch 152, Loss: 0.44655235754178146\n",
      "Epoch 153, Loss: 0.4462857402786373\n",
      "Epoch 154, Loss: 0.4460517369142754\n",
      "Epoch 155, Loss: 0.4457881796184935\n",
      "Epoch 156, Loss: 0.44542210886348255\n",
      "Epoch 157, Loss: 0.4450579467827662\n",
      "Epoch 158, Loss: 0.44473329320345206\n",
      "Epoch 159, Loss: 0.4444482514678638\n",
      "Epoch 160, Loss: 0.4441745674630603\n",
      "Epoch 161, Loss: 0.4439221813722748\n",
      "Epoch 162, Loss: 0.44363357870363757\n",
      "Epoch 163, Loss: 0.4433189071134186\n",
      "Epoch 164, Loss: 0.44295452976444744\n",
      "Epoch 165, Loss: 0.4426058108229731\n",
      "Epoch 166, Loss: 0.4422485787900922\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23549362294020545\n",
      "Valence RMSE: 0.24861703367669202\n",
      "Arousal RMSE: 0.22159436693384132\n",
      "Test R^2 score: tensor([0.3395, 0.6379], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.48867750560592643\n",
      "Num of epochs: 167\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Epoch 121, Loss: 0.4596922103619757\n",
      "Epoch 122, Loss: 0.45943389956138125\n",
      "Epoch 123, Loss: 0.45877818894117006\n",
      "Epoch 124, Loss: 0.45746538132874076\n",
      "Epoch 125, Loss: 0.4570462803121619\n",
      "Epoch 126, Loss: 0.45696189549769234\n",
      "Epoch 127, Loss: 0.45594920341421274\n",
      "Epoch 128, Loss: 0.45536359747685007\n",
      "Epoch 129, Loss: 0.4553290890752543\n",
      "Epoch 130, Loss: 0.4547195823074053\n",
      "Epoch 131, Loss: 0.45392838323998064\n",
      "Epoch 132, Loss: 0.4537853493595631\n",
      "Epoch 133, Loss: 0.45347437050767975\n",
      "Epoch 134, Loss: 0.45267431273812453\n",
      "Epoch 135, Loss: 0.45228737659373064\n",
      "Epoch 136, Loss: 0.4522154657432981\n",
      "Epoch 137, Loss: 0.45165964487407706\n",
      "Epoch 138, Loss: 0.4510751265462795\n",
      "Epoch 139, Loss: 0.45082650445054473\n",
      "Epoch 140, Loss: 0.4506281098405297\n",
      "Epoch 141, Loss: 0.4501911419656675\n",
      "Epoch 142, Loss: 0.4497001887659326\n",
      "Epoch 143, Loss: 0.44943436133718806\n",
      "Epoch 144, Loss: 0.449226296402944\n",
      "Epoch 145, Loss: 0.44886073897867573\n",
      "Epoch 146, Loss: 0.44844710376304864\n",
      "Epoch 147, Loss: 0.44810507013116724\n",
      "Epoch 148, Loss: 0.44788545862049767\n",
      "Epoch 149, Loss: 0.4476741441009651\n",
      "Epoch 150, Loss: 0.44735861731902055\n",
      "Epoch 151, Loss: 0.4469288720157105\n",
      "Epoch 152, Loss: 0.44655235754178146\n",
      "Epoch 153, Loss: 0.4462857402786373\n",
      "Epoch 154, Loss: 0.4460517369142754\n",
      "Epoch 155, Loss: 0.4457881796184935\n",
      "Epoch 156, Loss: 0.44542210886348255\n",
      "Epoch 157, Loss: 0.4450579467827662\n",
      "Epoch 158, Loss: 0.44473329320345206\n",
      "Epoch 159, Loss: 0.4444482514678638\n",
      "Epoch 160, Loss: 0.4441745674630603\n",
      "Epoch 161, Loss: 0.4439221813722748\n",
      "Epoch 162, Loss: 0.44363357870363757\n",
      "Epoch 163, Loss: 0.4433189071134186\n",
      "Epoch 164, Loss: 0.44295452976444744\n",
      "Epoch 165, Loss: 0.4426058108229731\n",
      "Epoch 166, Loss: 0.4422485787900922\n",
      "Epoch 167, Loss: 0.4419090985167976\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23563840585346435\n",
      "Valence RMSE: 0.2487257320460931\n",
      "Arousal RMSE: 0.22178013176220057\n",
      "Test R^2 score: tensor([0.3389, 0.6373], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.48808494861770935\n",
      "Num of epochs: 168\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Epoch 121, Loss: 0.4596922103619757\n",
      "Epoch 122, Loss: 0.45943389956138125\n",
      "Epoch 123, Loss: 0.45877818894117006\n",
      "Epoch 124, Loss: 0.45746538132874076\n",
      "Epoch 125, Loss: 0.4570462803121619\n",
      "Epoch 126, Loss: 0.45696189549769234\n",
      "Epoch 127, Loss: 0.45594920341421274\n",
      "Epoch 128, Loss: 0.45536359747685007\n",
      "Epoch 129, Loss: 0.4553290890752543\n",
      "Epoch 130, Loss: 0.4547195823074053\n",
      "Epoch 131, Loss: 0.45392838323998064\n",
      "Epoch 132, Loss: 0.4537853493595631\n",
      "Epoch 133, Loss: 0.45347437050767975\n",
      "Epoch 134, Loss: 0.45267431273812453\n",
      "Epoch 135, Loss: 0.45228737659373064\n",
      "Epoch 136, Loss: 0.4522154657432981\n",
      "Epoch 137, Loss: 0.45165964487407706\n",
      "Epoch 138, Loss: 0.4510751265462795\n",
      "Epoch 139, Loss: 0.45082650445054473\n",
      "Epoch 140, Loss: 0.4506281098405297\n",
      "Epoch 141, Loss: 0.4501911419656675\n",
      "Epoch 142, Loss: 0.4497001887659326\n",
      "Epoch 143, Loss: 0.44943436133718806\n",
      "Epoch 144, Loss: 0.449226296402944\n",
      "Epoch 145, Loss: 0.44886073897867573\n",
      "Epoch 146, Loss: 0.44844710376304864\n",
      "Epoch 147, Loss: 0.44810507013116724\n",
      "Epoch 148, Loss: 0.44788545862049767\n",
      "Epoch 149, Loss: 0.4476741441009651\n",
      "Epoch 150, Loss: 0.44735861731902055\n",
      "Epoch 151, Loss: 0.4469288720157105\n",
      "Epoch 152, Loss: 0.44655235754178146\n",
      "Epoch 153, Loss: 0.4462857402786373\n",
      "Epoch 154, Loss: 0.4460517369142754\n",
      "Epoch 155, Loss: 0.4457881796184935\n",
      "Epoch 156, Loss: 0.44542210886348255\n",
      "Epoch 157, Loss: 0.4450579467827662\n",
      "Epoch 158, Loss: 0.44473329320345206\n",
      "Epoch 159, Loss: 0.4444482514678638\n",
      "Epoch 160, Loss: 0.4441745674630603\n",
      "Epoch 161, Loss: 0.4439221813722748\n",
      "Epoch 162, Loss: 0.44363357870363757\n",
      "Epoch 163, Loss: 0.4433189071134186\n",
      "Epoch 164, Loss: 0.44295452976444744\n",
      "Epoch 165, Loss: 0.4426058108229731\n",
      "Epoch 166, Loss: 0.4422485787900922\n",
      "Epoch 167, Loss: 0.4419090985167976\n",
      "Epoch 168, Loss: 0.44158054387831963\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2357418415592914\n",
      "Valence RMSE: 0.24877312473090077\n",
      "Arousal RMSE: 0.2219467596860918\n",
      "Test R^2 score: tensor([0.3387, 0.6367], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.48768633048499305\n",
      "Num of epochs: 169\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Epoch 121, Loss: 0.4596922103619757\n",
      "Epoch 122, Loss: 0.45943389956138125\n",
      "Epoch 123, Loss: 0.45877818894117006\n",
      "Epoch 124, Loss: 0.45746538132874076\n",
      "Epoch 125, Loss: 0.4570462803121619\n",
      "Epoch 126, Loss: 0.45696189549769234\n",
      "Epoch 127, Loss: 0.45594920341421274\n",
      "Epoch 128, Loss: 0.45536359747685007\n",
      "Epoch 129, Loss: 0.4553290890752543\n",
      "Epoch 130, Loss: 0.4547195823074053\n",
      "Epoch 131, Loss: 0.45392838323998064\n",
      "Epoch 132, Loss: 0.4537853493595631\n",
      "Epoch 133, Loss: 0.45347437050767975\n",
      "Epoch 134, Loss: 0.45267431273812453\n",
      "Epoch 135, Loss: 0.45228737659373064\n",
      "Epoch 136, Loss: 0.4522154657432981\n",
      "Epoch 137, Loss: 0.45165964487407706\n",
      "Epoch 138, Loss: 0.4510751265462795\n",
      "Epoch 139, Loss: 0.45082650445054473\n",
      "Epoch 140, Loss: 0.4506281098405297\n",
      "Epoch 141, Loss: 0.4501911419656675\n",
      "Epoch 142, Loss: 0.4497001887659326\n",
      "Epoch 143, Loss: 0.44943436133718806\n",
      "Epoch 144, Loss: 0.449226296402944\n",
      "Epoch 145, Loss: 0.44886073897867573\n",
      "Epoch 146, Loss: 0.44844710376304864\n",
      "Epoch 147, Loss: 0.44810507013116724\n",
      "Epoch 148, Loss: 0.44788545862049767\n",
      "Epoch 149, Loss: 0.4476741441009651\n",
      "Epoch 150, Loss: 0.44735861731902055\n",
      "Epoch 151, Loss: 0.4469288720157105\n",
      "Epoch 152, Loss: 0.44655235754178146\n",
      "Epoch 153, Loss: 0.4462857402786373\n",
      "Epoch 154, Loss: 0.4460517369142754\n",
      "Epoch 155, Loss: 0.4457881796184935\n",
      "Epoch 156, Loss: 0.44542210886348255\n",
      "Epoch 157, Loss: 0.4450579467827662\n",
      "Epoch 158, Loss: 0.44473329320345206\n",
      "Epoch 159, Loss: 0.4444482514678638\n",
      "Epoch 160, Loss: 0.4441745674630603\n",
      "Epoch 161, Loss: 0.4439221813722748\n",
      "Epoch 162, Loss: 0.44363357870363757\n",
      "Epoch 163, Loss: 0.4433189071134186\n",
      "Epoch 164, Loss: 0.44295452976444744\n",
      "Epoch 165, Loss: 0.4426058108229731\n",
      "Epoch 166, Loss: 0.4422485787900922\n",
      "Epoch 167, Loss: 0.4419090985167976\n",
      "Epoch 168, Loss: 0.44158054387831963\n",
      "Epoch 169, Loss: 0.4412474388772268\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23580674405875407\n",
      "Valence RMSE: 0.24866582020297157\n",
      "Arousal RMSE: 0.22220475006166024\n",
      "Test R^2 score: tensor([0.3392, 0.6359], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4875489889472562\n",
      "Num of epochs: 170\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Epoch 121, Loss: 0.4596922103619757\n",
      "Epoch 122, Loss: 0.45943389956138125\n",
      "Epoch 123, Loss: 0.45877818894117006\n",
      "Epoch 124, Loss: 0.45746538132874076\n",
      "Epoch 125, Loss: 0.4570462803121619\n",
      "Epoch 126, Loss: 0.45696189549769234\n",
      "Epoch 127, Loss: 0.45594920341421274\n",
      "Epoch 128, Loss: 0.45536359747685007\n",
      "Epoch 129, Loss: 0.4553290890752543\n",
      "Epoch 130, Loss: 0.4547195823074053\n",
      "Epoch 131, Loss: 0.45392838323998064\n",
      "Epoch 132, Loss: 0.4537853493595631\n",
      "Epoch 133, Loss: 0.45347437050767975\n",
      "Epoch 134, Loss: 0.45267431273812453\n",
      "Epoch 135, Loss: 0.45228737659373064\n",
      "Epoch 136, Loss: 0.4522154657432981\n",
      "Epoch 137, Loss: 0.45165964487407706\n",
      "Epoch 138, Loss: 0.4510751265462795\n",
      "Epoch 139, Loss: 0.45082650445054473\n",
      "Epoch 140, Loss: 0.4506281098405297\n",
      "Epoch 141, Loss: 0.4501911419656675\n",
      "Epoch 142, Loss: 0.4497001887659326\n",
      "Epoch 143, Loss: 0.44943436133718806\n",
      "Epoch 144, Loss: 0.449226296402944\n",
      "Epoch 145, Loss: 0.44886073897867573\n",
      "Epoch 146, Loss: 0.44844710376304864\n",
      "Epoch 147, Loss: 0.44810507013116724\n",
      "Epoch 148, Loss: 0.44788545862049767\n",
      "Epoch 149, Loss: 0.4476741441009651\n",
      "Epoch 150, Loss: 0.44735861731902055\n",
      "Epoch 151, Loss: 0.4469288720157105\n",
      "Epoch 152, Loss: 0.44655235754178146\n",
      "Epoch 153, Loss: 0.4462857402786373\n",
      "Epoch 154, Loss: 0.4460517369142754\n",
      "Epoch 155, Loss: 0.4457881796184935\n",
      "Epoch 156, Loss: 0.44542210886348255\n",
      "Epoch 157, Loss: 0.4450579467827662\n",
      "Epoch 158, Loss: 0.44473329320345206\n",
      "Epoch 159, Loss: 0.4444482514678638\n",
      "Epoch 160, Loss: 0.4441745674630603\n",
      "Epoch 161, Loss: 0.4439221813722748\n",
      "Epoch 162, Loss: 0.44363357870363757\n",
      "Epoch 163, Loss: 0.4433189071134186\n",
      "Epoch 164, Loss: 0.44295452976444744\n",
      "Epoch 165, Loss: 0.4426058108229731\n",
      "Epoch 166, Loss: 0.4422485787900922\n",
      "Epoch 167, Loss: 0.4419090985167976\n",
      "Epoch 168, Loss: 0.44158054387831963\n",
      "Epoch 169, Loss: 0.4412474388772268\n",
      "Epoch 170, Loss: 0.4409277694136744\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23624832984641295\n",
      "Valence RMSE: 0.24945160716568338\n",
      "Arousal RMSE: 0.22226210291657153\n",
      "Test R^2 score: tensor([0.3351, 0.6357], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.48536366921688656\n",
      "Num of epochs: 171\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Epoch 121, Loss: 0.4596922103619757\n",
      "Epoch 122, Loss: 0.45943389956138125\n",
      "Epoch 123, Loss: 0.45877818894117006\n",
      "Epoch 124, Loss: 0.45746538132874076\n",
      "Epoch 125, Loss: 0.4570462803121619\n",
      "Epoch 126, Loss: 0.45696189549769234\n",
      "Epoch 127, Loss: 0.45594920341421274\n",
      "Epoch 128, Loss: 0.45536359747685007\n",
      "Epoch 129, Loss: 0.4553290890752543\n",
      "Epoch 130, Loss: 0.4547195823074053\n",
      "Epoch 131, Loss: 0.45392838323998064\n",
      "Epoch 132, Loss: 0.4537853493595631\n",
      "Epoch 133, Loss: 0.45347437050767975\n",
      "Epoch 134, Loss: 0.45267431273812453\n",
      "Epoch 135, Loss: 0.45228737659373064\n",
      "Epoch 136, Loss: 0.4522154657432981\n",
      "Epoch 137, Loss: 0.45165964487407706\n",
      "Epoch 138, Loss: 0.4510751265462795\n",
      "Epoch 139, Loss: 0.45082650445054473\n",
      "Epoch 140, Loss: 0.4506281098405297\n",
      "Epoch 141, Loss: 0.4501911419656675\n",
      "Epoch 142, Loss: 0.4497001887659326\n",
      "Epoch 143, Loss: 0.44943436133718806\n",
      "Epoch 144, Loss: 0.449226296402944\n",
      "Epoch 145, Loss: 0.44886073897867573\n",
      "Epoch 146, Loss: 0.44844710376304864\n",
      "Epoch 147, Loss: 0.44810507013116724\n",
      "Epoch 148, Loss: 0.44788545862049767\n",
      "Epoch 149, Loss: 0.4476741441009651\n",
      "Epoch 150, Loss: 0.44735861731902055\n",
      "Epoch 151, Loss: 0.4469288720157105\n",
      "Epoch 152, Loss: 0.44655235754178146\n",
      "Epoch 153, Loss: 0.4462857402786373\n",
      "Epoch 154, Loss: 0.4460517369142754\n",
      "Epoch 155, Loss: 0.4457881796184935\n",
      "Epoch 156, Loss: 0.44542210886348255\n",
      "Epoch 157, Loss: 0.4450579467827662\n",
      "Epoch 158, Loss: 0.44473329320345206\n",
      "Epoch 159, Loss: 0.4444482514678638\n",
      "Epoch 160, Loss: 0.4441745674630603\n",
      "Epoch 161, Loss: 0.4439221813722748\n",
      "Epoch 162, Loss: 0.44363357870363757\n",
      "Epoch 163, Loss: 0.4433189071134186\n",
      "Epoch 164, Loss: 0.44295452976444744\n",
      "Epoch 165, Loss: 0.4426058108229731\n",
      "Epoch 166, Loss: 0.4422485787900922\n",
      "Epoch 167, Loss: 0.4419090985167976\n",
      "Epoch 168, Loss: 0.44158054387831963\n",
      "Epoch 169, Loss: 0.4412474388772268\n",
      "Epoch 170, Loss: 0.4409277694136744\n",
      "Epoch 171, Loss: 0.44068045571539705\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23622933406559188\n",
      "Valence RMSE: 0.24869698379441993\n",
      "Arousal RMSE: 0.22306592477943385\n",
      "Test R^2 score: tensor([0.3391, 0.6330], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.48605218918957394\n",
      "Num of epochs: 172\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Epoch 121, Loss: 0.4596922103619757\n",
      "Epoch 122, Loss: 0.45943389956138125\n",
      "Epoch 123, Loss: 0.45877818894117006\n",
      "Epoch 124, Loss: 0.45746538132874076\n",
      "Epoch 125, Loss: 0.4570462803121619\n",
      "Epoch 126, Loss: 0.45696189549769234\n",
      "Epoch 127, Loss: 0.45594920341421274\n",
      "Epoch 128, Loss: 0.45536359747685007\n",
      "Epoch 129, Loss: 0.4553290890752543\n",
      "Epoch 130, Loss: 0.4547195823074053\n",
      "Epoch 131, Loss: 0.45392838323998064\n",
      "Epoch 132, Loss: 0.4537853493595631\n",
      "Epoch 133, Loss: 0.45347437050767975\n",
      "Epoch 134, Loss: 0.45267431273812453\n",
      "Epoch 135, Loss: 0.45228737659373064\n",
      "Epoch 136, Loss: 0.4522154657432981\n",
      "Epoch 137, Loss: 0.45165964487407706\n",
      "Epoch 138, Loss: 0.4510751265462795\n",
      "Epoch 139, Loss: 0.45082650445054473\n",
      "Epoch 140, Loss: 0.4506281098405297\n",
      "Epoch 141, Loss: 0.4501911419656675\n",
      "Epoch 142, Loss: 0.4497001887659326\n",
      "Epoch 143, Loss: 0.44943436133718806\n",
      "Epoch 144, Loss: 0.449226296402944\n",
      "Epoch 145, Loss: 0.44886073897867573\n",
      "Epoch 146, Loss: 0.44844710376304864\n",
      "Epoch 147, Loss: 0.44810507013116724\n",
      "Epoch 148, Loss: 0.44788545862049767\n",
      "Epoch 149, Loss: 0.4476741441009651\n",
      "Epoch 150, Loss: 0.44735861731902055\n",
      "Epoch 151, Loss: 0.4469288720157105\n",
      "Epoch 152, Loss: 0.44655235754178146\n",
      "Epoch 153, Loss: 0.4462857402786373\n",
      "Epoch 154, Loss: 0.4460517369142754\n",
      "Epoch 155, Loss: 0.4457881796184935\n",
      "Epoch 156, Loss: 0.44542210886348255\n",
      "Epoch 157, Loss: 0.4450579467827662\n",
      "Epoch 158, Loss: 0.44473329320345206\n",
      "Epoch 159, Loss: 0.4444482514678638\n",
      "Epoch 160, Loss: 0.4441745674630603\n",
      "Epoch 161, Loss: 0.4439221813722748\n",
      "Epoch 162, Loss: 0.44363357870363757\n",
      "Epoch 163, Loss: 0.4433189071134186\n",
      "Epoch 164, Loss: 0.44295452976444744\n",
      "Epoch 165, Loss: 0.4426058108229731\n",
      "Epoch 166, Loss: 0.4422485787900922\n",
      "Epoch 167, Loss: 0.4419090985167976\n",
      "Epoch 168, Loss: 0.44158054387831963\n",
      "Epoch 169, Loss: 0.4412474388772268\n",
      "Epoch 170, Loss: 0.4409277694136744\n",
      "Epoch 171, Loss: 0.44068045571539705\n",
      "Epoch 172, Loss: 0.4405979418732049\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23745911774017694\n",
      "Valence RMSE: 0.25099258055024964\n",
      "Arousal RMSE: 0.22310622964097807\n",
      "Test R^2 score: tensor([0.3268, 0.6329], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4798570143106072\n",
      "Num of epochs: 173\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Epoch 121, Loss: 0.4596922103619757\n",
      "Epoch 122, Loss: 0.45943389956138125\n",
      "Epoch 123, Loss: 0.45877818894117006\n",
      "Epoch 124, Loss: 0.45746538132874076\n",
      "Epoch 125, Loss: 0.4570462803121619\n",
      "Epoch 126, Loss: 0.45696189549769234\n",
      "Epoch 127, Loss: 0.45594920341421274\n",
      "Epoch 128, Loss: 0.45536359747685007\n",
      "Epoch 129, Loss: 0.4553290890752543\n",
      "Epoch 130, Loss: 0.4547195823074053\n",
      "Epoch 131, Loss: 0.45392838323998064\n",
      "Epoch 132, Loss: 0.4537853493595631\n",
      "Epoch 133, Loss: 0.45347437050767975\n",
      "Epoch 134, Loss: 0.45267431273812453\n",
      "Epoch 135, Loss: 0.45228737659373064\n",
      "Epoch 136, Loss: 0.4522154657432981\n",
      "Epoch 137, Loss: 0.45165964487407706\n",
      "Epoch 138, Loss: 0.4510751265462795\n",
      "Epoch 139, Loss: 0.45082650445054473\n",
      "Epoch 140, Loss: 0.4506281098405297\n",
      "Epoch 141, Loss: 0.4501911419656675\n",
      "Epoch 142, Loss: 0.4497001887659326\n",
      "Epoch 143, Loss: 0.44943436133718806\n",
      "Epoch 144, Loss: 0.449226296402944\n",
      "Epoch 145, Loss: 0.44886073897867573\n",
      "Epoch 146, Loss: 0.44844710376304864\n",
      "Epoch 147, Loss: 0.44810507013116724\n",
      "Epoch 148, Loss: 0.44788545862049767\n",
      "Epoch 149, Loss: 0.4476741441009651\n",
      "Epoch 150, Loss: 0.44735861731902055\n",
      "Epoch 151, Loss: 0.4469288720157105\n",
      "Epoch 152, Loss: 0.44655235754178146\n",
      "Epoch 153, Loss: 0.4462857402786373\n",
      "Epoch 154, Loss: 0.4460517369142754\n",
      "Epoch 155, Loss: 0.4457881796184935\n",
      "Epoch 156, Loss: 0.44542210886348255\n",
      "Epoch 157, Loss: 0.4450579467827662\n",
      "Epoch 158, Loss: 0.44473329320345206\n",
      "Epoch 159, Loss: 0.4444482514678638\n",
      "Epoch 160, Loss: 0.4441745674630603\n",
      "Epoch 161, Loss: 0.4439221813722748\n",
      "Epoch 162, Loss: 0.44363357870363757\n",
      "Epoch 163, Loss: 0.4433189071134186\n",
      "Epoch 164, Loss: 0.44295452976444744\n",
      "Epoch 165, Loss: 0.4426058108229731\n",
      "Epoch 166, Loss: 0.4422485787900922\n",
      "Epoch 167, Loss: 0.4419090985167976\n",
      "Epoch 168, Loss: 0.44158054387831963\n",
      "Epoch 169, Loss: 0.4412474388772268\n",
      "Epoch 170, Loss: 0.4409277694136744\n",
      "Epoch 171, Loss: 0.44068045571539705\n",
      "Epoch 172, Loss: 0.4405979418732049\n",
      "Epoch 173, Loss: 0.44094101686109266\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2371648532379737\n",
      "Valence RMSE: 0.2485910771260821\n",
      "Arousal RMSE: 0.2251595247731561\n",
      "Test R^2 score: tensor([0.3396, 0.6261], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.48287323516409864\n",
      "Num of epochs: 174\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Epoch 121, Loss: 0.4596922103619757\n",
      "Epoch 122, Loss: 0.45943389956138125\n",
      "Epoch 123, Loss: 0.45877818894117006\n",
      "Epoch 124, Loss: 0.45746538132874076\n",
      "Epoch 125, Loss: 0.4570462803121619\n",
      "Epoch 126, Loss: 0.45696189549769234\n",
      "Epoch 127, Loss: 0.45594920341421274\n",
      "Epoch 128, Loss: 0.45536359747685007\n",
      "Epoch 129, Loss: 0.4553290890752543\n",
      "Epoch 130, Loss: 0.4547195823074053\n",
      "Epoch 131, Loss: 0.45392838323998064\n",
      "Epoch 132, Loss: 0.4537853493595631\n",
      "Epoch 133, Loss: 0.45347437050767975\n",
      "Epoch 134, Loss: 0.45267431273812453\n",
      "Epoch 135, Loss: 0.45228737659373064\n",
      "Epoch 136, Loss: 0.4522154657432981\n",
      "Epoch 137, Loss: 0.45165964487407706\n",
      "Epoch 138, Loss: 0.4510751265462795\n",
      "Epoch 139, Loss: 0.45082650445054473\n",
      "Epoch 140, Loss: 0.4506281098405297\n",
      "Epoch 141, Loss: 0.4501911419656675\n",
      "Epoch 142, Loss: 0.4497001887659326\n",
      "Epoch 143, Loss: 0.44943436133718806\n",
      "Epoch 144, Loss: 0.449226296402944\n",
      "Epoch 145, Loss: 0.44886073897867573\n",
      "Epoch 146, Loss: 0.44844710376304864\n",
      "Epoch 147, Loss: 0.44810507013116724\n",
      "Epoch 148, Loss: 0.44788545862049767\n",
      "Epoch 149, Loss: 0.4476741441009651\n",
      "Epoch 150, Loss: 0.44735861731902055\n",
      "Epoch 151, Loss: 0.4469288720157105\n",
      "Epoch 152, Loss: 0.44655235754178146\n",
      "Epoch 153, Loss: 0.4462857402786373\n",
      "Epoch 154, Loss: 0.4460517369142754\n",
      "Epoch 155, Loss: 0.4457881796184935\n",
      "Epoch 156, Loss: 0.44542210886348255\n",
      "Epoch 157, Loss: 0.4450579467827662\n",
      "Epoch 158, Loss: 0.44473329320345206\n",
      "Epoch 159, Loss: 0.4444482514678638\n",
      "Epoch 160, Loss: 0.4441745674630603\n",
      "Epoch 161, Loss: 0.4439221813722748\n",
      "Epoch 162, Loss: 0.44363357870363757\n",
      "Epoch 163, Loss: 0.4433189071134186\n",
      "Epoch 164, Loss: 0.44295452976444744\n",
      "Epoch 165, Loss: 0.4426058108229731\n",
      "Epoch 166, Loss: 0.4422485787900922\n",
      "Epoch 167, Loss: 0.4419090985167976\n",
      "Epoch 168, Loss: 0.44158054387831963\n",
      "Epoch 169, Loss: 0.4412474388772268\n",
      "Epoch 170, Loss: 0.4409277694136744\n",
      "Epoch 171, Loss: 0.44068045571539705\n",
      "Epoch 172, Loss: 0.4405979418732049\n",
      "Epoch 173, Loss: 0.44094101686109266\n",
      "Epoch 174, Loss: 0.4417368233339534\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23826508516493156\n",
      "Valence RMSE: 0.25174250943045484\n",
      "Arousal RMSE: 0.22397814751212125\n",
      "Test R^2 score: tensor([0.3228, 0.6300], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4764051689700285\n",
      "Num of epochs: 175\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Epoch 121, Loss: 0.4596922103619757\n",
      "Epoch 122, Loss: 0.45943389956138125\n",
      "Epoch 123, Loss: 0.45877818894117006\n",
      "Epoch 124, Loss: 0.45746538132874076\n",
      "Epoch 125, Loss: 0.4570462803121619\n",
      "Epoch 126, Loss: 0.45696189549769234\n",
      "Epoch 127, Loss: 0.45594920341421274\n",
      "Epoch 128, Loss: 0.45536359747685007\n",
      "Epoch 129, Loss: 0.4553290890752543\n",
      "Epoch 130, Loss: 0.4547195823074053\n",
      "Epoch 131, Loss: 0.45392838323998064\n",
      "Epoch 132, Loss: 0.4537853493595631\n",
      "Epoch 133, Loss: 0.45347437050767975\n",
      "Epoch 134, Loss: 0.45267431273812453\n",
      "Epoch 135, Loss: 0.45228737659373064\n",
      "Epoch 136, Loss: 0.4522154657432981\n",
      "Epoch 137, Loss: 0.45165964487407706\n",
      "Epoch 138, Loss: 0.4510751265462795\n",
      "Epoch 139, Loss: 0.45082650445054473\n",
      "Epoch 140, Loss: 0.4506281098405297\n",
      "Epoch 141, Loss: 0.4501911419656675\n",
      "Epoch 142, Loss: 0.4497001887659326\n",
      "Epoch 143, Loss: 0.44943436133718806\n",
      "Epoch 144, Loss: 0.449226296402944\n",
      "Epoch 145, Loss: 0.44886073897867573\n",
      "Epoch 146, Loss: 0.44844710376304864\n",
      "Epoch 147, Loss: 0.44810507013116724\n",
      "Epoch 148, Loss: 0.44788545862049767\n",
      "Epoch 149, Loss: 0.4476741441009651\n",
      "Epoch 150, Loss: 0.44735861731902055\n",
      "Epoch 151, Loss: 0.4469288720157105\n",
      "Epoch 152, Loss: 0.44655235754178146\n",
      "Epoch 153, Loss: 0.4462857402786373\n",
      "Epoch 154, Loss: 0.4460517369142754\n",
      "Epoch 155, Loss: 0.4457881796184935\n",
      "Epoch 156, Loss: 0.44542210886348255\n",
      "Epoch 157, Loss: 0.4450579467827662\n",
      "Epoch 158, Loss: 0.44473329320345206\n",
      "Epoch 159, Loss: 0.4444482514678638\n",
      "Epoch 160, Loss: 0.4441745674630603\n",
      "Epoch 161, Loss: 0.4439221813722748\n",
      "Epoch 162, Loss: 0.44363357870363757\n",
      "Epoch 163, Loss: 0.4433189071134186\n",
      "Epoch 164, Loss: 0.44295452976444744\n",
      "Epoch 165, Loss: 0.4426058108229731\n",
      "Epoch 166, Loss: 0.4422485787900922\n",
      "Epoch 167, Loss: 0.4419090985167976\n",
      "Epoch 168, Loss: 0.44158054387831963\n",
      "Epoch 169, Loss: 0.4412474388772268\n",
      "Epoch 170, Loss: 0.4409277694136744\n",
      "Epoch 171, Loss: 0.44068045571539705\n",
      "Epoch 172, Loss: 0.4405979418732049\n",
      "Epoch 173, Loss: 0.44094101686109266\n",
      "Epoch 174, Loss: 0.4417368233339534\n",
      "Epoch 175, Loss: 0.4416582519101522\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2363327138941002\n",
      "Valence RMSE: 0.2481531521579483\n",
      "Arousal RMSE: 0.22388907161175908\n",
      "Test R^2 score: tensor([0.3420, 0.6303], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4861392200965855\n",
      "Num of epochs: 176\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Epoch 121, Loss: 0.4596922103619757\n",
      "Epoch 122, Loss: 0.45943389956138125\n",
      "Epoch 123, Loss: 0.45877818894117006\n",
      "Epoch 124, Loss: 0.45746538132874076\n",
      "Epoch 125, Loss: 0.4570462803121619\n",
      "Epoch 126, Loss: 0.45696189549769234\n",
      "Epoch 127, Loss: 0.45594920341421274\n",
      "Epoch 128, Loss: 0.45536359747685007\n",
      "Epoch 129, Loss: 0.4553290890752543\n",
      "Epoch 130, Loss: 0.4547195823074053\n",
      "Epoch 131, Loss: 0.45392838323998064\n",
      "Epoch 132, Loss: 0.4537853493595631\n",
      "Epoch 133, Loss: 0.45347437050767975\n",
      "Epoch 134, Loss: 0.45267431273812453\n",
      "Epoch 135, Loss: 0.45228737659373064\n",
      "Epoch 136, Loss: 0.4522154657432981\n",
      "Epoch 137, Loss: 0.45165964487407706\n",
      "Epoch 138, Loss: 0.4510751265462795\n",
      "Epoch 139, Loss: 0.45082650445054473\n",
      "Epoch 140, Loss: 0.4506281098405297\n",
      "Epoch 141, Loss: 0.4501911419656675\n",
      "Epoch 142, Loss: 0.4497001887659326\n",
      "Epoch 143, Loss: 0.44943436133718806\n",
      "Epoch 144, Loss: 0.449226296402944\n",
      "Epoch 145, Loss: 0.44886073897867573\n",
      "Epoch 146, Loss: 0.44844710376304864\n",
      "Epoch 147, Loss: 0.44810507013116724\n",
      "Epoch 148, Loss: 0.44788545862049767\n",
      "Epoch 149, Loss: 0.4476741441009651\n",
      "Epoch 150, Loss: 0.44735861731902055\n",
      "Epoch 151, Loss: 0.4469288720157105\n",
      "Epoch 152, Loss: 0.44655235754178146\n",
      "Epoch 153, Loss: 0.4462857402786373\n",
      "Epoch 154, Loss: 0.4460517369142754\n",
      "Epoch 155, Loss: 0.4457881796184935\n",
      "Epoch 156, Loss: 0.44542210886348255\n",
      "Epoch 157, Loss: 0.4450579467827662\n",
      "Epoch 158, Loss: 0.44473329320345206\n",
      "Epoch 159, Loss: 0.4444482514678638\n",
      "Epoch 160, Loss: 0.4441745674630603\n",
      "Epoch 161, Loss: 0.4439221813722748\n",
      "Epoch 162, Loss: 0.44363357870363757\n",
      "Epoch 163, Loss: 0.4433189071134186\n",
      "Epoch 164, Loss: 0.44295452976444744\n",
      "Epoch 165, Loss: 0.4426058108229731\n",
      "Epoch 166, Loss: 0.4422485787900922\n",
      "Epoch 167, Loss: 0.4419090985167976\n",
      "Epoch 168, Loss: 0.44158054387831963\n",
      "Epoch 169, Loss: 0.4412474388772268\n",
      "Epoch 170, Loss: 0.4409277694136744\n",
      "Epoch 171, Loss: 0.44068045571539705\n",
      "Epoch 172, Loss: 0.4405979418732049\n",
      "Epoch 173, Loss: 0.44094101686109266\n",
      "Epoch 174, Loss: 0.4417368233339534\n",
      "Epoch 175, Loss: 0.4416582519101522\n",
      "Epoch 176, Loss: 0.4396788970821407\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23627390798005066\n",
      "Valence RMSE: 0.248392140961447\n",
      "Arousal RMSE: 0.22349958275781986\n",
      "Test R^2 score: tensor([0.3407, 0.6316], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.48614772848406085\n",
      "Num of epochs: 177\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Epoch 121, Loss: 0.4596922103619757\n",
      "Epoch 122, Loss: 0.45943389956138125\n",
      "Epoch 123, Loss: 0.45877818894117006\n",
      "Epoch 124, Loss: 0.45746538132874076\n",
      "Epoch 125, Loss: 0.4570462803121619\n",
      "Epoch 126, Loss: 0.45696189549769234\n",
      "Epoch 127, Loss: 0.45594920341421274\n",
      "Epoch 128, Loss: 0.45536359747685007\n",
      "Epoch 129, Loss: 0.4553290890752543\n",
      "Epoch 130, Loss: 0.4547195823074053\n",
      "Epoch 131, Loss: 0.45392838323998064\n",
      "Epoch 132, Loss: 0.4537853493595631\n",
      "Epoch 133, Loss: 0.45347437050767975\n",
      "Epoch 134, Loss: 0.45267431273812453\n",
      "Epoch 135, Loss: 0.45228737659373064\n",
      "Epoch 136, Loss: 0.4522154657432981\n",
      "Epoch 137, Loss: 0.45165964487407706\n",
      "Epoch 138, Loss: 0.4510751265462795\n",
      "Epoch 139, Loss: 0.45082650445054473\n",
      "Epoch 140, Loss: 0.4506281098405297\n",
      "Epoch 141, Loss: 0.4501911419656675\n",
      "Epoch 142, Loss: 0.4497001887659326\n",
      "Epoch 143, Loss: 0.44943436133718806\n",
      "Epoch 144, Loss: 0.449226296402944\n",
      "Epoch 145, Loss: 0.44886073897867573\n",
      "Epoch 146, Loss: 0.44844710376304864\n",
      "Epoch 147, Loss: 0.44810507013116724\n",
      "Epoch 148, Loss: 0.44788545862049767\n",
      "Epoch 149, Loss: 0.4476741441009651\n",
      "Epoch 150, Loss: 0.44735861731902055\n",
      "Epoch 151, Loss: 0.4469288720157105\n",
      "Epoch 152, Loss: 0.44655235754178146\n",
      "Epoch 153, Loss: 0.4462857402786373\n",
      "Epoch 154, Loss: 0.4460517369142754\n",
      "Epoch 155, Loss: 0.4457881796184935\n",
      "Epoch 156, Loss: 0.44542210886348255\n",
      "Epoch 157, Loss: 0.4450579467827662\n",
      "Epoch 158, Loss: 0.44473329320345206\n",
      "Epoch 159, Loss: 0.4444482514678638\n",
      "Epoch 160, Loss: 0.4441745674630603\n",
      "Epoch 161, Loss: 0.4439221813722748\n",
      "Epoch 162, Loss: 0.44363357870363757\n",
      "Epoch 163, Loss: 0.4433189071134186\n",
      "Epoch 164, Loss: 0.44295452976444744\n",
      "Epoch 165, Loss: 0.4426058108229731\n",
      "Epoch 166, Loss: 0.4422485787900922\n",
      "Epoch 167, Loss: 0.4419090985167976\n",
      "Epoch 168, Loss: 0.44158054387831963\n",
      "Epoch 169, Loss: 0.4412474388772268\n",
      "Epoch 170, Loss: 0.4409277694136744\n",
      "Epoch 171, Loss: 0.44068045571539705\n",
      "Epoch 172, Loss: 0.4405979418732049\n",
      "Epoch 173, Loss: 0.44094101686109266\n",
      "Epoch 174, Loss: 0.4417368233339534\n",
      "Epoch 175, Loss: 0.4416582519101522\n",
      "Epoch 176, Loss: 0.4396788970821407\n",
      "Epoch 177, Loss: 0.4390195954561405\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23784523886836068\n",
      "Valence RMSE: 0.25118348175973093\n",
      "Arousal RMSE: 0.22371315069918\n",
      "Test R^2 score: tensor([0.3258, 0.6309], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.47834481774585363\n",
      "Num of epochs: 178\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Epoch 121, Loss: 0.4596922103619757\n",
      "Epoch 122, Loss: 0.45943389956138125\n",
      "Epoch 123, Loss: 0.45877818894117006\n",
      "Epoch 124, Loss: 0.45746538132874076\n",
      "Epoch 125, Loss: 0.4570462803121619\n",
      "Epoch 126, Loss: 0.45696189549769234\n",
      "Epoch 127, Loss: 0.45594920341421274\n",
      "Epoch 128, Loss: 0.45536359747685007\n",
      "Epoch 129, Loss: 0.4553290890752543\n",
      "Epoch 130, Loss: 0.4547195823074053\n",
      "Epoch 131, Loss: 0.45392838323998064\n",
      "Epoch 132, Loss: 0.4537853493595631\n",
      "Epoch 133, Loss: 0.45347437050767975\n",
      "Epoch 134, Loss: 0.45267431273812453\n",
      "Epoch 135, Loss: 0.45228737659373064\n",
      "Epoch 136, Loss: 0.4522154657432981\n",
      "Epoch 137, Loss: 0.45165964487407706\n",
      "Epoch 138, Loss: 0.4510751265462795\n",
      "Epoch 139, Loss: 0.45082650445054473\n",
      "Epoch 140, Loss: 0.4506281098405297\n",
      "Epoch 141, Loss: 0.4501911419656675\n",
      "Epoch 142, Loss: 0.4497001887659326\n",
      "Epoch 143, Loss: 0.44943436133718806\n",
      "Epoch 144, Loss: 0.449226296402944\n",
      "Epoch 145, Loss: 0.44886073897867573\n",
      "Epoch 146, Loss: 0.44844710376304864\n",
      "Epoch 147, Loss: 0.44810507013116724\n",
      "Epoch 148, Loss: 0.44788545862049767\n",
      "Epoch 149, Loss: 0.4476741441009651\n",
      "Epoch 150, Loss: 0.44735861731902055\n",
      "Epoch 151, Loss: 0.4469288720157105\n",
      "Epoch 152, Loss: 0.44655235754178146\n",
      "Epoch 153, Loss: 0.4462857402786373\n",
      "Epoch 154, Loss: 0.4460517369142754\n",
      "Epoch 155, Loss: 0.4457881796184935\n",
      "Epoch 156, Loss: 0.44542210886348255\n",
      "Epoch 157, Loss: 0.4450579467827662\n",
      "Epoch 158, Loss: 0.44473329320345206\n",
      "Epoch 159, Loss: 0.4444482514678638\n",
      "Epoch 160, Loss: 0.4441745674630603\n",
      "Epoch 161, Loss: 0.4439221813722748\n",
      "Epoch 162, Loss: 0.44363357870363757\n",
      "Epoch 163, Loss: 0.4433189071134186\n",
      "Epoch 164, Loss: 0.44295452976444744\n",
      "Epoch 165, Loss: 0.4426058108229731\n",
      "Epoch 166, Loss: 0.4422485787900922\n",
      "Epoch 167, Loss: 0.4419090985167976\n",
      "Epoch 168, Loss: 0.44158054387831963\n",
      "Epoch 169, Loss: 0.4412474388772268\n",
      "Epoch 170, Loss: 0.4409277694136744\n",
      "Epoch 171, Loss: 0.44068045571539705\n",
      "Epoch 172, Loss: 0.4405979418732049\n",
      "Epoch 173, Loss: 0.44094101686109266\n",
      "Epoch 174, Loss: 0.4417368233339534\n",
      "Epoch 175, Loss: 0.4416582519101522\n",
      "Epoch 176, Loss: 0.4396788970821407\n",
      "Epoch 177, Loss: 0.4390195954561405\n",
      "Epoch 178, Loss: 0.4400830988007806\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23699508720171547\n",
      "Valence RMSE: 0.2488344845133948\n",
      "Arousal RMSE: 0.2245322739217027\n",
      "Test R^2 score: tensor([0.3383, 0.6282], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4832664478367771\n",
      "Num of epochs: 179\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Epoch 121, Loss: 0.4596922103619757\n",
      "Epoch 122, Loss: 0.45943389956138125\n",
      "Epoch 123, Loss: 0.45877818894117006\n",
      "Epoch 124, Loss: 0.45746538132874076\n",
      "Epoch 125, Loss: 0.4570462803121619\n",
      "Epoch 126, Loss: 0.45696189549769234\n",
      "Epoch 127, Loss: 0.45594920341421274\n",
      "Epoch 128, Loss: 0.45536359747685007\n",
      "Epoch 129, Loss: 0.4553290890752543\n",
      "Epoch 130, Loss: 0.4547195823074053\n",
      "Epoch 131, Loss: 0.45392838323998064\n",
      "Epoch 132, Loss: 0.4537853493595631\n",
      "Epoch 133, Loss: 0.45347437050767975\n",
      "Epoch 134, Loss: 0.45267431273812453\n",
      "Epoch 135, Loss: 0.45228737659373064\n",
      "Epoch 136, Loss: 0.4522154657432981\n",
      "Epoch 137, Loss: 0.45165964487407706\n",
      "Epoch 138, Loss: 0.4510751265462795\n",
      "Epoch 139, Loss: 0.45082650445054473\n",
      "Epoch 140, Loss: 0.4506281098405297\n",
      "Epoch 141, Loss: 0.4501911419656675\n",
      "Epoch 142, Loss: 0.4497001887659326\n",
      "Epoch 143, Loss: 0.44943436133718806\n",
      "Epoch 144, Loss: 0.449226296402944\n",
      "Epoch 145, Loss: 0.44886073897867573\n",
      "Epoch 146, Loss: 0.44844710376304864\n",
      "Epoch 147, Loss: 0.44810507013116724\n",
      "Epoch 148, Loss: 0.44788545862049767\n",
      "Epoch 149, Loss: 0.4476741441009651\n",
      "Epoch 150, Loss: 0.44735861731902055\n",
      "Epoch 151, Loss: 0.4469288720157105\n",
      "Epoch 152, Loss: 0.44655235754178146\n",
      "Epoch 153, Loss: 0.4462857402786373\n",
      "Epoch 154, Loss: 0.4460517369142754\n",
      "Epoch 155, Loss: 0.4457881796184935\n",
      "Epoch 156, Loss: 0.44542210886348255\n",
      "Epoch 157, Loss: 0.4450579467827662\n",
      "Epoch 158, Loss: 0.44473329320345206\n",
      "Epoch 159, Loss: 0.4444482514678638\n",
      "Epoch 160, Loss: 0.4441745674630603\n",
      "Epoch 161, Loss: 0.4439221813722748\n",
      "Epoch 162, Loss: 0.44363357870363757\n",
      "Epoch 163, Loss: 0.4433189071134186\n",
      "Epoch 164, Loss: 0.44295452976444744\n",
      "Epoch 165, Loss: 0.4426058108229731\n",
      "Epoch 166, Loss: 0.4422485787900922\n",
      "Epoch 167, Loss: 0.4419090985167976\n",
      "Epoch 168, Loss: 0.44158054387831963\n",
      "Epoch 169, Loss: 0.4412474388772268\n",
      "Epoch 170, Loss: 0.4409277694136744\n",
      "Epoch 171, Loss: 0.44068045571539705\n",
      "Epoch 172, Loss: 0.4405979418732049\n",
      "Epoch 173, Loss: 0.44094101686109266\n",
      "Epoch 174, Loss: 0.4417368233339534\n",
      "Epoch 175, Loss: 0.4416582519101522\n",
      "Epoch 176, Loss: 0.4396788970821407\n",
      "Epoch 177, Loss: 0.4390195954561405\n",
      "Epoch 178, Loss: 0.4400830988007806\n",
      "Epoch 179, Loss: 0.43931775869233325\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23692175515019673\n",
      "Valence RMSE: 0.2497759874022669\n",
      "Arousal RMSE: 0.22332888806449142\n",
      "Test R^2 score: tensor([0.3333, 0.6322], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4827455855604932\n",
      "Num of epochs: 180\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Epoch 121, Loss: 0.4596922103619757\n",
      "Epoch 122, Loss: 0.45943389956138125\n",
      "Epoch 123, Loss: 0.45877818894117006\n",
      "Epoch 124, Loss: 0.45746538132874076\n",
      "Epoch 125, Loss: 0.4570462803121619\n",
      "Epoch 126, Loss: 0.45696189549769234\n",
      "Epoch 127, Loss: 0.45594920341421274\n",
      "Epoch 128, Loss: 0.45536359747685007\n",
      "Epoch 129, Loss: 0.4553290890752543\n",
      "Epoch 130, Loss: 0.4547195823074053\n",
      "Epoch 131, Loss: 0.45392838323998064\n",
      "Epoch 132, Loss: 0.4537853493595631\n",
      "Epoch 133, Loss: 0.45347437050767975\n",
      "Epoch 134, Loss: 0.45267431273812453\n",
      "Epoch 135, Loss: 0.45228737659373064\n",
      "Epoch 136, Loss: 0.4522154657432981\n",
      "Epoch 137, Loss: 0.45165964487407706\n",
      "Epoch 138, Loss: 0.4510751265462795\n",
      "Epoch 139, Loss: 0.45082650445054473\n",
      "Epoch 140, Loss: 0.4506281098405297\n",
      "Epoch 141, Loss: 0.4501911419656675\n",
      "Epoch 142, Loss: 0.4497001887659326\n",
      "Epoch 143, Loss: 0.44943436133718806\n",
      "Epoch 144, Loss: 0.449226296402944\n",
      "Epoch 145, Loss: 0.44886073897867573\n",
      "Epoch 146, Loss: 0.44844710376304864\n",
      "Epoch 147, Loss: 0.44810507013116724\n",
      "Epoch 148, Loss: 0.44788545862049767\n",
      "Epoch 149, Loss: 0.4476741441009651\n",
      "Epoch 150, Loss: 0.44735861731902055\n",
      "Epoch 151, Loss: 0.4469288720157105\n",
      "Epoch 152, Loss: 0.44655235754178146\n",
      "Epoch 153, Loss: 0.4462857402786373\n",
      "Epoch 154, Loss: 0.4460517369142754\n",
      "Epoch 155, Loss: 0.4457881796184935\n",
      "Epoch 156, Loss: 0.44542210886348255\n",
      "Epoch 157, Loss: 0.4450579467827662\n",
      "Epoch 158, Loss: 0.44473329320345206\n",
      "Epoch 159, Loss: 0.4444482514678638\n",
      "Epoch 160, Loss: 0.4441745674630603\n",
      "Epoch 161, Loss: 0.4439221813722748\n",
      "Epoch 162, Loss: 0.44363357870363757\n",
      "Epoch 163, Loss: 0.4433189071134186\n",
      "Epoch 164, Loss: 0.44295452976444744\n",
      "Epoch 165, Loss: 0.4426058108229731\n",
      "Epoch 166, Loss: 0.4422485787900922\n",
      "Epoch 167, Loss: 0.4419090985167976\n",
      "Epoch 168, Loss: 0.44158054387831963\n",
      "Epoch 169, Loss: 0.4412474388772268\n",
      "Epoch 170, Loss: 0.4409277694136744\n",
      "Epoch 171, Loss: 0.44068045571539705\n",
      "Epoch 172, Loss: 0.4405979418732049\n",
      "Epoch 173, Loss: 0.44094101686109266\n",
      "Epoch 174, Loss: 0.4417368233339534\n",
      "Epoch 175, Loss: 0.4416582519101522\n",
      "Epoch 176, Loss: 0.4396788970821407\n",
      "Epoch 177, Loss: 0.4390195954561405\n",
      "Epoch 178, Loss: 0.4400830988007806\n",
      "Epoch 179, Loss: 0.43931775869233325\n",
      "Epoch 180, Loss: 0.4380032505977321\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2376534903897378\n",
      "Valence RMSE: 0.2509776242687194\n",
      "Arousal RMSE: 0.22353656324028945\n",
      "Test R^2 score: tensor([0.3269, 0.6315], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.47918837389450125\n",
      "Num of epochs: 181\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Epoch 121, Loss: 0.4596922103619757\n",
      "Epoch 122, Loss: 0.45943389956138125\n",
      "Epoch 123, Loss: 0.45877818894117006\n",
      "Epoch 124, Loss: 0.45746538132874076\n",
      "Epoch 125, Loss: 0.4570462803121619\n",
      "Epoch 126, Loss: 0.45696189549769234\n",
      "Epoch 127, Loss: 0.45594920341421274\n",
      "Epoch 128, Loss: 0.45536359747685007\n",
      "Epoch 129, Loss: 0.4553290890752543\n",
      "Epoch 130, Loss: 0.4547195823074053\n",
      "Epoch 131, Loss: 0.45392838323998064\n",
      "Epoch 132, Loss: 0.4537853493595631\n",
      "Epoch 133, Loss: 0.45347437050767975\n",
      "Epoch 134, Loss: 0.45267431273812453\n",
      "Epoch 135, Loss: 0.45228737659373064\n",
      "Epoch 136, Loss: 0.4522154657432981\n",
      "Epoch 137, Loss: 0.45165964487407706\n",
      "Epoch 138, Loss: 0.4510751265462795\n",
      "Epoch 139, Loss: 0.45082650445054473\n",
      "Epoch 140, Loss: 0.4506281098405297\n",
      "Epoch 141, Loss: 0.4501911419656675\n",
      "Epoch 142, Loss: 0.4497001887659326\n",
      "Epoch 143, Loss: 0.44943436133718806\n",
      "Epoch 144, Loss: 0.449226296402944\n",
      "Epoch 145, Loss: 0.44886073897867573\n",
      "Epoch 146, Loss: 0.44844710376304864\n",
      "Epoch 147, Loss: 0.44810507013116724\n",
      "Epoch 148, Loss: 0.44788545862049767\n",
      "Epoch 149, Loss: 0.4476741441009651\n",
      "Epoch 150, Loss: 0.44735861731902055\n",
      "Epoch 151, Loss: 0.4469288720157105\n",
      "Epoch 152, Loss: 0.44655235754178146\n",
      "Epoch 153, Loss: 0.4462857402786373\n",
      "Epoch 154, Loss: 0.4460517369142754\n",
      "Epoch 155, Loss: 0.4457881796184935\n",
      "Epoch 156, Loss: 0.44542210886348255\n",
      "Epoch 157, Loss: 0.4450579467827662\n",
      "Epoch 158, Loss: 0.44473329320345206\n",
      "Epoch 159, Loss: 0.4444482514678638\n",
      "Epoch 160, Loss: 0.4441745674630603\n",
      "Epoch 161, Loss: 0.4439221813722748\n",
      "Epoch 162, Loss: 0.44363357870363757\n",
      "Epoch 163, Loss: 0.4433189071134186\n",
      "Epoch 164, Loss: 0.44295452976444744\n",
      "Epoch 165, Loss: 0.4426058108229731\n",
      "Epoch 166, Loss: 0.4422485787900922\n",
      "Epoch 167, Loss: 0.4419090985167976\n",
      "Epoch 168, Loss: 0.44158054387831963\n",
      "Epoch 169, Loss: 0.4412474388772268\n",
      "Epoch 170, Loss: 0.4409277694136744\n",
      "Epoch 171, Loss: 0.44068045571539705\n",
      "Epoch 172, Loss: 0.4405979418732049\n",
      "Epoch 173, Loss: 0.44094101686109266\n",
      "Epoch 174, Loss: 0.4417368233339534\n",
      "Epoch 175, Loss: 0.4416582519101522\n",
      "Epoch 176, Loss: 0.4396788970821407\n",
      "Epoch 177, Loss: 0.4390195954561405\n",
      "Epoch 178, Loss: 0.4400830988007806\n",
      "Epoch 179, Loss: 0.43931775869233325\n",
      "Epoch 180, Loss: 0.4380032505977321\n",
      "Epoch 181, Loss: 0.4384470974340077\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23725790108624614\n",
      "Valence RMSE: 0.24910174908723057\n",
      "Arousal RMSE: 0.22479088472930644\n",
      "Test R^2 score: tensor([0.3369, 0.6273], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4821269149496235\n",
      "Num of epochs: 182\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Epoch 121, Loss: 0.4596922103619757\n",
      "Epoch 122, Loss: 0.45943389956138125\n",
      "Epoch 123, Loss: 0.45877818894117006\n",
      "Epoch 124, Loss: 0.45746538132874076\n",
      "Epoch 125, Loss: 0.4570462803121619\n",
      "Epoch 126, Loss: 0.45696189549769234\n",
      "Epoch 127, Loss: 0.45594920341421274\n",
      "Epoch 128, Loss: 0.45536359747685007\n",
      "Epoch 129, Loss: 0.4553290890752543\n",
      "Epoch 130, Loss: 0.4547195823074053\n",
      "Epoch 131, Loss: 0.45392838323998064\n",
      "Epoch 132, Loss: 0.4537853493595631\n",
      "Epoch 133, Loss: 0.45347437050767975\n",
      "Epoch 134, Loss: 0.45267431273812453\n",
      "Epoch 135, Loss: 0.45228737659373064\n",
      "Epoch 136, Loss: 0.4522154657432981\n",
      "Epoch 137, Loss: 0.45165964487407706\n",
      "Epoch 138, Loss: 0.4510751265462795\n",
      "Epoch 139, Loss: 0.45082650445054473\n",
      "Epoch 140, Loss: 0.4506281098405297\n",
      "Epoch 141, Loss: 0.4501911419656675\n",
      "Epoch 142, Loss: 0.4497001887659326\n",
      "Epoch 143, Loss: 0.44943436133718806\n",
      "Epoch 144, Loss: 0.449226296402944\n",
      "Epoch 145, Loss: 0.44886073897867573\n",
      "Epoch 146, Loss: 0.44844710376304864\n",
      "Epoch 147, Loss: 0.44810507013116724\n",
      "Epoch 148, Loss: 0.44788545862049767\n",
      "Epoch 149, Loss: 0.4476741441009651\n",
      "Epoch 150, Loss: 0.44735861731902055\n",
      "Epoch 151, Loss: 0.4469288720157105\n",
      "Epoch 152, Loss: 0.44655235754178146\n",
      "Epoch 153, Loss: 0.4462857402786373\n",
      "Epoch 154, Loss: 0.4460517369142754\n",
      "Epoch 155, Loss: 0.4457881796184935\n",
      "Epoch 156, Loss: 0.44542210886348255\n",
      "Epoch 157, Loss: 0.4450579467827662\n",
      "Epoch 158, Loss: 0.44473329320345206\n",
      "Epoch 159, Loss: 0.4444482514678638\n",
      "Epoch 160, Loss: 0.4441745674630603\n",
      "Epoch 161, Loss: 0.4439221813722748\n",
      "Epoch 162, Loss: 0.44363357870363757\n",
      "Epoch 163, Loss: 0.4433189071134186\n",
      "Epoch 164, Loss: 0.44295452976444744\n",
      "Epoch 165, Loss: 0.4426058108229731\n",
      "Epoch 166, Loss: 0.4422485787900922\n",
      "Epoch 167, Loss: 0.4419090985167976\n",
      "Epoch 168, Loss: 0.44158054387831963\n",
      "Epoch 169, Loss: 0.4412474388772268\n",
      "Epoch 170, Loss: 0.4409277694136744\n",
      "Epoch 171, Loss: 0.44068045571539705\n",
      "Epoch 172, Loss: 0.4405979418732049\n",
      "Epoch 173, Loss: 0.44094101686109266\n",
      "Epoch 174, Loss: 0.4417368233339534\n",
      "Epoch 175, Loss: 0.4416582519101522\n",
      "Epoch 176, Loss: 0.4396788970821407\n",
      "Epoch 177, Loss: 0.4390195954561405\n",
      "Epoch 178, Loss: 0.4400830988007806\n",
      "Epoch 179, Loss: 0.43931775869233325\n",
      "Epoch 180, Loss: 0.4380032505977321\n",
      "Epoch 181, Loss: 0.4384470974340077\n",
      "Epoch 182, Loss: 0.4385167636581309\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23713514736454586\n",
      "Valence RMSE: 0.2500199429056631\n",
      "Arousal RMSE: 0.22350880157313427\n",
      "Test R^2 score: tensor([0.3320, 0.6316], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.48179768176339516\n",
      "Num of epochs: 183\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Epoch 121, Loss: 0.4596922103619757\n",
      "Epoch 122, Loss: 0.45943389956138125\n",
      "Epoch 123, Loss: 0.45877818894117006\n",
      "Epoch 124, Loss: 0.45746538132874076\n",
      "Epoch 125, Loss: 0.4570462803121619\n",
      "Epoch 126, Loss: 0.45696189549769234\n",
      "Epoch 127, Loss: 0.45594920341421274\n",
      "Epoch 128, Loss: 0.45536359747685007\n",
      "Epoch 129, Loss: 0.4553290890752543\n",
      "Epoch 130, Loss: 0.4547195823074053\n",
      "Epoch 131, Loss: 0.45392838323998064\n",
      "Epoch 132, Loss: 0.4537853493595631\n",
      "Epoch 133, Loss: 0.45347437050767975\n",
      "Epoch 134, Loss: 0.45267431273812453\n",
      "Epoch 135, Loss: 0.45228737659373064\n",
      "Epoch 136, Loss: 0.4522154657432981\n",
      "Epoch 137, Loss: 0.45165964487407706\n",
      "Epoch 138, Loss: 0.4510751265462795\n",
      "Epoch 139, Loss: 0.45082650445054473\n",
      "Epoch 140, Loss: 0.4506281098405297\n",
      "Epoch 141, Loss: 0.4501911419656675\n",
      "Epoch 142, Loss: 0.4497001887659326\n",
      "Epoch 143, Loss: 0.44943436133718806\n",
      "Epoch 144, Loss: 0.449226296402944\n",
      "Epoch 145, Loss: 0.44886073897867573\n",
      "Epoch 146, Loss: 0.44844710376304864\n",
      "Epoch 147, Loss: 0.44810507013116724\n",
      "Epoch 148, Loss: 0.44788545862049767\n",
      "Epoch 149, Loss: 0.4476741441009651\n",
      "Epoch 150, Loss: 0.44735861731902055\n",
      "Epoch 151, Loss: 0.4469288720157105\n",
      "Epoch 152, Loss: 0.44655235754178146\n",
      "Epoch 153, Loss: 0.4462857402786373\n",
      "Epoch 154, Loss: 0.4460517369142754\n",
      "Epoch 155, Loss: 0.4457881796184935\n",
      "Epoch 156, Loss: 0.44542210886348255\n",
      "Epoch 157, Loss: 0.4450579467827662\n",
      "Epoch 158, Loss: 0.44473329320345206\n",
      "Epoch 159, Loss: 0.4444482514678638\n",
      "Epoch 160, Loss: 0.4441745674630603\n",
      "Epoch 161, Loss: 0.4439221813722748\n",
      "Epoch 162, Loss: 0.44363357870363757\n",
      "Epoch 163, Loss: 0.4433189071134186\n",
      "Epoch 164, Loss: 0.44295452976444744\n",
      "Epoch 165, Loss: 0.4426058108229731\n",
      "Epoch 166, Loss: 0.4422485787900922\n",
      "Epoch 167, Loss: 0.4419090985167976\n",
      "Epoch 168, Loss: 0.44158054387831963\n",
      "Epoch 169, Loss: 0.4412474388772268\n",
      "Epoch 170, Loss: 0.4409277694136744\n",
      "Epoch 171, Loss: 0.44068045571539705\n",
      "Epoch 172, Loss: 0.4405979418732049\n",
      "Epoch 173, Loss: 0.44094101686109266\n",
      "Epoch 174, Loss: 0.4417368233339534\n",
      "Epoch 175, Loss: 0.4416582519101522\n",
      "Epoch 176, Loss: 0.4396788970821407\n",
      "Epoch 177, Loss: 0.4390195954561405\n",
      "Epoch 178, Loss: 0.4400830988007806\n",
      "Epoch 179, Loss: 0.43931775869233325\n",
      "Epoch 180, Loss: 0.4380032505977321\n",
      "Epoch 181, Loss: 0.4384470974340077\n",
      "Epoch 182, Loss: 0.4385167636581309\n",
      "Epoch 183, Loss: 0.43733043450040754\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23733679785451045\n",
      "Valence RMSE: 0.2503337812242977\n",
      "Arousal RMSE: 0.22358557469034304\n",
      "Test R^2 score: tensor([0.3303, 0.6313], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4808320996255102\n",
      "Num of epochs: 184\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Epoch 121, Loss: 0.4596922103619757\n",
      "Epoch 122, Loss: 0.45943389956138125\n",
      "Epoch 123, Loss: 0.45877818894117006\n",
      "Epoch 124, Loss: 0.45746538132874076\n",
      "Epoch 125, Loss: 0.4570462803121619\n",
      "Epoch 126, Loss: 0.45696189549769234\n",
      "Epoch 127, Loss: 0.45594920341421274\n",
      "Epoch 128, Loss: 0.45536359747685007\n",
      "Epoch 129, Loss: 0.4553290890752543\n",
      "Epoch 130, Loss: 0.4547195823074053\n",
      "Epoch 131, Loss: 0.45392838323998064\n",
      "Epoch 132, Loss: 0.4537853493595631\n",
      "Epoch 133, Loss: 0.45347437050767975\n",
      "Epoch 134, Loss: 0.45267431273812453\n",
      "Epoch 135, Loss: 0.45228737659373064\n",
      "Epoch 136, Loss: 0.4522154657432981\n",
      "Epoch 137, Loss: 0.45165964487407706\n",
      "Epoch 138, Loss: 0.4510751265462795\n",
      "Epoch 139, Loss: 0.45082650445054473\n",
      "Epoch 140, Loss: 0.4506281098405297\n",
      "Epoch 141, Loss: 0.4501911419656675\n",
      "Epoch 142, Loss: 0.4497001887659326\n",
      "Epoch 143, Loss: 0.44943436133718806\n",
      "Epoch 144, Loss: 0.449226296402944\n",
      "Epoch 145, Loss: 0.44886073897867573\n",
      "Epoch 146, Loss: 0.44844710376304864\n",
      "Epoch 147, Loss: 0.44810507013116724\n",
      "Epoch 148, Loss: 0.44788545862049767\n",
      "Epoch 149, Loss: 0.4476741441009651\n",
      "Epoch 150, Loss: 0.44735861731902055\n",
      "Epoch 151, Loss: 0.4469288720157105\n",
      "Epoch 152, Loss: 0.44655235754178146\n",
      "Epoch 153, Loss: 0.4462857402786373\n",
      "Epoch 154, Loss: 0.4460517369142754\n",
      "Epoch 155, Loss: 0.4457881796184935\n",
      "Epoch 156, Loss: 0.44542210886348255\n",
      "Epoch 157, Loss: 0.4450579467827662\n",
      "Epoch 158, Loss: 0.44473329320345206\n",
      "Epoch 159, Loss: 0.4444482514678638\n",
      "Epoch 160, Loss: 0.4441745674630603\n",
      "Epoch 161, Loss: 0.4439221813722748\n",
      "Epoch 162, Loss: 0.44363357870363757\n",
      "Epoch 163, Loss: 0.4433189071134186\n",
      "Epoch 164, Loss: 0.44295452976444744\n",
      "Epoch 165, Loss: 0.4426058108229731\n",
      "Epoch 166, Loss: 0.4422485787900922\n",
      "Epoch 167, Loss: 0.4419090985167976\n",
      "Epoch 168, Loss: 0.44158054387831963\n",
      "Epoch 169, Loss: 0.4412474388772268\n",
      "Epoch 170, Loss: 0.4409277694136744\n",
      "Epoch 171, Loss: 0.44068045571539705\n",
      "Epoch 172, Loss: 0.4405979418732049\n",
      "Epoch 173, Loss: 0.44094101686109266\n",
      "Epoch 174, Loss: 0.4417368233339534\n",
      "Epoch 175, Loss: 0.4416582519101522\n",
      "Epoch 176, Loss: 0.4396788970821407\n",
      "Epoch 177, Loss: 0.4390195954561405\n",
      "Epoch 178, Loss: 0.4400830988007806\n",
      "Epoch 179, Loss: 0.43931775869233325\n",
      "Epoch 180, Loss: 0.4380032505977321\n",
      "Epoch 181, Loss: 0.4384470974340077\n",
      "Epoch 182, Loss: 0.4385167636581309\n",
      "Epoch 183, Loss: 0.43733043450040754\n",
      "Epoch 184, Loss: 0.4373176399006318\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23720085747059608\n",
      "Valence RMSE: 0.24898642068623422\n",
      "Arousal RMSE: 0.22479825596171701\n",
      "Test R^2 score: tensor([0.3375, 0.6273], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.48242161550747314\n",
      "Num of epochs: 185\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Epoch 121, Loss: 0.4596922103619757\n",
      "Epoch 122, Loss: 0.45943389956138125\n",
      "Epoch 123, Loss: 0.45877818894117006\n",
      "Epoch 124, Loss: 0.45746538132874076\n",
      "Epoch 125, Loss: 0.4570462803121619\n",
      "Epoch 126, Loss: 0.45696189549769234\n",
      "Epoch 127, Loss: 0.45594920341421274\n",
      "Epoch 128, Loss: 0.45536359747685007\n",
      "Epoch 129, Loss: 0.4553290890752543\n",
      "Epoch 130, Loss: 0.4547195823074053\n",
      "Epoch 131, Loss: 0.45392838323998064\n",
      "Epoch 132, Loss: 0.4537853493595631\n",
      "Epoch 133, Loss: 0.45347437050767975\n",
      "Epoch 134, Loss: 0.45267431273812453\n",
      "Epoch 135, Loss: 0.45228737659373064\n",
      "Epoch 136, Loss: 0.4522154657432981\n",
      "Epoch 137, Loss: 0.45165964487407706\n",
      "Epoch 138, Loss: 0.4510751265462795\n",
      "Epoch 139, Loss: 0.45082650445054473\n",
      "Epoch 140, Loss: 0.4506281098405297\n",
      "Epoch 141, Loss: 0.4501911419656675\n",
      "Epoch 142, Loss: 0.4497001887659326\n",
      "Epoch 143, Loss: 0.44943436133718806\n",
      "Epoch 144, Loss: 0.449226296402944\n",
      "Epoch 145, Loss: 0.44886073897867573\n",
      "Epoch 146, Loss: 0.44844710376304864\n",
      "Epoch 147, Loss: 0.44810507013116724\n",
      "Epoch 148, Loss: 0.44788545862049767\n",
      "Epoch 149, Loss: 0.4476741441009651\n",
      "Epoch 150, Loss: 0.44735861731902055\n",
      "Epoch 151, Loss: 0.4469288720157105\n",
      "Epoch 152, Loss: 0.44655235754178146\n",
      "Epoch 153, Loss: 0.4462857402786373\n",
      "Epoch 154, Loss: 0.4460517369142754\n",
      "Epoch 155, Loss: 0.4457881796184935\n",
      "Epoch 156, Loss: 0.44542210886348255\n",
      "Epoch 157, Loss: 0.4450579467827662\n",
      "Epoch 158, Loss: 0.44473329320345206\n",
      "Epoch 159, Loss: 0.4444482514678638\n",
      "Epoch 160, Loss: 0.4441745674630603\n",
      "Epoch 161, Loss: 0.4439221813722748\n",
      "Epoch 162, Loss: 0.44363357870363757\n",
      "Epoch 163, Loss: 0.4433189071134186\n",
      "Epoch 164, Loss: 0.44295452976444744\n",
      "Epoch 165, Loss: 0.4426058108229731\n",
      "Epoch 166, Loss: 0.4422485787900922\n",
      "Epoch 167, Loss: 0.4419090985167976\n",
      "Epoch 168, Loss: 0.44158054387831963\n",
      "Epoch 169, Loss: 0.4412474388772268\n",
      "Epoch 170, Loss: 0.4409277694136744\n",
      "Epoch 171, Loss: 0.44068045571539705\n",
      "Epoch 172, Loss: 0.4405979418732049\n",
      "Epoch 173, Loss: 0.44094101686109266\n",
      "Epoch 174, Loss: 0.4417368233339534\n",
      "Epoch 175, Loss: 0.4416582519101522\n",
      "Epoch 176, Loss: 0.4396788970821407\n",
      "Epoch 177, Loss: 0.4390195954561405\n",
      "Epoch 178, Loss: 0.4400830988007806\n",
      "Epoch 179, Loss: 0.43931775869233325\n",
      "Epoch 180, Loss: 0.4380032505977321\n",
      "Epoch 181, Loss: 0.4384470974340077\n",
      "Epoch 182, Loss: 0.4385167636581309\n",
      "Epoch 183, Loss: 0.43733043450040754\n",
      "Epoch 184, Loss: 0.4373176399006318\n",
      "Epoch 185, Loss: 0.4375256973621567\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2373660761114572\n",
      "Valence RMSE: 0.2503366001484701\n",
      "Arousal RMSE: 0.22364457248769803\n",
      "Test R^2 score: tensor([0.3303, 0.6311], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.48072726245862174\n",
      "Num of epochs: 186\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Epoch 121, Loss: 0.4596922103619757\n",
      "Epoch 122, Loss: 0.45943389956138125\n",
      "Epoch 123, Loss: 0.45877818894117006\n",
      "Epoch 124, Loss: 0.45746538132874076\n",
      "Epoch 125, Loss: 0.4570462803121619\n",
      "Epoch 126, Loss: 0.45696189549769234\n",
      "Epoch 127, Loss: 0.45594920341421274\n",
      "Epoch 128, Loss: 0.45536359747685007\n",
      "Epoch 129, Loss: 0.4553290890752543\n",
      "Epoch 130, Loss: 0.4547195823074053\n",
      "Epoch 131, Loss: 0.45392838323998064\n",
      "Epoch 132, Loss: 0.4537853493595631\n",
      "Epoch 133, Loss: 0.45347437050767975\n",
      "Epoch 134, Loss: 0.45267431273812453\n",
      "Epoch 135, Loss: 0.45228737659373064\n",
      "Epoch 136, Loss: 0.4522154657432981\n",
      "Epoch 137, Loss: 0.45165964487407706\n",
      "Epoch 138, Loss: 0.4510751265462795\n",
      "Epoch 139, Loss: 0.45082650445054473\n",
      "Epoch 140, Loss: 0.4506281098405297\n",
      "Epoch 141, Loss: 0.4501911419656675\n",
      "Epoch 142, Loss: 0.4497001887659326\n",
      "Epoch 143, Loss: 0.44943436133718806\n",
      "Epoch 144, Loss: 0.449226296402944\n",
      "Epoch 145, Loss: 0.44886073897867573\n",
      "Epoch 146, Loss: 0.44844710376304864\n",
      "Epoch 147, Loss: 0.44810507013116724\n",
      "Epoch 148, Loss: 0.44788545862049767\n",
      "Epoch 149, Loss: 0.4476741441009651\n",
      "Epoch 150, Loss: 0.44735861731902055\n",
      "Epoch 151, Loss: 0.4469288720157105\n",
      "Epoch 152, Loss: 0.44655235754178146\n",
      "Epoch 153, Loss: 0.4462857402786373\n",
      "Epoch 154, Loss: 0.4460517369142754\n",
      "Epoch 155, Loss: 0.4457881796184935\n",
      "Epoch 156, Loss: 0.44542210886348255\n",
      "Epoch 157, Loss: 0.4450579467827662\n",
      "Epoch 158, Loss: 0.44473329320345206\n",
      "Epoch 159, Loss: 0.4444482514678638\n",
      "Epoch 160, Loss: 0.4441745674630603\n",
      "Epoch 161, Loss: 0.4439221813722748\n",
      "Epoch 162, Loss: 0.44363357870363757\n",
      "Epoch 163, Loss: 0.4433189071134186\n",
      "Epoch 164, Loss: 0.44295452976444744\n",
      "Epoch 165, Loss: 0.4426058108229731\n",
      "Epoch 166, Loss: 0.4422485787900922\n",
      "Epoch 167, Loss: 0.4419090985167976\n",
      "Epoch 168, Loss: 0.44158054387831963\n",
      "Epoch 169, Loss: 0.4412474388772268\n",
      "Epoch 170, Loss: 0.4409277694136744\n",
      "Epoch 171, Loss: 0.44068045571539705\n",
      "Epoch 172, Loss: 0.4405979418732049\n",
      "Epoch 173, Loss: 0.44094101686109266\n",
      "Epoch 174, Loss: 0.4417368233339534\n",
      "Epoch 175, Loss: 0.4416582519101522\n",
      "Epoch 176, Loss: 0.4396788970821407\n",
      "Epoch 177, Loss: 0.4390195954561405\n",
      "Epoch 178, Loss: 0.4400830988007806\n",
      "Epoch 179, Loss: 0.43931775869233325\n",
      "Epoch 180, Loss: 0.4380032505977321\n",
      "Epoch 181, Loss: 0.4384470974340077\n",
      "Epoch 182, Loss: 0.4385167636581309\n",
      "Epoch 183, Loss: 0.43733043450040754\n",
      "Epoch 184, Loss: 0.4373176399006318\n",
      "Epoch 185, Loss: 0.4375256973621567\n",
      "Epoch 186, Loss: 0.43659322296038683\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2376512695824578\n",
      "Valence RMSE: 0.2507955559853953\n",
      "Arousal RMSE: 0.22373609669940275\n",
      "Test R^2 score: tensor([0.3279, 0.6308], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.47934740070879595\n",
      "Num of epochs: 187\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Epoch 121, Loss: 0.4596922103619757\n",
      "Epoch 122, Loss: 0.45943389956138125\n",
      "Epoch 123, Loss: 0.45877818894117006\n",
      "Epoch 124, Loss: 0.45746538132874076\n",
      "Epoch 125, Loss: 0.4570462803121619\n",
      "Epoch 126, Loss: 0.45696189549769234\n",
      "Epoch 127, Loss: 0.45594920341421274\n",
      "Epoch 128, Loss: 0.45536359747685007\n",
      "Epoch 129, Loss: 0.4553290890752543\n",
      "Epoch 130, Loss: 0.4547195823074053\n",
      "Epoch 131, Loss: 0.45392838323998064\n",
      "Epoch 132, Loss: 0.4537853493595631\n",
      "Epoch 133, Loss: 0.45347437050767975\n",
      "Epoch 134, Loss: 0.45267431273812453\n",
      "Epoch 135, Loss: 0.45228737659373064\n",
      "Epoch 136, Loss: 0.4522154657432981\n",
      "Epoch 137, Loss: 0.45165964487407706\n",
      "Epoch 138, Loss: 0.4510751265462795\n",
      "Epoch 139, Loss: 0.45082650445054473\n",
      "Epoch 140, Loss: 0.4506281098405297\n",
      "Epoch 141, Loss: 0.4501911419656675\n",
      "Epoch 142, Loss: 0.4497001887659326\n",
      "Epoch 143, Loss: 0.44943436133718806\n",
      "Epoch 144, Loss: 0.449226296402944\n",
      "Epoch 145, Loss: 0.44886073897867573\n",
      "Epoch 146, Loss: 0.44844710376304864\n",
      "Epoch 147, Loss: 0.44810507013116724\n",
      "Epoch 148, Loss: 0.44788545862049767\n",
      "Epoch 149, Loss: 0.4476741441009651\n",
      "Epoch 150, Loss: 0.44735861731902055\n",
      "Epoch 151, Loss: 0.4469288720157105\n",
      "Epoch 152, Loss: 0.44655235754178146\n",
      "Epoch 153, Loss: 0.4462857402786373\n",
      "Epoch 154, Loss: 0.4460517369142754\n",
      "Epoch 155, Loss: 0.4457881796184935\n",
      "Epoch 156, Loss: 0.44542210886348255\n",
      "Epoch 157, Loss: 0.4450579467827662\n",
      "Epoch 158, Loss: 0.44473329320345206\n",
      "Epoch 159, Loss: 0.4444482514678638\n",
      "Epoch 160, Loss: 0.4441745674630603\n",
      "Epoch 161, Loss: 0.4439221813722748\n",
      "Epoch 162, Loss: 0.44363357870363757\n",
      "Epoch 163, Loss: 0.4433189071134186\n",
      "Epoch 164, Loss: 0.44295452976444744\n",
      "Epoch 165, Loss: 0.4426058108229731\n",
      "Epoch 166, Loss: 0.4422485787900922\n",
      "Epoch 167, Loss: 0.4419090985167976\n",
      "Epoch 168, Loss: 0.44158054387831963\n",
      "Epoch 169, Loss: 0.4412474388772268\n",
      "Epoch 170, Loss: 0.4409277694136744\n",
      "Epoch 171, Loss: 0.44068045571539705\n",
      "Epoch 172, Loss: 0.4405979418732049\n",
      "Epoch 173, Loss: 0.44094101686109266\n",
      "Epoch 174, Loss: 0.4417368233339534\n",
      "Epoch 175, Loss: 0.4416582519101522\n",
      "Epoch 176, Loss: 0.4396788970821407\n",
      "Epoch 177, Loss: 0.4390195954561405\n",
      "Epoch 178, Loss: 0.4400830988007806\n",
      "Epoch 179, Loss: 0.43931775869233325\n",
      "Epoch 180, Loss: 0.4380032505977321\n",
      "Epoch 181, Loss: 0.4384470974340077\n",
      "Epoch 182, Loss: 0.4385167636581309\n",
      "Epoch 183, Loss: 0.43733043450040754\n",
      "Epoch 184, Loss: 0.4373176399006318\n",
      "Epoch 185, Loss: 0.4375256973621567\n",
      "Epoch 186, Loss: 0.43659322296038683\n",
      "Epoch 187, Loss: 0.43644391045908093\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2375992933806526\n",
      "Valence RMSE: 0.24979723940286452\n",
      "Arousal RMSE: 0.2247402670121197\n",
      "Test R^2 score: tensor([0.3332, 0.6275], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.48035691452253254\n",
      "Num of epochs: 188\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Epoch 121, Loss: 0.4596922103619757\n",
      "Epoch 122, Loss: 0.45943389956138125\n",
      "Epoch 123, Loss: 0.45877818894117006\n",
      "Epoch 124, Loss: 0.45746538132874076\n",
      "Epoch 125, Loss: 0.4570462803121619\n",
      "Epoch 126, Loss: 0.45696189549769234\n",
      "Epoch 127, Loss: 0.45594920341421274\n",
      "Epoch 128, Loss: 0.45536359747685007\n",
      "Epoch 129, Loss: 0.4553290890752543\n",
      "Epoch 130, Loss: 0.4547195823074053\n",
      "Epoch 131, Loss: 0.45392838323998064\n",
      "Epoch 132, Loss: 0.4537853493595631\n",
      "Epoch 133, Loss: 0.45347437050767975\n",
      "Epoch 134, Loss: 0.45267431273812453\n",
      "Epoch 135, Loss: 0.45228737659373064\n",
      "Epoch 136, Loss: 0.4522154657432981\n",
      "Epoch 137, Loss: 0.45165964487407706\n",
      "Epoch 138, Loss: 0.4510751265462795\n",
      "Epoch 139, Loss: 0.45082650445054473\n",
      "Epoch 140, Loss: 0.4506281098405297\n",
      "Epoch 141, Loss: 0.4501911419656675\n",
      "Epoch 142, Loss: 0.4497001887659326\n",
      "Epoch 143, Loss: 0.44943436133718806\n",
      "Epoch 144, Loss: 0.449226296402944\n",
      "Epoch 145, Loss: 0.44886073897867573\n",
      "Epoch 146, Loss: 0.44844710376304864\n",
      "Epoch 147, Loss: 0.44810507013116724\n",
      "Epoch 148, Loss: 0.44788545862049767\n",
      "Epoch 149, Loss: 0.4476741441009651\n",
      "Epoch 150, Loss: 0.44735861731902055\n",
      "Epoch 151, Loss: 0.4469288720157105\n",
      "Epoch 152, Loss: 0.44655235754178146\n",
      "Epoch 153, Loss: 0.4462857402786373\n",
      "Epoch 154, Loss: 0.4460517369142754\n",
      "Epoch 155, Loss: 0.4457881796184935\n",
      "Epoch 156, Loss: 0.44542210886348255\n",
      "Epoch 157, Loss: 0.4450579467827662\n",
      "Epoch 158, Loss: 0.44473329320345206\n",
      "Epoch 159, Loss: 0.4444482514678638\n",
      "Epoch 160, Loss: 0.4441745674630603\n",
      "Epoch 161, Loss: 0.4439221813722748\n",
      "Epoch 162, Loss: 0.44363357870363757\n",
      "Epoch 163, Loss: 0.4433189071134186\n",
      "Epoch 164, Loss: 0.44295452976444744\n",
      "Epoch 165, Loss: 0.4426058108229731\n",
      "Epoch 166, Loss: 0.4422485787900922\n",
      "Epoch 167, Loss: 0.4419090985167976\n",
      "Epoch 168, Loss: 0.44158054387831963\n",
      "Epoch 169, Loss: 0.4412474388772268\n",
      "Epoch 170, Loss: 0.4409277694136744\n",
      "Epoch 171, Loss: 0.44068045571539705\n",
      "Epoch 172, Loss: 0.4405979418732049\n",
      "Epoch 173, Loss: 0.44094101686109266\n",
      "Epoch 174, Loss: 0.4417368233339534\n",
      "Epoch 175, Loss: 0.4416582519101522\n",
      "Epoch 176, Loss: 0.4396788970821407\n",
      "Epoch 177, Loss: 0.4390195954561405\n",
      "Epoch 178, Loss: 0.4400830988007806\n",
      "Epoch 179, Loss: 0.43931775869233325\n",
      "Epoch 180, Loss: 0.4380032505977321\n",
      "Epoch 181, Loss: 0.4384470974340077\n",
      "Epoch 182, Loss: 0.4385167636581309\n",
      "Epoch 183, Loss: 0.43733043450040754\n",
      "Epoch 184, Loss: 0.4373176399006318\n",
      "Epoch 185, Loss: 0.4375256973621567\n",
      "Epoch 186, Loss: 0.43659322296038683\n",
      "Epoch 187, Loss: 0.43644391045908093\n",
      "Epoch 188, Loss: 0.43663844358015674\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23776613032847987\n",
      "Valence RMSE: 0.2508725214684709\n",
      "Arousal RMSE: 0.2238938217879425\n",
      "Test R^2 score: tensor([0.3275, 0.6303], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4788807568462422\n",
      "Num of epochs: 189\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Epoch 121, Loss: 0.4596922103619757\n",
      "Epoch 122, Loss: 0.45943389956138125\n",
      "Epoch 123, Loss: 0.45877818894117006\n",
      "Epoch 124, Loss: 0.45746538132874076\n",
      "Epoch 125, Loss: 0.4570462803121619\n",
      "Epoch 126, Loss: 0.45696189549769234\n",
      "Epoch 127, Loss: 0.45594920341421274\n",
      "Epoch 128, Loss: 0.45536359747685007\n",
      "Epoch 129, Loss: 0.4553290890752543\n",
      "Epoch 130, Loss: 0.4547195823074053\n",
      "Epoch 131, Loss: 0.45392838323998064\n",
      "Epoch 132, Loss: 0.4537853493595631\n",
      "Epoch 133, Loss: 0.45347437050767975\n",
      "Epoch 134, Loss: 0.45267431273812453\n",
      "Epoch 135, Loss: 0.45228737659373064\n",
      "Epoch 136, Loss: 0.4522154657432981\n",
      "Epoch 137, Loss: 0.45165964487407706\n",
      "Epoch 138, Loss: 0.4510751265462795\n",
      "Epoch 139, Loss: 0.45082650445054473\n",
      "Epoch 140, Loss: 0.4506281098405297\n",
      "Epoch 141, Loss: 0.4501911419656675\n",
      "Epoch 142, Loss: 0.4497001887659326\n",
      "Epoch 143, Loss: 0.44943436133718806\n",
      "Epoch 144, Loss: 0.449226296402944\n",
      "Epoch 145, Loss: 0.44886073897867573\n",
      "Epoch 146, Loss: 0.44844710376304864\n",
      "Epoch 147, Loss: 0.44810507013116724\n",
      "Epoch 148, Loss: 0.44788545862049767\n",
      "Epoch 149, Loss: 0.4476741441009651\n",
      "Epoch 150, Loss: 0.44735861731902055\n",
      "Epoch 151, Loss: 0.4469288720157105\n",
      "Epoch 152, Loss: 0.44655235754178146\n",
      "Epoch 153, Loss: 0.4462857402786373\n",
      "Epoch 154, Loss: 0.4460517369142754\n",
      "Epoch 155, Loss: 0.4457881796184935\n",
      "Epoch 156, Loss: 0.44542210886348255\n",
      "Epoch 157, Loss: 0.4450579467827662\n",
      "Epoch 158, Loss: 0.44473329320345206\n",
      "Epoch 159, Loss: 0.4444482514678638\n",
      "Epoch 160, Loss: 0.4441745674630603\n",
      "Epoch 161, Loss: 0.4439221813722748\n",
      "Epoch 162, Loss: 0.44363357870363757\n",
      "Epoch 163, Loss: 0.4433189071134186\n",
      "Epoch 164, Loss: 0.44295452976444744\n",
      "Epoch 165, Loss: 0.4426058108229731\n",
      "Epoch 166, Loss: 0.4422485787900922\n",
      "Epoch 167, Loss: 0.4419090985167976\n",
      "Epoch 168, Loss: 0.44158054387831963\n",
      "Epoch 169, Loss: 0.4412474388772268\n",
      "Epoch 170, Loss: 0.4409277694136744\n",
      "Epoch 171, Loss: 0.44068045571539705\n",
      "Epoch 172, Loss: 0.4405979418732049\n",
      "Epoch 173, Loss: 0.44094101686109266\n",
      "Epoch 174, Loss: 0.4417368233339534\n",
      "Epoch 175, Loss: 0.4416582519101522\n",
      "Epoch 176, Loss: 0.4396788970821407\n",
      "Epoch 177, Loss: 0.4390195954561405\n",
      "Epoch 178, Loss: 0.4400830988007806\n",
      "Epoch 179, Loss: 0.43931775869233325\n",
      "Epoch 180, Loss: 0.4380032505977321\n",
      "Epoch 181, Loss: 0.4384470974340077\n",
      "Epoch 182, Loss: 0.4385167636581309\n",
      "Epoch 183, Loss: 0.43733043450040754\n",
      "Epoch 184, Loss: 0.4373176399006318\n",
      "Epoch 185, Loss: 0.4375256973621567\n",
      "Epoch 186, Loss: 0.43659322296038683\n",
      "Epoch 187, Loss: 0.43644391045908093\n",
      "Epoch 188, Loss: 0.43663844358015674\n",
      "Epoch 189, Loss: 0.43586286705793914\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2376479378278878\n",
      "Valence RMSE: 0.2505793476104006\n",
      "Arousal RMSE: 0.22397114827325693\n",
      "Test R^2 score: tensor([0.3290, 0.6300], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.47953853888715314\n",
      "Num of epochs: 190\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Epoch 121, Loss: 0.4596922103619757\n",
      "Epoch 122, Loss: 0.45943389956138125\n",
      "Epoch 123, Loss: 0.45877818894117006\n",
      "Epoch 124, Loss: 0.45746538132874076\n",
      "Epoch 125, Loss: 0.4570462803121619\n",
      "Epoch 126, Loss: 0.45696189549769234\n",
      "Epoch 127, Loss: 0.45594920341421274\n",
      "Epoch 128, Loss: 0.45536359747685007\n",
      "Epoch 129, Loss: 0.4553290890752543\n",
      "Epoch 130, Loss: 0.4547195823074053\n",
      "Epoch 131, Loss: 0.45392838323998064\n",
      "Epoch 132, Loss: 0.4537853493595631\n",
      "Epoch 133, Loss: 0.45347437050767975\n",
      "Epoch 134, Loss: 0.45267431273812453\n",
      "Epoch 135, Loss: 0.45228737659373064\n",
      "Epoch 136, Loss: 0.4522154657432981\n",
      "Epoch 137, Loss: 0.45165964487407706\n",
      "Epoch 138, Loss: 0.4510751265462795\n",
      "Epoch 139, Loss: 0.45082650445054473\n",
      "Epoch 140, Loss: 0.4506281098405297\n",
      "Epoch 141, Loss: 0.4501911419656675\n",
      "Epoch 142, Loss: 0.4497001887659326\n",
      "Epoch 143, Loss: 0.44943436133718806\n",
      "Epoch 144, Loss: 0.449226296402944\n",
      "Epoch 145, Loss: 0.44886073897867573\n",
      "Epoch 146, Loss: 0.44844710376304864\n",
      "Epoch 147, Loss: 0.44810507013116724\n",
      "Epoch 148, Loss: 0.44788545862049767\n",
      "Epoch 149, Loss: 0.4476741441009651\n",
      "Epoch 150, Loss: 0.44735861731902055\n",
      "Epoch 151, Loss: 0.4469288720157105\n",
      "Epoch 152, Loss: 0.44655235754178146\n",
      "Epoch 153, Loss: 0.4462857402786373\n",
      "Epoch 154, Loss: 0.4460517369142754\n",
      "Epoch 155, Loss: 0.4457881796184935\n",
      "Epoch 156, Loss: 0.44542210886348255\n",
      "Epoch 157, Loss: 0.4450579467827662\n",
      "Epoch 158, Loss: 0.44473329320345206\n",
      "Epoch 159, Loss: 0.4444482514678638\n",
      "Epoch 160, Loss: 0.4441745674630603\n",
      "Epoch 161, Loss: 0.4439221813722748\n",
      "Epoch 162, Loss: 0.44363357870363757\n",
      "Epoch 163, Loss: 0.4433189071134186\n",
      "Epoch 164, Loss: 0.44295452976444744\n",
      "Epoch 165, Loss: 0.4426058108229731\n",
      "Epoch 166, Loss: 0.4422485787900922\n",
      "Epoch 167, Loss: 0.4419090985167976\n",
      "Epoch 168, Loss: 0.44158054387831963\n",
      "Epoch 169, Loss: 0.4412474388772268\n",
      "Epoch 170, Loss: 0.4409277694136744\n",
      "Epoch 171, Loss: 0.44068045571539705\n",
      "Epoch 172, Loss: 0.4405979418732049\n",
      "Epoch 173, Loss: 0.44094101686109266\n",
      "Epoch 174, Loss: 0.4417368233339534\n",
      "Epoch 175, Loss: 0.4416582519101522\n",
      "Epoch 176, Loss: 0.4396788970821407\n",
      "Epoch 177, Loss: 0.4390195954561405\n",
      "Epoch 178, Loss: 0.4400830988007806\n",
      "Epoch 179, Loss: 0.43931775869233325\n",
      "Epoch 180, Loss: 0.4380032505977321\n",
      "Epoch 181, Loss: 0.4384470974340077\n",
      "Epoch 182, Loss: 0.4385167636581309\n",
      "Epoch 183, Loss: 0.43733043450040754\n",
      "Epoch 184, Loss: 0.4373176399006318\n",
      "Epoch 185, Loss: 0.4375256973621567\n",
      "Epoch 186, Loss: 0.43659322296038683\n",
      "Epoch 187, Loss: 0.43644391045908093\n",
      "Epoch 188, Loss: 0.43663844358015674\n",
      "Epoch 189, Loss: 0.43586286705793914\n",
      "Epoch 190, Loss: 0.4355663072844155\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23745737607178943\n",
      "Valence RMSE: 0.24952267977383488\n",
      "Arousal RMSE: 0.2247452851124639\n",
      "Test R^2 score: tensor([0.3347, 0.6275], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.48108108367390257\n",
      "Num of epochs: 191\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Epoch 121, Loss: 0.4596922103619757\n",
      "Epoch 122, Loss: 0.45943389956138125\n",
      "Epoch 123, Loss: 0.45877818894117006\n",
      "Epoch 124, Loss: 0.45746538132874076\n",
      "Epoch 125, Loss: 0.4570462803121619\n",
      "Epoch 126, Loss: 0.45696189549769234\n",
      "Epoch 127, Loss: 0.45594920341421274\n",
      "Epoch 128, Loss: 0.45536359747685007\n",
      "Epoch 129, Loss: 0.4553290890752543\n",
      "Epoch 130, Loss: 0.4547195823074053\n",
      "Epoch 131, Loss: 0.45392838323998064\n",
      "Epoch 132, Loss: 0.4537853493595631\n",
      "Epoch 133, Loss: 0.45347437050767975\n",
      "Epoch 134, Loss: 0.45267431273812453\n",
      "Epoch 135, Loss: 0.45228737659373064\n",
      "Epoch 136, Loss: 0.4522154657432981\n",
      "Epoch 137, Loss: 0.45165964487407706\n",
      "Epoch 138, Loss: 0.4510751265462795\n",
      "Epoch 139, Loss: 0.45082650445054473\n",
      "Epoch 140, Loss: 0.4506281098405297\n",
      "Epoch 141, Loss: 0.4501911419656675\n",
      "Epoch 142, Loss: 0.4497001887659326\n",
      "Epoch 143, Loss: 0.44943436133718806\n",
      "Epoch 144, Loss: 0.449226296402944\n",
      "Epoch 145, Loss: 0.44886073897867573\n",
      "Epoch 146, Loss: 0.44844710376304864\n",
      "Epoch 147, Loss: 0.44810507013116724\n",
      "Epoch 148, Loss: 0.44788545862049767\n",
      "Epoch 149, Loss: 0.4476741441009651\n",
      "Epoch 150, Loss: 0.44735861731902055\n",
      "Epoch 151, Loss: 0.4469288720157105\n",
      "Epoch 152, Loss: 0.44655235754178146\n",
      "Epoch 153, Loss: 0.4462857402786373\n",
      "Epoch 154, Loss: 0.4460517369142754\n",
      "Epoch 155, Loss: 0.4457881796184935\n",
      "Epoch 156, Loss: 0.44542210886348255\n",
      "Epoch 157, Loss: 0.4450579467827662\n",
      "Epoch 158, Loss: 0.44473329320345206\n",
      "Epoch 159, Loss: 0.4444482514678638\n",
      "Epoch 160, Loss: 0.4441745674630603\n",
      "Epoch 161, Loss: 0.4439221813722748\n",
      "Epoch 162, Loss: 0.44363357870363757\n",
      "Epoch 163, Loss: 0.4433189071134186\n",
      "Epoch 164, Loss: 0.44295452976444744\n",
      "Epoch 165, Loss: 0.4426058108229731\n",
      "Epoch 166, Loss: 0.4422485787900922\n",
      "Epoch 167, Loss: 0.4419090985167976\n",
      "Epoch 168, Loss: 0.44158054387831963\n",
      "Epoch 169, Loss: 0.4412474388772268\n",
      "Epoch 170, Loss: 0.4409277694136744\n",
      "Epoch 171, Loss: 0.44068045571539705\n",
      "Epoch 172, Loss: 0.4405979418732049\n",
      "Epoch 173, Loss: 0.44094101686109266\n",
      "Epoch 174, Loss: 0.4417368233339534\n",
      "Epoch 175, Loss: 0.4416582519101522\n",
      "Epoch 176, Loss: 0.4396788970821407\n",
      "Epoch 177, Loss: 0.4390195954561405\n",
      "Epoch 178, Loss: 0.4400830988007806\n",
      "Epoch 179, Loss: 0.43931775869233325\n",
      "Epoch 180, Loss: 0.4380032505977321\n",
      "Epoch 181, Loss: 0.4384470974340077\n",
      "Epoch 182, Loss: 0.4385167636581309\n",
      "Epoch 183, Loss: 0.43733043450040754\n",
      "Epoch 184, Loss: 0.4373176399006318\n",
      "Epoch 185, Loss: 0.4375256973621567\n",
      "Epoch 186, Loss: 0.43659322296038683\n",
      "Epoch 187, Loss: 0.43644391045908093\n",
      "Epoch 188, Loss: 0.43663844358015674\n",
      "Epoch 189, Loss: 0.43586286705793914\n",
      "Epoch 190, Loss: 0.4355663072844155\n",
      "Epoch 191, Loss: 0.4357040532214924\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23763961554121588\n",
      "Valence RMSE: 0.2505596727980669\n",
      "Arousal RMSE: 0.2239754989200369\n",
      "Test R^2 score: tensor([0.3291, 0.6300], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4795840333763682\n",
      "Num of epochs: 192\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Epoch 121, Loss: 0.4596922103619757\n",
      "Epoch 122, Loss: 0.45943389956138125\n",
      "Epoch 123, Loss: 0.45877818894117006\n",
      "Epoch 124, Loss: 0.45746538132874076\n",
      "Epoch 125, Loss: 0.4570462803121619\n",
      "Epoch 126, Loss: 0.45696189549769234\n",
      "Epoch 127, Loss: 0.45594920341421274\n",
      "Epoch 128, Loss: 0.45536359747685007\n",
      "Epoch 129, Loss: 0.4553290890752543\n",
      "Epoch 130, Loss: 0.4547195823074053\n",
      "Epoch 131, Loss: 0.45392838323998064\n",
      "Epoch 132, Loss: 0.4537853493595631\n",
      "Epoch 133, Loss: 0.45347437050767975\n",
      "Epoch 134, Loss: 0.45267431273812453\n",
      "Epoch 135, Loss: 0.45228737659373064\n",
      "Epoch 136, Loss: 0.4522154657432981\n",
      "Epoch 137, Loss: 0.45165964487407706\n",
      "Epoch 138, Loss: 0.4510751265462795\n",
      "Epoch 139, Loss: 0.45082650445054473\n",
      "Epoch 140, Loss: 0.4506281098405297\n",
      "Epoch 141, Loss: 0.4501911419656675\n",
      "Epoch 142, Loss: 0.4497001887659326\n",
      "Epoch 143, Loss: 0.44943436133718806\n",
      "Epoch 144, Loss: 0.449226296402944\n",
      "Epoch 145, Loss: 0.44886073897867573\n",
      "Epoch 146, Loss: 0.44844710376304864\n",
      "Epoch 147, Loss: 0.44810507013116724\n",
      "Epoch 148, Loss: 0.44788545862049767\n",
      "Epoch 149, Loss: 0.4476741441009651\n",
      "Epoch 150, Loss: 0.44735861731902055\n",
      "Epoch 151, Loss: 0.4469288720157105\n",
      "Epoch 152, Loss: 0.44655235754178146\n",
      "Epoch 153, Loss: 0.4462857402786373\n",
      "Epoch 154, Loss: 0.4460517369142754\n",
      "Epoch 155, Loss: 0.4457881796184935\n",
      "Epoch 156, Loss: 0.44542210886348255\n",
      "Epoch 157, Loss: 0.4450579467827662\n",
      "Epoch 158, Loss: 0.44473329320345206\n",
      "Epoch 159, Loss: 0.4444482514678638\n",
      "Epoch 160, Loss: 0.4441745674630603\n",
      "Epoch 161, Loss: 0.4439221813722748\n",
      "Epoch 162, Loss: 0.44363357870363757\n",
      "Epoch 163, Loss: 0.4433189071134186\n",
      "Epoch 164, Loss: 0.44295452976444744\n",
      "Epoch 165, Loss: 0.4426058108229731\n",
      "Epoch 166, Loss: 0.4422485787900922\n",
      "Epoch 167, Loss: 0.4419090985167976\n",
      "Epoch 168, Loss: 0.44158054387831963\n",
      "Epoch 169, Loss: 0.4412474388772268\n",
      "Epoch 170, Loss: 0.4409277694136744\n",
      "Epoch 171, Loss: 0.44068045571539705\n",
      "Epoch 172, Loss: 0.4405979418732049\n",
      "Epoch 173, Loss: 0.44094101686109266\n",
      "Epoch 174, Loss: 0.4417368233339534\n",
      "Epoch 175, Loss: 0.4416582519101522\n",
      "Epoch 176, Loss: 0.4396788970821407\n",
      "Epoch 177, Loss: 0.4390195954561405\n",
      "Epoch 178, Loss: 0.4400830988007806\n",
      "Epoch 179, Loss: 0.43931775869233325\n",
      "Epoch 180, Loss: 0.4380032505977321\n",
      "Epoch 181, Loss: 0.4384470974340077\n",
      "Epoch 182, Loss: 0.4385167636581309\n",
      "Epoch 183, Loss: 0.43733043450040754\n",
      "Epoch 184, Loss: 0.4373176399006318\n",
      "Epoch 185, Loss: 0.4375256973621567\n",
      "Epoch 186, Loss: 0.43659322296038683\n",
      "Epoch 187, Loss: 0.43644391045908093\n",
      "Epoch 188, Loss: 0.43663844358015674\n",
      "Epoch 189, Loss: 0.43586286705793914\n",
      "Epoch 190, Loss: 0.4355663072844155\n",
      "Epoch 191, Loss: 0.4357040532214924\n",
      "Epoch 192, Loss: 0.435134007760397\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23759885424078658\n",
      "Valence RMSE: 0.2504467546787488\n",
      "Arousal RMSE: 0.2240152989060161\n",
      "Test R^2 score: tensor([0.3297, 0.6299], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.47982055269060403\n",
      "Num of epochs: 193\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Epoch 121, Loss: 0.4596922103619757\n",
      "Epoch 122, Loss: 0.45943389956138125\n",
      "Epoch 123, Loss: 0.45877818894117006\n",
      "Epoch 124, Loss: 0.45746538132874076\n",
      "Epoch 125, Loss: 0.4570462803121619\n",
      "Epoch 126, Loss: 0.45696189549769234\n",
      "Epoch 127, Loss: 0.45594920341421274\n",
      "Epoch 128, Loss: 0.45536359747685007\n",
      "Epoch 129, Loss: 0.4553290890752543\n",
      "Epoch 130, Loss: 0.4547195823074053\n",
      "Epoch 131, Loss: 0.45392838323998064\n",
      "Epoch 132, Loss: 0.4537853493595631\n",
      "Epoch 133, Loss: 0.45347437050767975\n",
      "Epoch 134, Loss: 0.45267431273812453\n",
      "Epoch 135, Loss: 0.45228737659373064\n",
      "Epoch 136, Loss: 0.4522154657432981\n",
      "Epoch 137, Loss: 0.45165964487407706\n",
      "Epoch 138, Loss: 0.4510751265462795\n",
      "Epoch 139, Loss: 0.45082650445054473\n",
      "Epoch 140, Loss: 0.4506281098405297\n",
      "Epoch 141, Loss: 0.4501911419656675\n",
      "Epoch 142, Loss: 0.4497001887659326\n",
      "Epoch 143, Loss: 0.44943436133718806\n",
      "Epoch 144, Loss: 0.449226296402944\n",
      "Epoch 145, Loss: 0.44886073897867573\n",
      "Epoch 146, Loss: 0.44844710376304864\n",
      "Epoch 147, Loss: 0.44810507013116724\n",
      "Epoch 148, Loss: 0.44788545862049767\n",
      "Epoch 149, Loss: 0.4476741441009651\n",
      "Epoch 150, Loss: 0.44735861731902055\n",
      "Epoch 151, Loss: 0.4469288720157105\n",
      "Epoch 152, Loss: 0.44655235754178146\n",
      "Epoch 153, Loss: 0.4462857402786373\n",
      "Epoch 154, Loss: 0.4460517369142754\n",
      "Epoch 155, Loss: 0.4457881796184935\n",
      "Epoch 156, Loss: 0.44542210886348255\n",
      "Epoch 157, Loss: 0.4450579467827662\n",
      "Epoch 158, Loss: 0.44473329320345206\n",
      "Epoch 159, Loss: 0.4444482514678638\n",
      "Epoch 160, Loss: 0.4441745674630603\n",
      "Epoch 161, Loss: 0.4439221813722748\n",
      "Epoch 162, Loss: 0.44363357870363757\n",
      "Epoch 163, Loss: 0.4433189071134186\n",
      "Epoch 164, Loss: 0.44295452976444744\n",
      "Epoch 165, Loss: 0.4426058108229731\n",
      "Epoch 166, Loss: 0.4422485787900922\n",
      "Epoch 167, Loss: 0.4419090985167976\n",
      "Epoch 168, Loss: 0.44158054387831963\n",
      "Epoch 169, Loss: 0.4412474388772268\n",
      "Epoch 170, Loss: 0.4409277694136744\n",
      "Epoch 171, Loss: 0.44068045571539705\n",
      "Epoch 172, Loss: 0.4405979418732049\n",
      "Epoch 173, Loss: 0.44094101686109266\n",
      "Epoch 174, Loss: 0.4417368233339534\n",
      "Epoch 175, Loss: 0.4416582519101522\n",
      "Epoch 176, Loss: 0.4396788970821407\n",
      "Epoch 177, Loss: 0.4390195954561405\n",
      "Epoch 178, Loss: 0.4400830988007806\n",
      "Epoch 179, Loss: 0.43931775869233325\n",
      "Epoch 180, Loss: 0.4380032505977321\n",
      "Epoch 181, Loss: 0.4384470974340077\n",
      "Epoch 182, Loss: 0.4385167636581309\n",
      "Epoch 183, Loss: 0.43733043450040754\n",
      "Epoch 184, Loss: 0.4373176399006318\n",
      "Epoch 185, Loss: 0.4375256973621567\n",
      "Epoch 186, Loss: 0.43659322296038683\n",
      "Epoch 187, Loss: 0.43644391045908093\n",
      "Epoch 188, Loss: 0.43663844358015674\n",
      "Epoch 189, Loss: 0.43586286705793914\n",
      "Epoch 190, Loss: 0.4355663072844155\n",
      "Epoch 191, Loss: 0.4357040532214924\n",
      "Epoch 192, Loss: 0.435134007760397\n",
      "Epoch 193, Loss: 0.43474241127906293\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2377413300589616\n",
      "Valence RMSE: 0.2501197912494241\n",
      "Arousal RMSE: 0.2246819308750768\n",
      "Test R^2 score: tensor([0.3315, 0.6277], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4795920391750024\n",
      "Num of epochs: 194\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Epoch 121, Loss: 0.4596922103619757\n",
      "Epoch 122, Loss: 0.45943389956138125\n",
      "Epoch 123, Loss: 0.45877818894117006\n",
      "Epoch 124, Loss: 0.45746538132874076\n",
      "Epoch 125, Loss: 0.4570462803121619\n",
      "Epoch 126, Loss: 0.45696189549769234\n",
      "Epoch 127, Loss: 0.45594920341421274\n",
      "Epoch 128, Loss: 0.45536359747685007\n",
      "Epoch 129, Loss: 0.4553290890752543\n",
      "Epoch 130, Loss: 0.4547195823074053\n",
      "Epoch 131, Loss: 0.45392838323998064\n",
      "Epoch 132, Loss: 0.4537853493595631\n",
      "Epoch 133, Loss: 0.45347437050767975\n",
      "Epoch 134, Loss: 0.45267431273812453\n",
      "Epoch 135, Loss: 0.45228737659373064\n",
      "Epoch 136, Loss: 0.4522154657432981\n",
      "Epoch 137, Loss: 0.45165964487407706\n",
      "Epoch 138, Loss: 0.4510751265462795\n",
      "Epoch 139, Loss: 0.45082650445054473\n",
      "Epoch 140, Loss: 0.4506281098405297\n",
      "Epoch 141, Loss: 0.4501911419656675\n",
      "Epoch 142, Loss: 0.4497001887659326\n",
      "Epoch 143, Loss: 0.44943436133718806\n",
      "Epoch 144, Loss: 0.449226296402944\n",
      "Epoch 145, Loss: 0.44886073897867573\n",
      "Epoch 146, Loss: 0.44844710376304864\n",
      "Epoch 147, Loss: 0.44810507013116724\n",
      "Epoch 148, Loss: 0.44788545862049767\n",
      "Epoch 149, Loss: 0.4476741441009651\n",
      "Epoch 150, Loss: 0.44735861731902055\n",
      "Epoch 151, Loss: 0.4469288720157105\n",
      "Epoch 152, Loss: 0.44655235754178146\n",
      "Epoch 153, Loss: 0.4462857402786373\n",
      "Epoch 154, Loss: 0.4460517369142754\n",
      "Epoch 155, Loss: 0.4457881796184935\n",
      "Epoch 156, Loss: 0.44542210886348255\n",
      "Epoch 157, Loss: 0.4450579467827662\n",
      "Epoch 158, Loss: 0.44473329320345206\n",
      "Epoch 159, Loss: 0.4444482514678638\n",
      "Epoch 160, Loss: 0.4441745674630603\n",
      "Epoch 161, Loss: 0.4439221813722748\n",
      "Epoch 162, Loss: 0.44363357870363757\n",
      "Epoch 163, Loss: 0.4433189071134186\n",
      "Epoch 164, Loss: 0.44295452976444744\n",
      "Epoch 165, Loss: 0.4426058108229731\n",
      "Epoch 166, Loss: 0.4422485787900922\n",
      "Epoch 167, Loss: 0.4419090985167976\n",
      "Epoch 168, Loss: 0.44158054387831963\n",
      "Epoch 169, Loss: 0.4412474388772268\n",
      "Epoch 170, Loss: 0.4409277694136744\n",
      "Epoch 171, Loss: 0.44068045571539705\n",
      "Epoch 172, Loss: 0.4405979418732049\n",
      "Epoch 173, Loss: 0.44094101686109266\n",
      "Epoch 174, Loss: 0.4417368233339534\n",
      "Epoch 175, Loss: 0.4416582519101522\n",
      "Epoch 176, Loss: 0.4396788970821407\n",
      "Epoch 177, Loss: 0.4390195954561405\n",
      "Epoch 178, Loss: 0.4400830988007806\n",
      "Epoch 179, Loss: 0.43931775869233325\n",
      "Epoch 180, Loss: 0.4380032505977321\n",
      "Epoch 181, Loss: 0.4384470974340077\n",
      "Epoch 182, Loss: 0.4385167636581309\n",
      "Epoch 183, Loss: 0.43733043450040754\n",
      "Epoch 184, Loss: 0.4373176399006318\n",
      "Epoch 185, Loss: 0.4375256973621567\n",
      "Epoch 186, Loss: 0.43659322296038683\n",
      "Epoch 187, Loss: 0.43644391045908093\n",
      "Epoch 188, Loss: 0.43663844358015674\n",
      "Epoch 189, Loss: 0.43586286705793914\n",
      "Epoch 190, Loss: 0.4355663072844155\n",
      "Epoch 191, Loss: 0.4357040532214924\n",
      "Epoch 192, Loss: 0.435134007760397\n",
      "Epoch 193, Loss: 0.43474241127906293\n",
      "Epoch 194, Loss: 0.43480482310319635\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2382355619626971\n",
      "Valence RMSE: 0.2515428267447204\n",
      "Arousal RMSE: 0.224139626752254\n",
      "Test R^2 score: tensor([0.3239, 0.6295], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4766752926719773\n",
      "Num of epochs: 195\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Epoch 121, Loss: 0.4596922103619757\n",
      "Epoch 122, Loss: 0.45943389956138125\n",
      "Epoch 123, Loss: 0.45877818894117006\n",
      "Epoch 124, Loss: 0.45746538132874076\n",
      "Epoch 125, Loss: 0.4570462803121619\n",
      "Epoch 126, Loss: 0.45696189549769234\n",
      "Epoch 127, Loss: 0.45594920341421274\n",
      "Epoch 128, Loss: 0.45536359747685007\n",
      "Epoch 129, Loss: 0.4553290890752543\n",
      "Epoch 130, Loss: 0.4547195823074053\n",
      "Epoch 131, Loss: 0.45392838323998064\n",
      "Epoch 132, Loss: 0.4537853493595631\n",
      "Epoch 133, Loss: 0.45347437050767975\n",
      "Epoch 134, Loss: 0.45267431273812453\n",
      "Epoch 135, Loss: 0.45228737659373064\n",
      "Epoch 136, Loss: 0.4522154657432981\n",
      "Epoch 137, Loss: 0.45165964487407706\n",
      "Epoch 138, Loss: 0.4510751265462795\n",
      "Epoch 139, Loss: 0.45082650445054473\n",
      "Epoch 140, Loss: 0.4506281098405297\n",
      "Epoch 141, Loss: 0.4501911419656675\n",
      "Epoch 142, Loss: 0.4497001887659326\n",
      "Epoch 143, Loss: 0.44943436133718806\n",
      "Epoch 144, Loss: 0.449226296402944\n",
      "Epoch 145, Loss: 0.44886073897867573\n",
      "Epoch 146, Loss: 0.44844710376304864\n",
      "Epoch 147, Loss: 0.44810507013116724\n",
      "Epoch 148, Loss: 0.44788545862049767\n",
      "Epoch 149, Loss: 0.4476741441009651\n",
      "Epoch 150, Loss: 0.44735861731902055\n",
      "Epoch 151, Loss: 0.4469288720157105\n",
      "Epoch 152, Loss: 0.44655235754178146\n",
      "Epoch 153, Loss: 0.4462857402786373\n",
      "Epoch 154, Loss: 0.4460517369142754\n",
      "Epoch 155, Loss: 0.4457881796184935\n",
      "Epoch 156, Loss: 0.44542210886348255\n",
      "Epoch 157, Loss: 0.4450579467827662\n",
      "Epoch 158, Loss: 0.44473329320345206\n",
      "Epoch 159, Loss: 0.4444482514678638\n",
      "Epoch 160, Loss: 0.4441745674630603\n",
      "Epoch 161, Loss: 0.4439221813722748\n",
      "Epoch 162, Loss: 0.44363357870363757\n",
      "Epoch 163, Loss: 0.4433189071134186\n",
      "Epoch 164, Loss: 0.44295452976444744\n",
      "Epoch 165, Loss: 0.4426058108229731\n",
      "Epoch 166, Loss: 0.4422485787900922\n",
      "Epoch 167, Loss: 0.4419090985167976\n",
      "Epoch 168, Loss: 0.44158054387831963\n",
      "Epoch 169, Loss: 0.4412474388772268\n",
      "Epoch 170, Loss: 0.4409277694136744\n",
      "Epoch 171, Loss: 0.44068045571539705\n",
      "Epoch 172, Loss: 0.4405979418732049\n",
      "Epoch 173, Loss: 0.44094101686109266\n",
      "Epoch 174, Loss: 0.4417368233339534\n",
      "Epoch 175, Loss: 0.4416582519101522\n",
      "Epoch 176, Loss: 0.4396788970821407\n",
      "Epoch 177, Loss: 0.4390195954561405\n",
      "Epoch 178, Loss: 0.4400830988007806\n",
      "Epoch 179, Loss: 0.43931775869233325\n",
      "Epoch 180, Loss: 0.4380032505977321\n",
      "Epoch 181, Loss: 0.4384470974340077\n",
      "Epoch 182, Loss: 0.4385167636581309\n",
      "Epoch 183, Loss: 0.43733043450040754\n",
      "Epoch 184, Loss: 0.4373176399006318\n",
      "Epoch 185, Loss: 0.4375256973621567\n",
      "Epoch 186, Loss: 0.43659322296038683\n",
      "Epoch 187, Loss: 0.43644391045908093\n",
      "Epoch 188, Loss: 0.43663844358015674\n",
      "Epoch 189, Loss: 0.43586286705793914\n",
      "Epoch 190, Loss: 0.4355663072844155\n",
      "Epoch 191, Loss: 0.4357040532214924\n",
      "Epoch 192, Loss: 0.435134007760397\n",
      "Epoch 193, Loss: 0.43474241127906293\n",
      "Epoch 194, Loss: 0.43480482310319635\n",
      "Epoch 195, Loss: 0.43443826104982264\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2381355326228116\n",
      "Valence RMSE: 0.2511315299673249\n",
      "Arousal RMSE: 0.22438809783803357\n",
      "Test R^2 score: tensor([0.3261, 0.6287], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4773689907512878\n",
      "Num of epochs: 196\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Epoch 121, Loss: 0.4596922103619757\n",
      "Epoch 122, Loss: 0.45943389956138125\n",
      "Epoch 123, Loss: 0.45877818894117006\n",
      "Epoch 124, Loss: 0.45746538132874076\n",
      "Epoch 125, Loss: 0.4570462803121619\n",
      "Epoch 126, Loss: 0.45696189549769234\n",
      "Epoch 127, Loss: 0.45594920341421274\n",
      "Epoch 128, Loss: 0.45536359747685007\n",
      "Epoch 129, Loss: 0.4553290890752543\n",
      "Epoch 130, Loss: 0.4547195823074053\n",
      "Epoch 131, Loss: 0.45392838323998064\n",
      "Epoch 132, Loss: 0.4537853493595631\n",
      "Epoch 133, Loss: 0.45347437050767975\n",
      "Epoch 134, Loss: 0.45267431273812453\n",
      "Epoch 135, Loss: 0.45228737659373064\n",
      "Epoch 136, Loss: 0.4522154657432981\n",
      "Epoch 137, Loss: 0.45165964487407706\n",
      "Epoch 138, Loss: 0.4510751265462795\n",
      "Epoch 139, Loss: 0.45082650445054473\n",
      "Epoch 140, Loss: 0.4506281098405297\n",
      "Epoch 141, Loss: 0.4501911419656675\n",
      "Epoch 142, Loss: 0.4497001887659326\n",
      "Epoch 143, Loss: 0.44943436133718806\n",
      "Epoch 144, Loss: 0.449226296402944\n",
      "Epoch 145, Loss: 0.44886073897867573\n",
      "Epoch 146, Loss: 0.44844710376304864\n",
      "Epoch 147, Loss: 0.44810507013116724\n",
      "Epoch 148, Loss: 0.44788545862049767\n",
      "Epoch 149, Loss: 0.4476741441009651\n",
      "Epoch 150, Loss: 0.44735861731902055\n",
      "Epoch 151, Loss: 0.4469288720157105\n",
      "Epoch 152, Loss: 0.44655235754178146\n",
      "Epoch 153, Loss: 0.4462857402786373\n",
      "Epoch 154, Loss: 0.4460517369142754\n",
      "Epoch 155, Loss: 0.4457881796184935\n",
      "Epoch 156, Loss: 0.44542210886348255\n",
      "Epoch 157, Loss: 0.4450579467827662\n",
      "Epoch 158, Loss: 0.44473329320345206\n",
      "Epoch 159, Loss: 0.4444482514678638\n",
      "Epoch 160, Loss: 0.4441745674630603\n",
      "Epoch 161, Loss: 0.4439221813722748\n",
      "Epoch 162, Loss: 0.44363357870363757\n",
      "Epoch 163, Loss: 0.4433189071134186\n",
      "Epoch 164, Loss: 0.44295452976444744\n",
      "Epoch 165, Loss: 0.4426058108229731\n",
      "Epoch 166, Loss: 0.4422485787900922\n",
      "Epoch 167, Loss: 0.4419090985167976\n",
      "Epoch 168, Loss: 0.44158054387831963\n",
      "Epoch 169, Loss: 0.4412474388772268\n",
      "Epoch 170, Loss: 0.4409277694136744\n",
      "Epoch 171, Loss: 0.44068045571539705\n",
      "Epoch 172, Loss: 0.4405979418732049\n",
      "Epoch 173, Loss: 0.44094101686109266\n",
      "Epoch 174, Loss: 0.4417368233339534\n",
      "Epoch 175, Loss: 0.4416582519101522\n",
      "Epoch 176, Loss: 0.4396788970821407\n",
      "Epoch 177, Loss: 0.4390195954561405\n",
      "Epoch 178, Loss: 0.4400830988007806\n",
      "Epoch 179, Loss: 0.43931775869233325\n",
      "Epoch 180, Loss: 0.4380032505977321\n",
      "Epoch 181, Loss: 0.4384470974340077\n",
      "Epoch 182, Loss: 0.4385167636581309\n",
      "Epoch 183, Loss: 0.43733043450040754\n",
      "Epoch 184, Loss: 0.4373176399006318\n",
      "Epoch 185, Loss: 0.4375256973621567\n",
      "Epoch 186, Loss: 0.43659322296038683\n",
      "Epoch 187, Loss: 0.43644391045908093\n",
      "Epoch 188, Loss: 0.43663844358015674\n",
      "Epoch 189, Loss: 0.43586286705793914\n",
      "Epoch 190, Loss: 0.4355663072844155\n",
      "Epoch 191, Loss: 0.4357040532214924\n",
      "Epoch 192, Loss: 0.435134007760397\n",
      "Epoch 193, Loss: 0.43474241127906293\n",
      "Epoch 194, Loss: 0.43480482310319635\n",
      "Epoch 195, Loss: 0.43443826104982264\n",
      "Epoch 196, Loss: 0.43390767873021246\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23813878551475565\n",
      "Valence RMSE: 0.25073556727530916\n",
      "Arousal RMSE: 0.22483735818589956\n",
      "Test R^2 score: tensor([0.3282, 0.6272], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.47768654833163154\n",
      "Num of epochs: 197\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Epoch 121, Loss: 0.4596922103619757\n",
      "Epoch 122, Loss: 0.45943389956138125\n",
      "Epoch 123, Loss: 0.45877818894117006\n",
      "Epoch 124, Loss: 0.45746538132874076\n",
      "Epoch 125, Loss: 0.4570462803121619\n",
      "Epoch 126, Loss: 0.45696189549769234\n",
      "Epoch 127, Loss: 0.45594920341421274\n",
      "Epoch 128, Loss: 0.45536359747685007\n",
      "Epoch 129, Loss: 0.4553290890752543\n",
      "Epoch 130, Loss: 0.4547195823074053\n",
      "Epoch 131, Loss: 0.45392838323998064\n",
      "Epoch 132, Loss: 0.4537853493595631\n",
      "Epoch 133, Loss: 0.45347437050767975\n",
      "Epoch 134, Loss: 0.45267431273812453\n",
      "Epoch 135, Loss: 0.45228737659373064\n",
      "Epoch 136, Loss: 0.4522154657432981\n",
      "Epoch 137, Loss: 0.45165964487407706\n",
      "Epoch 138, Loss: 0.4510751265462795\n",
      "Epoch 139, Loss: 0.45082650445054473\n",
      "Epoch 140, Loss: 0.4506281098405297\n",
      "Epoch 141, Loss: 0.4501911419656675\n",
      "Epoch 142, Loss: 0.4497001887659326\n",
      "Epoch 143, Loss: 0.44943436133718806\n",
      "Epoch 144, Loss: 0.449226296402944\n",
      "Epoch 145, Loss: 0.44886073897867573\n",
      "Epoch 146, Loss: 0.44844710376304864\n",
      "Epoch 147, Loss: 0.44810507013116724\n",
      "Epoch 148, Loss: 0.44788545862049767\n",
      "Epoch 149, Loss: 0.4476741441009651\n",
      "Epoch 150, Loss: 0.44735861731902055\n",
      "Epoch 151, Loss: 0.4469288720157105\n",
      "Epoch 152, Loss: 0.44655235754178146\n",
      "Epoch 153, Loss: 0.4462857402786373\n",
      "Epoch 154, Loss: 0.4460517369142754\n",
      "Epoch 155, Loss: 0.4457881796184935\n",
      "Epoch 156, Loss: 0.44542210886348255\n",
      "Epoch 157, Loss: 0.4450579467827662\n",
      "Epoch 158, Loss: 0.44473329320345206\n",
      "Epoch 159, Loss: 0.4444482514678638\n",
      "Epoch 160, Loss: 0.4441745674630603\n",
      "Epoch 161, Loss: 0.4439221813722748\n",
      "Epoch 162, Loss: 0.44363357870363757\n",
      "Epoch 163, Loss: 0.4433189071134186\n",
      "Epoch 164, Loss: 0.44295452976444744\n",
      "Epoch 165, Loss: 0.4426058108229731\n",
      "Epoch 166, Loss: 0.4422485787900922\n",
      "Epoch 167, Loss: 0.4419090985167976\n",
      "Epoch 168, Loss: 0.44158054387831963\n",
      "Epoch 169, Loss: 0.4412474388772268\n",
      "Epoch 170, Loss: 0.4409277694136744\n",
      "Epoch 171, Loss: 0.44068045571539705\n",
      "Epoch 172, Loss: 0.4405979418732049\n",
      "Epoch 173, Loss: 0.44094101686109266\n",
      "Epoch 174, Loss: 0.4417368233339534\n",
      "Epoch 175, Loss: 0.4416582519101522\n",
      "Epoch 176, Loss: 0.4396788970821407\n",
      "Epoch 177, Loss: 0.4390195954561405\n",
      "Epoch 178, Loss: 0.4400830988007806\n",
      "Epoch 179, Loss: 0.43931775869233325\n",
      "Epoch 180, Loss: 0.4380032505977321\n",
      "Epoch 181, Loss: 0.4384470974340077\n",
      "Epoch 182, Loss: 0.4385167636581309\n",
      "Epoch 183, Loss: 0.43733043450040754\n",
      "Epoch 184, Loss: 0.4373176399006318\n",
      "Epoch 185, Loss: 0.4375256973621567\n",
      "Epoch 186, Loss: 0.43659322296038683\n",
      "Epoch 187, Loss: 0.43644391045908093\n",
      "Epoch 188, Loss: 0.43663844358015674\n",
      "Epoch 189, Loss: 0.43586286705793914\n",
      "Epoch 190, Loss: 0.4355663072844155\n",
      "Epoch 191, Loss: 0.4357040532214924\n",
      "Epoch 192, Loss: 0.435134007760397\n",
      "Epoch 193, Loss: 0.43474241127906293\n",
      "Epoch 194, Loss: 0.43480482310319635\n",
      "Epoch 195, Loss: 0.43443826104982264\n",
      "Epoch 196, Loss: 0.43390767873021246\n",
      "Epoch 197, Loss: 0.4338897691233524\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23842714315609748\n",
      "Valence RMSE: 0.25157933612246436\n",
      "Arousal RMSE: 0.22450577458795243\n",
      "Test R^2 score: tensor([0.3237, 0.6283], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4759714048376211\n",
      "Num of epochs: 198\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Epoch 121, Loss: 0.4596922103619757\n",
      "Epoch 122, Loss: 0.45943389956138125\n",
      "Epoch 123, Loss: 0.45877818894117006\n",
      "Epoch 124, Loss: 0.45746538132874076\n",
      "Epoch 125, Loss: 0.4570462803121619\n",
      "Epoch 126, Loss: 0.45696189549769234\n",
      "Epoch 127, Loss: 0.45594920341421274\n",
      "Epoch 128, Loss: 0.45536359747685007\n",
      "Epoch 129, Loss: 0.4553290890752543\n",
      "Epoch 130, Loss: 0.4547195823074053\n",
      "Epoch 131, Loss: 0.45392838323998064\n",
      "Epoch 132, Loss: 0.4537853493595631\n",
      "Epoch 133, Loss: 0.45347437050767975\n",
      "Epoch 134, Loss: 0.45267431273812453\n",
      "Epoch 135, Loss: 0.45228737659373064\n",
      "Epoch 136, Loss: 0.4522154657432981\n",
      "Epoch 137, Loss: 0.45165964487407706\n",
      "Epoch 138, Loss: 0.4510751265462795\n",
      "Epoch 139, Loss: 0.45082650445054473\n",
      "Epoch 140, Loss: 0.4506281098405297\n",
      "Epoch 141, Loss: 0.4501911419656675\n",
      "Epoch 142, Loss: 0.4497001887659326\n",
      "Epoch 143, Loss: 0.44943436133718806\n",
      "Epoch 144, Loss: 0.449226296402944\n",
      "Epoch 145, Loss: 0.44886073897867573\n",
      "Epoch 146, Loss: 0.44844710376304864\n",
      "Epoch 147, Loss: 0.44810507013116724\n",
      "Epoch 148, Loss: 0.44788545862049767\n",
      "Epoch 149, Loss: 0.4476741441009651\n",
      "Epoch 150, Loss: 0.44735861731902055\n",
      "Epoch 151, Loss: 0.4469288720157105\n",
      "Epoch 152, Loss: 0.44655235754178146\n",
      "Epoch 153, Loss: 0.4462857402786373\n",
      "Epoch 154, Loss: 0.4460517369142754\n",
      "Epoch 155, Loss: 0.4457881796184935\n",
      "Epoch 156, Loss: 0.44542210886348255\n",
      "Epoch 157, Loss: 0.4450579467827662\n",
      "Epoch 158, Loss: 0.44473329320345206\n",
      "Epoch 159, Loss: 0.4444482514678638\n",
      "Epoch 160, Loss: 0.4441745674630603\n",
      "Epoch 161, Loss: 0.4439221813722748\n",
      "Epoch 162, Loss: 0.44363357870363757\n",
      "Epoch 163, Loss: 0.4433189071134186\n",
      "Epoch 164, Loss: 0.44295452976444744\n",
      "Epoch 165, Loss: 0.4426058108229731\n",
      "Epoch 166, Loss: 0.4422485787900922\n",
      "Epoch 167, Loss: 0.4419090985167976\n",
      "Epoch 168, Loss: 0.44158054387831963\n",
      "Epoch 169, Loss: 0.4412474388772268\n",
      "Epoch 170, Loss: 0.4409277694136744\n",
      "Epoch 171, Loss: 0.44068045571539705\n",
      "Epoch 172, Loss: 0.4405979418732049\n",
      "Epoch 173, Loss: 0.44094101686109266\n",
      "Epoch 174, Loss: 0.4417368233339534\n",
      "Epoch 175, Loss: 0.4416582519101522\n",
      "Epoch 176, Loss: 0.4396788970821407\n",
      "Epoch 177, Loss: 0.4390195954561405\n",
      "Epoch 178, Loss: 0.4400830988007806\n",
      "Epoch 179, Loss: 0.43931775869233325\n",
      "Epoch 180, Loss: 0.4380032505977321\n",
      "Epoch 181, Loss: 0.4384470974340077\n",
      "Epoch 182, Loss: 0.4385167636581309\n",
      "Epoch 183, Loss: 0.43733043450040754\n",
      "Epoch 184, Loss: 0.4373176399006318\n",
      "Epoch 185, Loss: 0.4375256973621567\n",
      "Epoch 186, Loss: 0.43659322296038683\n",
      "Epoch 187, Loss: 0.43644391045908093\n",
      "Epoch 188, Loss: 0.43663844358015674\n",
      "Epoch 189, Loss: 0.43586286705793914\n",
      "Epoch 190, Loss: 0.4355663072844155\n",
      "Epoch 191, Loss: 0.4357040532214924\n",
      "Epoch 192, Loss: 0.435134007760397\n",
      "Epoch 193, Loss: 0.43474241127906293\n",
      "Epoch 194, Loss: 0.43480482310319635\n",
      "Epoch 195, Loss: 0.43443826104982264\n",
      "Epoch 196, Loss: 0.43390767873021246\n",
      "Epoch 197, Loss: 0.4338897691233524\n",
      "Epoch 198, Loss: 0.43377856877811044\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23788536244939706\n",
      "Valence RMSE: 0.2503804647018656\n",
      "Arousal RMSE: 0.22469649358866253\n",
      "Test R^2 score: tensor([0.3301, 0.6276], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.47887082361368\n",
      "Num of epochs: 199\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Epoch 121, Loss: 0.4596922103619757\n",
      "Epoch 122, Loss: 0.45943389956138125\n",
      "Epoch 123, Loss: 0.45877818894117006\n",
      "Epoch 124, Loss: 0.45746538132874076\n",
      "Epoch 125, Loss: 0.4570462803121619\n",
      "Epoch 126, Loss: 0.45696189549769234\n",
      "Epoch 127, Loss: 0.45594920341421274\n",
      "Epoch 128, Loss: 0.45536359747685007\n",
      "Epoch 129, Loss: 0.4553290890752543\n",
      "Epoch 130, Loss: 0.4547195823074053\n",
      "Epoch 131, Loss: 0.45392838323998064\n",
      "Epoch 132, Loss: 0.4537853493595631\n",
      "Epoch 133, Loss: 0.45347437050767975\n",
      "Epoch 134, Loss: 0.45267431273812453\n",
      "Epoch 135, Loss: 0.45228737659373064\n",
      "Epoch 136, Loss: 0.4522154657432981\n",
      "Epoch 137, Loss: 0.45165964487407706\n",
      "Epoch 138, Loss: 0.4510751265462795\n",
      "Epoch 139, Loss: 0.45082650445054473\n",
      "Epoch 140, Loss: 0.4506281098405297\n",
      "Epoch 141, Loss: 0.4501911419656675\n",
      "Epoch 142, Loss: 0.4497001887659326\n",
      "Epoch 143, Loss: 0.44943436133718806\n",
      "Epoch 144, Loss: 0.449226296402944\n",
      "Epoch 145, Loss: 0.44886073897867573\n",
      "Epoch 146, Loss: 0.44844710376304864\n",
      "Epoch 147, Loss: 0.44810507013116724\n",
      "Epoch 148, Loss: 0.44788545862049767\n",
      "Epoch 149, Loss: 0.4476741441009651\n",
      "Epoch 150, Loss: 0.44735861731902055\n",
      "Epoch 151, Loss: 0.4469288720157105\n",
      "Epoch 152, Loss: 0.44655235754178146\n",
      "Epoch 153, Loss: 0.4462857402786373\n",
      "Epoch 154, Loss: 0.4460517369142754\n",
      "Epoch 155, Loss: 0.4457881796184935\n",
      "Epoch 156, Loss: 0.44542210886348255\n",
      "Epoch 157, Loss: 0.4450579467827662\n",
      "Epoch 158, Loss: 0.44473329320345206\n",
      "Epoch 159, Loss: 0.4444482514678638\n",
      "Epoch 160, Loss: 0.4441745674630603\n",
      "Epoch 161, Loss: 0.4439221813722748\n",
      "Epoch 162, Loss: 0.44363357870363757\n",
      "Epoch 163, Loss: 0.4433189071134186\n",
      "Epoch 164, Loss: 0.44295452976444744\n",
      "Epoch 165, Loss: 0.4426058108229731\n",
      "Epoch 166, Loss: 0.4422485787900922\n",
      "Epoch 167, Loss: 0.4419090985167976\n",
      "Epoch 168, Loss: 0.44158054387831963\n",
      "Epoch 169, Loss: 0.4412474388772268\n",
      "Epoch 170, Loss: 0.4409277694136744\n",
      "Epoch 171, Loss: 0.44068045571539705\n",
      "Epoch 172, Loss: 0.4405979418732049\n",
      "Epoch 173, Loss: 0.44094101686109266\n",
      "Epoch 174, Loss: 0.4417368233339534\n",
      "Epoch 175, Loss: 0.4416582519101522\n",
      "Epoch 176, Loss: 0.4396788970821407\n",
      "Epoch 177, Loss: 0.4390195954561405\n",
      "Epoch 178, Loss: 0.4400830988007806\n",
      "Epoch 179, Loss: 0.43931775869233325\n",
      "Epoch 180, Loss: 0.4380032505977321\n",
      "Epoch 181, Loss: 0.4384470974340077\n",
      "Epoch 182, Loss: 0.4385167636581309\n",
      "Epoch 183, Loss: 0.43733043450040754\n",
      "Epoch 184, Loss: 0.4373176399006318\n",
      "Epoch 185, Loss: 0.4375256973621567\n",
      "Epoch 186, Loss: 0.43659322296038683\n",
      "Epoch 187, Loss: 0.43644391045908093\n",
      "Epoch 188, Loss: 0.43663844358015674\n",
      "Epoch 189, Loss: 0.43586286705793914\n",
      "Epoch 190, Loss: 0.4355663072844155\n",
      "Epoch 191, Loss: 0.4357040532214924\n",
      "Epoch 192, Loss: 0.435134007760397\n",
      "Epoch 193, Loss: 0.43474241127906293\n",
      "Epoch 194, Loss: 0.43480482310319635\n",
      "Epoch 195, Loss: 0.43443826104982264\n",
      "Epoch 196, Loss: 0.43390767873021246\n",
      "Epoch 197, Loss: 0.4338897691233524\n",
      "Epoch 198, Loss: 0.43377856877811044\n",
      "Epoch 199, Loss: 0.4331595506415052\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2378720257782592\n",
      "Valence RMSE: 0.2503563899506657\n",
      "Arousal RMSE: 0.22469508073515063\n",
      "Test R^2 score: tensor([0.3302, 0.6277], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.47893757519853825\n",
      "Num of epochs: 200\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Epoch 121, Loss: 0.4596922103619757\n",
      "Epoch 122, Loss: 0.45943389956138125\n",
      "Epoch 123, Loss: 0.45877818894117006\n",
      "Epoch 124, Loss: 0.45746538132874076\n",
      "Epoch 125, Loss: 0.4570462803121619\n",
      "Epoch 126, Loss: 0.45696189549769234\n",
      "Epoch 127, Loss: 0.45594920341421274\n",
      "Epoch 128, Loss: 0.45536359747685007\n",
      "Epoch 129, Loss: 0.4553290890752543\n",
      "Epoch 130, Loss: 0.4547195823074053\n",
      "Epoch 131, Loss: 0.45392838323998064\n",
      "Epoch 132, Loss: 0.4537853493595631\n",
      "Epoch 133, Loss: 0.45347437050767975\n",
      "Epoch 134, Loss: 0.45267431273812453\n",
      "Epoch 135, Loss: 0.45228737659373064\n",
      "Epoch 136, Loss: 0.4522154657432981\n",
      "Epoch 137, Loss: 0.45165964487407706\n",
      "Epoch 138, Loss: 0.4510751265462795\n",
      "Epoch 139, Loss: 0.45082650445054473\n",
      "Epoch 140, Loss: 0.4506281098405297\n",
      "Epoch 141, Loss: 0.4501911419656675\n",
      "Epoch 142, Loss: 0.4497001887659326\n",
      "Epoch 143, Loss: 0.44943436133718806\n",
      "Epoch 144, Loss: 0.449226296402944\n",
      "Epoch 145, Loss: 0.44886073897867573\n",
      "Epoch 146, Loss: 0.44844710376304864\n",
      "Epoch 147, Loss: 0.44810507013116724\n",
      "Epoch 148, Loss: 0.44788545862049767\n",
      "Epoch 149, Loss: 0.4476741441009651\n",
      "Epoch 150, Loss: 0.44735861731902055\n",
      "Epoch 151, Loss: 0.4469288720157105\n",
      "Epoch 152, Loss: 0.44655235754178146\n",
      "Epoch 153, Loss: 0.4462857402786373\n",
      "Epoch 154, Loss: 0.4460517369142754\n",
      "Epoch 155, Loss: 0.4457881796184935\n",
      "Epoch 156, Loss: 0.44542210886348255\n",
      "Epoch 157, Loss: 0.4450579467827662\n",
      "Epoch 158, Loss: 0.44473329320345206\n",
      "Epoch 159, Loss: 0.4444482514678638\n",
      "Epoch 160, Loss: 0.4441745674630603\n",
      "Epoch 161, Loss: 0.4439221813722748\n",
      "Epoch 162, Loss: 0.44363357870363757\n",
      "Epoch 163, Loss: 0.4433189071134186\n",
      "Epoch 164, Loss: 0.44295452976444744\n",
      "Epoch 165, Loss: 0.4426058108229731\n",
      "Epoch 166, Loss: 0.4422485787900922\n",
      "Epoch 167, Loss: 0.4419090985167976\n",
      "Epoch 168, Loss: 0.44158054387831963\n",
      "Epoch 169, Loss: 0.4412474388772268\n",
      "Epoch 170, Loss: 0.4409277694136744\n",
      "Epoch 171, Loss: 0.44068045571539705\n",
      "Epoch 172, Loss: 0.4405979418732049\n",
      "Epoch 173, Loss: 0.44094101686109266\n",
      "Epoch 174, Loss: 0.4417368233339534\n",
      "Epoch 175, Loss: 0.4416582519101522\n",
      "Epoch 176, Loss: 0.4396788970821407\n",
      "Epoch 177, Loss: 0.4390195954561405\n",
      "Epoch 178, Loss: 0.4400830988007806\n",
      "Epoch 179, Loss: 0.43931775869233325\n",
      "Epoch 180, Loss: 0.4380032505977321\n",
      "Epoch 181, Loss: 0.4384470974340077\n",
      "Epoch 182, Loss: 0.4385167636581309\n",
      "Epoch 183, Loss: 0.43733043450040754\n",
      "Epoch 184, Loss: 0.4373176399006318\n",
      "Epoch 185, Loss: 0.4375256973621567\n",
      "Epoch 186, Loss: 0.43659322296038683\n",
      "Epoch 187, Loss: 0.43644391045908093\n",
      "Epoch 188, Loss: 0.43663844358015674\n",
      "Epoch 189, Loss: 0.43586286705793914\n",
      "Epoch 190, Loss: 0.4355663072844155\n",
      "Epoch 191, Loss: 0.4357040532214924\n",
      "Epoch 192, Loss: 0.435134007760397\n",
      "Epoch 193, Loss: 0.43474241127906293\n",
      "Epoch 194, Loss: 0.43480482310319635\n",
      "Epoch 195, Loss: 0.43443826104982264\n",
      "Epoch 196, Loss: 0.43390767873021246\n",
      "Epoch 197, Loss: 0.4338897691233524\n",
      "Epoch 198, Loss: 0.43377856877811044\n",
      "Epoch 199, Loss: 0.4331595506415052\n",
      "Epoch 200, Loss: 0.43289415091975497\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23844505884194228\n",
      "Valence RMSE: 0.2515081838478495\n",
      "Arousal RMSE: 0.2246235197610236\n",
      "Test R^2 score: tensor([0.3240, 0.6279], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.47596765715590866\n",
      "Num of epochs: 201\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Epoch 121, Loss: 0.4596922103619757\n",
      "Epoch 122, Loss: 0.45943389956138125\n",
      "Epoch 123, Loss: 0.45877818894117006\n",
      "Epoch 124, Loss: 0.45746538132874076\n",
      "Epoch 125, Loss: 0.4570462803121619\n",
      "Epoch 126, Loss: 0.45696189549769234\n",
      "Epoch 127, Loss: 0.45594920341421274\n",
      "Epoch 128, Loss: 0.45536359747685007\n",
      "Epoch 129, Loss: 0.4553290890752543\n",
      "Epoch 130, Loss: 0.4547195823074053\n",
      "Epoch 131, Loss: 0.45392838323998064\n",
      "Epoch 132, Loss: 0.4537853493595631\n",
      "Epoch 133, Loss: 0.45347437050767975\n",
      "Epoch 134, Loss: 0.45267431273812453\n",
      "Epoch 135, Loss: 0.45228737659373064\n",
      "Epoch 136, Loss: 0.4522154657432981\n",
      "Epoch 137, Loss: 0.45165964487407706\n",
      "Epoch 138, Loss: 0.4510751265462795\n",
      "Epoch 139, Loss: 0.45082650445054473\n",
      "Epoch 140, Loss: 0.4506281098405297\n",
      "Epoch 141, Loss: 0.4501911419656675\n",
      "Epoch 142, Loss: 0.4497001887659326\n",
      "Epoch 143, Loss: 0.44943436133718806\n",
      "Epoch 144, Loss: 0.449226296402944\n",
      "Epoch 145, Loss: 0.44886073897867573\n",
      "Epoch 146, Loss: 0.44844710376304864\n",
      "Epoch 147, Loss: 0.44810507013116724\n",
      "Epoch 148, Loss: 0.44788545862049767\n",
      "Epoch 149, Loss: 0.4476741441009651\n",
      "Epoch 150, Loss: 0.44735861731902055\n",
      "Epoch 151, Loss: 0.4469288720157105\n",
      "Epoch 152, Loss: 0.44655235754178146\n",
      "Epoch 153, Loss: 0.4462857402786373\n",
      "Epoch 154, Loss: 0.4460517369142754\n",
      "Epoch 155, Loss: 0.4457881796184935\n",
      "Epoch 156, Loss: 0.44542210886348255\n",
      "Epoch 157, Loss: 0.4450579467827662\n",
      "Epoch 158, Loss: 0.44473329320345206\n",
      "Epoch 159, Loss: 0.4444482514678638\n",
      "Epoch 160, Loss: 0.4441745674630603\n",
      "Epoch 161, Loss: 0.4439221813722748\n",
      "Epoch 162, Loss: 0.44363357870363757\n",
      "Epoch 163, Loss: 0.4433189071134186\n",
      "Epoch 164, Loss: 0.44295452976444744\n",
      "Epoch 165, Loss: 0.4426058108229731\n",
      "Epoch 166, Loss: 0.4422485787900922\n",
      "Epoch 167, Loss: 0.4419090985167976\n",
      "Epoch 168, Loss: 0.44158054387831963\n",
      "Epoch 169, Loss: 0.4412474388772268\n",
      "Epoch 170, Loss: 0.4409277694136744\n",
      "Epoch 171, Loss: 0.44068045571539705\n",
      "Epoch 172, Loss: 0.4405979418732049\n",
      "Epoch 173, Loss: 0.44094101686109266\n",
      "Epoch 174, Loss: 0.4417368233339534\n",
      "Epoch 175, Loss: 0.4416582519101522\n",
      "Epoch 176, Loss: 0.4396788970821407\n",
      "Epoch 177, Loss: 0.4390195954561405\n",
      "Epoch 178, Loss: 0.4400830988007806\n",
      "Epoch 179, Loss: 0.43931775869233325\n",
      "Epoch 180, Loss: 0.4380032505977321\n",
      "Epoch 181, Loss: 0.4384470974340077\n",
      "Epoch 182, Loss: 0.4385167636581309\n",
      "Epoch 183, Loss: 0.43733043450040754\n",
      "Epoch 184, Loss: 0.4373176399006318\n",
      "Epoch 185, Loss: 0.4375256973621567\n",
      "Epoch 186, Loss: 0.43659322296038683\n",
      "Epoch 187, Loss: 0.43644391045908093\n",
      "Epoch 188, Loss: 0.43663844358015674\n",
      "Epoch 189, Loss: 0.43586286705793914\n",
      "Epoch 190, Loss: 0.4355663072844155\n",
      "Epoch 191, Loss: 0.4357040532214924\n",
      "Epoch 192, Loss: 0.435134007760397\n",
      "Epoch 193, Loss: 0.43474241127906293\n",
      "Epoch 194, Loss: 0.43480482310319635\n",
      "Epoch 195, Loss: 0.43443826104982264\n",
      "Epoch 196, Loss: 0.43390767873021246\n",
      "Epoch 197, Loss: 0.4338897691233524\n",
      "Epoch 198, Loss: 0.43377856877811044\n",
      "Epoch 199, Loss: 0.4331595506415052\n",
      "Epoch 200, Loss: 0.43289415091975497\n",
      "Epoch 201, Loss: 0.43294048069736957\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23820159798379958\n",
      "Valence RMSE: 0.2505880061644494\n",
      "Arousal RMSE: 0.2251347457204179\n",
      "Test R^2 score: tensor([0.3290, 0.6262], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4775883562524353\n",
      "Num of epochs: 202\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Epoch 121, Loss: 0.4596922103619757\n",
      "Epoch 122, Loss: 0.45943389956138125\n",
      "Epoch 123, Loss: 0.45877818894117006\n",
      "Epoch 124, Loss: 0.45746538132874076\n",
      "Epoch 125, Loss: 0.4570462803121619\n",
      "Epoch 126, Loss: 0.45696189549769234\n",
      "Epoch 127, Loss: 0.45594920341421274\n",
      "Epoch 128, Loss: 0.45536359747685007\n",
      "Epoch 129, Loss: 0.4553290890752543\n",
      "Epoch 130, Loss: 0.4547195823074053\n",
      "Epoch 131, Loss: 0.45392838323998064\n",
      "Epoch 132, Loss: 0.4537853493595631\n",
      "Epoch 133, Loss: 0.45347437050767975\n",
      "Epoch 134, Loss: 0.45267431273812453\n",
      "Epoch 135, Loss: 0.45228737659373064\n",
      "Epoch 136, Loss: 0.4522154657432981\n",
      "Epoch 137, Loss: 0.45165964487407706\n",
      "Epoch 138, Loss: 0.4510751265462795\n",
      "Epoch 139, Loss: 0.45082650445054473\n",
      "Epoch 140, Loss: 0.4506281098405297\n",
      "Epoch 141, Loss: 0.4501911419656675\n",
      "Epoch 142, Loss: 0.4497001887659326\n",
      "Epoch 143, Loss: 0.44943436133718806\n",
      "Epoch 144, Loss: 0.449226296402944\n",
      "Epoch 145, Loss: 0.44886073897867573\n",
      "Epoch 146, Loss: 0.44844710376304864\n",
      "Epoch 147, Loss: 0.44810507013116724\n",
      "Epoch 148, Loss: 0.44788545862049767\n",
      "Epoch 149, Loss: 0.4476741441009651\n",
      "Epoch 150, Loss: 0.44735861731902055\n",
      "Epoch 151, Loss: 0.4469288720157105\n",
      "Epoch 152, Loss: 0.44655235754178146\n",
      "Epoch 153, Loss: 0.4462857402786373\n",
      "Epoch 154, Loss: 0.4460517369142754\n",
      "Epoch 155, Loss: 0.4457881796184935\n",
      "Epoch 156, Loss: 0.44542210886348255\n",
      "Epoch 157, Loss: 0.4450579467827662\n",
      "Epoch 158, Loss: 0.44473329320345206\n",
      "Epoch 159, Loss: 0.4444482514678638\n",
      "Epoch 160, Loss: 0.4441745674630603\n",
      "Epoch 161, Loss: 0.4439221813722748\n",
      "Epoch 162, Loss: 0.44363357870363757\n",
      "Epoch 163, Loss: 0.4433189071134186\n",
      "Epoch 164, Loss: 0.44295452976444744\n",
      "Epoch 165, Loss: 0.4426058108229731\n",
      "Epoch 166, Loss: 0.4422485787900922\n",
      "Epoch 167, Loss: 0.4419090985167976\n",
      "Epoch 168, Loss: 0.44158054387831963\n",
      "Epoch 169, Loss: 0.4412474388772268\n",
      "Epoch 170, Loss: 0.4409277694136744\n",
      "Epoch 171, Loss: 0.44068045571539705\n",
      "Epoch 172, Loss: 0.4405979418732049\n",
      "Epoch 173, Loss: 0.44094101686109266\n",
      "Epoch 174, Loss: 0.4417368233339534\n",
      "Epoch 175, Loss: 0.4416582519101522\n",
      "Epoch 176, Loss: 0.4396788970821407\n",
      "Epoch 177, Loss: 0.4390195954561405\n",
      "Epoch 178, Loss: 0.4400830988007806\n",
      "Epoch 179, Loss: 0.43931775869233325\n",
      "Epoch 180, Loss: 0.4380032505977321\n",
      "Epoch 181, Loss: 0.4384470974340077\n",
      "Epoch 182, Loss: 0.4385167636581309\n",
      "Epoch 183, Loss: 0.43733043450040754\n",
      "Epoch 184, Loss: 0.4373176399006318\n",
      "Epoch 185, Loss: 0.4375256973621567\n",
      "Epoch 186, Loss: 0.43659322296038683\n",
      "Epoch 187, Loss: 0.43644391045908093\n",
      "Epoch 188, Loss: 0.43663844358015674\n",
      "Epoch 189, Loss: 0.43586286705793914\n",
      "Epoch 190, Loss: 0.4355663072844155\n",
      "Epoch 191, Loss: 0.4357040532214924\n",
      "Epoch 192, Loss: 0.435134007760397\n",
      "Epoch 193, Loss: 0.43474241127906293\n",
      "Epoch 194, Loss: 0.43480482310319635\n",
      "Epoch 195, Loss: 0.43443826104982264\n",
      "Epoch 196, Loss: 0.43390767873021246\n",
      "Epoch 197, Loss: 0.4338897691233524\n",
      "Epoch 198, Loss: 0.43377856877811044\n",
      "Epoch 199, Loss: 0.4331595506415052\n",
      "Epoch 200, Loss: 0.43289415091975497\n",
      "Epoch 201, Loss: 0.43294048069736957\n",
      "Epoch 202, Loss: 0.43262240576772243\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23841997054694863\n",
      "Valence RMSE: 0.2512866072315105\n",
      "Arousal RMSE: 0.22481816149344402\n",
      "Test R^2 score: tensor([0.3252, 0.6272], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.47624032515931225\n",
      "Num of epochs: 203\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Epoch 121, Loss: 0.4596922103619757\n",
      "Epoch 122, Loss: 0.45943389956138125\n",
      "Epoch 123, Loss: 0.45877818894117006\n",
      "Epoch 124, Loss: 0.45746538132874076\n",
      "Epoch 125, Loss: 0.4570462803121619\n",
      "Epoch 126, Loss: 0.45696189549769234\n",
      "Epoch 127, Loss: 0.45594920341421274\n",
      "Epoch 128, Loss: 0.45536359747685007\n",
      "Epoch 129, Loss: 0.4553290890752543\n",
      "Epoch 130, Loss: 0.4547195823074053\n",
      "Epoch 131, Loss: 0.45392838323998064\n",
      "Epoch 132, Loss: 0.4537853493595631\n",
      "Epoch 133, Loss: 0.45347437050767975\n",
      "Epoch 134, Loss: 0.45267431273812453\n",
      "Epoch 135, Loss: 0.45228737659373064\n",
      "Epoch 136, Loss: 0.4522154657432981\n",
      "Epoch 137, Loss: 0.45165964487407706\n",
      "Epoch 138, Loss: 0.4510751265462795\n",
      "Epoch 139, Loss: 0.45082650445054473\n",
      "Epoch 140, Loss: 0.4506281098405297\n",
      "Epoch 141, Loss: 0.4501911419656675\n",
      "Epoch 142, Loss: 0.4497001887659326\n",
      "Epoch 143, Loss: 0.44943436133718806\n",
      "Epoch 144, Loss: 0.449226296402944\n",
      "Epoch 145, Loss: 0.44886073897867573\n",
      "Epoch 146, Loss: 0.44844710376304864\n",
      "Epoch 147, Loss: 0.44810507013116724\n",
      "Epoch 148, Loss: 0.44788545862049767\n",
      "Epoch 149, Loss: 0.4476741441009651\n",
      "Epoch 150, Loss: 0.44735861731902055\n",
      "Epoch 151, Loss: 0.4469288720157105\n",
      "Epoch 152, Loss: 0.44655235754178146\n",
      "Epoch 153, Loss: 0.4462857402786373\n",
      "Epoch 154, Loss: 0.4460517369142754\n",
      "Epoch 155, Loss: 0.4457881796184935\n",
      "Epoch 156, Loss: 0.44542210886348255\n",
      "Epoch 157, Loss: 0.4450579467827662\n",
      "Epoch 158, Loss: 0.44473329320345206\n",
      "Epoch 159, Loss: 0.4444482514678638\n",
      "Epoch 160, Loss: 0.4441745674630603\n",
      "Epoch 161, Loss: 0.4439221813722748\n",
      "Epoch 162, Loss: 0.44363357870363757\n",
      "Epoch 163, Loss: 0.4433189071134186\n",
      "Epoch 164, Loss: 0.44295452976444744\n",
      "Epoch 165, Loss: 0.4426058108229731\n",
      "Epoch 166, Loss: 0.4422485787900922\n",
      "Epoch 167, Loss: 0.4419090985167976\n",
      "Epoch 168, Loss: 0.44158054387831963\n",
      "Epoch 169, Loss: 0.4412474388772268\n",
      "Epoch 170, Loss: 0.4409277694136744\n",
      "Epoch 171, Loss: 0.44068045571539705\n",
      "Epoch 172, Loss: 0.4405979418732049\n",
      "Epoch 173, Loss: 0.44094101686109266\n",
      "Epoch 174, Loss: 0.4417368233339534\n",
      "Epoch 175, Loss: 0.4416582519101522\n",
      "Epoch 176, Loss: 0.4396788970821407\n",
      "Epoch 177, Loss: 0.4390195954561405\n",
      "Epoch 178, Loss: 0.4400830988007806\n",
      "Epoch 179, Loss: 0.43931775869233325\n",
      "Epoch 180, Loss: 0.4380032505977321\n",
      "Epoch 181, Loss: 0.4384470974340077\n",
      "Epoch 182, Loss: 0.4385167636581309\n",
      "Epoch 183, Loss: 0.43733043450040754\n",
      "Epoch 184, Loss: 0.4373176399006318\n",
      "Epoch 185, Loss: 0.4375256973621567\n",
      "Epoch 186, Loss: 0.43659322296038683\n",
      "Epoch 187, Loss: 0.43644391045908093\n",
      "Epoch 188, Loss: 0.43663844358015674\n",
      "Epoch 189, Loss: 0.43586286705793914\n",
      "Epoch 190, Loss: 0.4355663072844155\n",
      "Epoch 191, Loss: 0.4357040532214924\n",
      "Epoch 192, Loss: 0.435134007760397\n",
      "Epoch 193, Loss: 0.43474241127906293\n",
      "Epoch 194, Loss: 0.43480482310319635\n",
      "Epoch 195, Loss: 0.43443826104982264\n",
      "Epoch 196, Loss: 0.43390767873021246\n",
      "Epoch 197, Loss: 0.4338897691233524\n",
      "Epoch 198, Loss: 0.43377856877811044\n",
      "Epoch 199, Loss: 0.4331595506415052\n",
      "Epoch 200, Loss: 0.43289415091975497\n",
      "Epoch 201, Loss: 0.43294048069736957\n",
      "Epoch 202, Loss: 0.43262240576772243\n",
      "Epoch 203, Loss: 0.43206541812705346\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23839087653134178\n",
      "Valence RMSE: 0.2511451299664524\n",
      "Arousal RMSE: 0.22491452536663045\n",
      "Test R^2 score: tensor([0.3260, 0.6269], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.47646031095003766\n",
      "Num of epochs: 204\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Epoch 121, Loss: 0.4596922103619757\n",
      "Epoch 122, Loss: 0.45943389956138125\n",
      "Epoch 123, Loss: 0.45877818894117006\n",
      "Epoch 124, Loss: 0.45746538132874076\n",
      "Epoch 125, Loss: 0.4570462803121619\n",
      "Epoch 126, Loss: 0.45696189549769234\n",
      "Epoch 127, Loss: 0.45594920341421274\n",
      "Epoch 128, Loss: 0.45536359747685007\n",
      "Epoch 129, Loss: 0.4553290890752543\n",
      "Epoch 130, Loss: 0.4547195823074053\n",
      "Epoch 131, Loss: 0.45392838323998064\n",
      "Epoch 132, Loss: 0.4537853493595631\n",
      "Epoch 133, Loss: 0.45347437050767975\n",
      "Epoch 134, Loss: 0.45267431273812453\n",
      "Epoch 135, Loss: 0.45228737659373064\n",
      "Epoch 136, Loss: 0.4522154657432981\n",
      "Epoch 137, Loss: 0.45165964487407706\n",
      "Epoch 138, Loss: 0.4510751265462795\n",
      "Epoch 139, Loss: 0.45082650445054473\n",
      "Epoch 140, Loss: 0.4506281098405297\n",
      "Epoch 141, Loss: 0.4501911419656675\n",
      "Epoch 142, Loss: 0.4497001887659326\n",
      "Epoch 143, Loss: 0.44943436133718806\n",
      "Epoch 144, Loss: 0.449226296402944\n",
      "Epoch 145, Loss: 0.44886073897867573\n",
      "Epoch 146, Loss: 0.44844710376304864\n",
      "Epoch 147, Loss: 0.44810507013116724\n",
      "Epoch 148, Loss: 0.44788545862049767\n",
      "Epoch 149, Loss: 0.4476741441009651\n",
      "Epoch 150, Loss: 0.44735861731902055\n",
      "Epoch 151, Loss: 0.4469288720157105\n",
      "Epoch 152, Loss: 0.44655235754178146\n",
      "Epoch 153, Loss: 0.4462857402786373\n",
      "Epoch 154, Loss: 0.4460517369142754\n",
      "Epoch 155, Loss: 0.4457881796184935\n",
      "Epoch 156, Loss: 0.44542210886348255\n",
      "Epoch 157, Loss: 0.4450579467827662\n",
      "Epoch 158, Loss: 0.44473329320345206\n",
      "Epoch 159, Loss: 0.4444482514678638\n",
      "Epoch 160, Loss: 0.4441745674630603\n",
      "Epoch 161, Loss: 0.4439221813722748\n",
      "Epoch 162, Loss: 0.44363357870363757\n",
      "Epoch 163, Loss: 0.4433189071134186\n",
      "Epoch 164, Loss: 0.44295452976444744\n",
      "Epoch 165, Loss: 0.4426058108229731\n",
      "Epoch 166, Loss: 0.4422485787900922\n",
      "Epoch 167, Loss: 0.4419090985167976\n",
      "Epoch 168, Loss: 0.44158054387831963\n",
      "Epoch 169, Loss: 0.4412474388772268\n",
      "Epoch 170, Loss: 0.4409277694136744\n",
      "Epoch 171, Loss: 0.44068045571539705\n",
      "Epoch 172, Loss: 0.4405979418732049\n",
      "Epoch 173, Loss: 0.44094101686109266\n",
      "Epoch 174, Loss: 0.4417368233339534\n",
      "Epoch 175, Loss: 0.4416582519101522\n",
      "Epoch 176, Loss: 0.4396788970821407\n",
      "Epoch 177, Loss: 0.4390195954561405\n",
      "Epoch 178, Loss: 0.4400830988007806\n",
      "Epoch 179, Loss: 0.43931775869233325\n",
      "Epoch 180, Loss: 0.4380032505977321\n",
      "Epoch 181, Loss: 0.4384470974340077\n",
      "Epoch 182, Loss: 0.4385167636581309\n",
      "Epoch 183, Loss: 0.43733043450040754\n",
      "Epoch 184, Loss: 0.4373176399006318\n",
      "Epoch 185, Loss: 0.4375256973621567\n",
      "Epoch 186, Loss: 0.43659322296038683\n",
      "Epoch 187, Loss: 0.43644391045908093\n",
      "Epoch 188, Loss: 0.43663844358015674\n",
      "Epoch 189, Loss: 0.43586286705793914\n",
      "Epoch 190, Loss: 0.4355663072844155\n",
      "Epoch 191, Loss: 0.4357040532214924\n",
      "Epoch 192, Loss: 0.435134007760397\n",
      "Epoch 193, Loss: 0.43474241127906293\n",
      "Epoch 194, Loss: 0.43480482310319635\n",
      "Epoch 195, Loss: 0.43443826104982264\n",
      "Epoch 196, Loss: 0.43390767873021246\n",
      "Epoch 197, Loss: 0.4338897691233524\n",
      "Epoch 198, Loss: 0.43377856877811044\n",
      "Epoch 199, Loss: 0.4331595506415052\n",
      "Epoch 200, Loss: 0.43289415091975497\n",
      "Epoch 201, Loss: 0.43294048069736957\n",
      "Epoch 202, Loss: 0.43262240576772243\n",
      "Epoch 203, Loss: 0.43206541812705346\n",
      "Epoch 204, Loss: 0.43172500614102416\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23841081366225625\n",
      "Valence RMSE: 0.2508550209447791\n",
      "Arousal RMSE: 0.22528024904325863\n",
      "Test R^2 score: tensor([0.3276, 0.6257], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.47663130351650174\n",
      "Num of epochs: 205\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Epoch 121, Loss: 0.4596922103619757\n",
      "Epoch 122, Loss: 0.45943389956138125\n",
      "Epoch 123, Loss: 0.45877818894117006\n",
      "Epoch 124, Loss: 0.45746538132874076\n",
      "Epoch 125, Loss: 0.4570462803121619\n",
      "Epoch 126, Loss: 0.45696189549769234\n",
      "Epoch 127, Loss: 0.45594920341421274\n",
      "Epoch 128, Loss: 0.45536359747685007\n",
      "Epoch 129, Loss: 0.4553290890752543\n",
      "Epoch 130, Loss: 0.4547195823074053\n",
      "Epoch 131, Loss: 0.45392838323998064\n",
      "Epoch 132, Loss: 0.4537853493595631\n",
      "Epoch 133, Loss: 0.45347437050767975\n",
      "Epoch 134, Loss: 0.45267431273812453\n",
      "Epoch 135, Loss: 0.45228737659373064\n",
      "Epoch 136, Loss: 0.4522154657432981\n",
      "Epoch 137, Loss: 0.45165964487407706\n",
      "Epoch 138, Loss: 0.4510751265462795\n",
      "Epoch 139, Loss: 0.45082650445054473\n",
      "Epoch 140, Loss: 0.4506281098405297\n",
      "Epoch 141, Loss: 0.4501911419656675\n",
      "Epoch 142, Loss: 0.4497001887659326\n",
      "Epoch 143, Loss: 0.44943436133718806\n",
      "Epoch 144, Loss: 0.449226296402944\n",
      "Epoch 145, Loss: 0.44886073897867573\n",
      "Epoch 146, Loss: 0.44844710376304864\n",
      "Epoch 147, Loss: 0.44810507013116724\n",
      "Epoch 148, Loss: 0.44788545862049767\n",
      "Epoch 149, Loss: 0.4476741441009651\n",
      "Epoch 150, Loss: 0.44735861731902055\n",
      "Epoch 151, Loss: 0.4469288720157105\n",
      "Epoch 152, Loss: 0.44655235754178146\n",
      "Epoch 153, Loss: 0.4462857402786373\n",
      "Epoch 154, Loss: 0.4460517369142754\n",
      "Epoch 155, Loss: 0.4457881796184935\n",
      "Epoch 156, Loss: 0.44542210886348255\n",
      "Epoch 157, Loss: 0.4450579467827662\n",
      "Epoch 158, Loss: 0.44473329320345206\n",
      "Epoch 159, Loss: 0.4444482514678638\n",
      "Epoch 160, Loss: 0.4441745674630603\n",
      "Epoch 161, Loss: 0.4439221813722748\n",
      "Epoch 162, Loss: 0.44363357870363757\n",
      "Epoch 163, Loss: 0.4433189071134186\n",
      "Epoch 164, Loss: 0.44295452976444744\n",
      "Epoch 165, Loss: 0.4426058108229731\n",
      "Epoch 166, Loss: 0.4422485787900922\n",
      "Epoch 167, Loss: 0.4419090985167976\n",
      "Epoch 168, Loss: 0.44158054387831963\n",
      "Epoch 169, Loss: 0.4412474388772268\n",
      "Epoch 170, Loss: 0.4409277694136744\n",
      "Epoch 171, Loss: 0.44068045571539705\n",
      "Epoch 172, Loss: 0.4405979418732049\n",
      "Epoch 173, Loss: 0.44094101686109266\n",
      "Epoch 174, Loss: 0.4417368233339534\n",
      "Epoch 175, Loss: 0.4416582519101522\n",
      "Epoch 176, Loss: 0.4396788970821407\n",
      "Epoch 177, Loss: 0.4390195954561405\n",
      "Epoch 178, Loss: 0.4400830988007806\n",
      "Epoch 179, Loss: 0.43931775869233325\n",
      "Epoch 180, Loss: 0.4380032505977321\n",
      "Epoch 181, Loss: 0.4384470974340077\n",
      "Epoch 182, Loss: 0.4385167636581309\n",
      "Epoch 183, Loss: 0.43733043450040754\n",
      "Epoch 184, Loss: 0.4373176399006318\n",
      "Epoch 185, Loss: 0.4375256973621567\n",
      "Epoch 186, Loss: 0.43659322296038683\n",
      "Epoch 187, Loss: 0.43644391045908093\n",
      "Epoch 188, Loss: 0.43663844358015674\n",
      "Epoch 189, Loss: 0.43586286705793914\n",
      "Epoch 190, Loss: 0.4355663072844155\n",
      "Epoch 191, Loss: 0.4357040532214924\n",
      "Epoch 192, Loss: 0.435134007760397\n",
      "Epoch 193, Loss: 0.43474241127906293\n",
      "Epoch 194, Loss: 0.43480482310319635\n",
      "Epoch 195, Loss: 0.43443826104982264\n",
      "Epoch 196, Loss: 0.43390767873021246\n",
      "Epoch 197, Loss: 0.4338897691233524\n",
      "Epoch 198, Loss: 0.43377856877811044\n",
      "Epoch 199, Loss: 0.4331595506415052\n",
      "Epoch 200, Loss: 0.43289415091975497\n",
      "Epoch 201, Loss: 0.43294048069736957\n",
      "Epoch 202, Loss: 0.43262240576772243\n",
      "Epoch 203, Loss: 0.43206541812705346\n",
      "Epoch 204, Loss: 0.43172500614102416\n",
      "Epoch 205, Loss: 0.4316686388133938\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2390604772474887\n",
      "Valence RMSE: 0.25210385625849796\n",
      "Arousal RMSE: 0.22526311110163758\n",
      "Test R^2 score: tensor([0.3208, 0.6258], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.47330377791477674\n",
      "Num of epochs: 206\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Epoch 121, Loss: 0.4596922103619757\n",
      "Epoch 122, Loss: 0.45943389956138125\n",
      "Epoch 123, Loss: 0.45877818894117006\n",
      "Epoch 124, Loss: 0.45746538132874076\n",
      "Epoch 125, Loss: 0.4570462803121619\n",
      "Epoch 126, Loss: 0.45696189549769234\n",
      "Epoch 127, Loss: 0.45594920341421274\n",
      "Epoch 128, Loss: 0.45536359747685007\n",
      "Epoch 129, Loss: 0.4553290890752543\n",
      "Epoch 130, Loss: 0.4547195823074053\n",
      "Epoch 131, Loss: 0.45392838323998064\n",
      "Epoch 132, Loss: 0.4537853493595631\n",
      "Epoch 133, Loss: 0.45347437050767975\n",
      "Epoch 134, Loss: 0.45267431273812453\n",
      "Epoch 135, Loss: 0.45228737659373064\n",
      "Epoch 136, Loss: 0.4522154657432981\n",
      "Epoch 137, Loss: 0.45165964487407706\n",
      "Epoch 138, Loss: 0.4510751265462795\n",
      "Epoch 139, Loss: 0.45082650445054473\n",
      "Epoch 140, Loss: 0.4506281098405297\n",
      "Epoch 141, Loss: 0.4501911419656675\n",
      "Epoch 142, Loss: 0.4497001887659326\n",
      "Epoch 143, Loss: 0.44943436133718806\n",
      "Epoch 144, Loss: 0.449226296402944\n",
      "Epoch 145, Loss: 0.44886073897867573\n",
      "Epoch 146, Loss: 0.44844710376304864\n",
      "Epoch 147, Loss: 0.44810507013116724\n",
      "Epoch 148, Loss: 0.44788545862049767\n",
      "Epoch 149, Loss: 0.4476741441009651\n",
      "Epoch 150, Loss: 0.44735861731902055\n",
      "Epoch 151, Loss: 0.4469288720157105\n",
      "Epoch 152, Loss: 0.44655235754178146\n",
      "Epoch 153, Loss: 0.4462857402786373\n",
      "Epoch 154, Loss: 0.4460517369142754\n",
      "Epoch 155, Loss: 0.4457881796184935\n",
      "Epoch 156, Loss: 0.44542210886348255\n",
      "Epoch 157, Loss: 0.4450579467827662\n",
      "Epoch 158, Loss: 0.44473329320345206\n",
      "Epoch 159, Loss: 0.4444482514678638\n",
      "Epoch 160, Loss: 0.4441745674630603\n",
      "Epoch 161, Loss: 0.4439221813722748\n",
      "Epoch 162, Loss: 0.44363357870363757\n",
      "Epoch 163, Loss: 0.4433189071134186\n",
      "Epoch 164, Loss: 0.44295452976444744\n",
      "Epoch 165, Loss: 0.4426058108229731\n",
      "Epoch 166, Loss: 0.4422485787900922\n",
      "Epoch 167, Loss: 0.4419090985167976\n",
      "Epoch 168, Loss: 0.44158054387831963\n",
      "Epoch 169, Loss: 0.4412474388772268\n",
      "Epoch 170, Loss: 0.4409277694136744\n",
      "Epoch 171, Loss: 0.44068045571539705\n",
      "Epoch 172, Loss: 0.4405979418732049\n",
      "Epoch 173, Loss: 0.44094101686109266\n",
      "Epoch 174, Loss: 0.4417368233339534\n",
      "Epoch 175, Loss: 0.4416582519101522\n",
      "Epoch 176, Loss: 0.4396788970821407\n",
      "Epoch 177, Loss: 0.4390195954561405\n",
      "Epoch 178, Loss: 0.4400830988007806\n",
      "Epoch 179, Loss: 0.43931775869233325\n",
      "Epoch 180, Loss: 0.4380032505977321\n",
      "Epoch 181, Loss: 0.4384470974340077\n",
      "Epoch 182, Loss: 0.4385167636581309\n",
      "Epoch 183, Loss: 0.43733043450040754\n",
      "Epoch 184, Loss: 0.4373176399006318\n",
      "Epoch 185, Loss: 0.4375256973621567\n",
      "Epoch 186, Loss: 0.43659322296038683\n",
      "Epoch 187, Loss: 0.43644391045908093\n",
      "Epoch 188, Loss: 0.43663844358015674\n",
      "Epoch 189, Loss: 0.43586286705793914\n",
      "Epoch 190, Loss: 0.4355663072844155\n",
      "Epoch 191, Loss: 0.4357040532214924\n",
      "Epoch 192, Loss: 0.435134007760397\n",
      "Epoch 193, Loss: 0.43474241127906293\n",
      "Epoch 194, Loss: 0.43480482310319635\n",
      "Epoch 195, Loss: 0.43443826104982264\n",
      "Epoch 196, Loss: 0.43390767873021246\n",
      "Epoch 197, Loss: 0.4338897691233524\n",
      "Epoch 198, Loss: 0.43377856877811044\n",
      "Epoch 199, Loss: 0.4331595506415052\n",
      "Epoch 200, Loss: 0.43289415091975497\n",
      "Epoch 201, Loss: 0.43294048069736957\n",
      "Epoch 202, Loss: 0.43262240576772243\n",
      "Epoch 203, Loss: 0.43206541812705346\n",
      "Epoch 204, Loss: 0.43172500614102416\n",
      "Epoch 205, Loss: 0.4316686388133938\n",
      "Epoch 206, Loss: 0.4315179158466364\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2386489499838549\n",
      "Valence RMSE: 0.2511486124593962\n",
      "Arousal RMSE: 0.22545735099240602\n",
      "Test R^2 score: tensor([0.3260, 0.6251], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4755494741983929\n",
      "Num of epochs: 207\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Epoch 121, Loss: 0.4596922103619757\n",
      "Epoch 122, Loss: 0.45943389956138125\n",
      "Epoch 123, Loss: 0.45877818894117006\n",
      "Epoch 124, Loss: 0.45746538132874076\n",
      "Epoch 125, Loss: 0.4570462803121619\n",
      "Epoch 126, Loss: 0.45696189549769234\n",
      "Epoch 127, Loss: 0.45594920341421274\n",
      "Epoch 128, Loss: 0.45536359747685007\n",
      "Epoch 129, Loss: 0.4553290890752543\n",
      "Epoch 130, Loss: 0.4547195823074053\n",
      "Epoch 131, Loss: 0.45392838323998064\n",
      "Epoch 132, Loss: 0.4537853493595631\n",
      "Epoch 133, Loss: 0.45347437050767975\n",
      "Epoch 134, Loss: 0.45267431273812453\n",
      "Epoch 135, Loss: 0.45228737659373064\n",
      "Epoch 136, Loss: 0.4522154657432981\n",
      "Epoch 137, Loss: 0.45165964487407706\n",
      "Epoch 138, Loss: 0.4510751265462795\n",
      "Epoch 139, Loss: 0.45082650445054473\n",
      "Epoch 140, Loss: 0.4506281098405297\n",
      "Epoch 141, Loss: 0.4501911419656675\n",
      "Epoch 142, Loss: 0.4497001887659326\n",
      "Epoch 143, Loss: 0.44943436133718806\n",
      "Epoch 144, Loss: 0.449226296402944\n",
      "Epoch 145, Loss: 0.44886073897867573\n",
      "Epoch 146, Loss: 0.44844710376304864\n",
      "Epoch 147, Loss: 0.44810507013116724\n",
      "Epoch 148, Loss: 0.44788545862049767\n",
      "Epoch 149, Loss: 0.4476741441009651\n",
      "Epoch 150, Loss: 0.44735861731902055\n",
      "Epoch 151, Loss: 0.4469288720157105\n",
      "Epoch 152, Loss: 0.44655235754178146\n",
      "Epoch 153, Loss: 0.4462857402786373\n",
      "Epoch 154, Loss: 0.4460517369142754\n",
      "Epoch 155, Loss: 0.4457881796184935\n",
      "Epoch 156, Loss: 0.44542210886348255\n",
      "Epoch 157, Loss: 0.4450579467827662\n",
      "Epoch 158, Loss: 0.44473329320345206\n",
      "Epoch 159, Loss: 0.4444482514678638\n",
      "Epoch 160, Loss: 0.4441745674630603\n",
      "Epoch 161, Loss: 0.4439221813722748\n",
      "Epoch 162, Loss: 0.44363357870363757\n",
      "Epoch 163, Loss: 0.4433189071134186\n",
      "Epoch 164, Loss: 0.44295452976444744\n",
      "Epoch 165, Loss: 0.4426058108229731\n",
      "Epoch 166, Loss: 0.4422485787900922\n",
      "Epoch 167, Loss: 0.4419090985167976\n",
      "Epoch 168, Loss: 0.44158054387831963\n",
      "Epoch 169, Loss: 0.4412474388772268\n",
      "Epoch 170, Loss: 0.4409277694136744\n",
      "Epoch 171, Loss: 0.44068045571539705\n",
      "Epoch 172, Loss: 0.4405979418732049\n",
      "Epoch 173, Loss: 0.44094101686109266\n",
      "Epoch 174, Loss: 0.4417368233339534\n",
      "Epoch 175, Loss: 0.4416582519101522\n",
      "Epoch 176, Loss: 0.4396788970821407\n",
      "Epoch 177, Loss: 0.4390195954561405\n",
      "Epoch 178, Loss: 0.4400830988007806\n",
      "Epoch 179, Loss: 0.43931775869233325\n",
      "Epoch 180, Loss: 0.4380032505977321\n",
      "Epoch 181, Loss: 0.4384470974340077\n",
      "Epoch 182, Loss: 0.4385167636581309\n",
      "Epoch 183, Loss: 0.43733043450040754\n",
      "Epoch 184, Loss: 0.4373176399006318\n",
      "Epoch 185, Loss: 0.4375256973621567\n",
      "Epoch 186, Loss: 0.43659322296038683\n",
      "Epoch 187, Loss: 0.43644391045908093\n",
      "Epoch 188, Loss: 0.43663844358015674\n",
      "Epoch 189, Loss: 0.43586286705793914\n",
      "Epoch 190, Loss: 0.4355663072844155\n",
      "Epoch 191, Loss: 0.4357040532214924\n",
      "Epoch 192, Loss: 0.435134007760397\n",
      "Epoch 193, Loss: 0.43474241127906293\n",
      "Epoch 194, Loss: 0.43480482310319635\n",
      "Epoch 195, Loss: 0.43443826104982264\n",
      "Epoch 196, Loss: 0.43390767873021246\n",
      "Epoch 197, Loss: 0.4338897691233524\n",
      "Epoch 198, Loss: 0.43377856877811044\n",
      "Epoch 199, Loss: 0.4331595506415052\n",
      "Epoch 200, Loss: 0.43289415091975497\n",
      "Epoch 201, Loss: 0.43294048069736957\n",
      "Epoch 202, Loss: 0.43262240576772243\n",
      "Epoch 203, Loss: 0.43206541812705346\n",
      "Epoch 204, Loss: 0.43172500614102416\n",
      "Epoch 205, Loss: 0.4316686388133938\n",
      "Epoch 206, Loss: 0.4315179158466364\n",
      "Epoch 207, Loss: 0.4310712381632941\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2388105826434892\n",
      "Valence RMSE: 0.2515586603059604\n",
      "Arousal RMSE: 0.22534247089733436\n",
      "Test R^2 score: tensor([0.3238, 0.6255], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4746390698115633\n",
      "Num of epochs: 208\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Epoch 121, Loss: 0.4596922103619757\n",
      "Epoch 122, Loss: 0.45943389956138125\n",
      "Epoch 123, Loss: 0.45877818894117006\n",
      "Epoch 124, Loss: 0.45746538132874076\n",
      "Epoch 125, Loss: 0.4570462803121619\n",
      "Epoch 126, Loss: 0.45696189549769234\n",
      "Epoch 127, Loss: 0.45594920341421274\n",
      "Epoch 128, Loss: 0.45536359747685007\n",
      "Epoch 129, Loss: 0.4553290890752543\n",
      "Epoch 130, Loss: 0.4547195823074053\n",
      "Epoch 131, Loss: 0.45392838323998064\n",
      "Epoch 132, Loss: 0.4537853493595631\n",
      "Epoch 133, Loss: 0.45347437050767975\n",
      "Epoch 134, Loss: 0.45267431273812453\n",
      "Epoch 135, Loss: 0.45228737659373064\n",
      "Epoch 136, Loss: 0.4522154657432981\n",
      "Epoch 137, Loss: 0.45165964487407706\n",
      "Epoch 138, Loss: 0.4510751265462795\n",
      "Epoch 139, Loss: 0.45082650445054473\n",
      "Epoch 140, Loss: 0.4506281098405297\n",
      "Epoch 141, Loss: 0.4501911419656675\n",
      "Epoch 142, Loss: 0.4497001887659326\n",
      "Epoch 143, Loss: 0.44943436133718806\n",
      "Epoch 144, Loss: 0.449226296402944\n",
      "Epoch 145, Loss: 0.44886073897867573\n",
      "Epoch 146, Loss: 0.44844710376304864\n",
      "Epoch 147, Loss: 0.44810507013116724\n",
      "Epoch 148, Loss: 0.44788545862049767\n",
      "Epoch 149, Loss: 0.4476741441009651\n",
      "Epoch 150, Loss: 0.44735861731902055\n",
      "Epoch 151, Loss: 0.4469288720157105\n",
      "Epoch 152, Loss: 0.44655235754178146\n",
      "Epoch 153, Loss: 0.4462857402786373\n",
      "Epoch 154, Loss: 0.4460517369142754\n",
      "Epoch 155, Loss: 0.4457881796184935\n",
      "Epoch 156, Loss: 0.44542210886348255\n",
      "Epoch 157, Loss: 0.4450579467827662\n",
      "Epoch 158, Loss: 0.44473329320345206\n",
      "Epoch 159, Loss: 0.4444482514678638\n",
      "Epoch 160, Loss: 0.4441745674630603\n",
      "Epoch 161, Loss: 0.4439221813722748\n",
      "Epoch 162, Loss: 0.44363357870363757\n",
      "Epoch 163, Loss: 0.4433189071134186\n",
      "Epoch 164, Loss: 0.44295452976444744\n",
      "Epoch 165, Loss: 0.4426058108229731\n",
      "Epoch 166, Loss: 0.4422485787900922\n",
      "Epoch 167, Loss: 0.4419090985167976\n",
      "Epoch 168, Loss: 0.44158054387831963\n",
      "Epoch 169, Loss: 0.4412474388772268\n",
      "Epoch 170, Loss: 0.4409277694136744\n",
      "Epoch 171, Loss: 0.44068045571539705\n",
      "Epoch 172, Loss: 0.4405979418732049\n",
      "Epoch 173, Loss: 0.44094101686109266\n",
      "Epoch 174, Loss: 0.4417368233339534\n",
      "Epoch 175, Loss: 0.4416582519101522\n",
      "Epoch 176, Loss: 0.4396788970821407\n",
      "Epoch 177, Loss: 0.4390195954561405\n",
      "Epoch 178, Loss: 0.4400830988007806\n",
      "Epoch 179, Loss: 0.43931775869233325\n",
      "Epoch 180, Loss: 0.4380032505977321\n",
      "Epoch 181, Loss: 0.4384470974340077\n",
      "Epoch 182, Loss: 0.4385167636581309\n",
      "Epoch 183, Loss: 0.43733043450040754\n",
      "Epoch 184, Loss: 0.4373176399006318\n",
      "Epoch 185, Loss: 0.4375256973621567\n",
      "Epoch 186, Loss: 0.43659322296038683\n",
      "Epoch 187, Loss: 0.43644391045908093\n",
      "Epoch 188, Loss: 0.43663844358015674\n",
      "Epoch 189, Loss: 0.43586286705793914\n",
      "Epoch 190, Loss: 0.4355663072844155\n",
      "Epoch 191, Loss: 0.4357040532214924\n",
      "Epoch 192, Loss: 0.435134007760397\n",
      "Epoch 193, Loss: 0.43474241127906293\n",
      "Epoch 194, Loss: 0.43480482310319635\n",
      "Epoch 195, Loss: 0.43443826104982264\n",
      "Epoch 196, Loss: 0.43390767873021246\n",
      "Epoch 197, Loss: 0.4338897691233524\n",
      "Epoch 198, Loss: 0.43377856877811044\n",
      "Epoch 199, Loss: 0.4331595506415052\n",
      "Epoch 200, Loss: 0.43289415091975497\n",
      "Epoch 201, Loss: 0.43294048069736957\n",
      "Epoch 202, Loss: 0.43262240576772243\n",
      "Epoch 203, Loss: 0.43206541812705346\n",
      "Epoch 204, Loss: 0.43172500614102416\n",
      "Epoch 205, Loss: 0.4316686388133938\n",
      "Epoch 206, Loss: 0.4315179158466364\n",
      "Epoch 207, Loss: 0.4310712381632941\n",
      "Epoch 208, Loss: 0.43059470035496\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23896277701725613\n",
      "Valence RMSE: 0.25175000768551853\n",
      "Arousal RMSE: 0.22545143874000695\n",
      "Test R^2 score: tensor([0.3227, 0.6251], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4739433674264018\n",
      "Num of epochs: 209\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Epoch 121, Loss: 0.4596922103619757\n",
      "Epoch 122, Loss: 0.45943389956138125\n",
      "Epoch 123, Loss: 0.45877818894117006\n",
      "Epoch 124, Loss: 0.45746538132874076\n",
      "Epoch 125, Loss: 0.4570462803121619\n",
      "Epoch 126, Loss: 0.45696189549769234\n",
      "Epoch 127, Loss: 0.45594920341421274\n",
      "Epoch 128, Loss: 0.45536359747685007\n",
      "Epoch 129, Loss: 0.4553290890752543\n",
      "Epoch 130, Loss: 0.4547195823074053\n",
      "Epoch 131, Loss: 0.45392838323998064\n",
      "Epoch 132, Loss: 0.4537853493595631\n",
      "Epoch 133, Loss: 0.45347437050767975\n",
      "Epoch 134, Loss: 0.45267431273812453\n",
      "Epoch 135, Loss: 0.45228737659373064\n",
      "Epoch 136, Loss: 0.4522154657432981\n",
      "Epoch 137, Loss: 0.45165964487407706\n",
      "Epoch 138, Loss: 0.4510751265462795\n",
      "Epoch 139, Loss: 0.45082650445054473\n",
      "Epoch 140, Loss: 0.4506281098405297\n",
      "Epoch 141, Loss: 0.4501911419656675\n",
      "Epoch 142, Loss: 0.4497001887659326\n",
      "Epoch 143, Loss: 0.44943436133718806\n",
      "Epoch 144, Loss: 0.449226296402944\n",
      "Epoch 145, Loss: 0.44886073897867573\n",
      "Epoch 146, Loss: 0.44844710376304864\n",
      "Epoch 147, Loss: 0.44810507013116724\n",
      "Epoch 148, Loss: 0.44788545862049767\n",
      "Epoch 149, Loss: 0.4476741441009651\n",
      "Epoch 150, Loss: 0.44735861731902055\n",
      "Epoch 151, Loss: 0.4469288720157105\n",
      "Epoch 152, Loss: 0.44655235754178146\n",
      "Epoch 153, Loss: 0.4462857402786373\n",
      "Epoch 154, Loss: 0.4460517369142754\n",
      "Epoch 155, Loss: 0.4457881796184935\n",
      "Epoch 156, Loss: 0.44542210886348255\n",
      "Epoch 157, Loss: 0.4450579467827662\n",
      "Epoch 158, Loss: 0.44473329320345206\n",
      "Epoch 159, Loss: 0.4444482514678638\n",
      "Epoch 160, Loss: 0.4441745674630603\n",
      "Epoch 161, Loss: 0.4439221813722748\n",
      "Epoch 162, Loss: 0.44363357870363757\n",
      "Epoch 163, Loss: 0.4433189071134186\n",
      "Epoch 164, Loss: 0.44295452976444744\n",
      "Epoch 165, Loss: 0.4426058108229731\n",
      "Epoch 166, Loss: 0.4422485787900922\n",
      "Epoch 167, Loss: 0.4419090985167976\n",
      "Epoch 168, Loss: 0.44158054387831963\n",
      "Epoch 169, Loss: 0.4412474388772268\n",
      "Epoch 170, Loss: 0.4409277694136744\n",
      "Epoch 171, Loss: 0.44068045571539705\n",
      "Epoch 172, Loss: 0.4405979418732049\n",
      "Epoch 173, Loss: 0.44094101686109266\n",
      "Epoch 174, Loss: 0.4417368233339534\n",
      "Epoch 175, Loss: 0.4416582519101522\n",
      "Epoch 176, Loss: 0.4396788970821407\n",
      "Epoch 177, Loss: 0.4390195954561405\n",
      "Epoch 178, Loss: 0.4400830988007806\n",
      "Epoch 179, Loss: 0.43931775869233325\n",
      "Epoch 180, Loss: 0.4380032505977321\n",
      "Epoch 181, Loss: 0.4384470974340077\n",
      "Epoch 182, Loss: 0.4385167636581309\n",
      "Epoch 183, Loss: 0.43733043450040754\n",
      "Epoch 184, Loss: 0.4373176399006318\n",
      "Epoch 185, Loss: 0.4375256973621567\n",
      "Epoch 186, Loss: 0.43659322296038683\n",
      "Epoch 187, Loss: 0.43644391045908093\n",
      "Epoch 188, Loss: 0.43663844358015674\n",
      "Epoch 189, Loss: 0.43586286705793914\n",
      "Epoch 190, Loss: 0.4355663072844155\n",
      "Epoch 191, Loss: 0.4357040532214924\n",
      "Epoch 192, Loss: 0.435134007760397\n",
      "Epoch 193, Loss: 0.43474241127906293\n",
      "Epoch 194, Loss: 0.43480482310319635\n",
      "Epoch 195, Loss: 0.43443826104982264\n",
      "Epoch 196, Loss: 0.43390767873021246\n",
      "Epoch 197, Loss: 0.4338897691233524\n",
      "Epoch 198, Loss: 0.43377856877811044\n",
      "Epoch 199, Loss: 0.4331595506415052\n",
      "Epoch 200, Loss: 0.43289415091975497\n",
      "Epoch 201, Loss: 0.43294048069736957\n",
      "Epoch 202, Loss: 0.43262240576772243\n",
      "Epoch 203, Loss: 0.43206541812705346\n",
      "Epoch 204, Loss: 0.43172500614102416\n",
      "Epoch 205, Loss: 0.4316686388133938\n",
      "Epoch 206, Loss: 0.4315179158466364\n",
      "Epoch 207, Loss: 0.4310712381632941\n",
      "Epoch 208, Loss: 0.43059470035496\n",
      "Epoch 209, Loss: 0.4303711221416647\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2388426641657824\n",
      "Valence RMSE: 0.25112892639129675\n",
      "Arousal RMSE: 0.22588912940018185\n",
      "Test R^2 score: tensor([0.3261, 0.6237], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4748836828215711\n",
      "Num of epochs: 210\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Epoch 121, Loss: 0.4596922103619757\n",
      "Epoch 122, Loss: 0.45943389956138125\n",
      "Epoch 123, Loss: 0.45877818894117006\n",
      "Epoch 124, Loss: 0.45746538132874076\n",
      "Epoch 125, Loss: 0.4570462803121619\n",
      "Epoch 126, Loss: 0.45696189549769234\n",
      "Epoch 127, Loss: 0.45594920341421274\n",
      "Epoch 128, Loss: 0.45536359747685007\n",
      "Epoch 129, Loss: 0.4553290890752543\n",
      "Epoch 130, Loss: 0.4547195823074053\n",
      "Epoch 131, Loss: 0.45392838323998064\n",
      "Epoch 132, Loss: 0.4537853493595631\n",
      "Epoch 133, Loss: 0.45347437050767975\n",
      "Epoch 134, Loss: 0.45267431273812453\n",
      "Epoch 135, Loss: 0.45228737659373064\n",
      "Epoch 136, Loss: 0.4522154657432981\n",
      "Epoch 137, Loss: 0.45165964487407706\n",
      "Epoch 138, Loss: 0.4510751265462795\n",
      "Epoch 139, Loss: 0.45082650445054473\n",
      "Epoch 140, Loss: 0.4506281098405297\n",
      "Epoch 141, Loss: 0.4501911419656675\n",
      "Epoch 142, Loss: 0.4497001887659326\n",
      "Epoch 143, Loss: 0.44943436133718806\n",
      "Epoch 144, Loss: 0.449226296402944\n",
      "Epoch 145, Loss: 0.44886073897867573\n",
      "Epoch 146, Loss: 0.44844710376304864\n",
      "Epoch 147, Loss: 0.44810507013116724\n",
      "Epoch 148, Loss: 0.44788545862049767\n",
      "Epoch 149, Loss: 0.4476741441009651\n",
      "Epoch 150, Loss: 0.44735861731902055\n",
      "Epoch 151, Loss: 0.4469288720157105\n",
      "Epoch 152, Loss: 0.44655235754178146\n",
      "Epoch 153, Loss: 0.4462857402786373\n",
      "Epoch 154, Loss: 0.4460517369142754\n",
      "Epoch 155, Loss: 0.4457881796184935\n",
      "Epoch 156, Loss: 0.44542210886348255\n",
      "Epoch 157, Loss: 0.4450579467827662\n",
      "Epoch 158, Loss: 0.44473329320345206\n",
      "Epoch 159, Loss: 0.4444482514678638\n",
      "Epoch 160, Loss: 0.4441745674630603\n",
      "Epoch 161, Loss: 0.4439221813722748\n",
      "Epoch 162, Loss: 0.44363357870363757\n",
      "Epoch 163, Loss: 0.4433189071134186\n",
      "Epoch 164, Loss: 0.44295452976444744\n",
      "Epoch 165, Loss: 0.4426058108229731\n",
      "Epoch 166, Loss: 0.4422485787900922\n",
      "Epoch 167, Loss: 0.4419090985167976\n",
      "Epoch 168, Loss: 0.44158054387831963\n",
      "Epoch 169, Loss: 0.4412474388772268\n",
      "Epoch 170, Loss: 0.4409277694136744\n",
      "Epoch 171, Loss: 0.44068045571539705\n",
      "Epoch 172, Loss: 0.4405979418732049\n",
      "Epoch 173, Loss: 0.44094101686109266\n",
      "Epoch 174, Loss: 0.4417368233339534\n",
      "Epoch 175, Loss: 0.4416582519101522\n",
      "Epoch 176, Loss: 0.4396788970821407\n",
      "Epoch 177, Loss: 0.4390195954561405\n",
      "Epoch 178, Loss: 0.4400830988007806\n",
      "Epoch 179, Loss: 0.43931775869233325\n",
      "Epoch 180, Loss: 0.4380032505977321\n",
      "Epoch 181, Loss: 0.4384470974340077\n",
      "Epoch 182, Loss: 0.4385167636581309\n",
      "Epoch 183, Loss: 0.43733043450040754\n",
      "Epoch 184, Loss: 0.4373176399006318\n",
      "Epoch 185, Loss: 0.4375256973621567\n",
      "Epoch 186, Loss: 0.43659322296038683\n",
      "Epoch 187, Loss: 0.43644391045908093\n",
      "Epoch 188, Loss: 0.43663844358015674\n",
      "Epoch 189, Loss: 0.43586286705793914\n",
      "Epoch 190, Loss: 0.4355663072844155\n",
      "Epoch 191, Loss: 0.4357040532214924\n",
      "Epoch 192, Loss: 0.435134007760397\n",
      "Epoch 193, Loss: 0.43474241127906293\n",
      "Epoch 194, Loss: 0.43480482310319635\n",
      "Epoch 195, Loss: 0.43443826104982264\n",
      "Epoch 196, Loss: 0.43390767873021246\n",
      "Epoch 197, Loss: 0.4338897691233524\n",
      "Epoch 198, Loss: 0.43377856877811044\n",
      "Epoch 199, Loss: 0.4331595506415052\n",
      "Epoch 200, Loss: 0.43289415091975497\n",
      "Epoch 201, Loss: 0.43294048069736957\n",
      "Epoch 202, Loss: 0.43262240576772243\n",
      "Epoch 203, Loss: 0.43206541812705346\n",
      "Epoch 204, Loss: 0.43172500614102416\n",
      "Epoch 205, Loss: 0.4316686388133938\n",
      "Epoch 206, Loss: 0.4315179158466364\n",
      "Epoch 207, Loss: 0.4310712381632941\n",
      "Epoch 208, Loss: 0.43059470035496\n",
      "Epoch 209, Loss: 0.4303711221416647\n",
      "Epoch 210, Loss: 0.43033088719595897\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23939288789157465\n",
      "Valence RMSE: 0.25216145220733344\n",
      "Arousal RMSE: 0.22590376616343683\n",
      "Test R^2 score: tensor([0.3205, 0.6236], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.47208276045811276\n",
      "Num of epochs: 211\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Epoch 121, Loss: 0.4596922103619757\n",
      "Epoch 122, Loss: 0.45943389956138125\n",
      "Epoch 123, Loss: 0.45877818894117006\n",
      "Epoch 124, Loss: 0.45746538132874076\n",
      "Epoch 125, Loss: 0.4570462803121619\n",
      "Epoch 126, Loss: 0.45696189549769234\n",
      "Epoch 127, Loss: 0.45594920341421274\n",
      "Epoch 128, Loss: 0.45536359747685007\n",
      "Epoch 129, Loss: 0.4553290890752543\n",
      "Epoch 130, Loss: 0.4547195823074053\n",
      "Epoch 131, Loss: 0.45392838323998064\n",
      "Epoch 132, Loss: 0.4537853493595631\n",
      "Epoch 133, Loss: 0.45347437050767975\n",
      "Epoch 134, Loss: 0.45267431273812453\n",
      "Epoch 135, Loss: 0.45228737659373064\n",
      "Epoch 136, Loss: 0.4522154657432981\n",
      "Epoch 137, Loss: 0.45165964487407706\n",
      "Epoch 138, Loss: 0.4510751265462795\n",
      "Epoch 139, Loss: 0.45082650445054473\n",
      "Epoch 140, Loss: 0.4506281098405297\n",
      "Epoch 141, Loss: 0.4501911419656675\n",
      "Epoch 142, Loss: 0.4497001887659326\n",
      "Epoch 143, Loss: 0.44943436133718806\n",
      "Epoch 144, Loss: 0.449226296402944\n",
      "Epoch 145, Loss: 0.44886073897867573\n",
      "Epoch 146, Loss: 0.44844710376304864\n",
      "Epoch 147, Loss: 0.44810507013116724\n",
      "Epoch 148, Loss: 0.44788545862049767\n",
      "Epoch 149, Loss: 0.4476741441009651\n",
      "Epoch 150, Loss: 0.44735861731902055\n",
      "Epoch 151, Loss: 0.4469288720157105\n",
      "Epoch 152, Loss: 0.44655235754178146\n",
      "Epoch 153, Loss: 0.4462857402786373\n",
      "Epoch 154, Loss: 0.4460517369142754\n",
      "Epoch 155, Loss: 0.4457881796184935\n",
      "Epoch 156, Loss: 0.44542210886348255\n",
      "Epoch 157, Loss: 0.4450579467827662\n",
      "Epoch 158, Loss: 0.44473329320345206\n",
      "Epoch 159, Loss: 0.4444482514678638\n",
      "Epoch 160, Loss: 0.4441745674630603\n",
      "Epoch 161, Loss: 0.4439221813722748\n",
      "Epoch 162, Loss: 0.44363357870363757\n",
      "Epoch 163, Loss: 0.4433189071134186\n",
      "Epoch 164, Loss: 0.44295452976444744\n",
      "Epoch 165, Loss: 0.4426058108229731\n",
      "Epoch 166, Loss: 0.4422485787900922\n",
      "Epoch 167, Loss: 0.4419090985167976\n",
      "Epoch 168, Loss: 0.44158054387831963\n",
      "Epoch 169, Loss: 0.4412474388772268\n",
      "Epoch 170, Loss: 0.4409277694136744\n",
      "Epoch 171, Loss: 0.44068045571539705\n",
      "Epoch 172, Loss: 0.4405979418732049\n",
      "Epoch 173, Loss: 0.44094101686109266\n",
      "Epoch 174, Loss: 0.4417368233339534\n",
      "Epoch 175, Loss: 0.4416582519101522\n",
      "Epoch 176, Loss: 0.4396788970821407\n",
      "Epoch 177, Loss: 0.4390195954561405\n",
      "Epoch 178, Loss: 0.4400830988007806\n",
      "Epoch 179, Loss: 0.43931775869233325\n",
      "Epoch 180, Loss: 0.4380032505977321\n",
      "Epoch 181, Loss: 0.4384470974340077\n",
      "Epoch 182, Loss: 0.4385167636581309\n",
      "Epoch 183, Loss: 0.43733043450040754\n",
      "Epoch 184, Loss: 0.4373176399006318\n",
      "Epoch 185, Loss: 0.4375256973621567\n",
      "Epoch 186, Loss: 0.43659322296038683\n",
      "Epoch 187, Loss: 0.43644391045908093\n",
      "Epoch 188, Loss: 0.43663844358015674\n",
      "Epoch 189, Loss: 0.43586286705793914\n",
      "Epoch 190, Loss: 0.4355663072844155\n",
      "Epoch 191, Loss: 0.4357040532214924\n",
      "Epoch 192, Loss: 0.435134007760397\n",
      "Epoch 193, Loss: 0.43474241127906293\n",
      "Epoch 194, Loss: 0.43480482310319635\n",
      "Epoch 195, Loss: 0.43443826104982264\n",
      "Epoch 196, Loss: 0.43390767873021246\n",
      "Epoch 197, Loss: 0.4338897691233524\n",
      "Epoch 198, Loss: 0.43377856877811044\n",
      "Epoch 199, Loss: 0.4331595506415052\n",
      "Epoch 200, Loss: 0.43289415091975497\n",
      "Epoch 201, Loss: 0.43294048069736957\n",
      "Epoch 202, Loss: 0.43262240576772243\n",
      "Epoch 203, Loss: 0.43206541812705346\n",
      "Epoch 204, Loss: 0.43172500614102416\n",
      "Epoch 205, Loss: 0.4316686388133938\n",
      "Epoch 206, Loss: 0.4315179158466364\n",
      "Epoch 207, Loss: 0.4310712381632941\n",
      "Epoch 208, Loss: 0.43059470035496\n",
      "Epoch 209, Loss: 0.4303711221416647\n",
      "Epoch 210, Loss: 0.43033088719595897\n",
      "Epoch 211, Loss: 0.43016580489992484\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23899198379041617\n",
      "Valence RMSE: 0.25118672043203716\n",
      "Arousal RMSE: 0.22614059368179495\n",
      "Test R^2 score: tensor([0.3258, 0.6228], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.47430941807179117\n",
      "Num of epochs: 212\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Epoch 121, Loss: 0.4596922103619757\n",
      "Epoch 122, Loss: 0.45943389956138125\n",
      "Epoch 123, Loss: 0.45877818894117006\n",
      "Epoch 124, Loss: 0.45746538132874076\n",
      "Epoch 125, Loss: 0.4570462803121619\n",
      "Epoch 126, Loss: 0.45696189549769234\n",
      "Epoch 127, Loss: 0.45594920341421274\n",
      "Epoch 128, Loss: 0.45536359747685007\n",
      "Epoch 129, Loss: 0.4553290890752543\n",
      "Epoch 130, Loss: 0.4547195823074053\n",
      "Epoch 131, Loss: 0.45392838323998064\n",
      "Epoch 132, Loss: 0.4537853493595631\n",
      "Epoch 133, Loss: 0.45347437050767975\n",
      "Epoch 134, Loss: 0.45267431273812453\n",
      "Epoch 135, Loss: 0.45228737659373064\n",
      "Epoch 136, Loss: 0.4522154657432981\n",
      "Epoch 137, Loss: 0.45165964487407706\n",
      "Epoch 138, Loss: 0.4510751265462795\n",
      "Epoch 139, Loss: 0.45082650445054473\n",
      "Epoch 140, Loss: 0.4506281098405297\n",
      "Epoch 141, Loss: 0.4501911419656675\n",
      "Epoch 142, Loss: 0.4497001887659326\n",
      "Epoch 143, Loss: 0.44943436133718806\n",
      "Epoch 144, Loss: 0.449226296402944\n",
      "Epoch 145, Loss: 0.44886073897867573\n",
      "Epoch 146, Loss: 0.44844710376304864\n",
      "Epoch 147, Loss: 0.44810507013116724\n",
      "Epoch 148, Loss: 0.44788545862049767\n",
      "Epoch 149, Loss: 0.4476741441009651\n",
      "Epoch 150, Loss: 0.44735861731902055\n",
      "Epoch 151, Loss: 0.4469288720157105\n",
      "Epoch 152, Loss: 0.44655235754178146\n",
      "Epoch 153, Loss: 0.4462857402786373\n",
      "Epoch 154, Loss: 0.4460517369142754\n",
      "Epoch 155, Loss: 0.4457881796184935\n",
      "Epoch 156, Loss: 0.44542210886348255\n",
      "Epoch 157, Loss: 0.4450579467827662\n",
      "Epoch 158, Loss: 0.44473329320345206\n",
      "Epoch 159, Loss: 0.4444482514678638\n",
      "Epoch 160, Loss: 0.4441745674630603\n",
      "Epoch 161, Loss: 0.4439221813722748\n",
      "Epoch 162, Loss: 0.44363357870363757\n",
      "Epoch 163, Loss: 0.4433189071134186\n",
      "Epoch 164, Loss: 0.44295452976444744\n",
      "Epoch 165, Loss: 0.4426058108229731\n",
      "Epoch 166, Loss: 0.4422485787900922\n",
      "Epoch 167, Loss: 0.4419090985167976\n",
      "Epoch 168, Loss: 0.44158054387831963\n",
      "Epoch 169, Loss: 0.4412474388772268\n",
      "Epoch 170, Loss: 0.4409277694136744\n",
      "Epoch 171, Loss: 0.44068045571539705\n",
      "Epoch 172, Loss: 0.4405979418732049\n",
      "Epoch 173, Loss: 0.44094101686109266\n",
      "Epoch 174, Loss: 0.4417368233339534\n",
      "Epoch 175, Loss: 0.4416582519101522\n",
      "Epoch 176, Loss: 0.4396788970821407\n",
      "Epoch 177, Loss: 0.4390195954561405\n",
      "Epoch 178, Loss: 0.4400830988007806\n",
      "Epoch 179, Loss: 0.43931775869233325\n",
      "Epoch 180, Loss: 0.4380032505977321\n",
      "Epoch 181, Loss: 0.4384470974340077\n",
      "Epoch 182, Loss: 0.4385167636581309\n",
      "Epoch 183, Loss: 0.43733043450040754\n",
      "Epoch 184, Loss: 0.4373176399006318\n",
      "Epoch 185, Loss: 0.4375256973621567\n",
      "Epoch 186, Loss: 0.43659322296038683\n",
      "Epoch 187, Loss: 0.43644391045908093\n",
      "Epoch 188, Loss: 0.43663844358015674\n",
      "Epoch 189, Loss: 0.43586286705793914\n",
      "Epoch 190, Loss: 0.4355663072844155\n",
      "Epoch 191, Loss: 0.4357040532214924\n",
      "Epoch 192, Loss: 0.435134007760397\n",
      "Epoch 193, Loss: 0.43474241127906293\n",
      "Epoch 194, Loss: 0.43480482310319635\n",
      "Epoch 195, Loss: 0.43443826104982264\n",
      "Epoch 196, Loss: 0.43390767873021246\n",
      "Epoch 197, Loss: 0.4338897691233524\n",
      "Epoch 198, Loss: 0.43377856877811044\n",
      "Epoch 199, Loss: 0.4331595506415052\n",
      "Epoch 200, Loss: 0.43289415091975497\n",
      "Epoch 201, Loss: 0.43294048069736957\n",
      "Epoch 202, Loss: 0.43262240576772243\n",
      "Epoch 203, Loss: 0.43206541812705346\n",
      "Epoch 204, Loss: 0.43172500614102416\n",
      "Epoch 205, Loss: 0.4316686388133938\n",
      "Epoch 206, Loss: 0.4315179158466364\n",
      "Epoch 207, Loss: 0.4310712381632941\n",
      "Epoch 208, Loss: 0.43059470035496\n",
      "Epoch 209, Loss: 0.4303711221416647\n",
      "Epoch 210, Loss: 0.43033088719595897\n",
      "Epoch 211, Loss: 0.43016580489992484\n",
      "Epoch 212, Loss: 0.429691817522032\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23934730913940094\n",
      "Valence RMSE: 0.25197081542944344\n",
      "Arousal RMSE: 0.22601985965035304\n",
      "Test R^2 score: tensor([0.3216, 0.6233], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.47240278925167706\n",
      "Num of epochs: 213\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Epoch 121, Loss: 0.4596922103619757\n",
      "Epoch 122, Loss: 0.45943389956138125\n",
      "Epoch 123, Loss: 0.45877818894117006\n",
      "Epoch 124, Loss: 0.45746538132874076\n",
      "Epoch 125, Loss: 0.4570462803121619\n",
      "Epoch 126, Loss: 0.45696189549769234\n",
      "Epoch 127, Loss: 0.45594920341421274\n",
      "Epoch 128, Loss: 0.45536359747685007\n",
      "Epoch 129, Loss: 0.4553290890752543\n",
      "Epoch 130, Loss: 0.4547195823074053\n",
      "Epoch 131, Loss: 0.45392838323998064\n",
      "Epoch 132, Loss: 0.4537853493595631\n",
      "Epoch 133, Loss: 0.45347437050767975\n",
      "Epoch 134, Loss: 0.45267431273812453\n",
      "Epoch 135, Loss: 0.45228737659373064\n",
      "Epoch 136, Loss: 0.4522154657432981\n",
      "Epoch 137, Loss: 0.45165964487407706\n",
      "Epoch 138, Loss: 0.4510751265462795\n",
      "Epoch 139, Loss: 0.45082650445054473\n",
      "Epoch 140, Loss: 0.4506281098405297\n",
      "Epoch 141, Loss: 0.4501911419656675\n",
      "Epoch 142, Loss: 0.4497001887659326\n",
      "Epoch 143, Loss: 0.44943436133718806\n",
      "Epoch 144, Loss: 0.449226296402944\n",
      "Epoch 145, Loss: 0.44886073897867573\n",
      "Epoch 146, Loss: 0.44844710376304864\n",
      "Epoch 147, Loss: 0.44810507013116724\n",
      "Epoch 148, Loss: 0.44788545862049767\n",
      "Epoch 149, Loss: 0.4476741441009651\n",
      "Epoch 150, Loss: 0.44735861731902055\n",
      "Epoch 151, Loss: 0.4469288720157105\n",
      "Epoch 152, Loss: 0.44655235754178146\n",
      "Epoch 153, Loss: 0.4462857402786373\n",
      "Epoch 154, Loss: 0.4460517369142754\n",
      "Epoch 155, Loss: 0.4457881796184935\n",
      "Epoch 156, Loss: 0.44542210886348255\n",
      "Epoch 157, Loss: 0.4450579467827662\n",
      "Epoch 158, Loss: 0.44473329320345206\n",
      "Epoch 159, Loss: 0.4444482514678638\n",
      "Epoch 160, Loss: 0.4441745674630603\n",
      "Epoch 161, Loss: 0.4439221813722748\n",
      "Epoch 162, Loss: 0.44363357870363757\n",
      "Epoch 163, Loss: 0.4433189071134186\n",
      "Epoch 164, Loss: 0.44295452976444744\n",
      "Epoch 165, Loss: 0.4426058108229731\n",
      "Epoch 166, Loss: 0.4422485787900922\n",
      "Epoch 167, Loss: 0.4419090985167976\n",
      "Epoch 168, Loss: 0.44158054387831963\n",
      "Epoch 169, Loss: 0.4412474388772268\n",
      "Epoch 170, Loss: 0.4409277694136744\n",
      "Epoch 171, Loss: 0.44068045571539705\n",
      "Epoch 172, Loss: 0.4405979418732049\n",
      "Epoch 173, Loss: 0.44094101686109266\n",
      "Epoch 174, Loss: 0.4417368233339534\n",
      "Epoch 175, Loss: 0.4416582519101522\n",
      "Epoch 176, Loss: 0.4396788970821407\n",
      "Epoch 177, Loss: 0.4390195954561405\n",
      "Epoch 178, Loss: 0.4400830988007806\n",
      "Epoch 179, Loss: 0.43931775869233325\n",
      "Epoch 180, Loss: 0.4380032505977321\n",
      "Epoch 181, Loss: 0.4384470974340077\n",
      "Epoch 182, Loss: 0.4385167636581309\n",
      "Epoch 183, Loss: 0.43733043450040754\n",
      "Epoch 184, Loss: 0.4373176399006318\n",
      "Epoch 185, Loss: 0.4375256973621567\n",
      "Epoch 186, Loss: 0.43659322296038683\n",
      "Epoch 187, Loss: 0.43644391045908093\n",
      "Epoch 188, Loss: 0.43663844358015674\n",
      "Epoch 189, Loss: 0.43586286705793914\n",
      "Epoch 190, Loss: 0.4355663072844155\n",
      "Epoch 191, Loss: 0.4357040532214924\n",
      "Epoch 192, Loss: 0.435134007760397\n",
      "Epoch 193, Loss: 0.43474241127906293\n",
      "Epoch 194, Loss: 0.43480482310319635\n",
      "Epoch 195, Loss: 0.43443826104982264\n",
      "Epoch 196, Loss: 0.43390767873021246\n",
      "Epoch 197, Loss: 0.4338897691233524\n",
      "Epoch 198, Loss: 0.43377856877811044\n",
      "Epoch 199, Loss: 0.4331595506415052\n",
      "Epoch 200, Loss: 0.43289415091975497\n",
      "Epoch 201, Loss: 0.43294048069736957\n",
      "Epoch 202, Loss: 0.43262240576772243\n",
      "Epoch 203, Loss: 0.43206541812705346\n",
      "Epoch 204, Loss: 0.43172500614102416\n",
      "Epoch 205, Loss: 0.4316686388133938\n",
      "Epoch 206, Loss: 0.4315179158466364\n",
      "Epoch 207, Loss: 0.4310712381632941\n",
      "Epoch 208, Loss: 0.43059470035496\n",
      "Epoch 209, Loss: 0.4303711221416647\n",
      "Epoch 210, Loss: 0.43033088719595897\n",
      "Epoch 211, Loss: 0.43016580489992484\n",
      "Epoch 212, Loss: 0.429691817522032\n",
      "Epoch 213, Loss: 0.4292099292794459\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23956693230325163\n",
      "Valence RMSE: 0.252230197366426\n",
      "Arousal RMSE: 0.22619583913696398\n",
      "Test R^2 score: tensor([0.3202, 0.6227], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4714105779793943\n",
      "Num of epochs: 214\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Epoch 121, Loss: 0.4596922103619757\n",
      "Epoch 122, Loss: 0.45943389956138125\n",
      "Epoch 123, Loss: 0.45877818894117006\n",
      "Epoch 124, Loss: 0.45746538132874076\n",
      "Epoch 125, Loss: 0.4570462803121619\n",
      "Epoch 126, Loss: 0.45696189549769234\n",
      "Epoch 127, Loss: 0.45594920341421274\n",
      "Epoch 128, Loss: 0.45536359747685007\n",
      "Epoch 129, Loss: 0.4553290890752543\n",
      "Epoch 130, Loss: 0.4547195823074053\n",
      "Epoch 131, Loss: 0.45392838323998064\n",
      "Epoch 132, Loss: 0.4537853493595631\n",
      "Epoch 133, Loss: 0.45347437050767975\n",
      "Epoch 134, Loss: 0.45267431273812453\n",
      "Epoch 135, Loss: 0.45228737659373064\n",
      "Epoch 136, Loss: 0.4522154657432981\n",
      "Epoch 137, Loss: 0.45165964487407706\n",
      "Epoch 138, Loss: 0.4510751265462795\n",
      "Epoch 139, Loss: 0.45082650445054473\n",
      "Epoch 140, Loss: 0.4506281098405297\n",
      "Epoch 141, Loss: 0.4501911419656675\n",
      "Epoch 142, Loss: 0.4497001887659326\n",
      "Epoch 143, Loss: 0.44943436133718806\n",
      "Epoch 144, Loss: 0.449226296402944\n",
      "Epoch 145, Loss: 0.44886073897867573\n",
      "Epoch 146, Loss: 0.44844710376304864\n",
      "Epoch 147, Loss: 0.44810507013116724\n",
      "Epoch 148, Loss: 0.44788545862049767\n",
      "Epoch 149, Loss: 0.4476741441009651\n",
      "Epoch 150, Loss: 0.44735861731902055\n",
      "Epoch 151, Loss: 0.4469288720157105\n",
      "Epoch 152, Loss: 0.44655235754178146\n",
      "Epoch 153, Loss: 0.4462857402786373\n",
      "Epoch 154, Loss: 0.4460517369142754\n",
      "Epoch 155, Loss: 0.4457881796184935\n",
      "Epoch 156, Loss: 0.44542210886348255\n",
      "Epoch 157, Loss: 0.4450579467827662\n",
      "Epoch 158, Loss: 0.44473329320345206\n",
      "Epoch 159, Loss: 0.4444482514678638\n",
      "Epoch 160, Loss: 0.4441745674630603\n",
      "Epoch 161, Loss: 0.4439221813722748\n",
      "Epoch 162, Loss: 0.44363357870363757\n",
      "Epoch 163, Loss: 0.4433189071134186\n",
      "Epoch 164, Loss: 0.44295452976444744\n",
      "Epoch 165, Loss: 0.4426058108229731\n",
      "Epoch 166, Loss: 0.4422485787900922\n",
      "Epoch 167, Loss: 0.4419090985167976\n",
      "Epoch 168, Loss: 0.44158054387831963\n",
      "Epoch 169, Loss: 0.4412474388772268\n",
      "Epoch 170, Loss: 0.4409277694136744\n",
      "Epoch 171, Loss: 0.44068045571539705\n",
      "Epoch 172, Loss: 0.4405979418732049\n",
      "Epoch 173, Loss: 0.44094101686109266\n",
      "Epoch 174, Loss: 0.4417368233339534\n",
      "Epoch 175, Loss: 0.4416582519101522\n",
      "Epoch 176, Loss: 0.4396788970821407\n",
      "Epoch 177, Loss: 0.4390195954561405\n",
      "Epoch 178, Loss: 0.4400830988007806\n",
      "Epoch 179, Loss: 0.43931775869233325\n",
      "Epoch 180, Loss: 0.4380032505977321\n",
      "Epoch 181, Loss: 0.4384470974340077\n",
      "Epoch 182, Loss: 0.4385167636581309\n",
      "Epoch 183, Loss: 0.43733043450040754\n",
      "Epoch 184, Loss: 0.4373176399006318\n",
      "Epoch 185, Loss: 0.4375256973621567\n",
      "Epoch 186, Loss: 0.43659322296038683\n",
      "Epoch 187, Loss: 0.43644391045908093\n",
      "Epoch 188, Loss: 0.43663844358015674\n",
      "Epoch 189, Loss: 0.43586286705793914\n",
      "Epoch 190, Loss: 0.4355663072844155\n",
      "Epoch 191, Loss: 0.4357040532214924\n",
      "Epoch 192, Loss: 0.435134007760397\n",
      "Epoch 193, Loss: 0.43474241127906293\n",
      "Epoch 194, Loss: 0.43480482310319635\n",
      "Epoch 195, Loss: 0.43443826104982264\n",
      "Epoch 196, Loss: 0.43390767873021246\n",
      "Epoch 197, Loss: 0.4338897691233524\n",
      "Epoch 198, Loss: 0.43377856877811044\n",
      "Epoch 199, Loss: 0.4331595506415052\n",
      "Epoch 200, Loss: 0.43289415091975497\n",
      "Epoch 201, Loss: 0.43294048069736957\n",
      "Epoch 202, Loss: 0.43262240576772243\n",
      "Epoch 203, Loss: 0.43206541812705346\n",
      "Epoch 204, Loss: 0.43172500614102416\n",
      "Epoch 205, Loss: 0.4316686388133938\n",
      "Epoch 206, Loss: 0.4315179158466364\n",
      "Epoch 207, Loss: 0.4310712381632941\n",
      "Epoch 208, Loss: 0.43059470035496\n",
      "Epoch 209, Loss: 0.4303711221416647\n",
      "Epoch 210, Loss: 0.43033088719595897\n",
      "Epoch 211, Loss: 0.43016580489992484\n",
      "Epoch 212, Loss: 0.429691817522032\n",
      "Epoch 213, Loss: 0.4292099292794459\n",
      "Epoch 214, Loss: 0.4289207205910941\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2395119348898553\n",
      "Valence RMSE: 0.25192614879904923\n",
      "Arousal RMSE: 0.22641808554230802\n",
      "Test R^2 score: tensor([0.3218, 0.6219], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.47185866346672917\n",
      "Num of epochs: 215\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Epoch 121, Loss: 0.4596922103619757\n",
      "Epoch 122, Loss: 0.45943389956138125\n",
      "Epoch 123, Loss: 0.45877818894117006\n",
      "Epoch 124, Loss: 0.45746538132874076\n",
      "Epoch 125, Loss: 0.4570462803121619\n",
      "Epoch 126, Loss: 0.45696189549769234\n",
      "Epoch 127, Loss: 0.45594920341421274\n",
      "Epoch 128, Loss: 0.45536359747685007\n",
      "Epoch 129, Loss: 0.4553290890752543\n",
      "Epoch 130, Loss: 0.4547195823074053\n",
      "Epoch 131, Loss: 0.45392838323998064\n",
      "Epoch 132, Loss: 0.4537853493595631\n",
      "Epoch 133, Loss: 0.45347437050767975\n",
      "Epoch 134, Loss: 0.45267431273812453\n",
      "Epoch 135, Loss: 0.45228737659373064\n",
      "Epoch 136, Loss: 0.4522154657432981\n",
      "Epoch 137, Loss: 0.45165964487407706\n",
      "Epoch 138, Loss: 0.4510751265462795\n",
      "Epoch 139, Loss: 0.45082650445054473\n",
      "Epoch 140, Loss: 0.4506281098405297\n",
      "Epoch 141, Loss: 0.4501911419656675\n",
      "Epoch 142, Loss: 0.4497001887659326\n",
      "Epoch 143, Loss: 0.44943436133718806\n",
      "Epoch 144, Loss: 0.449226296402944\n",
      "Epoch 145, Loss: 0.44886073897867573\n",
      "Epoch 146, Loss: 0.44844710376304864\n",
      "Epoch 147, Loss: 0.44810507013116724\n",
      "Epoch 148, Loss: 0.44788545862049767\n",
      "Epoch 149, Loss: 0.4476741441009651\n",
      "Epoch 150, Loss: 0.44735861731902055\n",
      "Epoch 151, Loss: 0.4469288720157105\n",
      "Epoch 152, Loss: 0.44655235754178146\n",
      "Epoch 153, Loss: 0.4462857402786373\n",
      "Epoch 154, Loss: 0.4460517369142754\n",
      "Epoch 155, Loss: 0.4457881796184935\n",
      "Epoch 156, Loss: 0.44542210886348255\n",
      "Epoch 157, Loss: 0.4450579467827662\n",
      "Epoch 158, Loss: 0.44473329320345206\n",
      "Epoch 159, Loss: 0.4444482514678638\n",
      "Epoch 160, Loss: 0.4441745674630603\n",
      "Epoch 161, Loss: 0.4439221813722748\n",
      "Epoch 162, Loss: 0.44363357870363757\n",
      "Epoch 163, Loss: 0.4433189071134186\n",
      "Epoch 164, Loss: 0.44295452976444744\n",
      "Epoch 165, Loss: 0.4426058108229731\n",
      "Epoch 166, Loss: 0.4422485787900922\n",
      "Epoch 167, Loss: 0.4419090985167976\n",
      "Epoch 168, Loss: 0.44158054387831963\n",
      "Epoch 169, Loss: 0.4412474388772268\n",
      "Epoch 170, Loss: 0.4409277694136744\n",
      "Epoch 171, Loss: 0.44068045571539705\n",
      "Epoch 172, Loss: 0.4405979418732049\n",
      "Epoch 173, Loss: 0.44094101686109266\n",
      "Epoch 174, Loss: 0.4417368233339534\n",
      "Epoch 175, Loss: 0.4416582519101522\n",
      "Epoch 176, Loss: 0.4396788970821407\n",
      "Epoch 177, Loss: 0.4390195954561405\n",
      "Epoch 178, Loss: 0.4400830988007806\n",
      "Epoch 179, Loss: 0.43931775869233325\n",
      "Epoch 180, Loss: 0.4380032505977321\n",
      "Epoch 181, Loss: 0.4384470974340077\n",
      "Epoch 182, Loss: 0.4385167636581309\n",
      "Epoch 183, Loss: 0.43733043450040754\n",
      "Epoch 184, Loss: 0.4373176399006318\n",
      "Epoch 185, Loss: 0.4375256973621567\n",
      "Epoch 186, Loss: 0.43659322296038683\n",
      "Epoch 187, Loss: 0.43644391045908093\n",
      "Epoch 188, Loss: 0.43663844358015674\n",
      "Epoch 189, Loss: 0.43586286705793914\n",
      "Epoch 190, Loss: 0.4355663072844155\n",
      "Epoch 191, Loss: 0.4357040532214924\n",
      "Epoch 192, Loss: 0.435134007760397\n",
      "Epoch 193, Loss: 0.43474241127906293\n",
      "Epoch 194, Loss: 0.43480482310319635\n",
      "Epoch 195, Loss: 0.43443826104982264\n",
      "Epoch 196, Loss: 0.43390767873021246\n",
      "Epoch 197, Loss: 0.4338897691233524\n",
      "Epoch 198, Loss: 0.43377856877811044\n",
      "Epoch 199, Loss: 0.4331595506415052\n",
      "Epoch 200, Loss: 0.43289415091975497\n",
      "Epoch 201, Loss: 0.43294048069736957\n",
      "Epoch 202, Loss: 0.43262240576772243\n",
      "Epoch 203, Loss: 0.43206541812705346\n",
      "Epoch 204, Loss: 0.43172500614102416\n",
      "Epoch 205, Loss: 0.4316686388133938\n",
      "Epoch 206, Loss: 0.4315179158466364\n",
      "Epoch 207, Loss: 0.4310712381632941\n",
      "Epoch 208, Loss: 0.43059470035496\n",
      "Epoch 209, Loss: 0.4303711221416647\n",
      "Epoch 210, Loss: 0.43033088719595897\n",
      "Epoch 211, Loss: 0.43016580489992484\n",
      "Epoch 212, Loss: 0.429691817522032\n",
      "Epoch 213, Loss: 0.4292099292794459\n",
      "Epoch 214, Loss: 0.4289207205910941\n",
      "Epoch 215, Loss: 0.4287978238495365\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23994131446752756\n",
      "Valence RMSE: 0.25273784634749114\n",
      "Arousal RMSE: 0.22642272368391356\n",
      "Test R^2 score: tensor([0.3174, 0.6219], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.46966224688382824\n",
      "Num of epochs: 216\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Epoch 121, Loss: 0.4596922103619757\n",
      "Epoch 122, Loss: 0.45943389956138125\n",
      "Epoch 123, Loss: 0.45877818894117006\n",
      "Epoch 124, Loss: 0.45746538132874076\n",
      "Epoch 125, Loss: 0.4570462803121619\n",
      "Epoch 126, Loss: 0.45696189549769234\n",
      "Epoch 127, Loss: 0.45594920341421274\n",
      "Epoch 128, Loss: 0.45536359747685007\n",
      "Epoch 129, Loss: 0.4553290890752543\n",
      "Epoch 130, Loss: 0.4547195823074053\n",
      "Epoch 131, Loss: 0.45392838323998064\n",
      "Epoch 132, Loss: 0.4537853493595631\n",
      "Epoch 133, Loss: 0.45347437050767975\n",
      "Epoch 134, Loss: 0.45267431273812453\n",
      "Epoch 135, Loss: 0.45228737659373064\n",
      "Epoch 136, Loss: 0.4522154657432981\n",
      "Epoch 137, Loss: 0.45165964487407706\n",
      "Epoch 138, Loss: 0.4510751265462795\n",
      "Epoch 139, Loss: 0.45082650445054473\n",
      "Epoch 140, Loss: 0.4506281098405297\n",
      "Epoch 141, Loss: 0.4501911419656675\n",
      "Epoch 142, Loss: 0.4497001887659326\n",
      "Epoch 143, Loss: 0.44943436133718806\n",
      "Epoch 144, Loss: 0.449226296402944\n",
      "Epoch 145, Loss: 0.44886073897867573\n",
      "Epoch 146, Loss: 0.44844710376304864\n",
      "Epoch 147, Loss: 0.44810507013116724\n",
      "Epoch 148, Loss: 0.44788545862049767\n",
      "Epoch 149, Loss: 0.4476741441009651\n",
      "Epoch 150, Loss: 0.44735861731902055\n",
      "Epoch 151, Loss: 0.4469288720157105\n",
      "Epoch 152, Loss: 0.44655235754178146\n",
      "Epoch 153, Loss: 0.4462857402786373\n",
      "Epoch 154, Loss: 0.4460517369142754\n",
      "Epoch 155, Loss: 0.4457881796184935\n",
      "Epoch 156, Loss: 0.44542210886348255\n",
      "Epoch 157, Loss: 0.4450579467827662\n",
      "Epoch 158, Loss: 0.44473329320345206\n",
      "Epoch 159, Loss: 0.4444482514678638\n",
      "Epoch 160, Loss: 0.4441745674630603\n",
      "Epoch 161, Loss: 0.4439221813722748\n",
      "Epoch 162, Loss: 0.44363357870363757\n",
      "Epoch 163, Loss: 0.4433189071134186\n",
      "Epoch 164, Loss: 0.44295452976444744\n",
      "Epoch 165, Loss: 0.4426058108229731\n",
      "Epoch 166, Loss: 0.4422485787900922\n",
      "Epoch 167, Loss: 0.4419090985167976\n",
      "Epoch 168, Loss: 0.44158054387831963\n",
      "Epoch 169, Loss: 0.4412474388772268\n",
      "Epoch 170, Loss: 0.4409277694136744\n",
      "Epoch 171, Loss: 0.44068045571539705\n",
      "Epoch 172, Loss: 0.4405979418732049\n",
      "Epoch 173, Loss: 0.44094101686109266\n",
      "Epoch 174, Loss: 0.4417368233339534\n",
      "Epoch 175, Loss: 0.4416582519101522\n",
      "Epoch 176, Loss: 0.4396788970821407\n",
      "Epoch 177, Loss: 0.4390195954561405\n",
      "Epoch 178, Loss: 0.4400830988007806\n",
      "Epoch 179, Loss: 0.43931775869233325\n",
      "Epoch 180, Loss: 0.4380032505977321\n",
      "Epoch 181, Loss: 0.4384470974340077\n",
      "Epoch 182, Loss: 0.4385167636581309\n",
      "Epoch 183, Loss: 0.43733043450040754\n",
      "Epoch 184, Loss: 0.4373176399006318\n",
      "Epoch 185, Loss: 0.4375256973621567\n",
      "Epoch 186, Loss: 0.43659322296038683\n",
      "Epoch 187, Loss: 0.43644391045908093\n",
      "Epoch 188, Loss: 0.43663844358015674\n",
      "Epoch 189, Loss: 0.43586286705793914\n",
      "Epoch 190, Loss: 0.4355663072844155\n",
      "Epoch 191, Loss: 0.4357040532214924\n",
      "Epoch 192, Loss: 0.435134007760397\n",
      "Epoch 193, Loss: 0.43474241127906293\n",
      "Epoch 194, Loss: 0.43480482310319635\n",
      "Epoch 195, Loss: 0.43443826104982264\n",
      "Epoch 196, Loss: 0.43390767873021246\n",
      "Epoch 197, Loss: 0.4338897691233524\n",
      "Epoch 198, Loss: 0.43377856877811044\n",
      "Epoch 199, Loss: 0.4331595506415052\n",
      "Epoch 200, Loss: 0.43289415091975497\n",
      "Epoch 201, Loss: 0.43294048069736957\n",
      "Epoch 202, Loss: 0.43262240576772243\n",
      "Epoch 203, Loss: 0.43206541812705346\n",
      "Epoch 204, Loss: 0.43172500614102416\n",
      "Epoch 205, Loss: 0.4316686388133938\n",
      "Epoch 206, Loss: 0.4315179158466364\n",
      "Epoch 207, Loss: 0.4310712381632941\n",
      "Epoch 208, Loss: 0.43059470035496\n",
      "Epoch 209, Loss: 0.4303711221416647\n",
      "Epoch 210, Loss: 0.43033088719595897\n",
      "Epoch 211, Loss: 0.43016580489992484\n",
      "Epoch 212, Loss: 0.429691817522032\n",
      "Epoch 213, Loss: 0.4292099292794459\n",
      "Epoch 214, Loss: 0.4289207205910941\n",
      "Epoch 215, Loss: 0.4287978238495365\n",
      "Epoch 216, Loss: 0.42870603658529494\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23944046114347223\n",
      "Valence RMSE: 0.2516533273142103\n",
      "Arousal RMSE: 0.22657023572588747\n",
      "Test R^2 score: tensor([0.3233, 0.6214], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.47233857251331246\n",
      "Num of epochs: 217\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Epoch 121, Loss: 0.4596922103619757\n",
      "Epoch 122, Loss: 0.45943389956138125\n",
      "Epoch 123, Loss: 0.45877818894117006\n",
      "Epoch 124, Loss: 0.45746538132874076\n",
      "Epoch 125, Loss: 0.4570462803121619\n",
      "Epoch 126, Loss: 0.45696189549769234\n",
      "Epoch 127, Loss: 0.45594920341421274\n",
      "Epoch 128, Loss: 0.45536359747685007\n",
      "Epoch 129, Loss: 0.4553290890752543\n",
      "Epoch 130, Loss: 0.4547195823074053\n",
      "Epoch 131, Loss: 0.45392838323998064\n",
      "Epoch 132, Loss: 0.4537853493595631\n",
      "Epoch 133, Loss: 0.45347437050767975\n",
      "Epoch 134, Loss: 0.45267431273812453\n",
      "Epoch 135, Loss: 0.45228737659373064\n",
      "Epoch 136, Loss: 0.4522154657432981\n",
      "Epoch 137, Loss: 0.45165964487407706\n",
      "Epoch 138, Loss: 0.4510751265462795\n",
      "Epoch 139, Loss: 0.45082650445054473\n",
      "Epoch 140, Loss: 0.4506281098405297\n",
      "Epoch 141, Loss: 0.4501911419656675\n",
      "Epoch 142, Loss: 0.4497001887659326\n",
      "Epoch 143, Loss: 0.44943436133718806\n",
      "Epoch 144, Loss: 0.449226296402944\n",
      "Epoch 145, Loss: 0.44886073897867573\n",
      "Epoch 146, Loss: 0.44844710376304864\n",
      "Epoch 147, Loss: 0.44810507013116724\n",
      "Epoch 148, Loss: 0.44788545862049767\n",
      "Epoch 149, Loss: 0.4476741441009651\n",
      "Epoch 150, Loss: 0.44735861731902055\n",
      "Epoch 151, Loss: 0.4469288720157105\n",
      "Epoch 152, Loss: 0.44655235754178146\n",
      "Epoch 153, Loss: 0.4462857402786373\n",
      "Epoch 154, Loss: 0.4460517369142754\n",
      "Epoch 155, Loss: 0.4457881796184935\n",
      "Epoch 156, Loss: 0.44542210886348255\n",
      "Epoch 157, Loss: 0.4450579467827662\n",
      "Epoch 158, Loss: 0.44473329320345206\n",
      "Epoch 159, Loss: 0.4444482514678638\n",
      "Epoch 160, Loss: 0.4441745674630603\n",
      "Epoch 161, Loss: 0.4439221813722748\n",
      "Epoch 162, Loss: 0.44363357870363757\n",
      "Epoch 163, Loss: 0.4433189071134186\n",
      "Epoch 164, Loss: 0.44295452976444744\n",
      "Epoch 165, Loss: 0.4426058108229731\n",
      "Epoch 166, Loss: 0.4422485787900922\n",
      "Epoch 167, Loss: 0.4419090985167976\n",
      "Epoch 168, Loss: 0.44158054387831963\n",
      "Epoch 169, Loss: 0.4412474388772268\n",
      "Epoch 170, Loss: 0.4409277694136744\n",
      "Epoch 171, Loss: 0.44068045571539705\n",
      "Epoch 172, Loss: 0.4405979418732049\n",
      "Epoch 173, Loss: 0.44094101686109266\n",
      "Epoch 174, Loss: 0.4417368233339534\n",
      "Epoch 175, Loss: 0.4416582519101522\n",
      "Epoch 176, Loss: 0.4396788970821407\n",
      "Epoch 177, Loss: 0.4390195954561405\n",
      "Epoch 178, Loss: 0.4400830988007806\n",
      "Epoch 179, Loss: 0.43931775869233325\n",
      "Epoch 180, Loss: 0.4380032505977321\n",
      "Epoch 181, Loss: 0.4384470974340077\n",
      "Epoch 182, Loss: 0.4385167636581309\n",
      "Epoch 183, Loss: 0.43733043450040754\n",
      "Epoch 184, Loss: 0.4373176399006318\n",
      "Epoch 185, Loss: 0.4375256973621567\n",
      "Epoch 186, Loss: 0.43659322296038683\n",
      "Epoch 187, Loss: 0.43644391045908093\n",
      "Epoch 188, Loss: 0.43663844358015674\n",
      "Epoch 189, Loss: 0.43586286705793914\n",
      "Epoch 190, Loss: 0.4355663072844155\n",
      "Epoch 191, Loss: 0.4357040532214924\n",
      "Epoch 192, Loss: 0.435134007760397\n",
      "Epoch 193, Loss: 0.43474241127906293\n",
      "Epoch 194, Loss: 0.43480482310319635\n",
      "Epoch 195, Loss: 0.43443826104982264\n",
      "Epoch 196, Loss: 0.43390767873021246\n",
      "Epoch 197, Loss: 0.4338897691233524\n",
      "Epoch 198, Loss: 0.43377856877811044\n",
      "Epoch 199, Loss: 0.4331595506415052\n",
      "Epoch 200, Loss: 0.43289415091975497\n",
      "Epoch 201, Loss: 0.43294048069736957\n",
      "Epoch 202, Loss: 0.43262240576772243\n",
      "Epoch 203, Loss: 0.43206541812705346\n",
      "Epoch 204, Loss: 0.43172500614102416\n",
      "Epoch 205, Loss: 0.4316686388133938\n",
      "Epoch 206, Loss: 0.4315179158466364\n",
      "Epoch 207, Loss: 0.4310712381632941\n",
      "Epoch 208, Loss: 0.43059470035496\n",
      "Epoch 209, Loss: 0.4303711221416647\n",
      "Epoch 210, Loss: 0.43033088719595897\n",
      "Epoch 211, Loss: 0.43016580489992484\n",
      "Epoch 212, Loss: 0.429691817522032\n",
      "Epoch 213, Loss: 0.4292099292794459\n",
      "Epoch 214, Loss: 0.4289207205910941\n",
      "Epoch 215, Loss: 0.4287978238495365\n",
      "Epoch 216, Loss: 0.42870603658529494\n",
      "Epoch 217, Loss: 0.42841980940051866\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23993903981434778\n",
      "Valence RMSE: 0.2527101516071174\n",
      "Arousal RMSE: 0.22644881304341177\n",
      "Test R^2 score: tensor([0.3176, 0.6218], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4696934713033583\n",
      "Num of epochs: 218\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Epoch 121, Loss: 0.4596922103619757\n",
      "Epoch 122, Loss: 0.45943389956138125\n",
      "Epoch 123, Loss: 0.45877818894117006\n",
      "Epoch 124, Loss: 0.45746538132874076\n",
      "Epoch 125, Loss: 0.4570462803121619\n",
      "Epoch 126, Loss: 0.45696189549769234\n",
      "Epoch 127, Loss: 0.45594920341421274\n",
      "Epoch 128, Loss: 0.45536359747685007\n",
      "Epoch 129, Loss: 0.4553290890752543\n",
      "Epoch 130, Loss: 0.4547195823074053\n",
      "Epoch 131, Loss: 0.45392838323998064\n",
      "Epoch 132, Loss: 0.4537853493595631\n",
      "Epoch 133, Loss: 0.45347437050767975\n",
      "Epoch 134, Loss: 0.45267431273812453\n",
      "Epoch 135, Loss: 0.45228737659373064\n",
      "Epoch 136, Loss: 0.4522154657432981\n",
      "Epoch 137, Loss: 0.45165964487407706\n",
      "Epoch 138, Loss: 0.4510751265462795\n",
      "Epoch 139, Loss: 0.45082650445054473\n",
      "Epoch 140, Loss: 0.4506281098405297\n",
      "Epoch 141, Loss: 0.4501911419656675\n",
      "Epoch 142, Loss: 0.4497001887659326\n",
      "Epoch 143, Loss: 0.44943436133718806\n",
      "Epoch 144, Loss: 0.449226296402944\n",
      "Epoch 145, Loss: 0.44886073897867573\n",
      "Epoch 146, Loss: 0.44844710376304864\n",
      "Epoch 147, Loss: 0.44810507013116724\n",
      "Epoch 148, Loss: 0.44788545862049767\n",
      "Epoch 149, Loss: 0.4476741441009651\n",
      "Epoch 150, Loss: 0.44735861731902055\n",
      "Epoch 151, Loss: 0.4469288720157105\n",
      "Epoch 152, Loss: 0.44655235754178146\n",
      "Epoch 153, Loss: 0.4462857402786373\n",
      "Epoch 154, Loss: 0.4460517369142754\n",
      "Epoch 155, Loss: 0.4457881796184935\n",
      "Epoch 156, Loss: 0.44542210886348255\n",
      "Epoch 157, Loss: 0.4450579467827662\n",
      "Epoch 158, Loss: 0.44473329320345206\n",
      "Epoch 159, Loss: 0.4444482514678638\n",
      "Epoch 160, Loss: 0.4441745674630603\n",
      "Epoch 161, Loss: 0.4439221813722748\n",
      "Epoch 162, Loss: 0.44363357870363757\n",
      "Epoch 163, Loss: 0.4433189071134186\n",
      "Epoch 164, Loss: 0.44295452976444744\n",
      "Epoch 165, Loss: 0.4426058108229731\n",
      "Epoch 166, Loss: 0.4422485787900922\n",
      "Epoch 167, Loss: 0.4419090985167976\n",
      "Epoch 168, Loss: 0.44158054387831963\n",
      "Epoch 169, Loss: 0.4412474388772268\n",
      "Epoch 170, Loss: 0.4409277694136744\n",
      "Epoch 171, Loss: 0.44068045571539705\n",
      "Epoch 172, Loss: 0.4405979418732049\n",
      "Epoch 173, Loss: 0.44094101686109266\n",
      "Epoch 174, Loss: 0.4417368233339534\n",
      "Epoch 175, Loss: 0.4416582519101522\n",
      "Epoch 176, Loss: 0.4396788970821407\n",
      "Epoch 177, Loss: 0.4390195954561405\n",
      "Epoch 178, Loss: 0.4400830988007806\n",
      "Epoch 179, Loss: 0.43931775869233325\n",
      "Epoch 180, Loss: 0.4380032505977321\n",
      "Epoch 181, Loss: 0.4384470974340077\n",
      "Epoch 182, Loss: 0.4385167636581309\n",
      "Epoch 183, Loss: 0.43733043450040754\n",
      "Epoch 184, Loss: 0.4373176399006318\n",
      "Epoch 185, Loss: 0.4375256973621567\n",
      "Epoch 186, Loss: 0.43659322296038683\n",
      "Epoch 187, Loss: 0.43644391045908093\n",
      "Epoch 188, Loss: 0.43663844358015674\n",
      "Epoch 189, Loss: 0.43586286705793914\n",
      "Epoch 190, Loss: 0.4355663072844155\n",
      "Epoch 191, Loss: 0.4357040532214924\n",
      "Epoch 192, Loss: 0.435134007760397\n",
      "Epoch 193, Loss: 0.43474241127906293\n",
      "Epoch 194, Loss: 0.43480482310319635\n",
      "Epoch 195, Loss: 0.43443826104982264\n",
      "Epoch 196, Loss: 0.43390767873021246\n",
      "Epoch 197, Loss: 0.4338897691233524\n",
      "Epoch 198, Loss: 0.43377856877811044\n",
      "Epoch 199, Loss: 0.4331595506415052\n",
      "Epoch 200, Loss: 0.43289415091975497\n",
      "Epoch 201, Loss: 0.43294048069736957\n",
      "Epoch 202, Loss: 0.43262240576772243\n",
      "Epoch 203, Loss: 0.43206541812705346\n",
      "Epoch 204, Loss: 0.43172500614102416\n",
      "Epoch 205, Loss: 0.4316686388133938\n",
      "Epoch 206, Loss: 0.4315179158466364\n",
      "Epoch 207, Loss: 0.4310712381632941\n",
      "Epoch 208, Loss: 0.43059470035496\n",
      "Epoch 209, Loss: 0.4303711221416647\n",
      "Epoch 210, Loss: 0.43033088719595897\n",
      "Epoch 211, Loss: 0.43016580489992484\n",
      "Epoch 212, Loss: 0.429691817522032\n",
      "Epoch 213, Loss: 0.4292099292794459\n",
      "Epoch 214, Loss: 0.4289207205910941\n",
      "Epoch 215, Loss: 0.4287978238495365\n",
      "Epoch 216, Loss: 0.42870603658529494\n",
      "Epoch 217, Loss: 0.42841980940051866\n",
      "Epoch 218, Loss: 0.4278918633922788\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23992079976320826\n",
      "Valence RMSE: 0.2525349953561071\n",
      "Arousal RMSE: 0.226605508402876\n",
      "Test R^2 score: tensor([0.3185, 0.6213], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4699045289964228\n",
      "Num of epochs: 219\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Epoch 121, Loss: 0.4596922103619757\n",
      "Epoch 122, Loss: 0.45943389956138125\n",
      "Epoch 123, Loss: 0.45877818894117006\n",
      "Epoch 124, Loss: 0.45746538132874076\n",
      "Epoch 125, Loss: 0.4570462803121619\n",
      "Epoch 126, Loss: 0.45696189549769234\n",
      "Epoch 127, Loss: 0.45594920341421274\n",
      "Epoch 128, Loss: 0.45536359747685007\n",
      "Epoch 129, Loss: 0.4553290890752543\n",
      "Epoch 130, Loss: 0.4547195823074053\n",
      "Epoch 131, Loss: 0.45392838323998064\n",
      "Epoch 132, Loss: 0.4537853493595631\n",
      "Epoch 133, Loss: 0.45347437050767975\n",
      "Epoch 134, Loss: 0.45267431273812453\n",
      "Epoch 135, Loss: 0.45228737659373064\n",
      "Epoch 136, Loss: 0.4522154657432981\n",
      "Epoch 137, Loss: 0.45165964487407706\n",
      "Epoch 138, Loss: 0.4510751265462795\n",
      "Epoch 139, Loss: 0.45082650445054473\n",
      "Epoch 140, Loss: 0.4506281098405297\n",
      "Epoch 141, Loss: 0.4501911419656675\n",
      "Epoch 142, Loss: 0.4497001887659326\n",
      "Epoch 143, Loss: 0.44943436133718806\n",
      "Epoch 144, Loss: 0.449226296402944\n",
      "Epoch 145, Loss: 0.44886073897867573\n",
      "Epoch 146, Loss: 0.44844710376304864\n",
      "Epoch 147, Loss: 0.44810507013116724\n",
      "Epoch 148, Loss: 0.44788545862049767\n",
      "Epoch 149, Loss: 0.4476741441009651\n",
      "Epoch 150, Loss: 0.44735861731902055\n",
      "Epoch 151, Loss: 0.4469288720157105\n",
      "Epoch 152, Loss: 0.44655235754178146\n",
      "Epoch 153, Loss: 0.4462857402786373\n",
      "Epoch 154, Loss: 0.4460517369142754\n",
      "Epoch 155, Loss: 0.4457881796184935\n",
      "Epoch 156, Loss: 0.44542210886348255\n",
      "Epoch 157, Loss: 0.4450579467827662\n",
      "Epoch 158, Loss: 0.44473329320345206\n",
      "Epoch 159, Loss: 0.4444482514678638\n",
      "Epoch 160, Loss: 0.4441745674630603\n",
      "Epoch 161, Loss: 0.4439221813722748\n",
      "Epoch 162, Loss: 0.44363357870363757\n",
      "Epoch 163, Loss: 0.4433189071134186\n",
      "Epoch 164, Loss: 0.44295452976444744\n",
      "Epoch 165, Loss: 0.4426058108229731\n",
      "Epoch 166, Loss: 0.4422485787900922\n",
      "Epoch 167, Loss: 0.4419090985167976\n",
      "Epoch 168, Loss: 0.44158054387831963\n",
      "Epoch 169, Loss: 0.4412474388772268\n",
      "Epoch 170, Loss: 0.4409277694136744\n",
      "Epoch 171, Loss: 0.44068045571539705\n",
      "Epoch 172, Loss: 0.4405979418732049\n",
      "Epoch 173, Loss: 0.44094101686109266\n",
      "Epoch 174, Loss: 0.4417368233339534\n",
      "Epoch 175, Loss: 0.4416582519101522\n",
      "Epoch 176, Loss: 0.4396788970821407\n",
      "Epoch 177, Loss: 0.4390195954561405\n",
      "Epoch 178, Loss: 0.4400830988007806\n",
      "Epoch 179, Loss: 0.43931775869233325\n",
      "Epoch 180, Loss: 0.4380032505977321\n",
      "Epoch 181, Loss: 0.4384470974340077\n",
      "Epoch 182, Loss: 0.4385167636581309\n",
      "Epoch 183, Loss: 0.43733043450040754\n",
      "Epoch 184, Loss: 0.4373176399006318\n",
      "Epoch 185, Loss: 0.4375256973621567\n",
      "Epoch 186, Loss: 0.43659322296038683\n",
      "Epoch 187, Loss: 0.43644391045908093\n",
      "Epoch 188, Loss: 0.43663844358015674\n",
      "Epoch 189, Loss: 0.43586286705793914\n",
      "Epoch 190, Loss: 0.4355663072844155\n",
      "Epoch 191, Loss: 0.4357040532214924\n",
      "Epoch 192, Loss: 0.435134007760397\n",
      "Epoch 193, Loss: 0.43474241127906293\n",
      "Epoch 194, Loss: 0.43480482310319635\n",
      "Epoch 195, Loss: 0.43443826104982264\n",
      "Epoch 196, Loss: 0.43390767873021246\n",
      "Epoch 197, Loss: 0.4338897691233524\n",
      "Epoch 198, Loss: 0.43377856877811044\n",
      "Epoch 199, Loss: 0.4331595506415052\n",
      "Epoch 200, Loss: 0.43289415091975497\n",
      "Epoch 201, Loss: 0.43294048069736957\n",
      "Epoch 202, Loss: 0.43262240576772243\n",
      "Epoch 203, Loss: 0.43206541812705346\n",
      "Epoch 204, Loss: 0.43172500614102416\n",
      "Epoch 205, Loss: 0.4316686388133938\n",
      "Epoch 206, Loss: 0.4315179158466364\n",
      "Epoch 207, Loss: 0.4310712381632941\n",
      "Epoch 208, Loss: 0.43059470035496\n",
      "Epoch 209, Loss: 0.4303711221416647\n",
      "Epoch 210, Loss: 0.43033088719595897\n",
      "Epoch 211, Loss: 0.43016580489992484\n",
      "Epoch 212, Loss: 0.429691817522032\n",
      "Epoch 213, Loss: 0.4292099292794459\n",
      "Epoch 214, Loss: 0.4289207205910941\n",
      "Epoch 215, Loss: 0.4287978238495365\n",
      "Epoch 216, Loss: 0.42870603658529494\n",
      "Epoch 217, Loss: 0.42841980940051866\n",
      "Epoch 218, Loss: 0.4278918633922788\n",
      "Epoch 219, Loss: 0.42748307113702383\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23996150527741336\n",
      "Valence RMSE: 0.25250558556037367\n",
      "Arousal RMSE: 0.2267244523442872\n",
      "Test R^2 score: tensor([0.3187, 0.6209], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4697850570865919\n",
      "Num of epochs: 220\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Epoch 121, Loss: 0.4596922103619757\n",
      "Epoch 122, Loss: 0.45943389956138125\n",
      "Epoch 123, Loss: 0.45877818894117006\n",
      "Epoch 124, Loss: 0.45746538132874076\n",
      "Epoch 125, Loss: 0.4570462803121619\n",
      "Epoch 126, Loss: 0.45696189549769234\n",
      "Epoch 127, Loss: 0.45594920341421274\n",
      "Epoch 128, Loss: 0.45536359747685007\n",
      "Epoch 129, Loss: 0.4553290890752543\n",
      "Epoch 130, Loss: 0.4547195823074053\n",
      "Epoch 131, Loss: 0.45392838323998064\n",
      "Epoch 132, Loss: 0.4537853493595631\n",
      "Epoch 133, Loss: 0.45347437050767975\n",
      "Epoch 134, Loss: 0.45267431273812453\n",
      "Epoch 135, Loss: 0.45228737659373064\n",
      "Epoch 136, Loss: 0.4522154657432981\n",
      "Epoch 137, Loss: 0.45165964487407706\n",
      "Epoch 138, Loss: 0.4510751265462795\n",
      "Epoch 139, Loss: 0.45082650445054473\n",
      "Epoch 140, Loss: 0.4506281098405297\n",
      "Epoch 141, Loss: 0.4501911419656675\n",
      "Epoch 142, Loss: 0.4497001887659326\n",
      "Epoch 143, Loss: 0.44943436133718806\n",
      "Epoch 144, Loss: 0.449226296402944\n",
      "Epoch 145, Loss: 0.44886073897867573\n",
      "Epoch 146, Loss: 0.44844710376304864\n",
      "Epoch 147, Loss: 0.44810507013116724\n",
      "Epoch 148, Loss: 0.44788545862049767\n",
      "Epoch 149, Loss: 0.4476741441009651\n",
      "Epoch 150, Loss: 0.44735861731902055\n",
      "Epoch 151, Loss: 0.4469288720157105\n",
      "Epoch 152, Loss: 0.44655235754178146\n",
      "Epoch 153, Loss: 0.4462857402786373\n",
      "Epoch 154, Loss: 0.4460517369142754\n",
      "Epoch 155, Loss: 0.4457881796184935\n",
      "Epoch 156, Loss: 0.44542210886348255\n",
      "Epoch 157, Loss: 0.4450579467827662\n",
      "Epoch 158, Loss: 0.44473329320345206\n",
      "Epoch 159, Loss: 0.4444482514678638\n",
      "Epoch 160, Loss: 0.4441745674630603\n",
      "Epoch 161, Loss: 0.4439221813722748\n",
      "Epoch 162, Loss: 0.44363357870363757\n",
      "Epoch 163, Loss: 0.4433189071134186\n",
      "Epoch 164, Loss: 0.44295452976444744\n",
      "Epoch 165, Loss: 0.4426058108229731\n",
      "Epoch 166, Loss: 0.4422485787900922\n",
      "Epoch 167, Loss: 0.4419090985167976\n",
      "Epoch 168, Loss: 0.44158054387831963\n",
      "Epoch 169, Loss: 0.4412474388772268\n",
      "Epoch 170, Loss: 0.4409277694136744\n",
      "Epoch 171, Loss: 0.44068045571539705\n",
      "Epoch 172, Loss: 0.4405979418732049\n",
      "Epoch 173, Loss: 0.44094101686109266\n",
      "Epoch 174, Loss: 0.4417368233339534\n",
      "Epoch 175, Loss: 0.4416582519101522\n",
      "Epoch 176, Loss: 0.4396788970821407\n",
      "Epoch 177, Loss: 0.4390195954561405\n",
      "Epoch 178, Loss: 0.4400830988007806\n",
      "Epoch 179, Loss: 0.43931775869233325\n",
      "Epoch 180, Loss: 0.4380032505977321\n",
      "Epoch 181, Loss: 0.4384470974340077\n",
      "Epoch 182, Loss: 0.4385167636581309\n",
      "Epoch 183, Loss: 0.43733043450040754\n",
      "Epoch 184, Loss: 0.4373176399006318\n",
      "Epoch 185, Loss: 0.4375256973621567\n",
      "Epoch 186, Loss: 0.43659322296038683\n",
      "Epoch 187, Loss: 0.43644391045908093\n",
      "Epoch 188, Loss: 0.43663844358015674\n",
      "Epoch 189, Loss: 0.43586286705793914\n",
      "Epoch 190, Loss: 0.4355663072844155\n",
      "Epoch 191, Loss: 0.4357040532214924\n",
      "Epoch 192, Loss: 0.435134007760397\n",
      "Epoch 193, Loss: 0.43474241127906293\n",
      "Epoch 194, Loss: 0.43480482310319635\n",
      "Epoch 195, Loss: 0.43443826104982264\n",
      "Epoch 196, Loss: 0.43390767873021246\n",
      "Epoch 197, Loss: 0.4338897691233524\n",
      "Epoch 198, Loss: 0.43377856877811044\n",
      "Epoch 199, Loss: 0.4331595506415052\n",
      "Epoch 200, Loss: 0.43289415091975497\n",
      "Epoch 201, Loss: 0.43294048069736957\n",
      "Epoch 202, Loss: 0.43262240576772243\n",
      "Epoch 203, Loss: 0.43206541812705346\n",
      "Epoch 204, Loss: 0.43172500614102416\n",
      "Epoch 205, Loss: 0.4316686388133938\n",
      "Epoch 206, Loss: 0.4315179158466364\n",
      "Epoch 207, Loss: 0.4310712381632941\n",
      "Epoch 208, Loss: 0.43059470035496\n",
      "Epoch 209, Loss: 0.4303711221416647\n",
      "Epoch 210, Loss: 0.43033088719595897\n",
      "Epoch 211, Loss: 0.43016580489992484\n",
      "Epoch 212, Loss: 0.429691817522032\n",
      "Epoch 213, Loss: 0.4292099292794459\n",
      "Epoch 214, Loss: 0.4289207205910941\n",
      "Epoch 215, Loss: 0.4287978238495365\n",
      "Epoch 216, Loss: 0.42870603658529494\n",
      "Epoch 217, Loss: 0.42841980940051866\n",
      "Epoch 218, Loss: 0.4278918633922788\n",
      "Epoch 219, Loss: 0.42748307113702383\n",
      "Epoch 220, Loss: 0.4271688511370021\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.24016662219893228\n",
      "Valence RMSE: 0.25291407331053073\n",
      "Arousal RMSE: 0.22670351642258402\n",
      "Test R^2 score: tensor([0.3165, 0.6210], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4687169612301283\n",
      "Num of epochs: 221\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Epoch 121, Loss: 0.4596922103619757\n",
      "Epoch 122, Loss: 0.45943389956138125\n",
      "Epoch 123, Loss: 0.45877818894117006\n",
      "Epoch 124, Loss: 0.45746538132874076\n",
      "Epoch 125, Loss: 0.4570462803121619\n",
      "Epoch 126, Loss: 0.45696189549769234\n",
      "Epoch 127, Loss: 0.45594920341421274\n",
      "Epoch 128, Loss: 0.45536359747685007\n",
      "Epoch 129, Loss: 0.4553290890752543\n",
      "Epoch 130, Loss: 0.4547195823074053\n",
      "Epoch 131, Loss: 0.45392838323998064\n",
      "Epoch 132, Loss: 0.4537853493595631\n",
      "Epoch 133, Loss: 0.45347437050767975\n",
      "Epoch 134, Loss: 0.45267431273812453\n",
      "Epoch 135, Loss: 0.45228737659373064\n",
      "Epoch 136, Loss: 0.4522154657432981\n",
      "Epoch 137, Loss: 0.45165964487407706\n",
      "Epoch 138, Loss: 0.4510751265462795\n",
      "Epoch 139, Loss: 0.45082650445054473\n",
      "Epoch 140, Loss: 0.4506281098405297\n",
      "Epoch 141, Loss: 0.4501911419656675\n",
      "Epoch 142, Loss: 0.4497001887659326\n",
      "Epoch 143, Loss: 0.44943436133718806\n",
      "Epoch 144, Loss: 0.449226296402944\n",
      "Epoch 145, Loss: 0.44886073897867573\n",
      "Epoch 146, Loss: 0.44844710376304864\n",
      "Epoch 147, Loss: 0.44810507013116724\n",
      "Epoch 148, Loss: 0.44788545862049767\n",
      "Epoch 149, Loss: 0.4476741441009651\n",
      "Epoch 150, Loss: 0.44735861731902055\n",
      "Epoch 151, Loss: 0.4469288720157105\n",
      "Epoch 152, Loss: 0.44655235754178146\n",
      "Epoch 153, Loss: 0.4462857402786373\n",
      "Epoch 154, Loss: 0.4460517369142754\n",
      "Epoch 155, Loss: 0.4457881796184935\n",
      "Epoch 156, Loss: 0.44542210886348255\n",
      "Epoch 157, Loss: 0.4450579467827662\n",
      "Epoch 158, Loss: 0.44473329320345206\n",
      "Epoch 159, Loss: 0.4444482514678638\n",
      "Epoch 160, Loss: 0.4441745674630603\n",
      "Epoch 161, Loss: 0.4439221813722748\n",
      "Epoch 162, Loss: 0.44363357870363757\n",
      "Epoch 163, Loss: 0.4433189071134186\n",
      "Epoch 164, Loss: 0.44295452976444744\n",
      "Epoch 165, Loss: 0.4426058108229731\n",
      "Epoch 166, Loss: 0.4422485787900922\n",
      "Epoch 167, Loss: 0.4419090985167976\n",
      "Epoch 168, Loss: 0.44158054387831963\n",
      "Epoch 169, Loss: 0.4412474388772268\n",
      "Epoch 170, Loss: 0.4409277694136744\n",
      "Epoch 171, Loss: 0.44068045571539705\n",
      "Epoch 172, Loss: 0.4405979418732049\n",
      "Epoch 173, Loss: 0.44094101686109266\n",
      "Epoch 174, Loss: 0.4417368233339534\n",
      "Epoch 175, Loss: 0.4416582519101522\n",
      "Epoch 176, Loss: 0.4396788970821407\n",
      "Epoch 177, Loss: 0.4390195954561405\n",
      "Epoch 178, Loss: 0.4400830988007806\n",
      "Epoch 179, Loss: 0.43931775869233325\n",
      "Epoch 180, Loss: 0.4380032505977321\n",
      "Epoch 181, Loss: 0.4384470974340077\n",
      "Epoch 182, Loss: 0.4385167636581309\n",
      "Epoch 183, Loss: 0.43733043450040754\n",
      "Epoch 184, Loss: 0.4373176399006318\n",
      "Epoch 185, Loss: 0.4375256973621567\n",
      "Epoch 186, Loss: 0.43659322296038683\n",
      "Epoch 187, Loss: 0.43644391045908093\n",
      "Epoch 188, Loss: 0.43663844358015674\n",
      "Epoch 189, Loss: 0.43586286705793914\n",
      "Epoch 190, Loss: 0.4355663072844155\n",
      "Epoch 191, Loss: 0.4357040532214924\n",
      "Epoch 192, Loss: 0.435134007760397\n",
      "Epoch 193, Loss: 0.43474241127906293\n",
      "Epoch 194, Loss: 0.43480482310319635\n",
      "Epoch 195, Loss: 0.43443826104982264\n",
      "Epoch 196, Loss: 0.43390767873021246\n",
      "Epoch 197, Loss: 0.4338897691233524\n",
      "Epoch 198, Loss: 0.43377856877811044\n",
      "Epoch 199, Loss: 0.4331595506415052\n",
      "Epoch 200, Loss: 0.43289415091975497\n",
      "Epoch 201, Loss: 0.43294048069736957\n",
      "Epoch 202, Loss: 0.43262240576772243\n",
      "Epoch 203, Loss: 0.43206541812705346\n",
      "Epoch 204, Loss: 0.43172500614102416\n",
      "Epoch 205, Loss: 0.4316686388133938\n",
      "Epoch 206, Loss: 0.4315179158466364\n",
      "Epoch 207, Loss: 0.4310712381632941\n",
      "Epoch 208, Loss: 0.43059470035496\n",
      "Epoch 209, Loss: 0.4303711221416647\n",
      "Epoch 210, Loss: 0.43033088719595897\n",
      "Epoch 211, Loss: 0.43016580489992484\n",
      "Epoch 212, Loss: 0.429691817522032\n",
      "Epoch 213, Loss: 0.4292099292794459\n",
      "Epoch 214, Loss: 0.4289207205910941\n",
      "Epoch 215, Loss: 0.4287978238495365\n",
      "Epoch 216, Loss: 0.42870603658529494\n",
      "Epoch 217, Loss: 0.42841980940051866\n",
      "Epoch 218, Loss: 0.4278918633922788\n",
      "Epoch 219, Loss: 0.42748307113702383\n",
      "Epoch 220, Loss: 0.4271688511370021\n",
      "Epoch 221, Loss: 0.4269432345934848\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2401273928901573\n",
      "Valence RMSE: 0.25263342587043736\n",
      "Arousal RMSE: 0.22693320992180532\n",
      "Test R^2 score: tensor([0.3180, 0.6202], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4690908015829368\n",
      "Num of epochs: 222\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Epoch 121, Loss: 0.4596922103619757\n",
      "Epoch 122, Loss: 0.45943389956138125\n",
      "Epoch 123, Loss: 0.45877818894117006\n",
      "Epoch 124, Loss: 0.45746538132874076\n",
      "Epoch 125, Loss: 0.4570462803121619\n",
      "Epoch 126, Loss: 0.45696189549769234\n",
      "Epoch 127, Loss: 0.45594920341421274\n",
      "Epoch 128, Loss: 0.45536359747685007\n",
      "Epoch 129, Loss: 0.4553290890752543\n",
      "Epoch 130, Loss: 0.4547195823074053\n",
      "Epoch 131, Loss: 0.45392838323998064\n",
      "Epoch 132, Loss: 0.4537853493595631\n",
      "Epoch 133, Loss: 0.45347437050767975\n",
      "Epoch 134, Loss: 0.45267431273812453\n",
      "Epoch 135, Loss: 0.45228737659373064\n",
      "Epoch 136, Loss: 0.4522154657432981\n",
      "Epoch 137, Loss: 0.45165964487407706\n",
      "Epoch 138, Loss: 0.4510751265462795\n",
      "Epoch 139, Loss: 0.45082650445054473\n",
      "Epoch 140, Loss: 0.4506281098405297\n",
      "Epoch 141, Loss: 0.4501911419656675\n",
      "Epoch 142, Loss: 0.4497001887659326\n",
      "Epoch 143, Loss: 0.44943436133718806\n",
      "Epoch 144, Loss: 0.449226296402944\n",
      "Epoch 145, Loss: 0.44886073897867573\n",
      "Epoch 146, Loss: 0.44844710376304864\n",
      "Epoch 147, Loss: 0.44810507013116724\n",
      "Epoch 148, Loss: 0.44788545862049767\n",
      "Epoch 149, Loss: 0.4476741441009651\n",
      "Epoch 150, Loss: 0.44735861731902055\n",
      "Epoch 151, Loss: 0.4469288720157105\n",
      "Epoch 152, Loss: 0.44655235754178146\n",
      "Epoch 153, Loss: 0.4462857402786373\n",
      "Epoch 154, Loss: 0.4460517369142754\n",
      "Epoch 155, Loss: 0.4457881796184935\n",
      "Epoch 156, Loss: 0.44542210886348255\n",
      "Epoch 157, Loss: 0.4450579467827662\n",
      "Epoch 158, Loss: 0.44473329320345206\n",
      "Epoch 159, Loss: 0.4444482514678638\n",
      "Epoch 160, Loss: 0.4441745674630603\n",
      "Epoch 161, Loss: 0.4439221813722748\n",
      "Epoch 162, Loss: 0.44363357870363757\n",
      "Epoch 163, Loss: 0.4433189071134186\n",
      "Epoch 164, Loss: 0.44295452976444744\n",
      "Epoch 165, Loss: 0.4426058108229731\n",
      "Epoch 166, Loss: 0.4422485787900922\n",
      "Epoch 167, Loss: 0.4419090985167976\n",
      "Epoch 168, Loss: 0.44158054387831963\n",
      "Epoch 169, Loss: 0.4412474388772268\n",
      "Epoch 170, Loss: 0.4409277694136744\n",
      "Epoch 171, Loss: 0.44068045571539705\n",
      "Epoch 172, Loss: 0.4405979418732049\n",
      "Epoch 173, Loss: 0.44094101686109266\n",
      "Epoch 174, Loss: 0.4417368233339534\n",
      "Epoch 175, Loss: 0.4416582519101522\n",
      "Epoch 176, Loss: 0.4396788970821407\n",
      "Epoch 177, Loss: 0.4390195954561405\n",
      "Epoch 178, Loss: 0.4400830988007806\n",
      "Epoch 179, Loss: 0.43931775869233325\n",
      "Epoch 180, Loss: 0.4380032505977321\n",
      "Epoch 181, Loss: 0.4384470974340077\n",
      "Epoch 182, Loss: 0.4385167636581309\n",
      "Epoch 183, Loss: 0.43733043450040754\n",
      "Epoch 184, Loss: 0.4373176399006318\n",
      "Epoch 185, Loss: 0.4375256973621567\n",
      "Epoch 186, Loss: 0.43659322296038683\n",
      "Epoch 187, Loss: 0.43644391045908093\n",
      "Epoch 188, Loss: 0.43663844358015674\n",
      "Epoch 189, Loss: 0.43586286705793914\n",
      "Epoch 190, Loss: 0.4355663072844155\n",
      "Epoch 191, Loss: 0.4357040532214924\n",
      "Epoch 192, Loss: 0.435134007760397\n",
      "Epoch 193, Loss: 0.43474241127906293\n",
      "Epoch 194, Loss: 0.43480482310319635\n",
      "Epoch 195, Loss: 0.43443826104982264\n",
      "Epoch 196, Loss: 0.43390767873021246\n",
      "Epoch 197, Loss: 0.4338897691233524\n",
      "Epoch 198, Loss: 0.43377856877811044\n",
      "Epoch 199, Loss: 0.4331595506415052\n",
      "Epoch 200, Loss: 0.43289415091975497\n",
      "Epoch 201, Loss: 0.43294048069736957\n",
      "Epoch 202, Loss: 0.43262240576772243\n",
      "Epoch 203, Loss: 0.43206541812705346\n",
      "Epoch 204, Loss: 0.43172500614102416\n",
      "Epoch 205, Loss: 0.4316686388133938\n",
      "Epoch 206, Loss: 0.4315179158466364\n",
      "Epoch 207, Loss: 0.4310712381632941\n",
      "Epoch 208, Loss: 0.43059470035496\n",
      "Epoch 209, Loss: 0.4303711221416647\n",
      "Epoch 210, Loss: 0.43033088719595897\n",
      "Epoch 211, Loss: 0.43016580489992484\n",
      "Epoch 212, Loss: 0.429691817522032\n",
      "Epoch 213, Loss: 0.4292099292794459\n",
      "Epoch 214, Loss: 0.4289207205910941\n",
      "Epoch 215, Loss: 0.4287978238495365\n",
      "Epoch 216, Loss: 0.42870603658529494\n",
      "Epoch 217, Loss: 0.42841980940051866\n",
      "Epoch 218, Loss: 0.4278918633922788\n",
      "Epoch 219, Loss: 0.42748307113702383\n",
      "Epoch 220, Loss: 0.4271688511370021\n",
      "Epoch 221, Loss: 0.4269432345934848\n",
      "Epoch 222, Loss: 0.4267651275776002\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2409645323811227\n",
      "Valence RMSE: 0.25410854626026264\n",
      "Arousal RMSE: 0.2270609135205843\n",
      "Test R^2 score: tensor([0.3100, 0.6198], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.46488310264592503\n",
      "Num of epochs: 223\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Epoch 121, Loss: 0.4596922103619757\n",
      "Epoch 122, Loss: 0.45943389956138125\n",
      "Epoch 123, Loss: 0.45877818894117006\n",
      "Epoch 124, Loss: 0.45746538132874076\n",
      "Epoch 125, Loss: 0.4570462803121619\n",
      "Epoch 126, Loss: 0.45696189549769234\n",
      "Epoch 127, Loss: 0.45594920341421274\n",
      "Epoch 128, Loss: 0.45536359747685007\n",
      "Epoch 129, Loss: 0.4553290890752543\n",
      "Epoch 130, Loss: 0.4547195823074053\n",
      "Epoch 131, Loss: 0.45392838323998064\n",
      "Epoch 132, Loss: 0.4537853493595631\n",
      "Epoch 133, Loss: 0.45347437050767975\n",
      "Epoch 134, Loss: 0.45267431273812453\n",
      "Epoch 135, Loss: 0.45228737659373064\n",
      "Epoch 136, Loss: 0.4522154657432981\n",
      "Epoch 137, Loss: 0.45165964487407706\n",
      "Epoch 138, Loss: 0.4510751265462795\n",
      "Epoch 139, Loss: 0.45082650445054473\n",
      "Epoch 140, Loss: 0.4506281098405297\n",
      "Epoch 141, Loss: 0.4501911419656675\n",
      "Epoch 142, Loss: 0.4497001887659326\n",
      "Epoch 143, Loss: 0.44943436133718806\n",
      "Epoch 144, Loss: 0.449226296402944\n",
      "Epoch 145, Loss: 0.44886073897867573\n",
      "Epoch 146, Loss: 0.44844710376304864\n",
      "Epoch 147, Loss: 0.44810507013116724\n",
      "Epoch 148, Loss: 0.44788545862049767\n",
      "Epoch 149, Loss: 0.4476741441009651\n",
      "Epoch 150, Loss: 0.44735861731902055\n",
      "Epoch 151, Loss: 0.4469288720157105\n",
      "Epoch 152, Loss: 0.44655235754178146\n",
      "Epoch 153, Loss: 0.4462857402786373\n",
      "Epoch 154, Loss: 0.4460517369142754\n",
      "Epoch 155, Loss: 0.4457881796184935\n",
      "Epoch 156, Loss: 0.44542210886348255\n",
      "Epoch 157, Loss: 0.4450579467827662\n",
      "Epoch 158, Loss: 0.44473329320345206\n",
      "Epoch 159, Loss: 0.4444482514678638\n",
      "Epoch 160, Loss: 0.4441745674630603\n",
      "Epoch 161, Loss: 0.4439221813722748\n",
      "Epoch 162, Loss: 0.44363357870363757\n",
      "Epoch 163, Loss: 0.4433189071134186\n",
      "Epoch 164, Loss: 0.44295452976444744\n",
      "Epoch 165, Loss: 0.4426058108229731\n",
      "Epoch 166, Loss: 0.4422485787900922\n",
      "Epoch 167, Loss: 0.4419090985167976\n",
      "Epoch 168, Loss: 0.44158054387831963\n",
      "Epoch 169, Loss: 0.4412474388772268\n",
      "Epoch 170, Loss: 0.4409277694136744\n",
      "Epoch 171, Loss: 0.44068045571539705\n",
      "Epoch 172, Loss: 0.4405979418732049\n",
      "Epoch 173, Loss: 0.44094101686109266\n",
      "Epoch 174, Loss: 0.4417368233339534\n",
      "Epoch 175, Loss: 0.4416582519101522\n",
      "Epoch 176, Loss: 0.4396788970821407\n",
      "Epoch 177, Loss: 0.4390195954561405\n",
      "Epoch 178, Loss: 0.4400830988007806\n",
      "Epoch 179, Loss: 0.43931775869233325\n",
      "Epoch 180, Loss: 0.4380032505977321\n",
      "Epoch 181, Loss: 0.4384470974340077\n",
      "Epoch 182, Loss: 0.4385167636581309\n",
      "Epoch 183, Loss: 0.43733043450040754\n",
      "Epoch 184, Loss: 0.4373176399006318\n",
      "Epoch 185, Loss: 0.4375256973621567\n",
      "Epoch 186, Loss: 0.43659322296038683\n",
      "Epoch 187, Loss: 0.43644391045908093\n",
      "Epoch 188, Loss: 0.43663844358015674\n",
      "Epoch 189, Loss: 0.43586286705793914\n",
      "Epoch 190, Loss: 0.4355663072844155\n",
      "Epoch 191, Loss: 0.4357040532214924\n",
      "Epoch 192, Loss: 0.435134007760397\n",
      "Epoch 193, Loss: 0.43474241127906293\n",
      "Epoch 194, Loss: 0.43480482310319635\n",
      "Epoch 195, Loss: 0.43443826104982264\n",
      "Epoch 196, Loss: 0.43390767873021246\n",
      "Epoch 197, Loss: 0.4338897691233524\n",
      "Epoch 198, Loss: 0.43377856877811044\n",
      "Epoch 199, Loss: 0.4331595506415052\n",
      "Epoch 200, Loss: 0.43289415091975497\n",
      "Epoch 201, Loss: 0.43294048069736957\n",
      "Epoch 202, Loss: 0.43262240576772243\n",
      "Epoch 203, Loss: 0.43206541812705346\n",
      "Epoch 204, Loss: 0.43172500614102416\n",
      "Epoch 205, Loss: 0.4316686388133938\n",
      "Epoch 206, Loss: 0.4315179158466364\n",
      "Epoch 207, Loss: 0.4310712381632941\n",
      "Epoch 208, Loss: 0.43059470035496\n",
      "Epoch 209, Loss: 0.4303711221416647\n",
      "Epoch 210, Loss: 0.43033088719595897\n",
      "Epoch 211, Loss: 0.43016580489992484\n",
      "Epoch 212, Loss: 0.429691817522032\n",
      "Epoch 213, Loss: 0.4292099292794459\n",
      "Epoch 214, Loss: 0.4289207205910941\n",
      "Epoch 215, Loss: 0.4287978238495365\n",
      "Epoch 216, Loss: 0.42870603658529494\n",
      "Epoch 217, Loss: 0.42841980940051866\n",
      "Epoch 218, Loss: 0.4278918633922788\n",
      "Epoch 219, Loss: 0.42748307113702383\n",
      "Epoch 220, Loss: 0.4271688511370021\n",
      "Epoch 221, Loss: 0.4269432345934848\n",
      "Epoch 222, Loss: 0.4267651275776002\n",
      "Epoch 223, Loss: 0.42668376428587257\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2405519632195746\n",
      "Valence RMSE: 0.2529582009458707\n",
      "Arousal RMSE: 0.22747009164242152\n",
      "Test R^2 score: tensor([0.3162, 0.6184], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4673138637087906\n",
      "Num of epochs: 224\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Epoch 121, Loss: 0.4596922103619757\n",
      "Epoch 122, Loss: 0.45943389956138125\n",
      "Epoch 123, Loss: 0.45877818894117006\n",
      "Epoch 124, Loss: 0.45746538132874076\n",
      "Epoch 125, Loss: 0.4570462803121619\n",
      "Epoch 126, Loss: 0.45696189549769234\n",
      "Epoch 127, Loss: 0.45594920341421274\n",
      "Epoch 128, Loss: 0.45536359747685007\n",
      "Epoch 129, Loss: 0.4553290890752543\n",
      "Epoch 130, Loss: 0.4547195823074053\n",
      "Epoch 131, Loss: 0.45392838323998064\n",
      "Epoch 132, Loss: 0.4537853493595631\n",
      "Epoch 133, Loss: 0.45347437050767975\n",
      "Epoch 134, Loss: 0.45267431273812453\n",
      "Epoch 135, Loss: 0.45228737659373064\n",
      "Epoch 136, Loss: 0.4522154657432981\n",
      "Epoch 137, Loss: 0.45165964487407706\n",
      "Epoch 138, Loss: 0.4510751265462795\n",
      "Epoch 139, Loss: 0.45082650445054473\n",
      "Epoch 140, Loss: 0.4506281098405297\n",
      "Epoch 141, Loss: 0.4501911419656675\n",
      "Epoch 142, Loss: 0.4497001887659326\n",
      "Epoch 143, Loss: 0.44943436133718806\n",
      "Epoch 144, Loss: 0.449226296402944\n",
      "Epoch 145, Loss: 0.44886073897867573\n",
      "Epoch 146, Loss: 0.44844710376304864\n",
      "Epoch 147, Loss: 0.44810507013116724\n",
      "Epoch 148, Loss: 0.44788545862049767\n",
      "Epoch 149, Loss: 0.4476741441009651\n",
      "Epoch 150, Loss: 0.44735861731902055\n",
      "Epoch 151, Loss: 0.4469288720157105\n",
      "Epoch 152, Loss: 0.44655235754178146\n",
      "Epoch 153, Loss: 0.4462857402786373\n",
      "Epoch 154, Loss: 0.4460517369142754\n",
      "Epoch 155, Loss: 0.4457881796184935\n",
      "Epoch 156, Loss: 0.44542210886348255\n",
      "Epoch 157, Loss: 0.4450579467827662\n",
      "Epoch 158, Loss: 0.44473329320345206\n",
      "Epoch 159, Loss: 0.4444482514678638\n",
      "Epoch 160, Loss: 0.4441745674630603\n",
      "Epoch 161, Loss: 0.4439221813722748\n",
      "Epoch 162, Loss: 0.44363357870363757\n",
      "Epoch 163, Loss: 0.4433189071134186\n",
      "Epoch 164, Loss: 0.44295452976444744\n",
      "Epoch 165, Loss: 0.4426058108229731\n",
      "Epoch 166, Loss: 0.4422485787900922\n",
      "Epoch 167, Loss: 0.4419090985167976\n",
      "Epoch 168, Loss: 0.44158054387831963\n",
      "Epoch 169, Loss: 0.4412474388772268\n",
      "Epoch 170, Loss: 0.4409277694136744\n",
      "Epoch 171, Loss: 0.44068045571539705\n",
      "Epoch 172, Loss: 0.4405979418732049\n",
      "Epoch 173, Loss: 0.44094101686109266\n",
      "Epoch 174, Loss: 0.4417368233339534\n",
      "Epoch 175, Loss: 0.4416582519101522\n",
      "Epoch 176, Loss: 0.4396788970821407\n",
      "Epoch 177, Loss: 0.4390195954561405\n",
      "Epoch 178, Loss: 0.4400830988007806\n",
      "Epoch 179, Loss: 0.43931775869233325\n",
      "Epoch 180, Loss: 0.4380032505977321\n",
      "Epoch 181, Loss: 0.4384470974340077\n",
      "Epoch 182, Loss: 0.4385167636581309\n",
      "Epoch 183, Loss: 0.43733043450040754\n",
      "Epoch 184, Loss: 0.4373176399006318\n",
      "Epoch 185, Loss: 0.4375256973621567\n",
      "Epoch 186, Loss: 0.43659322296038683\n",
      "Epoch 187, Loss: 0.43644391045908093\n",
      "Epoch 188, Loss: 0.43663844358015674\n",
      "Epoch 189, Loss: 0.43586286705793914\n",
      "Epoch 190, Loss: 0.4355663072844155\n",
      "Epoch 191, Loss: 0.4357040532214924\n",
      "Epoch 192, Loss: 0.435134007760397\n",
      "Epoch 193, Loss: 0.43474241127906293\n",
      "Epoch 194, Loss: 0.43480482310319635\n",
      "Epoch 195, Loss: 0.43443826104982264\n",
      "Epoch 196, Loss: 0.43390767873021246\n",
      "Epoch 197, Loss: 0.4338897691233524\n",
      "Epoch 198, Loss: 0.43377856877811044\n",
      "Epoch 199, Loss: 0.4331595506415052\n",
      "Epoch 200, Loss: 0.43289415091975497\n",
      "Epoch 201, Loss: 0.43294048069736957\n",
      "Epoch 202, Loss: 0.43262240576772243\n",
      "Epoch 203, Loss: 0.43206541812705346\n",
      "Epoch 204, Loss: 0.43172500614102416\n",
      "Epoch 205, Loss: 0.4316686388133938\n",
      "Epoch 206, Loss: 0.4315179158466364\n",
      "Epoch 207, Loss: 0.4310712381632941\n",
      "Epoch 208, Loss: 0.43059470035496\n",
      "Epoch 209, Loss: 0.4303711221416647\n",
      "Epoch 210, Loss: 0.43033088719595897\n",
      "Epoch 211, Loss: 0.43016580489992484\n",
      "Epoch 212, Loss: 0.429691817522032\n",
      "Epoch 213, Loss: 0.4292099292794459\n",
      "Epoch 214, Loss: 0.4289207205910941\n",
      "Epoch 215, Loss: 0.4287978238495365\n",
      "Epoch 216, Loss: 0.42870603658529494\n",
      "Epoch 217, Loss: 0.42841980940051866\n",
      "Epoch 218, Loss: 0.4278918633922788\n",
      "Epoch 219, Loss: 0.42748307113702383\n",
      "Epoch 220, Loss: 0.4271688511370021\n",
      "Epoch 221, Loss: 0.4269432345934848\n",
      "Epoch 222, Loss: 0.4267651275776002\n",
      "Epoch 223, Loss: 0.42668376428587257\n",
      "Epoch 224, Loss: 0.42653554195413756\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.24134002509098285\n",
      "Valence RMSE: 0.25449108914419843\n",
      "Arousal RMSE: 0.22742977150767246\n",
      "Test R^2 score: tensor([0.3079, 0.6185], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4632253866546387\n",
      "Num of epochs: 225\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Epoch 121, Loss: 0.4596922103619757\n",
      "Epoch 122, Loss: 0.45943389956138125\n",
      "Epoch 123, Loss: 0.45877818894117006\n",
      "Epoch 124, Loss: 0.45746538132874076\n",
      "Epoch 125, Loss: 0.4570462803121619\n",
      "Epoch 126, Loss: 0.45696189549769234\n",
      "Epoch 127, Loss: 0.45594920341421274\n",
      "Epoch 128, Loss: 0.45536359747685007\n",
      "Epoch 129, Loss: 0.4553290890752543\n",
      "Epoch 130, Loss: 0.4547195823074053\n",
      "Epoch 131, Loss: 0.45392838323998064\n",
      "Epoch 132, Loss: 0.4537853493595631\n",
      "Epoch 133, Loss: 0.45347437050767975\n",
      "Epoch 134, Loss: 0.45267431273812453\n",
      "Epoch 135, Loss: 0.45228737659373064\n",
      "Epoch 136, Loss: 0.4522154657432981\n",
      "Epoch 137, Loss: 0.45165964487407706\n",
      "Epoch 138, Loss: 0.4510751265462795\n",
      "Epoch 139, Loss: 0.45082650445054473\n",
      "Epoch 140, Loss: 0.4506281098405297\n",
      "Epoch 141, Loss: 0.4501911419656675\n",
      "Epoch 142, Loss: 0.4497001887659326\n",
      "Epoch 143, Loss: 0.44943436133718806\n",
      "Epoch 144, Loss: 0.449226296402944\n",
      "Epoch 145, Loss: 0.44886073897867573\n",
      "Epoch 146, Loss: 0.44844710376304864\n",
      "Epoch 147, Loss: 0.44810507013116724\n",
      "Epoch 148, Loss: 0.44788545862049767\n",
      "Epoch 149, Loss: 0.4476741441009651\n",
      "Epoch 150, Loss: 0.44735861731902055\n",
      "Epoch 151, Loss: 0.4469288720157105\n",
      "Epoch 152, Loss: 0.44655235754178146\n",
      "Epoch 153, Loss: 0.4462857402786373\n",
      "Epoch 154, Loss: 0.4460517369142754\n",
      "Epoch 155, Loss: 0.4457881796184935\n",
      "Epoch 156, Loss: 0.44542210886348255\n",
      "Epoch 157, Loss: 0.4450579467827662\n",
      "Epoch 158, Loss: 0.44473329320345206\n",
      "Epoch 159, Loss: 0.4444482514678638\n",
      "Epoch 160, Loss: 0.4441745674630603\n",
      "Epoch 161, Loss: 0.4439221813722748\n",
      "Epoch 162, Loss: 0.44363357870363757\n",
      "Epoch 163, Loss: 0.4433189071134186\n",
      "Epoch 164, Loss: 0.44295452976444744\n",
      "Epoch 165, Loss: 0.4426058108229731\n",
      "Epoch 166, Loss: 0.4422485787900922\n",
      "Epoch 167, Loss: 0.4419090985167976\n",
      "Epoch 168, Loss: 0.44158054387831963\n",
      "Epoch 169, Loss: 0.4412474388772268\n",
      "Epoch 170, Loss: 0.4409277694136744\n",
      "Epoch 171, Loss: 0.44068045571539705\n",
      "Epoch 172, Loss: 0.4405979418732049\n",
      "Epoch 173, Loss: 0.44094101686109266\n",
      "Epoch 174, Loss: 0.4417368233339534\n",
      "Epoch 175, Loss: 0.4416582519101522\n",
      "Epoch 176, Loss: 0.4396788970821407\n",
      "Epoch 177, Loss: 0.4390195954561405\n",
      "Epoch 178, Loss: 0.4400830988007806\n",
      "Epoch 179, Loss: 0.43931775869233325\n",
      "Epoch 180, Loss: 0.4380032505977321\n",
      "Epoch 181, Loss: 0.4384470974340077\n",
      "Epoch 182, Loss: 0.4385167636581309\n",
      "Epoch 183, Loss: 0.43733043450040754\n",
      "Epoch 184, Loss: 0.4373176399006318\n",
      "Epoch 185, Loss: 0.4375256973621567\n",
      "Epoch 186, Loss: 0.43659322296038683\n",
      "Epoch 187, Loss: 0.43644391045908093\n",
      "Epoch 188, Loss: 0.43663844358015674\n",
      "Epoch 189, Loss: 0.43586286705793914\n",
      "Epoch 190, Loss: 0.4355663072844155\n",
      "Epoch 191, Loss: 0.4357040532214924\n",
      "Epoch 192, Loss: 0.435134007760397\n",
      "Epoch 193, Loss: 0.43474241127906293\n",
      "Epoch 194, Loss: 0.43480482310319635\n",
      "Epoch 195, Loss: 0.43443826104982264\n",
      "Epoch 196, Loss: 0.43390767873021246\n",
      "Epoch 197, Loss: 0.4338897691233524\n",
      "Epoch 198, Loss: 0.43377856877811044\n",
      "Epoch 199, Loss: 0.4331595506415052\n",
      "Epoch 200, Loss: 0.43289415091975497\n",
      "Epoch 201, Loss: 0.43294048069736957\n",
      "Epoch 202, Loss: 0.43262240576772243\n",
      "Epoch 203, Loss: 0.43206541812705346\n",
      "Epoch 204, Loss: 0.43172500614102416\n",
      "Epoch 205, Loss: 0.4316686388133938\n",
      "Epoch 206, Loss: 0.4315179158466364\n",
      "Epoch 207, Loss: 0.4310712381632941\n",
      "Epoch 208, Loss: 0.43059470035496\n",
      "Epoch 209, Loss: 0.4303711221416647\n",
      "Epoch 210, Loss: 0.43033088719595897\n",
      "Epoch 211, Loss: 0.43016580489992484\n",
      "Epoch 212, Loss: 0.429691817522032\n",
      "Epoch 213, Loss: 0.4292099292794459\n",
      "Epoch 214, Loss: 0.4289207205910941\n",
      "Epoch 215, Loss: 0.4287978238495365\n",
      "Epoch 216, Loss: 0.42870603658529494\n",
      "Epoch 217, Loss: 0.42841980940051866\n",
      "Epoch 218, Loss: 0.4278918633922788\n",
      "Epoch 219, Loss: 0.42748307113702383\n",
      "Epoch 220, Loss: 0.4271688511370021\n",
      "Epoch 221, Loss: 0.4269432345934848\n",
      "Epoch 222, Loss: 0.4267651275776002\n",
      "Epoch 223, Loss: 0.42668376428587257\n",
      "Epoch 224, Loss: 0.42653554195413756\n",
      "Epoch 225, Loss: 0.42613293183996703\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2406903293537975\n",
      "Valence RMSE: 0.25325198019601053\n",
      "Arousal RMSE: 0.22743593343110613\n",
      "Test R^2 score: tensor([0.3146, 0.6185], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.46657658552478504\n",
      "Num of epochs: 226\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Epoch 121, Loss: 0.4596922103619757\n",
      "Epoch 122, Loss: 0.45943389956138125\n",
      "Epoch 123, Loss: 0.45877818894117006\n",
      "Epoch 124, Loss: 0.45746538132874076\n",
      "Epoch 125, Loss: 0.4570462803121619\n",
      "Epoch 126, Loss: 0.45696189549769234\n",
      "Epoch 127, Loss: 0.45594920341421274\n",
      "Epoch 128, Loss: 0.45536359747685007\n",
      "Epoch 129, Loss: 0.4553290890752543\n",
      "Epoch 130, Loss: 0.4547195823074053\n",
      "Epoch 131, Loss: 0.45392838323998064\n",
      "Epoch 132, Loss: 0.4537853493595631\n",
      "Epoch 133, Loss: 0.45347437050767975\n",
      "Epoch 134, Loss: 0.45267431273812453\n",
      "Epoch 135, Loss: 0.45228737659373064\n",
      "Epoch 136, Loss: 0.4522154657432981\n",
      "Epoch 137, Loss: 0.45165964487407706\n",
      "Epoch 138, Loss: 0.4510751265462795\n",
      "Epoch 139, Loss: 0.45082650445054473\n",
      "Epoch 140, Loss: 0.4506281098405297\n",
      "Epoch 141, Loss: 0.4501911419656675\n",
      "Epoch 142, Loss: 0.4497001887659326\n",
      "Epoch 143, Loss: 0.44943436133718806\n",
      "Epoch 144, Loss: 0.449226296402944\n",
      "Epoch 145, Loss: 0.44886073897867573\n",
      "Epoch 146, Loss: 0.44844710376304864\n",
      "Epoch 147, Loss: 0.44810507013116724\n",
      "Epoch 148, Loss: 0.44788545862049767\n",
      "Epoch 149, Loss: 0.4476741441009651\n",
      "Epoch 150, Loss: 0.44735861731902055\n",
      "Epoch 151, Loss: 0.4469288720157105\n",
      "Epoch 152, Loss: 0.44655235754178146\n",
      "Epoch 153, Loss: 0.4462857402786373\n",
      "Epoch 154, Loss: 0.4460517369142754\n",
      "Epoch 155, Loss: 0.4457881796184935\n",
      "Epoch 156, Loss: 0.44542210886348255\n",
      "Epoch 157, Loss: 0.4450579467827662\n",
      "Epoch 158, Loss: 0.44473329320345206\n",
      "Epoch 159, Loss: 0.4444482514678638\n",
      "Epoch 160, Loss: 0.4441745674630603\n",
      "Epoch 161, Loss: 0.4439221813722748\n",
      "Epoch 162, Loss: 0.44363357870363757\n",
      "Epoch 163, Loss: 0.4433189071134186\n",
      "Epoch 164, Loss: 0.44295452976444744\n",
      "Epoch 165, Loss: 0.4426058108229731\n",
      "Epoch 166, Loss: 0.4422485787900922\n",
      "Epoch 167, Loss: 0.4419090985167976\n",
      "Epoch 168, Loss: 0.44158054387831963\n",
      "Epoch 169, Loss: 0.4412474388772268\n",
      "Epoch 170, Loss: 0.4409277694136744\n",
      "Epoch 171, Loss: 0.44068045571539705\n",
      "Epoch 172, Loss: 0.4405979418732049\n",
      "Epoch 173, Loss: 0.44094101686109266\n",
      "Epoch 174, Loss: 0.4417368233339534\n",
      "Epoch 175, Loss: 0.4416582519101522\n",
      "Epoch 176, Loss: 0.4396788970821407\n",
      "Epoch 177, Loss: 0.4390195954561405\n",
      "Epoch 178, Loss: 0.4400830988007806\n",
      "Epoch 179, Loss: 0.43931775869233325\n",
      "Epoch 180, Loss: 0.4380032505977321\n",
      "Epoch 181, Loss: 0.4384470974340077\n",
      "Epoch 182, Loss: 0.4385167636581309\n",
      "Epoch 183, Loss: 0.43733043450040754\n",
      "Epoch 184, Loss: 0.4373176399006318\n",
      "Epoch 185, Loss: 0.4375256973621567\n",
      "Epoch 186, Loss: 0.43659322296038683\n",
      "Epoch 187, Loss: 0.43644391045908093\n",
      "Epoch 188, Loss: 0.43663844358015674\n",
      "Epoch 189, Loss: 0.43586286705793914\n",
      "Epoch 190, Loss: 0.4355663072844155\n",
      "Epoch 191, Loss: 0.4357040532214924\n",
      "Epoch 192, Loss: 0.435134007760397\n",
      "Epoch 193, Loss: 0.43474241127906293\n",
      "Epoch 194, Loss: 0.43480482310319635\n",
      "Epoch 195, Loss: 0.43443826104982264\n",
      "Epoch 196, Loss: 0.43390767873021246\n",
      "Epoch 197, Loss: 0.4338897691233524\n",
      "Epoch 198, Loss: 0.43377856877811044\n",
      "Epoch 199, Loss: 0.4331595506415052\n",
      "Epoch 200, Loss: 0.43289415091975497\n",
      "Epoch 201, Loss: 0.43294048069736957\n",
      "Epoch 202, Loss: 0.43262240576772243\n",
      "Epoch 203, Loss: 0.43206541812705346\n",
      "Epoch 204, Loss: 0.43172500614102416\n",
      "Epoch 205, Loss: 0.4316686388133938\n",
      "Epoch 206, Loss: 0.4315179158466364\n",
      "Epoch 207, Loss: 0.4310712381632941\n",
      "Epoch 208, Loss: 0.43059470035496\n",
      "Epoch 209, Loss: 0.4303711221416647\n",
      "Epoch 210, Loss: 0.43033088719595897\n",
      "Epoch 211, Loss: 0.43016580489992484\n",
      "Epoch 212, Loss: 0.429691817522032\n",
      "Epoch 213, Loss: 0.4292099292794459\n",
      "Epoch 214, Loss: 0.4289207205910941\n",
      "Epoch 215, Loss: 0.4287978238495365\n",
      "Epoch 216, Loss: 0.42870603658529494\n",
      "Epoch 217, Loss: 0.42841980940051866\n",
      "Epoch 218, Loss: 0.4278918633922788\n",
      "Epoch 219, Loss: 0.42748307113702383\n",
      "Epoch 220, Loss: 0.4271688511370021\n",
      "Epoch 221, Loss: 0.4269432345934848\n",
      "Epoch 222, Loss: 0.4267651275776002\n",
      "Epoch 223, Loss: 0.42668376428587257\n",
      "Epoch 224, Loss: 0.42653554195413756\n",
      "Epoch 225, Loss: 0.42613293183996703\n",
      "Epoch 226, Loss: 0.4255451632780942\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.24091721102658284\n",
      "Valence RMSE: 0.25384912235070245\n",
      "Arousal RMSE: 0.22725058464046288\n",
      "Test R^2 score: tensor([0.3114, 0.6191], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.46526943268384435\n",
      "Num of epochs: 227\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Epoch 121, Loss: 0.4596922103619757\n",
      "Epoch 122, Loss: 0.45943389956138125\n",
      "Epoch 123, Loss: 0.45877818894117006\n",
      "Epoch 124, Loss: 0.45746538132874076\n",
      "Epoch 125, Loss: 0.4570462803121619\n",
      "Epoch 126, Loss: 0.45696189549769234\n",
      "Epoch 127, Loss: 0.45594920341421274\n",
      "Epoch 128, Loss: 0.45536359747685007\n",
      "Epoch 129, Loss: 0.4553290890752543\n",
      "Epoch 130, Loss: 0.4547195823074053\n",
      "Epoch 131, Loss: 0.45392838323998064\n",
      "Epoch 132, Loss: 0.4537853493595631\n",
      "Epoch 133, Loss: 0.45347437050767975\n",
      "Epoch 134, Loss: 0.45267431273812453\n",
      "Epoch 135, Loss: 0.45228737659373064\n",
      "Epoch 136, Loss: 0.4522154657432981\n",
      "Epoch 137, Loss: 0.45165964487407706\n",
      "Epoch 138, Loss: 0.4510751265462795\n",
      "Epoch 139, Loss: 0.45082650445054473\n",
      "Epoch 140, Loss: 0.4506281098405297\n",
      "Epoch 141, Loss: 0.4501911419656675\n",
      "Epoch 142, Loss: 0.4497001887659326\n",
      "Epoch 143, Loss: 0.44943436133718806\n",
      "Epoch 144, Loss: 0.449226296402944\n",
      "Epoch 145, Loss: 0.44886073897867573\n",
      "Epoch 146, Loss: 0.44844710376304864\n",
      "Epoch 147, Loss: 0.44810507013116724\n",
      "Epoch 148, Loss: 0.44788545862049767\n",
      "Epoch 149, Loss: 0.4476741441009651\n",
      "Epoch 150, Loss: 0.44735861731902055\n",
      "Epoch 151, Loss: 0.4469288720157105\n",
      "Epoch 152, Loss: 0.44655235754178146\n",
      "Epoch 153, Loss: 0.4462857402786373\n",
      "Epoch 154, Loss: 0.4460517369142754\n",
      "Epoch 155, Loss: 0.4457881796184935\n",
      "Epoch 156, Loss: 0.44542210886348255\n",
      "Epoch 157, Loss: 0.4450579467827662\n",
      "Epoch 158, Loss: 0.44473329320345206\n",
      "Epoch 159, Loss: 0.4444482514678638\n",
      "Epoch 160, Loss: 0.4441745674630603\n",
      "Epoch 161, Loss: 0.4439221813722748\n",
      "Epoch 162, Loss: 0.44363357870363757\n",
      "Epoch 163, Loss: 0.4433189071134186\n",
      "Epoch 164, Loss: 0.44295452976444744\n",
      "Epoch 165, Loss: 0.4426058108229731\n",
      "Epoch 166, Loss: 0.4422485787900922\n",
      "Epoch 167, Loss: 0.4419090985167976\n",
      "Epoch 168, Loss: 0.44158054387831963\n",
      "Epoch 169, Loss: 0.4412474388772268\n",
      "Epoch 170, Loss: 0.4409277694136744\n",
      "Epoch 171, Loss: 0.44068045571539705\n",
      "Epoch 172, Loss: 0.4405979418732049\n",
      "Epoch 173, Loss: 0.44094101686109266\n",
      "Epoch 174, Loss: 0.4417368233339534\n",
      "Epoch 175, Loss: 0.4416582519101522\n",
      "Epoch 176, Loss: 0.4396788970821407\n",
      "Epoch 177, Loss: 0.4390195954561405\n",
      "Epoch 178, Loss: 0.4400830988007806\n",
      "Epoch 179, Loss: 0.43931775869233325\n",
      "Epoch 180, Loss: 0.4380032505977321\n",
      "Epoch 181, Loss: 0.4384470974340077\n",
      "Epoch 182, Loss: 0.4385167636581309\n",
      "Epoch 183, Loss: 0.43733043450040754\n",
      "Epoch 184, Loss: 0.4373176399006318\n",
      "Epoch 185, Loss: 0.4375256973621567\n",
      "Epoch 186, Loss: 0.43659322296038683\n",
      "Epoch 187, Loss: 0.43644391045908093\n",
      "Epoch 188, Loss: 0.43663844358015674\n",
      "Epoch 189, Loss: 0.43586286705793914\n",
      "Epoch 190, Loss: 0.4355663072844155\n",
      "Epoch 191, Loss: 0.4357040532214924\n",
      "Epoch 192, Loss: 0.435134007760397\n",
      "Epoch 193, Loss: 0.43474241127906293\n",
      "Epoch 194, Loss: 0.43480482310319635\n",
      "Epoch 195, Loss: 0.43443826104982264\n",
      "Epoch 196, Loss: 0.43390767873021246\n",
      "Epoch 197, Loss: 0.4338897691233524\n",
      "Epoch 198, Loss: 0.43377856877811044\n",
      "Epoch 199, Loss: 0.4331595506415052\n",
      "Epoch 200, Loss: 0.43289415091975497\n",
      "Epoch 201, Loss: 0.43294048069736957\n",
      "Epoch 202, Loss: 0.43262240576772243\n",
      "Epoch 203, Loss: 0.43206541812705346\n",
      "Epoch 204, Loss: 0.43172500614102416\n",
      "Epoch 205, Loss: 0.4316686388133938\n",
      "Epoch 206, Loss: 0.4315179158466364\n",
      "Epoch 207, Loss: 0.4310712381632941\n",
      "Epoch 208, Loss: 0.43059470035496\n",
      "Epoch 209, Loss: 0.4303711221416647\n",
      "Epoch 210, Loss: 0.43033088719595897\n",
      "Epoch 211, Loss: 0.43016580489992484\n",
      "Epoch 212, Loss: 0.429691817522032\n",
      "Epoch 213, Loss: 0.4292099292794459\n",
      "Epoch 214, Loss: 0.4289207205910941\n",
      "Epoch 215, Loss: 0.4287978238495365\n",
      "Epoch 216, Loss: 0.42870603658529494\n",
      "Epoch 217, Loss: 0.42841980940051866\n",
      "Epoch 218, Loss: 0.4278918633922788\n",
      "Epoch 219, Loss: 0.42748307113702383\n",
      "Epoch 220, Loss: 0.4271688511370021\n",
      "Epoch 221, Loss: 0.4269432345934848\n",
      "Epoch 222, Loss: 0.4267651275776002\n",
      "Epoch 223, Loss: 0.42668376428587257\n",
      "Epoch 224, Loss: 0.42653554195413756\n",
      "Epoch 225, Loss: 0.42613293183996703\n",
      "Epoch 226, Loss: 0.4255451632780942\n",
      "Epoch 227, Loss: 0.4251305043179084\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.24114854816091816\n",
      "Valence RMSE: 0.2540389339901197\n",
      "Arousal RMSE: 0.22752904117365094\n",
      "Test R^2 score: tensor([0.3104, 0.6182], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4642873834766974\n",
      "Num of epochs: 228\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Epoch 121, Loss: 0.4596922103619757\n",
      "Epoch 122, Loss: 0.45943389956138125\n",
      "Epoch 123, Loss: 0.45877818894117006\n",
      "Epoch 124, Loss: 0.45746538132874076\n",
      "Epoch 125, Loss: 0.4570462803121619\n",
      "Epoch 126, Loss: 0.45696189549769234\n",
      "Epoch 127, Loss: 0.45594920341421274\n",
      "Epoch 128, Loss: 0.45536359747685007\n",
      "Epoch 129, Loss: 0.4553290890752543\n",
      "Epoch 130, Loss: 0.4547195823074053\n",
      "Epoch 131, Loss: 0.45392838323998064\n",
      "Epoch 132, Loss: 0.4537853493595631\n",
      "Epoch 133, Loss: 0.45347437050767975\n",
      "Epoch 134, Loss: 0.45267431273812453\n",
      "Epoch 135, Loss: 0.45228737659373064\n",
      "Epoch 136, Loss: 0.4522154657432981\n",
      "Epoch 137, Loss: 0.45165964487407706\n",
      "Epoch 138, Loss: 0.4510751265462795\n",
      "Epoch 139, Loss: 0.45082650445054473\n",
      "Epoch 140, Loss: 0.4506281098405297\n",
      "Epoch 141, Loss: 0.4501911419656675\n",
      "Epoch 142, Loss: 0.4497001887659326\n",
      "Epoch 143, Loss: 0.44943436133718806\n",
      "Epoch 144, Loss: 0.449226296402944\n",
      "Epoch 145, Loss: 0.44886073897867573\n",
      "Epoch 146, Loss: 0.44844710376304864\n",
      "Epoch 147, Loss: 0.44810507013116724\n",
      "Epoch 148, Loss: 0.44788545862049767\n",
      "Epoch 149, Loss: 0.4476741441009651\n",
      "Epoch 150, Loss: 0.44735861731902055\n",
      "Epoch 151, Loss: 0.4469288720157105\n",
      "Epoch 152, Loss: 0.44655235754178146\n",
      "Epoch 153, Loss: 0.4462857402786373\n",
      "Epoch 154, Loss: 0.4460517369142754\n",
      "Epoch 155, Loss: 0.4457881796184935\n",
      "Epoch 156, Loss: 0.44542210886348255\n",
      "Epoch 157, Loss: 0.4450579467827662\n",
      "Epoch 158, Loss: 0.44473329320345206\n",
      "Epoch 159, Loss: 0.4444482514678638\n",
      "Epoch 160, Loss: 0.4441745674630603\n",
      "Epoch 161, Loss: 0.4439221813722748\n",
      "Epoch 162, Loss: 0.44363357870363757\n",
      "Epoch 163, Loss: 0.4433189071134186\n",
      "Epoch 164, Loss: 0.44295452976444744\n",
      "Epoch 165, Loss: 0.4426058108229731\n",
      "Epoch 166, Loss: 0.4422485787900922\n",
      "Epoch 167, Loss: 0.4419090985167976\n",
      "Epoch 168, Loss: 0.44158054387831963\n",
      "Epoch 169, Loss: 0.4412474388772268\n",
      "Epoch 170, Loss: 0.4409277694136744\n",
      "Epoch 171, Loss: 0.44068045571539705\n",
      "Epoch 172, Loss: 0.4405979418732049\n",
      "Epoch 173, Loss: 0.44094101686109266\n",
      "Epoch 174, Loss: 0.4417368233339534\n",
      "Epoch 175, Loss: 0.4416582519101522\n",
      "Epoch 176, Loss: 0.4396788970821407\n",
      "Epoch 177, Loss: 0.4390195954561405\n",
      "Epoch 178, Loss: 0.4400830988007806\n",
      "Epoch 179, Loss: 0.43931775869233325\n",
      "Epoch 180, Loss: 0.4380032505977321\n",
      "Epoch 181, Loss: 0.4384470974340077\n",
      "Epoch 182, Loss: 0.4385167636581309\n",
      "Epoch 183, Loss: 0.43733043450040754\n",
      "Epoch 184, Loss: 0.4373176399006318\n",
      "Epoch 185, Loss: 0.4375256973621567\n",
      "Epoch 186, Loss: 0.43659322296038683\n",
      "Epoch 187, Loss: 0.43644391045908093\n",
      "Epoch 188, Loss: 0.43663844358015674\n",
      "Epoch 189, Loss: 0.43586286705793914\n",
      "Epoch 190, Loss: 0.4355663072844155\n",
      "Epoch 191, Loss: 0.4357040532214924\n",
      "Epoch 192, Loss: 0.435134007760397\n",
      "Epoch 193, Loss: 0.43474241127906293\n",
      "Epoch 194, Loss: 0.43480482310319635\n",
      "Epoch 195, Loss: 0.43443826104982264\n",
      "Epoch 196, Loss: 0.43390767873021246\n",
      "Epoch 197, Loss: 0.4338897691233524\n",
      "Epoch 198, Loss: 0.43377856877811044\n",
      "Epoch 199, Loss: 0.4331595506415052\n",
      "Epoch 200, Loss: 0.43289415091975497\n",
      "Epoch 201, Loss: 0.43294048069736957\n",
      "Epoch 202, Loss: 0.43262240576772243\n",
      "Epoch 203, Loss: 0.43206541812705346\n",
      "Epoch 204, Loss: 0.43172500614102416\n",
      "Epoch 205, Loss: 0.4316686388133938\n",
      "Epoch 206, Loss: 0.4315179158466364\n",
      "Epoch 207, Loss: 0.4310712381632941\n",
      "Epoch 208, Loss: 0.43059470035496\n",
      "Epoch 209, Loss: 0.4303711221416647\n",
      "Epoch 210, Loss: 0.43033088719595897\n",
      "Epoch 211, Loss: 0.43016580489992484\n",
      "Epoch 212, Loss: 0.429691817522032\n",
      "Epoch 213, Loss: 0.4292099292794459\n",
      "Epoch 214, Loss: 0.4289207205910941\n",
      "Epoch 215, Loss: 0.4287978238495365\n",
      "Epoch 216, Loss: 0.42870603658529494\n",
      "Epoch 217, Loss: 0.42841980940051866\n",
      "Epoch 218, Loss: 0.4278918633922788\n",
      "Epoch 219, Loss: 0.42748307113702383\n",
      "Epoch 220, Loss: 0.4271688511370021\n",
      "Epoch 221, Loss: 0.4269432345934848\n",
      "Epoch 222, Loss: 0.4267651275776002\n",
      "Epoch 223, Loss: 0.42668376428587257\n",
      "Epoch 224, Loss: 0.42653554195413756\n",
      "Epoch 225, Loss: 0.42613293183996703\n",
      "Epoch 226, Loss: 0.4255451632780942\n",
      "Epoch 227, Loss: 0.4251305043179084\n",
      "Epoch 228, Loss: 0.42475934718412794\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.24150783755336916\n",
      "Valence RMSE: 0.2542274273053832\n",
      "Arousal RMSE: 0.22808000001116974\n",
      "Test R^2 score: tensor([0.3093, 0.6164], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4628498618673966\n",
      "Num of epochs: 229\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Epoch 121, Loss: 0.4596922103619757\n",
      "Epoch 122, Loss: 0.45943389956138125\n",
      "Epoch 123, Loss: 0.45877818894117006\n",
      "Epoch 124, Loss: 0.45746538132874076\n",
      "Epoch 125, Loss: 0.4570462803121619\n",
      "Epoch 126, Loss: 0.45696189549769234\n",
      "Epoch 127, Loss: 0.45594920341421274\n",
      "Epoch 128, Loss: 0.45536359747685007\n",
      "Epoch 129, Loss: 0.4553290890752543\n",
      "Epoch 130, Loss: 0.4547195823074053\n",
      "Epoch 131, Loss: 0.45392838323998064\n",
      "Epoch 132, Loss: 0.4537853493595631\n",
      "Epoch 133, Loss: 0.45347437050767975\n",
      "Epoch 134, Loss: 0.45267431273812453\n",
      "Epoch 135, Loss: 0.45228737659373064\n",
      "Epoch 136, Loss: 0.4522154657432981\n",
      "Epoch 137, Loss: 0.45165964487407706\n",
      "Epoch 138, Loss: 0.4510751265462795\n",
      "Epoch 139, Loss: 0.45082650445054473\n",
      "Epoch 140, Loss: 0.4506281098405297\n",
      "Epoch 141, Loss: 0.4501911419656675\n",
      "Epoch 142, Loss: 0.4497001887659326\n",
      "Epoch 143, Loss: 0.44943436133718806\n",
      "Epoch 144, Loss: 0.449226296402944\n",
      "Epoch 145, Loss: 0.44886073897867573\n",
      "Epoch 146, Loss: 0.44844710376304864\n",
      "Epoch 147, Loss: 0.44810507013116724\n",
      "Epoch 148, Loss: 0.44788545862049767\n",
      "Epoch 149, Loss: 0.4476741441009651\n",
      "Epoch 150, Loss: 0.44735861731902055\n",
      "Epoch 151, Loss: 0.4469288720157105\n",
      "Epoch 152, Loss: 0.44655235754178146\n",
      "Epoch 153, Loss: 0.4462857402786373\n",
      "Epoch 154, Loss: 0.4460517369142754\n",
      "Epoch 155, Loss: 0.4457881796184935\n",
      "Epoch 156, Loss: 0.44542210886348255\n",
      "Epoch 157, Loss: 0.4450579467827662\n",
      "Epoch 158, Loss: 0.44473329320345206\n",
      "Epoch 159, Loss: 0.4444482514678638\n",
      "Epoch 160, Loss: 0.4441745674630603\n",
      "Epoch 161, Loss: 0.4439221813722748\n",
      "Epoch 162, Loss: 0.44363357870363757\n",
      "Epoch 163, Loss: 0.4433189071134186\n",
      "Epoch 164, Loss: 0.44295452976444744\n",
      "Epoch 165, Loss: 0.4426058108229731\n",
      "Epoch 166, Loss: 0.4422485787900922\n",
      "Epoch 167, Loss: 0.4419090985167976\n",
      "Epoch 168, Loss: 0.44158054387831963\n",
      "Epoch 169, Loss: 0.4412474388772268\n",
      "Epoch 170, Loss: 0.4409277694136744\n",
      "Epoch 171, Loss: 0.44068045571539705\n",
      "Epoch 172, Loss: 0.4405979418732049\n",
      "Epoch 173, Loss: 0.44094101686109266\n",
      "Epoch 174, Loss: 0.4417368233339534\n",
      "Epoch 175, Loss: 0.4416582519101522\n",
      "Epoch 176, Loss: 0.4396788970821407\n",
      "Epoch 177, Loss: 0.4390195954561405\n",
      "Epoch 178, Loss: 0.4400830988007806\n",
      "Epoch 179, Loss: 0.43931775869233325\n",
      "Epoch 180, Loss: 0.4380032505977321\n",
      "Epoch 181, Loss: 0.4384470974340077\n",
      "Epoch 182, Loss: 0.4385167636581309\n",
      "Epoch 183, Loss: 0.43733043450040754\n",
      "Epoch 184, Loss: 0.4373176399006318\n",
      "Epoch 185, Loss: 0.4375256973621567\n",
      "Epoch 186, Loss: 0.43659322296038683\n",
      "Epoch 187, Loss: 0.43644391045908093\n",
      "Epoch 188, Loss: 0.43663844358015674\n",
      "Epoch 189, Loss: 0.43586286705793914\n",
      "Epoch 190, Loss: 0.4355663072844155\n",
      "Epoch 191, Loss: 0.4357040532214924\n",
      "Epoch 192, Loss: 0.435134007760397\n",
      "Epoch 193, Loss: 0.43474241127906293\n",
      "Epoch 194, Loss: 0.43480482310319635\n",
      "Epoch 195, Loss: 0.43443826104982264\n",
      "Epoch 196, Loss: 0.43390767873021246\n",
      "Epoch 197, Loss: 0.4338897691233524\n",
      "Epoch 198, Loss: 0.43377856877811044\n",
      "Epoch 199, Loss: 0.4331595506415052\n",
      "Epoch 200, Loss: 0.43289415091975497\n",
      "Epoch 201, Loss: 0.43294048069736957\n",
      "Epoch 202, Loss: 0.43262240576772243\n",
      "Epoch 203, Loss: 0.43206541812705346\n",
      "Epoch 204, Loss: 0.43172500614102416\n",
      "Epoch 205, Loss: 0.4316686388133938\n",
      "Epoch 206, Loss: 0.4315179158466364\n",
      "Epoch 207, Loss: 0.4310712381632941\n",
      "Epoch 208, Loss: 0.43059470035496\n",
      "Epoch 209, Loss: 0.4303711221416647\n",
      "Epoch 210, Loss: 0.43033088719595897\n",
      "Epoch 211, Loss: 0.43016580489992484\n",
      "Epoch 212, Loss: 0.429691817522032\n",
      "Epoch 213, Loss: 0.4292099292794459\n",
      "Epoch 214, Loss: 0.4289207205910941\n",
      "Epoch 215, Loss: 0.4287978238495365\n",
      "Epoch 216, Loss: 0.42870603658529494\n",
      "Epoch 217, Loss: 0.42841980940051866\n",
      "Epoch 218, Loss: 0.4278918633922788\n",
      "Epoch 219, Loss: 0.42748307113702383\n",
      "Epoch 220, Loss: 0.4271688511370021\n",
      "Epoch 221, Loss: 0.4269432345934848\n",
      "Epoch 222, Loss: 0.4267651275776002\n",
      "Epoch 223, Loss: 0.42668376428587257\n",
      "Epoch 224, Loss: 0.42653554195413756\n",
      "Epoch 225, Loss: 0.42613293183996703\n",
      "Epoch 226, Loss: 0.4255451632780942\n",
      "Epoch 227, Loss: 0.4251305043179084\n",
      "Epoch 228, Loss: 0.42475934718412794\n",
      "Epoch 229, Loss: 0.42452283256309487\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.24216062911420672\n",
      "Valence RMSE: 0.2552903435637103\n",
      "Arousal RMSE: 0.22827698322235546\n",
      "Test R^2 score: tensor([0.3036, 0.6157], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.45962475088133714\n",
      "Num of epochs: 230\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Epoch 121, Loss: 0.4596922103619757\n",
      "Epoch 122, Loss: 0.45943389956138125\n",
      "Epoch 123, Loss: 0.45877818894117006\n",
      "Epoch 124, Loss: 0.45746538132874076\n",
      "Epoch 125, Loss: 0.4570462803121619\n",
      "Epoch 126, Loss: 0.45696189549769234\n",
      "Epoch 127, Loss: 0.45594920341421274\n",
      "Epoch 128, Loss: 0.45536359747685007\n",
      "Epoch 129, Loss: 0.4553290890752543\n",
      "Epoch 130, Loss: 0.4547195823074053\n",
      "Epoch 131, Loss: 0.45392838323998064\n",
      "Epoch 132, Loss: 0.4537853493595631\n",
      "Epoch 133, Loss: 0.45347437050767975\n",
      "Epoch 134, Loss: 0.45267431273812453\n",
      "Epoch 135, Loss: 0.45228737659373064\n",
      "Epoch 136, Loss: 0.4522154657432981\n",
      "Epoch 137, Loss: 0.45165964487407706\n",
      "Epoch 138, Loss: 0.4510751265462795\n",
      "Epoch 139, Loss: 0.45082650445054473\n",
      "Epoch 140, Loss: 0.4506281098405297\n",
      "Epoch 141, Loss: 0.4501911419656675\n",
      "Epoch 142, Loss: 0.4497001887659326\n",
      "Epoch 143, Loss: 0.44943436133718806\n",
      "Epoch 144, Loss: 0.449226296402944\n",
      "Epoch 145, Loss: 0.44886073897867573\n",
      "Epoch 146, Loss: 0.44844710376304864\n",
      "Epoch 147, Loss: 0.44810507013116724\n",
      "Epoch 148, Loss: 0.44788545862049767\n",
      "Epoch 149, Loss: 0.4476741441009651\n",
      "Epoch 150, Loss: 0.44735861731902055\n",
      "Epoch 151, Loss: 0.4469288720157105\n",
      "Epoch 152, Loss: 0.44655235754178146\n",
      "Epoch 153, Loss: 0.4462857402786373\n",
      "Epoch 154, Loss: 0.4460517369142754\n",
      "Epoch 155, Loss: 0.4457881796184935\n",
      "Epoch 156, Loss: 0.44542210886348255\n",
      "Epoch 157, Loss: 0.4450579467827662\n",
      "Epoch 158, Loss: 0.44473329320345206\n",
      "Epoch 159, Loss: 0.4444482514678638\n",
      "Epoch 160, Loss: 0.4441745674630603\n",
      "Epoch 161, Loss: 0.4439221813722748\n",
      "Epoch 162, Loss: 0.44363357870363757\n",
      "Epoch 163, Loss: 0.4433189071134186\n",
      "Epoch 164, Loss: 0.44295452976444744\n",
      "Epoch 165, Loss: 0.4426058108229731\n",
      "Epoch 166, Loss: 0.4422485787900922\n",
      "Epoch 167, Loss: 0.4419090985167976\n",
      "Epoch 168, Loss: 0.44158054387831963\n",
      "Epoch 169, Loss: 0.4412474388772268\n",
      "Epoch 170, Loss: 0.4409277694136744\n",
      "Epoch 171, Loss: 0.44068045571539705\n",
      "Epoch 172, Loss: 0.4405979418732049\n",
      "Epoch 173, Loss: 0.44094101686109266\n",
      "Epoch 174, Loss: 0.4417368233339534\n",
      "Epoch 175, Loss: 0.4416582519101522\n",
      "Epoch 176, Loss: 0.4396788970821407\n",
      "Epoch 177, Loss: 0.4390195954561405\n",
      "Epoch 178, Loss: 0.4400830988007806\n",
      "Epoch 179, Loss: 0.43931775869233325\n",
      "Epoch 180, Loss: 0.4380032505977321\n",
      "Epoch 181, Loss: 0.4384470974340077\n",
      "Epoch 182, Loss: 0.4385167636581309\n",
      "Epoch 183, Loss: 0.43733043450040754\n",
      "Epoch 184, Loss: 0.4373176399006318\n",
      "Epoch 185, Loss: 0.4375256973621567\n",
      "Epoch 186, Loss: 0.43659322296038683\n",
      "Epoch 187, Loss: 0.43644391045908093\n",
      "Epoch 188, Loss: 0.43663844358015674\n",
      "Epoch 189, Loss: 0.43586286705793914\n",
      "Epoch 190, Loss: 0.4355663072844155\n",
      "Epoch 191, Loss: 0.4357040532214924\n",
      "Epoch 192, Loss: 0.435134007760397\n",
      "Epoch 193, Loss: 0.43474241127906293\n",
      "Epoch 194, Loss: 0.43480482310319635\n",
      "Epoch 195, Loss: 0.43443826104982264\n",
      "Epoch 196, Loss: 0.43390767873021246\n",
      "Epoch 197, Loss: 0.4338897691233524\n",
      "Epoch 198, Loss: 0.43377856877811044\n",
      "Epoch 199, Loss: 0.4331595506415052\n",
      "Epoch 200, Loss: 0.43289415091975497\n",
      "Epoch 201, Loss: 0.43294048069736957\n",
      "Epoch 202, Loss: 0.43262240576772243\n",
      "Epoch 203, Loss: 0.43206541812705346\n",
      "Epoch 204, Loss: 0.43172500614102416\n",
      "Epoch 205, Loss: 0.4316686388133938\n",
      "Epoch 206, Loss: 0.4315179158466364\n",
      "Epoch 207, Loss: 0.4310712381632941\n",
      "Epoch 208, Loss: 0.43059470035496\n",
      "Epoch 209, Loss: 0.4303711221416647\n",
      "Epoch 210, Loss: 0.43033088719595897\n",
      "Epoch 211, Loss: 0.43016580489992484\n",
      "Epoch 212, Loss: 0.429691817522032\n",
      "Epoch 213, Loss: 0.4292099292794459\n",
      "Epoch 214, Loss: 0.4289207205910941\n",
      "Epoch 215, Loss: 0.4287978238495365\n",
      "Epoch 216, Loss: 0.42870603658529494\n",
      "Epoch 217, Loss: 0.42841980940051866\n",
      "Epoch 218, Loss: 0.4278918633922788\n",
      "Epoch 219, Loss: 0.42748307113702383\n",
      "Epoch 220, Loss: 0.4271688511370021\n",
      "Epoch 221, Loss: 0.4269432345934848\n",
      "Epoch 222, Loss: 0.4267651275776002\n",
      "Epoch 223, Loss: 0.42668376428587257\n",
      "Epoch 224, Loss: 0.42653554195413756\n",
      "Epoch 225, Loss: 0.42613293183996703\n",
      "Epoch 226, Loss: 0.4255451632780942\n",
      "Epoch 227, Loss: 0.4251305043179084\n",
      "Epoch 228, Loss: 0.42475934718412794\n",
      "Epoch 229, Loss: 0.42452283256309487\n",
      "Epoch 230, Loss: 0.4244024016195508\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.24142627646177148\n",
      "Valence RMSE: 0.25367004104675805\n",
      "Arousal RMSE: 0.22852746926294765\n",
      "Test R^2 score: tensor([0.3124, 0.6148], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.46360901722086295\n",
      "Num of epochs: 231\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Epoch 121, Loss: 0.4596922103619757\n",
      "Epoch 122, Loss: 0.45943389956138125\n",
      "Epoch 123, Loss: 0.45877818894117006\n",
      "Epoch 124, Loss: 0.45746538132874076\n",
      "Epoch 125, Loss: 0.4570462803121619\n",
      "Epoch 126, Loss: 0.45696189549769234\n",
      "Epoch 127, Loss: 0.45594920341421274\n",
      "Epoch 128, Loss: 0.45536359747685007\n",
      "Epoch 129, Loss: 0.4553290890752543\n",
      "Epoch 130, Loss: 0.4547195823074053\n",
      "Epoch 131, Loss: 0.45392838323998064\n",
      "Epoch 132, Loss: 0.4537853493595631\n",
      "Epoch 133, Loss: 0.45347437050767975\n",
      "Epoch 134, Loss: 0.45267431273812453\n",
      "Epoch 135, Loss: 0.45228737659373064\n",
      "Epoch 136, Loss: 0.4522154657432981\n",
      "Epoch 137, Loss: 0.45165964487407706\n",
      "Epoch 138, Loss: 0.4510751265462795\n",
      "Epoch 139, Loss: 0.45082650445054473\n",
      "Epoch 140, Loss: 0.4506281098405297\n",
      "Epoch 141, Loss: 0.4501911419656675\n",
      "Epoch 142, Loss: 0.4497001887659326\n",
      "Epoch 143, Loss: 0.44943436133718806\n",
      "Epoch 144, Loss: 0.449226296402944\n",
      "Epoch 145, Loss: 0.44886073897867573\n",
      "Epoch 146, Loss: 0.44844710376304864\n",
      "Epoch 147, Loss: 0.44810507013116724\n",
      "Epoch 148, Loss: 0.44788545862049767\n",
      "Epoch 149, Loss: 0.4476741441009651\n",
      "Epoch 150, Loss: 0.44735861731902055\n",
      "Epoch 151, Loss: 0.4469288720157105\n",
      "Epoch 152, Loss: 0.44655235754178146\n",
      "Epoch 153, Loss: 0.4462857402786373\n",
      "Epoch 154, Loss: 0.4460517369142754\n",
      "Epoch 155, Loss: 0.4457881796184935\n",
      "Epoch 156, Loss: 0.44542210886348255\n",
      "Epoch 157, Loss: 0.4450579467827662\n",
      "Epoch 158, Loss: 0.44473329320345206\n",
      "Epoch 159, Loss: 0.4444482514678638\n",
      "Epoch 160, Loss: 0.4441745674630603\n",
      "Epoch 161, Loss: 0.4439221813722748\n",
      "Epoch 162, Loss: 0.44363357870363757\n",
      "Epoch 163, Loss: 0.4433189071134186\n",
      "Epoch 164, Loss: 0.44295452976444744\n",
      "Epoch 165, Loss: 0.4426058108229731\n",
      "Epoch 166, Loss: 0.4422485787900922\n",
      "Epoch 167, Loss: 0.4419090985167976\n",
      "Epoch 168, Loss: 0.44158054387831963\n",
      "Epoch 169, Loss: 0.4412474388772268\n",
      "Epoch 170, Loss: 0.4409277694136744\n",
      "Epoch 171, Loss: 0.44068045571539705\n",
      "Epoch 172, Loss: 0.4405979418732049\n",
      "Epoch 173, Loss: 0.44094101686109266\n",
      "Epoch 174, Loss: 0.4417368233339534\n",
      "Epoch 175, Loss: 0.4416582519101522\n",
      "Epoch 176, Loss: 0.4396788970821407\n",
      "Epoch 177, Loss: 0.4390195954561405\n",
      "Epoch 178, Loss: 0.4400830988007806\n",
      "Epoch 179, Loss: 0.43931775869233325\n",
      "Epoch 180, Loss: 0.4380032505977321\n",
      "Epoch 181, Loss: 0.4384470974340077\n",
      "Epoch 182, Loss: 0.4385167636581309\n",
      "Epoch 183, Loss: 0.43733043450040754\n",
      "Epoch 184, Loss: 0.4373176399006318\n",
      "Epoch 185, Loss: 0.4375256973621567\n",
      "Epoch 186, Loss: 0.43659322296038683\n",
      "Epoch 187, Loss: 0.43644391045908093\n",
      "Epoch 188, Loss: 0.43663844358015674\n",
      "Epoch 189, Loss: 0.43586286705793914\n",
      "Epoch 190, Loss: 0.4355663072844155\n",
      "Epoch 191, Loss: 0.4357040532214924\n",
      "Epoch 192, Loss: 0.435134007760397\n",
      "Epoch 193, Loss: 0.43474241127906293\n",
      "Epoch 194, Loss: 0.43480482310319635\n",
      "Epoch 195, Loss: 0.43443826104982264\n",
      "Epoch 196, Loss: 0.43390767873021246\n",
      "Epoch 197, Loss: 0.4338897691233524\n",
      "Epoch 198, Loss: 0.43377856877811044\n",
      "Epoch 199, Loss: 0.4331595506415052\n",
      "Epoch 200, Loss: 0.43289415091975497\n",
      "Epoch 201, Loss: 0.43294048069736957\n",
      "Epoch 202, Loss: 0.43262240576772243\n",
      "Epoch 203, Loss: 0.43206541812705346\n",
      "Epoch 204, Loss: 0.43172500614102416\n",
      "Epoch 205, Loss: 0.4316686388133938\n",
      "Epoch 206, Loss: 0.4315179158466364\n",
      "Epoch 207, Loss: 0.4310712381632941\n",
      "Epoch 208, Loss: 0.43059470035496\n",
      "Epoch 209, Loss: 0.4303711221416647\n",
      "Epoch 210, Loss: 0.43033088719595897\n",
      "Epoch 211, Loss: 0.43016580489992484\n",
      "Epoch 212, Loss: 0.429691817522032\n",
      "Epoch 213, Loss: 0.4292099292794459\n",
      "Epoch 214, Loss: 0.4289207205910941\n",
      "Epoch 215, Loss: 0.4287978238495365\n",
      "Epoch 216, Loss: 0.42870603658529494\n",
      "Epoch 217, Loss: 0.42841980940051866\n",
      "Epoch 218, Loss: 0.4278918633922788\n",
      "Epoch 219, Loss: 0.42748307113702383\n",
      "Epoch 220, Loss: 0.4271688511370021\n",
      "Epoch 221, Loss: 0.4269432345934848\n",
      "Epoch 222, Loss: 0.4267651275776002\n",
      "Epoch 223, Loss: 0.42668376428587257\n",
      "Epoch 224, Loss: 0.42653554195413756\n",
      "Epoch 225, Loss: 0.42613293183996703\n",
      "Epoch 226, Loss: 0.4255451632780942\n",
      "Epoch 227, Loss: 0.4251305043179084\n",
      "Epoch 228, Loss: 0.42475934718412794\n",
      "Epoch 229, Loss: 0.42452283256309487\n",
      "Epoch 230, Loss: 0.4244024016195508\n",
      "Epoch 231, Loss: 0.4242604946441205\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.24208278318853538\n",
      "Valence RMSE: 0.2549554548154588\n",
      "Arousal RMSE: 0.2284860255955654\n",
      "Test R^2 score: tensor([0.3054, 0.6150], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4601856457809304\n",
      "Num of epochs: 232\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Epoch 121, Loss: 0.4596922103619757\n",
      "Epoch 122, Loss: 0.45943389956138125\n",
      "Epoch 123, Loss: 0.45877818894117006\n",
      "Epoch 124, Loss: 0.45746538132874076\n",
      "Epoch 125, Loss: 0.4570462803121619\n",
      "Epoch 126, Loss: 0.45696189549769234\n",
      "Epoch 127, Loss: 0.45594920341421274\n",
      "Epoch 128, Loss: 0.45536359747685007\n",
      "Epoch 129, Loss: 0.4553290890752543\n",
      "Epoch 130, Loss: 0.4547195823074053\n",
      "Epoch 131, Loss: 0.45392838323998064\n",
      "Epoch 132, Loss: 0.4537853493595631\n",
      "Epoch 133, Loss: 0.45347437050767975\n",
      "Epoch 134, Loss: 0.45267431273812453\n",
      "Epoch 135, Loss: 0.45228737659373064\n",
      "Epoch 136, Loss: 0.4522154657432981\n",
      "Epoch 137, Loss: 0.45165964487407706\n",
      "Epoch 138, Loss: 0.4510751265462795\n",
      "Epoch 139, Loss: 0.45082650445054473\n",
      "Epoch 140, Loss: 0.4506281098405297\n",
      "Epoch 141, Loss: 0.4501911419656675\n",
      "Epoch 142, Loss: 0.4497001887659326\n",
      "Epoch 143, Loss: 0.44943436133718806\n",
      "Epoch 144, Loss: 0.449226296402944\n",
      "Epoch 145, Loss: 0.44886073897867573\n",
      "Epoch 146, Loss: 0.44844710376304864\n",
      "Epoch 147, Loss: 0.44810507013116724\n",
      "Epoch 148, Loss: 0.44788545862049767\n",
      "Epoch 149, Loss: 0.4476741441009651\n",
      "Epoch 150, Loss: 0.44735861731902055\n",
      "Epoch 151, Loss: 0.4469288720157105\n",
      "Epoch 152, Loss: 0.44655235754178146\n",
      "Epoch 153, Loss: 0.4462857402786373\n",
      "Epoch 154, Loss: 0.4460517369142754\n",
      "Epoch 155, Loss: 0.4457881796184935\n",
      "Epoch 156, Loss: 0.44542210886348255\n",
      "Epoch 157, Loss: 0.4450579467827662\n",
      "Epoch 158, Loss: 0.44473329320345206\n",
      "Epoch 159, Loss: 0.4444482514678638\n",
      "Epoch 160, Loss: 0.4441745674630603\n",
      "Epoch 161, Loss: 0.4439221813722748\n",
      "Epoch 162, Loss: 0.44363357870363757\n",
      "Epoch 163, Loss: 0.4433189071134186\n",
      "Epoch 164, Loss: 0.44295452976444744\n",
      "Epoch 165, Loss: 0.4426058108229731\n",
      "Epoch 166, Loss: 0.4422485787900922\n",
      "Epoch 167, Loss: 0.4419090985167976\n",
      "Epoch 168, Loss: 0.44158054387831963\n",
      "Epoch 169, Loss: 0.4412474388772268\n",
      "Epoch 170, Loss: 0.4409277694136744\n",
      "Epoch 171, Loss: 0.44068045571539705\n",
      "Epoch 172, Loss: 0.4405979418732049\n",
      "Epoch 173, Loss: 0.44094101686109266\n",
      "Epoch 174, Loss: 0.4417368233339534\n",
      "Epoch 175, Loss: 0.4416582519101522\n",
      "Epoch 176, Loss: 0.4396788970821407\n",
      "Epoch 177, Loss: 0.4390195954561405\n",
      "Epoch 178, Loss: 0.4400830988007806\n",
      "Epoch 179, Loss: 0.43931775869233325\n",
      "Epoch 180, Loss: 0.4380032505977321\n",
      "Epoch 181, Loss: 0.4384470974340077\n",
      "Epoch 182, Loss: 0.4385167636581309\n",
      "Epoch 183, Loss: 0.43733043450040754\n",
      "Epoch 184, Loss: 0.4373176399006318\n",
      "Epoch 185, Loss: 0.4375256973621567\n",
      "Epoch 186, Loss: 0.43659322296038683\n",
      "Epoch 187, Loss: 0.43644391045908093\n",
      "Epoch 188, Loss: 0.43663844358015674\n",
      "Epoch 189, Loss: 0.43586286705793914\n",
      "Epoch 190, Loss: 0.4355663072844155\n",
      "Epoch 191, Loss: 0.4357040532214924\n",
      "Epoch 192, Loss: 0.435134007760397\n",
      "Epoch 193, Loss: 0.43474241127906293\n",
      "Epoch 194, Loss: 0.43480482310319635\n",
      "Epoch 195, Loss: 0.43443826104982264\n",
      "Epoch 196, Loss: 0.43390767873021246\n",
      "Epoch 197, Loss: 0.4338897691233524\n",
      "Epoch 198, Loss: 0.43377856877811044\n",
      "Epoch 199, Loss: 0.4331595506415052\n",
      "Epoch 200, Loss: 0.43289415091975497\n",
      "Epoch 201, Loss: 0.43294048069736957\n",
      "Epoch 202, Loss: 0.43262240576772243\n",
      "Epoch 203, Loss: 0.43206541812705346\n",
      "Epoch 204, Loss: 0.43172500614102416\n",
      "Epoch 205, Loss: 0.4316686388133938\n",
      "Epoch 206, Loss: 0.4315179158466364\n",
      "Epoch 207, Loss: 0.4310712381632941\n",
      "Epoch 208, Loss: 0.43059470035496\n",
      "Epoch 209, Loss: 0.4303711221416647\n",
      "Epoch 210, Loss: 0.43033088719595897\n",
      "Epoch 211, Loss: 0.43016580489992484\n",
      "Epoch 212, Loss: 0.429691817522032\n",
      "Epoch 213, Loss: 0.4292099292794459\n",
      "Epoch 214, Loss: 0.4289207205910941\n",
      "Epoch 215, Loss: 0.4287978238495365\n",
      "Epoch 216, Loss: 0.42870603658529494\n",
      "Epoch 217, Loss: 0.42841980940051866\n",
      "Epoch 218, Loss: 0.4278918633922788\n",
      "Epoch 219, Loss: 0.42748307113702383\n",
      "Epoch 220, Loss: 0.4271688511370021\n",
      "Epoch 221, Loss: 0.4269432345934848\n",
      "Epoch 222, Loss: 0.4267651275776002\n",
      "Epoch 223, Loss: 0.42668376428587257\n",
      "Epoch 224, Loss: 0.42653554195413756\n",
      "Epoch 225, Loss: 0.42613293183996703\n",
      "Epoch 226, Loss: 0.4255451632780942\n",
      "Epoch 227, Loss: 0.4251305043179084\n",
      "Epoch 228, Loss: 0.42475934718412794\n",
      "Epoch 229, Loss: 0.42452283256309487\n",
      "Epoch 230, Loss: 0.4244024016195508\n",
      "Epoch 231, Loss: 0.4242604946441205\n",
      "Epoch 232, Loss: 0.424134367950937\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2415089129089682\n",
      "Valence RMSE: 0.2535866170589545\n",
      "Arousal RMSE: 0.22879453157262813\n",
      "Test R^2 score: tensor([0.3128, 0.6139], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.46338475370462345\n",
      "Num of epochs: 233\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Epoch 121, Loss: 0.4596922103619757\n",
      "Epoch 122, Loss: 0.45943389956138125\n",
      "Epoch 123, Loss: 0.45877818894117006\n",
      "Epoch 124, Loss: 0.45746538132874076\n",
      "Epoch 125, Loss: 0.4570462803121619\n",
      "Epoch 126, Loss: 0.45696189549769234\n",
      "Epoch 127, Loss: 0.45594920341421274\n",
      "Epoch 128, Loss: 0.45536359747685007\n",
      "Epoch 129, Loss: 0.4553290890752543\n",
      "Epoch 130, Loss: 0.4547195823074053\n",
      "Epoch 131, Loss: 0.45392838323998064\n",
      "Epoch 132, Loss: 0.4537853493595631\n",
      "Epoch 133, Loss: 0.45347437050767975\n",
      "Epoch 134, Loss: 0.45267431273812453\n",
      "Epoch 135, Loss: 0.45228737659373064\n",
      "Epoch 136, Loss: 0.4522154657432981\n",
      "Epoch 137, Loss: 0.45165964487407706\n",
      "Epoch 138, Loss: 0.4510751265462795\n",
      "Epoch 139, Loss: 0.45082650445054473\n",
      "Epoch 140, Loss: 0.4506281098405297\n",
      "Epoch 141, Loss: 0.4501911419656675\n",
      "Epoch 142, Loss: 0.4497001887659326\n",
      "Epoch 143, Loss: 0.44943436133718806\n",
      "Epoch 144, Loss: 0.449226296402944\n",
      "Epoch 145, Loss: 0.44886073897867573\n",
      "Epoch 146, Loss: 0.44844710376304864\n",
      "Epoch 147, Loss: 0.44810507013116724\n",
      "Epoch 148, Loss: 0.44788545862049767\n",
      "Epoch 149, Loss: 0.4476741441009651\n",
      "Epoch 150, Loss: 0.44735861731902055\n",
      "Epoch 151, Loss: 0.4469288720157105\n",
      "Epoch 152, Loss: 0.44655235754178146\n",
      "Epoch 153, Loss: 0.4462857402786373\n",
      "Epoch 154, Loss: 0.4460517369142754\n",
      "Epoch 155, Loss: 0.4457881796184935\n",
      "Epoch 156, Loss: 0.44542210886348255\n",
      "Epoch 157, Loss: 0.4450579467827662\n",
      "Epoch 158, Loss: 0.44473329320345206\n",
      "Epoch 159, Loss: 0.4444482514678638\n",
      "Epoch 160, Loss: 0.4441745674630603\n",
      "Epoch 161, Loss: 0.4439221813722748\n",
      "Epoch 162, Loss: 0.44363357870363757\n",
      "Epoch 163, Loss: 0.4433189071134186\n",
      "Epoch 164, Loss: 0.44295452976444744\n",
      "Epoch 165, Loss: 0.4426058108229731\n",
      "Epoch 166, Loss: 0.4422485787900922\n",
      "Epoch 167, Loss: 0.4419090985167976\n",
      "Epoch 168, Loss: 0.44158054387831963\n",
      "Epoch 169, Loss: 0.4412474388772268\n",
      "Epoch 170, Loss: 0.4409277694136744\n",
      "Epoch 171, Loss: 0.44068045571539705\n",
      "Epoch 172, Loss: 0.4405979418732049\n",
      "Epoch 173, Loss: 0.44094101686109266\n",
      "Epoch 174, Loss: 0.4417368233339534\n",
      "Epoch 175, Loss: 0.4416582519101522\n",
      "Epoch 176, Loss: 0.4396788970821407\n",
      "Epoch 177, Loss: 0.4390195954561405\n",
      "Epoch 178, Loss: 0.4400830988007806\n",
      "Epoch 179, Loss: 0.43931775869233325\n",
      "Epoch 180, Loss: 0.4380032505977321\n",
      "Epoch 181, Loss: 0.4384470974340077\n",
      "Epoch 182, Loss: 0.4385167636581309\n",
      "Epoch 183, Loss: 0.43733043450040754\n",
      "Epoch 184, Loss: 0.4373176399006318\n",
      "Epoch 185, Loss: 0.4375256973621567\n",
      "Epoch 186, Loss: 0.43659322296038683\n",
      "Epoch 187, Loss: 0.43644391045908093\n",
      "Epoch 188, Loss: 0.43663844358015674\n",
      "Epoch 189, Loss: 0.43586286705793914\n",
      "Epoch 190, Loss: 0.4355663072844155\n",
      "Epoch 191, Loss: 0.4357040532214924\n",
      "Epoch 192, Loss: 0.435134007760397\n",
      "Epoch 193, Loss: 0.43474241127906293\n",
      "Epoch 194, Loss: 0.43480482310319635\n",
      "Epoch 195, Loss: 0.43443826104982264\n",
      "Epoch 196, Loss: 0.43390767873021246\n",
      "Epoch 197, Loss: 0.4338897691233524\n",
      "Epoch 198, Loss: 0.43377856877811044\n",
      "Epoch 199, Loss: 0.4331595506415052\n",
      "Epoch 200, Loss: 0.43289415091975497\n",
      "Epoch 201, Loss: 0.43294048069736957\n",
      "Epoch 202, Loss: 0.43262240576772243\n",
      "Epoch 203, Loss: 0.43206541812705346\n",
      "Epoch 204, Loss: 0.43172500614102416\n",
      "Epoch 205, Loss: 0.4316686388133938\n",
      "Epoch 206, Loss: 0.4315179158466364\n",
      "Epoch 207, Loss: 0.4310712381632941\n",
      "Epoch 208, Loss: 0.43059470035496\n",
      "Epoch 209, Loss: 0.4303711221416647\n",
      "Epoch 210, Loss: 0.43033088719595897\n",
      "Epoch 211, Loss: 0.43016580489992484\n",
      "Epoch 212, Loss: 0.429691817522032\n",
      "Epoch 213, Loss: 0.4292099292794459\n",
      "Epoch 214, Loss: 0.4289207205910941\n",
      "Epoch 215, Loss: 0.4287978238495365\n",
      "Epoch 216, Loss: 0.42870603658529494\n",
      "Epoch 217, Loss: 0.42841980940051866\n",
      "Epoch 218, Loss: 0.4278918633922788\n",
      "Epoch 219, Loss: 0.42748307113702383\n",
      "Epoch 220, Loss: 0.4271688511370021\n",
      "Epoch 221, Loss: 0.4269432345934848\n",
      "Epoch 222, Loss: 0.4267651275776002\n",
      "Epoch 223, Loss: 0.42668376428587257\n",
      "Epoch 224, Loss: 0.42653554195413756\n",
      "Epoch 225, Loss: 0.42613293183996703\n",
      "Epoch 226, Loss: 0.4255451632780942\n",
      "Epoch 227, Loss: 0.4251305043179084\n",
      "Epoch 228, Loss: 0.42475934718412794\n",
      "Epoch 229, Loss: 0.42452283256309487\n",
      "Epoch 230, Loss: 0.4244024016195508\n",
      "Epoch 231, Loss: 0.4242604946441205\n",
      "Epoch 232, Loss: 0.424134367950937\n",
      "Epoch 233, Loss: 0.42372608274856827\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.24232632383575078\n",
      "Valence RMSE: 0.2550437212695008\n",
      "Arousal RMSE: 0.2289034615039788\n",
      "Test R^2 score: tensor([0.3049, 0.6136], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.45924107350061777\n",
      "Num of epochs: 234\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Epoch 121, Loss: 0.4596922103619757\n",
      "Epoch 122, Loss: 0.45943389956138125\n",
      "Epoch 123, Loss: 0.45877818894117006\n",
      "Epoch 124, Loss: 0.45746538132874076\n",
      "Epoch 125, Loss: 0.4570462803121619\n",
      "Epoch 126, Loss: 0.45696189549769234\n",
      "Epoch 127, Loss: 0.45594920341421274\n",
      "Epoch 128, Loss: 0.45536359747685007\n",
      "Epoch 129, Loss: 0.4553290890752543\n",
      "Epoch 130, Loss: 0.4547195823074053\n",
      "Epoch 131, Loss: 0.45392838323998064\n",
      "Epoch 132, Loss: 0.4537853493595631\n",
      "Epoch 133, Loss: 0.45347437050767975\n",
      "Epoch 134, Loss: 0.45267431273812453\n",
      "Epoch 135, Loss: 0.45228737659373064\n",
      "Epoch 136, Loss: 0.4522154657432981\n",
      "Epoch 137, Loss: 0.45165964487407706\n",
      "Epoch 138, Loss: 0.4510751265462795\n",
      "Epoch 139, Loss: 0.45082650445054473\n",
      "Epoch 140, Loss: 0.4506281098405297\n",
      "Epoch 141, Loss: 0.4501911419656675\n",
      "Epoch 142, Loss: 0.4497001887659326\n",
      "Epoch 143, Loss: 0.44943436133718806\n",
      "Epoch 144, Loss: 0.449226296402944\n",
      "Epoch 145, Loss: 0.44886073897867573\n",
      "Epoch 146, Loss: 0.44844710376304864\n",
      "Epoch 147, Loss: 0.44810507013116724\n",
      "Epoch 148, Loss: 0.44788545862049767\n",
      "Epoch 149, Loss: 0.4476741441009651\n",
      "Epoch 150, Loss: 0.44735861731902055\n",
      "Epoch 151, Loss: 0.4469288720157105\n",
      "Epoch 152, Loss: 0.44655235754178146\n",
      "Epoch 153, Loss: 0.4462857402786373\n",
      "Epoch 154, Loss: 0.4460517369142754\n",
      "Epoch 155, Loss: 0.4457881796184935\n",
      "Epoch 156, Loss: 0.44542210886348255\n",
      "Epoch 157, Loss: 0.4450579467827662\n",
      "Epoch 158, Loss: 0.44473329320345206\n",
      "Epoch 159, Loss: 0.4444482514678638\n",
      "Epoch 160, Loss: 0.4441745674630603\n",
      "Epoch 161, Loss: 0.4439221813722748\n",
      "Epoch 162, Loss: 0.44363357870363757\n",
      "Epoch 163, Loss: 0.4433189071134186\n",
      "Epoch 164, Loss: 0.44295452976444744\n",
      "Epoch 165, Loss: 0.4426058108229731\n",
      "Epoch 166, Loss: 0.4422485787900922\n",
      "Epoch 167, Loss: 0.4419090985167976\n",
      "Epoch 168, Loss: 0.44158054387831963\n",
      "Epoch 169, Loss: 0.4412474388772268\n",
      "Epoch 170, Loss: 0.4409277694136744\n",
      "Epoch 171, Loss: 0.44068045571539705\n",
      "Epoch 172, Loss: 0.4405979418732049\n",
      "Epoch 173, Loss: 0.44094101686109266\n",
      "Epoch 174, Loss: 0.4417368233339534\n",
      "Epoch 175, Loss: 0.4416582519101522\n",
      "Epoch 176, Loss: 0.4396788970821407\n",
      "Epoch 177, Loss: 0.4390195954561405\n",
      "Epoch 178, Loss: 0.4400830988007806\n",
      "Epoch 179, Loss: 0.43931775869233325\n",
      "Epoch 180, Loss: 0.4380032505977321\n",
      "Epoch 181, Loss: 0.4384470974340077\n",
      "Epoch 182, Loss: 0.4385167636581309\n",
      "Epoch 183, Loss: 0.43733043450040754\n",
      "Epoch 184, Loss: 0.4373176399006318\n",
      "Epoch 185, Loss: 0.4375256973621567\n",
      "Epoch 186, Loss: 0.43659322296038683\n",
      "Epoch 187, Loss: 0.43644391045908093\n",
      "Epoch 188, Loss: 0.43663844358015674\n",
      "Epoch 189, Loss: 0.43586286705793914\n",
      "Epoch 190, Loss: 0.4355663072844155\n",
      "Epoch 191, Loss: 0.4357040532214924\n",
      "Epoch 192, Loss: 0.435134007760397\n",
      "Epoch 193, Loss: 0.43474241127906293\n",
      "Epoch 194, Loss: 0.43480482310319635\n",
      "Epoch 195, Loss: 0.43443826104982264\n",
      "Epoch 196, Loss: 0.43390767873021246\n",
      "Epoch 197, Loss: 0.4338897691233524\n",
      "Epoch 198, Loss: 0.43377856877811044\n",
      "Epoch 199, Loss: 0.4331595506415052\n",
      "Epoch 200, Loss: 0.43289415091975497\n",
      "Epoch 201, Loss: 0.43294048069736957\n",
      "Epoch 202, Loss: 0.43262240576772243\n",
      "Epoch 203, Loss: 0.43206541812705346\n",
      "Epoch 204, Loss: 0.43172500614102416\n",
      "Epoch 205, Loss: 0.4316686388133938\n",
      "Epoch 206, Loss: 0.4315179158466364\n",
      "Epoch 207, Loss: 0.4310712381632941\n",
      "Epoch 208, Loss: 0.43059470035496\n",
      "Epoch 209, Loss: 0.4303711221416647\n",
      "Epoch 210, Loss: 0.43033088719595897\n",
      "Epoch 211, Loss: 0.43016580489992484\n",
      "Epoch 212, Loss: 0.429691817522032\n",
      "Epoch 213, Loss: 0.4292099292794459\n",
      "Epoch 214, Loss: 0.4289207205910941\n",
      "Epoch 215, Loss: 0.4287978238495365\n",
      "Epoch 216, Loss: 0.42870603658529494\n",
      "Epoch 217, Loss: 0.42841980940051866\n",
      "Epoch 218, Loss: 0.4278918633922788\n",
      "Epoch 219, Loss: 0.42748307113702383\n",
      "Epoch 220, Loss: 0.4271688511370021\n",
      "Epoch 221, Loss: 0.4269432345934848\n",
      "Epoch 222, Loss: 0.4267651275776002\n",
      "Epoch 223, Loss: 0.42668376428587257\n",
      "Epoch 224, Loss: 0.42653554195413756\n",
      "Epoch 225, Loss: 0.42613293183996703\n",
      "Epoch 226, Loss: 0.4255451632780942\n",
      "Epoch 227, Loss: 0.4251305043179084\n",
      "Epoch 228, Loss: 0.42475934718412794\n",
      "Epoch 229, Loss: 0.42452283256309487\n",
      "Epoch 230, Loss: 0.4244024016195508\n",
      "Epoch 231, Loss: 0.4242604946441205\n",
      "Epoch 232, Loss: 0.424134367950937\n",
      "Epoch 233, Loss: 0.42372608274856827\n",
      "Epoch 234, Loss: 0.4231492155631465\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.24234013000771704\n",
      "Valence RMSE: 0.25471417224241\n",
      "Arousal RMSE: 0.22929929717113892\n",
      "Test R^2 score: tensor([0.3067, 0.6122], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.45946983372811606\n",
      "Num of epochs: 235\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Epoch 121, Loss: 0.4596922103619757\n",
      "Epoch 122, Loss: 0.45943389956138125\n",
      "Epoch 123, Loss: 0.45877818894117006\n",
      "Epoch 124, Loss: 0.45746538132874076\n",
      "Epoch 125, Loss: 0.4570462803121619\n",
      "Epoch 126, Loss: 0.45696189549769234\n",
      "Epoch 127, Loss: 0.45594920341421274\n",
      "Epoch 128, Loss: 0.45536359747685007\n",
      "Epoch 129, Loss: 0.4553290890752543\n",
      "Epoch 130, Loss: 0.4547195823074053\n",
      "Epoch 131, Loss: 0.45392838323998064\n",
      "Epoch 132, Loss: 0.4537853493595631\n",
      "Epoch 133, Loss: 0.45347437050767975\n",
      "Epoch 134, Loss: 0.45267431273812453\n",
      "Epoch 135, Loss: 0.45228737659373064\n",
      "Epoch 136, Loss: 0.4522154657432981\n",
      "Epoch 137, Loss: 0.45165964487407706\n",
      "Epoch 138, Loss: 0.4510751265462795\n",
      "Epoch 139, Loss: 0.45082650445054473\n",
      "Epoch 140, Loss: 0.4506281098405297\n",
      "Epoch 141, Loss: 0.4501911419656675\n",
      "Epoch 142, Loss: 0.4497001887659326\n",
      "Epoch 143, Loss: 0.44943436133718806\n",
      "Epoch 144, Loss: 0.449226296402944\n",
      "Epoch 145, Loss: 0.44886073897867573\n",
      "Epoch 146, Loss: 0.44844710376304864\n",
      "Epoch 147, Loss: 0.44810507013116724\n",
      "Epoch 148, Loss: 0.44788545862049767\n",
      "Epoch 149, Loss: 0.4476741441009651\n",
      "Epoch 150, Loss: 0.44735861731902055\n",
      "Epoch 151, Loss: 0.4469288720157105\n",
      "Epoch 152, Loss: 0.44655235754178146\n",
      "Epoch 153, Loss: 0.4462857402786373\n",
      "Epoch 154, Loss: 0.4460517369142754\n",
      "Epoch 155, Loss: 0.4457881796184935\n",
      "Epoch 156, Loss: 0.44542210886348255\n",
      "Epoch 157, Loss: 0.4450579467827662\n",
      "Epoch 158, Loss: 0.44473329320345206\n",
      "Epoch 159, Loss: 0.4444482514678638\n",
      "Epoch 160, Loss: 0.4441745674630603\n",
      "Epoch 161, Loss: 0.4439221813722748\n",
      "Epoch 162, Loss: 0.44363357870363757\n",
      "Epoch 163, Loss: 0.4433189071134186\n",
      "Epoch 164, Loss: 0.44295452976444744\n",
      "Epoch 165, Loss: 0.4426058108229731\n",
      "Epoch 166, Loss: 0.4422485787900922\n",
      "Epoch 167, Loss: 0.4419090985167976\n",
      "Epoch 168, Loss: 0.44158054387831963\n",
      "Epoch 169, Loss: 0.4412474388772268\n",
      "Epoch 170, Loss: 0.4409277694136744\n",
      "Epoch 171, Loss: 0.44068045571539705\n",
      "Epoch 172, Loss: 0.4405979418732049\n",
      "Epoch 173, Loss: 0.44094101686109266\n",
      "Epoch 174, Loss: 0.4417368233339534\n",
      "Epoch 175, Loss: 0.4416582519101522\n",
      "Epoch 176, Loss: 0.4396788970821407\n",
      "Epoch 177, Loss: 0.4390195954561405\n",
      "Epoch 178, Loss: 0.4400830988007806\n",
      "Epoch 179, Loss: 0.43931775869233325\n",
      "Epoch 180, Loss: 0.4380032505977321\n",
      "Epoch 181, Loss: 0.4384470974340077\n",
      "Epoch 182, Loss: 0.4385167636581309\n",
      "Epoch 183, Loss: 0.43733043450040754\n",
      "Epoch 184, Loss: 0.4373176399006318\n",
      "Epoch 185, Loss: 0.4375256973621567\n",
      "Epoch 186, Loss: 0.43659322296038683\n",
      "Epoch 187, Loss: 0.43644391045908093\n",
      "Epoch 188, Loss: 0.43663844358015674\n",
      "Epoch 189, Loss: 0.43586286705793914\n",
      "Epoch 190, Loss: 0.4355663072844155\n",
      "Epoch 191, Loss: 0.4357040532214924\n",
      "Epoch 192, Loss: 0.435134007760397\n",
      "Epoch 193, Loss: 0.43474241127906293\n",
      "Epoch 194, Loss: 0.43480482310319635\n",
      "Epoch 195, Loss: 0.43443826104982264\n",
      "Epoch 196, Loss: 0.43390767873021246\n",
      "Epoch 197, Loss: 0.4338897691233524\n",
      "Epoch 198, Loss: 0.43377856877811044\n",
      "Epoch 199, Loss: 0.4331595506415052\n",
      "Epoch 200, Loss: 0.43289415091975497\n",
      "Epoch 201, Loss: 0.43294048069736957\n",
      "Epoch 202, Loss: 0.43262240576772243\n",
      "Epoch 203, Loss: 0.43206541812705346\n",
      "Epoch 204, Loss: 0.43172500614102416\n",
      "Epoch 205, Loss: 0.4316686388133938\n",
      "Epoch 206, Loss: 0.4315179158466364\n",
      "Epoch 207, Loss: 0.4310712381632941\n",
      "Epoch 208, Loss: 0.43059470035496\n",
      "Epoch 209, Loss: 0.4303711221416647\n",
      "Epoch 210, Loss: 0.43033088719595897\n",
      "Epoch 211, Loss: 0.43016580489992484\n",
      "Epoch 212, Loss: 0.429691817522032\n",
      "Epoch 213, Loss: 0.4292099292794459\n",
      "Epoch 214, Loss: 0.4289207205910941\n",
      "Epoch 215, Loss: 0.4287978238495365\n",
      "Epoch 216, Loss: 0.42870603658529494\n",
      "Epoch 217, Loss: 0.42841980940051866\n",
      "Epoch 218, Loss: 0.4278918633922788\n",
      "Epoch 219, Loss: 0.42748307113702383\n",
      "Epoch 220, Loss: 0.4271688511370021\n",
      "Epoch 221, Loss: 0.4269432345934848\n",
      "Epoch 222, Loss: 0.4267651275776002\n",
      "Epoch 223, Loss: 0.42668376428587257\n",
      "Epoch 224, Loss: 0.42653554195413756\n",
      "Epoch 225, Loss: 0.42613293183996703\n",
      "Epoch 226, Loss: 0.4255451632780942\n",
      "Epoch 227, Loss: 0.4251305043179084\n",
      "Epoch 228, Loss: 0.42475934718412794\n",
      "Epoch 229, Loss: 0.42452283256309487\n",
      "Epoch 230, Loss: 0.4244024016195508\n",
      "Epoch 231, Loss: 0.4242604946441205\n",
      "Epoch 232, Loss: 0.424134367950937\n",
      "Epoch 233, Loss: 0.42372608274856827\n",
      "Epoch 234, Loss: 0.4231492155631465\n",
      "Epoch 235, Loss: 0.4226291943984805\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.24250486203789107\n",
      "Valence RMSE: 0.25507034279703306\n",
      "Arousal RMSE: 0.22925168799692008\n",
      "Test R^2 score: tensor([0.3048, 0.6124], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.45858020870393945\n",
      "Num of epochs: 236\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Epoch 121, Loss: 0.4596922103619757\n",
      "Epoch 122, Loss: 0.45943389956138125\n",
      "Epoch 123, Loss: 0.45877818894117006\n",
      "Epoch 124, Loss: 0.45746538132874076\n",
      "Epoch 125, Loss: 0.4570462803121619\n",
      "Epoch 126, Loss: 0.45696189549769234\n",
      "Epoch 127, Loss: 0.45594920341421274\n",
      "Epoch 128, Loss: 0.45536359747685007\n",
      "Epoch 129, Loss: 0.4553290890752543\n",
      "Epoch 130, Loss: 0.4547195823074053\n",
      "Epoch 131, Loss: 0.45392838323998064\n",
      "Epoch 132, Loss: 0.4537853493595631\n",
      "Epoch 133, Loss: 0.45347437050767975\n",
      "Epoch 134, Loss: 0.45267431273812453\n",
      "Epoch 135, Loss: 0.45228737659373064\n",
      "Epoch 136, Loss: 0.4522154657432981\n",
      "Epoch 137, Loss: 0.45165964487407706\n",
      "Epoch 138, Loss: 0.4510751265462795\n",
      "Epoch 139, Loss: 0.45082650445054473\n",
      "Epoch 140, Loss: 0.4506281098405297\n",
      "Epoch 141, Loss: 0.4501911419656675\n",
      "Epoch 142, Loss: 0.4497001887659326\n",
      "Epoch 143, Loss: 0.44943436133718806\n",
      "Epoch 144, Loss: 0.449226296402944\n",
      "Epoch 145, Loss: 0.44886073897867573\n",
      "Epoch 146, Loss: 0.44844710376304864\n",
      "Epoch 147, Loss: 0.44810507013116724\n",
      "Epoch 148, Loss: 0.44788545862049767\n",
      "Epoch 149, Loss: 0.4476741441009651\n",
      "Epoch 150, Loss: 0.44735861731902055\n",
      "Epoch 151, Loss: 0.4469288720157105\n",
      "Epoch 152, Loss: 0.44655235754178146\n",
      "Epoch 153, Loss: 0.4462857402786373\n",
      "Epoch 154, Loss: 0.4460517369142754\n",
      "Epoch 155, Loss: 0.4457881796184935\n",
      "Epoch 156, Loss: 0.44542210886348255\n",
      "Epoch 157, Loss: 0.4450579467827662\n",
      "Epoch 158, Loss: 0.44473329320345206\n",
      "Epoch 159, Loss: 0.4444482514678638\n",
      "Epoch 160, Loss: 0.4441745674630603\n",
      "Epoch 161, Loss: 0.4439221813722748\n",
      "Epoch 162, Loss: 0.44363357870363757\n",
      "Epoch 163, Loss: 0.4433189071134186\n",
      "Epoch 164, Loss: 0.44295452976444744\n",
      "Epoch 165, Loss: 0.4426058108229731\n",
      "Epoch 166, Loss: 0.4422485787900922\n",
      "Epoch 167, Loss: 0.4419090985167976\n",
      "Epoch 168, Loss: 0.44158054387831963\n",
      "Epoch 169, Loss: 0.4412474388772268\n",
      "Epoch 170, Loss: 0.4409277694136744\n",
      "Epoch 171, Loss: 0.44068045571539705\n",
      "Epoch 172, Loss: 0.4405979418732049\n",
      "Epoch 173, Loss: 0.44094101686109266\n",
      "Epoch 174, Loss: 0.4417368233339534\n",
      "Epoch 175, Loss: 0.4416582519101522\n",
      "Epoch 176, Loss: 0.4396788970821407\n",
      "Epoch 177, Loss: 0.4390195954561405\n",
      "Epoch 178, Loss: 0.4400830988007806\n",
      "Epoch 179, Loss: 0.43931775869233325\n",
      "Epoch 180, Loss: 0.4380032505977321\n",
      "Epoch 181, Loss: 0.4384470974340077\n",
      "Epoch 182, Loss: 0.4385167636581309\n",
      "Epoch 183, Loss: 0.43733043450040754\n",
      "Epoch 184, Loss: 0.4373176399006318\n",
      "Epoch 185, Loss: 0.4375256973621567\n",
      "Epoch 186, Loss: 0.43659322296038683\n",
      "Epoch 187, Loss: 0.43644391045908093\n",
      "Epoch 188, Loss: 0.43663844358015674\n",
      "Epoch 189, Loss: 0.43586286705793914\n",
      "Epoch 190, Loss: 0.4355663072844155\n",
      "Epoch 191, Loss: 0.4357040532214924\n",
      "Epoch 192, Loss: 0.435134007760397\n",
      "Epoch 193, Loss: 0.43474241127906293\n",
      "Epoch 194, Loss: 0.43480482310319635\n",
      "Epoch 195, Loss: 0.43443826104982264\n",
      "Epoch 196, Loss: 0.43390767873021246\n",
      "Epoch 197, Loss: 0.4338897691233524\n",
      "Epoch 198, Loss: 0.43377856877811044\n",
      "Epoch 199, Loss: 0.4331595506415052\n",
      "Epoch 200, Loss: 0.43289415091975497\n",
      "Epoch 201, Loss: 0.43294048069736957\n",
      "Epoch 202, Loss: 0.43262240576772243\n",
      "Epoch 203, Loss: 0.43206541812705346\n",
      "Epoch 204, Loss: 0.43172500614102416\n",
      "Epoch 205, Loss: 0.4316686388133938\n",
      "Epoch 206, Loss: 0.4315179158466364\n",
      "Epoch 207, Loss: 0.4310712381632941\n",
      "Epoch 208, Loss: 0.43059470035496\n",
      "Epoch 209, Loss: 0.4303711221416647\n",
      "Epoch 210, Loss: 0.43033088719595897\n",
      "Epoch 211, Loss: 0.43016580489992484\n",
      "Epoch 212, Loss: 0.429691817522032\n",
      "Epoch 213, Loss: 0.4292099292794459\n",
      "Epoch 214, Loss: 0.4289207205910941\n",
      "Epoch 215, Loss: 0.4287978238495365\n",
      "Epoch 216, Loss: 0.42870603658529494\n",
      "Epoch 217, Loss: 0.42841980940051866\n",
      "Epoch 218, Loss: 0.4278918633922788\n",
      "Epoch 219, Loss: 0.42748307113702383\n",
      "Epoch 220, Loss: 0.4271688511370021\n",
      "Epoch 221, Loss: 0.4269432345934848\n",
      "Epoch 222, Loss: 0.4267651275776002\n",
      "Epoch 223, Loss: 0.42668376428587257\n",
      "Epoch 224, Loss: 0.42653554195413756\n",
      "Epoch 225, Loss: 0.42613293183996703\n",
      "Epoch 226, Loss: 0.4255451632780942\n",
      "Epoch 227, Loss: 0.4251305043179084\n",
      "Epoch 228, Loss: 0.42475934718412794\n",
      "Epoch 229, Loss: 0.42452283256309487\n",
      "Epoch 230, Loss: 0.4244024016195508\n",
      "Epoch 231, Loss: 0.4242604946441205\n",
      "Epoch 232, Loss: 0.424134367950937\n",
      "Epoch 233, Loss: 0.42372608274856827\n",
      "Epoch 234, Loss: 0.4231492155631465\n",
      "Epoch 235, Loss: 0.4226291943984805\n",
      "Epoch 236, Loss: 0.4221723179942825\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.24260300527243303\n",
      "Valence RMSE: 0.25527824528841186\n",
      "Arousal RMSE: 0.2292279516483576\n",
      "Test R^2 score: tensor([0.3036, 0.6125], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.45805343269755827\n",
      "Num of epochs: 237\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Epoch 121, Loss: 0.4596922103619757\n",
      "Epoch 122, Loss: 0.45943389956138125\n",
      "Epoch 123, Loss: 0.45877818894117006\n",
      "Epoch 124, Loss: 0.45746538132874076\n",
      "Epoch 125, Loss: 0.4570462803121619\n",
      "Epoch 126, Loss: 0.45696189549769234\n",
      "Epoch 127, Loss: 0.45594920341421274\n",
      "Epoch 128, Loss: 0.45536359747685007\n",
      "Epoch 129, Loss: 0.4553290890752543\n",
      "Epoch 130, Loss: 0.4547195823074053\n",
      "Epoch 131, Loss: 0.45392838323998064\n",
      "Epoch 132, Loss: 0.4537853493595631\n",
      "Epoch 133, Loss: 0.45347437050767975\n",
      "Epoch 134, Loss: 0.45267431273812453\n",
      "Epoch 135, Loss: 0.45228737659373064\n",
      "Epoch 136, Loss: 0.4522154657432981\n",
      "Epoch 137, Loss: 0.45165964487407706\n",
      "Epoch 138, Loss: 0.4510751265462795\n",
      "Epoch 139, Loss: 0.45082650445054473\n",
      "Epoch 140, Loss: 0.4506281098405297\n",
      "Epoch 141, Loss: 0.4501911419656675\n",
      "Epoch 142, Loss: 0.4497001887659326\n",
      "Epoch 143, Loss: 0.44943436133718806\n",
      "Epoch 144, Loss: 0.449226296402944\n",
      "Epoch 145, Loss: 0.44886073897867573\n",
      "Epoch 146, Loss: 0.44844710376304864\n",
      "Epoch 147, Loss: 0.44810507013116724\n",
      "Epoch 148, Loss: 0.44788545862049767\n",
      "Epoch 149, Loss: 0.4476741441009651\n",
      "Epoch 150, Loss: 0.44735861731902055\n",
      "Epoch 151, Loss: 0.4469288720157105\n",
      "Epoch 152, Loss: 0.44655235754178146\n",
      "Epoch 153, Loss: 0.4462857402786373\n",
      "Epoch 154, Loss: 0.4460517369142754\n",
      "Epoch 155, Loss: 0.4457881796184935\n",
      "Epoch 156, Loss: 0.44542210886348255\n",
      "Epoch 157, Loss: 0.4450579467827662\n",
      "Epoch 158, Loss: 0.44473329320345206\n",
      "Epoch 159, Loss: 0.4444482514678638\n",
      "Epoch 160, Loss: 0.4441745674630603\n",
      "Epoch 161, Loss: 0.4439221813722748\n",
      "Epoch 162, Loss: 0.44363357870363757\n",
      "Epoch 163, Loss: 0.4433189071134186\n",
      "Epoch 164, Loss: 0.44295452976444744\n",
      "Epoch 165, Loss: 0.4426058108229731\n",
      "Epoch 166, Loss: 0.4422485787900922\n",
      "Epoch 167, Loss: 0.4419090985167976\n",
      "Epoch 168, Loss: 0.44158054387831963\n",
      "Epoch 169, Loss: 0.4412474388772268\n",
      "Epoch 170, Loss: 0.4409277694136744\n",
      "Epoch 171, Loss: 0.44068045571539705\n",
      "Epoch 172, Loss: 0.4405979418732049\n",
      "Epoch 173, Loss: 0.44094101686109266\n",
      "Epoch 174, Loss: 0.4417368233339534\n",
      "Epoch 175, Loss: 0.4416582519101522\n",
      "Epoch 176, Loss: 0.4396788970821407\n",
      "Epoch 177, Loss: 0.4390195954561405\n",
      "Epoch 178, Loss: 0.4400830988007806\n",
      "Epoch 179, Loss: 0.43931775869233325\n",
      "Epoch 180, Loss: 0.4380032505977321\n",
      "Epoch 181, Loss: 0.4384470974340077\n",
      "Epoch 182, Loss: 0.4385167636581309\n",
      "Epoch 183, Loss: 0.43733043450040754\n",
      "Epoch 184, Loss: 0.4373176399006318\n",
      "Epoch 185, Loss: 0.4375256973621567\n",
      "Epoch 186, Loss: 0.43659322296038683\n",
      "Epoch 187, Loss: 0.43644391045908093\n",
      "Epoch 188, Loss: 0.43663844358015674\n",
      "Epoch 189, Loss: 0.43586286705793914\n",
      "Epoch 190, Loss: 0.4355663072844155\n",
      "Epoch 191, Loss: 0.4357040532214924\n",
      "Epoch 192, Loss: 0.435134007760397\n",
      "Epoch 193, Loss: 0.43474241127906293\n",
      "Epoch 194, Loss: 0.43480482310319635\n",
      "Epoch 195, Loss: 0.43443826104982264\n",
      "Epoch 196, Loss: 0.43390767873021246\n",
      "Epoch 197, Loss: 0.4338897691233524\n",
      "Epoch 198, Loss: 0.43377856877811044\n",
      "Epoch 199, Loss: 0.4331595506415052\n",
      "Epoch 200, Loss: 0.43289415091975497\n",
      "Epoch 201, Loss: 0.43294048069736957\n",
      "Epoch 202, Loss: 0.43262240576772243\n",
      "Epoch 203, Loss: 0.43206541812705346\n",
      "Epoch 204, Loss: 0.43172500614102416\n",
      "Epoch 205, Loss: 0.4316686388133938\n",
      "Epoch 206, Loss: 0.4315179158466364\n",
      "Epoch 207, Loss: 0.4310712381632941\n",
      "Epoch 208, Loss: 0.43059470035496\n",
      "Epoch 209, Loss: 0.4303711221416647\n",
      "Epoch 210, Loss: 0.43033088719595897\n",
      "Epoch 211, Loss: 0.43016580489992484\n",
      "Epoch 212, Loss: 0.429691817522032\n",
      "Epoch 213, Loss: 0.4292099292794459\n",
      "Epoch 214, Loss: 0.4289207205910941\n",
      "Epoch 215, Loss: 0.4287978238495365\n",
      "Epoch 216, Loss: 0.42870603658529494\n",
      "Epoch 217, Loss: 0.42841980940051866\n",
      "Epoch 218, Loss: 0.4278918633922788\n",
      "Epoch 219, Loss: 0.42748307113702383\n",
      "Epoch 220, Loss: 0.4271688511370021\n",
      "Epoch 221, Loss: 0.4269432345934848\n",
      "Epoch 222, Loss: 0.4267651275776002\n",
      "Epoch 223, Loss: 0.42668376428587257\n",
      "Epoch 224, Loss: 0.42653554195413756\n",
      "Epoch 225, Loss: 0.42613293183996703\n",
      "Epoch 226, Loss: 0.4255451632780942\n",
      "Epoch 227, Loss: 0.4251305043179084\n",
      "Epoch 228, Loss: 0.42475934718412794\n",
      "Epoch 229, Loss: 0.42452283256309487\n",
      "Epoch 230, Loss: 0.4244024016195508\n",
      "Epoch 231, Loss: 0.4242604946441205\n",
      "Epoch 232, Loss: 0.424134367950937\n",
      "Epoch 233, Loss: 0.42372608274856827\n",
      "Epoch 234, Loss: 0.4231492155631465\n",
      "Epoch 235, Loss: 0.4226291943984805\n",
      "Epoch 236, Loss: 0.4221723179942825\n",
      "Epoch 237, Loss: 0.4218955917984354\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.24238215573591806\n",
      "Valence RMSE: 0.2545985001396239\n",
      "Arousal RMSE: 0.22951649737009236\n",
      "Test R^2 score: tensor([0.3073, 0.6115], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.45941713117770855\n",
      "Num of epochs: 238\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Epoch 121, Loss: 0.4596922103619757\n",
      "Epoch 122, Loss: 0.45943389956138125\n",
      "Epoch 123, Loss: 0.45877818894117006\n",
      "Epoch 124, Loss: 0.45746538132874076\n",
      "Epoch 125, Loss: 0.4570462803121619\n",
      "Epoch 126, Loss: 0.45696189549769234\n",
      "Epoch 127, Loss: 0.45594920341421274\n",
      "Epoch 128, Loss: 0.45536359747685007\n",
      "Epoch 129, Loss: 0.4553290890752543\n",
      "Epoch 130, Loss: 0.4547195823074053\n",
      "Epoch 131, Loss: 0.45392838323998064\n",
      "Epoch 132, Loss: 0.4537853493595631\n",
      "Epoch 133, Loss: 0.45347437050767975\n",
      "Epoch 134, Loss: 0.45267431273812453\n",
      "Epoch 135, Loss: 0.45228737659373064\n",
      "Epoch 136, Loss: 0.4522154657432981\n",
      "Epoch 137, Loss: 0.45165964487407706\n",
      "Epoch 138, Loss: 0.4510751265462795\n",
      "Epoch 139, Loss: 0.45082650445054473\n",
      "Epoch 140, Loss: 0.4506281098405297\n",
      "Epoch 141, Loss: 0.4501911419656675\n",
      "Epoch 142, Loss: 0.4497001887659326\n",
      "Epoch 143, Loss: 0.44943436133718806\n",
      "Epoch 144, Loss: 0.449226296402944\n",
      "Epoch 145, Loss: 0.44886073897867573\n",
      "Epoch 146, Loss: 0.44844710376304864\n",
      "Epoch 147, Loss: 0.44810507013116724\n",
      "Epoch 148, Loss: 0.44788545862049767\n",
      "Epoch 149, Loss: 0.4476741441009651\n",
      "Epoch 150, Loss: 0.44735861731902055\n",
      "Epoch 151, Loss: 0.4469288720157105\n",
      "Epoch 152, Loss: 0.44655235754178146\n",
      "Epoch 153, Loss: 0.4462857402786373\n",
      "Epoch 154, Loss: 0.4460517369142754\n",
      "Epoch 155, Loss: 0.4457881796184935\n",
      "Epoch 156, Loss: 0.44542210886348255\n",
      "Epoch 157, Loss: 0.4450579467827662\n",
      "Epoch 158, Loss: 0.44473329320345206\n",
      "Epoch 159, Loss: 0.4444482514678638\n",
      "Epoch 160, Loss: 0.4441745674630603\n",
      "Epoch 161, Loss: 0.4439221813722748\n",
      "Epoch 162, Loss: 0.44363357870363757\n",
      "Epoch 163, Loss: 0.4433189071134186\n",
      "Epoch 164, Loss: 0.44295452976444744\n",
      "Epoch 165, Loss: 0.4426058108229731\n",
      "Epoch 166, Loss: 0.4422485787900922\n",
      "Epoch 167, Loss: 0.4419090985167976\n",
      "Epoch 168, Loss: 0.44158054387831963\n",
      "Epoch 169, Loss: 0.4412474388772268\n",
      "Epoch 170, Loss: 0.4409277694136744\n",
      "Epoch 171, Loss: 0.44068045571539705\n",
      "Epoch 172, Loss: 0.4405979418732049\n",
      "Epoch 173, Loss: 0.44094101686109266\n",
      "Epoch 174, Loss: 0.4417368233339534\n",
      "Epoch 175, Loss: 0.4416582519101522\n",
      "Epoch 176, Loss: 0.4396788970821407\n",
      "Epoch 177, Loss: 0.4390195954561405\n",
      "Epoch 178, Loss: 0.4400830988007806\n",
      "Epoch 179, Loss: 0.43931775869233325\n",
      "Epoch 180, Loss: 0.4380032505977321\n",
      "Epoch 181, Loss: 0.4384470974340077\n",
      "Epoch 182, Loss: 0.4385167636581309\n",
      "Epoch 183, Loss: 0.43733043450040754\n",
      "Epoch 184, Loss: 0.4373176399006318\n",
      "Epoch 185, Loss: 0.4375256973621567\n",
      "Epoch 186, Loss: 0.43659322296038683\n",
      "Epoch 187, Loss: 0.43644391045908093\n",
      "Epoch 188, Loss: 0.43663844358015674\n",
      "Epoch 189, Loss: 0.43586286705793914\n",
      "Epoch 190, Loss: 0.4355663072844155\n",
      "Epoch 191, Loss: 0.4357040532214924\n",
      "Epoch 192, Loss: 0.435134007760397\n",
      "Epoch 193, Loss: 0.43474241127906293\n",
      "Epoch 194, Loss: 0.43480482310319635\n",
      "Epoch 195, Loss: 0.43443826104982264\n",
      "Epoch 196, Loss: 0.43390767873021246\n",
      "Epoch 197, Loss: 0.4338897691233524\n",
      "Epoch 198, Loss: 0.43377856877811044\n",
      "Epoch 199, Loss: 0.4331595506415052\n",
      "Epoch 200, Loss: 0.43289415091975497\n",
      "Epoch 201, Loss: 0.43294048069736957\n",
      "Epoch 202, Loss: 0.43262240576772243\n",
      "Epoch 203, Loss: 0.43206541812705346\n",
      "Epoch 204, Loss: 0.43172500614102416\n",
      "Epoch 205, Loss: 0.4316686388133938\n",
      "Epoch 206, Loss: 0.4315179158466364\n",
      "Epoch 207, Loss: 0.4310712381632941\n",
      "Epoch 208, Loss: 0.43059470035496\n",
      "Epoch 209, Loss: 0.4303711221416647\n",
      "Epoch 210, Loss: 0.43033088719595897\n",
      "Epoch 211, Loss: 0.43016580489992484\n",
      "Epoch 212, Loss: 0.429691817522032\n",
      "Epoch 213, Loss: 0.4292099292794459\n",
      "Epoch 214, Loss: 0.4289207205910941\n",
      "Epoch 215, Loss: 0.4287978238495365\n",
      "Epoch 216, Loss: 0.42870603658529494\n",
      "Epoch 217, Loss: 0.42841980940051866\n",
      "Epoch 218, Loss: 0.4278918633922788\n",
      "Epoch 219, Loss: 0.42748307113702383\n",
      "Epoch 220, Loss: 0.4271688511370021\n",
      "Epoch 221, Loss: 0.4269432345934848\n",
      "Epoch 222, Loss: 0.4267651275776002\n",
      "Epoch 223, Loss: 0.42668376428587257\n",
      "Epoch 224, Loss: 0.42653554195413756\n",
      "Epoch 225, Loss: 0.42613293183996703\n",
      "Epoch 226, Loss: 0.4255451632780942\n",
      "Epoch 227, Loss: 0.4251305043179084\n",
      "Epoch 228, Loss: 0.42475934718412794\n",
      "Epoch 229, Loss: 0.42452283256309487\n",
      "Epoch 230, Loss: 0.4244024016195508\n",
      "Epoch 231, Loss: 0.4242604946441205\n",
      "Epoch 232, Loss: 0.424134367950937\n",
      "Epoch 233, Loss: 0.42372608274856827\n",
      "Epoch 234, Loss: 0.4231492155631465\n",
      "Epoch 235, Loss: 0.4226291943984805\n",
      "Epoch 236, Loss: 0.4221723179942825\n",
      "Epoch 237, Loss: 0.4218955917984354\n",
      "Epoch 238, Loss: 0.4216767834434851\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.24300426653876098\n",
      "Valence RMSE: 0.2556330624691364\n",
      "Arousal RMSE: 0.2296821379313876\n",
      "Test R^2 score: tensor([0.3017, 0.6109], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4563162717755429\n",
      "Num of epochs: 239\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Epoch 121, Loss: 0.4596922103619757\n",
      "Epoch 122, Loss: 0.45943389956138125\n",
      "Epoch 123, Loss: 0.45877818894117006\n",
      "Epoch 124, Loss: 0.45746538132874076\n",
      "Epoch 125, Loss: 0.4570462803121619\n",
      "Epoch 126, Loss: 0.45696189549769234\n",
      "Epoch 127, Loss: 0.45594920341421274\n",
      "Epoch 128, Loss: 0.45536359747685007\n",
      "Epoch 129, Loss: 0.4553290890752543\n",
      "Epoch 130, Loss: 0.4547195823074053\n",
      "Epoch 131, Loss: 0.45392838323998064\n",
      "Epoch 132, Loss: 0.4537853493595631\n",
      "Epoch 133, Loss: 0.45347437050767975\n",
      "Epoch 134, Loss: 0.45267431273812453\n",
      "Epoch 135, Loss: 0.45228737659373064\n",
      "Epoch 136, Loss: 0.4522154657432981\n",
      "Epoch 137, Loss: 0.45165964487407706\n",
      "Epoch 138, Loss: 0.4510751265462795\n",
      "Epoch 139, Loss: 0.45082650445054473\n",
      "Epoch 140, Loss: 0.4506281098405297\n",
      "Epoch 141, Loss: 0.4501911419656675\n",
      "Epoch 142, Loss: 0.4497001887659326\n",
      "Epoch 143, Loss: 0.44943436133718806\n",
      "Epoch 144, Loss: 0.449226296402944\n",
      "Epoch 145, Loss: 0.44886073897867573\n",
      "Epoch 146, Loss: 0.44844710376304864\n",
      "Epoch 147, Loss: 0.44810507013116724\n",
      "Epoch 148, Loss: 0.44788545862049767\n",
      "Epoch 149, Loss: 0.4476741441009651\n",
      "Epoch 150, Loss: 0.44735861731902055\n",
      "Epoch 151, Loss: 0.4469288720157105\n",
      "Epoch 152, Loss: 0.44655235754178146\n",
      "Epoch 153, Loss: 0.4462857402786373\n",
      "Epoch 154, Loss: 0.4460517369142754\n",
      "Epoch 155, Loss: 0.4457881796184935\n",
      "Epoch 156, Loss: 0.44542210886348255\n",
      "Epoch 157, Loss: 0.4450579467827662\n",
      "Epoch 158, Loss: 0.44473329320345206\n",
      "Epoch 159, Loss: 0.4444482514678638\n",
      "Epoch 160, Loss: 0.4441745674630603\n",
      "Epoch 161, Loss: 0.4439221813722748\n",
      "Epoch 162, Loss: 0.44363357870363757\n",
      "Epoch 163, Loss: 0.4433189071134186\n",
      "Epoch 164, Loss: 0.44295452976444744\n",
      "Epoch 165, Loss: 0.4426058108229731\n",
      "Epoch 166, Loss: 0.4422485787900922\n",
      "Epoch 167, Loss: 0.4419090985167976\n",
      "Epoch 168, Loss: 0.44158054387831963\n",
      "Epoch 169, Loss: 0.4412474388772268\n",
      "Epoch 170, Loss: 0.4409277694136744\n",
      "Epoch 171, Loss: 0.44068045571539705\n",
      "Epoch 172, Loss: 0.4405979418732049\n",
      "Epoch 173, Loss: 0.44094101686109266\n",
      "Epoch 174, Loss: 0.4417368233339534\n",
      "Epoch 175, Loss: 0.4416582519101522\n",
      "Epoch 176, Loss: 0.4396788970821407\n",
      "Epoch 177, Loss: 0.4390195954561405\n",
      "Epoch 178, Loss: 0.4400830988007806\n",
      "Epoch 179, Loss: 0.43931775869233325\n",
      "Epoch 180, Loss: 0.4380032505977321\n",
      "Epoch 181, Loss: 0.4384470974340077\n",
      "Epoch 182, Loss: 0.4385167636581309\n",
      "Epoch 183, Loss: 0.43733043450040754\n",
      "Epoch 184, Loss: 0.4373176399006318\n",
      "Epoch 185, Loss: 0.4375256973621567\n",
      "Epoch 186, Loss: 0.43659322296038683\n",
      "Epoch 187, Loss: 0.43644391045908093\n",
      "Epoch 188, Loss: 0.43663844358015674\n",
      "Epoch 189, Loss: 0.43586286705793914\n",
      "Epoch 190, Loss: 0.4355663072844155\n",
      "Epoch 191, Loss: 0.4357040532214924\n",
      "Epoch 192, Loss: 0.435134007760397\n",
      "Epoch 193, Loss: 0.43474241127906293\n",
      "Epoch 194, Loss: 0.43480482310319635\n",
      "Epoch 195, Loss: 0.43443826104982264\n",
      "Epoch 196, Loss: 0.43390767873021246\n",
      "Epoch 197, Loss: 0.4338897691233524\n",
      "Epoch 198, Loss: 0.43377856877811044\n",
      "Epoch 199, Loss: 0.4331595506415052\n",
      "Epoch 200, Loss: 0.43289415091975497\n",
      "Epoch 201, Loss: 0.43294048069736957\n",
      "Epoch 202, Loss: 0.43262240576772243\n",
      "Epoch 203, Loss: 0.43206541812705346\n",
      "Epoch 204, Loss: 0.43172500614102416\n",
      "Epoch 205, Loss: 0.4316686388133938\n",
      "Epoch 206, Loss: 0.4315179158466364\n",
      "Epoch 207, Loss: 0.4310712381632941\n",
      "Epoch 208, Loss: 0.43059470035496\n",
      "Epoch 209, Loss: 0.4303711221416647\n",
      "Epoch 210, Loss: 0.43033088719595897\n",
      "Epoch 211, Loss: 0.43016580489992484\n",
      "Epoch 212, Loss: 0.429691817522032\n",
      "Epoch 213, Loss: 0.4292099292794459\n",
      "Epoch 214, Loss: 0.4289207205910941\n",
      "Epoch 215, Loss: 0.4287978238495365\n",
      "Epoch 216, Loss: 0.42870603658529494\n",
      "Epoch 217, Loss: 0.42841980940051866\n",
      "Epoch 218, Loss: 0.4278918633922788\n",
      "Epoch 219, Loss: 0.42748307113702383\n",
      "Epoch 220, Loss: 0.4271688511370021\n",
      "Epoch 221, Loss: 0.4269432345934848\n",
      "Epoch 222, Loss: 0.4267651275776002\n",
      "Epoch 223, Loss: 0.42668376428587257\n",
      "Epoch 224, Loss: 0.42653554195413756\n",
      "Epoch 225, Loss: 0.42613293183996703\n",
      "Epoch 226, Loss: 0.4255451632780942\n",
      "Epoch 227, Loss: 0.4251305043179084\n",
      "Epoch 228, Loss: 0.42475934718412794\n",
      "Epoch 229, Loss: 0.42452283256309487\n",
      "Epoch 230, Loss: 0.4244024016195508\n",
      "Epoch 231, Loss: 0.4242604946441205\n",
      "Epoch 232, Loss: 0.424134367950937\n",
      "Epoch 233, Loss: 0.42372608274856827\n",
      "Epoch 234, Loss: 0.4231492155631465\n",
      "Epoch 235, Loss: 0.4226291943984805\n",
      "Epoch 236, Loss: 0.4221723179942825\n",
      "Epoch 237, Loss: 0.4218955917984354\n",
      "Epoch 238, Loss: 0.4216767834434851\n",
      "Epoch 239, Loss: 0.42145169178243785\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.24308537086770432\n",
      "Valence RMSE: 0.255217135602783\n",
      "Arousal RMSE: 0.23031545487545854\n",
      "Test R^2 score: tensor([0.3040, 0.6088], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.45637727757868757\n",
      "Num of epochs: 240\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Epoch 121, Loss: 0.4596922103619757\n",
      "Epoch 122, Loss: 0.45943389956138125\n",
      "Epoch 123, Loss: 0.45877818894117006\n",
      "Epoch 124, Loss: 0.45746538132874076\n",
      "Epoch 125, Loss: 0.4570462803121619\n",
      "Epoch 126, Loss: 0.45696189549769234\n",
      "Epoch 127, Loss: 0.45594920341421274\n",
      "Epoch 128, Loss: 0.45536359747685007\n",
      "Epoch 129, Loss: 0.4553290890752543\n",
      "Epoch 130, Loss: 0.4547195823074053\n",
      "Epoch 131, Loss: 0.45392838323998064\n",
      "Epoch 132, Loss: 0.4537853493595631\n",
      "Epoch 133, Loss: 0.45347437050767975\n",
      "Epoch 134, Loss: 0.45267431273812453\n",
      "Epoch 135, Loss: 0.45228737659373064\n",
      "Epoch 136, Loss: 0.4522154657432981\n",
      "Epoch 137, Loss: 0.45165964487407706\n",
      "Epoch 138, Loss: 0.4510751265462795\n",
      "Epoch 139, Loss: 0.45082650445054473\n",
      "Epoch 140, Loss: 0.4506281098405297\n",
      "Epoch 141, Loss: 0.4501911419656675\n",
      "Epoch 142, Loss: 0.4497001887659326\n",
      "Epoch 143, Loss: 0.44943436133718806\n",
      "Epoch 144, Loss: 0.449226296402944\n",
      "Epoch 145, Loss: 0.44886073897867573\n",
      "Epoch 146, Loss: 0.44844710376304864\n",
      "Epoch 147, Loss: 0.44810507013116724\n",
      "Epoch 148, Loss: 0.44788545862049767\n",
      "Epoch 149, Loss: 0.4476741441009651\n",
      "Epoch 150, Loss: 0.44735861731902055\n",
      "Epoch 151, Loss: 0.4469288720157105\n",
      "Epoch 152, Loss: 0.44655235754178146\n",
      "Epoch 153, Loss: 0.4462857402786373\n",
      "Epoch 154, Loss: 0.4460517369142754\n",
      "Epoch 155, Loss: 0.4457881796184935\n",
      "Epoch 156, Loss: 0.44542210886348255\n",
      "Epoch 157, Loss: 0.4450579467827662\n",
      "Epoch 158, Loss: 0.44473329320345206\n",
      "Epoch 159, Loss: 0.4444482514678638\n",
      "Epoch 160, Loss: 0.4441745674630603\n",
      "Epoch 161, Loss: 0.4439221813722748\n",
      "Epoch 162, Loss: 0.44363357870363757\n",
      "Epoch 163, Loss: 0.4433189071134186\n",
      "Epoch 164, Loss: 0.44295452976444744\n",
      "Epoch 165, Loss: 0.4426058108229731\n",
      "Epoch 166, Loss: 0.4422485787900922\n",
      "Epoch 167, Loss: 0.4419090985167976\n",
      "Epoch 168, Loss: 0.44158054387831963\n",
      "Epoch 169, Loss: 0.4412474388772268\n",
      "Epoch 170, Loss: 0.4409277694136744\n",
      "Epoch 171, Loss: 0.44068045571539705\n",
      "Epoch 172, Loss: 0.4405979418732049\n",
      "Epoch 173, Loss: 0.44094101686109266\n",
      "Epoch 174, Loss: 0.4417368233339534\n",
      "Epoch 175, Loss: 0.4416582519101522\n",
      "Epoch 176, Loss: 0.4396788970821407\n",
      "Epoch 177, Loss: 0.4390195954561405\n",
      "Epoch 178, Loss: 0.4400830988007806\n",
      "Epoch 179, Loss: 0.43931775869233325\n",
      "Epoch 180, Loss: 0.4380032505977321\n",
      "Epoch 181, Loss: 0.4384470974340077\n",
      "Epoch 182, Loss: 0.4385167636581309\n",
      "Epoch 183, Loss: 0.43733043450040754\n",
      "Epoch 184, Loss: 0.4373176399006318\n",
      "Epoch 185, Loss: 0.4375256973621567\n",
      "Epoch 186, Loss: 0.43659322296038683\n",
      "Epoch 187, Loss: 0.43644391045908093\n",
      "Epoch 188, Loss: 0.43663844358015674\n",
      "Epoch 189, Loss: 0.43586286705793914\n",
      "Epoch 190, Loss: 0.4355663072844155\n",
      "Epoch 191, Loss: 0.4357040532214924\n",
      "Epoch 192, Loss: 0.435134007760397\n",
      "Epoch 193, Loss: 0.43474241127906293\n",
      "Epoch 194, Loss: 0.43480482310319635\n",
      "Epoch 195, Loss: 0.43443826104982264\n",
      "Epoch 196, Loss: 0.43390767873021246\n",
      "Epoch 197, Loss: 0.4338897691233524\n",
      "Epoch 198, Loss: 0.43377856877811044\n",
      "Epoch 199, Loss: 0.4331595506415052\n",
      "Epoch 200, Loss: 0.43289415091975497\n",
      "Epoch 201, Loss: 0.43294048069736957\n",
      "Epoch 202, Loss: 0.43262240576772243\n",
      "Epoch 203, Loss: 0.43206541812705346\n",
      "Epoch 204, Loss: 0.43172500614102416\n",
      "Epoch 205, Loss: 0.4316686388133938\n",
      "Epoch 206, Loss: 0.4315179158466364\n",
      "Epoch 207, Loss: 0.4310712381632941\n",
      "Epoch 208, Loss: 0.43059470035496\n",
      "Epoch 209, Loss: 0.4303711221416647\n",
      "Epoch 210, Loss: 0.43033088719595897\n",
      "Epoch 211, Loss: 0.43016580489992484\n",
      "Epoch 212, Loss: 0.429691817522032\n",
      "Epoch 213, Loss: 0.4292099292794459\n",
      "Epoch 214, Loss: 0.4289207205910941\n",
      "Epoch 215, Loss: 0.4287978238495365\n",
      "Epoch 216, Loss: 0.42870603658529494\n",
      "Epoch 217, Loss: 0.42841980940051866\n",
      "Epoch 218, Loss: 0.4278918633922788\n",
      "Epoch 219, Loss: 0.42748307113702383\n",
      "Epoch 220, Loss: 0.4271688511370021\n",
      "Epoch 221, Loss: 0.4269432345934848\n",
      "Epoch 222, Loss: 0.4267651275776002\n",
      "Epoch 223, Loss: 0.42668376428587257\n",
      "Epoch 224, Loss: 0.42653554195413756\n",
      "Epoch 225, Loss: 0.42613293183996703\n",
      "Epoch 226, Loss: 0.4255451632780942\n",
      "Epoch 227, Loss: 0.4251305043179084\n",
      "Epoch 228, Loss: 0.42475934718412794\n",
      "Epoch 229, Loss: 0.42452283256309487\n",
      "Epoch 230, Loss: 0.4244024016195508\n",
      "Epoch 231, Loss: 0.4242604946441205\n",
      "Epoch 232, Loss: 0.424134367950937\n",
      "Epoch 233, Loss: 0.42372608274856827\n",
      "Epoch 234, Loss: 0.4231492155631465\n",
      "Epoch 235, Loss: 0.4226291943984805\n",
      "Epoch 236, Loss: 0.4221723179942825\n",
      "Epoch 237, Loss: 0.4218955917984354\n",
      "Epoch 238, Loss: 0.4216767834434851\n",
      "Epoch 239, Loss: 0.42145169178243785\n",
      "Epoch 240, Loss: 0.4212605805865247\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.24382302907268474\n",
      "Valence RMSE: 0.2567333573838578\n",
      "Arousal RMSE: 0.23018975263632155\n",
      "Test R^2 score: tensor([0.2957, 0.6092], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.452443344210911\n",
      "Num of epochs: 241\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Epoch 121, Loss: 0.4596922103619757\n",
      "Epoch 122, Loss: 0.45943389956138125\n",
      "Epoch 123, Loss: 0.45877818894117006\n",
      "Epoch 124, Loss: 0.45746538132874076\n",
      "Epoch 125, Loss: 0.4570462803121619\n",
      "Epoch 126, Loss: 0.45696189549769234\n",
      "Epoch 127, Loss: 0.45594920341421274\n",
      "Epoch 128, Loss: 0.45536359747685007\n",
      "Epoch 129, Loss: 0.4553290890752543\n",
      "Epoch 130, Loss: 0.4547195823074053\n",
      "Epoch 131, Loss: 0.45392838323998064\n",
      "Epoch 132, Loss: 0.4537853493595631\n",
      "Epoch 133, Loss: 0.45347437050767975\n",
      "Epoch 134, Loss: 0.45267431273812453\n",
      "Epoch 135, Loss: 0.45228737659373064\n",
      "Epoch 136, Loss: 0.4522154657432981\n",
      "Epoch 137, Loss: 0.45165964487407706\n",
      "Epoch 138, Loss: 0.4510751265462795\n",
      "Epoch 139, Loss: 0.45082650445054473\n",
      "Epoch 140, Loss: 0.4506281098405297\n",
      "Epoch 141, Loss: 0.4501911419656675\n",
      "Epoch 142, Loss: 0.4497001887659326\n",
      "Epoch 143, Loss: 0.44943436133718806\n",
      "Epoch 144, Loss: 0.449226296402944\n",
      "Epoch 145, Loss: 0.44886073897867573\n",
      "Epoch 146, Loss: 0.44844710376304864\n",
      "Epoch 147, Loss: 0.44810507013116724\n",
      "Epoch 148, Loss: 0.44788545862049767\n",
      "Epoch 149, Loss: 0.4476741441009651\n",
      "Epoch 150, Loss: 0.44735861731902055\n",
      "Epoch 151, Loss: 0.4469288720157105\n",
      "Epoch 152, Loss: 0.44655235754178146\n",
      "Epoch 153, Loss: 0.4462857402786373\n",
      "Epoch 154, Loss: 0.4460517369142754\n",
      "Epoch 155, Loss: 0.4457881796184935\n",
      "Epoch 156, Loss: 0.44542210886348255\n",
      "Epoch 157, Loss: 0.4450579467827662\n",
      "Epoch 158, Loss: 0.44473329320345206\n",
      "Epoch 159, Loss: 0.4444482514678638\n",
      "Epoch 160, Loss: 0.4441745674630603\n",
      "Epoch 161, Loss: 0.4439221813722748\n",
      "Epoch 162, Loss: 0.44363357870363757\n",
      "Epoch 163, Loss: 0.4433189071134186\n",
      "Epoch 164, Loss: 0.44295452976444744\n",
      "Epoch 165, Loss: 0.4426058108229731\n",
      "Epoch 166, Loss: 0.4422485787900922\n",
      "Epoch 167, Loss: 0.4419090985167976\n",
      "Epoch 168, Loss: 0.44158054387831963\n",
      "Epoch 169, Loss: 0.4412474388772268\n",
      "Epoch 170, Loss: 0.4409277694136744\n",
      "Epoch 171, Loss: 0.44068045571539705\n",
      "Epoch 172, Loss: 0.4405979418732049\n",
      "Epoch 173, Loss: 0.44094101686109266\n",
      "Epoch 174, Loss: 0.4417368233339534\n",
      "Epoch 175, Loss: 0.4416582519101522\n",
      "Epoch 176, Loss: 0.4396788970821407\n",
      "Epoch 177, Loss: 0.4390195954561405\n",
      "Epoch 178, Loss: 0.4400830988007806\n",
      "Epoch 179, Loss: 0.43931775869233325\n",
      "Epoch 180, Loss: 0.4380032505977321\n",
      "Epoch 181, Loss: 0.4384470974340077\n",
      "Epoch 182, Loss: 0.4385167636581309\n",
      "Epoch 183, Loss: 0.43733043450040754\n",
      "Epoch 184, Loss: 0.4373176399006318\n",
      "Epoch 185, Loss: 0.4375256973621567\n",
      "Epoch 186, Loss: 0.43659322296038683\n",
      "Epoch 187, Loss: 0.43644391045908093\n",
      "Epoch 188, Loss: 0.43663844358015674\n",
      "Epoch 189, Loss: 0.43586286705793914\n",
      "Epoch 190, Loss: 0.4355663072844155\n",
      "Epoch 191, Loss: 0.4357040532214924\n",
      "Epoch 192, Loss: 0.435134007760397\n",
      "Epoch 193, Loss: 0.43474241127906293\n",
      "Epoch 194, Loss: 0.43480482310319635\n",
      "Epoch 195, Loss: 0.43443826104982264\n",
      "Epoch 196, Loss: 0.43390767873021246\n",
      "Epoch 197, Loss: 0.4338897691233524\n",
      "Epoch 198, Loss: 0.43377856877811044\n",
      "Epoch 199, Loss: 0.4331595506415052\n",
      "Epoch 200, Loss: 0.43289415091975497\n",
      "Epoch 201, Loss: 0.43294048069736957\n",
      "Epoch 202, Loss: 0.43262240576772243\n",
      "Epoch 203, Loss: 0.43206541812705346\n",
      "Epoch 204, Loss: 0.43172500614102416\n",
      "Epoch 205, Loss: 0.4316686388133938\n",
      "Epoch 206, Loss: 0.4315179158466364\n",
      "Epoch 207, Loss: 0.4310712381632941\n",
      "Epoch 208, Loss: 0.43059470035496\n",
      "Epoch 209, Loss: 0.4303711221416647\n",
      "Epoch 210, Loss: 0.43033088719595897\n",
      "Epoch 211, Loss: 0.43016580489992484\n",
      "Epoch 212, Loss: 0.429691817522032\n",
      "Epoch 213, Loss: 0.4292099292794459\n",
      "Epoch 214, Loss: 0.4289207205910941\n",
      "Epoch 215, Loss: 0.4287978238495365\n",
      "Epoch 216, Loss: 0.42870603658529494\n",
      "Epoch 217, Loss: 0.42841980940051866\n",
      "Epoch 218, Loss: 0.4278918633922788\n",
      "Epoch 219, Loss: 0.42748307113702383\n",
      "Epoch 220, Loss: 0.4271688511370021\n",
      "Epoch 221, Loss: 0.4269432345934848\n",
      "Epoch 222, Loss: 0.4267651275776002\n",
      "Epoch 223, Loss: 0.42668376428587257\n",
      "Epoch 224, Loss: 0.42653554195413756\n",
      "Epoch 225, Loss: 0.42613293183996703\n",
      "Epoch 226, Loss: 0.4255451632780942\n",
      "Epoch 227, Loss: 0.4251305043179084\n",
      "Epoch 228, Loss: 0.42475934718412794\n",
      "Epoch 229, Loss: 0.42452283256309487\n",
      "Epoch 230, Loss: 0.4244024016195508\n",
      "Epoch 231, Loss: 0.4242604946441205\n",
      "Epoch 232, Loss: 0.424134367950937\n",
      "Epoch 233, Loss: 0.42372608274856827\n",
      "Epoch 234, Loss: 0.4231492155631465\n",
      "Epoch 235, Loss: 0.4226291943984805\n",
      "Epoch 236, Loss: 0.4221723179942825\n",
      "Epoch 237, Loss: 0.4218955917984354\n",
      "Epoch 238, Loss: 0.4216767834434851\n",
      "Epoch 239, Loss: 0.42145169178243785\n",
      "Epoch 240, Loss: 0.4212605805865247\n",
      "Epoch 241, Loss: 0.42123380253494425\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.24347678835091882\n",
      "Valence RMSE: 0.25536081280603795\n",
      "Arousal RMSE: 0.2309821383016355\n",
      "Test R^2 score: tensor([0.3032, 0.6065], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.45485127916756873\n",
      "Num of epochs: 242\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Epoch 121, Loss: 0.4596922103619757\n",
      "Epoch 122, Loss: 0.45943389956138125\n",
      "Epoch 123, Loss: 0.45877818894117006\n",
      "Epoch 124, Loss: 0.45746538132874076\n",
      "Epoch 125, Loss: 0.4570462803121619\n",
      "Epoch 126, Loss: 0.45696189549769234\n",
      "Epoch 127, Loss: 0.45594920341421274\n",
      "Epoch 128, Loss: 0.45536359747685007\n",
      "Epoch 129, Loss: 0.4553290890752543\n",
      "Epoch 130, Loss: 0.4547195823074053\n",
      "Epoch 131, Loss: 0.45392838323998064\n",
      "Epoch 132, Loss: 0.4537853493595631\n",
      "Epoch 133, Loss: 0.45347437050767975\n",
      "Epoch 134, Loss: 0.45267431273812453\n",
      "Epoch 135, Loss: 0.45228737659373064\n",
      "Epoch 136, Loss: 0.4522154657432981\n",
      "Epoch 137, Loss: 0.45165964487407706\n",
      "Epoch 138, Loss: 0.4510751265462795\n",
      "Epoch 139, Loss: 0.45082650445054473\n",
      "Epoch 140, Loss: 0.4506281098405297\n",
      "Epoch 141, Loss: 0.4501911419656675\n",
      "Epoch 142, Loss: 0.4497001887659326\n",
      "Epoch 143, Loss: 0.44943436133718806\n",
      "Epoch 144, Loss: 0.449226296402944\n",
      "Epoch 145, Loss: 0.44886073897867573\n",
      "Epoch 146, Loss: 0.44844710376304864\n",
      "Epoch 147, Loss: 0.44810507013116724\n",
      "Epoch 148, Loss: 0.44788545862049767\n",
      "Epoch 149, Loss: 0.4476741441009651\n",
      "Epoch 150, Loss: 0.44735861731902055\n",
      "Epoch 151, Loss: 0.4469288720157105\n",
      "Epoch 152, Loss: 0.44655235754178146\n",
      "Epoch 153, Loss: 0.4462857402786373\n",
      "Epoch 154, Loss: 0.4460517369142754\n",
      "Epoch 155, Loss: 0.4457881796184935\n",
      "Epoch 156, Loss: 0.44542210886348255\n",
      "Epoch 157, Loss: 0.4450579467827662\n",
      "Epoch 158, Loss: 0.44473329320345206\n",
      "Epoch 159, Loss: 0.4444482514678638\n",
      "Epoch 160, Loss: 0.4441745674630603\n",
      "Epoch 161, Loss: 0.4439221813722748\n",
      "Epoch 162, Loss: 0.44363357870363757\n",
      "Epoch 163, Loss: 0.4433189071134186\n",
      "Epoch 164, Loss: 0.44295452976444744\n",
      "Epoch 165, Loss: 0.4426058108229731\n",
      "Epoch 166, Loss: 0.4422485787900922\n",
      "Epoch 167, Loss: 0.4419090985167976\n",
      "Epoch 168, Loss: 0.44158054387831963\n",
      "Epoch 169, Loss: 0.4412474388772268\n",
      "Epoch 170, Loss: 0.4409277694136744\n",
      "Epoch 171, Loss: 0.44068045571539705\n",
      "Epoch 172, Loss: 0.4405979418732049\n",
      "Epoch 173, Loss: 0.44094101686109266\n",
      "Epoch 174, Loss: 0.4417368233339534\n",
      "Epoch 175, Loss: 0.4416582519101522\n",
      "Epoch 176, Loss: 0.4396788970821407\n",
      "Epoch 177, Loss: 0.4390195954561405\n",
      "Epoch 178, Loss: 0.4400830988007806\n",
      "Epoch 179, Loss: 0.43931775869233325\n",
      "Epoch 180, Loss: 0.4380032505977321\n",
      "Epoch 181, Loss: 0.4384470974340077\n",
      "Epoch 182, Loss: 0.4385167636581309\n",
      "Epoch 183, Loss: 0.43733043450040754\n",
      "Epoch 184, Loss: 0.4373176399006318\n",
      "Epoch 185, Loss: 0.4375256973621567\n",
      "Epoch 186, Loss: 0.43659322296038683\n",
      "Epoch 187, Loss: 0.43644391045908093\n",
      "Epoch 188, Loss: 0.43663844358015674\n",
      "Epoch 189, Loss: 0.43586286705793914\n",
      "Epoch 190, Loss: 0.4355663072844155\n",
      "Epoch 191, Loss: 0.4357040532214924\n",
      "Epoch 192, Loss: 0.435134007760397\n",
      "Epoch 193, Loss: 0.43474241127906293\n",
      "Epoch 194, Loss: 0.43480482310319635\n",
      "Epoch 195, Loss: 0.43443826104982264\n",
      "Epoch 196, Loss: 0.43390767873021246\n",
      "Epoch 197, Loss: 0.4338897691233524\n",
      "Epoch 198, Loss: 0.43377856877811044\n",
      "Epoch 199, Loss: 0.4331595506415052\n",
      "Epoch 200, Loss: 0.43289415091975497\n",
      "Epoch 201, Loss: 0.43294048069736957\n",
      "Epoch 202, Loss: 0.43262240576772243\n",
      "Epoch 203, Loss: 0.43206541812705346\n",
      "Epoch 204, Loss: 0.43172500614102416\n",
      "Epoch 205, Loss: 0.4316686388133938\n",
      "Epoch 206, Loss: 0.4315179158466364\n",
      "Epoch 207, Loss: 0.4310712381632941\n",
      "Epoch 208, Loss: 0.43059470035496\n",
      "Epoch 209, Loss: 0.4303711221416647\n",
      "Epoch 210, Loss: 0.43033088719595897\n",
      "Epoch 211, Loss: 0.43016580489992484\n",
      "Epoch 212, Loss: 0.429691817522032\n",
      "Epoch 213, Loss: 0.4292099292794459\n",
      "Epoch 214, Loss: 0.4289207205910941\n",
      "Epoch 215, Loss: 0.4287978238495365\n",
      "Epoch 216, Loss: 0.42870603658529494\n",
      "Epoch 217, Loss: 0.42841980940051866\n",
      "Epoch 218, Loss: 0.4278918633922788\n",
      "Epoch 219, Loss: 0.42748307113702383\n",
      "Epoch 220, Loss: 0.4271688511370021\n",
      "Epoch 221, Loss: 0.4269432345934848\n",
      "Epoch 222, Loss: 0.4267651275776002\n",
      "Epoch 223, Loss: 0.42668376428587257\n",
      "Epoch 224, Loss: 0.42653554195413756\n",
      "Epoch 225, Loss: 0.42613293183996703\n",
      "Epoch 226, Loss: 0.4255451632780942\n",
      "Epoch 227, Loss: 0.4251305043179084\n",
      "Epoch 228, Loss: 0.42475934718412794\n",
      "Epoch 229, Loss: 0.42452283256309487\n",
      "Epoch 230, Loss: 0.4244024016195508\n",
      "Epoch 231, Loss: 0.4242604946441205\n",
      "Epoch 232, Loss: 0.424134367950937\n",
      "Epoch 233, Loss: 0.42372608274856827\n",
      "Epoch 234, Loss: 0.4231492155631465\n",
      "Epoch 235, Loss: 0.4226291943984805\n",
      "Epoch 236, Loss: 0.4221723179942825\n",
      "Epoch 237, Loss: 0.4218955917984354\n",
      "Epoch 238, Loss: 0.4216767834434851\n",
      "Epoch 239, Loss: 0.42145169178243785\n",
      "Epoch 240, Loss: 0.4212605805865247\n",
      "Epoch 241, Loss: 0.42123380253494425\n",
      "Epoch 242, Loss: 0.42125483246926426\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.24457105534941778\n",
      "Valence RMSE: 0.25751942502511477\n",
      "Arousal RMSE: 0.2308977002141647\n",
      "Test R^2 score: tensor([0.2913, 0.6068], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.44907982076322156\n",
      "Num of epochs: 243\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Epoch 121, Loss: 0.4596922103619757\n",
      "Epoch 122, Loss: 0.45943389956138125\n",
      "Epoch 123, Loss: 0.45877818894117006\n",
      "Epoch 124, Loss: 0.45746538132874076\n",
      "Epoch 125, Loss: 0.4570462803121619\n",
      "Epoch 126, Loss: 0.45696189549769234\n",
      "Epoch 127, Loss: 0.45594920341421274\n",
      "Epoch 128, Loss: 0.45536359747685007\n",
      "Epoch 129, Loss: 0.4553290890752543\n",
      "Epoch 130, Loss: 0.4547195823074053\n",
      "Epoch 131, Loss: 0.45392838323998064\n",
      "Epoch 132, Loss: 0.4537853493595631\n",
      "Epoch 133, Loss: 0.45347437050767975\n",
      "Epoch 134, Loss: 0.45267431273812453\n",
      "Epoch 135, Loss: 0.45228737659373064\n",
      "Epoch 136, Loss: 0.4522154657432981\n",
      "Epoch 137, Loss: 0.45165964487407706\n",
      "Epoch 138, Loss: 0.4510751265462795\n",
      "Epoch 139, Loss: 0.45082650445054473\n",
      "Epoch 140, Loss: 0.4506281098405297\n",
      "Epoch 141, Loss: 0.4501911419656675\n",
      "Epoch 142, Loss: 0.4497001887659326\n",
      "Epoch 143, Loss: 0.44943436133718806\n",
      "Epoch 144, Loss: 0.449226296402944\n",
      "Epoch 145, Loss: 0.44886073897867573\n",
      "Epoch 146, Loss: 0.44844710376304864\n",
      "Epoch 147, Loss: 0.44810507013116724\n",
      "Epoch 148, Loss: 0.44788545862049767\n",
      "Epoch 149, Loss: 0.4476741441009651\n",
      "Epoch 150, Loss: 0.44735861731902055\n",
      "Epoch 151, Loss: 0.4469288720157105\n",
      "Epoch 152, Loss: 0.44655235754178146\n",
      "Epoch 153, Loss: 0.4462857402786373\n",
      "Epoch 154, Loss: 0.4460517369142754\n",
      "Epoch 155, Loss: 0.4457881796184935\n",
      "Epoch 156, Loss: 0.44542210886348255\n",
      "Epoch 157, Loss: 0.4450579467827662\n",
      "Epoch 158, Loss: 0.44473329320345206\n",
      "Epoch 159, Loss: 0.4444482514678638\n",
      "Epoch 160, Loss: 0.4441745674630603\n",
      "Epoch 161, Loss: 0.4439221813722748\n",
      "Epoch 162, Loss: 0.44363357870363757\n",
      "Epoch 163, Loss: 0.4433189071134186\n",
      "Epoch 164, Loss: 0.44295452976444744\n",
      "Epoch 165, Loss: 0.4426058108229731\n",
      "Epoch 166, Loss: 0.4422485787900922\n",
      "Epoch 167, Loss: 0.4419090985167976\n",
      "Epoch 168, Loss: 0.44158054387831963\n",
      "Epoch 169, Loss: 0.4412474388772268\n",
      "Epoch 170, Loss: 0.4409277694136744\n",
      "Epoch 171, Loss: 0.44068045571539705\n",
      "Epoch 172, Loss: 0.4405979418732049\n",
      "Epoch 173, Loss: 0.44094101686109266\n",
      "Epoch 174, Loss: 0.4417368233339534\n",
      "Epoch 175, Loss: 0.4416582519101522\n",
      "Epoch 176, Loss: 0.4396788970821407\n",
      "Epoch 177, Loss: 0.4390195954561405\n",
      "Epoch 178, Loss: 0.4400830988007806\n",
      "Epoch 179, Loss: 0.43931775869233325\n",
      "Epoch 180, Loss: 0.4380032505977321\n",
      "Epoch 181, Loss: 0.4384470974340077\n",
      "Epoch 182, Loss: 0.4385167636581309\n",
      "Epoch 183, Loss: 0.43733043450040754\n",
      "Epoch 184, Loss: 0.4373176399006318\n",
      "Epoch 185, Loss: 0.4375256973621567\n",
      "Epoch 186, Loss: 0.43659322296038683\n",
      "Epoch 187, Loss: 0.43644391045908093\n",
      "Epoch 188, Loss: 0.43663844358015674\n",
      "Epoch 189, Loss: 0.43586286705793914\n",
      "Epoch 190, Loss: 0.4355663072844155\n",
      "Epoch 191, Loss: 0.4357040532214924\n",
      "Epoch 192, Loss: 0.435134007760397\n",
      "Epoch 193, Loss: 0.43474241127906293\n",
      "Epoch 194, Loss: 0.43480482310319635\n",
      "Epoch 195, Loss: 0.43443826104982264\n",
      "Epoch 196, Loss: 0.43390767873021246\n",
      "Epoch 197, Loss: 0.4338897691233524\n",
      "Epoch 198, Loss: 0.43377856877811044\n",
      "Epoch 199, Loss: 0.4331595506415052\n",
      "Epoch 200, Loss: 0.43289415091975497\n",
      "Epoch 201, Loss: 0.43294048069736957\n",
      "Epoch 202, Loss: 0.43262240576772243\n",
      "Epoch 203, Loss: 0.43206541812705346\n",
      "Epoch 204, Loss: 0.43172500614102416\n",
      "Epoch 205, Loss: 0.4316686388133938\n",
      "Epoch 206, Loss: 0.4315179158466364\n",
      "Epoch 207, Loss: 0.4310712381632941\n",
      "Epoch 208, Loss: 0.43059470035496\n",
      "Epoch 209, Loss: 0.4303711221416647\n",
      "Epoch 210, Loss: 0.43033088719595897\n",
      "Epoch 211, Loss: 0.43016580489992484\n",
      "Epoch 212, Loss: 0.429691817522032\n",
      "Epoch 213, Loss: 0.4292099292794459\n",
      "Epoch 214, Loss: 0.4289207205910941\n",
      "Epoch 215, Loss: 0.4287978238495365\n",
      "Epoch 216, Loss: 0.42870603658529494\n",
      "Epoch 217, Loss: 0.42841980940051866\n",
      "Epoch 218, Loss: 0.4278918633922788\n",
      "Epoch 219, Loss: 0.42748307113702383\n",
      "Epoch 220, Loss: 0.4271688511370021\n",
      "Epoch 221, Loss: 0.4269432345934848\n",
      "Epoch 222, Loss: 0.4267651275776002\n",
      "Epoch 223, Loss: 0.42668376428587257\n",
      "Epoch 224, Loss: 0.42653554195413756\n",
      "Epoch 225, Loss: 0.42613293183996703\n",
      "Epoch 226, Loss: 0.4255451632780942\n",
      "Epoch 227, Loss: 0.4251305043179084\n",
      "Epoch 228, Loss: 0.42475934718412794\n",
      "Epoch 229, Loss: 0.42452283256309487\n",
      "Epoch 230, Loss: 0.4244024016195508\n",
      "Epoch 231, Loss: 0.4242604946441205\n",
      "Epoch 232, Loss: 0.424134367950937\n",
      "Epoch 233, Loss: 0.42372608274856827\n",
      "Epoch 234, Loss: 0.4231492155631465\n",
      "Epoch 235, Loss: 0.4226291943984805\n",
      "Epoch 236, Loss: 0.4221723179942825\n",
      "Epoch 237, Loss: 0.4218955917984354\n",
      "Epoch 238, Loss: 0.4216767834434851\n",
      "Epoch 239, Loss: 0.42145169178243785\n",
      "Epoch 240, Loss: 0.4212605805865247\n",
      "Epoch 241, Loss: 0.42123380253494425\n",
      "Epoch 242, Loss: 0.42125483246926426\n",
      "Epoch 243, Loss: 0.42124070061060576\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.24382899681140324\n",
      "Valence RMSE: 0.2553429166270681\n",
      "Arousal RMSE: 0.2317437254824664\n",
      "Test R^2 score: tensor([0.3033, 0.6039], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4536006219746088\n",
      "Num of epochs: 244\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Epoch 121, Loss: 0.4596922103619757\n",
      "Epoch 122, Loss: 0.45943389956138125\n",
      "Epoch 123, Loss: 0.45877818894117006\n",
      "Epoch 124, Loss: 0.45746538132874076\n",
      "Epoch 125, Loss: 0.4570462803121619\n",
      "Epoch 126, Loss: 0.45696189549769234\n",
      "Epoch 127, Loss: 0.45594920341421274\n",
      "Epoch 128, Loss: 0.45536359747685007\n",
      "Epoch 129, Loss: 0.4553290890752543\n",
      "Epoch 130, Loss: 0.4547195823074053\n",
      "Epoch 131, Loss: 0.45392838323998064\n",
      "Epoch 132, Loss: 0.4537853493595631\n",
      "Epoch 133, Loss: 0.45347437050767975\n",
      "Epoch 134, Loss: 0.45267431273812453\n",
      "Epoch 135, Loss: 0.45228737659373064\n",
      "Epoch 136, Loss: 0.4522154657432981\n",
      "Epoch 137, Loss: 0.45165964487407706\n",
      "Epoch 138, Loss: 0.4510751265462795\n",
      "Epoch 139, Loss: 0.45082650445054473\n",
      "Epoch 140, Loss: 0.4506281098405297\n",
      "Epoch 141, Loss: 0.4501911419656675\n",
      "Epoch 142, Loss: 0.4497001887659326\n",
      "Epoch 143, Loss: 0.44943436133718806\n",
      "Epoch 144, Loss: 0.449226296402944\n",
      "Epoch 145, Loss: 0.44886073897867573\n",
      "Epoch 146, Loss: 0.44844710376304864\n",
      "Epoch 147, Loss: 0.44810507013116724\n",
      "Epoch 148, Loss: 0.44788545862049767\n",
      "Epoch 149, Loss: 0.4476741441009651\n",
      "Epoch 150, Loss: 0.44735861731902055\n",
      "Epoch 151, Loss: 0.4469288720157105\n",
      "Epoch 152, Loss: 0.44655235754178146\n",
      "Epoch 153, Loss: 0.4462857402786373\n",
      "Epoch 154, Loss: 0.4460517369142754\n",
      "Epoch 155, Loss: 0.4457881796184935\n",
      "Epoch 156, Loss: 0.44542210886348255\n",
      "Epoch 157, Loss: 0.4450579467827662\n",
      "Epoch 158, Loss: 0.44473329320345206\n",
      "Epoch 159, Loss: 0.4444482514678638\n",
      "Epoch 160, Loss: 0.4441745674630603\n",
      "Epoch 161, Loss: 0.4439221813722748\n",
      "Epoch 162, Loss: 0.44363357870363757\n",
      "Epoch 163, Loss: 0.4433189071134186\n",
      "Epoch 164, Loss: 0.44295452976444744\n",
      "Epoch 165, Loss: 0.4426058108229731\n",
      "Epoch 166, Loss: 0.4422485787900922\n",
      "Epoch 167, Loss: 0.4419090985167976\n",
      "Epoch 168, Loss: 0.44158054387831963\n",
      "Epoch 169, Loss: 0.4412474388772268\n",
      "Epoch 170, Loss: 0.4409277694136744\n",
      "Epoch 171, Loss: 0.44068045571539705\n",
      "Epoch 172, Loss: 0.4405979418732049\n",
      "Epoch 173, Loss: 0.44094101686109266\n",
      "Epoch 174, Loss: 0.4417368233339534\n",
      "Epoch 175, Loss: 0.4416582519101522\n",
      "Epoch 176, Loss: 0.4396788970821407\n",
      "Epoch 177, Loss: 0.4390195954561405\n",
      "Epoch 178, Loss: 0.4400830988007806\n",
      "Epoch 179, Loss: 0.43931775869233325\n",
      "Epoch 180, Loss: 0.4380032505977321\n",
      "Epoch 181, Loss: 0.4384470974340077\n",
      "Epoch 182, Loss: 0.4385167636581309\n",
      "Epoch 183, Loss: 0.43733043450040754\n",
      "Epoch 184, Loss: 0.4373176399006318\n",
      "Epoch 185, Loss: 0.4375256973621567\n",
      "Epoch 186, Loss: 0.43659322296038683\n",
      "Epoch 187, Loss: 0.43644391045908093\n",
      "Epoch 188, Loss: 0.43663844358015674\n",
      "Epoch 189, Loss: 0.43586286705793914\n",
      "Epoch 190, Loss: 0.4355663072844155\n",
      "Epoch 191, Loss: 0.4357040532214924\n",
      "Epoch 192, Loss: 0.435134007760397\n",
      "Epoch 193, Loss: 0.43474241127906293\n",
      "Epoch 194, Loss: 0.43480482310319635\n",
      "Epoch 195, Loss: 0.43443826104982264\n",
      "Epoch 196, Loss: 0.43390767873021246\n",
      "Epoch 197, Loss: 0.4338897691233524\n",
      "Epoch 198, Loss: 0.43377856877811044\n",
      "Epoch 199, Loss: 0.4331595506415052\n",
      "Epoch 200, Loss: 0.43289415091975497\n",
      "Epoch 201, Loss: 0.43294048069736957\n",
      "Epoch 202, Loss: 0.43262240576772243\n",
      "Epoch 203, Loss: 0.43206541812705346\n",
      "Epoch 204, Loss: 0.43172500614102416\n",
      "Epoch 205, Loss: 0.4316686388133938\n",
      "Epoch 206, Loss: 0.4315179158466364\n",
      "Epoch 207, Loss: 0.4310712381632941\n",
      "Epoch 208, Loss: 0.43059470035496\n",
      "Epoch 209, Loss: 0.4303711221416647\n",
      "Epoch 210, Loss: 0.43033088719595897\n",
      "Epoch 211, Loss: 0.43016580489992484\n",
      "Epoch 212, Loss: 0.429691817522032\n",
      "Epoch 213, Loss: 0.4292099292794459\n",
      "Epoch 214, Loss: 0.4289207205910941\n",
      "Epoch 215, Loss: 0.4287978238495365\n",
      "Epoch 216, Loss: 0.42870603658529494\n",
      "Epoch 217, Loss: 0.42841980940051866\n",
      "Epoch 218, Loss: 0.4278918633922788\n",
      "Epoch 219, Loss: 0.42748307113702383\n",
      "Epoch 220, Loss: 0.4271688511370021\n",
      "Epoch 221, Loss: 0.4269432345934848\n",
      "Epoch 222, Loss: 0.4267651275776002\n",
      "Epoch 223, Loss: 0.42668376428587257\n",
      "Epoch 224, Loss: 0.42653554195413756\n",
      "Epoch 225, Loss: 0.42613293183996703\n",
      "Epoch 226, Loss: 0.4255451632780942\n",
      "Epoch 227, Loss: 0.4251305043179084\n",
      "Epoch 228, Loss: 0.42475934718412794\n",
      "Epoch 229, Loss: 0.42452283256309487\n",
      "Epoch 230, Loss: 0.4244024016195508\n",
      "Epoch 231, Loss: 0.4242604946441205\n",
      "Epoch 232, Loss: 0.424134367950937\n",
      "Epoch 233, Loss: 0.42372608274856827\n",
      "Epoch 234, Loss: 0.4231492155631465\n",
      "Epoch 235, Loss: 0.4226291943984805\n",
      "Epoch 236, Loss: 0.4221723179942825\n",
      "Epoch 237, Loss: 0.4218955917984354\n",
      "Epoch 238, Loss: 0.4216767834434851\n",
      "Epoch 239, Loss: 0.42145169178243785\n",
      "Epoch 240, Loss: 0.4212605805865247\n",
      "Epoch 241, Loss: 0.42123380253494425\n",
      "Epoch 242, Loss: 0.42125483246926426\n",
      "Epoch 243, Loss: 0.42124070061060576\n",
      "Epoch 244, Loss: 0.42073760533336746\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.24457254598304318\n",
      "Valence RMSE: 0.2573017627280106\n",
      "Arousal RMSE: 0.23114338276125077\n",
      "Test R^2 score: tensor([0.2925, 0.6060], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.449259954820702\n",
      "Num of epochs: 245\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Epoch 121, Loss: 0.4596922103619757\n",
      "Epoch 122, Loss: 0.45943389956138125\n",
      "Epoch 123, Loss: 0.45877818894117006\n",
      "Epoch 124, Loss: 0.45746538132874076\n",
      "Epoch 125, Loss: 0.4570462803121619\n",
      "Epoch 126, Loss: 0.45696189549769234\n",
      "Epoch 127, Loss: 0.45594920341421274\n",
      "Epoch 128, Loss: 0.45536359747685007\n",
      "Epoch 129, Loss: 0.4553290890752543\n",
      "Epoch 130, Loss: 0.4547195823074053\n",
      "Epoch 131, Loss: 0.45392838323998064\n",
      "Epoch 132, Loss: 0.4537853493595631\n",
      "Epoch 133, Loss: 0.45347437050767975\n",
      "Epoch 134, Loss: 0.45267431273812453\n",
      "Epoch 135, Loss: 0.45228737659373064\n",
      "Epoch 136, Loss: 0.4522154657432981\n",
      "Epoch 137, Loss: 0.45165964487407706\n",
      "Epoch 138, Loss: 0.4510751265462795\n",
      "Epoch 139, Loss: 0.45082650445054473\n",
      "Epoch 140, Loss: 0.4506281098405297\n",
      "Epoch 141, Loss: 0.4501911419656675\n",
      "Epoch 142, Loss: 0.4497001887659326\n",
      "Epoch 143, Loss: 0.44943436133718806\n",
      "Epoch 144, Loss: 0.449226296402944\n",
      "Epoch 145, Loss: 0.44886073897867573\n",
      "Epoch 146, Loss: 0.44844710376304864\n",
      "Epoch 147, Loss: 0.44810507013116724\n",
      "Epoch 148, Loss: 0.44788545862049767\n",
      "Epoch 149, Loss: 0.4476741441009651\n",
      "Epoch 150, Loss: 0.44735861731902055\n",
      "Epoch 151, Loss: 0.4469288720157105\n",
      "Epoch 152, Loss: 0.44655235754178146\n",
      "Epoch 153, Loss: 0.4462857402786373\n",
      "Epoch 154, Loss: 0.4460517369142754\n",
      "Epoch 155, Loss: 0.4457881796184935\n",
      "Epoch 156, Loss: 0.44542210886348255\n",
      "Epoch 157, Loss: 0.4450579467827662\n",
      "Epoch 158, Loss: 0.44473329320345206\n",
      "Epoch 159, Loss: 0.4444482514678638\n",
      "Epoch 160, Loss: 0.4441745674630603\n",
      "Epoch 161, Loss: 0.4439221813722748\n",
      "Epoch 162, Loss: 0.44363357870363757\n",
      "Epoch 163, Loss: 0.4433189071134186\n",
      "Epoch 164, Loss: 0.44295452976444744\n",
      "Epoch 165, Loss: 0.4426058108229731\n",
      "Epoch 166, Loss: 0.4422485787900922\n",
      "Epoch 167, Loss: 0.4419090985167976\n",
      "Epoch 168, Loss: 0.44158054387831963\n",
      "Epoch 169, Loss: 0.4412474388772268\n",
      "Epoch 170, Loss: 0.4409277694136744\n",
      "Epoch 171, Loss: 0.44068045571539705\n",
      "Epoch 172, Loss: 0.4405979418732049\n",
      "Epoch 173, Loss: 0.44094101686109266\n",
      "Epoch 174, Loss: 0.4417368233339534\n",
      "Epoch 175, Loss: 0.4416582519101522\n",
      "Epoch 176, Loss: 0.4396788970821407\n",
      "Epoch 177, Loss: 0.4390195954561405\n",
      "Epoch 178, Loss: 0.4400830988007806\n",
      "Epoch 179, Loss: 0.43931775869233325\n",
      "Epoch 180, Loss: 0.4380032505977321\n",
      "Epoch 181, Loss: 0.4384470974340077\n",
      "Epoch 182, Loss: 0.4385167636581309\n",
      "Epoch 183, Loss: 0.43733043450040754\n",
      "Epoch 184, Loss: 0.4373176399006318\n",
      "Epoch 185, Loss: 0.4375256973621567\n",
      "Epoch 186, Loss: 0.43659322296038683\n",
      "Epoch 187, Loss: 0.43644391045908093\n",
      "Epoch 188, Loss: 0.43663844358015674\n",
      "Epoch 189, Loss: 0.43586286705793914\n",
      "Epoch 190, Loss: 0.4355663072844155\n",
      "Epoch 191, Loss: 0.4357040532214924\n",
      "Epoch 192, Loss: 0.435134007760397\n",
      "Epoch 193, Loss: 0.43474241127906293\n",
      "Epoch 194, Loss: 0.43480482310319635\n",
      "Epoch 195, Loss: 0.43443826104982264\n",
      "Epoch 196, Loss: 0.43390767873021246\n",
      "Epoch 197, Loss: 0.4338897691233524\n",
      "Epoch 198, Loss: 0.43377856877811044\n",
      "Epoch 199, Loss: 0.4331595506415052\n",
      "Epoch 200, Loss: 0.43289415091975497\n",
      "Epoch 201, Loss: 0.43294048069736957\n",
      "Epoch 202, Loss: 0.43262240576772243\n",
      "Epoch 203, Loss: 0.43206541812705346\n",
      "Epoch 204, Loss: 0.43172500614102416\n",
      "Epoch 205, Loss: 0.4316686388133938\n",
      "Epoch 206, Loss: 0.4315179158466364\n",
      "Epoch 207, Loss: 0.4310712381632941\n",
      "Epoch 208, Loss: 0.43059470035496\n",
      "Epoch 209, Loss: 0.4303711221416647\n",
      "Epoch 210, Loss: 0.43033088719595897\n",
      "Epoch 211, Loss: 0.43016580489992484\n",
      "Epoch 212, Loss: 0.429691817522032\n",
      "Epoch 213, Loss: 0.4292099292794459\n",
      "Epoch 214, Loss: 0.4289207205910941\n",
      "Epoch 215, Loss: 0.4287978238495365\n",
      "Epoch 216, Loss: 0.42870603658529494\n",
      "Epoch 217, Loss: 0.42841980940051866\n",
      "Epoch 218, Loss: 0.4278918633922788\n",
      "Epoch 219, Loss: 0.42748307113702383\n",
      "Epoch 220, Loss: 0.4271688511370021\n",
      "Epoch 221, Loss: 0.4269432345934848\n",
      "Epoch 222, Loss: 0.4267651275776002\n",
      "Epoch 223, Loss: 0.42668376428587257\n",
      "Epoch 224, Loss: 0.42653554195413756\n",
      "Epoch 225, Loss: 0.42613293183996703\n",
      "Epoch 226, Loss: 0.4255451632780942\n",
      "Epoch 227, Loss: 0.4251305043179084\n",
      "Epoch 228, Loss: 0.42475934718412794\n",
      "Epoch 229, Loss: 0.42452283256309487\n",
      "Epoch 230, Loss: 0.4244024016195508\n",
      "Epoch 231, Loss: 0.4242604946441205\n",
      "Epoch 232, Loss: 0.424134367950937\n",
      "Epoch 233, Loss: 0.42372608274856827\n",
      "Epoch 234, Loss: 0.4231492155631465\n",
      "Epoch 235, Loss: 0.4226291943984805\n",
      "Epoch 236, Loss: 0.4221723179942825\n",
      "Epoch 237, Loss: 0.4218955917984354\n",
      "Epoch 238, Loss: 0.4216767834434851\n",
      "Epoch 239, Loss: 0.42145169178243785\n",
      "Epoch 240, Loss: 0.4212605805865247\n",
      "Epoch 241, Loss: 0.42123380253494425\n",
      "Epoch 242, Loss: 0.42125483246926426\n",
      "Epoch 243, Loss: 0.42124070061060576\n",
      "Epoch 244, Loss: 0.42073760533336746\n",
      "Epoch 245, Loss: 0.42002867294257734\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.24393082880472355\n",
      "Valence RMSE: 0.2560886083598184\n",
      "Arousal RMSE: 0.2311344265812652\n",
      "Test R^2 score: tensor([0.2992, 0.6060], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.45260295000368855\n",
      "Num of epochs: 246\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Epoch 121, Loss: 0.4596922103619757\n",
      "Epoch 122, Loss: 0.45943389956138125\n",
      "Epoch 123, Loss: 0.45877818894117006\n",
      "Epoch 124, Loss: 0.45746538132874076\n",
      "Epoch 125, Loss: 0.4570462803121619\n",
      "Epoch 126, Loss: 0.45696189549769234\n",
      "Epoch 127, Loss: 0.45594920341421274\n",
      "Epoch 128, Loss: 0.45536359747685007\n",
      "Epoch 129, Loss: 0.4553290890752543\n",
      "Epoch 130, Loss: 0.4547195823074053\n",
      "Epoch 131, Loss: 0.45392838323998064\n",
      "Epoch 132, Loss: 0.4537853493595631\n",
      "Epoch 133, Loss: 0.45347437050767975\n",
      "Epoch 134, Loss: 0.45267431273812453\n",
      "Epoch 135, Loss: 0.45228737659373064\n",
      "Epoch 136, Loss: 0.4522154657432981\n",
      "Epoch 137, Loss: 0.45165964487407706\n",
      "Epoch 138, Loss: 0.4510751265462795\n",
      "Epoch 139, Loss: 0.45082650445054473\n",
      "Epoch 140, Loss: 0.4506281098405297\n",
      "Epoch 141, Loss: 0.4501911419656675\n",
      "Epoch 142, Loss: 0.4497001887659326\n",
      "Epoch 143, Loss: 0.44943436133718806\n",
      "Epoch 144, Loss: 0.449226296402944\n",
      "Epoch 145, Loss: 0.44886073897867573\n",
      "Epoch 146, Loss: 0.44844710376304864\n",
      "Epoch 147, Loss: 0.44810507013116724\n",
      "Epoch 148, Loss: 0.44788545862049767\n",
      "Epoch 149, Loss: 0.4476741441009651\n",
      "Epoch 150, Loss: 0.44735861731902055\n",
      "Epoch 151, Loss: 0.4469288720157105\n",
      "Epoch 152, Loss: 0.44655235754178146\n",
      "Epoch 153, Loss: 0.4462857402786373\n",
      "Epoch 154, Loss: 0.4460517369142754\n",
      "Epoch 155, Loss: 0.4457881796184935\n",
      "Epoch 156, Loss: 0.44542210886348255\n",
      "Epoch 157, Loss: 0.4450579467827662\n",
      "Epoch 158, Loss: 0.44473329320345206\n",
      "Epoch 159, Loss: 0.4444482514678638\n",
      "Epoch 160, Loss: 0.4441745674630603\n",
      "Epoch 161, Loss: 0.4439221813722748\n",
      "Epoch 162, Loss: 0.44363357870363757\n",
      "Epoch 163, Loss: 0.4433189071134186\n",
      "Epoch 164, Loss: 0.44295452976444744\n",
      "Epoch 165, Loss: 0.4426058108229731\n",
      "Epoch 166, Loss: 0.4422485787900922\n",
      "Epoch 167, Loss: 0.4419090985167976\n",
      "Epoch 168, Loss: 0.44158054387831963\n",
      "Epoch 169, Loss: 0.4412474388772268\n",
      "Epoch 170, Loss: 0.4409277694136744\n",
      "Epoch 171, Loss: 0.44068045571539705\n",
      "Epoch 172, Loss: 0.4405979418732049\n",
      "Epoch 173, Loss: 0.44094101686109266\n",
      "Epoch 174, Loss: 0.4417368233339534\n",
      "Epoch 175, Loss: 0.4416582519101522\n",
      "Epoch 176, Loss: 0.4396788970821407\n",
      "Epoch 177, Loss: 0.4390195954561405\n",
      "Epoch 178, Loss: 0.4400830988007806\n",
      "Epoch 179, Loss: 0.43931775869233325\n",
      "Epoch 180, Loss: 0.4380032505977321\n",
      "Epoch 181, Loss: 0.4384470974340077\n",
      "Epoch 182, Loss: 0.4385167636581309\n",
      "Epoch 183, Loss: 0.43733043450040754\n",
      "Epoch 184, Loss: 0.4373176399006318\n",
      "Epoch 185, Loss: 0.4375256973621567\n",
      "Epoch 186, Loss: 0.43659322296038683\n",
      "Epoch 187, Loss: 0.43644391045908093\n",
      "Epoch 188, Loss: 0.43663844358015674\n",
      "Epoch 189, Loss: 0.43586286705793914\n",
      "Epoch 190, Loss: 0.4355663072844155\n",
      "Epoch 191, Loss: 0.4357040532214924\n",
      "Epoch 192, Loss: 0.435134007760397\n",
      "Epoch 193, Loss: 0.43474241127906293\n",
      "Epoch 194, Loss: 0.43480482310319635\n",
      "Epoch 195, Loss: 0.43443826104982264\n",
      "Epoch 196, Loss: 0.43390767873021246\n",
      "Epoch 197, Loss: 0.4338897691233524\n",
      "Epoch 198, Loss: 0.43377856877811044\n",
      "Epoch 199, Loss: 0.4331595506415052\n",
      "Epoch 200, Loss: 0.43289415091975497\n",
      "Epoch 201, Loss: 0.43294048069736957\n",
      "Epoch 202, Loss: 0.43262240576772243\n",
      "Epoch 203, Loss: 0.43206541812705346\n",
      "Epoch 204, Loss: 0.43172500614102416\n",
      "Epoch 205, Loss: 0.4316686388133938\n",
      "Epoch 206, Loss: 0.4315179158466364\n",
      "Epoch 207, Loss: 0.4310712381632941\n",
      "Epoch 208, Loss: 0.43059470035496\n",
      "Epoch 209, Loss: 0.4303711221416647\n",
      "Epoch 210, Loss: 0.43033088719595897\n",
      "Epoch 211, Loss: 0.43016580489992484\n",
      "Epoch 212, Loss: 0.429691817522032\n",
      "Epoch 213, Loss: 0.4292099292794459\n",
      "Epoch 214, Loss: 0.4289207205910941\n",
      "Epoch 215, Loss: 0.4287978238495365\n",
      "Epoch 216, Loss: 0.42870603658529494\n",
      "Epoch 217, Loss: 0.42841980940051866\n",
      "Epoch 218, Loss: 0.4278918633922788\n",
      "Epoch 219, Loss: 0.42748307113702383\n",
      "Epoch 220, Loss: 0.4271688511370021\n",
      "Epoch 221, Loss: 0.4269432345934848\n",
      "Epoch 222, Loss: 0.4267651275776002\n",
      "Epoch 223, Loss: 0.42668376428587257\n",
      "Epoch 224, Loss: 0.42653554195413756\n",
      "Epoch 225, Loss: 0.42613293183996703\n",
      "Epoch 226, Loss: 0.4255451632780942\n",
      "Epoch 227, Loss: 0.4251305043179084\n",
      "Epoch 228, Loss: 0.42475934718412794\n",
      "Epoch 229, Loss: 0.42452283256309487\n",
      "Epoch 230, Loss: 0.4244024016195508\n",
      "Epoch 231, Loss: 0.4242604946441205\n",
      "Epoch 232, Loss: 0.424134367950937\n",
      "Epoch 233, Loss: 0.42372608274856827\n",
      "Epoch 234, Loss: 0.4231492155631465\n",
      "Epoch 235, Loss: 0.4226291943984805\n",
      "Epoch 236, Loss: 0.4221723179942825\n",
      "Epoch 237, Loss: 0.4218955917984354\n",
      "Epoch 238, Loss: 0.4216767834434851\n",
      "Epoch 239, Loss: 0.42145169178243785\n",
      "Epoch 240, Loss: 0.4212605805865247\n",
      "Epoch 241, Loss: 0.42123380253494425\n",
      "Epoch 242, Loss: 0.42125483246926426\n",
      "Epoch 243, Loss: 0.42124070061060576\n",
      "Epoch 244, Loss: 0.42073760533336746\n",
      "Epoch 245, Loss: 0.42002867294257734\n",
      "Epoch 246, Loss: 0.41900436945460057\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.24417809956339515\n",
      "Valence RMSE: 0.2563985006822399\n",
      "Arousal RMSE: 0.23131298593179336\n",
      "Test R^2 score: tensor([0.2975, 0.6054], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4514499082009622\n",
      "Num of epochs: 247\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Epoch 121, Loss: 0.4596922103619757\n",
      "Epoch 122, Loss: 0.45943389956138125\n",
      "Epoch 123, Loss: 0.45877818894117006\n",
      "Epoch 124, Loss: 0.45746538132874076\n",
      "Epoch 125, Loss: 0.4570462803121619\n",
      "Epoch 126, Loss: 0.45696189549769234\n",
      "Epoch 127, Loss: 0.45594920341421274\n",
      "Epoch 128, Loss: 0.45536359747685007\n",
      "Epoch 129, Loss: 0.4553290890752543\n",
      "Epoch 130, Loss: 0.4547195823074053\n",
      "Epoch 131, Loss: 0.45392838323998064\n",
      "Epoch 132, Loss: 0.4537853493595631\n",
      "Epoch 133, Loss: 0.45347437050767975\n",
      "Epoch 134, Loss: 0.45267431273812453\n",
      "Epoch 135, Loss: 0.45228737659373064\n",
      "Epoch 136, Loss: 0.4522154657432981\n",
      "Epoch 137, Loss: 0.45165964487407706\n",
      "Epoch 138, Loss: 0.4510751265462795\n",
      "Epoch 139, Loss: 0.45082650445054473\n",
      "Epoch 140, Loss: 0.4506281098405297\n",
      "Epoch 141, Loss: 0.4501911419656675\n",
      "Epoch 142, Loss: 0.4497001887659326\n",
      "Epoch 143, Loss: 0.44943436133718806\n",
      "Epoch 144, Loss: 0.449226296402944\n",
      "Epoch 145, Loss: 0.44886073897867573\n",
      "Epoch 146, Loss: 0.44844710376304864\n",
      "Epoch 147, Loss: 0.44810507013116724\n",
      "Epoch 148, Loss: 0.44788545862049767\n",
      "Epoch 149, Loss: 0.4476741441009651\n",
      "Epoch 150, Loss: 0.44735861731902055\n",
      "Epoch 151, Loss: 0.4469288720157105\n",
      "Epoch 152, Loss: 0.44655235754178146\n",
      "Epoch 153, Loss: 0.4462857402786373\n",
      "Epoch 154, Loss: 0.4460517369142754\n",
      "Epoch 155, Loss: 0.4457881796184935\n",
      "Epoch 156, Loss: 0.44542210886348255\n",
      "Epoch 157, Loss: 0.4450579467827662\n",
      "Epoch 158, Loss: 0.44473329320345206\n",
      "Epoch 159, Loss: 0.4444482514678638\n",
      "Epoch 160, Loss: 0.4441745674630603\n",
      "Epoch 161, Loss: 0.4439221813722748\n",
      "Epoch 162, Loss: 0.44363357870363757\n",
      "Epoch 163, Loss: 0.4433189071134186\n",
      "Epoch 164, Loss: 0.44295452976444744\n",
      "Epoch 165, Loss: 0.4426058108229731\n",
      "Epoch 166, Loss: 0.4422485787900922\n",
      "Epoch 167, Loss: 0.4419090985167976\n",
      "Epoch 168, Loss: 0.44158054387831963\n",
      "Epoch 169, Loss: 0.4412474388772268\n",
      "Epoch 170, Loss: 0.4409277694136744\n",
      "Epoch 171, Loss: 0.44068045571539705\n",
      "Epoch 172, Loss: 0.4405979418732049\n",
      "Epoch 173, Loss: 0.44094101686109266\n",
      "Epoch 174, Loss: 0.4417368233339534\n",
      "Epoch 175, Loss: 0.4416582519101522\n",
      "Epoch 176, Loss: 0.4396788970821407\n",
      "Epoch 177, Loss: 0.4390195954561405\n",
      "Epoch 178, Loss: 0.4400830988007806\n",
      "Epoch 179, Loss: 0.43931775869233325\n",
      "Epoch 180, Loss: 0.4380032505977321\n",
      "Epoch 181, Loss: 0.4384470974340077\n",
      "Epoch 182, Loss: 0.4385167636581309\n",
      "Epoch 183, Loss: 0.43733043450040754\n",
      "Epoch 184, Loss: 0.4373176399006318\n",
      "Epoch 185, Loss: 0.4375256973621567\n",
      "Epoch 186, Loss: 0.43659322296038683\n",
      "Epoch 187, Loss: 0.43644391045908093\n",
      "Epoch 188, Loss: 0.43663844358015674\n",
      "Epoch 189, Loss: 0.43586286705793914\n",
      "Epoch 190, Loss: 0.4355663072844155\n",
      "Epoch 191, Loss: 0.4357040532214924\n",
      "Epoch 192, Loss: 0.435134007760397\n",
      "Epoch 193, Loss: 0.43474241127906293\n",
      "Epoch 194, Loss: 0.43480482310319635\n",
      "Epoch 195, Loss: 0.43443826104982264\n",
      "Epoch 196, Loss: 0.43390767873021246\n",
      "Epoch 197, Loss: 0.4338897691233524\n",
      "Epoch 198, Loss: 0.43377856877811044\n",
      "Epoch 199, Loss: 0.4331595506415052\n",
      "Epoch 200, Loss: 0.43289415091975497\n",
      "Epoch 201, Loss: 0.43294048069736957\n",
      "Epoch 202, Loss: 0.43262240576772243\n",
      "Epoch 203, Loss: 0.43206541812705346\n",
      "Epoch 204, Loss: 0.43172500614102416\n",
      "Epoch 205, Loss: 0.4316686388133938\n",
      "Epoch 206, Loss: 0.4315179158466364\n",
      "Epoch 207, Loss: 0.4310712381632941\n",
      "Epoch 208, Loss: 0.43059470035496\n",
      "Epoch 209, Loss: 0.4303711221416647\n",
      "Epoch 210, Loss: 0.43033088719595897\n",
      "Epoch 211, Loss: 0.43016580489992484\n",
      "Epoch 212, Loss: 0.429691817522032\n",
      "Epoch 213, Loss: 0.4292099292794459\n",
      "Epoch 214, Loss: 0.4289207205910941\n",
      "Epoch 215, Loss: 0.4287978238495365\n",
      "Epoch 216, Loss: 0.42870603658529494\n",
      "Epoch 217, Loss: 0.42841980940051866\n",
      "Epoch 218, Loss: 0.4278918633922788\n",
      "Epoch 219, Loss: 0.42748307113702383\n",
      "Epoch 220, Loss: 0.4271688511370021\n",
      "Epoch 221, Loss: 0.4269432345934848\n",
      "Epoch 222, Loss: 0.4267651275776002\n",
      "Epoch 223, Loss: 0.42668376428587257\n",
      "Epoch 224, Loss: 0.42653554195413756\n",
      "Epoch 225, Loss: 0.42613293183996703\n",
      "Epoch 226, Loss: 0.4255451632780942\n",
      "Epoch 227, Loss: 0.4251305043179084\n",
      "Epoch 228, Loss: 0.42475934718412794\n",
      "Epoch 229, Loss: 0.42452283256309487\n",
      "Epoch 230, Loss: 0.4244024016195508\n",
      "Epoch 231, Loss: 0.4242604946441205\n",
      "Epoch 232, Loss: 0.424134367950937\n",
      "Epoch 233, Loss: 0.42372608274856827\n",
      "Epoch 234, Loss: 0.4231492155631465\n",
      "Epoch 235, Loss: 0.4226291943984805\n",
      "Epoch 236, Loss: 0.4221723179942825\n",
      "Epoch 237, Loss: 0.4218955917984354\n",
      "Epoch 238, Loss: 0.4216767834434851\n",
      "Epoch 239, Loss: 0.42145169178243785\n",
      "Epoch 240, Loss: 0.4212605805865247\n",
      "Epoch 241, Loss: 0.42123380253494425\n",
      "Epoch 242, Loss: 0.42125483246926426\n",
      "Epoch 243, Loss: 0.42124070061060576\n",
      "Epoch 244, Loss: 0.42073760533336746\n",
      "Epoch 245, Loss: 0.42002867294257734\n",
      "Epoch 246, Loss: 0.41900436945460057\n",
      "Epoch 247, Loss: 0.41841940783018283\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.24490949780687216\n",
      "Valence RMSE: 0.257311043610082\n",
      "Arousal RMSE: 0.23184553277628428\n",
      "Test R^2 score: tensor([0.2925, 0.6036], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4480356822847217\n",
      "Num of epochs: 248\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Epoch 121, Loss: 0.4596922103619757\n",
      "Epoch 122, Loss: 0.45943389956138125\n",
      "Epoch 123, Loss: 0.45877818894117006\n",
      "Epoch 124, Loss: 0.45746538132874076\n",
      "Epoch 125, Loss: 0.4570462803121619\n",
      "Epoch 126, Loss: 0.45696189549769234\n",
      "Epoch 127, Loss: 0.45594920341421274\n",
      "Epoch 128, Loss: 0.45536359747685007\n",
      "Epoch 129, Loss: 0.4553290890752543\n",
      "Epoch 130, Loss: 0.4547195823074053\n",
      "Epoch 131, Loss: 0.45392838323998064\n",
      "Epoch 132, Loss: 0.4537853493595631\n",
      "Epoch 133, Loss: 0.45347437050767975\n",
      "Epoch 134, Loss: 0.45267431273812453\n",
      "Epoch 135, Loss: 0.45228737659373064\n",
      "Epoch 136, Loss: 0.4522154657432981\n",
      "Epoch 137, Loss: 0.45165964487407706\n",
      "Epoch 138, Loss: 0.4510751265462795\n",
      "Epoch 139, Loss: 0.45082650445054473\n",
      "Epoch 140, Loss: 0.4506281098405297\n",
      "Epoch 141, Loss: 0.4501911419656675\n",
      "Epoch 142, Loss: 0.4497001887659326\n",
      "Epoch 143, Loss: 0.44943436133718806\n",
      "Epoch 144, Loss: 0.449226296402944\n",
      "Epoch 145, Loss: 0.44886073897867573\n",
      "Epoch 146, Loss: 0.44844710376304864\n",
      "Epoch 147, Loss: 0.44810507013116724\n",
      "Epoch 148, Loss: 0.44788545862049767\n",
      "Epoch 149, Loss: 0.4476741441009651\n",
      "Epoch 150, Loss: 0.44735861731902055\n",
      "Epoch 151, Loss: 0.4469288720157105\n",
      "Epoch 152, Loss: 0.44655235754178146\n",
      "Epoch 153, Loss: 0.4462857402786373\n",
      "Epoch 154, Loss: 0.4460517369142754\n",
      "Epoch 155, Loss: 0.4457881796184935\n",
      "Epoch 156, Loss: 0.44542210886348255\n",
      "Epoch 157, Loss: 0.4450579467827662\n",
      "Epoch 158, Loss: 0.44473329320345206\n",
      "Epoch 159, Loss: 0.4444482514678638\n",
      "Epoch 160, Loss: 0.4441745674630603\n",
      "Epoch 161, Loss: 0.4439221813722748\n",
      "Epoch 162, Loss: 0.44363357870363757\n",
      "Epoch 163, Loss: 0.4433189071134186\n",
      "Epoch 164, Loss: 0.44295452976444744\n",
      "Epoch 165, Loss: 0.4426058108229731\n",
      "Epoch 166, Loss: 0.4422485787900922\n",
      "Epoch 167, Loss: 0.4419090985167976\n",
      "Epoch 168, Loss: 0.44158054387831963\n",
      "Epoch 169, Loss: 0.4412474388772268\n",
      "Epoch 170, Loss: 0.4409277694136744\n",
      "Epoch 171, Loss: 0.44068045571539705\n",
      "Epoch 172, Loss: 0.4405979418732049\n",
      "Epoch 173, Loss: 0.44094101686109266\n",
      "Epoch 174, Loss: 0.4417368233339534\n",
      "Epoch 175, Loss: 0.4416582519101522\n",
      "Epoch 176, Loss: 0.4396788970821407\n",
      "Epoch 177, Loss: 0.4390195954561405\n",
      "Epoch 178, Loss: 0.4400830988007806\n",
      "Epoch 179, Loss: 0.43931775869233325\n",
      "Epoch 180, Loss: 0.4380032505977321\n",
      "Epoch 181, Loss: 0.4384470974340077\n",
      "Epoch 182, Loss: 0.4385167636581309\n",
      "Epoch 183, Loss: 0.43733043450040754\n",
      "Epoch 184, Loss: 0.4373176399006318\n",
      "Epoch 185, Loss: 0.4375256973621567\n",
      "Epoch 186, Loss: 0.43659322296038683\n",
      "Epoch 187, Loss: 0.43644391045908093\n",
      "Epoch 188, Loss: 0.43663844358015674\n",
      "Epoch 189, Loss: 0.43586286705793914\n",
      "Epoch 190, Loss: 0.4355663072844155\n",
      "Epoch 191, Loss: 0.4357040532214924\n",
      "Epoch 192, Loss: 0.435134007760397\n",
      "Epoch 193, Loss: 0.43474241127906293\n",
      "Epoch 194, Loss: 0.43480482310319635\n",
      "Epoch 195, Loss: 0.43443826104982264\n",
      "Epoch 196, Loss: 0.43390767873021246\n",
      "Epoch 197, Loss: 0.4338897691233524\n",
      "Epoch 198, Loss: 0.43377856877811044\n",
      "Epoch 199, Loss: 0.4331595506415052\n",
      "Epoch 200, Loss: 0.43289415091975497\n",
      "Epoch 201, Loss: 0.43294048069736957\n",
      "Epoch 202, Loss: 0.43262240576772243\n",
      "Epoch 203, Loss: 0.43206541812705346\n",
      "Epoch 204, Loss: 0.43172500614102416\n",
      "Epoch 205, Loss: 0.4316686388133938\n",
      "Epoch 206, Loss: 0.4315179158466364\n",
      "Epoch 207, Loss: 0.4310712381632941\n",
      "Epoch 208, Loss: 0.43059470035496\n",
      "Epoch 209, Loss: 0.4303711221416647\n",
      "Epoch 210, Loss: 0.43033088719595897\n",
      "Epoch 211, Loss: 0.43016580489992484\n",
      "Epoch 212, Loss: 0.429691817522032\n",
      "Epoch 213, Loss: 0.4292099292794459\n",
      "Epoch 214, Loss: 0.4289207205910941\n",
      "Epoch 215, Loss: 0.4287978238495365\n",
      "Epoch 216, Loss: 0.42870603658529494\n",
      "Epoch 217, Loss: 0.42841980940051866\n",
      "Epoch 218, Loss: 0.4278918633922788\n",
      "Epoch 219, Loss: 0.42748307113702383\n",
      "Epoch 220, Loss: 0.4271688511370021\n",
      "Epoch 221, Loss: 0.4269432345934848\n",
      "Epoch 222, Loss: 0.4267651275776002\n",
      "Epoch 223, Loss: 0.42668376428587257\n",
      "Epoch 224, Loss: 0.42653554195413756\n",
      "Epoch 225, Loss: 0.42613293183996703\n",
      "Epoch 226, Loss: 0.4255451632780942\n",
      "Epoch 227, Loss: 0.4251305043179084\n",
      "Epoch 228, Loss: 0.42475934718412794\n",
      "Epoch 229, Loss: 0.42452283256309487\n",
      "Epoch 230, Loss: 0.4244024016195508\n",
      "Epoch 231, Loss: 0.4242604946441205\n",
      "Epoch 232, Loss: 0.424134367950937\n",
      "Epoch 233, Loss: 0.42372608274856827\n",
      "Epoch 234, Loss: 0.4231492155631465\n",
      "Epoch 235, Loss: 0.4226291943984805\n",
      "Epoch 236, Loss: 0.4221723179942825\n",
      "Epoch 237, Loss: 0.4218955917984354\n",
      "Epoch 238, Loss: 0.4216767834434851\n",
      "Epoch 239, Loss: 0.42145169178243785\n",
      "Epoch 240, Loss: 0.4212605805865247\n",
      "Epoch 241, Loss: 0.42123380253494425\n",
      "Epoch 242, Loss: 0.42125483246926426\n",
      "Epoch 243, Loss: 0.42124070061060576\n",
      "Epoch 244, Loss: 0.42073760533336746\n",
      "Epoch 245, Loss: 0.42002867294257734\n",
      "Epoch 246, Loss: 0.41900436945460057\n",
      "Epoch 247, Loss: 0.41841940783018283\n",
      "Epoch 248, Loss: 0.4184544673382656\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.24507849855848748\n",
      "Valence RMSE: 0.25675657130077356\n",
      "Arousal RMSE: 0.2328153861007384\n",
      "Test R^2 score: tensor([0.2955, 0.6003], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4478968536391194\n",
      "Num of epochs: 249\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Epoch 121, Loss: 0.4596922103619757\n",
      "Epoch 122, Loss: 0.45943389956138125\n",
      "Epoch 123, Loss: 0.45877818894117006\n",
      "Epoch 124, Loss: 0.45746538132874076\n",
      "Epoch 125, Loss: 0.4570462803121619\n",
      "Epoch 126, Loss: 0.45696189549769234\n",
      "Epoch 127, Loss: 0.45594920341421274\n",
      "Epoch 128, Loss: 0.45536359747685007\n",
      "Epoch 129, Loss: 0.4553290890752543\n",
      "Epoch 130, Loss: 0.4547195823074053\n",
      "Epoch 131, Loss: 0.45392838323998064\n",
      "Epoch 132, Loss: 0.4537853493595631\n",
      "Epoch 133, Loss: 0.45347437050767975\n",
      "Epoch 134, Loss: 0.45267431273812453\n",
      "Epoch 135, Loss: 0.45228737659373064\n",
      "Epoch 136, Loss: 0.4522154657432981\n",
      "Epoch 137, Loss: 0.45165964487407706\n",
      "Epoch 138, Loss: 0.4510751265462795\n",
      "Epoch 139, Loss: 0.45082650445054473\n",
      "Epoch 140, Loss: 0.4506281098405297\n",
      "Epoch 141, Loss: 0.4501911419656675\n",
      "Epoch 142, Loss: 0.4497001887659326\n",
      "Epoch 143, Loss: 0.44943436133718806\n",
      "Epoch 144, Loss: 0.449226296402944\n",
      "Epoch 145, Loss: 0.44886073897867573\n",
      "Epoch 146, Loss: 0.44844710376304864\n",
      "Epoch 147, Loss: 0.44810507013116724\n",
      "Epoch 148, Loss: 0.44788545862049767\n",
      "Epoch 149, Loss: 0.4476741441009651\n",
      "Epoch 150, Loss: 0.44735861731902055\n",
      "Epoch 151, Loss: 0.4469288720157105\n",
      "Epoch 152, Loss: 0.44655235754178146\n",
      "Epoch 153, Loss: 0.4462857402786373\n",
      "Epoch 154, Loss: 0.4460517369142754\n",
      "Epoch 155, Loss: 0.4457881796184935\n",
      "Epoch 156, Loss: 0.44542210886348255\n",
      "Epoch 157, Loss: 0.4450579467827662\n",
      "Epoch 158, Loss: 0.44473329320345206\n",
      "Epoch 159, Loss: 0.4444482514678638\n",
      "Epoch 160, Loss: 0.4441745674630603\n",
      "Epoch 161, Loss: 0.4439221813722748\n",
      "Epoch 162, Loss: 0.44363357870363757\n",
      "Epoch 163, Loss: 0.4433189071134186\n",
      "Epoch 164, Loss: 0.44295452976444744\n",
      "Epoch 165, Loss: 0.4426058108229731\n",
      "Epoch 166, Loss: 0.4422485787900922\n",
      "Epoch 167, Loss: 0.4419090985167976\n",
      "Epoch 168, Loss: 0.44158054387831963\n",
      "Epoch 169, Loss: 0.4412474388772268\n",
      "Epoch 170, Loss: 0.4409277694136744\n",
      "Epoch 171, Loss: 0.44068045571539705\n",
      "Epoch 172, Loss: 0.4405979418732049\n",
      "Epoch 173, Loss: 0.44094101686109266\n",
      "Epoch 174, Loss: 0.4417368233339534\n",
      "Epoch 175, Loss: 0.4416582519101522\n",
      "Epoch 176, Loss: 0.4396788970821407\n",
      "Epoch 177, Loss: 0.4390195954561405\n",
      "Epoch 178, Loss: 0.4400830988007806\n",
      "Epoch 179, Loss: 0.43931775869233325\n",
      "Epoch 180, Loss: 0.4380032505977321\n",
      "Epoch 181, Loss: 0.4384470974340077\n",
      "Epoch 182, Loss: 0.4385167636581309\n",
      "Epoch 183, Loss: 0.43733043450040754\n",
      "Epoch 184, Loss: 0.4373176399006318\n",
      "Epoch 185, Loss: 0.4375256973621567\n",
      "Epoch 186, Loss: 0.43659322296038683\n",
      "Epoch 187, Loss: 0.43644391045908093\n",
      "Epoch 188, Loss: 0.43663844358015674\n",
      "Epoch 189, Loss: 0.43586286705793914\n",
      "Epoch 190, Loss: 0.4355663072844155\n",
      "Epoch 191, Loss: 0.4357040532214924\n",
      "Epoch 192, Loss: 0.435134007760397\n",
      "Epoch 193, Loss: 0.43474241127906293\n",
      "Epoch 194, Loss: 0.43480482310319635\n",
      "Epoch 195, Loss: 0.43443826104982264\n",
      "Epoch 196, Loss: 0.43390767873021246\n",
      "Epoch 197, Loss: 0.4338897691233524\n",
      "Epoch 198, Loss: 0.43377856877811044\n",
      "Epoch 199, Loss: 0.4331595506415052\n",
      "Epoch 200, Loss: 0.43289415091975497\n",
      "Epoch 201, Loss: 0.43294048069736957\n",
      "Epoch 202, Loss: 0.43262240576772243\n",
      "Epoch 203, Loss: 0.43206541812705346\n",
      "Epoch 204, Loss: 0.43172500614102416\n",
      "Epoch 205, Loss: 0.4316686388133938\n",
      "Epoch 206, Loss: 0.4315179158466364\n",
      "Epoch 207, Loss: 0.4310712381632941\n",
      "Epoch 208, Loss: 0.43059470035496\n",
      "Epoch 209, Loss: 0.4303711221416647\n",
      "Epoch 210, Loss: 0.43033088719595897\n",
      "Epoch 211, Loss: 0.43016580489992484\n",
      "Epoch 212, Loss: 0.429691817522032\n",
      "Epoch 213, Loss: 0.4292099292794459\n",
      "Epoch 214, Loss: 0.4289207205910941\n",
      "Epoch 215, Loss: 0.4287978238495365\n",
      "Epoch 216, Loss: 0.42870603658529494\n",
      "Epoch 217, Loss: 0.42841980940051866\n",
      "Epoch 218, Loss: 0.4278918633922788\n",
      "Epoch 219, Loss: 0.42748307113702383\n",
      "Epoch 220, Loss: 0.4271688511370021\n",
      "Epoch 221, Loss: 0.4269432345934848\n",
      "Epoch 222, Loss: 0.4267651275776002\n",
      "Epoch 223, Loss: 0.42668376428587257\n",
      "Epoch 224, Loss: 0.42653554195413756\n",
      "Epoch 225, Loss: 0.42613293183996703\n",
      "Epoch 226, Loss: 0.4255451632780942\n",
      "Epoch 227, Loss: 0.4251305043179084\n",
      "Epoch 228, Loss: 0.42475934718412794\n",
      "Epoch 229, Loss: 0.42452283256309487\n",
      "Epoch 230, Loss: 0.4244024016195508\n",
      "Epoch 231, Loss: 0.4242604946441205\n",
      "Epoch 232, Loss: 0.424134367950937\n",
      "Epoch 233, Loss: 0.42372608274856827\n",
      "Epoch 234, Loss: 0.4231492155631465\n",
      "Epoch 235, Loss: 0.4226291943984805\n",
      "Epoch 236, Loss: 0.4221723179942825\n",
      "Epoch 237, Loss: 0.4218955917984354\n",
      "Epoch 238, Loss: 0.4216767834434851\n",
      "Epoch 239, Loss: 0.42145169178243785\n",
      "Epoch 240, Loss: 0.4212605805865247\n",
      "Epoch 241, Loss: 0.42123380253494425\n",
      "Epoch 242, Loss: 0.42125483246926426\n",
      "Epoch 243, Loss: 0.42124070061060576\n",
      "Epoch 244, Loss: 0.42073760533336746\n",
      "Epoch 245, Loss: 0.42002867294257734\n",
      "Epoch 246, Loss: 0.41900436945460057\n",
      "Epoch 247, Loss: 0.41841940783018283\n",
      "Epoch 248, Loss: 0.4184544673382656\n",
      "Epoch 249, Loss: 0.4187823781263808\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.24615127796089928\n",
      "Valence RMSE: 0.2587488682315789\n",
      "Arousal RMSE: 0.23287319826988423\n",
      "Test R^2 score: tensor([0.2846, 0.6001], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.44231011756890115\n",
      "Num of epochs: 250\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Epoch 121, Loss: 0.4596922103619757\n",
      "Epoch 122, Loss: 0.45943389956138125\n",
      "Epoch 123, Loss: 0.45877818894117006\n",
      "Epoch 124, Loss: 0.45746538132874076\n",
      "Epoch 125, Loss: 0.4570462803121619\n",
      "Epoch 126, Loss: 0.45696189549769234\n",
      "Epoch 127, Loss: 0.45594920341421274\n",
      "Epoch 128, Loss: 0.45536359747685007\n",
      "Epoch 129, Loss: 0.4553290890752543\n",
      "Epoch 130, Loss: 0.4547195823074053\n",
      "Epoch 131, Loss: 0.45392838323998064\n",
      "Epoch 132, Loss: 0.4537853493595631\n",
      "Epoch 133, Loss: 0.45347437050767975\n",
      "Epoch 134, Loss: 0.45267431273812453\n",
      "Epoch 135, Loss: 0.45228737659373064\n",
      "Epoch 136, Loss: 0.4522154657432981\n",
      "Epoch 137, Loss: 0.45165964487407706\n",
      "Epoch 138, Loss: 0.4510751265462795\n",
      "Epoch 139, Loss: 0.45082650445054473\n",
      "Epoch 140, Loss: 0.4506281098405297\n",
      "Epoch 141, Loss: 0.4501911419656675\n",
      "Epoch 142, Loss: 0.4497001887659326\n",
      "Epoch 143, Loss: 0.44943436133718806\n",
      "Epoch 144, Loss: 0.449226296402944\n",
      "Epoch 145, Loss: 0.44886073897867573\n",
      "Epoch 146, Loss: 0.44844710376304864\n",
      "Epoch 147, Loss: 0.44810507013116724\n",
      "Epoch 148, Loss: 0.44788545862049767\n",
      "Epoch 149, Loss: 0.4476741441009651\n",
      "Epoch 150, Loss: 0.44735861731902055\n",
      "Epoch 151, Loss: 0.4469288720157105\n",
      "Epoch 152, Loss: 0.44655235754178146\n",
      "Epoch 153, Loss: 0.4462857402786373\n",
      "Epoch 154, Loss: 0.4460517369142754\n",
      "Epoch 155, Loss: 0.4457881796184935\n",
      "Epoch 156, Loss: 0.44542210886348255\n",
      "Epoch 157, Loss: 0.4450579467827662\n",
      "Epoch 158, Loss: 0.44473329320345206\n",
      "Epoch 159, Loss: 0.4444482514678638\n",
      "Epoch 160, Loss: 0.4441745674630603\n",
      "Epoch 161, Loss: 0.4439221813722748\n",
      "Epoch 162, Loss: 0.44363357870363757\n",
      "Epoch 163, Loss: 0.4433189071134186\n",
      "Epoch 164, Loss: 0.44295452976444744\n",
      "Epoch 165, Loss: 0.4426058108229731\n",
      "Epoch 166, Loss: 0.4422485787900922\n",
      "Epoch 167, Loss: 0.4419090985167976\n",
      "Epoch 168, Loss: 0.44158054387831963\n",
      "Epoch 169, Loss: 0.4412474388772268\n",
      "Epoch 170, Loss: 0.4409277694136744\n",
      "Epoch 171, Loss: 0.44068045571539705\n",
      "Epoch 172, Loss: 0.4405979418732049\n",
      "Epoch 173, Loss: 0.44094101686109266\n",
      "Epoch 174, Loss: 0.4417368233339534\n",
      "Epoch 175, Loss: 0.4416582519101522\n",
      "Epoch 176, Loss: 0.4396788970821407\n",
      "Epoch 177, Loss: 0.4390195954561405\n",
      "Epoch 178, Loss: 0.4400830988007806\n",
      "Epoch 179, Loss: 0.43931775869233325\n",
      "Epoch 180, Loss: 0.4380032505977321\n",
      "Epoch 181, Loss: 0.4384470974340077\n",
      "Epoch 182, Loss: 0.4385167636581309\n",
      "Epoch 183, Loss: 0.43733043450040754\n",
      "Epoch 184, Loss: 0.4373176399006318\n",
      "Epoch 185, Loss: 0.4375256973621567\n",
      "Epoch 186, Loss: 0.43659322296038683\n",
      "Epoch 187, Loss: 0.43644391045908093\n",
      "Epoch 188, Loss: 0.43663844358015674\n",
      "Epoch 189, Loss: 0.43586286705793914\n",
      "Epoch 190, Loss: 0.4355663072844155\n",
      "Epoch 191, Loss: 0.4357040532214924\n",
      "Epoch 192, Loss: 0.435134007760397\n",
      "Epoch 193, Loss: 0.43474241127906293\n",
      "Epoch 194, Loss: 0.43480482310319635\n",
      "Epoch 195, Loss: 0.43443826104982264\n",
      "Epoch 196, Loss: 0.43390767873021246\n",
      "Epoch 197, Loss: 0.4338897691233524\n",
      "Epoch 198, Loss: 0.43377856877811044\n",
      "Epoch 199, Loss: 0.4331595506415052\n",
      "Epoch 200, Loss: 0.43289415091975497\n",
      "Epoch 201, Loss: 0.43294048069736957\n",
      "Epoch 202, Loss: 0.43262240576772243\n",
      "Epoch 203, Loss: 0.43206541812705346\n",
      "Epoch 204, Loss: 0.43172500614102416\n",
      "Epoch 205, Loss: 0.4316686388133938\n",
      "Epoch 206, Loss: 0.4315179158466364\n",
      "Epoch 207, Loss: 0.4310712381632941\n",
      "Epoch 208, Loss: 0.43059470035496\n",
      "Epoch 209, Loss: 0.4303711221416647\n",
      "Epoch 210, Loss: 0.43033088719595897\n",
      "Epoch 211, Loss: 0.43016580489992484\n",
      "Epoch 212, Loss: 0.429691817522032\n",
      "Epoch 213, Loss: 0.4292099292794459\n",
      "Epoch 214, Loss: 0.4289207205910941\n",
      "Epoch 215, Loss: 0.4287978238495365\n",
      "Epoch 216, Loss: 0.42870603658529494\n",
      "Epoch 217, Loss: 0.42841980940051866\n",
      "Epoch 218, Loss: 0.4278918633922788\n",
      "Epoch 219, Loss: 0.42748307113702383\n",
      "Epoch 220, Loss: 0.4271688511370021\n",
      "Epoch 221, Loss: 0.4269432345934848\n",
      "Epoch 222, Loss: 0.4267651275776002\n",
      "Epoch 223, Loss: 0.42668376428587257\n",
      "Epoch 224, Loss: 0.42653554195413756\n",
      "Epoch 225, Loss: 0.42613293183996703\n",
      "Epoch 226, Loss: 0.4255451632780942\n",
      "Epoch 227, Loss: 0.4251305043179084\n",
      "Epoch 228, Loss: 0.42475934718412794\n",
      "Epoch 229, Loss: 0.42452283256309487\n",
      "Epoch 230, Loss: 0.4244024016195508\n",
      "Epoch 231, Loss: 0.4242604946441205\n",
      "Epoch 232, Loss: 0.424134367950937\n",
      "Epoch 233, Loss: 0.42372608274856827\n",
      "Epoch 234, Loss: 0.4231492155631465\n",
      "Epoch 235, Loss: 0.4226291943984805\n",
      "Epoch 236, Loss: 0.4221723179942825\n",
      "Epoch 237, Loss: 0.4218955917984354\n",
      "Epoch 238, Loss: 0.4216767834434851\n",
      "Epoch 239, Loss: 0.42145169178243785\n",
      "Epoch 240, Loss: 0.4212605805865247\n",
      "Epoch 241, Loss: 0.42123380253494425\n",
      "Epoch 242, Loss: 0.42125483246926426\n",
      "Epoch 243, Loss: 0.42124070061060576\n",
      "Epoch 244, Loss: 0.42073760533336746\n",
      "Epoch 245, Loss: 0.42002867294257734\n",
      "Epoch 246, Loss: 0.41900436945460057\n",
      "Epoch 247, Loss: 0.41841940783018283\n",
      "Epoch 248, Loss: 0.4184544673382656\n",
      "Epoch 249, Loss: 0.4187823781263808\n",
      "Epoch 250, Loss: 0.41914970907160026\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.24494309094472744\n",
      "Valence RMSE: 0.25626165286818386\n",
      "Arousal RMSE: 0.23307552611198895\n",
      "Test R^2 score: tensor([0.2983, 0.5994], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.44880653850122515\n",
      "Num of epochs: 251\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Epoch 121, Loss: 0.4596922103619757\n",
      "Epoch 122, Loss: 0.45943389956138125\n",
      "Epoch 123, Loss: 0.45877818894117006\n",
      "Epoch 124, Loss: 0.45746538132874076\n",
      "Epoch 125, Loss: 0.4570462803121619\n",
      "Epoch 126, Loss: 0.45696189549769234\n",
      "Epoch 127, Loss: 0.45594920341421274\n",
      "Epoch 128, Loss: 0.45536359747685007\n",
      "Epoch 129, Loss: 0.4553290890752543\n",
      "Epoch 130, Loss: 0.4547195823074053\n",
      "Epoch 131, Loss: 0.45392838323998064\n",
      "Epoch 132, Loss: 0.4537853493595631\n",
      "Epoch 133, Loss: 0.45347437050767975\n",
      "Epoch 134, Loss: 0.45267431273812453\n",
      "Epoch 135, Loss: 0.45228737659373064\n",
      "Epoch 136, Loss: 0.4522154657432981\n",
      "Epoch 137, Loss: 0.45165964487407706\n",
      "Epoch 138, Loss: 0.4510751265462795\n",
      "Epoch 139, Loss: 0.45082650445054473\n",
      "Epoch 140, Loss: 0.4506281098405297\n",
      "Epoch 141, Loss: 0.4501911419656675\n",
      "Epoch 142, Loss: 0.4497001887659326\n",
      "Epoch 143, Loss: 0.44943436133718806\n",
      "Epoch 144, Loss: 0.449226296402944\n",
      "Epoch 145, Loss: 0.44886073897867573\n",
      "Epoch 146, Loss: 0.44844710376304864\n",
      "Epoch 147, Loss: 0.44810507013116724\n",
      "Epoch 148, Loss: 0.44788545862049767\n",
      "Epoch 149, Loss: 0.4476741441009651\n",
      "Epoch 150, Loss: 0.44735861731902055\n",
      "Epoch 151, Loss: 0.4469288720157105\n",
      "Epoch 152, Loss: 0.44655235754178146\n",
      "Epoch 153, Loss: 0.4462857402786373\n",
      "Epoch 154, Loss: 0.4460517369142754\n",
      "Epoch 155, Loss: 0.4457881796184935\n",
      "Epoch 156, Loss: 0.44542210886348255\n",
      "Epoch 157, Loss: 0.4450579467827662\n",
      "Epoch 158, Loss: 0.44473329320345206\n",
      "Epoch 159, Loss: 0.4444482514678638\n",
      "Epoch 160, Loss: 0.4441745674630603\n",
      "Epoch 161, Loss: 0.4439221813722748\n",
      "Epoch 162, Loss: 0.44363357870363757\n",
      "Epoch 163, Loss: 0.4433189071134186\n",
      "Epoch 164, Loss: 0.44295452976444744\n",
      "Epoch 165, Loss: 0.4426058108229731\n",
      "Epoch 166, Loss: 0.4422485787900922\n",
      "Epoch 167, Loss: 0.4419090985167976\n",
      "Epoch 168, Loss: 0.44158054387831963\n",
      "Epoch 169, Loss: 0.4412474388772268\n",
      "Epoch 170, Loss: 0.4409277694136744\n",
      "Epoch 171, Loss: 0.44068045571539705\n",
      "Epoch 172, Loss: 0.4405979418732049\n",
      "Epoch 173, Loss: 0.44094101686109266\n",
      "Epoch 174, Loss: 0.4417368233339534\n",
      "Epoch 175, Loss: 0.4416582519101522\n",
      "Epoch 176, Loss: 0.4396788970821407\n",
      "Epoch 177, Loss: 0.4390195954561405\n",
      "Epoch 178, Loss: 0.4400830988007806\n",
      "Epoch 179, Loss: 0.43931775869233325\n",
      "Epoch 180, Loss: 0.4380032505977321\n",
      "Epoch 181, Loss: 0.4384470974340077\n",
      "Epoch 182, Loss: 0.4385167636581309\n",
      "Epoch 183, Loss: 0.43733043450040754\n",
      "Epoch 184, Loss: 0.4373176399006318\n",
      "Epoch 185, Loss: 0.4375256973621567\n",
      "Epoch 186, Loss: 0.43659322296038683\n",
      "Epoch 187, Loss: 0.43644391045908093\n",
      "Epoch 188, Loss: 0.43663844358015674\n",
      "Epoch 189, Loss: 0.43586286705793914\n",
      "Epoch 190, Loss: 0.4355663072844155\n",
      "Epoch 191, Loss: 0.4357040532214924\n",
      "Epoch 192, Loss: 0.435134007760397\n",
      "Epoch 193, Loss: 0.43474241127906293\n",
      "Epoch 194, Loss: 0.43480482310319635\n",
      "Epoch 195, Loss: 0.43443826104982264\n",
      "Epoch 196, Loss: 0.43390767873021246\n",
      "Epoch 197, Loss: 0.4338897691233524\n",
      "Epoch 198, Loss: 0.43377856877811044\n",
      "Epoch 199, Loss: 0.4331595506415052\n",
      "Epoch 200, Loss: 0.43289415091975497\n",
      "Epoch 201, Loss: 0.43294048069736957\n",
      "Epoch 202, Loss: 0.43262240576772243\n",
      "Epoch 203, Loss: 0.43206541812705346\n",
      "Epoch 204, Loss: 0.43172500614102416\n",
      "Epoch 205, Loss: 0.4316686388133938\n",
      "Epoch 206, Loss: 0.4315179158466364\n",
      "Epoch 207, Loss: 0.4310712381632941\n",
      "Epoch 208, Loss: 0.43059470035496\n",
      "Epoch 209, Loss: 0.4303711221416647\n",
      "Epoch 210, Loss: 0.43033088719595897\n",
      "Epoch 211, Loss: 0.43016580489992484\n",
      "Epoch 212, Loss: 0.429691817522032\n",
      "Epoch 213, Loss: 0.4292099292794459\n",
      "Epoch 214, Loss: 0.4289207205910941\n",
      "Epoch 215, Loss: 0.4287978238495365\n",
      "Epoch 216, Loss: 0.42870603658529494\n",
      "Epoch 217, Loss: 0.42841980940051866\n",
      "Epoch 218, Loss: 0.4278918633922788\n",
      "Epoch 219, Loss: 0.42748307113702383\n",
      "Epoch 220, Loss: 0.4271688511370021\n",
      "Epoch 221, Loss: 0.4269432345934848\n",
      "Epoch 222, Loss: 0.4267651275776002\n",
      "Epoch 223, Loss: 0.42668376428587257\n",
      "Epoch 224, Loss: 0.42653554195413756\n",
      "Epoch 225, Loss: 0.42613293183996703\n",
      "Epoch 226, Loss: 0.4255451632780942\n",
      "Epoch 227, Loss: 0.4251305043179084\n",
      "Epoch 228, Loss: 0.42475934718412794\n",
      "Epoch 229, Loss: 0.42452283256309487\n",
      "Epoch 230, Loss: 0.4244024016195508\n",
      "Epoch 231, Loss: 0.4242604946441205\n",
      "Epoch 232, Loss: 0.424134367950937\n",
      "Epoch 233, Loss: 0.42372608274856827\n",
      "Epoch 234, Loss: 0.4231492155631465\n",
      "Epoch 235, Loss: 0.4226291943984805\n",
      "Epoch 236, Loss: 0.4221723179942825\n",
      "Epoch 237, Loss: 0.4218955917984354\n",
      "Epoch 238, Loss: 0.4216767834434851\n",
      "Epoch 239, Loss: 0.42145169178243785\n",
      "Epoch 240, Loss: 0.4212605805865247\n",
      "Epoch 241, Loss: 0.42123380253494425\n",
      "Epoch 242, Loss: 0.42125483246926426\n",
      "Epoch 243, Loss: 0.42124070061060576\n",
      "Epoch 244, Loss: 0.42073760533336746\n",
      "Epoch 245, Loss: 0.42002867294257734\n",
      "Epoch 246, Loss: 0.41900436945460057\n",
      "Epoch 247, Loss: 0.41841940783018283\n",
      "Epoch 248, Loss: 0.4184544673382656\n",
      "Epoch 249, Loss: 0.4187823781263808\n",
      "Epoch 250, Loss: 0.41914970907160026\n",
      "Epoch 251, Loss: 0.418702737788617\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.24574201317332092\n",
      "Valence RMSE: 0.25813402406415903\n",
      "Arousal RMSE: 0.23269099616744401\n",
      "Test R^2 score: tensor([0.2880, 0.6007], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.44432092855878286\n",
      "Num of epochs: 252\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Epoch 121, Loss: 0.4596922103619757\n",
      "Epoch 122, Loss: 0.45943389956138125\n",
      "Epoch 123, Loss: 0.45877818894117006\n",
      "Epoch 124, Loss: 0.45746538132874076\n",
      "Epoch 125, Loss: 0.4570462803121619\n",
      "Epoch 126, Loss: 0.45696189549769234\n",
      "Epoch 127, Loss: 0.45594920341421274\n",
      "Epoch 128, Loss: 0.45536359747685007\n",
      "Epoch 129, Loss: 0.4553290890752543\n",
      "Epoch 130, Loss: 0.4547195823074053\n",
      "Epoch 131, Loss: 0.45392838323998064\n",
      "Epoch 132, Loss: 0.4537853493595631\n",
      "Epoch 133, Loss: 0.45347437050767975\n",
      "Epoch 134, Loss: 0.45267431273812453\n",
      "Epoch 135, Loss: 0.45228737659373064\n",
      "Epoch 136, Loss: 0.4522154657432981\n",
      "Epoch 137, Loss: 0.45165964487407706\n",
      "Epoch 138, Loss: 0.4510751265462795\n",
      "Epoch 139, Loss: 0.45082650445054473\n",
      "Epoch 140, Loss: 0.4506281098405297\n",
      "Epoch 141, Loss: 0.4501911419656675\n",
      "Epoch 142, Loss: 0.4497001887659326\n",
      "Epoch 143, Loss: 0.44943436133718806\n",
      "Epoch 144, Loss: 0.449226296402944\n",
      "Epoch 145, Loss: 0.44886073897867573\n",
      "Epoch 146, Loss: 0.44844710376304864\n",
      "Epoch 147, Loss: 0.44810507013116724\n",
      "Epoch 148, Loss: 0.44788545862049767\n",
      "Epoch 149, Loss: 0.4476741441009651\n",
      "Epoch 150, Loss: 0.44735861731902055\n",
      "Epoch 151, Loss: 0.4469288720157105\n",
      "Epoch 152, Loss: 0.44655235754178146\n",
      "Epoch 153, Loss: 0.4462857402786373\n",
      "Epoch 154, Loss: 0.4460517369142754\n",
      "Epoch 155, Loss: 0.4457881796184935\n",
      "Epoch 156, Loss: 0.44542210886348255\n",
      "Epoch 157, Loss: 0.4450579467827662\n",
      "Epoch 158, Loss: 0.44473329320345206\n",
      "Epoch 159, Loss: 0.4444482514678638\n",
      "Epoch 160, Loss: 0.4441745674630603\n",
      "Epoch 161, Loss: 0.4439221813722748\n",
      "Epoch 162, Loss: 0.44363357870363757\n",
      "Epoch 163, Loss: 0.4433189071134186\n",
      "Epoch 164, Loss: 0.44295452976444744\n",
      "Epoch 165, Loss: 0.4426058108229731\n",
      "Epoch 166, Loss: 0.4422485787900922\n",
      "Epoch 167, Loss: 0.4419090985167976\n",
      "Epoch 168, Loss: 0.44158054387831963\n",
      "Epoch 169, Loss: 0.4412474388772268\n",
      "Epoch 170, Loss: 0.4409277694136744\n",
      "Epoch 171, Loss: 0.44068045571539705\n",
      "Epoch 172, Loss: 0.4405979418732049\n",
      "Epoch 173, Loss: 0.44094101686109266\n",
      "Epoch 174, Loss: 0.4417368233339534\n",
      "Epoch 175, Loss: 0.4416582519101522\n",
      "Epoch 176, Loss: 0.4396788970821407\n",
      "Epoch 177, Loss: 0.4390195954561405\n",
      "Epoch 178, Loss: 0.4400830988007806\n",
      "Epoch 179, Loss: 0.43931775869233325\n",
      "Epoch 180, Loss: 0.4380032505977321\n",
      "Epoch 181, Loss: 0.4384470974340077\n",
      "Epoch 182, Loss: 0.4385167636581309\n",
      "Epoch 183, Loss: 0.43733043450040754\n",
      "Epoch 184, Loss: 0.4373176399006318\n",
      "Epoch 185, Loss: 0.4375256973621567\n",
      "Epoch 186, Loss: 0.43659322296038683\n",
      "Epoch 187, Loss: 0.43644391045908093\n",
      "Epoch 188, Loss: 0.43663844358015674\n",
      "Epoch 189, Loss: 0.43586286705793914\n",
      "Epoch 190, Loss: 0.4355663072844155\n",
      "Epoch 191, Loss: 0.4357040532214924\n",
      "Epoch 192, Loss: 0.435134007760397\n",
      "Epoch 193, Loss: 0.43474241127906293\n",
      "Epoch 194, Loss: 0.43480482310319635\n",
      "Epoch 195, Loss: 0.43443826104982264\n",
      "Epoch 196, Loss: 0.43390767873021246\n",
      "Epoch 197, Loss: 0.4338897691233524\n",
      "Epoch 198, Loss: 0.43377856877811044\n",
      "Epoch 199, Loss: 0.4331595506415052\n",
      "Epoch 200, Loss: 0.43289415091975497\n",
      "Epoch 201, Loss: 0.43294048069736957\n",
      "Epoch 202, Loss: 0.43262240576772243\n",
      "Epoch 203, Loss: 0.43206541812705346\n",
      "Epoch 204, Loss: 0.43172500614102416\n",
      "Epoch 205, Loss: 0.4316686388133938\n",
      "Epoch 206, Loss: 0.4315179158466364\n",
      "Epoch 207, Loss: 0.4310712381632941\n",
      "Epoch 208, Loss: 0.43059470035496\n",
      "Epoch 209, Loss: 0.4303711221416647\n",
      "Epoch 210, Loss: 0.43033088719595897\n",
      "Epoch 211, Loss: 0.43016580489992484\n",
      "Epoch 212, Loss: 0.429691817522032\n",
      "Epoch 213, Loss: 0.4292099292794459\n",
      "Epoch 214, Loss: 0.4289207205910941\n",
      "Epoch 215, Loss: 0.4287978238495365\n",
      "Epoch 216, Loss: 0.42870603658529494\n",
      "Epoch 217, Loss: 0.42841980940051866\n",
      "Epoch 218, Loss: 0.4278918633922788\n",
      "Epoch 219, Loss: 0.42748307113702383\n",
      "Epoch 220, Loss: 0.4271688511370021\n",
      "Epoch 221, Loss: 0.4269432345934848\n",
      "Epoch 222, Loss: 0.4267651275776002\n",
      "Epoch 223, Loss: 0.42668376428587257\n",
      "Epoch 224, Loss: 0.42653554195413756\n",
      "Epoch 225, Loss: 0.42613293183996703\n",
      "Epoch 226, Loss: 0.4255451632780942\n",
      "Epoch 227, Loss: 0.4251305043179084\n",
      "Epoch 228, Loss: 0.42475934718412794\n",
      "Epoch 229, Loss: 0.42452283256309487\n",
      "Epoch 230, Loss: 0.4244024016195508\n",
      "Epoch 231, Loss: 0.4242604946441205\n",
      "Epoch 232, Loss: 0.424134367950937\n",
      "Epoch 233, Loss: 0.42372608274856827\n",
      "Epoch 234, Loss: 0.4231492155631465\n",
      "Epoch 235, Loss: 0.4226291943984805\n",
      "Epoch 236, Loss: 0.4221723179942825\n",
      "Epoch 237, Loss: 0.4218955917984354\n",
      "Epoch 238, Loss: 0.4216767834434851\n",
      "Epoch 239, Loss: 0.42145169178243785\n",
      "Epoch 240, Loss: 0.4212605805865247\n",
      "Epoch 241, Loss: 0.42123380253494425\n",
      "Epoch 242, Loss: 0.42125483246926426\n",
      "Epoch 243, Loss: 0.42124070061060576\n",
      "Epoch 244, Loss: 0.42073760533336746\n",
      "Epoch 245, Loss: 0.42002867294257734\n",
      "Epoch 246, Loss: 0.41900436945460057\n",
      "Epoch 247, Loss: 0.41841940783018283\n",
      "Epoch 248, Loss: 0.4184544673382656\n",
      "Epoch 249, Loss: 0.4187823781263808\n",
      "Epoch 250, Loss: 0.41914970907160026\n",
      "Epoch 251, Loss: 0.418702737788617\n",
      "Epoch 252, Loss: 0.41762532180686956\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.24553445660883413\n",
      "Valence RMSE: 0.257493756868917\n",
      "Arousal RMSE: 0.23296202252281797\n",
      "Test R^2 score: tensor([0.2915, 0.5998], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4456194840905451\n",
      "Num of epochs: 253\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Epoch 121, Loss: 0.4596922103619757\n",
      "Epoch 122, Loss: 0.45943389956138125\n",
      "Epoch 123, Loss: 0.45877818894117006\n",
      "Epoch 124, Loss: 0.45746538132874076\n",
      "Epoch 125, Loss: 0.4570462803121619\n",
      "Epoch 126, Loss: 0.45696189549769234\n",
      "Epoch 127, Loss: 0.45594920341421274\n",
      "Epoch 128, Loss: 0.45536359747685007\n",
      "Epoch 129, Loss: 0.4553290890752543\n",
      "Epoch 130, Loss: 0.4547195823074053\n",
      "Epoch 131, Loss: 0.45392838323998064\n",
      "Epoch 132, Loss: 0.4537853493595631\n",
      "Epoch 133, Loss: 0.45347437050767975\n",
      "Epoch 134, Loss: 0.45267431273812453\n",
      "Epoch 135, Loss: 0.45228737659373064\n",
      "Epoch 136, Loss: 0.4522154657432981\n",
      "Epoch 137, Loss: 0.45165964487407706\n",
      "Epoch 138, Loss: 0.4510751265462795\n",
      "Epoch 139, Loss: 0.45082650445054473\n",
      "Epoch 140, Loss: 0.4506281098405297\n",
      "Epoch 141, Loss: 0.4501911419656675\n",
      "Epoch 142, Loss: 0.4497001887659326\n",
      "Epoch 143, Loss: 0.44943436133718806\n",
      "Epoch 144, Loss: 0.449226296402944\n",
      "Epoch 145, Loss: 0.44886073897867573\n",
      "Epoch 146, Loss: 0.44844710376304864\n",
      "Epoch 147, Loss: 0.44810507013116724\n",
      "Epoch 148, Loss: 0.44788545862049767\n",
      "Epoch 149, Loss: 0.4476741441009651\n",
      "Epoch 150, Loss: 0.44735861731902055\n",
      "Epoch 151, Loss: 0.4469288720157105\n",
      "Epoch 152, Loss: 0.44655235754178146\n",
      "Epoch 153, Loss: 0.4462857402786373\n",
      "Epoch 154, Loss: 0.4460517369142754\n",
      "Epoch 155, Loss: 0.4457881796184935\n",
      "Epoch 156, Loss: 0.44542210886348255\n",
      "Epoch 157, Loss: 0.4450579467827662\n",
      "Epoch 158, Loss: 0.44473329320345206\n",
      "Epoch 159, Loss: 0.4444482514678638\n",
      "Epoch 160, Loss: 0.4441745674630603\n",
      "Epoch 161, Loss: 0.4439221813722748\n",
      "Epoch 162, Loss: 0.44363357870363757\n",
      "Epoch 163, Loss: 0.4433189071134186\n",
      "Epoch 164, Loss: 0.44295452976444744\n",
      "Epoch 165, Loss: 0.4426058108229731\n",
      "Epoch 166, Loss: 0.4422485787900922\n",
      "Epoch 167, Loss: 0.4419090985167976\n",
      "Epoch 168, Loss: 0.44158054387831963\n",
      "Epoch 169, Loss: 0.4412474388772268\n",
      "Epoch 170, Loss: 0.4409277694136744\n",
      "Epoch 171, Loss: 0.44068045571539705\n",
      "Epoch 172, Loss: 0.4405979418732049\n",
      "Epoch 173, Loss: 0.44094101686109266\n",
      "Epoch 174, Loss: 0.4417368233339534\n",
      "Epoch 175, Loss: 0.4416582519101522\n",
      "Epoch 176, Loss: 0.4396788970821407\n",
      "Epoch 177, Loss: 0.4390195954561405\n",
      "Epoch 178, Loss: 0.4400830988007806\n",
      "Epoch 179, Loss: 0.43931775869233325\n",
      "Epoch 180, Loss: 0.4380032505977321\n",
      "Epoch 181, Loss: 0.4384470974340077\n",
      "Epoch 182, Loss: 0.4385167636581309\n",
      "Epoch 183, Loss: 0.43733043450040754\n",
      "Epoch 184, Loss: 0.4373176399006318\n",
      "Epoch 185, Loss: 0.4375256973621567\n",
      "Epoch 186, Loss: 0.43659322296038683\n",
      "Epoch 187, Loss: 0.43644391045908093\n",
      "Epoch 188, Loss: 0.43663844358015674\n",
      "Epoch 189, Loss: 0.43586286705793914\n",
      "Epoch 190, Loss: 0.4355663072844155\n",
      "Epoch 191, Loss: 0.4357040532214924\n",
      "Epoch 192, Loss: 0.435134007760397\n",
      "Epoch 193, Loss: 0.43474241127906293\n",
      "Epoch 194, Loss: 0.43480482310319635\n",
      "Epoch 195, Loss: 0.43443826104982264\n",
      "Epoch 196, Loss: 0.43390767873021246\n",
      "Epoch 197, Loss: 0.4338897691233524\n",
      "Epoch 198, Loss: 0.43377856877811044\n",
      "Epoch 199, Loss: 0.4331595506415052\n",
      "Epoch 200, Loss: 0.43289415091975497\n",
      "Epoch 201, Loss: 0.43294048069736957\n",
      "Epoch 202, Loss: 0.43262240576772243\n",
      "Epoch 203, Loss: 0.43206541812705346\n",
      "Epoch 204, Loss: 0.43172500614102416\n",
      "Epoch 205, Loss: 0.4316686388133938\n",
      "Epoch 206, Loss: 0.4315179158466364\n",
      "Epoch 207, Loss: 0.4310712381632941\n",
      "Epoch 208, Loss: 0.43059470035496\n",
      "Epoch 209, Loss: 0.4303711221416647\n",
      "Epoch 210, Loss: 0.43033088719595897\n",
      "Epoch 211, Loss: 0.43016580489992484\n",
      "Epoch 212, Loss: 0.429691817522032\n",
      "Epoch 213, Loss: 0.4292099292794459\n",
      "Epoch 214, Loss: 0.4289207205910941\n",
      "Epoch 215, Loss: 0.4287978238495365\n",
      "Epoch 216, Loss: 0.42870603658529494\n",
      "Epoch 217, Loss: 0.42841980940051866\n",
      "Epoch 218, Loss: 0.4278918633922788\n",
      "Epoch 219, Loss: 0.42748307113702383\n",
      "Epoch 220, Loss: 0.4271688511370021\n",
      "Epoch 221, Loss: 0.4269432345934848\n",
      "Epoch 222, Loss: 0.4267651275776002\n",
      "Epoch 223, Loss: 0.42668376428587257\n",
      "Epoch 224, Loss: 0.42653554195413756\n",
      "Epoch 225, Loss: 0.42613293183996703\n",
      "Epoch 226, Loss: 0.4255451632780942\n",
      "Epoch 227, Loss: 0.4251305043179084\n",
      "Epoch 228, Loss: 0.42475934718412794\n",
      "Epoch 229, Loss: 0.42452283256309487\n",
      "Epoch 230, Loss: 0.4244024016195508\n",
      "Epoch 231, Loss: 0.4242604946441205\n",
      "Epoch 232, Loss: 0.424134367950937\n",
      "Epoch 233, Loss: 0.42372608274856827\n",
      "Epoch 234, Loss: 0.4231492155631465\n",
      "Epoch 235, Loss: 0.4226291943984805\n",
      "Epoch 236, Loss: 0.4221723179942825\n",
      "Epoch 237, Loss: 0.4218955917984354\n",
      "Epoch 238, Loss: 0.4216767834434851\n",
      "Epoch 239, Loss: 0.42145169178243785\n",
      "Epoch 240, Loss: 0.4212605805865247\n",
      "Epoch 241, Loss: 0.42123380253494425\n",
      "Epoch 242, Loss: 0.42125483246926426\n",
      "Epoch 243, Loss: 0.42124070061060576\n",
      "Epoch 244, Loss: 0.42073760533336746\n",
      "Epoch 245, Loss: 0.42002867294257734\n",
      "Epoch 246, Loss: 0.41900436945460057\n",
      "Epoch 247, Loss: 0.41841940783018283\n",
      "Epoch 248, Loss: 0.4184544673382656\n",
      "Epoch 249, Loss: 0.4187823781263808\n",
      "Epoch 250, Loss: 0.41914970907160026\n",
      "Epoch 251, Loss: 0.418702737788617\n",
      "Epoch 252, Loss: 0.41762532180686956\n",
      "Epoch 253, Loss: 0.41665732452727816\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2459625080479023\n",
      "Valence RMSE: 0.2577976538277949\n",
      "Arousal RMSE: 0.23352832892673478\n",
      "Test R^2 score: tensor([0.2898, 0.5978], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4438086512260808\n",
      "Num of epochs: 254\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Epoch 121, Loss: 0.4596922103619757\n",
      "Epoch 122, Loss: 0.45943389956138125\n",
      "Epoch 123, Loss: 0.45877818894117006\n",
      "Epoch 124, Loss: 0.45746538132874076\n",
      "Epoch 125, Loss: 0.4570462803121619\n",
      "Epoch 126, Loss: 0.45696189549769234\n",
      "Epoch 127, Loss: 0.45594920341421274\n",
      "Epoch 128, Loss: 0.45536359747685007\n",
      "Epoch 129, Loss: 0.4553290890752543\n",
      "Epoch 130, Loss: 0.4547195823074053\n",
      "Epoch 131, Loss: 0.45392838323998064\n",
      "Epoch 132, Loss: 0.4537853493595631\n",
      "Epoch 133, Loss: 0.45347437050767975\n",
      "Epoch 134, Loss: 0.45267431273812453\n",
      "Epoch 135, Loss: 0.45228737659373064\n",
      "Epoch 136, Loss: 0.4522154657432981\n",
      "Epoch 137, Loss: 0.45165964487407706\n",
      "Epoch 138, Loss: 0.4510751265462795\n",
      "Epoch 139, Loss: 0.45082650445054473\n",
      "Epoch 140, Loss: 0.4506281098405297\n",
      "Epoch 141, Loss: 0.4501911419656675\n",
      "Epoch 142, Loss: 0.4497001887659326\n",
      "Epoch 143, Loss: 0.44943436133718806\n",
      "Epoch 144, Loss: 0.449226296402944\n",
      "Epoch 145, Loss: 0.44886073897867573\n",
      "Epoch 146, Loss: 0.44844710376304864\n",
      "Epoch 147, Loss: 0.44810507013116724\n",
      "Epoch 148, Loss: 0.44788545862049767\n",
      "Epoch 149, Loss: 0.4476741441009651\n",
      "Epoch 150, Loss: 0.44735861731902055\n",
      "Epoch 151, Loss: 0.4469288720157105\n",
      "Epoch 152, Loss: 0.44655235754178146\n",
      "Epoch 153, Loss: 0.4462857402786373\n",
      "Epoch 154, Loss: 0.4460517369142754\n",
      "Epoch 155, Loss: 0.4457881796184935\n",
      "Epoch 156, Loss: 0.44542210886348255\n",
      "Epoch 157, Loss: 0.4450579467827662\n",
      "Epoch 158, Loss: 0.44473329320345206\n",
      "Epoch 159, Loss: 0.4444482514678638\n",
      "Epoch 160, Loss: 0.4441745674630603\n",
      "Epoch 161, Loss: 0.4439221813722748\n",
      "Epoch 162, Loss: 0.44363357870363757\n",
      "Epoch 163, Loss: 0.4433189071134186\n",
      "Epoch 164, Loss: 0.44295452976444744\n",
      "Epoch 165, Loss: 0.4426058108229731\n",
      "Epoch 166, Loss: 0.4422485787900922\n",
      "Epoch 167, Loss: 0.4419090985167976\n",
      "Epoch 168, Loss: 0.44158054387831963\n",
      "Epoch 169, Loss: 0.4412474388772268\n",
      "Epoch 170, Loss: 0.4409277694136744\n",
      "Epoch 171, Loss: 0.44068045571539705\n",
      "Epoch 172, Loss: 0.4405979418732049\n",
      "Epoch 173, Loss: 0.44094101686109266\n",
      "Epoch 174, Loss: 0.4417368233339534\n",
      "Epoch 175, Loss: 0.4416582519101522\n",
      "Epoch 176, Loss: 0.4396788970821407\n",
      "Epoch 177, Loss: 0.4390195954561405\n",
      "Epoch 178, Loss: 0.4400830988007806\n",
      "Epoch 179, Loss: 0.43931775869233325\n",
      "Epoch 180, Loss: 0.4380032505977321\n",
      "Epoch 181, Loss: 0.4384470974340077\n",
      "Epoch 182, Loss: 0.4385167636581309\n",
      "Epoch 183, Loss: 0.43733043450040754\n",
      "Epoch 184, Loss: 0.4373176399006318\n",
      "Epoch 185, Loss: 0.4375256973621567\n",
      "Epoch 186, Loss: 0.43659322296038683\n",
      "Epoch 187, Loss: 0.43644391045908093\n",
      "Epoch 188, Loss: 0.43663844358015674\n",
      "Epoch 189, Loss: 0.43586286705793914\n",
      "Epoch 190, Loss: 0.4355663072844155\n",
      "Epoch 191, Loss: 0.4357040532214924\n",
      "Epoch 192, Loss: 0.435134007760397\n",
      "Epoch 193, Loss: 0.43474241127906293\n",
      "Epoch 194, Loss: 0.43480482310319635\n",
      "Epoch 195, Loss: 0.43443826104982264\n",
      "Epoch 196, Loss: 0.43390767873021246\n",
      "Epoch 197, Loss: 0.4338897691233524\n",
      "Epoch 198, Loss: 0.43377856877811044\n",
      "Epoch 199, Loss: 0.4331595506415052\n",
      "Epoch 200, Loss: 0.43289415091975497\n",
      "Epoch 201, Loss: 0.43294048069736957\n",
      "Epoch 202, Loss: 0.43262240576772243\n",
      "Epoch 203, Loss: 0.43206541812705346\n",
      "Epoch 204, Loss: 0.43172500614102416\n",
      "Epoch 205, Loss: 0.4316686388133938\n",
      "Epoch 206, Loss: 0.4315179158466364\n",
      "Epoch 207, Loss: 0.4310712381632941\n",
      "Epoch 208, Loss: 0.43059470035496\n",
      "Epoch 209, Loss: 0.4303711221416647\n",
      "Epoch 210, Loss: 0.43033088719595897\n",
      "Epoch 211, Loss: 0.43016580489992484\n",
      "Epoch 212, Loss: 0.429691817522032\n",
      "Epoch 213, Loss: 0.4292099292794459\n",
      "Epoch 214, Loss: 0.4289207205910941\n",
      "Epoch 215, Loss: 0.4287978238495365\n",
      "Epoch 216, Loss: 0.42870603658529494\n",
      "Epoch 217, Loss: 0.42841980940051866\n",
      "Epoch 218, Loss: 0.4278918633922788\n",
      "Epoch 219, Loss: 0.42748307113702383\n",
      "Epoch 220, Loss: 0.4271688511370021\n",
      "Epoch 221, Loss: 0.4269432345934848\n",
      "Epoch 222, Loss: 0.4267651275776002\n",
      "Epoch 223, Loss: 0.42668376428587257\n",
      "Epoch 224, Loss: 0.42653554195413756\n",
      "Epoch 225, Loss: 0.42613293183996703\n",
      "Epoch 226, Loss: 0.4255451632780942\n",
      "Epoch 227, Loss: 0.4251305043179084\n",
      "Epoch 228, Loss: 0.42475934718412794\n",
      "Epoch 229, Loss: 0.42452283256309487\n",
      "Epoch 230, Loss: 0.4244024016195508\n",
      "Epoch 231, Loss: 0.4242604946441205\n",
      "Epoch 232, Loss: 0.424134367950937\n",
      "Epoch 233, Loss: 0.42372608274856827\n",
      "Epoch 234, Loss: 0.4231492155631465\n",
      "Epoch 235, Loss: 0.4226291943984805\n",
      "Epoch 236, Loss: 0.4221723179942825\n",
      "Epoch 237, Loss: 0.4218955917984354\n",
      "Epoch 238, Loss: 0.4216767834434851\n",
      "Epoch 239, Loss: 0.42145169178243785\n",
      "Epoch 240, Loss: 0.4212605805865247\n",
      "Epoch 241, Loss: 0.42123380253494425\n",
      "Epoch 242, Loss: 0.42125483246926426\n",
      "Epoch 243, Loss: 0.42124070061060576\n",
      "Epoch 244, Loss: 0.42073760533336746\n",
      "Epoch 245, Loss: 0.42002867294257734\n",
      "Epoch 246, Loss: 0.41900436945460057\n",
      "Epoch 247, Loss: 0.41841940783018283\n",
      "Epoch 248, Loss: 0.4184544673382656\n",
      "Epoch 249, Loss: 0.4187823781263808\n",
      "Epoch 250, Loss: 0.41914970907160026\n",
      "Epoch 251, Loss: 0.418702737788617\n",
      "Epoch 252, Loss: 0.41762532180686956\n",
      "Epoch 253, Loss: 0.41665732452727816\n",
      "Epoch 254, Loss: 0.4164608109104539\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.24668399624236695\n",
      "Valence RMSE: 0.2588250935964413\n",
      "Arousal RMSE: 0.23391357149383538\n",
      "Test R^2 score: tensor([0.2841, 0.5965], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.44030856497509774\n",
      "Num of epochs: 255\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Epoch 121, Loss: 0.4596922103619757\n",
      "Epoch 122, Loss: 0.45943389956138125\n",
      "Epoch 123, Loss: 0.45877818894117006\n",
      "Epoch 124, Loss: 0.45746538132874076\n",
      "Epoch 125, Loss: 0.4570462803121619\n",
      "Epoch 126, Loss: 0.45696189549769234\n",
      "Epoch 127, Loss: 0.45594920341421274\n",
      "Epoch 128, Loss: 0.45536359747685007\n",
      "Epoch 129, Loss: 0.4553290890752543\n",
      "Epoch 130, Loss: 0.4547195823074053\n",
      "Epoch 131, Loss: 0.45392838323998064\n",
      "Epoch 132, Loss: 0.4537853493595631\n",
      "Epoch 133, Loss: 0.45347437050767975\n",
      "Epoch 134, Loss: 0.45267431273812453\n",
      "Epoch 135, Loss: 0.45228737659373064\n",
      "Epoch 136, Loss: 0.4522154657432981\n",
      "Epoch 137, Loss: 0.45165964487407706\n",
      "Epoch 138, Loss: 0.4510751265462795\n",
      "Epoch 139, Loss: 0.45082650445054473\n",
      "Epoch 140, Loss: 0.4506281098405297\n",
      "Epoch 141, Loss: 0.4501911419656675\n",
      "Epoch 142, Loss: 0.4497001887659326\n",
      "Epoch 143, Loss: 0.44943436133718806\n",
      "Epoch 144, Loss: 0.449226296402944\n",
      "Epoch 145, Loss: 0.44886073897867573\n",
      "Epoch 146, Loss: 0.44844710376304864\n",
      "Epoch 147, Loss: 0.44810507013116724\n",
      "Epoch 148, Loss: 0.44788545862049767\n",
      "Epoch 149, Loss: 0.4476741441009651\n",
      "Epoch 150, Loss: 0.44735861731902055\n",
      "Epoch 151, Loss: 0.4469288720157105\n",
      "Epoch 152, Loss: 0.44655235754178146\n",
      "Epoch 153, Loss: 0.4462857402786373\n",
      "Epoch 154, Loss: 0.4460517369142754\n",
      "Epoch 155, Loss: 0.4457881796184935\n",
      "Epoch 156, Loss: 0.44542210886348255\n",
      "Epoch 157, Loss: 0.4450579467827662\n",
      "Epoch 158, Loss: 0.44473329320345206\n",
      "Epoch 159, Loss: 0.4444482514678638\n",
      "Epoch 160, Loss: 0.4441745674630603\n",
      "Epoch 161, Loss: 0.4439221813722748\n",
      "Epoch 162, Loss: 0.44363357870363757\n",
      "Epoch 163, Loss: 0.4433189071134186\n",
      "Epoch 164, Loss: 0.44295452976444744\n",
      "Epoch 165, Loss: 0.4426058108229731\n",
      "Epoch 166, Loss: 0.4422485787900922\n",
      "Epoch 167, Loss: 0.4419090985167976\n",
      "Epoch 168, Loss: 0.44158054387831963\n",
      "Epoch 169, Loss: 0.4412474388772268\n",
      "Epoch 170, Loss: 0.4409277694136744\n",
      "Epoch 171, Loss: 0.44068045571539705\n",
      "Epoch 172, Loss: 0.4405979418732049\n",
      "Epoch 173, Loss: 0.44094101686109266\n",
      "Epoch 174, Loss: 0.4417368233339534\n",
      "Epoch 175, Loss: 0.4416582519101522\n",
      "Epoch 176, Loss: 0.4396788970821407\n",
      "Epoch 177, Loss: 0.4390195954561405\n",
      "Epoch 178, Loss: 0.4400830988007806\n",
      "Epoch 179, Loss: 0.43931775869233325\n",
      "Epoch 180, Loss: 0.4380032505977321\n",
      "Epoch 181, Loss: 0.4384470974340077\n",
      "Epoch 182, Loss: 0.4385167636581309\n",
      "Epoch 183, Loss: 0.43733043450040754\n",
      "Epoch 184, Loss: 0.4373176399006318\n",
      "Epoch 185, Loss: 0.4375256973621567\n",
      "Epoch 186, Loss: 0.43659322296038683\n",
      "Epoch 187, Loss: 0.43644391045908093\n",
      "Epoch 188, Loss: 0.43663844358015674\n",
      "Epoch 189, Loss: 0.43586286705793914\n",
      "Epoch 190, Loss: 0.4355663072844155\n",
      "Epoch 191, Loss: 0.4357040532214924\n",
      "Epoch 192, Loss: 0.435134007760397\n",
      "Epoch 193, Loss: 0.43474241127906293\n",
      "Epoch 194, Loss: 0.43480482310319635\n",
      "Epoch 195, Loss: 0.43443826104982264\n",
      "Epoch 196, Loss: 0.43390767873021246\n",
      "Epoch 197, Loss: 0.4338897691233524\n",
      "Epoch 198, Loss: 0.43377856877811044\n",
      "Epoch 199, Loss: 0.4331595506415052\n",
      "Epoch 200, Loss: 0.43289415091975497\n",
      "Epoch 201, Loss: 0.43294048069736957\n",
      "Epoch 202, Loss: 0.43262240576772243\n",
      "Epoch 203, Loss: 0.43206541812705346\n",
      "Epoch 204, Loss: 0.43172500614102416\n",
      "Epoch 205, Loss: 0.4316686388133938\n",
      "Epoch 206, Loss: 0.4315179158466364\n",
      "Epoch 207, Loss: 0.4310712381632941\n",
      "Epoch 208, Loss: 0.43059470035496\n",
      "Epoch 209, Loss: 0.4303711221416647\n",
      "Epoch 210, Loss: 0.43033088719595897\n",
      "Epoch 211, Loss: 0.43016580489992484\n",
      "Epoch 212, Loss: 0.429691817522032\n",
      "Epoch 213, Loss: 0.4292099292794459\n",
      "Epoch 214, Loss: 0.4289207205910941\n",
      "Epoch 215, Loss: 0.4287978238495365\n",
      "Epoch 216, Loss: 0.42870603658529494\n",
      "Epoch 217, Loss: 0.42841980940051866\n",
      "Epoch 218, Loss: 0.4278918633922788\n",
      "Epoch 219, Loss: 0.42748307113702383\n",
      "Epoch 220, Loss: 0.4271688511370021\n",
      "Epoch 221, Loss: 0.4269432345934848\n",
      "Epoch 222, Loss: 0.4267651275776002\n",
      "Epoch 223, Loss: 0.42668376428587257\n",
      "Epoch 224, Loss: 0.42653554195413756\n",
      "Epoch 225, Loss: 0.42613293183996703\n",
      "Epoch 226, Loss: 0.4255451632780942\n",
      "Epoch 227, Loss: 0.4251305043179084\n",
      "Epoch 228, Loss: 0.42475934718412794\n",
      "Epoch 229, Loss: 0.42452283256309487\n",
      "Epoch 230, Loss: 0.4244024016195508\n",
      "Epoch 231, Loss: 0.4242604946441205\n",
      "Epoch 232, Loss: 0.424134367950937\n",
      "Epoch 233, Loss: 0.42372608274856827\n",
      "Epoch 234, Loss: 0.4231492155631465\n",
      "Epoch 235, Loss: 0.4226291943984805\n",
      "Epoch 236, Loss: 0.4221723179942825\n",
      "Epoch 237, Loss: 0.4218955917984354\n",
      "Epoch 238, Loss: 0.4216767834434851\n",
      "Epoch 239, Loss: 0.42145169178243785\n",
      "Epoch 240, Loss: 0.4212605805865247\n",
      "Epoch 241, Loss: 0.42123380253494425\n",
      "Epoch 242, Loss: 0.42125483246926426\n",
      "Epoch 243, Loss: 0.42124070061060576\n",
      "Epoch 244, Loss: 0.42073760533336746\n",
      "Epoch 245, Loss: 0.42002867294257734\n",
      "Epoch 246, Loss: 0.41900436945460057\n",
      "Epoch 247, Loss: 0.41841940783018283\n",
      "Epoch 248, Loss: 0.4184544673382656\n",
      "Epoch 249, Loss: 0.4187823781263808\n",
      "Epoch 250, Loss: 0.41914970907160026\n",
      "Epoch 251, Loss: 0.418702737788617\n",
      "Epoch 252, Loss: 0.41762532180686956\n",
      "Epoch 253, Loss: 0.41665732452727816\n",
      "Epoch 254, Loss: 0.4164608109104539\n",
      "Epoch 255, Loss: 0.4169362050698102\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.24585715198170513\n",
      "Valence RMSE: 0.2563749828272391\n",
      "Arousal RMSE: 0.23486878579633222\n",
      "Test R^2 score: tensor([0.2976, 0.5932], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4454017941681436\n",
      "Num of epochs: 256\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Epoch 121, Loss: 0.4596922103619757\n",
      "Epoch 122, Loss: 0.45943389956138125\n",
      "Epoch 123, Loss: 0.45877818894117006\n",
      "Epoch 124, Loss: 0.45746538132874076\n",
      "Epoch 125, Loss: 0.4570462803121619\n",
      "Epoch 126, Loss: 0.45696189549769234\n",
      "Epoch 127, Loss: 0.45594920341421274\n",
      "Epoch 128, Loss: 0.45536359747685007\n",
      "Epoch 129, Loss: 0.4553290890752543\n",
      "Epoch 130, Loss: 0.4547195823074053\n",
      "Epoch 131, Loss: 0.45392838323998064\n",
      "Epoch 132, Loss: 0.4537853493595631\n",
      "Epoch 133, Loss: 0.45347437050767975\n",
      "Epoch 134, Loss: 0.45267431273812453\n",
      "Epoch 135, Loss: 0.45228737659373064\n",
      "Epoch 136, Loss: 0.4522154657432981\n",
      "Epoch 137, Loss: 0.45165964487407706\n",
      "Epoch 138, Loss: 0.4510751265462795\n",
      "Epoch 139, Loss: 0.45082650445054473\n",
      "Epoch 140, Loss: 0.4506281098405297\n",
      "Epoch 141, Loss: 0.4501911419656675\n",
      "Epoch 142, Loss: 0.4497001887659326\n",
      "Epoch 143, Loss: 0.44943436133718806\n",
      "Epoch 144, Loss: 0.449226296402944\n",
      "Epoch 145, Loss: 0.44886073897867573\n",
      "Epoch 146, Loss: 0.44844710376304864\n",
      "Epoch 147, Loss: 0.44810507013116724\n",
      "Epoch 148, Loss: 0.44788545862049767\n",
      "Epoch 149, Loss: 0.4476741441009651\n",
      "Epoch 150, Loss: 0.44735861731902055\n",
      "Epoch 151, Loss: 0.4469288720157105\n",
      "Epoch 152, Loss: 0.44655235754178146\n",
      "Epoch 153, Loss: 0.4462857402786373\n",
      "Epoch 154, Loss: 0.4460517369142754\n",
      "Epoch 155, Loss: 0.4457881796184935\n",
      "Epoch 156, Loss: 0.44542210886348255\n",
      "Epoch 157, Loss: 0.4450579467827662\n",
      "Epoch 158, Loss: 0.44473329320345206\n",
      "Epoch 159, Loss: 0.4444482514678638\n",
      "Epoch 160, Loss: 0.4441745674630603\n",
      "Epoch 161, Loss: 0.4439221813722748\n",
      "Epoch 162, Loss: 0.44363357870363757\n",
      "Epoch 163, Loss: 0.4433189071134186\n",
      "Epoch 164, Loss: 0.44295452976444744\n",
      "Epoch 165, Loss: 0.4426058108229731\n",
      "Epoch 166, Loss: 0.4422485787900922\n",
      "Epoch 167, Loss: 0.4419090985167976\n",
      "Epoch 168, Loss: 0.44158054387831963\n",
      "Epoch 169, Loss: 0.4412474388772268\n",
      "Epoch 170, Loss: 0.4409277694136744\n",
      "Epoch 171, Loss: 0.44068045571539705\n",
      "Epoch 172, Loss: 0.4405979418732049\n",
      "Epoch 173, Loss: 0.44094101686109266\n",
      "Epoch 174, Loss: 0.4417368233339534\n",
      "Epoch 175, Loss: 0.4416582519101522\n",
      "Epoch 176, Loss: 0.4396788970821407\n",
      "Epoch 177, Loss: 0.4390195954561405\n",
      "Epoch 178, Loss: 0.4400830988007806\n",
      "Epoch 179, Loss: 0.43931775869233325\n",
      "Epoch 180, Loss: 0.4380032505977321\n",
      "Epoch 181, Loss: 0.4384470974340077\n",
      "Epoch 182, Loss: 0.4385167636581309\n",
      "Epoch 183, Loss: 0.43733043450040754\n",
      "Epoch 184, Loss: 0.4373176399006318\n",
      "Epoch 185, Loss: 0.4375256973621567\n",
      "Epoch 186, Loss: 0.43659322296038683\n",
      "Epoch 187, Loss: 0.43644391045908093\n",
      "Epoch 188, Loss: 0.43663844358015674\n",
      "Epoch 189, Loss: 0.43586286705793914\n",
      "Epoch 190, Loss: 0.4355663072844155\n",
      "Epoch 191, Loss: 0.4357040532214924\n",
      "Epoch 192, Loss: 0.435134007760397\n",
      "Epoch 193, Loss: 0.43474241127906293\n",
      "Epoch 194, Loss: 0.43480482310319635\n",
      "Epoch 195, Loss: 0.43443826104982264\n",
      "Epoch 196, Loss: 0.43390767873021246\n",
      "Epoch 197, Loss: 0.4338897691233524\n",
      "Epoch 198, Loss: 0.43377856877811044\n",
      "Epoch 199, Loss: 0.4331595506415052\n",
      "Epoch 200, Loss: 0.43289415091975497\n",
      "Epoch 201, Loss: 0.43294048069736957\n",
      "Epoch 202, Loss: 0.43262240576772243\n",
      "Epoch 203, Loss: 0.43206541812705346\n",
      "Epoch 204, Loss: 0.43172500614102416\n",
      "Epoch 205, Loss: 0.4316686388133938\n",
      "Epoch 206, Loss: 0.4315179158466364\n",
      "Epoch 207, Loss: 0.4310712381632941\n",
      "Epoch 208, Loss: 0.43059470035496\n",
      "Epoch 209, Loss: 0.4303711221416647\n",
      "Epoch 210, Loss: 0.43033088719595897\n",
      "Epoch 211, Loss: 0.43016580489992484\n",
      "Epoch 212, Loss: 0.429691817522032\n",
      "Epoch 213, Loss: 0.4292099292794459\n",
      "Epoch 214, Loss: 0.4289207205910941\n",
      "Epoch 215, Loss: 0.4287978238495365\n",
      "Epoch 216, Loss: 0.42870603658529494\n",
      "Epoch 217, Loss: 0.42841980940051866\n",
      "Epoch 218, Loss: 0.4278918633922788\n",
      "Epoch 219, Loss: 0.42748307113702383\n",
      "Epoch 220, Loss: 0.4271688511370021\n",
      "Epoch 221, Loss: 0.4269432345934848\n",
      "Epoch 222, Loss: 0.4267651275776002\n",
      "Epoch 223, Loss: 0.42668376428587257\n",
      "Epoch 224, Loss: 0.42653554195413756\n",
      "Epoch 225, Loss: 0.42613293183996703\n",
      "Epoch 226, Loss: 0.4255451632780942\n",
      "Epoch 227, Loss: 0.4251305043179084\n",
      "Epoch 228, Loss: 0.42475934718412794\n",
      "Epoch 229, Loss: 0.42452283256309487\n",
      "Epoch 230, Loss: 0.4244024016195508\n",
      "Epoch 231, Loss: 0.4242604946441205\n",
      "Epoch 232, Loss: 0.424134367950937\n",
      "Epoch 233, Loss: 0.42372608274856827\n",
      "Epoch 234, Loss: 0.4231492155631465\n",
      "Epoch 235, Loss: 0.4226291943984805\n",
      "Epoch 236, Loss: 0.4221723179942825\n",
      "Epoch 237, Loss: 0.4218955917984354\n",
      "Epoch 238, Loss: 0.4216767834434851\n",
      "Epoch 239, Loss: 0.42145169178243785\n",
      "Epoch 240, Loss: 0.4212605805865247\n",
      "Epoch 241, Loss: 0.42123380253494425\n",
      "Epoch 242, Loss: 0.42125483246926426\n",
      "Epoch 243, Loss: 0.42124070061060576\n",
      "Epoch 244, Loss: 0.42073760533336746\n",
      "Epoch 245, Loss: 0.42002867294257734\n",
      "Epoch 246, Loss: 0.41900436945460057\n",
      "Epoch 247, Loss: 0.41841940783018283\n",
      "Epoch 248, Loss: 0.4184544673382656\n",
      "Epoch 249, Loss: 0.4187823781263808\n",
      "Epoch 250, Loss: 0.41914970907160026\n",
      "Epoch 251, Loss: 0.418702737788617\n",
      "Epoch 252, Loss: 0.41762532180686956\n",
      "Epoch 253, Loss: 0.41665732452727816\n",
      "Epoch 254, Loss: 0.4164608109104539\n",
      "Epoch 255, Loss: 0.4169362050698102\n",
      "Epoch 256, Loss: 0.4170439641107097\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.24688697387661346\n",
      "Valence RMSE: 0.25894447895929956\n",
      "Arousal RMSE: 0.23420954838861732\n",
      "Test R^2 score: tensor([0.2835, 0.5955], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4394673793979686\n",
      "Num of epochs: 257\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Epoch 121, Loss: 0.4596922103619757\n",
      "Epoch 122, Loss: 0.45943389956138125\n",
      "Epoch 123, Loss: 0.45877818894117006\n",
      "Epoch 124, Loss: 0.45746538132874076\n",
      "Epoch 125, Loss: 0.4570462803121619\n",
      "Epoch 126, Loss: 0.45696189549769234\n",
      "Epoch 127, Loss: 0.45594920341421274\n",
      "Epoch 128, Loss: 0.45536359747685007\n",
      "Epoch 129, Loss: 0.4553290890752543\n",
      "Epoch 130, Loss: 0.4547195823074053\n",
      "Epoch 131, Loss: 0.45392838323998064\n",
      "Epoch 132, Loss: 0.4537853493595631\n",
      "Epoch 133, Loss: 0.45347437050767975\n",
      "Epoch 134, Loss: 0.45267431273812453\n",
      "Epoch 135, Loss: 0.45228737659373064\n",
      "Epoch 136, Loss: 0.4522154657432981\n",
      "Epoch 137, Loss: 0.45165964487407706\n",
      "Epoch 138, Loss: 0.4510751265462795\n",
      "Epoch 139, Loss: 0.45082650445054473\n",
      "Epoch 140, Loss: 0.4506281098405297\n",
      "Epoch 141, Loss: 0.4501911419656675\n",
      "Epoch 142, Loss: 0.4497001887659326\n",
      "Epoch 143, Loss: 0.44943436133718806\n",
      "Epoch 144, Loss: 0.449226296402944\n",
      "Epoch 145, Loss: 0.44886073897867573\n",
      "Epoch 146, Loss: 0.44844710376304864\n",
      "Epoch 147, Loss: 0.44810507013116724\n",
      "Epoch 148, Loss: 0.44788545862049767\n",
      "Epoch 149, Loss: 0.4476741441009651\n",
      "Epoch 150, Loss: 0.44735861731902055\n",
      "Epoch 151, Loss: 0.4469288720157105\n",
      "Epoch 152, Loss: 0.44655235754178146\n",
      "Epoch 153, Loss: 0.4462857402786373\n",
      "Epoch 154, Loss: 0.4460517369142754\n",
      "Epoch 155, Loss: 0.4457881796184935\n",
      "Epoch 156, Loss: 0.44542210886348255\n",
      "Epoch 157, Loss: 0.4450579467827662\n",
      "Epoch 158, Loss: 0.44473329320345206\n",
      "Epoch 159, Loss: 0.4444482514678638\n",
      "Epoch 160, Loss: 0.4441745674630603\n",
      "Epoch 161, Loss: 0.4439221813722748\n",
      "Epoch 162, Loss: 0.44363357870363757\n",
      "Epoch 163, Loss: 0.4433189071134186\n",
      "Epoch 164, Loss: 0.44295452976444744\n",
      "Epoch 165, Loss: 0.4426058108229731\n",
      "Epoch 166, Loss: 0.4422485787900922\n",
      "Epoch 167, Loss: 0.4419090985167976\n",
      "Epoch 168, Loss: 0.44158054387831963\n",
      "Epoch 169, Loss: 0.4412474388772268\n",
      "Epoch 170, Loss: 0.4409277694136744\n",
      "Epoch 171, Loss: 0.44068045571539705\n",
      "Epoch 172, Loss: 0.4405979418732049\n",
      "Epoch 173, Loss: 0.44094101686109266\n",
      "Epoch 174, Loss: 0.4417368233339534\n",
      "Epoch 175, Loss: 0.4416582519101522\n",
      "Epoch 176, Loss: 0.4396788970821407\n",
      "Epoch 177, Loss: 0.4390195954561405\n",
      "Epoch 178, Loss: 0.4400830988007806\n",
      "Epoch 179, Loss: 0.43931775869233325\n",
      "Epoch 180, Loss: 0.4380032505977321\n",
      "Epoch 181, Loss: 0.4384470974340077\n",
      "Epoch 182, Loss: 0.4385167636581309\n",
      "Epoch 183, Loss: 0.43733043450040754\n",
      "Epoch 184, Loss: 0.4373176399006318\n",
      "Epoch 185, Loss: 0.4375256973621567\n",
      "Epoch 186, Loss: 0.43659322296038683\n",
      "Epoch 187, Loss: 0.43644391045908093\n",
      "Epoch 188, Loss: 0.43663844358015674\n",
      "Epoch 189, Loss: 0.43586286705793914\n",
      "Epoch 190, Loss: 0.4355663072844155\n",
      "Epoch 191, Loss: 0.4357040532214924\n",
      "Epoch 192, Loss: 0.435134007760397\n",
      "Epoch 193, Loss: 0.43474241127906293\n",
      "Epoch 194, Loss: 0.43480482310319635\n",
      "Epoch 195, Loss: 0.43443826104982264\n",
      "Epoch 196, Loss: 0.43390767873021246\n",
      "Epoch 197, Loss: 0.4338897691233524\n",
      "Epoch 198, Loss: 0.43377856877811044\n",
      "Epoch 199, Loss: 0.4331595506415052\n",
      "Epoch 200, Loss: 0.43289415091975497\n",
      "Epoch 201, Loss: 0.43294048069736957\n",
      "Epoch 202, Loss: 0.43262240576772243\n",
      "Epoch 203, Loss: 0.43206541812705346\n",
      "Epoch 204, Loss: 0.43172500614102416\n",
      "Epoch 205, Loss: 0.4316686388133938\n",
      "Epoch 206, Loss: 0.4315179158466364\n",
      "Epoch 207, Loss: 0.4310712381632941\n",
      "Epoch 208, Loss: 0.43059470035496\n",
      "Epoch 209, Loss: 0.4303711221416647\n",
      "Epoch 210, Loss: 0.43033088719595897\n",
      "Epoch 211, Loss: 0.43016580489992484\n",
      "Epoch 212, Loss: 0.429691817522032\n",
      "Epoch 213, Loss: 0.4292099292794459\n",
      "Epoch 214, Loss: 0.4289207205910941\n",
      "Epoch 215, Loss: 0.4287978238495365\n",
      "Epoch 216, Loss: 0.42870603658529494\n",
      "Epoch 217, Loss: 0.42841980940051866\n",
      "Epoch 218, Loss: 0.4278918633922788\n",
      "Epoch 219, Loss: 0.42748307113702383\n",
      "Epoch 220, Loss: 0.4271688511370021\n",
      "Epoch 221, Loss: 0.4269432345934848\n",
      "Epoch 222, Loss: 0.4267651275776002\n",
      "Epoch 223, Loss: 0.42668376428587257\n",
      "Epoch 224, Loss: 0.42653554195413756\n",
      "Epoch 225, Loss: 0.42613293183996703\n",
      "Epoch 226, Loss: 0.4255451632780942\n",
      "Epoch 227, Loss: 0.4251305043179084\n",
      "Epoch 228, Loss: 0.42475934718412794\n",
      "Epoch 229, Loss: 0.42452283256309487\n",
      "Epoch 230, Loss: 0.4244024016195508\n",
      "Epoch 231, Loss: 0.4242604946441205\n",
      "Epoch 232, Loss: 0.424134367950937\n",
      "Epoch 233, Loss: 0.42372608274856827\n",
      "Epoch 234, Loss: 0.4231492155631465\n",
      "Epoch 235, Loss: 0.4226291943984805\n",
      "Epoch 236, Loss: 0.4221723179942825\n",
      "Epoch 237, Loss: 0.4218955917984354\n",
      "Epoch 238, Loss: 0.4216767834434851\n",
      "Epoch 239, Loss: 0.42145169178243785\n",
      "Epoch 240, Loss: 0.4212605805865247\n",
      "Epoch 241, Loss: 0.42123380253494425\n",
      "Epoch 242, Loss: 0.42125483246926426\n",
      "Epoch 243, Loss: 0.42124070061060576\n",
      "Epoch 244, Loss: 0.42073760533336746\n",
      "Epoch 245, Loss: 0.42002867294257734\n",
      "Epoch 246, Loss: 0.41900436945460057\n",
      "Epoch 247, Loss: 0.41841940783018283\n",
      "Epoch 248, Loss: 0.4184544673382656\n",
      "Epoch 249, Loss: 0.4187823781263808\n",
      "Epoch 250, Loss: 0.41914970907160026\n",
      "Epoch 251, Loss: 0.418702737788617\n",
      "Epoch 252, Loss: 0.41762532180686956\n",
      "Epoch 253, Loss: 0.41665732452727816\n",
      "Epoch 254, Loss: 0.4164608109104539\n",
      "Epoch 255, Loss: 0.4169362050698102\n",
      "Epoch 256, Loss: 0.4170439641107097\n",
      "Epoch 257, Loss: 0.41618098523090385\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.24667296795731722\n",
      "Valence RMSE: 0.25850432008601276\n",
      "Arousal RMSE: 0.2342447923404308\n",
      "Test R^2 score: tensor([0.2859, 0.5953], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.440623415267034\n",
      "Num of epochs: 258\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Epoch 121, Loss: 0.4596922103619757\n",
      "Epoch 122, Loss: 0.45943389956138125\n",
      "Epoch 123, Loss: 0.45877818894117006\n",
      "Epoch 124, Loss: 0.45746538132874076\n",
      "Epoch 125, Loss: 0.4570462803121619\n",
      "Epoch 126, Loss: 0.45696189549769234\n",
      "Epoch 127, Loss: 0.45594920341421274\n",
      "Epoch 128, Loss: 0.45536359747685007\n",
      "Epoch 129, Loss: 0.4553290890752543\n",
      "Epoch 130, Loss: 0.4547195823074053\n",
      "Epoch 131, Loss: 0.45392838323998064\n",
      "Epoch 132, Loss: 0.4537853493595631\n",
      "Epoch 133, Loss: 0.45347437050767975\n",
      "Epoch 134, Loss: 0.45267431273812453\n",
      "Epoch 135, Loss: 0.45228737659373064\n",
      "Epoch 136, Loss: 0.4522154657432981\n",
      "Epoch 137, Loss: 0.45165964487407706\n",
      "Epoch 138, Loss: 0.4510751265462795\n",
      "Epoch 139, Loss: 0.45082650445054473\n",
      "Epoch 140, Loss: 0.4506281098405297\n",
      "Epoch 141, Loss: 0.4501911419656675\n",
      "Epoch 142, Loss: 0.4497001887659326\n",
      "Epoch 143, Loss: 0.44943436133718806\n",
      "Epoch 144, Loss: 0.449226296402944\n",
      "Epoch 145, Loss: 0.44886073897867573\n",
      "Epoch 146, Loss: 0.44844710376304864\n",
      "Epoch 147, Loss: 0.44810507013116724\n",
      "Epoch 148, Loss: 0.44788545862049767\n",
      "Epoch 149, Loss: 0.4476741441009651\n",
      "Epoch 150, Loss: 0.44735861731902055\n",
      "Epoch 151, Loss: 0.4469288720157105\n",
      "Epoch 152, Loss: 0.44655235754178146\n",
      "Epoch 153, Loss: 0.4462857402786373\n",
      "Epoch 154, Loss: 0.4460517369142754\n",
      "Epoch 155, Loss: 0.4457881796184935\n",
      "Epoch 156, Loss: 0.44542210886348255\n",
      "Epoch 157, Loss: 0.4450579467827662\n",
      "Epoch 158, Loss: 0.44473329320345206\n",
      "Epoch 159, Loss: 0.4444482514678638\n",
      "Epoch 160, Loss: 0.4441745674630603\n",
      "Epoch 161, Loss: 0.4439221813722748\n",
      "Epoch 162, Loss: 0.44363357870363757\n",
      "Epoch 163, Loss: 0.4433189071134186\n",
      "Epoch 164, Loss: 0.44295452976444744\n",
      "Epoch 165, Loss: 0.4426058108229731\n",
      "Epoch 166, Loss: 0.4422485787900922\n",
      "Epoch 167, Loss: 0.4419090985167976\n",
      "Epoch 168, Loss: 0.44158054387831963\n",
      "Epoch 169, Loss: 0.4412474388772268\n",
      "Epoch 170, Loss: 0.4409277694136744\n",
      "Epoch 171, Loss: 0.44068045571539705\n",
      "Epoch 172, Loss: 0.4405979418732049\n",
      "Epoch 173, Loss: 0.44094101686109266\n",
      "Epoch 174, Loss: 0.4417368233339534\n",
      "Epoch 175, Loss: 0.4416582519101522\n",
      "Epoch 176, Loss: 0.4396788970821407\n",
      "Epoch 177, Loss: 0.4390195954561405\n",
      "Epoch 178, Loss: 0.4400830988007806\n",
      "Epoch 179, Loss: 0.43931775869233325\n",
      "Epoch 180, Loss: 0.4380032505977321\n",
      "Epoch 181, Loss: 0.4384470974340077\n",
      "Epoch 182, Loss: 0.4385167636581309\n",
      "Epoch 183, Loss: 0.43733043450040754\n",
      "Epoch 184, Loss: 0.4373176399006318\n",
      "Epoch 185, Loss: 0.4375256973621567\n",
      "Epoch 186, Loss: 0.43659322296038683\n",
      "Epoch 187, Loss: 0.43644391045908093\n",
      "Epoch 188, Loss: 0.43663844358015674\n",
      "Epoch 189, Loss: 0.43586286705793914\n",
      "Epoch 190, Loss: 0.4355663072844155\n",
      "Epoch 191, Loss: 0.4357040532214924\n",
      "Epoch 192, Loss: 0.435134007760397\n",
      "Epoch 193, Loss: 0.43474241127906293\n",
      "Epoch 194, Loss: 0.43480482310319635\n",
      "Epoch 195, Loss: 0.43443826104982264\n",
      "Epoch 196, Loss: 0.43390767873021246\n",
      "Epoch 197, Loss: 0.4338897691233524\n",
      "Epoch 198, Loss: 0.43377856877811044\n",
      "Epoch 199, Loss: 0.4331595506415052\n",
      "Epoch 200, Loss: 0.43289415091975497\n",
      "Epoch 201, Loss: 0.43294048069736957\n",
      "Epoch 202, Loss: 0.43262240576772243\n",
      "Epoch 203, Loss: 0.43206541812705346\n",
      "Epoch 204, Loss: 0.43172500614102416\n",
      "Epoch 205, Loss: 0.4316686388133938\n",
      "Epoch 206, Loss: 0.4315179158466364\n",
      "Epoch 207, Loss: 0.4310712381632941\n",
      "Epoch 208, Loss: 0.43059470035496\n",
      "Epoch 209, Loss: 0.4303711221416647\n",
      "Epoch 210, Loss: 0.43033088719595897\n",
      "Epoch 211, Loss: 0.43016580489992484\n",
      "Epoch 212, Loss: 0.429691817522032\n",
      "Epoch 213, Loss: 0.4292099292794459\n",
      "Epoch 214, Loss: 0.4289207205910941\n",
      "Epoch 215, Loss: 0.4287978238495365\n",
      "Epoch 216, Loss: 0.42870603658529494\n",
      "Epoch 217, Loss: 0.42841980940051866\n",
      "Epoch 218, Loss: 0.4278918633922788\n",
      "Epoch 219, Loss: 0.42748307113702383\n",
      "Epoch 220, Loss: 0.4271688511370021\n",
      "Epoch 221, Loss: 0.4269432345934848\n",
      "Epoch 222, Loss: 0.4267651275776002\n",
      "Epoch 223, Loss: 0.42668376428587257\n",
      "Epoch 224, Loss: 0.42653554195413756\n",
      "Epoch 225, Loss: 0.42613293183996703\n",
      "Epoch 226, Loss: 0.4255451632780942\n",
      "Epoch 227, Loss: 0.4251305043179084\n",
      "Epoch 228, Loss: 0.42475934718412794\n",
      "Epoch 229, Loss: 0.42452283256309487\n",
      "Epoch 230, Loss: 0.4244024016195508\n",
      "Epoch 231, Loss: 0.4242604946441205\n",
      "Epoch 232, Loss: 0.424134367950937\n",
      "Epoch 233, Loss: 0.42372608274856827\n",
      "Epoch 234, Loss: 0.4231492155631465\n",
      "Epoch 235, Loss: 0.4226291943984805\n",
      "Epoch 236, Loss: 0.4221723179942825\n",
      "Epoch 237, Loss: 0.4218955917984354\n",
      "Epoch 238, Loss: 0.4216767834434851\n",
      "Epoch 239, Loss: 0.42145169178243785\n",
      "Epoch 240, Loss: 0.4212605805865247\n",
      "Epoch 241, Loss: 0.42123380253494425\n",
      "Epoch 242, Loss: 0.42125483246926426\n",
      "Epoch 243, Loss: 0.42124070061060576\n",
      "Epoch 244, Loss: 0.42073760533336746\n",
      "Epoch 245, Loss: 0.42002867294257734\n",
      "Epoch 246, Loss: 0.41900436945460057\n",
      "Epoch 247, Loss: 0.41841940783018283\n",
      "Epoch 248, Loss: 0.4184544673382656\n",
      "Epoch 249, Loss: 0.4187823781263808\n",
      "Epoch 250, Loss: 0.41914970907160026\n",
      "Epoch 251, Loss: 0.418702737788617\n",
      "Epoch 252, Loss: 0.41762532180686956\n",
      "Epoch 253, Loss: 0.41665732452727816\n",
      "Epoch 254, Loss: 0.4164608109104539\n",
      "Epoch 255, Loss: 0.4169362050698102\n",
      "Epoch 256, Loss: 0.4170439641107097\n",
      "Epoch 257, Loss: 0.41618098523090385\n",
      "Epoch 258, Loss: 0.41521870035352226\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.24675478446596083\n",
      "Valence RMSE: 0.258265262430326\n",
      "Arousal RMSE: 0.23468042426985763\n",
      "Test R^2 score: tensor([0.2872, 0.5938], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.44053020176449204\n",
      "Num of epochs: 259\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Epoch 121, Loss: 0.4596922103619757\n",
      "Epoch 122, Loss: 0.45943389956138125\n",
      "Epoch 123, Loss: 0.45877818894117006\n",
      "Epoch 124, Loss: 0.45746538132874076\n",
      "Epoch 125, Loss: 0.4570462803121619\n",
      "Epoch 126, Loss: 0.45696189549769234\n",
      "Epoch 127, Loss: 0.45594920341421274\n",
      "Epoch 128, Loss: 0.45536359747685007\n",
      "Epoch 129, Loss: 0.4553290890752543\n",
      "Epoch 130, Loss: 0.4547195823074053\n",
      "Epoch 131, Loss: 0.45392838323998064\n",
      "Epoch 132, Loss: 0.4537853493595631\n",
      "Epoch 133, Loss: 0.45347437050767975\n",
      "Epoch 134, Loss: 0.45267431273812453\n",
      "Epoch 135, Loss: 0.45228737659373064\n",
      "Epoch 136, Loss: 0.4522154657432981\n",
      "Epoch 137, Loss: 0.45165964487407706\n",
      "Epoch 138, Loss: 0.4510751265462795\n",
      "Epoch 139, Loss: 0.45082650445054473\n",
      "Epoch 140, Loss: 0.4506281098405297\n",
      "Epoch 141, Loss: 0.4501911419656675\n",
      "Epoch 142, Loss: 0.4497001887659326\n",
      "Epoch 143, Loss: 0.44943436133718806\n",
      "Epoch 144, Loss: 0.449226296402944\n",
      "Epoch 145, Loss: 0.44886073897867573\n",
      "Epoch 146, Loss: 0.44844710376304864\n",
      "Epoch 147, Loss: 0.44810507013116724\n",
      "Epoch 148, Loss: 0.44788545862049767\n",
      "Epoch 149, Loss: 0.4476741441009651\n",
      "Epoch 150, Loss: 0.44735861731902055\n",
      "Epoch 151, Loss: 0.4469288720157105\n",
      "Epoch 152, Loss: 0.44655235754178146\n",
      "Epoch 153, Loss: 0.4462857402786373\n",
      "Epoch 154, Loss: 0.4460517369142754\n",
      "Epoch 155, Loss: 0.4457881796184935\n",
      "Epoch 156, Loss: 0.44542210886348255\n",
      "Epoch 157, Loss: 0.4450579467827662\n",
      "Epoch 158, Loss: 0.44473329320345206\n",
      "Epoch 159, Loss: 0.4444482514678638\n",
      "Epoch 160, Loss: 0.4441745674630603\n",
      "Epoch 161, Loss: 0.4439221813722748\n",
      "Epoch 162, Loss: 0.44363357870363757\n",
      "Epoch 163, Loss: 0.4433189071134186\n",
      "Epoch 164, Loss: 0.44295452976444744\n",
      "Epoch 165, Loss: 0.4426058108229731\n",
      "Epoch 166, Loss: 0.4422485787900922\n",
      "Epoch 167, Loss: 0.4419090985167976\n",
      "Epoch 168, Loss: 0.44158054387831963\n",
      "Epoch 169, Loss: 0.4412474388772268\n",
      "Epoch 170, Loss: 0.4409277694136744\n",
      "Epoch 171, Loss: 0.44068045571539705\n",
      "Epoch 172, Loss: 0.4405979418732049\n",
      "Epoch 173, Loss: 0.44094101686109266\n",
      "Epoch 174, Loss: 0.4417368233339534\n",
      "Epoch 175, Loss: 0.4416582519101522\n",
      "Epoch 176, Loss: 0.4396788970821407\n",
      "Epoch 177, Loss: 0.4390195954561405\n",
      "Epoch 178, Loss: 0.4400830988007806\n",
      "Epoch 179, Loss: 0.43931775869233325\n",
      "Epoch 180, Loss: 0.4380032505977321\n",
      "Epoch 181, Loss: 0.4384470974340077\n",
      "Epoch 182, Loss: 0.4385167636581309\n",
      "Epoch 183, Loss: 0.43733043450040754\n",
      "Epoch 184, Loss: 0.4373176399006318\n",
      "Epoch 185, Loss: 0.4375256973621567\n",
      "Epoch 186, Loss: 0.43659322296038683\n",
      "Epoch 187, Loss: 0.43644391045908093\n",
      "Epoch 188, Loss: 0.43663844358015674\n",
      "Epoch 189, Loss: 0.43586286705793914\n",
      "Epoch 190, Loss: 0.4355663072844155\n",
      "Epoch 191, Loss: 0.4357040532214924\n",
      "Epoch 192, Loss: 0.435134007760397\n",
      "Epoch 193, Loss: 0.43474241127906293\n",
      "Epoch 194, Loss: 0.43480482310319635\n",
      "Epoch 195, Loss: 0.43443826104982264\n",
      "Epoch 196, Loss: 0.43390767873021246\n",
      "Epoch 197, Loss: 0.4338897691233524\n",
      "Epoch 198, Loss: 0.43377856877811044\n",
      "Epoch 199, Loss: 0.4331595506415052\n",
      "Epoch 200, Loss: 0.43289415091975497\n",
      "Epoch 201, Loss: 0.43294048069736957\n",
      "Epoch 202, Loss: 0.43262240576772243\n",
      "Epoch 203, Loss: 0.43206541812705346\n",
      "Epoch 204, Loss: 0.43172500614102416\n",
      "Epoch 205, Loss: 0.4316686388133938\n",
      "Epoch 206, Loss: 0.4315179158466364\n",
      "Epoch 207, Loss: 0.4310712381632941\n",
      "Epoch 208, Loss: 0.43059470035496\n",
      "Epoch 209, Loss: 0.4303711221416647\n",
      "Epoch 210, Loss: 0.43033088719595897\n",
      "Epoch 211, Loss: 0.43016580489992484\n",
      "Epoch 212, Loss: 0.429691817522032\n",
      "Epoch 213, Loss: 0.4292099292794459\n",
      "Epoch 214, Loss: 0.4289207205910941\n",
      "Epoch 215, Loss: 0.4287978238495365\n",
      "Epoch 216, Loss: 0.42870603658529494\n",
      "Epoch 217, Loss: 0.42841980940051866\n",
      "Epoch 218, Loss: 0.4278918633922788\n",
      "Epoch 219, Loss: 0.42748307113702383\n",
      "Epoch 220, Loss: 0.4271688511370021\n",
      "Epoch 221, Loss: 0.4269432345934848\n",
      "Epoch 222, Loss: 0.4267651275776002\n",
      "Epoch 223, Loss: 0.42668376428587257\n",
      "Epoch 224, Loss: 0.42653554195413756\n",
      "Epoch 225, Loss: 0.42613293183996703\n",
      "Epoch 226, Loss: 0.4255451632780942\n",
      "Epoch 227, Loss: 0.4251305043179084\n",
      "Epoch 228, Loss: 0.42475934718412794\n",
      "Epoch 229, Loss: 0.42452283256309487\n",
      "Epoch 230, Loss: 0.4244024016195508\n",
      "Epoch 231, Loss: 0.4242604946441205\n",
      "Epoch 232, Loss: 0.424134367950937\n",
      "Epoch 233, Loss: 0.42372608274856827\n",
      "Epoch 234, Loss: 0.4231492155631465\n",
      "Epoch 235, Loss: 0.4226291943984805\n",
      "Epoch 236, Loss: 0.4221723179942825\n",
      "Epoch 237, Loss: 0.4218955917984354\n",
      "Epoch 238, Loss: 0.4216767834434851\n",
      "Epoch 239, Loss: 0.42145169178243785\n",
      "Epoch 240, Loss: 0.4212605805865247\n",
      "Epoch 241, Loss: 0.42123380253494425\n",
      "Epoch 242, Loss: 0.42125483246926426\n",
      "Epoch 243, Loss: 0.42124070061060576\n",
      "Epoch 244, Loss: 0.42073760533336746\n",
      "Epoch 245, Loss: 0.42002867294257734\n",
      "Epoch 246, Loss: 0.41900436945460057\n",
      "Epoch 247, Loss: 0.41841940783018283\n",
      "Epoch 248, Loss: 0.4184544673382656\n",
      "Epoch 249, Loss: 0.4187823781263808\n",
      "Epoch 250, Loss: 0.41914970907160026\n",
      "Epoch 251, Loss: 0.418702737788617\n",
      "Epoch 252, Loss: 0.41762532180686956\n",
      "Epoch 253, Loss: 0.41665732452727816\n",
      "Epoch 254, Loss: 0.4164608109104539\n",
      "Epoch 255, Loss: 0.4169362050698102\n",
      "Epoch 256, Loss: 0.4170439641107097\n",
      "Epoch 257, Loss: 0.41618098523090385\n",
      "Epoch 258, Loss: 0.41521870035352226\n",
      "Epoch 259, Loss: 0.41507164322372797\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.24741338244239036\n",
      "Valence RMSE: 0.2591936864181259\n",
      "Arousal RMSE: 0.23504339289618684\n",
      "Test R^2 score: tensor([0.2821, 0.5926], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4373346205535417\n",
      "Num of epochs: 260\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Epoch 121, Loss: 0.4596922103619757\n",
      "Epoch 122, Loss: 0.45943389956138125\n",
      "Epoch 123, Loss: 0.45877818894117006\n",
      "Epoch 124, Loss: 0.45746538132874076\n",
      "Epoch 125, Loss: 0.4570462803121619\n",
      "Epoch 126, Loss: 0.45696189549769234\n",
      "Epoch 127, Loss: 0.45594920341421274\n",
      "Epoch 128, Loss: 0.45536359747685007\n",
      "Epoch 129, Loss: 0.4553290890752543\n",
      "Epoch 130, Loss: 0.4547195823074053\n",
      "Epoch 131, Loss: 0.45392838323998064\n",
      "Epoch 132, Loss: 0.4537853493595631\n",
      "Epoch 133, Loss: 0.45347437050767975\n",
      "Epoch 134, Loss: 0.45267431273812453\n",
      "Epoch 135, Loss: 0.45228737659373064\n",
      "Epoch 136, Loss: 0.4522154657432981\n",
      "Epoch 137, Loss: 0.45165964487407706\n",
      "Epoch 138, Loss: 0.4510751265462795\n",
      "Epoch 139, Loss: 0.45082650445054473\n",
      "Epoch 140, Loss: 0.4506281098405297\n",
      "Epoch 141, Loss: 0.4501911419656675\n",
      "Epoch 142, Loss: 0.4497001887659326\n",
      "Epoch 143, Loss: 0.44943436133718806\n",
      "Epoch 144, Loss: 0.449226296402944\n",
      "Epoch 145, Loss: 0.44886073897867573\n",
      "Epoch 146, Loss: 0.44844710376304864\n",
      "Epoch 147, Loss: 0.44810507013116724\n",
      "Epoch 148, Loss: 0.44788545862049767\n",
      "Epoch 149, Loss: 0.4476741441009651\n",
      "Epoch 150, Loss: 0.44735861731902055\n",
      "Epoch 151, Loss: 0.4469288720157105\n",
      "Epoch 152, Loss: 0.44655235754178146\n",
      "Epoch 153, Loss: 0.4462857402786373\n",
      "Epoch 154, Loss: 0.4460517369142754\n",
      "Epoch 155, Loss: 0.4457881796184935\n",
      "Epoch 156, Loss: 0.44542210886348255\n",
      "Epoch 157, Loss: 0.4450579467827662\n",
      "Epoch 158, Loss: 0.44473329320345206\n",
      "Epoch 159, Loss: 0.4444482514678638\n",
      "Epoch 160, Loss: 0.4441745674630603\n",
      "Epoch 161, Loss: 0.4439221813722748\n",
      "Epoch 162, Loss: 0.44363357870363757\n",
      "Epoch 163, Loss: 0.4433189071134186\n",
      "Epoch 164, Loss: 0.44295452976444744\n",
      "Epoch 165, Loss: 0.4426058108229731\n",
      "Epoch 166, Loss: 0.4422485787900922\n",
      "Epoch 167, Loss: 0.4419090985167976\n",
      "Epoch 168, Loss: 0.44158054387831963\n",
      "Epoch 169, Loss: 0.4412474388772268\n",
      "Epoch 170, Loss: 0.4409277694136744\n",
      "Epoch 171, Loss: 0.44068045571539705\n",
      "Epoch 172, Loss: 0.4405979418732049\n",
      "Epoch 173, Loss: 0.44094101686109266\n",
      "Epoch 174, Loss: 0.4417368233339534\n",
      "Epoch 175, Loss: 0.4416582519101522\n",
      "Epoch 176, Loss: 0.4396788970821407\n",
      "Epoch 177, Loss: 0.4390195954561405\n",
      "Epoch 178, Loss: 0.4400830988007806\n",
      "Epoch 179, Loss: 0.43931775869233325\n",
      "Epoch 180, Loss: 0.4380032505977321\n",
      "Epoch 181, Loss: 0.4384470974340077\n",
      "Epoch 182, Loss: 0.4385167636581309\n",
      "Epoch 183, Loss: 0.43733043450040754\n",
      "Epoch 184, Loss: 0.4373176399006318\n",
      "Epoch 185, Loss: 0.4375256973621567\n",
      "Epoch 186, Loss: 0.43659322296038683\n",
      "Epoch 187, Loss: 0.43644391045908093\n",
      "Epoch 188, Loss: 0.43663844358015674\n",
      "Epoch 189, Loss: 0.43586286705793914\n",
      "Epoch 190, Loss: 0.4355663072844155\n",
      "Epoch 191, Loss: 0.4357040532214924\n",
      "Epoch 192, Loss: 0.435134007760397\n",
      "Epoch 193, Loss: 0.43474241127906293\n",
      "Epoch 194, Loss: 0.43480482310319635\n",
      "Epoch 195, Loss: 0.43443826104982264\n",
      "Epoch 196, Loss: 0.43390767873021246\n",
      "Epoch 197, Loss: 0.4338897691233524\n",
      "Epoch 198, Loss: 0.43377856877811044\n",
      "Epoch 199, Loss: 0.4331595506415052\n",
      "Epoch 200, Loss: 0.43289415091975497\n",
      "Epoch 201, Loss: 0.43294048069736957\n",
      "Epoch 202, Loss: 0.43262240576772243\n",
      "Epoch 203, Loss: 0.43206541812705346\n",
      "Epoch 204, Loss: 0.43172500614102416\n",
      "Epoch 205, Loss: 0.4316686388133938\n",
      "Epoch 206, Loss: 0.4315179158466364\n",
      "Epoch 207, Loss: 0.4310712381632941\n",
      "Epoch 208, Loss: 0.43059470035496\n",
      "Epoch 209, Loss: 0.4303711221416647\n",
      "Epoch 210, Loss: 0.43033088719595897\n",
      "Epoch 211, Loss: 0.43016580489992484\n",
      "Epoch 212, Loss: 0.429691817522032\n",
      "Epoch 213, Loss: 0.4292099292794459\n",
      "Epoch 214, Loss: 0.4289207205910941\n",
      "Epoch 215, Loss: 0.4287978238495365\n",
      "Epoch 216, Loss: 0.42870603658529494\n",
      "Epoch 217, Loss: 0.42841980940051866\n",
      "Epoch 218, Loss: 0.4278918633922788\n",
      "Epoch 219, Loss: 0.42748307113702383\n",
      "Epoch 220, Loss: 0.4271688511370021\n",
      "Epoch 221, Loss: 0.4269432345934848\n",
      "Epoch 222, Loss: 0.4267651275776002\n",
      "Epoch 223, Loss: 0.42668376428587257\n",
      "Epoch 224, Loss: 0.42653554195413756\n",
      "Epoch 225, Loss: 0.42613293183996703\n",
      "Epoch 226, Loss: 0.4255451632780942\n",
      "Epoch 227, Loss: 0.4251305043179084\n",
      "Epoch 228, Loss: 0.42475934718412794\n",
      "Epoch 229, Loss: 0.42452283256309487\n",
      "Epoch 230, Loss: 0.4244024016195508\n",
      "Epoch 231, Loss: 0.4242604946441205\n",
      "Epoch 232, Loss: 0.424134367950937\n",
      "Epoch 233, Loss: 0.42372608274856827\n",
      "Epoch 234, Loss: 0.4231492155631465\n",
      "Epoch 235, Loss: 0.4226291943984805\n",
      "Epoch 236, Loss: 0.4221723179942825\n",
      "Epoch 237, Loss: 0.4218955917984354\n",
      "Epoch 238, Loss: 0.4216767834434851\n",
      "Epoch 239, Loss: 0.42145169178243785\n",
      "Epoch 240, Loss: 0.4212605805865247\n",
      "Epoch 241, Loss: 0.42123380253494425\n",
      "Epoch 242, Loss: 0.42125483246926426\n",
      "Epoch 243, Loss: 0.42124070061060576\n",
      "Epoch 244, Loss: 0.42073760533336746\n",
      "Epoch 245, Loss: 0.42002867294257734\n",
      "Epoch 246, Loss: 0.41900436945460057\n",
      "Epoch 247, Loss: 0.41841940783018283\n",
      "Epoch 248, Loss: 0.4184544673382656\n",
      "Epoch 249, Loss: 0.4187823781263808\n",
      "Epoch 250, Loss: 0.41914970907160026\n",
      "Epoch 251, Loss: 0.418702737788617\n",
      "Epoch 252, Loss: 0.41762532180686956\n",
      "Epoch 253, Loss: 0.41665732452727816\n",
      "Epoch 254, Loss: 0.4164608109104539\n",
      "Epoch 255, Loss: 0.4169362050698102\n",
      "Epoch 256, Loss: 0.4170439641107097\n",
      "Epoch 257, Loss: 0.41618098523090385\n",
      "Epoch 258, Loss: 0.41521870035352226\n",
      "Epoch 259, Loss: 0.41507164322372797\n",
      "Epoch 260, Loss: 0.4154855816987037\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.24682610959456439\n",
      "Valence RMSE: 0.25727341194624576\n",
      "Arousal RMSE: 0.23591661293921867\n",
      "Test R^2 score: tensor([0.2927, 0.5895], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4411170907286921\n",
      "Num of epochs: 261\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Epoch 121, Loss: 0.4596922103619757\n",
      "Epoch 122, Loss: 0.45943389956138125\n",
      "Epoch 123, Loss: 0.45877818894117006\n",
      "Epoch 124, Loss: 0.45746538132874076\n",
      "Epoch 125, Loss: 0.4570462803121619\n",
      "Epoch 126, Loss: 0.45696189549769234\n",
      "Epoch 127, Loss: 0.45594920341421274\n",
      "Epoch 128, Loss: 0.45536359747685007\n",
      "Epoch 129, Loss: 0.4553290890752543\n",
      "Epoch 130, Loss: 0.4547195823074053\n",
      "Epoch 131, Loss: 0.45392838323998064\n",
      "Epoch 132, Loss: 0.4537853493595631\n",
      "Epoch 133, Loss: 0.45347437050767975\n",
      "Epoch 134, Loss: 0.45267431273812453\n",
      "Epoch 135, Loss: 0.45228737659373064\n",
      "Epoch 136, Loss: 0.4522154657432981\n",
      "Epoch 137, Loss: 0.45165964487407706\n",
      "Epoch 138, Loss: 0.4510751265462795\n",
      "Epoch 139, Loss: 0.45082650445054473\n",
      "Epoch 140, Loss: 0.4506281098405297\n",
      "Epoch 141, Loss: 0.4501911419656675\n",
      "Epoch 142, Loss: 0.4497001887659326\n",
      "Epoch 143, Loss: 0.44943436133718806\n",
      "Epoch 144, Loss: 0.449226296402944\n",
      "Epoch 145, Loss: 0.44886073897867573\n",
      "Epoch 146, Loss: 0.44844710376304864\n",
      "Epoch 147, Loss: 0.44810507013116724\n",
      "Epoch 148, Loss: 0.44788545862049767\n",
      "Epoch 149, Loss: 0.4476741441009651\n",
      "Epoch 150, Loss: 0.44735861731902055\n",
      "Epoch 151, Loss: 0.4469288720157105\n",
      "Epoch 152, Loss: 0.44655235754178146\n",
      "Epoch 153, Loss: 0.4462857402786373\n",
      "Epoch 154, Loss: 0.4460517369142754\n",
      "Epoch 155, Loss: 0.4457881796184935\n",
      "Epoch 156, Loss: 0.44542210886348255\n",
      "Epoch 157, Loss: 0.4450579467827662\n",
      "Epoch 158, Loss: 0.44473329320345206\n",
      "Epoch 159, Loss: 0.4444482514678638\n",
      "Epoch 160, Loss: 0.4441745674630603\n",
      "Epoch 161, Loss: 0.4439221813722748\n",
      "Epoch 162, Loss: 0.44363357870363757\n",
      "Epoch 163, Loss: 0.4433189071134186\n",
      "Epoch 164, Loss: 0.44295452976444744\n",
      "Epoch 165, Loss: 0.4426058108229731\n",
      "Epoch 166, Loss: 0.4422485787900922\n",
      "Epoch 167, Loss: 0.4419090985167976\n",
      "Epoch 168, Loss: 0.44158054387831963\n",
      "Epoch 169, Loss: 0.4412474388772268\n",
      "Epoch 170, Loss: 0.4409277694136744\n",
      "Epoch 171, Loss: 0.44068045571539705\n",
      "Epoch 172, Loss: 0.4405979418732049\n",
      "Epoch 173, Loss: 0.44094101686109266\n",
      "Epoch 174, Loss: 0.4417368233339534\n",
      "Epoch 175, Loss: 0.4416582519101522\n",
      "Epoch 176, Loss: 0.4396788970821407\n",
      "Epoch 177, Loss: 0.4390195954561405\n",
      "Epoch 178, Loss: 0.4400830988007806\n",
      "Epoch 179, Loss: 0.43931775869233325\n",
      "Epoch 180, Loss: 0.4380032505977321\n",
      "Epoch 181, Loss: 0.4384470974340077\n",
      "Epoch 182, Loss: 0.4385167636581309\n",
      "Epoch 183, Loss: 0.43733043450040754\n",
      "Epoch 184, Loss: 0.4373176399006318\n",
      "Epoch 185, Loss: 0.4375256973621567\n",
      "Epoch 186, Loss: 0.43659322296038683\n",
      "Epoch 187, Loss: 0.43644391045908093\n",
      "Epoch 188, Loss: 0.43663844358015674\n",
      "Epoch 189, Loss: 0.43586286705793914\n",
      "Epoch 190, Loss: 0.4355663072844155\n",
      "Epoch 191, Loss: 0.4357040532214924\n",
      "Epoch 192, Loss: 0.435134007760397\n",
      "Epoch 193, Loss: 0.43474241127906293\n",
      "Epoch 194, Loss: 0.43480482310319635\n",
      "Epoch 195, Loss: 0.43443826104982264\n",
      "Epoch 196, Loss: 0.43390767873021246\n",
      "Epoch 197, Loss: 0.4338897691233524\n",
      "Epoch 198, Loss: 0.43377856877811044\n",
      "Epoch 199, Loss: 0.4331595506415052\n",
      "Epoch 200, Loss: 0.43289415091975497\n",
      "Epoch 201, Loss: 0.43294048069736957\n",
      "Epoch 202, Loss: 0.43262240576772243\n",
      "Epoch 203, Loss: 0.43206541812705346\n",
      "Epoch 204, Loss: 0.43172500614102416\n",
      "Epoch 205, Loss: 0.4316686388133938\n",
      "Epoch 206, Loss: 0.4315179158466364\n",
      "Epoch 207, Loss: 0.4310712381632941\n",
      "Epoch 208, Loss: 0.43059470035496\n",
      "Epoch 209, Loss: 0.4303711221416647\n",
      "Epoch 210, Loss: 0.43033088719595897\n",
      "Epoch 211, Loss: 0.43016580489992484\n",
      "Epoch 212, Loss: 0.429691817522032\n",
      "Epoch 213, Loss: 0.4292099292794459\n",
      "Epoch 214, Loss: 0.4289207205910941\n",
      "Epoch 215, Loss: 0.4287978238495365\n",
      "Epoch 216, Loss: 0.42870603658529494\n",
      "Epoch 217, Loss: 0.42841980940051866\n",
      "Epoch 218, Loss: 0.4278918633922788\n",
      "Epoch 219, Loss: 0.42748307113702383\n",
      "Epoch 220, Loss: 0.4271688511370021\n",
      "Epoch 221, Loss: 0.4269432345934848\n",
      "Epoch 222, Loss: 0.4267651275776002\n",
      "Epoch 223, Loss: 0.42668376428587257\n",
      "Epoch 224, Loss: 0.42653554195413756\n",
      "Epoch 225, Loss: 0.42613293183996703\n",
      "Epoch 226, Loss: 0.4255451632780942\n",
      "Epoch 227, Loss: 0.4251305043179084\n",
      "Epoch 228, Loss: 0.42475934718412794\n",
      "Epoch 229, Loss: 0.42452283256309487\n",
      "Epoch 230, Loss: 0.4244024016195508\n",
      "Epoch 231, Loss: 0.4242604946441205\n",
      "Epoch 232, Loss: 0.424134367950937\n",
      "Epoch 233, Loss: 0.42372608274856827\n",
      "Epoch 234, Loss: 0.4231492155631465\n",
      "Epoch 235, Loss: 0.4226291943984805\n",
      "Epoch 236, Loss: 0.4221723179942825\n",
      "Epoch 237, Loss: 0.4218955917984354\n",
      "Epoch 238, Loss: 0.4216767834434851\n",
      "Epoch 239, Loss: 0.42145169178243785\n",
      "Epoch 240, Loss: 0.4212605805865247\n",
      "Epoch 241, Loss: 0.42123380253494425\n",
      "Epoch 242, Loss: 0.42125483246926426\n",
      "Epoch 243, Loss: 0.42124070061060576\n",
      "Epoch 244, Loss: 0.42073760533336746\n",
      "Epoch 245, Loss: 0.42002867294257734\n",
      "Epoch 246, Loss: 0.41900436945460057\n",
      "Epoch 247, Loss: 0.41841940783018283\n",
      "Epoch 248, Loss: 0.4184544673382656\n",
      "Epoch 249, Loss: 0.4187823781263808\n",
      "Epoch 250, Loss: 0.41914970907160026\n",
      "Epoch 251, Loss: 0.418702737788617\n",
      "Epoch 252, Loss: 0.41762532180686956\n",
      "Epoch 253, Loss: 0.41665732452727816\n",
      "Epoch 254, Loss: 0.4164608109104539\n",
      "Epoch 255, Loss: 0.4169362050698102\n",
      "Epoch 256, Loss: 0.4170439641107097\n",
      "Epoch 257, Loss: 0.41618098523090385\n",
      "Epoch 258, Loss: 0.41521870035352226\n",
      "Epoch 259, Loss: 0.41507164322372797\n",
      "Epoch 260, Loss: 0.4154855816987037\n",
      "Epoch 261, Loss: 0.4152875627561342\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2476297310522634\n",
      "Valence RMSE: 0.25922897443220083\n",
      "Arousal RMSE: 0.23545977621849135\n",
      "Test R^2 score: tensor([0.2819, 0.5911], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.43651446250974557\n",
      "Num of epochs: 262\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Epoch 121, Loss: 0.4596922103619757\n",
      "Epoch 122, Loss: 0.45943389956138125\n",
      "Epoch 123, Loss: 0.45877818894117006\n",
      "Epoch 124, Loss: 0.45746538132874076\n",
      "Epoch 125, Loss: 0.4570462803121619\n",
      "Epoch 126, Loss: 0.45696189549769234\n",
      "Epoch 127, Loss: 0.45594920341421274\n",
      "Epoch 128, Loss: 0.45536359747685007\n",
      "Epoch 129, Loss: 0.4553290890752543\n",
      "Epoch 130, Loss: 0.4547195823074053\n",
      "Epoch 131, Loss: 0.45392838323998064\n",
      "Epoch 132, Loss: 0.4537853493595631\n",
      "Epoch 133, Loss: 0.45347437050767975\n",
      "Epoch 134, Loss: 0.45267431273812453\n",
      "Epoch 135, Loss: 0.45228737659373064\n",
      "Epoch 136, Loss: 0.4522154657432981\n",
      "Epoch 137, Loss: 0.45165964487407706\n",
      "Epoch 138, Loss: 0.4510751265462795\n",
      "Epoch 139, Loss: 0.45082650445054473\n",
      "Epoch 140, Loss: 0.4506281098405297\n",
      "Epoch 141, Loss: 0.4501911419656675\n",
      "Epoch 142, Loss: 0.4497001887659326\n",
      "Epoch 143, Loss: 0.44943436133718806\n",
      "Epoch 144, Loss: 0.449226296402944\n",
      "Epoch 145, Loss: 0.44886073897867573\n",
      "Epoch 146, Loss: 0.44844710376304864\n",
      "Epoch 147, Loss: 0.44810507013116724\n",
      "Epoch 148, Loss: 0.44788545862049767\n",
      "Epoch 149, Loss: 0.4476741441009651\n",
      "Epoch 150, Loss: 0.44735861731902055\n",
      "Epoch 151, Loss: 0.4469288720157105\n",
      "Epoch 152, Loss: 0.44655235754178146\n",
      "Epoch 153, Loss: 0.4462857402786373\n",
      "Epoch 154, Loss: 0.4460517369142754\n",
      "Epoch 155, Loss: 0.4457881796184935\n",
      "Epoch 156, Loss: 0.44542210886348255\n",
      "Epoch 157, Loss: 0.4450579467827662\n",
      "Epoch 158, Loss: 0.44473329320345206\n",
      "Epoch 159, Loss: 0.4444482514678638\n",
      "Epoch 160, Loss: 0.4441745674630603\n",
      "Epoch 161, Loss: 0.4439221813722748\n",
      "Epoch 162, Loss: 0.44363357870363757\n",
      "Epoch 163, Loss: 0.4433189071134186\n",
      "Epoch 164, Loss: 0.44295452976444744\n",
      "Epoch 165, Loss: 0.4426058108229731\n",
      "Epoch 166, Loss: 0.4422485787900922\n",
      "Epoch 167, Loss: 0.4419090985167976\n",
      "Epoch 168, Loss: 0.44158054387831963\n",
      "Epoch 169, Loss: 0.4412474388772268\n",
      "Epoch 170, Loss: 0.4409277694136744\n",
      "Epoch 171, Loss: 0.44068045571539705\n",
      "Epoch 172, Loss: 0.4405979418732049\n",
      "Epoch 173, Loss: 0.44094101686109266\n",
      "Epoch 174, Loss: 0.4417368233339534\n",
      "Epoch 175, Loss: 0.4416582519101522\n",
      "Epoch 176, Loss: 0.4396788970821407\n",
      "Epoch 177, Loss: 0.4390195954561405\n",
      "Epoch 178, Loss: 0.4400830988007806\n",
      "Epoch 179, Loss: 0.43931775869233325\n",
      "Epoch 180, Loss: 0.4380032505977321\n",
      "Epoch 181, Loss: 0.4384470974340077\n",
      "Epoch 182, Loss: 0.4385167636581309\n",
      "Epoch 183, Loss: 0.43733043450040754\n",
      "Epoch 184, Loss: 0.4373176399006318\n",
      "Epoch 185, Loss: 0.4375256973621567\n",
      "Epoch 186, Loss: 0.43659322296038683\n",
      "Epoch 187, Loss: 0.43644391045908093\n",
      "Epoch 188, Loss: 0.43663844358015674\n",
      "Epoch 189, Loss: 0.43586286705793914\n",
      "Epoch 190, Loss: 0.4355663072844155\n",
      "Epoch 191, Loss: 0.4357040532214924\n",
      "Epoch 192, Loss: 0.435134007760397\n",
      "Epoch 193, Loss: 0.43474241127906293\n",
      "Epoch 194, Loss: 0.43480482310319635\n",
      "Epoch 195, Loss: 0.43443826104982264\n",
      "Epoch 196, Loss: 0.43390767873021246\n",
      "Epoch 197, Loss: 0.4338897691233524\n",
      "Epoch 198, Loss: 0.43377856877811044\n",
      "Epoch 199, Loss: 0.4331595506415052\n",
      "Epoch 200, Loss: 0.43289415091975497\n",
      "Epoch 201, Loss: 0.43294048069736957\n",
      "Epoch 202, Loss: 0.43262240576772243\n",
      "Epoch 203, Loss: 0.43206541812705346\n",
      "Epoch 204, Loss: 0.43172500614102416\n",
      "Epoch 205, Loss: 0.4316686388133938\n",
      "Epoch 206, Loss: 0.4315179158466364\n",
      "Epoch 207, Loss: 0.4310712381632941\n",
      "Epoch 208, Loss: 0.43059470035496\n",
      "Epoch 209, Loss: 0.4303711221416647\n",
      "Epoch 210, Loss: 0.43033088719595897\n",
      "Epoch 211, Loss: 0.43016580489992484\n",
      "Epoch 212, Loss: 0.429691817522032\n",
      "Epoch 213, Loss: 0.4292099292794459\n",
      "Epoch 214, Loss: 0.4289207205910941\n",
      "Epoch 215, Loss: 0.4287978238495365\n",
      "Epoch 216, Loss: 0.42870603658529494\n",
      "Epoch 217, Loss: 0.42841980940051866\n",
      "Epoch 218, Loss: 0.4278918633922788\n",
      "Epoch 219, Loss: 0.42748307113702383\n",
      "Epoch 220, Loss: 0.4271688511370021\n",
      "Epoch 221, Loss: 0.4269432345934848\n",
      "Epoch 222, Loss: 0.4267651275776002\n",
      "Epoch 223, Loss: 0.42668376428587257\n",
      "Epoch 224, Loss: 0.42653554195413756\n",
      "Epoch 225, Loss: 0.42613293183996703\n",
      "Epoch 226, Loss: 0.4255451632780942\n",
      "Epoch 227, Loss: 0.4251305043179084\n",
      "Epoch 228, Loss: 0.42475934718412794\n",
      "Epoch 229, Loss: 0.42452283256309487\n",
      "Epoch 230, Loss: 0.4244024016195508\n",
      "Epoch 231, Loss: 0.4242604946441205\n",
      "Epoch 232, Loss: 0.424134367950937\n",
      "Epoch 233, Loss: 0.42372608274856827\n",
      "Epoch 234, Loss: 0.4231492155631465\n",
      "Epoch 235, Loss: 0.4226291943984805\n",
      "Epoch 236, Loss: 0.4221723179942825\n",
      "Epoch 237, Loss: 0.4218955917984354\n",
      "Epoch 238, Loss: 0.4216767834434851\n",
      "Epoch 239, Loss: 0.42145169178243785\n",
      "Epoch 240, Loss: 0.4212605805865247\n",
      "Epoch 241, Loss: 0.42123380253494425\n",
      "Epoch 242, Loss: 0.42125483246926426\n",
      "Epoch 243, Loss: 0.42124070061060576\n",
      "Epoch 244, Loss: 0.42073760533336746\n",
      "Epoch 245, Loss: 0.42002867294257734\n",
      "Epoch 246, Loss: 0.41900436945460057\n",
      "Epoch 247, Loss: 0.41841940783018283\n",
      "Epoch 248, Loss: 0.4184544673382656\n",
      "Epoch 249, Loss: 0.4187823781263808\n",
      "Epoch 250, Loss: 0.41914970907160026\n",
      "Epoch 251, Loss: 0.418702737788617\n",
      "Epoch 252, Loss: 0.41762532180686956\n",
      "Epoch 253, Loss: 0.41665732452727816\n",
      "Epoch 254, Loss: 0.4164608109104539\n",
      "Epoch 255, Loss: 0.4169362050698102\n",
      "Epoch 256, Loss: 0.4170439641107097\n",
      "Epoch 257, Loss: 0.41618098523090385\n",
      "Epoch 258, Loss: 0.41521870035352226\n",
      "Epoch 259, Loss: 0.41507164322372797\n",
      "Epoch 260, Loss: 0.4154855816987037\n",
      "Epoch 261, Loss: 0.4152875627561342\n",
      "Epoch 262, Loss: 0.4146051629547779\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2475518632886312\n",
      "Valence RMSE: 0.258770808285231\n",
      "Arousal RMSE: 0.23579974303369058\n",
      "Test R^2 score: tensor([0.2844, 0.5899], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4371917325739714\n",
      "Num of epochs: 263\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Epoch 121, Loss: 0.4596922103619757\n",
      "Epoch 122, Loss: 0.45943389956138125\n",
      "Epoch 123, Loss: 0.45877818894117006\n",
      "Epoch 124, Loss: 0.45746538132874076\n",
      "Epoch 125, Loss: 0.4570462803121619\n",
      "Epoch 126, Loss: 0.45696189549769234\n",
      "Epoch 127, Loss: 0.45594920341421274\n",
      "Epoch 128, Loss: 0.45536359747685007\n",
      "Epoch 129, Loss: 0.4553290890752543\n",
      "Epoch 130, Loss: 0.4547195823074053\n",
      "Epoch 131, Loss: 0.45392838323998064\n",
      "Epoch 132, Loss: 0.4537853493595631\n",
      "Epoch 133, Loss: 0.45347437050767975\n",
      "Epoch 134, Loss: 0.45267431273812453\n",
      "Epoch 135, Loss: 0.45228737659373064\n",
      "Epoch 136, Loss: 0.4522154657432981\n",
      "Epoch 137, Loss: 0.45165964487407706\n",
      "Epoch 138, Loss: 0.4510751265462795\n",
      "Epoch 139, Loss: 0.45082650445054473\n",
      "Epoch 140, Loss: 0.4506281098405297\n",
      "Epoch 141, Loss: 0.4501911419656675\n",
      "Epoch 142, Loss: 0.4497001887659326\n",
      "Epoch 143, Loss: 0.44943436133718806\n",
      "Epoch 144, Loss: 0.449226296402944\n",
      "Epoch 145, Loss: 0.44886073897867573\n",
      "Epoch 146, Loss: 0.44844710376304864\n",
      "Epoch 147, Loss: 0.44810507013116724\n",
      "Epoch 148, Loss: 0.44788545862049767\n",
      "Epoch 149, Loss: 0.4476741441009651\n",
      "Epoch 150, Loss: 0.44735861731902055\n",
      "Epoch 151, Loss: 0.4469288720157105\n",
      "Epoch 152, Loss: 0.44655235754178146\n",
      "Epoch 153, Loss: 0.4462857402786373\n",
      "Epoch 154, Loss: 0.4460517369142754\n",
      "Epoch 155, Loss: 0.4457881796184935\n",
      "Epoch 156, Loss: 0.44542210886348255\n",
      "Epoch 157, Loss: 0.4450579467827662\n",
      "Epoch 158, Loss: 0.44473329320345206\n",
      "Epoch 159, Loss: 0.4444482514678638\n",
      "Epoch 160, Loss: 0.4441745674630603\n",
      "Epoch 161, Loss: 0.4439221813722748\n",
      "Epoch 162, Loss: 0.44363357870363757\n",
      "Epoch 163, Loss: 0.4433189071134186\n",
      "Epoch 164, Loss: 0.44295452976444744\n",
      "Epoch 165, Loss: 0.4426058108229731\n",
      "Epoch 166, Loss: 0.4422485787900922\n",
      "Epoch 167, Loss: 0.4419090985167976\n",
      "Epoch 168, Loss: 0.44158054387831963\n",
      "Epoch 169, Loss: 0.4412474388772268\n",
      "Epoch 170, Loss: 0.4409277694136744\n",
      "Epoch 171, Loss: 0.44068045571539705\n",
      "Epoch 172, Loss: 0.4405979418732049\n",
      "Epoch 173, Loss: 0.44094101686109266\n",
      "Epoch 174, Loss: 0.4417368233339534\n",
      "Epoch 175, Loss: 0.4416582519101522\n",
      "Epoch 176, Loss: 0.4396788970821407\n",
      "Epoch 177, Loss: 0.4390195954561405\n",
      "Epoch 178, Loss: 0.4400830988007806\n",
      "Epoch 179, Loss: 0.43931775869233325\n",
      "Epoch 180, Loss: 0.4380032505977321\n",
      "Epoch 181, Loss: 0.4384470974340077\n",
      "Epoch 182, Loss: 0.4385167636581309\n",
      "Epoch 183, Loss: 0.43733043450040754\n",
      "Epoch 184, Loss: 0.4373176399006318\n",
      "Epoch 185, Loss: 0.4375256973621567\n",
      "Epoch 186, Loss: 0.43659322296038683\n",
      "Epoch 187, Loss: 0.43644391045908093\n",
      "Epoch 188, Loss: 0.43663844358015674\n",
      "Epoch 189, Loss: 0.43586286705793914\n",
      "Epoch 190, Loss: 0.4355663072844155\n",
      "Epoch 191, Loss: 0.4357040532214924\n",
      "Epoch 192, Loss: 0.435134007760397\n",
      "Epoch 193, Loss: 0.43474241127906293\n",
      "Epoch 194, Loss: 0.43480482310319635\n",
      "Epoch 195, Loss: 0.43443826104982264\n",
      "Epoch 196, Loss: 0.43390767873021246\n",
      "Epoch 197, Loss: 0.4338897691233524\n",
      "Epoch 198, Loss: 0.43377856877811044\n",
      "Epoch 199, Loss: 0.4331595506415052\n",
      "Epoch 200, Loss: 0.43289415091975497\n",
      "Epoch 201, Loss: 0.43294048069736957\n",
      "Epoch 202, Loss: 0.43262240576772243\n",
      "Epoch 203, Loss: 0.43206541812705346\n",
      "Epoch 204, Loss: 0.43172500614102416\n",
      "Epoch 205, Loss: 0.4316686388133938\n",
      "Epoch 206, Loss: 0.4315179158466364\n",
      "Epoch 207, Loss: 0.4310712381632941\n",
      "Epoch 208, Loss: 0.43059470035496\n",
      "Epoch 209, Loss: 0.4303711221416647\n",
      "Epoch 210, Loss: 0.43033088719595897\n",
      "Epoch 211, Loss: 0.43016580489992484\n",
      "Epoch 212, Loss: 0.429691817522032\n",
      "Epoch 213, Loss: 0.4292099292794459\n",
      "Epoch 214, Loss: 0.4289207205910941\n",
      "Epoch 215, Loss: 0.4287978238495365\n",
      "Epoch 216, Loss: 0.42870603658529494\n",
      "Epoch 217, Loss: 0.42841980940051866\n",
      "Epoch 218, Loss: 0.4278918633922788\n",
      "Epoch 219, Loss: 0.42748307113702383\n",
      "Epoch 220, Loss: 0.4271688511370021\n",
      "Epoch 221, Loss: 0.4269432345934848\n",
      "Epoch 222, Loss: 0.4267651275776002\n",
      "Epoch 223, Loss: 0.42668376428587257\n",
      "Epoch 224, Loss: 0.42653554195413756\n",
      "Epoch 225, Loss: 0.42613293183996703\n",
      "Epoch 226, Loss: 0.4255451632780942\n",
      "Epoch 227, Loss: 0.4251305043179084\n",
      "Epoch 228, Loss: 0.42475934718412794\n",
      "Epoch 229, Loss: 0.42452283256309487\n",
      "Epoch 230, Loss: 0.4244024016195508\n",
      "Epoch 231, Loss: 0.4242604946441205\n",
      "Epoch 232, Loss: 0.424134367950937\n",
      "Epoch 233, Loss: 0.42372608274856827\n",
      "Epoch 234, Loss: 0.4231492155631465\n",
      "Epoch 235, Loss: 0.4226291943984805\n",
      "Epoch 236, Loss: 0.4221723179942825\n",
      "Epoch 237, Loss: 0.4218955917984354\n",
      "Epoch 238, Loss: 0.4216767834434851\n",
      "Epoch 239, Loss: 0.42145169178243785\n",
      "Epoch 240, Loss: 0.4212605805865247\n",
      "Epoch 241, Loss: 0.42123380253494425\n",
      "Epoch 242, Loss: 0.42125483246926426\n",
      "Epoch 243, Loss: 0.42124070061060576\n",
      "Epoch 244, Loss: 0.42073760533336746\n",
      "Epoch 245, Loss: 0.42002867294257734\n",
      "Epoch 246, Loss: 0.41900436945460057\n",
      "Epoch 247, Loss: 0.41841940783018283\n",
      "Epoch 248, Loss: 0.4184544673382656\n",
      "Epoch 249, Loss: 0.4187823781263808\n",
      "Epoch 250, Loss: 0.41914970907160026\n",
      "Epoch 251, Loss: 0.418702737788617\n",
      "Epoch 252, Loss: 0.41762532180686956\n",
      "Epoch 253, Loss: 0.41665732452727816\n",
      "Epoch 254, Loss: 0.4164608109104539\n",
      "Epoch 255, Loss: 0.4169362050698102\n",
      "Epoch 256, Loss: 0.4170439641107097\n",
      "Epoch 257, Loss: 0.41618098523090385\n",
      "Epoch 258, Loss: 0.41521870035352226\n",
      "Epoch 259, Loss: 0.41507164322372797\n",
      "Epoch 260, Loss: 0.4154855816987037\n",
      "Epoch 261, Loss: 0.4152875627561342\n",
      "Epoch 262, Loss: 0.4146051629547779\n",
      "Epoch 263, Loss: 0.4138602536580612\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.24739253851841003\n",
      "Valence RMSE: 0.258293795545467\n",
      "Arousal RMSE: 0.23598824422390727\n",
      "Test R^2 score: tensor([0.2871, 0.5893], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4381816237778891\n",
      "Num of epochs: 264\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Epoch 121, Loss: 0.4596922103619757\n",
      "Epoch 122, Loss: 0.45943389956138125\n",
      "Epoch 123, Loss: 0.45877818894117006\n",
      "Epoch 124, Loss: 0.45746538132874076\n",
      "Epoch 125, Loss: 0.4570462803121619\n",
      "Epoch 126, Loss: 0.45696189549769234\n",
      "Epoch 127, Loss: 0.45594920341421274\n",
      "Epoch 128, Loss: 0.45536359747685007\n",
      "Epoch 129, Loss: 0.4553290890752543\n",
      "Epoch 130, Loss: 0.4547195823074053\n",
      "Epoch 131, Loss: 0.45392838323998064\n",
      "Epoch 132, Loss: 0.4537853493595631\n",
      "Epoch 133, Loss: 0.45347437050767975\n",
      "Epoch 134, Loss: 0.45267431273812453\n",
      "Epoch 135, Loss: 0.45228737659373064\n",
      "Epoch 136, Loss: 0.4522154657432981\n",
      "Epoch 137, Loss: 0.45165964487407706\n",
      "Epoch 138, Loss: 0.4510751265462795\n",
      "Epoch 139, Loss: 0.45082650445054473\n",
      "Epoch 140, Loss: 0.4506281098405297\n",
      "Epoch 141, Loss: 0.4501911419656675\n",
      "Epoch 142, Loss: 0.4497001887659326\n",
      "Epoch 143, Loss: 0.44943436133718806\n",
      "Epoch 144, Loss: 0.449226296402944\n",
      "Epoch 145, Loss: 0.44886073897867573\n",
      "Epoch 146, Loss: 0.44844710376304864\n",
      "Epoch 147, Loss: 0.44810507013116724\n",
      "Epoch 148, Loss: 0.44788545862049767\n",
      "Epoch 149, Loss: 0.4476741441009651\n",
      "Epoch 150, Loss: 0.44735861731902055\n",
      "Epoch 151, Loss: 0.4469288720157105\n",
      "Epoch 152, Loss: 0.44655235754178146\n",
      "Epoch 153, Loss: 0.4462857402786373\n",
      "Epoch 154, Loss: 0.4460517369142754\n",
      "Epoch 155, Loss: 0.4457881796184935\n",
      "Epoch 156, Loss: 0.44542210886348255\n",
      "Epoch 157, Loss: 0.4450579467827662\n",
      "Epoch 158, Loss: 0.44473329320345206\n",
      "Epoch 159, Loss: 0.4444482514678638\n",
      "Epoch 160, Loss: 0.4441745674630603\n",
      "Epoch 161, Loss: 0.4439221813722748\n",
      "Epoch 162, Loss: 0.44363357870363757\n",
      "Epoch 163, Loss: 0.4433189071134186\n",
      "Epoch 164, Loss: 0.44295452976444744\n",
      "Epoch 165, Loss: 0.4426058108229731\n",
      "Epoch 166, Loss: 0.4422485787900922\n",
      "Epoch 167, Loss: 0.4419090985167976\n",
      "Epoch 168, Loss: 0.44158054387831963\n",
      "Epoch 169, Loss: 0.4412474388772268\n",
      "Epoch 170, Loss: 0.4409277694136744\n",
      "Epoch 171, Loss: 0.44068045571539705\n",
      "Epoch 172, Loss: 0.4405979418732049\n",
      "Epoch 173, Loss: 0.44094101686109266\n",
      "Epoch 174, Loss: 0.4417368233339534\n",
      "Epoch 175, Loss: 0.4416582519101522\n",
      "Epoch 176, Loss: 0.4396788970821407\n",
      "Epoch 177, Loss: 0.4390195954561405\n",
      "Epoch 178, Loss: 0.4400830988007806\n",
      "Epoch 179, Loss: 0.43931775869233325\n",
      "Epoch 180, Loss: 0.4380032505977321\n",
      "Epoch 181, Loss: 0.4384470974340077\n",
      "Epoch 182, Loss: 0.4385167636581309\n",
      "Epoch 183, Loss: 0.43733043450040754\n",
      "Epoch 184, Loss: 0.4373176399006318\n",
      "Epoch 185, Loss: 0.4375256973621567\n",
      "Epoch 186, Loss: 0.43659322296038683\n",
      "Epoch 187, Loss: 0.43644391045908093\n",
      "Epoch 188, Loss: 0.43663844358015674\n",
      "Epoch 189, Loss: 0.43586286705793914\n",
      "Epoch 190, Loss: 0.4355663072844155\n",
      "Epoch 191, Loss: 0.4357040532214924\n",
      "Epoch 192, Loss: 0.435134007760397\n",
      "Epoch 193, Loss: 0.43474241127906293\n",
      "Epoch 194, Loss: 0.43480482310319635\n",
      "Epoch 195, Loss: 0.43443826104982264\n",
      "Epoch 196, Loss: 0.43390767873021246\n",
      "Epoch 197, Loss: 0.4338897691233524\n",
      "Epoch 198, Loss: 0.43377856877811044\n",
      "Epoch 199, Loss: 0.4331595506415052\n",
      "Epoch 200, Loss: 0.43289415091975497\n",
      "Epoch 201, Loss: 0.43294048069736957\n",
      "Epoch 202, Loss: 0.43262240576772243\n",
      "Epoch 203, Loss: 0.43206541812705346\n",
      "Epoch 204, Loss: 0.43172500614102416\n",
      "Epoch 205, Loss: 0.4316686388133938\n",
      "Epoch 206, Loss: 0.4315179158466364\n",
      "Epoch 207, Loss: 0.4310712381632941\n",
      "Epoch 208, Loss: 0.43059470035496\n",
      "Epoch 209, Loss: 0.4303711221416647\n",
      "Epoch 210, Loss: 0.43033088719595897\n",
      "Epoch 211, Loss: 0.43016580489992484\n",
      "Epoch 212, Loss: 0.429691817522032\n",
      "Epoch 213, Loss: 0.4292099292794459\n",
      "Epoch 214, Loss: 0.4289207205910941\n",
      "Epoch 215, Loss: 0.4287978238495365\n",
      "Epoch 216, Loss: 0.42870603658529494\n",
      "Epoch 217, Loss: 0.42841980940051866\n",
      "Epoch 218, Loss: 0.4278918633922788\n",
      "Epoch 219, Loss: 0.42748307113702383\n",
      "Epoch 220, Loss: 0.4271688511370021\n",
      "Epoch 221, Loss: 0.4269432345934848\n",
      "Epoch 222, Loss: 0.4267651275776002\n",
      "Epoch 223, Loss: 0.42668376428587257\n",
      "Epoch 224, Loss: 0.42653554195413756\n",
      "Epoch 225, Loss: 0.42613293183996703\n",
      "Epoch 226, Loss: 0.4255451632780942\n",
      "Epoch 227, Loss: 0.4251305043179084\n",
      "Epoch 228, Loss: 0.42475934718412794\n",
      "Epoch 229, Loss: 0.42452283256309487\n",
      "Epoch 230, Loss: 0.4244024016195508\n",
      "Epoch 231, Loss: 0.4242604946441205\n",
      "Epoch 232, Loss: 0.424134367950937\n",
      "Epoch 233, Loss: 0.42372608274856827\n",
      "Epoch 234, Loss: 0.4231492155631465\n",
      "Epoch 235, Loss: 0.4226291943984805\n",
      "Epoch 236, Loss: 0.4221723179942825\n",
      "Epoch 237, Loss: 0.4218955917984354\n",
      "Epoch 238, Loss: 0.4216767834434851\n",
      "Epoch 239, Loss: 0.42145169178243785\n",
      "Epoch 240, Loss: 0.4212605805865247\n",
      "Epoch 241, Loss: 0.42123380253494425\n",
      "Epoch 242, Loss: 0.42125483246926426\n",
      "Epoch 243, Loss: 0.42124070061060576\n",
      "Epoch 244, Loss: 0.42073760533336746\n",
      "Epoch 245, Loss: 0.42002867294257734\n",
      "Epoch 246, Loss: 0.41900436945460057\n",
      "Epoch 247, Loss: 0.41841940783018283\n",
      "Epoch 248, Loss: 0.4184544673382656\n",
      "Epoch 249, Loss: 0.4187823781263808\n",
      "Epoch 250, Loss: 0.41914970907160026\n",
      "Epoch 251, Loss: 0.418702737788617\n",
      "Epoch 252, Loss: 0.41762532180686956\n",
      "Epoch 253, Loss: 0.41665732452727816\n",
      "Epoch 254, Loss: 0.4164608109104539\n",
      "Epoch 255, Loss: 0.4169362050698102\n",
      "Epoch 256, Loss: 0.4170439641107097\n",
      "Epoch 257, Loss: 0.41618098523090385\n",
      "Epoch 258, Loss: 0.41521870035352226\n",
      "Epoch 259, Loss: 0.41507164322372797\n",
      "Epoch 260, Loss: 0.4154855816987037\n",
      "Epoch 261, Loss: 0.4152875627561342\n",
      "Epoch 262, Loss: 0.4146051629547779\n",
      "Epoch 263, Loss: 0.4138602536580612\n",
      "Epoch 264, Loss: 0.4135535008669373\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2477237850902987\n",
      "Valence RMSE: 0.25885372018513775\n",
      "Arousal RMSE: 0.23606969086531074\n",
      "Test R^2 score: tensor([0.2840, 0.5890], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4364927147799973\n",
      "Num of epochs: 265\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Epoch 121, Loss: 0.4596922103619757\n",
      "Epoch 122, Loss: 0.45943389956138125\n",
      "Epoch 123, Loss: 0.45877818894117006\n",
      "Epoch 124, Loss: 0.45746538132874076\n",
      "Epoch 125, Loss: 0.4570462803121619\n",
      "Epoch 126, Loss: 0.45696189549769234\n",
      "Epoch 127, Loss: 0.45594920341421274\n",
      "Epoch 128, Loss: 0.45536359747685007\n",
      "Epoch 129, Loss: 0.4553290890752543\n",
      "Epoch 130, Loss: 0.4547195823074053\n",
      "Epoch 131, Loss: 0.45392838323998064\n",
      "Epoch 132, Loss: 0.4537853493595631\n",
      "Epoch 133, Loss: 0.45347437050767975\n",
      "Epoch 134, Loss: 0.45267431273812453\n",
      "Epoch 135, Loss: 0.45228737659373064\n",
      "Epoch 136, Loss: 0.4522154657432981\n",
      "Epoch 137, Loss: 0.45165964487407706\n",
      "Epoch 138, Loss: 0.4510751265462795\n",
      "Epoch 139, Loss: 0.45082650445054473\n",
      "Epoch 140, Loss: 0.4506281098405297\n",
      "Epoch 141, Loss: 0.4501911419656675\n",
      "Epoch 142, Loss: 0.4497001887659326\n",
      "Epoch 143, Loss: 0.44943436133718806\n",
      "Epoch 144, Loss: 0.449226296402944\n",
      "Epoch 145, Loss: 0.44886073897867573\n",
      "Epoch 146, Loss: 0.44844710376304864\n",
      "Epoch 147, Loss: 0.44810507013116724\n",
      "Epoch 148, Loss: 0.44788545862049767\n",
      "Epoch 149, Loss: 0.4476741441009651\n",
      "Epoch 150, Loss: 0.44735861731902055\n",
      "Epoch 151, Loss: 0.4469288720157105\n",
      "Epoch 152, Loss: 0.44655235754178146\n",
      "Epoch 153, Loss: 0.4462857402786373\n",
      "Epoch 154, Loss: 0.4460517369142754\n",
      "Epoch 155, Loss: 0.4457881796184935\n",
      "Epoch 156, Loss: 0.44542210886348255\n",
      "Epoch 157, Loss: 0.4450579467827662\n",
      "Epoch 158, Loss: 0.44473329320345206\n",
      "Epoch 159, Loss: 0.4444482514678638\n",
      "Epoch 160, Loss: 0.4441745674630603\n",
      "Epoch 161, Loss: 0.4439221813722748\n",
      "Epoch 162, Loss: 0.44363357870363757\n",
      "Epoch 163, Loss: 0.4433189071134186\n",
      "Epoch 164, Loss: 0.44295452976444744\n",
      "Epoch 165, Loss: 0.4426058108229731\n",
      "Epoch 166, Loss: 0.4422485787900922\n",
      "Epoch 167, Loss: 0.4419090985167976\n",
      "Epoch 168, Loss: 0.44158054387831963\n",
      "Epoch 169, Loss: 0.4412474388772268\n",
      "Epoch 170, Loss: 0.4409277694136744\n",
      "Epoch 171, Loss: 0.44068045571539705\n",
      "Epoch 172, Loss: 0.4405979418732049\n",
      "Epoch 173, Loss: 0.44094101686109266\n",
      "Epoch 174, Loss: 0.4417368233339534\n",
      "Epoch 175, Loss: 0.4416582519101522\n",
      "Epoch 176, Loss: 0.4396788970821407\n",
      "Epoch 177, Loss: 0.4390195954561405\n",
      "Epoch 178, Loss: 0.4400830988007806\n",
      "Epoch 179, Loss: 0.43931775869233325\n",
      "Epoch 180, Loss: 0.4380032505977321\n",
      "Epoch 181, Loss: 0.4384470974340077\n",
      "Epoch 182, Loss: 0.4385167636581309\n",
      "Epoch 183, Loss: 0.43733043450040754\n",
      "Epoch 184, Loss: 0.4373176399006318\n",
      "Epoch 185, Loss: 0.4375256973621567\n",
      "Epoch 186, Loss: 0.43659322296038683\n",
      "Epoch 187, Loss: 0.43644391045908093\n",
      "Epoch 188, Loss: 0.43663844358015674\n",
      "Epoch 189, Loss: 0.43586286705793914\n",
      "Epoch 190, Loss: 0.4355663072844155\n",
      "Epoch 191, Loss: 0.4357040532214924\n",
      "Epoch 192, Loss: 0.435134007760397\n",
      "Epoch 193, Loss: 0.43474241127906293\n",
      "Epoch 194, Loss: 0.43480482310319635\n",
      "Epoch 195, Loss: 0.43443826104982264\n",
      "Epoch 196, Loss: 0.43390767873021246\n",
      "Epoch 197, Loss: 0.4338897691233524\n",
      "Epoch 198, Loss: 0.43377856877811044\n",
      "Epoch 199, Loss: 0.4331595506415052\n",
      "Epoch 200, Loss: 0.43289415091975497\n",
      "Epoch 201, Loss: 0.43294048069736957\n",
      "Epoch 202, Loss: 0.43262240576772243\n",
      "Epoch 203, Loss: 0.43206541812705346\n",
      "Epoch 204, Loss: 0.43172500614102416\n",
      "Epoch 205, Loss: 0.4316686388133938\n",
      "Epoch 206, Loss: 0.4315179158466364\n",
      "Epoch 207, Loss: 0.4310712381632941\n",
      "Epoch 208, Loss: 0.43059470035496\n",
      "Epoch 209, Loss: 0.4303711221416647\n",
      "Epoch 210, Loss: 0.43033088719595897\n",
      "Epoch 211, Loss: 0.43016580489992484\n",
      "Epoch 212, Loss: 0.429691817522032\n",
      "Epoch 213, Loss: 0.4292099292794459\n",
      "Epoch 214, Loss: 0.4289207205910941\n",
      "Epoch 215, Loss: 0.4287978238495365\n",
      "Epoch 216, Loss: 0.42870603658529494\n",
      "Epoch 217, Loss: 0.42841980940051866\n",
      "Epoch 218, Loss: 0.4278918633922788\n",
      "Epoch 219, Loss: 0.42748307113702383\n",
      "Epoch 220, Loss: 0.4271688511370021\n",
      "Epoch 221, Loss: 0.4269432345934848\n",
      "Epoch 222, Loss: 0.4267651275776002\n",
      "Epoch 223, Loss: 0.42668376428587257\n",
      "Epoch 224, Loss: 0.42653554195413756\n",
      "Epoch 225, Loss: 0.42613293183996703\n",
      "Epoch 226, Loss: 0.4255451632780942\n",
      "Epoch 227, Loss: 0.4251305043179084\n",
      "Epoch 228, Loss: 0.42475934718412794\n",
      "Epoch 229, Loss: 0.42452283256309487\n",
      "Epoch 230, Loss: 0.4244024016195508\n",
      "Epoch 231, Loss: 0.4242604946441205\n",
      "Epoch 232, Loss: 0.424134367950937\n",
      "Epoch 233, Loss: 0.42372608274856827\n",
      "Epoch 234, Loss: 0.4231492155631465\n",
      "Epoch 235, Loss: 0.4226291943984805\n",
      "Epoch 236, Loss: 0.4221723179942825\n",
      "Epoch 237, Loss: 0.4218955917984354\n",
      "Epoch 238, Loss: 0.4216767834434851\n",
      "Epoch 239, Loss: 0.42145169178243785\n",
      "Epoch 240, Loss: 0.4212605805865247\n",
      "Epoch 241, Loss: 0.42123380253494425\n",
      "Epoch 242, Loss: 0.42125483246926426\n",
      "Epoch 243, Loss: 0.42124070061060576\n",
      "Epoch 244, Loss: 0.42073760533336746\n",
      "Epoch 245, Loss: 0.42002867294257734\n",
      "Epoch 246, Loss: 0.41900436945460057\n",
      "Epoch 247, Loss: 0.41841940783018283\n",
      "Epoch 248, Loss: 0.4184544673382656\n",
      "Epoch 249, Loss: 0.4187823781263808\n",
      "Epoch 250, Loss: 0.41914970907160026\n",
      "Epoch 251, Loss: 0.418702737788617\n",
      "Epoch 252, Loss: 0.41762532180686956\n",
      "Epoch 253, Loss: 0.41665732452727816\n",
      "Epoch 254, Loss: 0.4164608109104539\n",
      "Epoch 255, Loss: 0.4169362050698102\n",
      "Epoch 256, Loss: 0.4170439641107097\n",
      "Epoch 257, Loss: 0.41618098523090385\n",
      "Epoch 258, Loss: 0.41521870035352226\n",
      "Epoch 259, Loss: 0.41507164322372797\n",
      "Epoch 260, Loss: 0.4154855816987037\n",
      "Epoch 261, Loss: 0.4152875627561342\n",
      "Epoch 262, Loss: 0.4146051629547779\n",
      "Epoch 263, Loss: 0.4138602536580612\n",
      "Epoch 264, Loss: 0.4135535008669373\n",
      "Epoch 265, Loss: 0.4136485063474015\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.24802376879020072\n",
      "Valence RMSE: 0.258601977986835\n",
      "Arousal RMSE: 0.23697383136347863\n",
      "Test R^2 score: tensor([0.2854, 0.5858], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4356115943405735\n",
      "Num of epochs: 266\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Epoch 121, Loss: 0.4596922103619757\n",
      "Epoch 122, Loss: 0.45943389956138125\n",
      "Epoch 123, Loss: 0.45877818894117006\n",
      "Epoch 124, Loss: 0.45746538132874076\n",
      "Epoch 125, Loss: 0.4570462803121619\n",
      "Epoch 126, Loss: 0.45696189549769234\n",
      "Epoch 127, Loss: 0.45594920341421274\n",
      "Epoch 128, Loss: 0.45536359747685007\n",
      "Epoch 129, Loss: 0.4553290890752543\n",
      "Epoch 130, Loss: 0.4547195823074053\n",
      "Epoch 131, Loss: 0.45392838323998064\n",
      "Epoch 132, Loss: 0.4537853493595631\n",
      "Epoch 133, Loss: 0.45347437050767975\n",
      "Epoch 134, Loss: 0.45267431273812453\n",
      "Epoch 135, Loss: 0.45228737659373064\n",
      "Epoch 136, Loss: 0.4522154657432981\n",
      "Epoch 137, Loss: 0.45165964487407706\n",
      "Epoch 138, Loss: 0.4510751265462795\n",
      "Epoch 139, Loss: 0.45082650445054473\n",
      "Epoch 140, Loss: 0.4506281098405297\n",
      "Epoch 141, Loss: 0.4501911419656675\n",
      "Epoch 142, Loss: 0.4497001887659326\n",
      "Epoch 143, Loss: 0.44943436133718806\n",
      "Epoch 144, Loss: 0.449226296402944\n",
      "Epoch 145, Loss: 0.44886073897867573\n",
      "Epoch 146, Loss: 0.44844710376304864\n",
      "Epoch 147, Loss: 0.44810507013116724\n",
      "Epoch 148, Loss: 0.44788545862049767\n",
      "Epoch 149, Loss: 0.4476741441009651\n",
      "Epoch 150, Loss: 0.44735861731902055\n",
      "Epoch 151, Loss: 0.4469288720157105\n",
      "Epoch 152, Loss: 0.44655235754178146\n",
      "Epoch 153, Loss: 0.4462857402786373\n",
      "Epoch 154, Loss: 0.4460517369142754\n",
      "Epoch 155, Loss: 0.4457881796184935\n",
      "Epoch 156, Loss: 0.44542210886348255\n",
      "Epoch 157, Loss: 0.4450579467827662\n",
      "Epoch 158, Loss: 0.44473329320345206\n",
      "Epoch 159, Loss: 0.4444482514678638\n",
      "Epoch 160, Loss: 0.4441745674630603\n",
      "Epoch 161, Loss: 0.4439221813722748\n",
      "Epoch 162, Loss: 0.44363357870363757\n",
      "Epoch 163, Loss: 0.4433189071134186\n",
      "Epoch 164, Loss: 0.44295452976444744\n",
      "Epoch 165, Loss: 0.4426058108229731\n",
      "Epoch 166, Loss: 0.4422485787900922\n",
      "Epoch 167, Loss: 0.4419090985167976\n",
      "Epoch 168, Loss: 0.44158054387831963\n",
      "Epoch 169, Loss: 0.4412474388772268\n",
      "Epoch 170, Loss: 0.4409277694136744\n",
      "Epoch 171, Loss: 0.44068045571539705\n",
      "Epoch 172, Loss: 0.4405979418732049\n",
      "Epoch 173, Loss: 0.44094101686109266\n",
      "Epoch 174, Loss: 0.4417368233339534\n",
      "Epoch 175, Loss: 0.4416582519101522\n",
      "Epoch 176, Loss: 0.4396788970821407\n",
      "Epoch 177, Loss: 0.4390195954561405\n",
      "Epoch 178, Loss: 0.4400830988007806\n",
      "Epoch 179, Loss: 0.43931775869233325\n",
      "Epoch 180, Loss: 0.4380032505977321\n",
      "Epoch 181, Loss: 0.4384470974340077\n",
      "Epoch 182, Loss: 0.4385167636581309\n",
      "Epoch 183, Loss: 0.43733043450040754\n",
      "Epoch 184, Loss: 0.4373176399006318\n",
      "Epoch 185, Loss: 0.4375256973621567\n",
      "Epoch 186, Loss: 0.43659322296038683\n",
      "Epoch 187, Loss: 0.43644391045908093\n",
      "Epoch 188, Loss: 0.43663844358015674\n",
      "Epoch 189, Loss: 0.43586286705793914\n",
      "Epoch 190, Loss: 0.4355663072844155\n",
      "Epoch 191, Loss: 0.4357040532214924\n",
      "Epoch 192, Loss: 0.435134007760397\n",
      "Epoch 193, Loss: 0.43474241127906293\n",
      "Epoch 194, Loss: 0.43480482310319635\n",
      "Epoch 195, Loss: 0.43443826104982264\n",
      "Epoch 196, Loss: 0.43390767873021246\n",
      "Epoch 197, Loss: 0.4338897691233524\n",
      "Epoch 198, Loss: 0.43377856877811044\n",
      "Epoch 199, Loss: 0.4331595506415052\n",
      "Epoch 200, Loss: 0.43289415091975497\n",
      "Epoch 201, Loss: 0.43294048069736957\n",
      "Epoch 202, Loss: 0.43262240576772243\n",
      "Epoch 203, Loss: 0.43206541812705346\n",
      "Epoch 204, Loss: 0.43172500614102416\n",
      "Epoch 205, Loss: 0.4316686388133938\n",
      "Epoch 206, Loss: 0.4315179158466364\n",
      "Epoch 207, Loss: 0.4310712381632941\n",
      "Epoch 208, Loss: 0.43059470035496\n",
      "Epoch 209, Loss: 0.4303711221416647\n",
      "Epoch 210, Loss: 0.43033088719595897\n",
      "Epoch 211, Loss: 0.43016580489992484\n",
      "Epoch 212, Loss: 0.429691817522032\n",
      "Epoch 213, Loss: 0.4292099292794459\n",
      "Epoch 214, Loss: 0.4289207205910941\n",
      "Epoch 215, Loss: 0.4287978238495365\n",
      "Epoch 216, Loss: 0.42870603658529494\n",
      "Epoch 217, Loss: 0.42841980940051866\n",
      "Epoch 218, Loss: 0.4278918633922788\n",
      "Epoch 219, Loss: 0.42748307113702383\n",
      "Epoch 220, Loss: 0.4271688511370021\n",
      "Epoch 221, Loss: 0.4269432345934848\n",
      "Epoch 222, Loss: 0.4267651275776002\n",
      "Epoch 223, Loss: 0.42668376428587257\n",
      "Epoch 224, Loss: 0.42653554195413756\n",
      "Epoch 225, Loss: 0.42613293183996703\n",
      "Epoch 226, Loss: 0.4255451632780942\n",
      "Epoch 227, Loss: 0.4251305043179084\n",
      "Epoch 228, Loss: 0.42475934718412794\n",
      "Epoch 229, Loss: 0.42452283256309487\n",
      "Epoch 230, Loss: 0.4244024016195508\n",
      "Epoch 231, Loss: 0.4242604946441205\n",
      "Epoch 232, Loss: 0.424134367950937\n",
      "Epoch 233, Loss: 0.42372608274856827\n",
      "Epoch 234, Loss: 0.4231492155631465\n",
      "Epoch 235, Loss: 0.4226291943984805\n",
      "Epoch 236, Loss: 0.4221723179942825\n",
      "Epoch 237, Loss: 0.4218955917984354\n",
      "Epoch 238, Loss: 0.4216767834434851\n",
      "Epoch 239, Loss: 0.42145169178243785\n",
      "Epoch 240, Loss: 0.4212605805865247\n",
      "Epoch 241, Loss: 0.42123380253494425\n",
      "Epoch 242, Loss: 0.42125483246926426\n",
      "Epoch 243, Loss: 0.42124070061060576\n",
      "Epoch 244, Loss: 0.42073760533336746\n",
      "Epoch 245, Loss: 0.42002867294257734\n",
      "Epoch 246, Loss: 0.41900436945460057\n",
      "Epoch 247, Loss: 0.41841940783018283\n",
      "Epoch 248, Loss: 0.4184544673382656\n",
      "Epoch 249, Loss: 0.4187823781263808\n",
      "Epoch 250, Loss: 0.41914970907160026\n",
      "Epoch 251, Loss: 0.418702737788617\n",
      "Epoch 252, Loss: 0.41762532180686956\n",
      "Epoch 253, Loss: 0.41665732452727816\n",
      "Epoch 254, Loss: 0.4164608109104539\n",
      "Epoch 255, Loss: 0.4169362050698102\n",
      "Epoch 256, Loss: 0.4170439641107097\n",
      "Epoch 257, Loss: 0.41618098523090385\n",
      "Epoch 258, Loss: 0.41521870035352226\n",
      "Epoch 259, Loss: 0.41507164322372797\n",
      "Epoch 260, Loss: 0.4154855816987037\n",
      "Epoch 261, Loss: 0.4152875627561342\n",
      "Epoch 262, Loss: 0.4146051629547779\n",
      "Epoch 263, Loss: 0.4138602536580612\n",
      "Epoch 264, Loss: 0.4135535008669373\n",
      "Epoch 265, Loss: 0.4136485063474015\n",
      "Epoch 266, Loss: 0.4135539332507534\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.24899124323236774\n",
      "Valence RMSE: 0.26048144094151554\n",
      "Arousal RMSE: 0.23694450265374795\n",
      "Test R^2 score: tensor([0.2750, 0.5859], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.43045024120187436\n",
      "Num of epochs: 267\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Epoch 121, Loss: 0.4596922103619757\n",
      "Epoch 122, Loss: 0.45943389956138125\n",
      "Epoch 123, Loss: 0.45877818894117006\n",
      "Epoch 124, Loss: 0.45746538132874076\n",
      "Epoch 125, Loss: 0.4570462803121619\n",
      "Epoch 126, Loss: 0.45696189549769234\n",
      "Epoch 127, Loss: 0.45594920341421274\n",
      "Epoch 128, Loss: 0.45536359747685007\n",
      "Epoch 129, Loss: 0.4553290890752543\n",
      "Epoch 130, Loss: 0.4547195823074053\n",
      "Epoch 131, Loss: 0.45392838323998064\n",
      "Epoch 132, Loss: 0.4537853493595631\n",
      "Epoch 133, Loss: 0.45347437050767975\n",
      "Epoch 134, Loss: 0.45267431273812453\n",
      "Epoch 135, Loss: 0.45228737659373064\n",
      "Epoch 136, Loss: 0.4522154657432981\n",
      "Epoch 137, Loss: 0.45165964487407706\n",
      "Epoch 138, Loss: 0.4510751265462795\n",
      "Epoch 139, Loss: 0.45082650445054473\n",
      "Epoch 140, Loss: 0.4506281098405297\n",
      "Epoch 141, Loss: 0.4501911419656675\n",
      "Epoch 142, Loss: 0.4497001887659326\n",
      "Epoch 143, Loss: 0.44943436133718806\n",
      "Epoch 144, Loss: 0.449226296402944\n",
      "Epoch 145, Loss: 0.44886073897867573\n",
      "Epoch 146, Loss: 0.44844710376304864\n",
      "Epoch 147, Loss: 0.44810507013116724\n",
      "Epoch 148, Loss: 0.44788545862049767\n",
      "Epoch 149, Loss: 0.4476741441009651\n",
      "Epoch 150, Loss: 0.44735861731902055\n",
      "Epoch 151, Loss: 0.4469288720157105\n",
      "Epoch 152, Loss: 0.44655235754178146\n",
      "Epoch 153, Loss: 0.4462857402786373\n",
      "Epoch 154, Loss: 0.4460517369142754\n",
      "Epoch 155, Loss: 0.4457881796184935\n",
      "Epoch 156, Loss: 0.44542210886348255\n",
      "Epoch 157, Loss: 0.4450579467827662\n",
      "Epoch 158, Loss: 0.44473329320345206\n",
      "Epoch 159, Loss: 0.4444482514678638\n",
      "Epoch 160, Loss: 0.4441745674630603\n",
      "Epoch 161, Loss: 0.4439221813722748\n",
      "Epoch 162, Loss: 0.44363357870363757\n",
      "Epoch 163, Loss: 0.4433189071134186\n",
      "Epoch 164, Loss: 0.44295452976444744\n",
      "Epoch 165, Loss: 0.4426058108229731\n",
      "Epoch 166, Loss: 0.4422485787900922\n",
      "Epoch 167, Loss: 0.4419090985167976\n",
      "Epoch 168, Loss: 0.44158054387831963\n",
      "Epoch 169, Loss: 0.4412474388772268\n",
      "Epoch 170, Loss: 0.4409277694136744\n",
      "Epoch 171, Loss: 0.44068045571539705\n",
      "Epoch 172, Loss: 0.4405979418732049\n",
      "Epoch 173, Loss: 0.44094101686109266\n",
      "Epoch 174, Loss: 0.4417368233339534\n",
      "Epoch 175, Loss: 0.4416582519101522\n",
      "Epoch 176, Loss: 0.4396788970821407\n",
      "Epoch 177, Loss: 0.4390195954561405\n",
      "Epoch 178, Loss: 0.4400830988007806\n",
      "Epoch 179, Loss: 0.43931775869233325\n",
      "Epoch 180, Loss: 0.4380032505977321\n",
      "Epoch 181, Loss: 0.4384470974340077\n",
      "Epoch 182, Loss: 0.4385167636581309\n",
      "Epoch 183, Loss: 0.43733043450040754\n",
      "Epoch 184, Loss: 0.4373176399006318\n",
      "Epoch 185, Loss: 0.4375256973621567\n",
      "Epoch 186, Loss: 0.43659322296038683\n",
      "Epoch 187, Loss: 0.43644391045908093\n",
      "Epoch 188, Loss: 0.43663844358015674\n",
      "Epoch 189, Loss: 0.43586286705793914\n",
      "Epoch 190, Loss: 0.4355663072844155\n",
      "Epoch 191, Loss: 0.4357040532214924\n",
      "Epoch 192, Loss: 0.435134007760397\n",
      "Epoch 193, Loss: 0.43474241127906293\n",
      "Epoch 194, Loss: 0.43480482310319635\n",
      "Epoch 195, Loss: 0.43443826104982264\n",
      "Epoch 196, Loss: 0.43390767873021246\n",
      "Epoch 197, Loss: 0.4338897691233524\n",
      "Epoch 198, Loss: 0.43377856877811044\n",
      "Epoch 199, Loss: 0.4331595506415052\n",
      "Epoch 200, Loss: 0.43289415091975497\n",
      "Epoch 201, Loss: 0.43294048069736957\n",
      "Epoch 202, Loss: 0.43262240576772243\n",
      "Epoch 203, Loss: 0.43206541812705346\n",
      "Epoch 204, Loss: 0.43172500614102416\n",
      "Epoch 205, Loss: 0.4316686388133938\n",
      "Epoch 206, Loss: 0.4315179158466364\n",
      "Epoch 207, Loss: 0.4310712381632941\n",
      "Epoch 208, Loss: 0.43059470035496\n",
      "Epoch 209, Loss: 0.4303711221416647\n",
      "Epoch 210, Loss: 0.43033088719595897\n",
      "Epoch 211, Loss: 0.43016580489992484\n",
      "Epoch 212, Loss: 0.429691817522032\n",
      "Epoch 213, Loss: 0.4292099292794459\n",
      "Epoch 214, Loss: 0.4289207205910941\n",
      "Epoch 215, Loss: 0.4287978238495365\n",
      "Epoch 216, Loss: 0.42870603658529494\n",
      "Epoch 217, Loss: 0.42841980940051866\n",
      "Epoch 218, Loss: 0.4278918633922788\n",
      "Epoch 219, Loss: 0.42748307113702383\n",
      "Epoch 220, Loss: 0.4271688511370021\n",
      "Epoch 221, Loss: 0.4269432345934848\n",
      "Epoch 222, Loss: 0.4267651275776002\n",
      "Epoch 223, Loss: 0.42668376428587257\n",
      "Epoch 224, Loss: 0.42653554195413756\n",
      "Epoch 225, Loss: 0.42613293183996703\n",
      "Epoch 226, Loss: 0.4255451632780942\n",
      "Epoch 227, Loss: 0.4251305043179084\n",
      "Epoch 228, Loss: 0.42475934718412794\n",
      "Epoch 229, Loss: 0.42452283256309487\n",
      "Epoch 230, Loss: 0.4244024016195508\n",
      "Epoch 231, Loss: 0.4242604946441205\n",
      "Epoch 232, Loss: 0.424134367950937\n",
      "Epoch 233, Loss: 0.42372608274856827\n",
      "Epoch 234, Loss: 0.4231492155631465\n",
      "Epoch 235, Loss: 0.4226291943984805\n",
      "Epoch 236, Loss: 0.4221723179942825\n",
      "Epoch 237, Loss: 0.4218955917984354\n",
      "Epoch 238, Loss: 0.4216767834434851\n",
      "Epoch 239, Loss: 0.42145169178243785\n",
      "Epoch 240, Loss: 0.4212605805865247\n",
      "Epoch 241, Loss: 0.42123380253494425\n",
      "Epoch 242, Loss: 0.42125483246926426\n",
      "Epoch 243, Loss: 0.42124070061060576\n",
      "Epoch 244, Loss: 0.42073760533336746\n",
      "Epoch 245, Loss: 0.42002867294257734\n",
      "Epoch 246, Loss: 0.41900436945460057\n",
      "Epoch 247, Loss: 0.41841940783018283\n",
      "Epoch 248, Loss: 0.4184544673382656\n",
      "Epoch 249, Loss: 0.4187823781263808\n",
      "Epoch 250, Loss: 0.41914970907160026\n",
      "Epoch 251, Loss: 0.418702737788617\n",
      "Epoch 252, Loss: 0.41762532180686956\n",
      "Epoch 253, Loss: 0.41665732452727816\n",
      "Epoch 254, Loss: 0.4164608109104539\n",
      "Epoch 255, Loss: 0.4169362050698102\n",
      "Epoch 256, Loss: 0.4170439641107097\n",
      "Epoch 257, Loss: 0.41618098523090385\n",
      "Epoch 258, Loss: 0.41521870035352226\n",
      "Epoch 259, Loss: 0.41507164322372797\n",
      "Epoch 260, Loss: 0.4154855816987037\n",
      "Epoch 261, Loss: 0.4152875627561342\n",
      "Epoch 262, Loss: 0.4146051629547779\n",
      "Epoch 263, Loss: 0.4138602536580612\n",
      "Epoch 264, Loss: 0.4135535008669373\n",
      "Epoch 265, Loss: 0.4136485063474015\n",
      "Epoch 266, Loss: 0.4135539332507534\n",
      "Epoch 267, Loss: 0.4133421191440525\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.24813100105775687\n",
      "Valence RMSE: 0.2584300342794857\n",
      "Arousal RMSE: 0.2373855613851713\n",
      "Test R^2 score: tensor([0.2863, 0.5844], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4353663937123353\n",
      "Num of epochs: 268\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Epoch 121, Loss: 0.4596922103619757\n",
      "Epoch 122, Loss: 0.45943389956138125\n",
      "Epoch 123, Loss: 0.45877818894117006\n",
      "Epoch 124, Loss: 0.45746538132874076\n",
      "Epoch 125, Loss: 0.4570462803121619\n",
      "Epoch 126, Loss: 0.45696189549769234\n",
      "Epoch 127, Loss: 0.45594920341421274\n",
      "Epoch 128, Loss: 0.45536359747685007\n",
      "Epoch 129, Loss: 0.4553290890752543\n",
      "Epoch 130, Loss: 0.4547195823074053\n",
      "Epoch 131, Loss: 0.45392838323998064\n",
      "Epoch 132, Loss: 0.4537853493595631\n",
      "Epoch 133, Loss: 0.45347437050767975\n",
      "Epoch 134, Loss: 0.45267431273812453\n",
      "Epoch 135, Loss: 0.45228737659373064\n",
      "Epoch 136, Loss: 0.4522154657432981\n",
      "Epoch 137, Loss: 0.45165964487407706\n",
      "Epoch 138, Loss: 0.4510751265462795\n",
      "Epoch 139, Loss: 0.45082650445054473\n",
      "Epoch 140, Loss: 0.4506281098405297\n",
      "Epoch 141, Loss: 0.4501911419656675\n",
      "Epoch 142, Loss: 0.4497001887659326\n",
      "Epoch 143, Loss: 0.44943436133718806\n",
      "Epoch 144, Loss: 0.449226296402944\n",
      "Epoch 145, Loss: 0.44886073897867573\n",
      "Epoch 146, Loss: 0.44844710376304864\n",
      "Epoch 147, Loss: 0.44810507013116724\n",
      "Epoch 148, Loss: 0.44788545862049767\n",
      "Epoch 149, Loss: 0.4476741441009651\n",
      "Epoch 150, Loss: 0.44735861731902055\n",
      "Epoch 151, Loss: 0.4469288720157105\n",
      "Epoch 152, Loss: 0.44655235754178146\n",
      "Epoch 153, Loss: 0.4462857402786373\n",
      "Epoch 154, Loss: 0.4460517369142754\n",
      "Epoch 155, Loss: 0.4457881796184935\n",
      "Epoch 156, Loss: 0.44542210886348255\n",
      "Epoch 157, Loss: 0.4450579467827662\n",
      "Epoch 158, Loss: 0.44473329320345206\n",
      "Epoch 159, Loss: 0.4444482514678638\n",
      "Epoch 160, Loss: 0.4441745674630603\n",
      "Epoch 161, Loss: 0.4439221813722748\n",
      "Epoch 162, Loss: 0.44363357870363757\n",
      "Epoch 163, Loss: 0.4433189071134186\n",
      "Epoch 164, Loss: 0.44295452976444744\n",
      "Epoch 165, Loss: 0.4426058108229731\n",
      "Epoch 166, Loss: 0.4422485787900922\n",
      "Epoch 167, Loss: 0.4419090985167976\n",
      "Epoch 168, Loss: 0.44158054387831963\n",
      "Epoch 169, Loss: 0.4412474388772268\n",
      "Epoch 170, Loss: 0.4409277694136744\n",
      "Epoch 171, Loss: 0.44068045571539705\n",
      "Epoch 172, Loss: 0.4405979418732049\n",
      "Epoch 173, Loss: 0.44094101686109266\n",
      "Epoch 174, Loss: 0.4417368233339534\n",
      "Epoch 175, Loss: 0.4416582519101522\n",
      "Epoch 176, Loss: 0.4396788970821407\n",
      "Epoch 177, Loss: 0.4390195954561405\n",
      "Epoch 178, Loss: 0.4400830988007806\n",
      "Epoch 179, Loss: 0.43931775869233325\n",
      "Epoch 180, Loss: 0.4380032505977321\n",
      "Epoch 181, Loss: 0.4384470974340077\n",
      "Epoch 182, Loss: 0.4385167636581309\n",
      "Epoch 183, Loss: 0.43733043450040754\n",
      "Epoch 184, Loss: 0.4373176399006318\n",
      "Epoch 185, Loss: 0.4375256973621567\n",
      "Epoch 186, Loss: 0.43659322296038683\n",
      "Epoch 187, Loss: 0.43644391045908093\n",
      "Epoch 188, Loss: 0.43663844358015674\n",
      "Epoch 189, Loss: 0.43586286705793914\n",
      "Epoch 190, Loss: 0.4355663072844155\n",
      "Epoch 191, Loss: 0.4357040532214924\n",
      "Epoch 192, Loss: 0.435134007760397\n",
      "Epoch 193, Loss: 0.43474241127906293\n",
      "Epoch 194, Loss: 0.43480482310319635\n",
      "Epoch 195, Loss: 0.43443826104982264\n",
      "Epoch 196, Loss: 0.43390767873021246\n",
      "Epoch 197, Loss: 0.4338897691233524\n",
      "Epoch 198, Loss: 0.43377856877811044\n",
      "Epoch 199, Loss: 0.4331595506415052\n",
      "Epoch 200, Loss: 0.43289415091975497\n",
      "Epoch 201, Loss: 0.43294048069736957\n",
      "Epoch 202, Loss: 0.43262240576772243\n",
      "Epoch 203, Loss: 0.43206541812705346\n",
      "Epoch 204, Loss: 0.43172500614102416\n",
      "Epoch 205, Loss: 0.4316686388133938\n",
      "Epoch 206, Loss: 0.4315179158466364\n",
      "Epoch 207, Loss: 0.4310712381632941\n",
      "Epoch 208, Loss: 0.43059470035496\n",
      "Epoch 209, Loss: 0.4303711221416647\n",
      "Epoch 210, Loss: 0.43033088719595897\n",
      "Epoch 211, Loss: 0.43016580489992484\n",
      "Epoch 212, Loss: 0.429691817522032\n",
      "Epoch 213, Loss: 0.4292099292794459\n",
      "Epoch 214, Loss: 0.4289207205910941\n",
      "Epoch 215, Loss: 0.4287978238495365\n",
      "Epoch 216, Loss: 0.42870603658529494\n",
      "Epoch 217, Loss: 0.42841980940051866\n",
      "Epoch 218, Loss: 0.4278918633922788\n",
      "Epoch 219, Loss: 0.42748307113702383\n",
      "Epoch 220, Loss: 0.4271688511370021\n",
      "Epoch 221, Loss: 0.4269432345934848\n",
      "Epoch 222, Loss: 0.4267651275776002\n",
      "Epoch 223, Loss: 0.42668376428587257\n",
      "Epoch 224, Loss: 0.42653554195413756\n",
      "Epoch 225, Loss: 0.42613293183996703\n",
      "Epoch 226, Loss: 0.4255451632780942\n",
      "Epoch 227, Loss: 0.4251305043179084\n",
      "Epoch 228, Loss: 0.42475934718412794\n",
      "Epoch 229, Loss: 0.42452283256309487\n",
      "Epoch 230, Loss: 0.4244024016195508\n",
      "Epoch 231, Loss: 0.4242604946441205\n",
      "Epoch 232, Loss: 0.424134367950937\n",
      "Epoch 233, Loss: 0.42372608274856827\n",
      "Epoch 234, Loss: 0.4231492155631465\n",
      "Epoch 235, Loss: 0.4226291943984805\n",
      "Epoch 236, Loss: 0.4221723179942825\n",
      "Epoch 237, Loss: 0.4218955917984354\n",
      "Epoch 238, Loss: 0.4216767834434851\n",
      "Epoch 239, Loss: 0.42145169178243785\n",
      "Epoch 240, Loss: 0.4212605805865247\n",
      "Epoch 241, Loss: 0.42123380253494425\n",
      "Epoch 242, Loss: 0.42125483246926426\n",
      "Epoch 243, Loss: 0.42124070061060576\n",
      "Epoch 244, Loss: 0.42073760533336746\n",
      "Epoch 245, Loss: 0.42002867294257734\n",
      "Epoch 246, Loss: 0.41900436945460057\n",
      "Epoch 247, Loss: 0.41841940783018283\n",
      "Epoch 248, Loss: 0.4184544673382656\n",
      "Epoch 249, Loss: 0.4187823781263808\n",
      "Epoch 250, Loss: 0.41914970907160026\n",
      "Epoch 251, Loss: 0.418702737788617\n",
      "Epoch 252, Loss: 0.41762532180686956\n",
      "Epoch 253, Loss: 0.41665732452727816\n",
      "Epoch 254, Loss: 0.4164608109104539\n",
      "Epoch 255, Loss: 0.4169362050698102\n",
      "Epoch 256, Loss: 0.4170439641107097\n",
      "Epoch 257, Loss: 0.41618098523090385\n",
      "Epoch 258, Loss: 0.41521870035352226\n",
      "Epoch 259, Loss: 0.41507164322372797\n",
      "Epoch 260, Loss: 0.4154855816987037\n",
      "Epoch 261, Loss: 0.4152875627561342\n",
      "Epoch 262, Loss: 0.4146051629547779\n",
      "Epoch 263, Loss: 0.4138602536580612\n",
      "Epoch 264, Loss: 0.4135535008669373\n",
      "Epoch 265, Loss: 0.4136485063474015\n",
      "Epoch 266, Loss: 0.4135539332507534\n",
      "Epoch 267, Loss: 0.4133421191440525\n",
      "Epoch 268, Loss: 0.412685660282317\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.24797897460540486\n",
      "Valence RMSE: 0.2585504375825097\n",
      "Arousal RMSE: 0.23693630983576325\n",
      "Test R^2 score: tensor([0.2857, 0.5860], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.435819577742764\n",
      "Num of epochs: 269\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Epoch 121, Loss: 0.4596922103619757\n",
      "Epoch 122, Loss: 0.45943389956138125\n",
      "Epoch 123, Loss: 0.45877818894117006\n",
      "Epoch 124, Loss: 0.45746538132874076\n",
      "Epoch 125, Loss: 0.4570462803121619\n",
      "Epoch 126, Loss: 0.45696189549769234\n",
      "Epoch 127, Loss: 0.45594920341421274\n",
      "Epoch 128, Loss: 0.45536359747685007\n",
      "Epoch 129, Loss: 0.4553290890752543\n",
      "Epoch 130, Loss: 0.4547195823074053\n",
      "Epoch 131, Loss: 0.45392838323998064\n",
      "Epoch 132, Loss: 0.4537853493595631\n",
      "Epoch 133, Loss: 0.45347437050767975\n",
      "Epoch 134, Loss: 0.45267431273812453\n",
      "Epoch 135, Loss: 0.45228737659373064\n",
      "Epoch 136, Loss: 0.4522154657432981\n",
      "Epoch 137, Loss: 0.45165964487407706\n",
      "Epoch 138, Loss: 0.4510751265462795\n",
      "Epoch 139, Loss: 0.45082650445054473\n",
      "Epoch 140, Loss: 0.4506281098405297\n",
      "Epoch 141, Loss: 0.4501911419656675\n",
      "Epoch 142, Loss: 0.4497001887659326\n",
      "Epoch 143, Loss: 0.44943436133718806\n",
      "Epoch 144, Loss: 0.449226296402944\n",
      "Epoch 145, Loss: 0.44886073897867573\n",
      "Epoch 146, Loss: 0.44844710376304864\n",
      "Epoch 147, Loss: 0.44810507013116724\n",
      "Epoch 148, Loss: 0.44788545862049767\n",
      "Epoch 149, Loss: 0.4476741441009651\n",
      "Epoch 150, Loss: 0.44735861731902055\n",
      "Epoch 151, Loss: 0.4469288720157105\n",
      "Epoch 152, Loss: 0.44655235754178146\n",
      "Epoch 153, Loss: 0.4462857402786373\n",
      "Epoch 154, Loss: 0.4460517369142754\n",
      "Epoch 155, Loss: 0.4457881796184935\n",
      "Epoch 156, Loss: 0.44542210886348255\n",
      "Epoch 157, Loss: 0.4450579467827662\n",
      "Epoch 158, Loss: 0.44473329320345206\n",
      "Epoch 159, Loss: 0.4444482514678638\n",
      "Epoch 160, Loss: 0.4441745674630603\n",
      "Epoch 161, Loss: 0.4439221813722748\n",
      "Epoch 162, Loss: 0.44363357870363757\n",
      "Epoch 163, Loss: 0.4433189071134186\n",
      "Epoch 164, Loss: 0.44295452976444744\n",
      "Epoch 165, Loss: 0.4426058108229731\n",
      "Epoch 166, Loss: 0.4422485787900922\n",
      "Epoch 167, Loss: 0.4419090985167976\n",
      "Epoch 168, Loss: 0.44158054387831963\n",
      "Epoch 169, Loss: 0.4412474388772268\n",
      "Epoch 170, Loss: 0.4409277694136744\n",
      "Epoch 171, Loss: 0.44068045571539705\n",
      "Epoch 172, Loss: 0.4405979418732049\n",
      "Epoch 173, Loss: 0.44094101686109266\n",
      "Epoch 174, Loss: 0.4417368233339534\n",
      "Epoch 175, Loss: 0.4416582519101522\n",
      "Epoch 176, Loss: 0.4396788970821407\n",
      "Epoch 177, Loss: 0.4390195954561405\n",
      "Epoch 178, Loss: 0.4400830988007806\n",
      "Epoch 179, Loss: 0.43931775869233325\n",
      "Epoch 180, Loss: 0.4380032505977321\n",
      "Epoch 181, Loss: 0.4384470974340077\n",
      "Epoch 182, Loss: 0.4385167636581309\n",
      "Epoch 183, Loss: 0.43733043450040754\n",
      "Epoch 184, Loss: 0.4373176399006318\n",
      "Epoch 185, Loss: 0.4375256973621567\n",
      "Epoch 186, Loss: 0.43659322296038683\n",
      "Epoch 187, Loss: 0.43644391045908093\n",
      "Epoch 188, Loss: 0.43663844358015674\n",
      "Epoch 189, Loss: 0.43586286705793914\n",
      "Epoch 190, Loss: 0.4355663072844155\n",
      "Epoch 191, Loss: 0.4357040532214924\n",
      "Epoch 192, Loss: 0.435134007760397\n",
      "Epoch 193, Loss: 0.43474241127906293\n",
      "Epoch 194, Loss: 0.43480482310319635\n",
      "Epoch 195, Loss: 0.43443826104982264\n",
      "Epoch 196, Loss: 0.43390767873021246\n",
      "Epoch 197, Loss: 0.4338897691233524\n",
      "Epoch 198, Loss: 0.43377856877811044\n",
      "Epoch 199, Loss: 0.4331595506415052\n",
      "Epoch 200, Loss: 0.43289415091975497\n",
      "Epoch 201, Loss: 0.43294048069736957\n",
      "Epoch 202, Loss: 0.43262240576772243\n",
      "Epoch 203, Loss: 0.43206541812705346\n",
      "Epoch 204, Loss: 0.43172500614102416\n",
      "Epoch 205, Loss: 0.4316686388133938\n",
      "Epoch 206, Loss: 0.4315179158466364\n",
      "Epoch 207, Loss: 0.4310712381632941\n",
      "Epoch 208, Loss: 0.43059470035496\n",
      "Epoch 209, Loss: 0.4303711221416647\n",
      "Epoch 210, Loss: 0.43033088719595897\n",
      "Epoch 211, Loss: 0.43016580489992484\n",
      "Epoch 212, Loss: 0.429691817522032\n",
      "Epoch 213, Loss: 0.4292099292794459\n",
      "Epoch 214, Loss: 0.4289207205910941\n",
      "Epoch 215, Loss: 0.4287978238495365\n",
      "Epoch 216, Loss: 0.42870603658529494\n",
      "Epoch 217, Loss: 0.42841980940051866\n",
      "Epoch 218, Loss: 0.4278918633922788\n",
      "Epoch 219, Loss: 0.42748307113702383\n",
      "Epoch 220, Loss: 0.4271688511370021\n",
      "Epoch 221, Loss: 0.4269432345934848\n",
      "Epoch 222, Loss: 0.4267651275776002\n",
      "Epoch 223, Loss: 0.42668376428587257\n",
      "Epoch 224, Loss: 0.42653554195413756\n",
      "Epoch 225, Loss: 0.42613293183996703\n",
      "Epoch 226, Loss: 0.4255451632780942\n",
      "Epoch 227, Loss: 0.4251305043179084\n",
      "Epoch 228, Loss: 0.42475934718412794\n",
      "Epoch 229, Loss: 0.42452283256309487\n",
      "Epoch 230, Loss: 0.4244024016195508\n",
      "Epoch 231, Loss: 0.4242604946441205\n",
      "Epoch 232, Loss: 0.424134367950937\n",
      "Epoch 233, Loss: 0.42372608274856827\n",
      "Epoch 234, Loss: 0.4231492155631465\n",
      "Epoch 235, Loss: 0.4226291943984805\n",
      "Epoch 236, Loss: 0.4221723179942825\n",
      "Epoch 237, Loss: 0.4218955917984354\n",
      "Epoch 238, Loss: 0.4216767834434851\n",
      "Epoch 239, Loss: 0.42145169178243785\n",
      "Epoch 240, Loss: 0.4212605805865247\n",
      "Epoch 241, Loss: 0.42123380253494425\n",
      "Epoch 242, Loss: 0.42125483246926426\n",
      "Epoch 243, Loss: 0.42124070061060576\n",
      "Epoch 244, Loss: 0.42073760533336746\n",
      "Epoch 245, Loss: 0.42002867294257734\n",
      "Epoch 246, Loss: 0.41900436945460057\n",
      "Epoch 247, Loss: 0.41841940783018283\n",
      "Epoch 248, Loss: 0.4184544673382656\n",
      "Epoch 249, Loss: 0.4187823781263808\n",
      "Epoch 250, Loss: 0.41914970907160026\n",
      "Epoch 251, Loss: 0.418702737788617\n",
      "Epoch 252, Loss: 0.41762532180686956\n",
      "Epoch 253, Loss: 0.41665732452727816\n",
      "Epoch 254, Loss: 0.4164608109104539\n",
      "Epoch 255, Loss: 0.4169362050698102\n",
      "Epoch 256, Loss: 0.4170439641107097\n",
      "Epoch 257, Loss: 0.41618098523090385\n",
      "Epoch 258, Loss: 0.41521870035352226\n",
      "Epoch 259, Loss: 0.41507164322372797\n",
      "Epoch 260, Loss: 0.4154855816987037\n",
      "Epoch 261, Loss: 0.4152875627561342\n",
      "Epoch 262, Loss: 0.4146051629547779\n",
      "Epoch 263, Loss: 0.4138602536580612\n",
      "Epoch 264, Loss: 0.4135535008669373\n",
      "Epoch 265, Loss: 0.4136485063474015\n",
      "Epoch 266, Loss: 0.4135539332507534\n",
      "Epoch 267, Loss: 0.4133421191440525\n",
      "Epoch 268, Loss: 0.412685660282317\n",
      "Epoch 269, Loss: 0.4122655129807526\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.24858084678372203\n",
      "Valence RMSE: 0.2593484699604581\n",
      "Arousal RMSE: 0.23732519020237602\n",
      "Test R^2 score: tensor([0.2812, 0.5846], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.43293123556845575\n",
      "Num of epochs: 270\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Epoch 121, Loss: 0.4596922103619757\n",
      "Epoch 122, Loss: 0.45943389956138125\n",
      "Epoch 123, Loss: 0.45877818894117006\n",
      "Epoch 124, Loss: 0.45746538132874076\n",
      "Epoch 125, Loss: 0.4570462803121619\n",
      "Epoch 126, Loss: 0.45696189549769234\n",
      "Epoch 127, Loss: 0.45594920341421274\n",
      "Epoch 128, Loss: 0.45536359747685007\n",
      "Epoch 129, Loss: 0.4553290890752543\n",
      "Epoch 130, Loss: 0.4547195823074053\n",
      "Epoch 131, Loss: 0.45392838323998064\n",
      "Epoch 132, Loss: 0.4537853493595631\n",
      "Epoch 133, Loss: 0.45347437050767975\n",
      "Epoch 134, Loss: 0.45267431273812453\n",
      "Epoch 135, Loss: 0.45228737659373064\n",
      "Epoch 136, Loss: 0.4522154657432981\n",
      "Epoch 137, Loss: 0.45165964487407706\n",
      "Epoch 138, Loss: 0.4510751265462795\n",
      "Epoch 139, Loss: 0.45082650445054473\n",
      "Epoch 140, Loss: 0.4506281098405297\n",
      "Epoch 141, Loss: 0.4501911419656675\n",
      "Epoch 142, Loss: 0.4497001887659326\n",
      "Epoch 143, Loss: 0.44943436133718806\n",
      "Epoch 144, Loss: 0.449226296402944\n",
      "Epoch 145, Loss: 0.44886073897867573\n",
      "Epoch 146, Loss: 0.44844710376304864\n",
      "Epoch 147, Loss: 0.44810507013116724\n",
      "Epoch 148, Loss: 0.44788545862049767\n",
      "Epoch 149, Loss: 0.4476741441009651\n",
      "Epoch 150, Loss: 0.44735861731902055\n",
      "Epoch 151, Loss: 0.4469288720157105\n",
      "Epoch 152, Loss: 0.44655235754178146\n",
      "Epoch 153, Loss: 0.4462857402786373\n",
      "Epoch 154, Loss: 0.4460517369142754\n",
      "Epoch 155, Loss: 0.4457881796184935\n",
      "Epoch 156, Loss: 0.44542210886348255\n",
      "Epoch 157, Loss: 0.4450579467827662\n",
      "Epoch 158, Loss: 0.44473329320345206\n",
      "Epoch 159, Loss: 0.4444482514678638\n",
      "Epoch 160, Loss: 0.4441745674630603\n",
      "Epoch 161, Loss: 0.4439221813722748\n",
      "Epoch 162, Loss: 0.44363357870363757\n",
      "Epoch 163, Loss: 0.4433189071134186\n",
      "Epoch 164, Loss: 0.44295452976444744\n",
      "Epoch 165, Loss: 0.4426058108229731\n",
      "Epoch 166, Loss: 0.4422485787900922\n",
      "Epoch 167, Loss: 0.4419090985167976\n",
      "Epoch 168, Loss: 0.44158054387831963\n",
      "Epoch 169, Loss: 0.4412474388772268\n",
      "Epoch 170, Loss: 0.4409277694136744\n",
      "Epoch 171, Loss: 0.44068045571539705\n",
      "Epoch 172, Loss: 0.4405979418732049\n",
      "Epoch 173, Loss: 0.44094101686109266\n",
      "Epoch 174, Loss: 0.4417368233339534\n",
      "Epoch 175, Loss: 0.4416582519101522\n",
      "Epoch 176, Loss: 0.4396788970821407\n",
      "Epoch 177, Loss: 0.4390195954561405\n",
      "Epoch 178, Loss: 0.4400830988007806\n",
      "Epoch 179, Loss: 0.43931775869233325\n",
      "Epoch 180, Loss: 0.4380032505977321\n",
      "Epoch 181, Loss: 0.4384470974340077\n",
      "Epoch 182, Loss: 0.4385167636581309\n",
      "Epoch 183, Loss: 0.43733043450040754\n",
      "Epoch 184, Loss: 0.4373176399006318\n",
      "Epoch 185, Loss: 0.4375256973621567\n",
      "Epoch 186, Loss: 0.43659322296038683\n",
      "Epoch 187, Loss: 0.43644391045908093\n",
      "Epoch 188, Loss: 0.43663844358015674\n",
      "Epoch 189, Loss: 0.43586286705793914\n",
      "Epoch 190, Loss: 0.4355663072844155\n",
      "Epoch 191, Loss: 0.4357040532214924\n",
      "Epoch 192, Loss: 0.435134007760397\n",
      "Epoch 193, Loss: 0.43474241127906293\n",
      "Epoch 194, Loss: 0.43480482310319635\n",
      "Epoch 195, Loss: 0.43443826104982264\n",
      "Epoch 196, Loss: 0.43390767873021246\n",
      "Epoch 197, Loss: 0.4338897691233524\n",
      "Epoch 198, Loss: 0.43377856877811044\n",
      "Epoch 199, Loss: 0.4331595506415052\n",
      "Epoch 200, Loss: 0.43289415091975497\n",
      "Epoch 201, Loss: 0.43294048069736957\n",
      "Epoch 202, Loss: 0.43262240576772243\n",
      "Epoch 203, Loss: 0.43206541812705346\n",
      "Epoch 204, Loss: 0.43172500614102416\n",
      "Epoch 205, Loss: 0.4316686388133938\n",
      "Epoch 206, Loss: 0.4315179158466364\n",
      "Epoch 207, Loss: 0.4310712381632941\n",
      "Epoch 208, Loss: 0.43059470035496\n",
      "Epoch 209, Loss: 0.4303711221416647\n",
      "Epoch 210, Loss: 0.43033088719595897\n",
      "Epoch 211, Loss: 0.43016580489992484\n",
      "Epoch 212, Loss: 0.429691817522032\n",
      "Epoch 213, Loss: 0.4292099292794459\n",
      "Epoch 214, Loss: 0.4289207205910941\n",
      "Epoch 215, Loss: 0.4287978238495365\n",
      "Epoch 216, Loss: 0.42870603658529494\n",
      "Epoch 217, Loss: 0.42841980940051866\n",
      "Epoch 218, Loss: 0.4278918633922788\n",
      "Epoch 219, Loss: 0.42748307113702383\n",
      "Epoch 220, Loss: 0.4271688511370021\n",
      "Epoch 221, Loss: 0.4269432345934848\n",
      "Epoch 222, Loss: 0.4267651275776002\n",
      "Epoch 223, Loss: 0.42668376428587257\n",
      "Epoch 224, Loss: 0.42653554195413756\n",
      "Epoch 225, Loss: 0.42613293183996703\n",
      "Epoch 226, Loss: 0.4255451632780942\n",
      "Epoch 227, Loss: 0.4251305043179084\n",
      "Epoch 228, Loss: 0.42475934718412794\n",
      "Epoch 229, Loss: 0.42452283256309487\n",
      "Epoch 230, Loss: 0.4244024016195508\n",
      "Epoch 231, Loss: 0.4242604946441205\n",
      "Epoch 232, Loss: 0.424134367950937\n",
      "Epoch 233, Loss: 0.42372608274856827\n",
      "Epoch 234, Loss: 0.4231492155631465\n",
      "Epoch 235, Loss: 0.4226291943984805\n",
      "Epoch 236, Loss: 0.4221723179942825\n",
      "Epoch 237, Loss: 0.4218955917984354\n",
      "Epoch 238, Loss: 0.4216767834434851\n",
      "Epoch 239, Loss: 0.42145169178243785\n",
      "Epoch 240, Loss: 0.4212605805865247\n",
      "Epoch 241, Loss: 0.42123380253494425\n",
      "Epoch 242, Loss: 0.42125483246926426\n",
      "Epoch 243, Loss: 0.42124070061060576\n",
      "Epoch 244, Loss: 0.42073760533336746\n",
      "Epoch 245, Loss: 0.42002867294257734\n",
      "Epoch 246, Loss: 0.41900436945460057\n",
      "Epoch 247, Loss: 0.41841940783018283\n",
      "Epoch 248, Loss: 0.4184544673382656\n",
      "Epoch 249, Loss: 0.4187823781263808\n",
      "Epoch 250, Loss: 0.41914970907160026\n",
      "Epoch 251, Loss: 0.418702737788617\n",
      "Epoch 252, Loss: 0.41762532180686956\n",
      "Epoch 253, Loss: 0.41665732452727816\n",
      "Epoch 254, Loss: 0.4164608109104539\n",
      "Epoch 255, Loss: 0.4169362050698102\n",
      "Epoch 256, Loss: 0.4170439641107097\n",
      "Epoch 257, Loss: 0.41618098523090385\n",
      "Epoch 258, Loss: 0.41521870035352226\n",
      "Epoch 259, Loss: 0.41507164322372797\n",
      "Epoch 260, Loss: 0.4154855816987037\n",
      "Epoch 261, Loss: 0.4152875627561342\n",
      "Epoch 262, Loss: 0.4146051629547779\n",
      "Epoch 263, Loss: 0.4138602536580612\n",
      "Epoch 264, Loss: 0.4135535008669373\n",
      "Epoch 265, Loss: 0.4136485063474015\n",
      "Epoch 266, Loss: 0.4135539332507534\n",
      "Epoch 267, Loss: 0.4133421191440525\n",
      "Epoch 268, Loss: 0.412685660282317\n",
      "Epoch 269, Loss: 0.4122655129807526\n",
      "Epoch 270, Loss: 0.4118217259860175\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.24935387353033014\n",
      "Valence RMSE: 0.2602890908529776\n",
      "Arousal RMSE: 0.23791657712755143\n",
      "Test R^2 score: tensor([0.2760, 0.5825], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.429283307770994\n",
      "Num of epochs: 271\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Epoch 121, Loss: 0.4596922103619757\n",
      "Epoch 122, Loss: 0.45943389956138125\n",
      "Epoch 123, Loss: 0.45877818894117006\n",
      "Epoch 124, Loss: 0.45746538132874076\n",
      "Epoch 125, Loss: 0.4570462803121619\n",
      "Epoch 126, Loss: 0.45696189549769234\n",
      "Epoch 127, Loss: 0.45594920341421274\n",
      "Epoch 128, Loss: 0.45536359747685007\n",
      "Epoch 129, Loss: 0.4553290890752543\n",
      "Epoch 130, Loss: 0.4547195823074053\n",
      "Epoch 131, Loss: 0.45392838323998064\n",
      "Epoch 132, Loss: 0.4537853493595631\n",
      "Epoch 133, Loss: 0.45347437050767975\n",
      "Epoch 134, Loss: 0.45267431273812453\n",
      "Epoch 135, Loss: 0.45228737659373064\n",
      "Epoch 136, Loss: 0.4522154657432981\n",
      "Epoch 137, Loss: 0.45165964487407706\n",
      "Epoch 138, Loss: 0.4510751265462795\n",
      "Epoch 139, Loss: 0.45082650445054473\n",
      "Epoch 140, Loss: 0.4506281098405297\n",
      "Epoch 141, Loss: 0.4501911419656675\n",
      "Epoch 142, Loss: 0.4497001887659326\n",
      "Epoch 143, Loss: 0.44943436133718806\n",
      "Epoch 144, Loss: 0.449226296402944\n",
      "Epoch 145, Loss: 0.44886073897867573\n",
      "Epoch 146, Loss: 0.44844710376304864\n",
      "Epoch 147, Loss: 0.44810507013116724\n",
      "Epoch 148, Loss: 0.44788545862049767\n",
      "Epoch 149, Loss: 0.4476741441009651\n",
      "Epoch 150, Loss: 0.44735861731902055\n",
      "Epoch 151, Loss: 0.4469288720157105\n",
      "Epoch 152, Loss: 0.44655235754178146\n",
      "Epoch 153, Loss: 0.4462857402786373\n",
      "Epoch 154, Loss: 0.4460517369142754\n",
      "Epoch 155, Loss: 0.4457881796184935\n",
      "Epoch 156, Loss: 0.44542210886348255\n",
      "Epoch 157, Loss: 0.4450579467827662\n",
      "Epoch 158, Loss: 0.44473329320345206\n",
      "Epoch 159, Loss: 0.4444482514678638\n",
      "Epoch 160, Loss: 0.4441745674630603\n",
      "Epoch 161, Loss: 0.4439221813722748\n",
      "Epoch 162, Loss: 0.44363357870363757\n",
      "Epoch 163, Loss: 0.4433189071134186\n",
      "Epoch 164, Loss: 0.44295452976444744\n",
      "Epoch 165, Loss: 0.4426058108229731\n",
      "Epoch 166, Loss: 0.4422485787900922\n",
      "Epoch 167, Loss: 0.4419090985167976\n",
      "Epoch 168, Loss: 0.44158054387831963\n",
      "Epoch 169, Loss: 0.4412474388772268\n",
      "Epoch 170, Loss: 0.4409277694136744\n",
      "Epoch 171, Loss: 0.44068045571539705\n",
      "Epoch 172, Loss: 0.4405979418732049\n",
      "Epoch 173, Loss: 0.44094101686109266\n",
      "Epoch 174, Loss: 0.4417368233339534\n",
      "Epoch 175, Loss: 0.4416582519101522\n",
      "Epoch 176, Loss: 0.4396788970821407\n",
      "Epoch 177, Loss: 0.4390195954561405\n",
      "Epoch 178, Loss: 0.4400830988007806\n",
      "Epoch 179, Loss: 0.43931775869233325\n",
      "Epoch 180, Loss: 0.4380032505977321\n",
      "Epoch 181, Loss: 0.4384470974340077\n",
      "Epoch 182, Loss: 0.4385167636581309\n",
      "Epoch 183, Loss: 0.43733043450040754\n",
      "Epoch 184, Loss: 0.4373176399006318\n",
      "Epoch 185, Loss: 0.4375256973621567\n",
      "Epoch 186, Loss: 0.43659322296038683\n",
      "Epoch 187, Loss: 0.43644391045908093\n",
      "Epoch 188, Loss: 0.43663844358015674\n",
      "Epoch 189, Loss: 0.43586286705793914\n",
      "Epoch 190, Loss: 0.4355663072844155\n",
      "Epoch 191, Loss: 0.4357040532214924\n",
      "Epoch 192, Loss: 0.435134007760397\n",
      "Epoch 193, Loss: 0.43474241127906293\n",
      "Epoch 194, Loss: 0.43480482310319635\n",
      "Epoch 195, Loss: 0.43443826104982264\n",
      "Epoch 196, Loss: 0.43390767873021246\n",
      "Epoch 197, Loss: 0.4338897691233524\n",
      "Epoch 198, Loss: 0.43377856877811044\n",
      "Epoch 199, Loss: 0.4331595506415052\n",
      "Epoch 200, Loss: 0.43289415091975497\n",
      "Epoch 201, Loss: 0.43294048069736957\n",
      "Epoch 202, Loss: 0.43262240576772243\n",
      "Epoch 203, Loss: 0.43206541812705346\n",
      "Epoch 204, Loss: 0.43172500614102416\n",
      "Epoch 205, Loss: 0.4316686388133938\n",
      "Epoch 206, Loss: 0.4315179158466364\n",
      "Epoch 207, Loss: 0.4310712381632941\n",
      "Epoch 208, Loss: 0.43059470035496\n",
      "Epoch 209, Loss: 0.4303711221416647\n",
      "Epoch 210, Loss: 0.43033088719595897\n",
      "Epoch 211, Loss: 0.43016580489992484\n",
      "Epoch 212, Loss: 0.429691817522032\n",
      "Epoch 213, Loss: 0.4292099292794459\n",
      "Epoch 214, Loss: 0.4289207205910941\n",
      "Epoch 215, Loss: 0.4287978238495365\n",
      "Epoch 216, Loss: 0.42870603658529494\n",
      "Epoch 217, Loss: 0.42841980940051866\n",
      "Epoch 218, Loss: 0.4278918633922788\n",
      "Epoch 219, Loss: 0.42748307113702383\n",
      "Epoch 220, Loss: 0.4271688511370021\n",
      "Epoch 221, Loss: 0.4269432345934848\n",
      "Epoch 222, Loss: 0.4267651275776002\n",
      "Epoch 223, Loss: 0.42668376428587257\n",
      "Epoch 224, Loss: 0.42653554195413756\n",
      "Epoch 225, Loss: 0.42613293183996703\n",
      "Epoch 226, Loss: 0.4255451632780942\n",
      "Epoch 227, Loss: 0.4251305043179084\n",
      "Epoch 228, Loss: 0.42475934718412794\n",
      "Epoch 229, Loss: 0.42452283256309487\n",
      "Epoch 230, Loss: 0.4244024016195508\n",
      "Epoch 231, Loss: 0.4242604946441205\n",
      "Epoch 232, Loss: 0.424134367950937\n",
      "Epoch 233, Loss: 0.42372608274856827\n",
      "Epoch 234, Loss: 0.4231492155631465\n",
      "Epoch 235, Loss: 0.4226291943984805\n",
      "Epoch 236, Loss: 0.4221723179942825\n",
      "Epoch 237, Loss: 0.4218955917984354\n",
      "Epoch 238, Loss: 0.4216767834434851\n",
      "Epoch 239, Loss: 0.42145169178243785\n",
      "Epoch 240, Loss: 0.4212605805865247\n",
      "Epoch 241, Loss: 0.42123380253494425\n",
      "Epoch 242, Loss: 0.42125483246926426\n",
      "Epoch 243, Loss: 0.42124070061060576\n",
      "Epoch 244, Loss: 0.42073760533336746\n",
      "Epoch 245, Loss: 0.42002867294257734\n",
      "Epoch 246, Loss: 0.41900436945460057\n",
      "Epoch 247, Loss: 0.41841940783018283\n",
      "Epoch 248, Loss: 0.4184544673382656\n",
      "Epoch 249, Loss: 0.4187823781263808\n",
      "Epoch 250, Loss: 0.41914970907160026\n",
      "Epoch 251, Loss: 0.418702737788617\n",
      "Epoch 252, Loss: 0.41762532180686956\n",
      "Epoch 253, Loss: 0.41665732452727816\n",
      "Epoch 254, Loss: 0.4164608109104539\n",
      "Epoch 255, Loss: 0.4169362050698102\n",
      "Epoch 256, Loss: 0.4170439641107097\n",
      "Epoch 257, Loss: 0.41618098523090385\n",
      "Epoch 258, Loss: 0.41521870035352226\n",
      "Epoch 259, Loss: 0.41507164322372797\n",
      "Epoch 260, Loss: 0.4154855816987037\n",
      "Epoch 261, Loss: 0.4152875627561342\n",
      "Epoch 262, Loss: 0.4146051629547779\n",
      "Epoch 263, Loss: 0.4138602536580612\n",
      "Epoch 264, Loss: 0.4135535008669373\n",
      "Epoch 265, Loss: 0.4136485063474015\n",
      "Epoch 266, Loss: 0.4135539332507534\n",
      "Epoch 267, Loss: 0.4133421191440525\n",
      "Epoch 268, Loss: 0.412685660282317\n",
      "Epoch 269, Loss: 0.4122655129807526\n",
      "Epoch 270, Loss: 0.4118217259860175\n",
      "Epoch 271, Loss: 0.41165256945543577\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.24936084255885244\n",
      "Valence RMSE: 0.2603305070640085\n",
      "Arousal RMSE: 0.23788586905303513\n",
      "Test R^2 score: tensor([0.2758, 0.5827], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4292219794928593\n",
      "Num of epochs: 272\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Epoch 121, Loss: 0.4596922103619757\n",
      "Epoch 122, Loss: 0.45943389956138125\n",
      "Epoch 123, Loss: 0.45877818894117006\n",
      "Epoch 124, Loss: 0.45746538132874076\n",
      "Epoch 125, Loss: 0.4570462803121619\n",
      "Epoch 126, Loss: 0.45696189549769234\n",
      "Epoch 127, Loss: 0.45594920341421274\n",
      "Epoch 128, Loss: 0.45536359747685007\n",
      "Epoch 129, Loss: 0.4553290890752543\n",
      "Epoch 130, Loss: 0.4547195823074053\n",
      "Epoch 131, Loss: 0.45392838323998064\n",
      "Epoch 132, Loss: 0.4537853493595631\n",
      "Epoch 133, Loss: 0.45347437050767975\n",
      "Epoch 134, Loss: 0.45267431273812453\n",
      "Epoch 135, Loss: 0.45228737659373064\n",
      "Epoch 136, Loss: 0.4522154657432981\n",
      "Epoch 137, Loss: 0.45165964487407706\n",
      "Epoch 138, Loss: 0.4510751265462795\n",
      "Epoch 139, Loss: 0.45082650445054473\n",
      "Epoch 140, Loss: 0.4506281098405297\n",
      "Epoch 141, Loss: 0.4501911419656675\n",
      "Epoch 142, Loss: 0.4497001887659326\n",
      "Epoch 143, Loss: 0.44943436133718806\n",
      "Epoch 144, Loss: 0.449226296402944\n",
      "Epoch 145, Loss: 0.44886073897867573\n",
      "Epoch 146, Loss: 0.44844710376304864\n",
      "Epoch 147, Loss: 0.44810507013116724\n",
      "Epoch 148, Loss: 0.44788545862049767\n",
      "Epoch 149, Loss: 0.4476741441009651\n",
      "Epoch 150, Loss: 0.44735861731902055\n",
      "Epoch 151, Loss: 0.4469288720157105\n",
      "Epoch 152, Loss: 0.44655235754178146\n",
      "Epoch 153, Loss: 0.4462857402786373\n",
      "Epoch 154, Loss: 0.4460517369142754\n",
      "Epoch 155, Loss: 0.4457881796184935\n",
      "Epoch 156, Loss: 0.44542210886348255\n",
      "Epoch 157, Loss: 0.4450579467827662\n",
      "Epoch 158, Loss: 0.44473329320345206\n",
      "Epoch 159, Loss: 0.4444482514678638\n",
      "Epoch 160, Loss: 0.4441745674630603\n",
      "Epoch 161, Loss: 0.4439221813722748\n",
      "Epoch 162, Loss: 0.44363357870363757\n",
      "Epoch 163, Loss: 0.4433189071134186\n",
      "Epoch 164, Loss: 0.44295452976444744\n",
      "Epoch 165, Loss: 0.4426058108229731\n",
      "Epoch 166, Loss: 0.4422485787900922\n",
      "Epoch 167, Loss: 0.4419090985167976\n",
      "Epoch 168, Loss: 0.44158054387831963\n",
      "Epoch 169, Loss: 0.4412474388772268\n",
      "Epoch 170, Loss: 0.4409277694136744\n",
      "Epoch 171, Loss: 0.44068045571539705\n",
      "Epoch 172, Loss: 0.4405979418732049\n",
      "Epoch 173, Loss: 0.44094101686109266\n",
      "Epoch 174, Loss: 0.4417368233339534\n",
      "Epoch 175, Loss: 0.4416582519101522\n",
      "Epoch 176, Loss: 0.4396788970821407\n",
      "Epoch 177, Loss: 0.4390195954561405\n",
      "Epoch 178, Loss: 0.4400830988007806\n",
      "Epoch 179, Loss: 0.43931775869233325\n",
      "Epoch 180, Loss: 0.4380032505977321\n",
      "Epoch 181, Loss: 0.4384470974340077\n",
      "Epoch 182, Loss: 0.4385167636581309\n",
      "Epoch 183, Loss: 0.43733043450040754\n",
      "Epoch 184, Loss: 0.4373176399006318\n",
      "Epoch 185, Loss: 0.4375256973621567\n",
      "Epoch 186, Loss: 0.43659322296038683\n",
      "Epoch 187, Loss: 0.43644391045908093\n",
      "Epoch 188, Loss: 0.43663844358015674\n",
      "Epoch 189, Loss: 0.43586286705793914\n",
      "Epoch 190, Loss: 0.4355663072844155\n",
      "Epoch 191, Loss: 0.4357040532214924\n",
      "Epoch 192, Loss: 0.435134007760397\n",
      "Epoch 193, Loss: 0.43474241127906293\n",
      "Epoch 194, Loss: 0.43480482310319635\n",
      "Epoch 195, Loss: 0.43443826104982264\n",
      "Epoch 196, Loss: 0.43390767873021246\n",
      "Epoch 197, Loss: 0.4338897691233524\n",
      "Epoch 198, Loss: 0.43377856877811044\n",
      "Epoch 199, Loss: 0.4331595506415052\n",
      "Epoch 200, Loss: 0.43289415091975497\n",
      "Epoch 201, Loss: 0.43294048069736957\n",
      "Epoch 202, Loss: 0.43262240576772243\n",
      "Epoch 203, Loss: 0.43206541812705346\n",
      "Epoch 204, Loss: 0.43172500614102416\n",
      "Epoch 205, Loss: 0.4316686388133938\n",
      "Epoch 206, Loss: 0.4315179158466364\n",
      "Epoch 207, Loss: 0.4310712381632941\n",
      "Epoch 208, Loss: 0.43059470035496\n",
      "Epoch 209, Loss: 0.4303711221416647\n",
      "Epoch 210, Loss: 0.43033088719595897\n",
      "Epoch 211, Loss: 0.43016580489992484\n",
      "Epoch 212, Loss: 0.429691817522032\n",
      "Epoch 213, Loss: 0.4292099292794459\n",
      "Epoch 214, Loss: 0.4289207205910941\n",
      "Epoch 215, Loss: 0.4287978238495365\n",
      "Epoch 216, Loss: 0.42870603658529494\n",
      "Epoch 217, Loss: 0.42841980940051866\n",
      "Epoch 218, Loss: 0.4278918633922788\n",
      "Epoch 219, Loss: 0.42748307113702383\n",
      "Epoch 220, Loss: 0.4271688511370021\n",
      "Epoch 221, Loss: 0.4269432345934848\n",
      "Epoch 222, Loss: 0.4267651275776002\n",
      "Epoch 223, Loss: 0.42668376428587257\n",
      "Epoch 224, Loss: 0.42653554195413756\n",
      "Epoch 225, Loss: 0.42613293183996703\n",
      "Epoch 226, Loss: 0.4255451632780942\n",
      "Epoch 227, Loss: 0.4251305043179084\n",
      "Epoch 228, Loss: 0.42475934718412794\n",
      "Epoch 229, Loss: 0.42452283256309487\n",
      "Epoch 230, Loss: 0.4244024016195508\n",
      "Epoch 231, Loss: 0.4242604946441205\n",
      "Epoch 232, Loss: 0.424134367950937\n",
      "Epoch 233, Loss: 0.42372608274856827\n",
      "Epoch 234, Loss: 0.4231492155631465\n",
      "Epoch 235, Loss: 0.4226291943984805\n",
      "Epoch 236, Loss: 0.4221723179942825\n",
      "Epoch 237, Loss: 0.4218955917984354\n",
      "Epoch 238, Loss: 0.4216767834434851\n",
      "Epoch 239, Loss: 0.42145169178243785\n",
      "Epoch 240, Loss: 0.4212605805865247\n",
      "Epoch 241, Loss: 0.42123380253494425\n",
      "Epoch 242, Loss: 0.42125483246926426\n",
      "Epoch 243, Loss: 0.42124070061060576\n",
      "Epoch 244, Loss: 0.42073760533336746\n",
      "Epoch 245, Loss: 0.42002867294257734\n",
      "Epoch 246, Loss: 0.41900436945460057\n",
      "Epoch 247, Loss: 0.41841940783018283\n",
      "Epoch 248, Loss: 0.4184544673382656\n",
      "Epoch 249, Loss: 0.4187823781263808\n",
      "Epoch 250, Loss: 0.41914970907160026\n",
      "Epoch 251, Loss: 0.418702737788617\n",
      "Epoch 252, Loss: 0.41762532180686956\n",
      "Epoch 253, Loss: 0.41665732452727816\n",
      "Epoch 254, Loss: 0.4164608109104539\n",
      "Epoch 255, Loss: 0.4169362050698102\n",
      "Epoch 256, Loss: 0.4170439641107097\n",
      "Epoch 257, Loss: 0.41618098523090385\n",
      "Epoch 258, Loss: 0.41521870035352226\n",
      "Epoch 259, Loss: 0.41507164322372797\n",
      "Epoch 260, Loss: 0.4154855816987037\n",
      "Epoch 261, Loss: 0.4152875627561342\n",
      "Epoch 262, Loss: 0.4146051629547779\n",
      "Epoch 263, Loss: 0.4138602536580612\n",
      "Epoch 264, Loss: 0.4135535008669373\n",
      "Epoch 265, Loss: 0.4136485063474015\n",
      "Epoch 266, Loss: 0.4135539332507534\n",
      "Epoch 267, Loss: 0.4133421191440525\n",
      "Epoch 268, Loss: 0.412685660282317\n",
      "Epoch 269, Loss: 0.4122655129807526\n",
      "Epoch 270, Loss: 0.4118217259860175\n",
      "Epoch 271, Loss: 0.41165256945543577\n",
      "Epoch 272, Loss: 0.41146929239414193\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.24888618320284753\n",
      "Valence RMSE: 0.2589200431177292\n",
      "Arousal RMSE: 0.23843044195420185\n",
      "Test R^2 score: tensor([0.2836, 0.5807], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4321786066151128\n",
      "Num of epochs: 273\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Epoch 121, Loss: 0.4596922103619757\n",
      "Epoch 122, Loss: 0.45943389956138125\n",
      "Epoch 123, Loss: 0.45877818894117006\n",
      "Epoch 124, Loss: 0.45746538132874076\n",
      "Epoch 125, Loss: 0.4570462803121619\n",
      "Epoch 126, Loss: 0.45696189549769234\n",
      "Epoch 127, Loss: 0.45594920341421274\n",
      "Epoch 128, Loss: 0.45536359747685007\n",
      "Epoch 129, Loss: 0.4553290890752543\n",
      "Epoch 130, Loss: 0.4547195823074053\n",
      "Epoch 131, Loss: 0.45392838323998064\n",
      "Epoch 132, Loss: 0.4537853493595631\n",
      "Epoch 133, Loss: 0.45347437050767975\n",
      "Epoch 134, Loss: 0.45267431273812453\n",
      "Epoch 135, Loss: 0.45228737659373064\n",
      "Epoch 136, Loss: 0.4522154657432981\n",
      "Epoch 137, Loss: 0.45165964487407706\n",
      "Epoch 138, Loss: 0.4510751265462795\n",
      "Epoch 139, Loss: 0.45082650445054473\n",
      "Epoch 140, Loss: 0.4506281098405297\n",
      "Epoch 141, Loss: 0.4501911419656675\n",
      "Epoch 142, Loss: 0.4497001887659326\n",
      "Epoch 143, Loss: 0.44943436133718806\n",
      "Epoch 144, Loss: 0.449226296402944\n",
      "Epoch 145, Loss: 0.44886073897867573\n",
      "Epoch 146, Loss: 0.44844710376304864\n",
      "Epoch 147, Loss: 0.44810507013116724\n",
      "Epoch 148, Loss: 0.44788545862049767\n",
      "Epoch 149, Loss: 0.4476741441009651\n",
      "Epoch 150, Loss: 0.44735861731902055\n",
      "Epoch 151, Loss: 0.4469288720157105\n",
      "Epoch 152, Loss: 0.44655235754178146\n",
      "Epoch 153, Loss: 0.4462857402786373\n",
      "Epoch 154, Loss: 0.4460517369142754\n",
      "Epoch 155, Loss: 0.4457881796184935\n",
      "Epoch 156, Loss: 0.44542210886348255\n",
      "Epoch 157, Loss: 0.4450579467827662\n",
      "Epoch 158, Loss: 0.44473329320345206\n",
      "Epoch 159, Loss: 0.4444482514678638\n",
      "Epoch 160, Loss: 0.4441745674630603\n",
      "Epoch 161, Loss: 0.4439221813722748\n",
      "Epoch 162, Loss: 0.44363357870363757\n",
      "Epoch 163, Loss: 0.4433189071134186\n",
      "Epoch 164, Loss: 0.44295452976444744\n",
      "Epoch 165, Loss: 0.4426058108229731\n",
      "Epoch 166, Loss: 0.4422485787900922\n",
      "Epoch 167, Loss: 0.4419090985167976\n",
      "Epoch 168, Loss: 0.44158054387831963\n",
      "Epoch 169, Loss: 0.4412474388772268\n",
      "Epoch 170, Loss: 0.4409277694136744\n",
      "Epoch 171, Loss: 0.44068045571539705\n",
      "Epoch 172, Loss: 0.4405979418732049\n",
      "Epoch 173, Loss: 0.44094101686109266\n",
      "Epoch 174, Loss: 0.4417368233339534\n",
      "Epoch 175, Loss: 0.4416582519101522\n",
      "Epoch 176, Loss: 0.4396788970821407\n",
      "Epoch 177, Loss: 0.4390195954561405\n",
      "Epoch 178, Loss: 0.4400830988007806\n",
      "Epoch 179, Loss: 0.43931775869233325\n",
      "Epoch 180, Loss: 0.4380032505977321\n",
      "Epoch 181, Loss: 0.4384470974340077\n",
      "Epoch 182, Loss: 0.4385167636581309\n",
      "Epoch 183, Loss: 0.43733043450040754\n",
      "Epoch 184, Loss: 0.4373176399006318\n",
      "Epoch 185, Loss: 0.4375256973621567\n",
      "Epoch 186, Loss: 0.43659322296038683\n",
      "Epoch 187, Loss: 0.43644391045908093\n",
      "Epoch 188, Loss: 0.43663844358015674\n",
      "Epoch 189, Loss: 0.43586286705793914\n",
      "Epoch 190, Loss: 0.4355663072844155\n",
      "Epoch 191, Loss: 0.4357040532214924\n",
      "Epoch 192, Loss: 0.435134007760397\n",
      "Epoch 193, Loss: 0.43474241127906293\n",
      "Epoch 194, Loss: 0.43480482310319635\n",
      "Epoch 195, Loss: 0.43443826104982264\n",
      "Epoch 196, Loss: 0.43390767873021246\n",
      "Epoch 197, Loss: 0.4338897691233524\n",
      "Epoch 198, Loss: 0.43377856877811044\n",
      "Epoch 199, Loss: 0.4331595506415052\n",
      "Epoch 200, Loss: 0.43289415091975497\n",
      "Epoch 201, Loss: 0.43294048069736957\n",
      "Epoch 202, Loss: 0.43262240576772243\n",
      "Epoch 203, Loss: 0.43206541812705346\n",
      "Epoch 204, Loss: 0.43172500614102416\n",
      "Epoch 205, Loss: 0.4316686388133938\n",
      "Epoch 206, Loss: 0.4315179158466364\n",
      "Epoch 207, Loss: 0.4310712381632941\n",
      "Epoch 208, Loss: 0.43059470035496\n",
      "Epoch 209, Loss: 0.4303711221416647\n",
      "Epoch 210, Loss: 0.43033088719595897\n",
      "Epoch 211, Loss: 0.43016580489992484\n",
      "Epoch 212, Loss: 0.429691817522032\n",
      "Epoch 213, Loss: 0.4292099292794459\n",
      "Epoch 214, Loss: 0.4289207205910941\n",
      "Epoch 215, Loss: 0.4287978238495365\n",
      "Epoch 216, Loss: 0.42870603658529494\n",
      "Epoch 217, Loss: 0.42841980940051866\n",
      "Epoch 218, Loss: 0.4278918633922788\n",
      "Epoch 219, Loss: 0.42748307113702383\n",
      "Epoch 220, Loss: 0.4271688511370021\n",
      "Epoch 221, Loss: 0.4269432345934848\n",
      "Epoch 222, Loss: 0.4267651275776002\n",
      "Epoch 223, Loss: 0.42668376428587257\n",
      "Epoch 224, Loss: 0.42653554195413756\n",
      "Epoch 225, Loss: 0.42613293183996703\n",
      "Epoch 226, Loss: 0.4255451632780942\n",
      "Epoch 227, Loss: 0.4251305043179084\n",
      "Epoch 228, Loss: 0.42475934718412794\n",
      "Epoch 229, Loss: 0.42452283256309487\n",
      "Epoch 230, Loss: 0.4244024016195508\n",
      "Epoch 231, Loss: 0.4242604946441205\n",
      "Epoch 232, Loss: 0.424134367950937\n",
      "Epoch 233, Loss: 0.42372608274856827\n",
      "Epoch 234, Loss: 0.4231492155631465\n",
      "Epoch 235, Loss: 0.4226291943984805\n",
      "Epoch 236, Loss: 0.4221723179942825\n",
      "Epoch 237, Loss: 0.4218955917984354\n",
      "Epoch 238, Loss: 0.4216767834434851\n",
      "Epoch 239, Loss: 0.42145169178243785\n",
      "Epoch 240, Loss: 0.4212605805865247\n",
      "Epoch 241, Loss: 0.42123380253494425\n",
      "Epoch 242, Loss: 0.42125483246926426\n",
      "Epoch 243, Loss: 0.42124070061060576\n",
      "Epoch 244, Loss: 0.42073760533336746\n",
      "Epoch 245, Loss: 0.42002867294257734\n",
      "Epoch 246, Loss: 0.41900436945460057\n",
      "Epoch 247, Loss: 0.41841940783018283\n",
      "Epoch 248, Loss: 0.4184544673382656\n",
      "Epoch 249, Loss: 0.4187823781263808\n",
      "Epoch 250, Loss: 0.41914970907160026\n",
      "Epoch 251, Loss: 0.418702737788617\n",
      "Epoch 252, Loss: 0.41762532180686956\n",
      "Epoch 253, Loss: 0.41665732452727816\n",
      "Epoch 254, Loss: 0.4164608109104539\n",
      "Epoch 255, Loss: 0.4169362050698102\n",
      "Epoch 256, Loss: 0.4170439641107097\n",
      "Epoch 257, Loss: 0.41618098523090385\n",
      "Epoch 258, Loss: 0.41521870035352226\n",
      "Epoch 259, Loss: 0.41507164322372797\n",
      "Epoch 260, Loss: 0.4154855816987037\n",
      "Epoch 261, Loss: 0.4152875627561342\n",
      "Epoch 262, Loss: 0.4146051629547779\n",
      "Epoch 263, Loss: 0.4138602536580612\n",
      "Epoch 264, Loss: 0.4135535008669373\n",
      "Epoch 265, Loss: 0.4136485063474015\n",
      "Epoch 266, Loss: 0.4135539332507534\n",
      "Epoch 267, Loss: 0.4133421191440525\n",
      "Epoch 268, Loss: 0.412685660282317\n",
      "Epoch 269, Loss: 0.4122655129807526\n",
      "Epoch 270, Loss: 0.4118217259860175\n",
      "Epoch 271, Loss: 0.41165256945543577\n",
      "Epoch 272, Loss: 0.41146929239414193\n",
      "Epoch 273, Loss: 0.4115028618809221\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.24998803661303487\n",
      "Valence RMSE: 0.2608710163257817\n",
      "Arousal RMSE: 0.2386091987758097\n",
      "Test R^2 score: tensor([0.2728, 0.5801], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4264458462546617\n",
      "Num of epochs: 274\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Epoch 121, Loss: 0.4596922103619757\n",
      "Epoch 122, Loss: 0.45943389956138125\n",
      "Epoch 123, Loss: 0.45877818894117006\n",
      "Epoch 124, Loss: 0.45746538132874076\n",
      "Epoch 125, Loss: 0.4570462803121619\n",
      "Epoch 126, Loss: 0.45696189549769234\n",
      "Epoch 127, Loss: 0.45594920341421274\n",
      "Epoch 128, Loss: 0.45536359747685007\n",
      "Epoch 129, Loss: 0.4553290890752543\n",
      "Epoch 130, Loss: 0.4547195823074053\n",
      "Epoch 131, Loss: 0.45392838323998064\n",
      "Epoch 132, Loss: 0.4537853493595631\n",
      "Epoch 133, Loss: 0.45347437050767975\n",
      "Epoch 134, Loss: 0.45267431273812453\n",
      "Epoch 135, Loss: 0.45228737659373064\n",
      "Epoch 136, Loss: 0.4522154657432981\n",
      "Epoch 137, Loss: 0.45165964487407706\n",
      "Epoch 138, Loss: 0.4510751265462795\n",
      "Epoch 139, Loss: 0.45082650445054473\n",
      "Epoch 140, Loss: 0.4506281098405297\n",
      "Epoch 141, Loss: 0.4501911419656675\n",
      "Epoch 142, Loss: 0.4497001887659326\n",
      "Epoch 143, Loss: 0.44943436133718806\n",
      "Epoch 144, Loss: 0.449226296402944\n",
      "Epoch 145, Loss: 0.44886073897867573\n",
      "Epoch 146, Loss: 0.44844710376304864\n",
      "Epoch 147, Loss: 0.44810507013116724\n",
      "Epoch 148, Loss: 0.44788545862049767\n",
      "Epoch 149, Loss: 0.4476741441009651\n",
      "Epoch 150, Loss: 0.44735861731902055\n",
      "Epoch 151, Loss: 0.4469288720157105\n",
      "Epoch 152, Loss: 0.44655235754178146\n",
      "Epoch 153, Loss: 0.4462857402786373\n",
      "Epoch 154, Loss: 0.4460517369142754\n",
      "Epoch 155, Loss: 0.4457881796184935\n",
      "Epoch 156, Loss: 0.44542210886348255\n",
      "Epoch 157, Loss: 0.4450579467827662\n",
      "Epoch 158, Loss: 0.44473329320345206\n",
      "Epoch 159, Loss: 0.4444482514678638\n",
      "Epoch 160, Loss: 0.4441745674630603\n",
      "Epoch 161, Loss: 0.4439221813722748\n",
      "Epoch 162, Loss: 0.44363357870363757\n",
      "Epoch 163, Loss: 0.4433189071134186\n",
      "Epoch 164, Loss: 0.44295452976444744\n",
      "Epoch 165, Loss: 0.4426058108229731\n",
      "Epoch 166, Loss: 0.4422485787900922\n",
      "Epoch 167, Loss: 0.4419090985167976\n",
      "Epoch 168, Loss: 0.44158054387831963\n",
      "Epoch 169, Loss: 0.4412474388772268\n",
      "Epoch 170, Loss: 0.4409277694136744\n",
      "Epoch 171, Loss: 0.44068045571539705\n",
      "Epoch 172, Loss: 0.4405979418732049\n",
      "Epoch 173, Loss: 0.44094101686109266\n",
      "Epoch 174, Loss: 0.4417368233339534\n",
      "Epoch 175, Loss: 0.4416582519101522\n",
      "Epoch 176, Loss: 0.4396788970821407\n",
      "Epoch 177, Loss: 0.4390195954561405\n",
      "Epoch 178, Loss: 0.4400830988007806\n",
      "Epoch 179, Loss: 0.43931775869233325\n",
      "Epoch 180, Loss: 0.4380032505977321\n",
      "Epoch 181, Loss: 0.4384470974340077\n",
      "Epoch 182, Loss: 0.4385167636581309\n",
      "Epoch 183, Loss: 0.43733043450040754\n",
      "Epoch 184, Loss: 0.4373176399006318\n",
      "Epoch 185, Loss: 0.4375256973621567\n",
      "Epoch 186, Loss: 0.43659322296038683\n",
      "Epoch 187, Loss: 0.43644391045908093\n",
      "Epoch 188, Loss: 0.43663844358015674\n",
      "Epoch 189, Loss: 0.43586286705793914\n",
      "Epoch 190, Loss: 0.4355663072844155\n",
      "Epoch 191, Loss: 0.4357040532214924\n",
      "Epoch 192, Loss: 0.435134007760397\n",
      "Epoch 193, Loss: 0.43474241127906293\n",
      "Epoch 194, Loss: 0.43480482310319635\n",
      "Epoch 195, Loss: 0.43443826104982264\n",
      "Epoch 196, Loss: 0.43390767873021246\n",
      "Epoch 197, Loss: 0.4338897691233524\n",
      "Epoch 198, Loss: 0.43377856877811044\n",
      "Epoch 199, Loss: 0.4331595506415052\n",
      "Epoch 200, Loss: 0.43289415091975497\n",
      "Epoch 201, Loss: 0.43294048069736957\n",
      "Epoch 202, Loss: 0.43262240576772243\n",
      "Epoch 203, Loss: 0.43206541812705346\n",
      "Epoch 204, Loss: 0.43172500614102416\n",
      "Epoch 205, Loss: 0.4316686388133938\n",
      "Epoch 206, Loss: 0.4315179158466364\n",
      "Epoch 207, Loss: 0.4310712381632941\n",
      "Epoch 208, Loss: 0.43059470035496\n",
      "Epoch 209, Loss: 0.4303711221416647\n",
      "Epoch 210, Loss: 0.43033088719595897\n",
      "Epoch 211, Loss: 0.43016580489992484\n",
      "Epoch 212, Loss: 0.429691817522032\n",
      "Epoch 213, Loss: 0.4292099292794459\n",
      "Epoch 214, Loss: 0.4289207205910941\n",
      "Epoch 215, Loss: 0.4287978238495365\n",
      "Epoch 216, Loss: 0.42870603658529494\n",
      "Epoch 217, Loss: 0.42841980940051866\n",
      "Epoch 218, Loss: 0.4278918633922788\n",
      "Epoch 219, Loss: 0.42748307113702383\n",
      "Epoch 220, Loss: 0.4271688511370021\n",
      "Epoch 221, Loss: 0.4269432345934848\n",
      "Epoch 222, Loss: 0.4267651275776002\n",
      "Epoch 223, Loss: 0.42668376428587257\n",
      "Epoch 224, Loss: 0.42653554195413756\n",
      "Epoch 225, Loss: 0.42613293183996703\n",
      "Epoch 226, Loss: 0.4255451632780942\n",
      "Epoch 227, Loss: 0.4251305043179084\n",
      "Epoch 228, Loss: 0.42475934718412794\n",
      "Epoch 229, Loss: 0.42452283256309487\n",
      "Epoch 230, Loss: 0.4244024016195508\n",
      "Epoch 231, Loss: 0.4242604946441205\n",
      "Epoch 232, Loss: 0.424134367950937\n",
      "Epoch 233, Loss: 0.42372608274856827\n",
      "Epoch 234, Loss: 0.4231492155631465\n",
      "Epoch 235, Loss: 0.4226291943984805\n",
      "Epoch 236, Loss: 0.4221723179942825\n",
      "Epoch 237, Loss: 0.4218955917984354\n",
      "Epoch 238, Loss: 0.4216767834434851\n",
      "Epoch 239, Loss: 0.42145169178243785\n",
      "Epoch 240, Loss: 0.4212605805865247\n",
      "Epoch 241, Loss: 0.42123380253494425\n",
      "Epoch 242, Loss: 0.42125483246926426\n",
      "Epoch 243, Loss: 0.42124070061060576\n",
      "Epoch 244, Loss: 0.42073760533336746\n",
      "Epoch 245, Loss: 0.42002867294257734\n",
      "Epoch 246, Loss: 0.41900436945460057\n",
      "Epoch 247, Loss: 0.41841940783018283\n",
      "Epoch 248, Loss: 0.4184544673382656\n",
      "Epoch 249, Loss: 0.4187823781263808\n",
      "Epoch 250, Loss: 0.41914970907160026\n",
      "Epoch 251, Loss: 0.418702737788617\n",
      "Epoch 252, Loss: 0.41762532180686956\n",
      "Epoch 253, Loss: 0.41665732452727816\n",
      "Epoch 254, Loss: 0.4164608109104539\n",
      "Epoch 255, Loss: 0.4169362050698102\n",
      "Epoch 256, Loss: 0.4170439641107097\n",
      "Epoch 257, Loss: 0.41618098523090385\n",
      "Epoch 258, Loss: 0.41521870035352226\n",
      "Epoch 259, Loss: 0.41507164322372797\n",
      "Epoch 260, Loss: 0.4154855816987037\n",
      "Epoch 261, Loss: 0.4152875627561342\n",
      "Epoch 262, Loss: 0.4146051629547779\n",
      "Epoch 263, Loss: 0.4138602536580612\n",
      "Epoch 264, Loss: 0.4135535008669373\n",
      "Epoch 265, Loss: 0.4136485063474015\n",
      "Epoch 266, Loss: 0.4135539332507534\n",
      "Epoch 267, Loss: 0.4133421191440525\n",
      "Epoch 268, Loss: 0.412685660282317\n",
      "Epoch 269, Loss: 0.4122655129807526\n",
      "Epoch 270, Loss: 0.4118217259860175\n",
      "Epoch 271, Loss: 0.41165256945543577\n",
      "Epoch 272, Loss: 0.41146929239414193\n",
      "Epoch 273, Loss: 0.4115028618809221\n",
      "Epoch 274, Loss: 0.4117686232412549\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2502632366075632\n",
      "Valence RMSE: 0.26019485030338874\n",
      "Arousal RMSE: 0.2399208516785972\n",
      "Test R^2 score: tensor([0.2765, 0.5755], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4260138217438106\n",
      "Num of epochs: 275\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Epoch 121, Loss: 0.4596922103619757\n",
      "Epoch 122, Loss: 0.45943389956138125\n",
      "Epoch 123, Loss: 0.45877818894117006\n",
      "Epoch 124, Loss: 0.45746538132874076\n",
      "Epoch 125, Loss: 0.4570462803121619\n",
      "Epoch 126, Loss: 0.45696189549769234\n",
      "Epoch 127, Loss: 0.45594920341421274\n",
      "Epoch 128, Loss: 0.45536359747685007\n",
      "Epoch 129, Loss: 0.4553290890752543\n",
      "Epoch 130, Loss: 0.4547195823074053\n",
      "Epoch 131, Loss: 0.45392838323998064\n",
      "Epoch 132, Loss: 0.4537853493595631\n",
      "Epoch 133, Loss: 0.45347437050767975\n",
      "Epoch 134, Loss: 0.45267431273812453\n",
      "Epoch 135, Loss: 0.45228737659373064\n",
      "Epoch 136, Loss: 0.4522154657432981\n",
      "Epoch 137, Loss: 0.45165964487407706\n",
      "Epoch 138, Loss: 0.4510751265462795\n",
      "Epoch 139, Loss: 0.45082650445054473\n",
      "Epoch 140, Loss: 0.4506281098405297\n",
      "Epoch 141, Loss: 0.4501911419656675\n",
      "Epoch 142, Loss: 0.4497001887659326\n",
      "Epoch 143, Loss: 0.44943436133718806\n",
      "Epoch 144, Loss: 0.449226296402944\n",
      "Epoch 145, Loss: 0.44886073897867573\n",
      "Epoch 146, Loss: 0.44844710376304864\n",
      "Epoch 147, Loss: 0.44810507013116724\n",
      "Epoch 148, Loss: 0.44788545862049767\n",
      "Epoch 149, Loss: 0.4476741441009651\n",
      "Epoch 150, Loss: 0.44735861731902055\n",
      "Epoch 151, Loss: 0.4469288720157105\n",
      "Epoch 152, Loss: 0.44655235754178146\n",
      "Epoch 153, Loss: 0.4462857402786373\n",
      "Epoch 154, Loss: 0.4460517369142754\n",
      "Epoch 155, Loss: 0.4457881796184935\n",
      "Epoch 156, Loss: 0.44542210886348255\n",
      "Epoch 157, Loss: 0.4450579467827662\n",
      "Epoch 158, Loss: 0.44473329320345206\n",
      "Epoch 159, Loss: 0.4444482514678638\n",
      "Epoch 160, Loss: 0.4441745674630603\n",
      "Epoch 161, Loss: 0.4439221813722748\n",
      "Epoch 162, Loss: 0.44363357870363757\n",
      "Epoch 163, Loss: 0.4433189071134186\n",
      "Epoch 164, Loss: 0.44295452976444744\n",
      "Epoch 165, Loss: 0.4426058108229731\n",
      "Epoch 166, Loss: 0.4422485787900922\n",
      "Epoch 167, Loss: 0.4419090985167976\n",
      "Epoch 168, Loss: 0.44158054387831963\n",
      "Epoch 169, Loss: 0.4412474388772268\n",
      "Epoch 170, Loss: 0.4409277694136744\n",
      "Epoch 171, Loss: 0.44068045571539705\n",
      "Epoch 172, Loss: 0.4405979418732049\n",
      "Epoch 173, Loss: 0.44094101686109266\n",
      "Epoch 174, Loss: 0.4417368233339534\n",
      "Epoch 175, Loss: 0.4416582519101522\n",
      "Epoch 176, Loss: 0.4396788970821407\n",
      "Epoch 177, Loss: 0.4390195954561405\n",
      "Epoch 178, Loss: 0.4400830988007806\n",
      "Epoch 179, Loss: 0.43931775869233325\n",
      "Epoch 180, Loss: 0.4380032505977321\n",
      "Epoch 181, Loss: 0.4384470974340077\n",
      "Epoch 182, Loss: 0.4385167636581309\n",
      "Epoch 183, Loss: 0.43733043450040754\n",
      "Epoch 184, Loss: 0.4373176399006318\n",
      "Epoch 185, Loss: 0.4375256973621567\n",
      "Epoch 186, Loss: 0.43659322296038683\n",
      "Epoch 187, Loss: 0.43644391045908093\n",
      "Epoch 188, Loss: 0.43663844358015674\n",
      "Epoch 189, Loss: 0.43586286705793914\n",
      "Epoch 190, Loss: 0.4355663072844155\n",
      "Epoch 191, Loss: 0.4357040532214924\n",
      "Epoch 192, Loss: 0.435134007760397\n",
      "Epoch 193, Loss: 0.43474241127906293\n",
      "Epoch 194, Loss: 0.43480482310319635\n",
      "Epoch 195, Loss: 0.43443826104982264\n",
      "Epoch 196, Loss: 0.43390767873021246\n",
      "Epoch 197, Loss: 0.4338897691233524\n",
      "Epoch 198, Loss: 0.43377856877811044\n",
      "Epoch 199, Loss: 0.4331595506415052\n",
      "Epoch 200, Loss: 0.43289415091975497\n",
      "Epoch 201, Loss: 0.43294048069736957\n",
      "Epoch 202, Loss: 0.43262240576772243\n",
      "Epoch 203, Loss: 0.43206541812705346\n",
      "Epoch 204, Loss: 0.43172500614102416\n",
      "Epoch 205, Loss: 0.4316686388133938\n",
      "Epoch 206, Loss: 0.4315179158466364\n",
      "Epoch 207, Loss: 0.4310712381632941\n",
      "Epoch 208, Loss: 0.43059470035496\n",
      "Epoch 209, Loss: 0.4303711221416647\n",
      "Epoch 210, Loss: 0.43033088719595897\n",
      "Epoch 211, Loss: 0.43016580489992484\n",
      "Epoch 212, Loss: 0.429691817522032\n",
      "Epoch 213, Loss: 0.4292099292794459\n",
      "Epoch 214, Loss: 0.4289207205910941\n",
      "Epoch 215, Loss: 0.4287978238495365\n",
      "Epoch 216, Loss: 0.42870603658529494\n",
      "Epoch 217, Loss: 0.42841980940051866\n",
      "Epoch 218, Loss: 0.4278918633922788\n",
      "Epoch 219, Loss: 0.42748307113702383\n",
      "Epoch 220, Loss: 0.4271688511370021\n",
      "Epoch 221, Loss: 0.4269432345934848\n",
      "Epoch 222, Loss: 0.4267651275776002\n",
      "Epoch 223, Loss: 0.42668376428587257\n",
      "Epoch 224, Loss: 0.42653554195413756\n",
      "Epoch 225, Loss: 0.42613293183996703\n",
      "Epoch 226, Loss: 0.4255451632780942\n",
      "Epoch 227, Loss: 0.4251305043179084\n",
      "Epoch 228, Loss: 0.42475934718412794\n",
      "Epoch 229, Loss: 0.42452283256309487\n",
      "Epoch 230, Loss: 0.4244024016195508\n",
      "Epoch 231, Loss: 0.4242604946441205\n",
      "Epoch 232, Loss: 0.424134367950937\n",
      "Epoch 233, Loss: 0.42372608274856827\n",
      "Epoch 234, Loss: 0.4231492155631465\n",
      "Epoch 235, Loss: 0.4226291943984805\n",
      "Epoch 236, Loss: 0.4221723179942825\n",
      "Epoch 237, Loss: 0.4218955917984354\n",
      "Epoch 238, Loss: 0.4216767834434851\n",
      "Epoch 239, Loss: 0.42145169178243785\n",
      "Epoch 240, Loss: 0.4212605805865247\n",
      "Epoch 241, Loss: 0.42123380253494425\n",
      "Epoch 242, Loss: 0.42125483246926426\n",
      "Epoch 243, Loss: 0.42124070061060576\n",
      "Epoch 244, Loss: 0.42073760533336746\n",
      "Epoch 245, Loss: 0.42002867294257734\n",
      "Epoch 246, Loss: 0.41900436945460057\n",
      "Epoch 247, Loss: 0.41841940783018283\n",
      "Epoch 248, Loss: 0.4184544673382656\n",
      "Epoch 249, Loss: 0.4187823781263808\n",
      "Epoch 250, Loss: 0.41914970907160026\n",
      "Epoch 251, Loss: 0.418702737788617\n",
      "Epoch 252, Loss: 0.41762532180686956\n",
      "Epoch 253, Loss: 0.41665732452727816\n",
      "Epoch 254, Loss: 0.4164608109104539\n",
      "Epoch 255, Loss: 0.4169362050698102\n",
      "Epoch 256, Loss: 0.4170439641107097\n",
      "Epoch 257, Loss: 0.41618098523090385\n",
      "Epoch 258, Loss: 0.41521870035352226\n",
      "Epoch 259, Loss: 0.41507164322372797\n",
      "Epoch 260, Loss: 0.4154855816987037\n",
      "Epoch 261, Loss: 0.4152875627561342\n",
      "Epoch 262, Loss: 0.4146051629547779\n",
      "Epoch 263, Loss: 0.4138602536580612\n",
      "Epoch 264, Loss: 0.4135535008669373\n",
      "Epoch 265, Loss: 0.4136485063474015\n",
      "Epoch 266, Loss: 0.4135539332507534\n",
      "Epoch 267, Loss: 0.4133421191440525\n",
      "Epoch 268, Loss: 0.412685660282317\n",
      "Epoch 269, Loss: 0.4122655129807526\n",
      "Epoch 270, Loss: 0.4118217259860175\n",
      "Epoch 271, Loss: 0.41165256945543577\n",
      "Epoch 272, Loss: 0.41146929239414193\n",
      "Epoch 273, Loss: 0.4115028618809221\n",
      "Epoch 274, Loss: 0.4117686232412549\n",
      "Epoch 275, Loss: 0.4121111105932055\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2513009775459222\n",
      "Valence RMSE: 0.2620439895140691\n",
      "Arousal RMSE: 0.24007771698061145\n",
      "Test R^2 score: tensor([0.2662, 0.5749], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4205764918304413\n",
      "Num of epochs: 276\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Epoch 121, Loss: 0.4596922103619757\n",
      "Epoch 122, Loss: 0.45943389956138125\n",
      "Epoch 123, Loss: 0.45877818894117006\n",
      "Epoch 124, Loss: 0.45746538132874076\n",
      "Epoch 125, Loss: 0.4570462803121619\n",
      "Epoch 126, Loss: 0.45696189549769234\n",
      "Epoch 127, Loss: 0.45594920341421274\n",
      "Epoch 128, Loss: 0.45536359747685007\n",
      "Epoch 129, Loss: 0.4553290890752543\n",
      "Epoch 130, Loss: 0.4547195823074053\n",
      "Epoch 131, Loss: 0.45392838323998064\n",
      "Epoch 132, Loss: 0.4537853493595631\n",
      "Epoch 133, Loss: 0.45347437050767975\n",
      "Epoch 134, Loss: 0.45267431273812453\n",
      "Epoch 135, Loss: 0.45228737659373064\n",
      "Epoch 136, Loss: 0.4522154657432981\n",
      "Epoch 137, Loss: 0.45165964487407706\n",
      "Epoch 138, Loss: 0.4510751265462795\n",
      "Epoch 139, Loss: 0.45082650445054473\n",
      "Epoch 140, Loss: 0.4506281098405297\n",
      "Epoch 141, Loss: 0.4501911419656675\n",
      "Epoch 142, Loss: 0.4497001887659326\n",
      "Epoch 143, Loss: 0.44943436133718806\n",
      "Epoch 144, Loss: 0.449226296402944\n",
      "Epoch 145, Loss: 0.44886073897867573\n",
      "Epoch 146, Loss: 0.44844710376304864\n",
      "Epoch 147, Loss: 0.44810507013116724\n",
      "Epoch 148, Loss: 0.44788545862049767\n",
      "Epoch 149, Loss: 0.4476741441009651\n",
      "Epoch 150, Loss: 0.44735861731902055\n",
      "Epoch 151, Loss: 0.4469288720157105\n",
      "Epoch 152, Loss: 0.44655235754178146\n",
      "Epoch 153, Loss: 0.4462857402786373\n",
      "Epoch 154, Loss: 0.4460517369142754\n",
      "Epoch 155, Loss: 0.4457881796184935\n",
      "Epoch 156, Loss: 0.44542210886348255\n",
      "Epoch 157, Loss: 0.4450579467827662\n",
      "Epoch 158, Loss: 0.44473329320345206\n",
      "Epoch 159, Loss: 0.4444482514678638\n",
      "Epoch 160, Loss: 0.4441745674630603\n",
      "Epoch 161, Loss: 0.4439221813722748\n",
      "Epoch 162, Loss: 0.44363357870363757\n",
      "Epoch 163, Loss: 0.4433189071134186\n",
      "Epoch 164, Loss: 0.44295452976444744\n",
      "Epoch 165, Loss: 0.4426058108229731\n",
      "Epoch 166, Loss: 0.4422485787900922\n",
      "Epoch 167, Loss: 0.4419090985167976\n",
      "Epoch 168, Loss: 0.44158054387831963\n",
      "Epoch 169, Loss: 0.4412474388772268\n",
      "Epoch 170, Loss: 0.4409277694136744\n",
      "Epoch 171, Loss: 0.44068045571539705\n",
      "Epoch 172, Loss: 0.4405979418732049\n",
      "Epoch 173, Loss: 0.44094101686109266\n",
      "Epoch 174, Loss: 0.4417368233339534\n",
      "Epoch 175, Loss: 0.4416582519101522\n",
      "Epoch 176, Loss: 0.4396788970821407\n",
      "Epoch 177, Loss: 0.4390195954561405\n",
      "Epoch 178, Loss: 0.4400830988007806\n",
      "Epoch 179, Loss: 0.43931775869233325\n",
      "Epoch 180, Loss: 0.4380032505977321\n",
      "Epoch 181, Loss: 0.4384470974340077\n",
      "Epoch 182, Loss: 0.4385167636581309\n",
      "Epoch 183, Loss: 0.43733043450040754\n",
      "Epoch 184, Loss: 0.4373176399006318\n",
      "Epoch 185, Loss: 0.4375256973621567\n",
      "Epoch 186, Loss: 0.43659322296038683\n",
      "Epoch 187, Loss: 0.43644391045908093\n",
      "Epoch 188, Loss: 0.43663844358015674\n",
      "Epoch 189, Loss: 0.43586286705793914\n",
      "Epoch 190, Loss: 0.4355663072844155\n",
      "Epoch 191, Loss: 0.4357040532214924\n",
      "Epoch 192, Loss: 0.435134007760397\n",
      "Epoch 193, Loss: 0.43474241127906293\n",
      "Epoch 194, Loss: 0.43480482310319635\n",
      "Epoch 195, Loss: 0.43443826104982264\n",
      "Epoch 196, Loss: 0.43390767873021246\n",
      "Epoch 197, Loss: 0.4338897691233524\n",
      "Epoch 198, Loss: 0.43377856877811044\n",
      "Epoch 199, Loss: 0.4331595506415052\n",
      "Epoch 200, Loss: 0.43289415091975497\n",
      "Epoch 201, Loss: 0.43294048069736957\n",
      "Epoch 202, Loss: 0.43262240576772243\n",
      "Epoch 203, Loss: 0.43206541812705346\n",
      "Epoch 204, Loss: 0.43172500614102416\n",
      "Epoch 205, Loss: 0.4316686388133938\n",
      "Epoch 206, Loss: 0.4315179158466364\n",
      "Epoch 207, Loss: 0.4310712381632941\n",
      "Epoch 208, Loss: 0.43059470035496\n",
      "Epoch 209, Loss: 0.4303711221416647\n",
      "Epoch 210, Loss: 0.43033088719595897\n",
      "Epoch 211, Loss: 0.43016580489992484\n",
      "Epoch 212, Loss: 0.429691817522032\n",
      "Epoch 213, Loss: 0.4292099292794459\n",
      "Epoch 214, Loss: 0.4289207205910941\n",
      "Epoch 215, Loss: 0.4287978238495365\n",
      "Epoch 216, Loss: 0.42870603658529494\n",
      "Epoch 217, Loss: 0.42841980940051866\n",
      "Epoch 218, Loss: 0.4278918633922788\n",
      "Epoch 219, Loss: 0.42748307113702383\n",
      "Epoch 220, Loss: 0.4271688511370021\n",
      "Epoch 221, Loss: 0.4269432345934848\n",
      "Epoch 222, Loss: 0.4267651275776002\n",
      "Epoch 223, Loss: 0.42668376428587257\n",
      "Epoch 224, Loss: 0.42653554195413756\n",
      "Epoch 225, Loss: 0.42613293183996703\n",
      "Epoch 226, Loss: 0.4255451632780942\n",
      "Epoch 227, Loss: 0.4251305043179084\n",
      "Epoch 228, Loss: 0.42475934718412794\n",
      "Epoch 229, Loss: 0.42452283256309487\n",
      "Epoch 230, Loss: 0.4244024016195508\n",
      "Epoch 231, Loss: 0.4242604946441205\n",
      "Epoch 232, Loss: 0.424134367950937\n",
      "Epoch 233, Loss: 0.42372608274856827\n",
      "Epoch 234, Loss: 0.4231492155631465\n",
      "Epoch 235, Loss: 0.4226291943984805\n",
      "Epoch 236, Loss: 0.4221723179942825\n",
      "Epoch 237, Loss: 0.4218955917984354\n",
      "Epoch 238, Loss: 0.4216767834434851\n",
      "Epoch 239, Loss: 0.42145169178243785\n",
      "Epoch 240, Loss: 0.4212605805865247\n",
      "Epoch 241, Loss: 0.42123380253494425\n",
      "Epoch 242, Loss: 0.42125483246926426\n",
      "Epoch 243, Loss: 0.42124070061060576\n",
      "Epoch 244, Loss: 0.42073760533336746\n",
      "Epoch 245, Loss: 0.42002867294257734\n",
      "Epoch 246, Loss: 0.41900436945460057\n",
      "Epoch 247, Loss: 0.41841940783018283\n",
      "Epoch 248, Loss: 0.4184544673382656\n",
      "Epoch 249, Loss: 0.4187823781263808\n",
      "Epoch 250, Loss: 0.41914970907160026\n",
      "Epoch 251, Loss: 0.418702737788617\n",
      "Epoch 252, Loss: 0.41762532180686956\n",
      "Epoch 253, Loss: 0.41665732452727816\n",
      "Epoch 254, Loss: 0.4164608109104539\n",
      "Epoch 255, Loss: 0.4169362050698102\n",
      "Epoch 256, Loss: 0.4170439641107097\n",
      "Epoch 257, Loss: 0.41618098523090385\n",
      "Epoch 258, Loss: 0.41521870035352226\n",
      "Epoch 259, Loss: 0.41507164322372797\n",
      "Epoch 260, Loss: 0.4154855816987037\n",
      "Epoch 261, Loss: 0.4152875627561342\n",
      "Epoch 262, Loss: 0.4146051629547779\n",
      "Epoch 263, Loss: 0.4138602536580612\n",
      "Epoch 264, Loss: 0.4135535008669373\n",
      "Epoch 265, Loss: 0.4136485063474015\n",
      "Epoch 266, Loss: 0.4135539332507534\n",
      "Epoch 267, Loss: 0.4133421191440525\n",
      "Epoch 268, Loss: 0.412685660282317\n",
      "Epoch 269, Loss: 0.4122655129807526\n",
      "Epoch 270, Loss: 0.4118217259860175\n",
      "Epoch 271, Loss: 0.41165256945543577\n",
      "Epoch 272, Loss: 0.41146929239414193\n",
      "Epoch 273, Loss: 0.4115028618809221\n",
      "Epoch 274, Loss: 0.4117686232412549\n",
      "Epoch 275, Loss: 0.4121111105932055\n",
      "Epoch 276, Loss: 0.4130580342565522\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.24965671168512274\n",
      "Valence RMSE: 0.2587830473668514\n",
      "Arousal RMSE: 0.24018384994495967\n",
      "Test R^2 score: tensor([0.2844, 0.5746], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.42946299256957055\n",
      "Num of epochs: 277\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Epoch 121, Loss: 0.4596922103619757\n",
      "Epoch 122, Loss: 0.45943389956138125\n",
      "Epoch 123, Loss: 0.45877818894117006\n",
      "Epoch 124, Loss: 0.45746538132874076\n",
      "Epoch 125, Loss: 0.4570462803121619\n",
      "Epoch 126, Loss: 0.45696189549769234\n",
      "Epoch 127, Loss: 0.45594920341421274\n",
      "Epoch 128, Loss: 0.45536359747685007\n",
      "Epoch 129, Loss: 0.4553290890752543\n",
      "Epoch 130, Loss: 0.4547195823074053\n",
      "Epoch 131, Loss: 0.45392838323998064\n",
      "Epoch 132, Loss: 0.4537853493595631\n",
      "Epoch 133, Loss: 0.45347437050767975\n",
      "Epoch 134, Loss: 0.45267431273812453\n",
      "Epoch 135, Loss: 0.45228737659373064\n",
      "Epoch 136, Loss: 0.4522154657432981\n",
      "Epoch 137, Loss: 0.45165964487407706\n",
      "Epoch 138, Loss: 0.4510751265462795\n",
      "Epoch 139, Loss: 0.45082650445054473\n",
      "Epoch 140, Loss: 0.4506281098405297\n",
      "Epoch 141, Loss: 0.4501911419656675\n",
      "Epoch 142, Loss: 0.4497001887659326\n",
      "Epoch 143, Loss: 0.44943436133718806\n",
      "Epoch 144, Loss: 0.449226296402944\n",
      "Epoch 145, Loss: 0.44886073897867573\n",
      "Epoch 146, Loss: 0.44844710376304864\n",
      "Epoch 147, Loss: 0.44810507013116724\n",
      "Epoch 148, Loss: 0.44788545862049767\n",
      "Epoch 149, Loss: 0.4476741441009651\n",
      "Epoch 150, Loss: 0.44735861731902055\n",
      "Epoch 151, Loss: 0.4469288720157105\n",
      "Epoch 152, Loss: 0.44655235754178146\n",
      "Epoch 153, Loss: 0.4462857402786373\n",
      "Epoch 154, Loss: 0.4460517369142754\n",
      "Epoch 155, Loss: 0.4457881796184935\n",
      "Epoch 156, Loss: 0.44542210886348255\n",
      "Epoch 157, Loss: 0.4450579467827662\n",
      "Epoch 158, Loss: 0.44473329320345206\n",
      "Epoch 159, Loss: 0.4444482514678638\n",
      "Epoch 160, Loss: 0.4441745674630603\n",
      "Epoch 161, Loss: 0.4439221813722748\n",
      "Epoch 162, Loss: 0.44363357870363757\n",
      "Epoch 163, Loss: 0.4433189071134186\n",
      "Epoch 164, Loss: 0.44295452976444744\n",
      "Epoch 165, Loss: 0.4426058108229731\n",
      "Epoch 166, Loss: 0.4422485787900922\n",
      "Epoch 167, Loss: 0.4419090985167976\n",
      "Epoch 168, Loss: 0.44158054387831963\n",
      "Epoch 169, Loss: 0.4412474388772268\n",
      "Epoch 170, Loss: 0.4409277694136744\n",
      "Epoch 171, Loss: 0.44068045571539705\n",
      "Epoch 172, Loss: 0.4405979418732049\n",
      "Epoch 173, Loss: 0.44094101686109266\n",
      "Epoch 174, Loss: 0.4417368233339534\n",
      "Epoch 175, Loss: 0.4416582519101522\n",
      "Epoch 176, Loss: 0.4396788970821407\n",
      "Epoch 177, Loss: 0.4390195954561405\n",
      "Epoch 178, Loss: 0.4400830988007806\n",
      "Epoch 179, Loss: 0.43931775869233325\n",
      "Epoch 180, Loss: 0.4380032505977321\n",
      "Epoch 181, Loss: 0.4384470974340077\n",
      "Epoch 182, Loss: 0.4385167636581309\n",
      "Epoch 183, Loss: 0.43733043450040754\n",
      "Epoch 184, Loss: 0.4373176399006318\n",
      "Epoch 185, Loss: 0.4375256973621567\n",
      "Epoch 186, Loss: 0.43659322296038683\n",
      "Epoch 187, Loss: 0.43644391045908093\n",
      "Epoch 188, Loss: 0.43663844358015674\n",
      "Epoch 189, Loss: 0.43586286705793914\n",
      "Epoch 190, Loss: 0.4355663072844155\n",
      "Epoch 191, Loss: 0.4357040532214924\n",
      "Epoch 192, Loss: 0.435134007760397\n",
      "Epoch 193, Loss: 0.43474241127906293\n",
      "Epoch 194, Loss: 0.43480482310319635\n",
      "Epoch 195, Loss: 0.43443826104982264\n",
      "Epoch 196, Loss: 0.43390767873021246\n",
      "Epoch 197, Loss: 0.4338897691233524\n",
      "Epoch 198, Loss: 0.43377856877811044\n",
      "Epoch 199, Loss: 0.4331595506415052\n",
      "Epoch 200, Loss: 0.43289415091975497\n",
      "Epoch 201, Loss: 0.43294048069736957\n",
      "Epoch 202, Loss: 0.43262240576772243\n",
      "Epoch 203, Loss: 0.43206541812705346\n",
      "Epoch 204, Loss: 0.43172500614102416\n",
      "Epoch 205, Loss: 0.4316686388133938\n",
      "Epoch 206, Loss: 0.4315179158466364\n",
      "Epoch 207, Loss: 0.4310712381632941\n",
      "Epoch 208, Loss: 0.43059470035496\n",
      "Epoch 209, Loss: 0.4303711221416647\n",
      "Epoch 210, Loss: 0.43033088719595897\n",
      "Epoch 211, Loss: 0.43016580489992484\n",
      "Epoch 212, Loss: 0.429691817522032\n",
      "Epoch 213, Loss: 0.4292099292794459\n",
      "Epoch 214, Loss: 0.4289207205910941\n",
      "Epoch 215, Loss: 0.4287978238495365\n",
      "Epoch 216, Loss: 0.42870603658529494\n",
      "Epoch 217, Loss: 0.42841980940051866\n",
      "Epoch 218, Loss: 0.4278918633922788\n",
      "Epoch 219, Loss: 0.42748307113702383\n",
      "Epoch 220, Loss: 0.4271688511370021\n",
      "Epoch 221, Loss: 0.4269432345934848\n",
      "Epoch 222, Loss: 0.4267651275776002\n",
      "Epoch 223, Loss: 0.42668376428587257\n",
      "Epoch 224, Loss: 0.42653554195413756\n",
      "Epoch 225, Loss: 0.42613293183996703\n",
      "Epoch 226, Loss: 0.4255451632780942\n",
      "Epoch 227, Loss: 0.4251305043179084\n",
      "Epoch 228, Loss: 0.42475934718412794\n",
      "Epoch 229, Loss: 0.42452283256309487\n",
      "Epoch 230, Loss: 0.4244024016195508\n",
      "Epoch 231, Loss: 0.4242604946441205\n",
      "Epoch 232, Loss: 0.424134367950937\n",
      "Epoch 233, Loss: 0.42372608274856827\n",
      "Epoch 234, Loss: 0.4231492155631465\n",
      "Epoch 235, Loss: 0.4226291943984805\n",
      "Epoch 236, Loss: 0.4221723179942825\n",
      "Epoch 237, Loss: 0.4218955917984354\n",
      "Epoch 238, Loss: 0.4216767834434851\n",
      "Epoch 239, Loss: 0.42145169178243785\n",
      "Epoch 240, Loss: 0.4212605805865247\n",
      "Epoch 241, Loss: 0.42123380253494425\n",
      "Epoch 242, Loss: 0.42125483246926426\n",
      "Epoch 243, Loss: 0.42124070061060576\n",
      "Epoch 244, Loss: 0.42073760533336746\n",
      "Epoch 245, Loss: 0.42002867294257734\n",
      "Epoch 246, Loss: 0.41900436945460057\n",
      "Epoch 247, Loss: 0.41841940783018283\n",
      "Epoch 248, Loss: 0.4184544673382656\n",
      "Epoch 249, Loss: 0.4187823781263808\n",
      "Epoch 250, Loss: 0.41914970907160026\n",
      "Epoch 251, Loss: 0.418702737788617\n",
      "Epoch 252, Loss: 0.41762532180686956\n",
      "Epoch 253, Loss: 0.41665732452727816\n",
      "Epoch 254, Loss: 0.4164608109104539\n",
      "Epoch 255, Loss: 0.4169362050698102\n",
      "Epoch 256, Loss: 0.4170439641107097\n",
      "Epoch 257, Loss: 0.41618098523090385\n",
      "Epoch 258, Loss: 0.41521870035352226\n",
      "Epoch 259, Loss: 0.41507164322372797\n",
      "Epoch 260, Loss: 0.4154855816987037\n",
      "Epoch 261, Loss: 0.4152875627561342\n",
      "Epoch 262, Loss: 0.4146051629547779\n",
      "Epoch 263, Loss: 0.4138602536580612\n",
      "Epoch 264, Loss: 0.4135535008669373\n",
      "Epoch 265, Loss: 0.4136485063474015\n",
      "Epoch 266, Loss: 0.4135539332507534\n",
      "Epoch 267, Loss: 0.4133421191440525\n",
      "Epoch 268, Loss: 0.412685660282317\n",
      "Epoch 269, Loss: 0.4122655129807526\n",
      "Epoch 270, Loss: 0.4118217259860175\n",
      "Epoch 271, Loss: 0.41165256945543577\n",
      "Epoch 272, Loss: 0.41146929239414193\n",
      "Epoch 273, Loss: 0.4115028618809221\n",
      "Epoch 274, Loss: 0.4117686232412549\n",
      "Epoch 275, Loss: 0.4121111105932055\n",
      "Epoch 276, Loss: 0.4130580342565522\n",
      "Epoch 277, Loss: 0.4121949346593443\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.25037514374476016\n",
      "Valence RMSE: 0.2612025006979889\n",
      "Arousal RMSE: 0.2390578985089924\n",
      "Test R^2 score: tensor([0.2709, 0.5785], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4247308591319932\n",
      "Num of epochs: 278\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Epoch 121, Loss: 0.4596922103619757\n",
      "Epoch 122, Loss: 0.45943389956138125\n",
      "Epoch 123, Loss: 0.45877818894117006\n",
      "Epoch 124, Loss: 0.45746538132874076\n",
      "Epoch 125, Loss: 0.4570462803121619\n",
      "Epoch 126, Loss: 0.45696189549769234\n",
      "Epoch 127, Loss: 0.45594920341421274\n",
      "Epoch 128, Loss: 0.45536359747685007\n",
      "Epoch 129, Loss: 0.4553290890752543\n",
      "Epoch 130, Loss: 0.4547195823074053\n",
      "Epoch 131, Loss: 0.45392838323998064\n",
      "Epoch 132, Loss: 0.4537853493595631\n",
      "Epoch 133, Loss: 0.45347437050767975\n",
      "Epoch 134, Loss: 0.45267431273812453\n",
      "Epoch 135, Loss: 0.45228737659373064\n",
      "Epoch 136, Loss: 0.4522154657432981\n",
      "Epoch 137, Loss: 0.45165964487407706\n",
      "Epoch 138, Loss: 0.4510751265462795\n",
      "Epoch 139, Loss: 0.45082650445054473\n",
      "Epoch 140, Loss: 0.4506281098405297\n",
      "Epoch 141, Loss: 0.4501911419656675\n",
      "Epoch 142, Loss: 0.4497001887659326\n",
      "Epoch 143, Loss: 0.44943436133718806\n",
      "Epoch 144, Loss: 0.449226296402944\n",
      "Epoch 145, Loss: 0.44886073897867573\n",
      "Epoch 146, Loss: 0.44844710376304864\n",
      "Epoch 147, Loss: 0.44810507013116724\n",
      "Epoch 148, Loss: 0.44788545862049767\n",
      "Epoch 149, Loss: 0.4476741441009651\n",
      "Epoch 150, Loss: 0.44735861731902055\n",
      "Epoch 151, Loss: 0.4469288720157105\n",
      "Epoch 152, Loss: 0.44655235754178146\n",
      "Epoch 153, Loss: 0.4462857402786373\n",
      "Epoch 154, Loss: 0.4460517369142754\n",
      "Epoch 155, Loss: 0.4457881796184935\n",
      "Epoch 156, Loss: 0.44542210886348255\n",
      "Epoch 157, Loss: 0.4450579467827662\n",
      "Epoch 158, Loss: 0.44473329320345206\n",
      "Epoch 159, Loss: 0.4444482514678638\n",
      "Epoch 160, Loss: 0.4441745674630603\n",
      "Epoch 161, Loss: 0.4439221813722748\n",
      "Epoch 162, Loss: 0.44363357870363757\n",
      "Epoch 163, Loss: 0.4433189071134186\n",
      "Epoch 164, Loss: 0.44295452976444744\n",
      "Epoch 165, Loss: 0.4426058108229731\n",
      "Epoch 166, Loss: 0.4422485787900922\n",
      "Epoch 167, Loss: 0.4419090985167976\n",
      "Epoch 168, Loss: 0.44158054387831963\n",
      "Epoch 169, Loss: 0.4412474388772268\n",
      "Epoch 170, Loss: 0.4409277694136744\n",
      "Epoch 171, Loss: 0.44068045571539705\n",
      "Epoch 172, Loss: 0.4405979418732049\n",
      "Epoch 173, Loss: 0.44094101686109266\n",
      "Epoch 174, Loss: 0.4417368233339534\n",
      "Epoch 175, Loss: 0.4416582519101522\n",
      "Epoch 176, Loss: 0.4396788970821407\n",
      "Epoch 177, Loss: 0.4390195954561405\n",
      "Epoch 178, Loss: 0.4400830988007806\n",
      "Epoch 179, Loss: 0.43931775869233325\n",
      "Epoch 180, Loss: 0.4380032505977321\n",
      "Epoch 181, Loss: 0.4384470974340077\n",
      "Epoch 182, Loss: 0.4385167636581309\n",
      "Epoch 183, Loss: 0.43733043450040754\n",
      "Epoch 184, Loss: 0.4373176399006318\n",
      "Epoch 185, Loss: 0.4375256973621567\n",
      "Epoch 186, Loss: 0.43659322296038683\n",
      "Epoch 187, Loss: 0.43644391045908093\n",
      "Epoch 188, Loss: 0.43663844358015674\n",
      "Epoch 189, Loss: 0.43586286705793914\n",
      "Epoch 190, Loss: 0.4355663072844155\n",
      "Epoch 191, Loss: 0.4357040532214924\n",
      "Epoch 192, Loss: 0.435134007760397\n",
      "Epoch 193, Loss: 0.43474241127906293\n",
      "Epoch 194, Loss: 0.43480482310319635\n",
      "Epoch 195, Loss: 0.43443826104982264\n",
      "Epoch 196, Loss: 0.43390767873021246\n",
      "Epoch 197, Loss: 0.4338897691233524\n",
      "Epoch 198, Loss: 0.43377856877811044\n",
      "Epoch 199, Loss: 0.4331595506415052\n",
      "Epoch 200, Loss: 0.43289415091975497\n",
      "Epoch 201, Loss: 0.43294048069736957\n",
      "Epoch 202, Loss: 0.43262240576772243\n",
      "Epoch 203, Loss: 0.43206541812705346\n",
      "Epoch 204, Loss: 0.43172500614102416\n",
      "Epoch 205, Loss: 0.4316686388133938\n",
      "Epoch 206, Loss: 0.4315179158466364\n",
      "Epoch 207, Loss: 0.4310712381632941\n",
      "Epoch 208, Loss: 0.43059470035496\n",
      "Epoch 209, Loss: 0.4303711221416647\n",
      "Epoch 210, Loss: 0.43033088719595897\n",
      "Epoch 211, Loss: 0.43016580489992484\n",
      "Epoch 212, Loss: 0.429691817522032\n",
      "Epoch 213, Loss: 0.4292099292794459\n",
      "Epoch 214, Loss: 0.4289207205910941\n",
      "Epoch 215, Loss: 0.4287978238495365\n",
      "Epoch 216, Loss: 0.42870603658529494\n",
      "Epoch 217, Loss: 0.42841980940051866\n",
      "Epoch 218, Loss: 0.4278918633922788\n",
      "Epoch 219, Loss: 0.42748307113702383\n",
      "Epoch 220, Loss: 0.4271688511370021\n",
      "Epoch 221, Loss: 0.4269432345934848\n",
      "Epoch 222, Loss: 0.4267651275776002\n",
      "Epoch 223, Loss: 0.42668376428587257\n",
      "Epoch 224, Loss: 0.42653554195413756\n",
      "Epoch 225, Loss: 0.42613293183996703\n",
      "Epoch 226, Loss: 0.4255451632780942\n",
      "Epoch 227, Loss: 0.4251305043179084\n",
      "Epoch 228, Loss: 0.42475934718412794\n",
      "Epoch 229, Loss: 0.42452283256309487\n",
      "Epoch 230, Loss: 0.4244024016195508\n",
      "Epoch 231, Loss: 0.4242604946441205\n",
      "Epoch 232, Loss: 0.424134367950937\n",
      "Epoch 233, Loss: 0.42372608274856827\n",
      "Epoch 234, Loss: 0.4231492155631465\n",
      "Epoch 235, Loss: 0.4226291943984805\n",
      "Epoch 236, Loss: 0.4221723179942825\n",
      "Epoch 237, Loss: 0.4218955917984354\n",
      "Epoch 238, Loss: 0.4216767834434851\n",
      "Epoch 239, Loss: 0.42145169178243785\n",
      "Epoch 240, Loss: 0.4212605805865247\n",
      "Epoch 241, Loss: 0.42123380253494425\n",
      "Epoch 242, Loss: 0.42125483246926426\n",
      "Epoch 243, Loss: 0.42124070061060576\n",
      "Epoch 244, Loss: 0.42073760533336746\n",
      "Epoch 245, Loss: 0.42002867294257734\n",
      "Epoch 246, Loss: 0.41900436945460057\n",
      "Epoch 247, Loss: 0.41841940783018283\n",
      "Epoch 248, Loss: 0.4184544673382656\n",
      "Epoch 249, Loss: 0.4187823781263808\n",
      "Epoch 250, Loss: 0.41914970907160026\n",
      "Epoch 251, Loss: 0.418702737788617\n",
      "Epoch 252, Loss: 0.41762532180686956\n",
      "Epoch 253, Loss: 0.41665732452727816\n",
      "Epoch 254, Loss: 0.4164608109104539\n",
      "Epoch 255, Loss: 0.4169362050698102\n",
      "Epoch 256, Loss: 0.4170439641107097\n",
      "Epoch 257, Loss: 0.41618098523090385\n",
      "Epoch 258, Loss: 0.41521870035352226\n",
      "Epoch 259, Loss: 0.41507164322372797\n",
      "Epoch 260, Loss: 0.4154855816987037\n",
      "Epoch 261, Loss: 0.4152875627561342\n",
      "Epoch 262, Loss: 0.4146051629547779\n",
      "Epoch 263, Loss: 0.4138602536580612\n",
      "Epoch 264, Loss: 0.4135535008669373\n",
      "Epoch 265, Loss: 0.4136485063474015\n",
      "Epoch 266, Loss: 0.4135539332507534\n",
      "Epoch 267, Loss: 0.4133421191440525\n",
      "Epoch 268, Loss: 0.412685660282317\n",
      "Epoch 269, Loss: 0.4122655129807526\n",
      "Epoch 270, Loss: 0.4118217259860175\n",
      "Epoch 271, Loss: 0.41165256945543577\n",
      "Epoch 272, Loss: 0.41146929239414193\n",
      "Epoch 273, Loss: 0.4115028618809221\n",
      "Epoch 274, Loss: 0.4117686232412549\n",
      "Epoch 275, Loss: 0.4121111105932055\n",
      "Epoch 276, Loss: 0.4130580342565522\n",
      "Epoch 277, Loss: 0.4121949346593443\n",
      "Epoch 278, Loss: 0.41057389204015954\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2503747766451246\n",
      "Valence RMSE: 0.26100378275924774\n",
      "Arousal RMSE: 0.23927407495496758\n",
      "Test R^2 score: tensor([0.2720, 0.5778], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.424904009678837\n",
      "Num of epochs: 279\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Epoch 121, Loss: 0.4596922103619757\n",
      "Epoch 122, Loss: 0.45943389956138125\n",
      "Epoch 123, Loss: 0.45877818894117006\n",
      "Epoch 124, Loss: 0.45746538132874076\n",
      "Epoch 125, Loss: 0.4570462803121619\n",
      "Epoch 126, Loss: 0.45696189549769234\n",
      "Epoch 127, Loss: 0.45594920341421274\n",
      "Epoch 128, Loss: 0.45536359747685007\n",
      "Epoch 129, Loss: 0.4553290890752543\n",
      "Epoch 130, Loss: 0.4547195823074053\n",
      "Epoch 131, Loss: 0.45392838323998064\n",
      "Epoch 132, Loss: 0.4537853493595631\n",
      "Epoch 133, Loss: 0.45347437050767975\n",
      "Epoch 134, Loss: 0.45267431273812453\n",
      "Epoch 135, Loss: 0.45228737659373064\n",
      "Epoch 136, Loss: 0.4522154657432981\n",
      "Epoch 137, Loss: 0.45165964487407706\n",
      "Epoch 138, Loss: 0.4510751265462795\n",
      "Epoch 139, Loss: 0.45082650445054473\n",
      "Epoch 140, Loss: 0.4506281098405297\n",
      "Epoch 141, Loss: 0.4501911419656675\n",
      "Epoch 142, Loss: 0.4497001887659326\n",
      "Epoch 143, Loss: 0.44943436133718806\n",
      "Epoch 144, Loss: 0.449226296402944\n",
      "Epoch 145, Loss: 0.44886073897867573\n",
      "Epoch 146, Loss: 0.44844710376304864\n",
      "Epoch 147, Loss: 0.44810507013116724\n",
      "Epoch 148, Loss: 0.44788545862049767\n",
      "Epoch 149, Loss: 0.4476741441009651\n",
      "Epoch 150, Loss: 0.44735861731902055\n",
      "Epoch 151, Loss: 0.4469288720157105\n",
      "Epoch 152, Loss: 0.44655235754178146\n",
      "Epoch 153, Loss: 0.4462857402786373\n",
      "Epoch 154, Loss: 0.4460517369142754\n",
      "Epoch 155, Loss: 0.4457881796184935\n",
      "Epoch 156, Loss: 0.44542210886348255\n",
      "Epoch 157, Loss: 0.4450579467827662\n",
      "Epoch 158, Loss: 0.44473329320345206\n",
      "Epoch 159, Loss: 0.4444482514678638\n",
      "Epoch 160, Loss: 0.4441745674630603\n",
      "Epoch 161, Loss: 0.4439221813722748\n",
      "Epoch 162, Loss: 0.44363357870363757\n",
      "Epoch 163, Loss: 0.4433189071134186\n",
      "Epoch 164, Loss: 0.44295452976444744\n",
      "Epoch 165, Loss: 0.4426058108229731\n",
      "Epoch 166, Loss: 0.4422485787900922\n",
      "Epoch 167, Loss: 0.4419090985167976\n",
      "Epoch 168, Loss: 0.44158054387831963\n",
      "Epoch 169, Loss: 0.4412474388772268\n",
      "Epoch 170, Loss: 0.4409277694136744\n",
      "Epoch 171, Loss: 0.44068045571539705\n",
      "Epoch 172, Loss: 0.4405979418732049\n",
      "Epoch 173, Loss: 0.44094101686109266\n",
      "Epoch 174, Loss: 0.4417368233339534\n",
      "Epoch 175, Loss: 0.4416582519101522\n",
      "Epoch 176, Loss: 0.4396788970821407\n",
      "Epoch 177, Loss: 0.4390195954561405\n",
      "Epoch 178, Loss: 0.4400830988007806\n",
      "Epoch 179, Loss: 0.43931775869233325\n",
      "Epoch 180, Loss: 0.4380032505977321\n",
      "Epoch 181, Loss: 0.4384470974340077\n",
      "Epoch 182, Loss: 0.4385167636581309\n",
      "Epoch 183, Loss: 0.43733043450040754\n",
      "Epoch 184, Loss: 0.4373176399006318\n",
      "Epoch 185, Loss: 0.4375256973621567\n",
      "Epoch 186, Loss: 0.43659322296038683\n",
      "Epoch 187, Loss: 0.43644391045908093\n",
      "Epoch 188, Loss: 0.43663844358015674\n",
      "Epoch 189, Loss: 0.43586286705793914\n",
      "Epoch 190, Loss: 0.4355663072844155\n",
      "Epoch 191, Loss: 0.4357040532214924\n",
      "Epoch 192, Loss: 0.435134007760397\n",
      "Epoch 193, Loss: 0.43474241127906293\n",
      "Epoch 194, Loss: 0.43480482310319635\n",
      "Epoch 195, Loss: 0.43443826104982264\n",
      "Epoch 196, Loss: 0.43390767873021246\n",
      "Epoch 197, Loss: 0.4338897691233524\n",
      "Epoch 198, Loss: 0.43377856877811044\n",
      "Epoch 199, Loss: 0.4331595506415052\n",
      "Epoch 200, Loss: 0.43289415091975497\n",
      "Epoch 201, Loss: 0.43294048069736957\n",
      "Epoch 202, Loss: 0.43262240576772243\n",
      "Epoch 203, Loss: 0.43206541812705346\n",
      "Epoch 204, Loss: 0.43172500614102416\n",
      "Epoch 205, Loss: 0.4316686388133938\n",
      "Epoch 206, Loss: 0.4315179158466364\n",
      "Epoch 207, Loss: 0.4310712381632941\n",
      "Epoch 208, Loss: 0.43059470035496\n",
      "Epoch 209, Loss: 0.4303711221416647\n",
      "Epoch 210, Loss: 0.43033088719595897\n",
      "Epoch 211, Loss: 0.43016580489992484\n",
      "Epoch 212, Loss: 0.429691817522032\n",
      "Epoch 213, Loss: 0.4292099292794459\n",
      "Epoch 214, Loss: 0.4289207205910941\n",
      "Epoch 215, Loss: 0.4287978238495365\n",
      "Epoch 216, Loss: 0.42870603658529494\n",
      "Epoch 217, Loss: 0.42841980940051866\n",
      "Epoch 218, Loss: 0.4278918633922788\n",
      "Epoch 219, Loss: 0.42748307113702383\n",
      "Epoch 220, Loss: 0.4271688511370021\n",
      "Epoch 221, Loss: 0.4269432345934848\n",
      "Epoch 222, Loss: 0.4267651275776002\n",
      "Epoch 223, Loss: 0.42668376428587257\n",
      "Epoch 224, Loss: 0.42653554195413756\n",
      "Epoch 225, Loss: 0.42613293183996703\n",
      "Epoch 226, Loss: 0.4255451632780942\n",
      "Epoch 227, Loss: 0.4251305043179084\n",
      "Epoch 228, Loss: 0.42475934718412794\n",
      "Epoch 229, Loss: 0.42452283256309487\n",
      "Epoch 230, Loss: 0.4244024016195508\n",
      "Epoch 231, Loss: 0.4242604946441205\n",
      "Epoch 232, Loss: 0.424134367950937\n",
      "Epoch 233, Loss: 0.42372608274856827\n",
      "Epoch 234, Loss: 0.4231492155631465\n",
      "Epoch 235, Loss: 0.4226291943984805\n",
      "Epoch 236, Loss: 0.4221723179942825\n",
      "Epoch 237, Loss: 0.4218955917984354\n",
      "Epoch 238, Loss: 0.4216767834434851\n",
      "Epoch 239, Loss: 0.42145169178243785\n",
      "Epoch 240, Loss: 0.4212605805865247\n",
      "Epoch 241, Loss: 0.42123380253494425\n",
      "Epoch 242, Loss: 0.42125483246926426\n",
      "Epoch 243, Loss: 0.42124070061060576\n",
      "Epoch 244, Loss: 0.42073760533336746\n",
      "Epoch 245, Loss: 0.42002867294257734\n",
      "Epoch 246, Loss: 0.41900436945460057\n",
      "Epoch 247, Loss: 0.41841940783018283\n",
      "Epoch 248, Loss: 0.4184544673382656\n",
      "Epoch 249, Loss: 0.4187823781263808\n",
      "Epoch 250, Loss: 0.41914970907160026\n",
      "Epoch 251, Loss: 0.418702737788617\n",
      "Epoch 252, Loss: 0.41762532180686956\n",
      "Epoch 253, Loss: 0.41665732452727816\n",
      "Epoch 254, Loss: 0.4164608109104539\n",
      "Epoch 255, Loss: 0.4169362050698102\n",
      "Epoch 256, Loss: 0.4170439641107097\n",
      "Epoch 257, Loss: 0.41618098523090385\n",
      "Epoch 258, Loss: 0.41521870035352226\n",
      "Epoch 259, Loss: 0.41507164322372797\n",
      "Epoch 260, Loss: 0.4154855816987037\n",
      "Epoch 261, Loss: 0.4152875627561342\n",
      "Epoch 262, Loss: 0.4146051629547779\n",
      "Epoch 263, Loss: 0.4138602536580612\n",
      "Epoch 264, Loss: 0.4135535008669373\n",
      "Epoch 265, Loss: 0.4136485063474015\n",
      "Epoch 266, Loss: 0.4135539332507534\n",
      "Epoch 267, Loss: 0.4133421191440525\n",
      "Epoch 268, Loss: 0.412685660282317\n",
      "Epoch 269, Loss: 0.4122655129807526\n",
      "Epoch 270, Loss: 0.4118217259860175\n",
      "Epoch 271, Loss: 0.41165256945543577\n",
      "Epoch 272, Loss: 0.41146929239414193\n",
      "Epoch 273, Loss: 0.4115028618809221\n",
      "Epoch 274, Loss: 0.4117686232412549\n",
      "Epoch 275, Loss: 0.4121111105932055\n",
      "Epoch 276, Loss: 0.4130580342565522\n",
      "Epoch 277, Loss: 0.4121949346593443\n",
      "Epoch 278, Loss: 0.41057389204015954\n",
      "Epoch 279, Loss: 0.4095484089667836\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2503884767418151\n",
      "Valence RMSE: 0.26038121278797943\n",
      "Arousal RMSE: 0.23998000457795213\n",
      "Test R^2 score: tensor([0.2755, 0.5753], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.42539078898319155\n",
      "Num of epochs: 280\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Epoch 121, Loss: 0.4596922103619757\n",
      "Epoch 122, Loss: 0.45943389956138125\n",
      "Epoch 123, Loss: 0.45877818894117006\n",
      "Epoch 124, Loss: 0.45746538132874076\n",
      "Epoch 125, Loss: 0.4570462803121619\n",
      "Epoch 126, Loss: 0.45696189549769234\n",
      "Epoch 127, Loss: 0.45594920341421274\n",
      "Epoch 128, Loss: 0.45536359747685007\n",
      "Epoch 129, Loss: 0.4553290890752543\n",
      "Epoch 130, Loss: 0.4547195823074053\n",
      "Epoch 131, Loss: 0.45392838323998064\n",
      "Epoch 132, Loss: 0.4537853493595631\n",
      "Epoch 133, Loss: 0.45347437050767975\n",
      "Epoch 134, Loss: 0.45267431273812453\n",
      "Epoch 135, Loss: 0.45228737659373064\n",
      "Epoch 136, Loss: 0.4522154657432981\n",
      "Epoch 137, Loss: 0.45165964487407706\n",
      "Epoch 138, Loss: 0.4510751265462795\n",
      "Epoch 139, Loss: 0.45082650445054473\n",
      "Epoch 140, Loss: 0.4506281098405297\n",
      "Epoch 141, Loss: 0.4501911419656675\n",
      "Epoch 142, Loss: 0.4497001887659326\n",
      "Epoch 143, Loss: 0.44943436133718806\n",
      "Epoch 144, Loss: 0.449226296402944\n",
      "Epoch 145, Loss: 0.44886073897867573\n",
      "Epoch 146, Loss: 0.44844710376304864\n",
      "Epoch 147, Loss: 0.44810507013116724\n",
      "Epoch 148, Loss: 0.44788545862049767\n",
      "Epoch 149, Loss: 0.4476741441009651\n",
      "Epoch 150, Loss: 0.44735861731902055\n",
      "Epoch 151, Loss: 0.4469288720157105\n",
      "Epoch 152, Loss: 0.44655235754178146\n",
      "Epoch 153, Loss: 0.4462857402786373\n",
      "Epoch 154, Loss: 0.4460517369142754\n",
      "Epoch 155, Loss: 0.4457881796184935\n",
      "Epoch 156, Loss: 0.44542210886348255\n",
      "Epoch 157, Loss: 0.4450579467827662\n",
      "Epoch 158, Loss: 0.44473329320345206\n",
      "Epoch 159, Loss: 0.4444482514678638\n",
      "Epoch 160, Loss: 0.4441745674630603\n",
      "Epoch 161, Loss: 0.4439221813722748\n",
      "Epoch 162, Loss: 0.44363357870363757\n",
      "Epoch 163, Loss: 0.4433189071134186\n",
      "Epoch 164, Loss: 0.44295452976444744\n",
      "Epoch 165, Loss: 0.4426058108229731\n",
      "Epoch 166, Loss: 0.4422485787900922\n",
      "Epoch 167, Loss: 0.4419090985167976\n",
      "Epoch 168, Loss: 0.44158054387831963\n",
      "Epoch 169, Loss: 0.4412474388772268\n",
      "Epoch 170, Loss: 0.4409277694136744\n",
      "Epoch 171, Loss: 0.44068045571539705\n",
      "Epoch 172, Loss: 0.4405979418732049\n",
      "Epoch 173, Loss: 0.44094101686109266\n",
      "Epoch 174, Loss: 0.4417368233339534\n",
      "Epoch 175, Loss: 0.4416582519101522\n",
      "Epoch 176, Loss: 0.4396788970821407\n",
      "Epoch 177, Loss: 0.4390195954561405\n",
      "Epoch 178, Loss: 0.4400830988007806\n",
      "Epoch 179, Loss: 0.43931775869233325\n",
      "Epoch 180, Loss: 0.4380032505977321\n",
      "Epoch 181, Loss: 0.4384470974340077\n",
      "Epoch 182, Loss: 0.4385167636581309\n",
      "Epoch 183, Loss: 0.43733043450040754\n",
      "Epoch 184, Loss: 0.4373176399006318\n",
      "Epoch 185, Loss: 0.4375256973621567\n",
      "Epoch 186, Loss: 0.43659322296038683\n",
      "Epoch 187, Loss: 0.43644391045908093\n",
      "Epoch 188, Loss: 0.43663844358015674\n",
      "Epoch 189, Loss: 0.43586286705793914\n",
      "Epoch 190, Loss: 0.4355663072844155\n",
      "Epoch 191, Loss: 0.4357040532214924\n",
      "Epoch 192, Loss: 0.435134007760397\n",
      "Epoch 193, Loss: 0.43474241127906293\n",
      "Epoch 194, Loss: 0.43480482310319635\n",
      "Epoch 195, Loss: 0.43443826104982264\n",
      "Epoch 196, Loss: 0.43390767873021246\n",
      "Epoch 197, Loss: 0.4338897691233524\n",
      "Epoch 198, Loss: 0.43377856877811044\n",
      "Epoch 199, Loss: 0.4331595506415052\n",
      "Epoch 200, Loss: 0.43289415091975497\n",
      "Epoch 201, Loss: 0.43294048069736957\n",
      "Epoch 202, Loss: 0.43262240576772243\n",
      "Epoch 203, Loss: 0.43206541812705346\n",
      "Epoch 204, Loss: 0.43172500614102416\n",
      "Epoch 205, Loss: 0.4316686388133938\n",
      "Epoch 206, Loss: 0.4315179158466364\n",
      "Epoch 207, Loss: 0.4310712381632941\n",
      "Epoch 208, Loss: 0.43059470035496\n",
      "Epoch 209, Loss: 0.4303711221416647\n",
      "Epoch 210, Loss: 0.43033088719595897\n",
      "Epoch 211, Loss: 0.43016580489992484\n",
      "Epoch 212, Loss: 0.429691817522032\n",
      "Epoch 213, Loss: 0.4292099292794459\n",
      "Epoch 214, Loss: 0.4289207205910941\n",
      "Epoch 215, Loss: 0.4287978238495365\n",
      "Epoch 216, Loss: 0.42870603658529494\n",
      "Epoch 217, Loss: 0.42841980940051866\n",
      "Epoch 218, Loss: 0.4278918633922788\n",
      "Epoch 219, Loss: 0.42748307113702383\n",
      "Epoch 220, Loss: 0.4271688511370021\n",
      "Epoch 221, Loss: 0.4269432345934848\n",
      "Epoch 222, Loss: 0.4267651275776002\n",
      "Epoch 223, Loss: 0.42668376428587257\n",
      "Epoch 224, Loss: 0.42653554195413756\n",
      "Epoch 225, Loss: 0.42613293183996703\n",
      "Epoch 226, Loss: 0.4255451632780942\n",
      "Epoch 227, Loss: 0.4251305043179084\n",
      "Epoch 228, Loss: 0.42475934718412794\n",
      "Epoch 229, Loss: 0.42452283256309487\n",
      "Epoch 230, Loss: 0.4244024016195508\n",
      "Epoch 231, Loss: 0.4242604946441205\n",
      "Epoch 232, Loss: 0.424134367950937\n",
      "Epoch 233, Loss: 0.42372608274856827\n",
      "Epoch 234, Loss: 0.4231492155631465\n",
      "Epoch 235, Loss: 0.4226291943984805\n",
      "Epoch 236, Loss: 0.4221723179942825\n",
      "Epoch 237, Loss: 0.4218955917984354\n",
      "Epoch 238, Loss: 0.4216767834434851\n",
      "Epoch 239, Loss: 0.42145169178243785\n",
      "Epoch 240, Loss: 0.4212605805865247\n",
      "Epoch 241, Loss: 0.42123380253494425\n",
      "Epoch 242, Loss: 0.42125483246926426\n",
      "Epoch 243, Loss: 0.42124070061060576\n",
      "Epoch 244, Loss: 0.42073760533336746\n",
      "Epoch 245, Loss: 0.42002867294257734\n",
      "Epoch 246, Loss: 0.41900436945460057\n",
      "Epoch 247, Loss: 0.41841940783018283\n",
      "Epoch 248, Loss: 0.4184544673382656\n",
      "Epoch 249, Loss: 0.4187823781263808\n",
      "Epoch 250, Loss: 0.41914970907160026\n",
      "Epoch 251, Loss: 0.418702737788617\n",
      "Epoch 252, Loss: 0.41762532180686956\n",
      "Epoch 253, Loss: 0.41665732452727816\n",
      "Epoch 254, Loss: 0.4164608109104539\n",
      "Epoch 255, Loss: 0.4169362050698102\n",
      "Epoch 256, Loss: 0.4170439641107097\n",
      "Epoch 257, Loss: 0.41618098523090385\n",
      "Epoch 258, Loss: 0.41521870035352226\n",
      "Epoch 259, Loss: 0.41507164322372797\n",
      "Epoch 260, Loss: 0.4154855816987037\n",
      "Epoch 261, Loss: 0.4152875627561342\n",
      "Epoch 262, Loss: 0.4146051629547779\n",
      "Epoch 263, Loss: 0.4138602536580612\n",
      "Epoch 264, Loss: 0.4135535008669373\n",
      "Epoch 265, Loss: 0.4136485063474015\n",
      "Epoch 266, Loss: 0.4135539332507534\n",
      "Epoch 267, Loss: 0.4133421191440525\n",
      "Epoch 268, Loss: 0.412685660282317\n",
      "Epoch 269, Loss: 0.4122655129807526\n",
      "Epoch 270, Loss: 0.4118217259860175\n",
      "Epoch 271, Loss: 0.41165256945543577\n",
      "Epoch 272, Loss: 0.41146929239414193\n",
      "Epoch 273, Loss: 0.4115028618809221\n",
      "Epoch 274, Loss: 0.4117686232412549\n",
      "Epoch 275, Loss: 0.4121111105932055\n",
      "Epoch 276, Loss: 0.4130580342565522\n",
      "Epoch 277, Loss: 0.4121949346593443\n",
      "Epoch 278, Loss: 0.41057389204015954\n",
      "Epoch 279, Loss: 0.4095484089667836\n",
      "Epoch 280, Loss: 0.4098107108594438\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.25152302117879366\n",
      "Valence RMSE: 0.26200472651816165\n",
      "Arousal RMSE: 0.2405850860879765\n",
      "Test R^2 score: tensor([0.2664, 0.5731], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.41978714927578076\n",
      "Num of epochs: 281\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Epoch 121, Loss: 0.4596922103619757\n",
      "Epoch 122, Loss: 0.45943389956138125\n",
      "Epoch 123, Loss: 0.45877818894117006\n",
      "Epoch 124, Loss: 0.45746538132874076\n",
      "Epoch 125, Loss: 0.4570462803121619\n",
      "Epoch 126, Loss: 0.45696189549769234\n",
      "Epoch 127, Loss: 0.45594920341421274\n",
      "Epoch 128, Loss: 0.45536359747685007\n",
      "Epoch 129, Loss: 0.4553290890752543\n",
      "Epoch 130, Loss: 0.4547195823074053\n",
      "Epoch 131, Loss: 0.45392838323998064\n",
      "Epoch 132, Loss: 0.4537853493595631\n",
      "Epoch 133, Loss: 0.45347437050767975\n",
      "Epoch 134, Loss: 0.45267431273812453\n",
      "Epoch 135, Loss: 0.45228737659373064\n",
      "Epoch 136, Loss: 0.4522154657432981\n",
      "Epoch 137, Loss: 0.45165964487407706\n",
      "Epoch 138, Loss: 0.4510751265462795\n",
      "Epoch 139, Loss: 0.45082650445054473\n",
      "Epoch 140, Loss: 0.4506281098405297\n",
      "Epoch 141, Loss: 0.4501911419656675\n",
      "Epoch 142, Loss: 0.4497001887659326\n",
      "Epoch 143, Loss: 0.44943436133718806\n",
      "Epoch 144, Loss: 0.449226296402944\n",
      "Epoch 145, Loss: 0.44886073897867573\n",
      "Epoch 146, Loss: 0.44844710376304864\n",
      "Epoch 147, Loss: 0.44810507013116724\n",
      "Epoch 148, Loss: 0.44788545862049767\n",
      "Epoch 149, Loss: 0.4476741441009651\n",
      "Epoch 150, Loss: 0.44735861731902055\n",
      "Epoch 151, Loss: 0.4469288720157105\n",
      "Epoch 152, Loss: 0.44655235754178146\n",
      "Epoch 153, Loss: 0.4462857402786373\n",
      "Epoch 154, Loss: 0.4460517369142754\n",
      "Epoch 155, Loss: 0.4457881796184935\n",
      "Epoch 156, Loss: 0.44542210886348255\n",
      "Epoch 157, Loss: 0.4450579467827662\n",
      "Epoch 158, Loss: 0.44473329320345206\n",
      "Epoch 159, Loss: 0.4444482514678638\n",
      "Epoch 160, Loss: 0.4441745674630603\n",
      "Epoch 161, Loss: 0.4439221813722748\n",
      "Epoch 162, Loss: 0.44363357870363757\n",
      "Epoch 163, Loss: 0.4433189071134186\n",
      "Epoch 164, Loss: 0.44295452976444744\n",
      "Epoch 165, Loss: 0.4426058108229731\n",
      "Epoch 166, Loss: 0.4422485787900922\n",
      "Epoch 167, Loss: 0.4419090985167976\n",
      "Epoch 168, Loss: 0.44158054387831963\n",
      "Epoch 169, Loss: 0.4412474388772268\n",
      "Epoch 170, Loss: 0.4409277694136744\n",
      "Epoch 171, Loss: 0.44068045571539705\n",
      "Epoch 172, Loss: 0.4405979418732049\n",
      "Epoch 173, Loss: 0.44094101686109266\n",
      "Epoch 174, Loss: 0.4417368233339534\n",
      "Epoch 175, Loss: 0.4416582519101522\n",
      "Epoch 176, Loss: 0.4396788970821407\n",
      "Epoch 177, Loss: 0.4390195954561405\n",
      "Epoch 178, Loss: 0.4400830988007806\n",
      "Epoch 179, Loss: 0.43931775869233325\n",
      "Epoch 180, Loss: 0.4380032505977321\n",
      "Epoch 181, Loss: 0.4384470974340077\n",
      "Epoch 182, Loss: 0.4385167636581309\n",
      "Epoch 183, Loss: 0.43733043450040754\n",
      "Epoch 184, Loss: 0.4373176399006318\n",
      "Epoch 185, Loss: 0.4375256973621567\n",
      "Epoch 186, Loss: 0.43659322296038683\n",
      "Epoch 187, Loss: 0.43644391045908093\n",
      "Epoch 188, Loss: 0.43663844358015674\n",
      "Epoch 189, Loss: 0.43586286705793914\n",
      "Epoch 190, Loss: 0.4355663072844155\n",
      "Epoch 191, Loss: 0.4357040532214924\n",
      "Epoch 192, Loss: 0.435134007760397\n",
      "Epoch 193, Loss: 0.43474241127906293\n",
      "Epoch 194, Loss: 0.43480482310319635\n",
      "Epoch 195, Loss: 0.43443826104982264\n",
      "Epoch 196, Loss: 0.43390767873021246\n",
      "Epoch 197, Loss: 0.4338897691233524\n",
      "Epoch 198, Loss: 0.43377856877811044\n",
      "Epoch 199, Loss: 0.4331595506415052\n",
      "Epoch 200, Loss: 0.43289415091975497\n",
      "Epoch 201, Loss: 0.43294048069736957\n",
      "Epoch 202, Loss: 0.43262240576772243\n",
      "Epoch 203, Loss: 0.43206541812705346\n",
      "Epoch 204, Loss: 0.43172500614102416\n",
      "Epoch 205, Loss: 0.4316686388133938\n",
      "Epoch 206, Loss: 0.4315179158466364\n",
      "Epoch 207, Loss: 0.4310712381632941\n",
      "Epoch 208, Loss: 0.43059470035496\n",
      "Epoch 209, Loss: 0.4303711221416647\n",
      "Epoch 210, Loss: 0.43033088719595897\n",
      "Epoch 211, Loss: 0.43016580489992484\n",
      "Epoch 212, Loss: 0.429691817522032\n",
      "Epoch 213, Loss: 0.4292099292794459\n",
      "Epoch 214, Loss: 0.4289207205910941\n",
      "Epoch 215, Loss: 0.4287978238495365\n",
      "Epoch 216, Loss: 0.42870603658529494\n",
      "Epoch 217, Loss: 0.42841980940051866\n",
      "Epoch 218, Loss: 0.4278918633922788\n",
      "Epoch 219, Loss: 0.42748307113702383\n",
      "Epoch 220, Loss: 0.4271688511370021\n",
      "Epoch 221, Loss: 0.4269432345934848\n",
      "Epoch 222, Loss: 0.4267651275776002\n",
      "Epoch 223, Loss: 0.42668376428587257\n",
      "Epoch 224, Loss: 0.42653554195413756\n",
      "Epoch 225, Loss: 0.42613293183996703\n",
      "Epoch 226, Loss: 0.4255451632780942\n",
      "Epoch 227, Loss: 0.4251305043179084\n",
      "Epoch 228, Loss: 0.42475934718412794\n",
      "Epoch 229, Loss: 0.42452283256309487\n",
      "Epoch 230, Loss: 0.4244024016195508\n",
      "Epoch 231, Loss: 0.4242604946441205\n",
      "Epoch 232, Loss: 0.424134367950937\n",
      "Epoch 233, Loss: 0.42372608274856827\n",
      "Epoch 234, Loss: 0.4231492155631465\n",
      "Epoch 235, Loss: 0.4226291943984805\n",
      "Epoch 236, Loss: 0.4221723179942825\n",
      "Epoch 237, Loss: 0.4218955917984354\n",
      "Epoch 238, Loss: 0.4216767834434851\n",
      "Epoch 239, Loss: 0.42145169178243785\n",
      "Epoch 240, Loss: 0.4212605805865247\n",
      "Epoch 241, Loss: 0.42123380253494425\n",
      "Epoch 242, Loss: 0.42125483246926426\n",
      "Epoch 243, Loss: 0.42124070061060576\n",
      "Epoch 244, Loss: 0.42073760533336746\n",
      "Epoch 245, Loss: 0.42002867294257734\n",
      "Epoch 246, Loss: 0.41900436945460057\n",
      "Epoch 247, Loss: 0.41841940783018283\n",
      "Epoch 248, Loss: 0.4184544673382656\n",
      "Epoch 249, Loss: 0.4187823781263808\n",
      "Epoch 250, Loss: 0.41914970907160026\n",
      "Epoch 251, Loss: 0.418702737788617\n",
      "Epoch 252, Loss: 0.41762532180686956\n",
      "Epoch 253, Loss: 0.41665732452727816\n",
      "Epoch 254, Loss: 0.4164608109104539\n",
      "Epoch 255, Loss: 0.4169362050698102\n",
      "Epoch 256, Loss: 0.4170439641107097\n",
      "Epoch 257, Loss: 0.41618098523090385\n",
      "Epoch 258, Loss: 0.41521870035352226\n",
      "Epoch 259, Loss: 0.41507164322372797\n",
      "Epoch 260, Loss: 0.4154855816987037\n",
      "Epoch 261, Loss: 0.4152875627561342\n",
      "Epoch 262, Loss: 0.4146051629547779\n",
      "Epoch 263, Loss: 0.4138602536580612\n",
      "Epoch 264, Loss: 0.4135535008669373\n",
      "Epoch 265, Loss: 0.4136485063474015\n",
      "Epoch 266, Loss: 0.4135539332507534\n",
      "Epoch 267, Loss: 0.4133421191440525\n",
      "Epoch 268, Loss: 0.412685660282317\n",
      "Epoch 269, Loss: 0.4122655129807526\n",
      "Epoch 270, Loss: 0.4118217259860175\n",
      "Epoch 271, Loss: 0.41165256945543577\n",
      "Epoch 272, Loss: 0.41146929239414193\n",
      "Epoch 273, Loss: 0.4115028618809221\n",
      "Epoch 274, Loss: 0.4117686232412549\n",
      "Epoch 275, Loss: 0.4121111105932055\n",
      "Epoch 276, Loss: 0.4130580342565522\n",
      "Epoch 277, Loss: 0.4121949346593443\n",
      "Epoch 278, Loss: 0.41057389204015954\n",
      "Epoch 279, Loss: 0.4095484089667836\n",
      "Epoch 280, Loss: 0.4098107108594438\n",
      "Epoch 281, Loss: 0.41080122599126295\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2508048704223094\n",
      "Valence RMSE: 0.25996744509785163\n",
      "Arousal RMSE: 0.24129461979994157\n",
      "Test R^2 score: tensor([0.2778, 0.5706], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4242081175578614\n",
      "Num of epochs: 282\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Epoch 121, Loss: 0.4596922103619757\n",
      "Epoch 122, Loss: 0.45943389956138125\n",
      "Epoch 123, Loss: 0.45877818894117006\n",
      "Epoch 124, Loss: 0.45746538132874076\n",
      "Epoch 125, Loss: 0.4570462803121619\n",
      "Epoch 126, Loss: 0.45696189549769234\n",
      "Epoch 127, Loss: 0.45594920341421274\n",
      "Epoch 128, Loss: 0.45536359747685007\n",
      "Epoch 129, Loss: 0.4553290890752543\n",
      "Epoch 130, Loss: 0.4547195823074053\n",
      "Epoch 131, Loss: 0.45392838323998064\n",
      "Epoch 132, Loss: 0.4537853493595631\n",
      "Epoch 133, Loss: 0.45347437050767975\n",
      "Epoch 134, Loss: 0.45267431273812453\n",
      "Epoch 135, Loss: 0.45228737659373064\n",
      "Epoch 136, Loss: 0.4522154657432981\n",
      "Epoch 137, Loss: 0.45165964487407706\n",
      "Epoch 138, Loss: 0.4510751265462795\n",
      "Epoch 139, Loss: 0.45082650445054473\n",
      "Epoch 140, Loss: 0.4506281098405297\n",
      "Epoch 141, Loss: 0.4501911419656675\n",
      "Epoch 142, Loss: 0.4497001887659326\n",
      "Epoch 143, Loss: 0.44943436133718806\n",
      "Epoch 144, Loss: 0.449226296402944\n",
      "Epoch 145, Loss: 0.44886073897867573\n",
      "Epoch 146, Loss: 0.44844710376304864\n",
      "Epoch 147, Loss: 0.44810507013116724\n",
      "Epoch 148, Loss: 0.44788545862049767\n",
      "Epoch 149, Loss: 0.4476741441009651\n",
      "Epoch 150, Loss: 0.44735861731902055\n",
      "Epoch 151, Loss: 0.4469288720157105\n",
      "Epoch 152, Loss: 0.44655235754178146\n",
      "Epoch 153, Loss: 0.4462857402786373\n",
      "Epoch 154, Loss: 0.4460517369142754\n",
      "Epoch 155, Loss: 0.4457881796184935\n",
      "Epoch 156, Loss: 0.44542210886348255\n",
      "Epoch 157, Loss: 0.4450579467827662\n",
      "Epoch 158, Loss: 0.44473329320345206\n",
      "Epoch 159, Loss: 0.4444482514678638\n",
      "Epoch 160, Loss: 0.4441745674630603\n",
      "Epoch 161, Loss: 0.4439221813722748\n",
      "Epoch 162, Loss: 0.44363357870363757\n",
      "Epoch 163, Loss: 0.4433189071134186\n",
      "Epoch 164, Loss: 0.44295452976444744\n",
      "Epoch 165, Loss: 0.4426058108229731\n",
      "Epoch 166, Loss: 0.4422485787900922\n",
      "Epoch 167, Loss: 0.4419090985167976\n",
      "Epoch 168, Loss: 0.44158054387831963\n",
      "Epoch 169, Loss: 0.4412474388772268\n",
      "Epoch 170, Loss: 0.4409277694136744\n",
      "Epoch 171, Loss: 0.44068045571539705\n",
      "Epoch 172, Loss: 0.4405979418732049\n",
      "Epoch 173, Loss: 0.44094101686109266\n",
      "Epoch 174, Loss: 0.4417368233339534\n",
      "Epoch 175, Loss: 0.4416582519101522\n",
      "Epoch 176, Loss: 0.4396788970821407\n",
      "Epoch 177, Loss: 0.4390195954561405\n",
      "Epoch 178, Loss: 0.4400830988007806\n",
      "Epoch 179, Loss: 0.43931775869233325\n",
      "Epoch 180, Loss: 0.4380032505977321\n",
      "Epoch 181, Loss: 0.4384470974340077\n",
      "Epoch 182, Loss: 0.4385167636581309\n",
      "Epoch 183, Loss: 0.43733043450040754\n",
      "Epoch 184, Loss: 0.4373176399006318\n",
      "Epoch 185, Loss: 0.4375256973621567\n",
      "Epoch 186, Loss: 0.43659322296038683\n",
      "Epoch 187, Loss: 0.43644391045908093\n",
      "Epoch 188, Loss: 0.43663844358015674\n",
      "Epoch 189, Loss: 0.43586286705793914\n",
      "Epoch 190, Loss: 0.4355663072844155\n",
      "Epoch 191, Loss: 0.4357040532214924\n",
      "Epoch 192, Loss: 0.435134007760397\n",
      "Epoch 193, Loss: 0.43474241127906293\n",
      "Epoch 194, Loss: 0.43480482310319635\n",
      "Epoch 195, Loss: 0.43443826104982264\n",
      "Epoch 196, Loss: 0.43390767873021246\n",
      "Epoch 197, Loss: 0.4338897691233524\n",
      "Epoch 198, Loss: 0.43377856877811044\n",
      "Epoch 199, Loss: 0.4331595506415052\n",
      "Epoch 200, Loss: 0.43289415091975497\n",
      "Epoch 201, Loss: 0.43294048069736957\n",
      "Epoch 202, Loss: 0.43262240576772243\n",
      "Epoch 203, Loss: 0.43206541812705346\n",
      "Epoch 204, Loss: 0.43172500614102416\n",
      "Epoch 205, Loss: 0.4316686388133938\n",
      "Epoch 206, Loss: 0.4315179158466364\n",
      "Epoch 207, Loss: 0.4310712381632941\n",
      "Epoch 208, Loss: 0.43059470035496\n",
      "Epoch 209, Loss: 0.4303711221416647\n",
      "Epoch 210, Loss: 0.43033088719595897\n",
      "Epoch 211, Loss: 0.43016580489992484\n",
      "Epoch 212, Loss: 0.429691817522032\n",
      "Epoch 213, Loss: 0.4292099292794459\n",
      "Epoch 214, Loss: 0.4289207205910941\n",
      "Epoch 215, Loss: 0.4287978238495365\n",
      "Epoch 216, Loss: 0.42870603658529494\n",
      "Epoch 217, Loss: 0.42841980940051866\n",
      "Epoch 218, Loss: 0.4278918633922788\n",
      "Epoch 219, Loss: 0.42748307113702383\n",
      "Epoch 220, Loss: 0.4271688511370021\n",
      "Epoch 221, Loss: 0.4269432345934848\n",
      "Epoch 222, Loss: 0.4267651275776002\n",
      "Epoch 223, Loss: 0.42668376428587257\n",
      "Epoch 224, Loss: 0.42653554195413756\n",
      "Epoch 225, Loss: 0.42613293183996703\n",
      "Epoch 226, Loss: 0.4255451632780942\n",
      "Epoch 227, Loss: 0.4251305043179084\n",
      "Epoch 228, Loss: 0.42475934718412794\n",
      "Epoch 229, Loss: 0.42452283256309487\n",
      "Epoch 230, Loss: 0.4244024016195508\n",
      "Epoch 231, Loss: 0.4242604946441205\n",
      "Epoch 232, Loss: 0.424134367950937\n",
      "Epoch 233, Loss: 0.42372608274856827\n",
      "Epoch 234, Loss: 0.4231492155631465\n",
      "Epoch 235, Loss: 0.4226291943984805\n",
      "Epoch 236, Loss: 0.4221723179942825\n",
      "Epoch 237, Loss: 0.4218955917984354\n",
      "Epoch 238, Loss: 0.4216767834434851\n",
      "Epoch 239, Loss: 0.42145169178243785\n",
      "Epoch 240, Loss: 0.4212605805865247\n",
      "Epoch 241, Loss: 0.42123380253494425\n",
      "Epoch 242, Loss: 0.42125483246926426\n",
      "Epoch 243, Loss: 0.42124070061060576\n",
      "Epoch 244, Loss: 0.42073760533336746\n",
      "Epoch 245, Loss: 0.42002867294257734\n",
      "Epoch 246, Loss: 0.41900436945460057\n",
      "Epoch 247, Loss: 0.41841940783018283\n",
      "Epoch 248, Loss: 0.4184544673382656\n",
      "Epoch 249, Loss: 0.4187823781263808\n",
      "Epoch 250, Loss: 0.41914970907160026\n",
      "Epoch 251, Loss: 0.418702737788617\n",
      "Epoch 252, Loss: 0.41762532180686956\n",
      "Epoch 253, Loss: 0.41665732452727816\n",
      "Epoch 254, Loss: 0.4164608109104539\n",
      "Epoch 255, Loss: 0.4169362050698102\n",
      "Epoch 256, Loss: 0.4170439641107097\n",
      "Epoch 257, Loss: 0.41618098523090385\n",
      "Epoch 258, Loss: 0.41521870035352226\n",
      "Epoch 259, Loss: 0.41507164322372797\n",
      "Epoch 260, Loss: 0.4154855816987037\n",
      "Epoch 261, Loss: 0.4152875627561342\n",
      "Epoch 262, Loss: 0.4146051629547779\n",
      "Epoch 263, Loss: 0.4138602536580612\n",
      "Epoch 264, Loss: 0.4135535008669373\n",
      "Epoch 265, Loss: 0.4136485063474015\n",
      "Epoch 266, Loss: 0.4135539332507534\n",
      "Epoch 267, Loss: 0.4133421191440525\n",
      "Epoch 268, Loss: 0.412685660282317\n",
      "Epoch 269, Loss: 0.4122655129807526\n",
      "Epoch 270, Loss: 0.4118217259860175\n",
      "Epoch 271, Loss: 0.41165256945543577\n",
      "Epoch 272, Loss: 0.41146929239414193\n",
      "Epoch 273, Loss: 0.4115028618809221\n",
      "Epoch 274, Loss: 0.4117686232412549\n",
      "Epoch 275, Loss: 0.4121111105932055\n",
      "Epoch 276, Loss: 0.4130580342565522\n",
      "Epoch 277, Loss: 0.4121949346593443\n",
      "Epoch 278, Loss: 0.41057389204015954\n",
      "Epoch 279, Loss: 0.4095484089667836\n",
      "Epoch 280, Loss: 0.4098107108594438\n",
      "Epoch 281, Loss: 0.41080122599126295\n",
      "Epoch 282, Loss: 0.41057013564631983\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2514967044328438\n",
      "Valence RMSE: 0.2621577083884986\n",
      "Arousal RMSE: 0.2403633096245209\n",
      "Test R^2 score: tensor([0.2656, 0.5739], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4197520271186098\n",
      "Num of epochs: 283\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Epoch 121, Loss: 0.4596922103619757\n",
      "Epoch 122, Loss: 0.45943389956138125\n",
      "Epoch 123, Loss: 0.45877818894117006\n",
      "Epoch 124, Loss: 0.45746538132874076\n",
      "Epoch 125, Loss: 0.4570462803121619\n",
      "Epoch 126, Loss: 0.45696189549769234\n",
      "Epoch 127, Loss: 0.45594920341421274\n",
      "Epoch 128, Loss: 0.45536359747685007\n",
      "Epoch 129, Loss: 0.4553290890752543\n",
      "Epoch 130, Loss: 0.4547195823074053\n",
      "Epoch 131, Loss: 0.45392838323998064\n",
      "Epoch 132, Loss: 0.4537853493595631\n",
      "Epoch 133, Loss: 0.45347437050767975\n",
      "Epoch 134, Loss: 0.45267431273812453\n",
      "Epoch 135, Loss: 0.45228737659373064\n",
      "Epoch 136, Loss: 0.4522154657432981\n",
      "Epoch 137, Loss: 0.45165964487407706\n",
      "Epoch 138, Loss: 0.4510751265462795\n",
      "Epoch 139, Loss: 0.45082650445054473\n",
      "Epoch 140, Loss: 0.4506281098405297\n",
      "Epoch 141, Loss: 0.4501911419656675\n",
      "Epoch 142, Loss: 0.4497001887659326\n",
      "Epoch 143, Loss: 0.44943436133718806\n",
      "Epoch 144, Loss: 0.449226296402944\n",
      "Epoch 145, Loss: 0.44886073897867573\n",
      "Epoch 146, Loss: 0.44844710376304864\n",
      "Epoch 147, Loss: 0.44810507013116724\n",
      "Epoch 148, Loss: 0.44788545862049767\n",
      "Epoch 149, Loss: 0.4476741441009651\n",
      "Epoch 150, Loss: 0.44735861731902055\n",
      "Epoch 151, Loss: 0.4469288720157105\n",
      "Epoch 152, Loss: 0.44655235754178146\n",
      "Epoch 153, Loss: 0.4462857402786373\n",
      "Epoch 154, Loss: 0.4460517369142754\n",
      "Epoch 155, Loss: 0.4457881796184935\n",
      "Epoch 156, Loss: 0.44542210886348255\n",
      "Epoch 157, Loss: 0.4450579467827662\n",
      "Epoch 158, Loss: 0.44473329320345206\n",
      "Epoch 159, Loss: 0.4444482514678638\n",
      "Epoch 160, Loss: 0.4441745674630603\n",
      "Epoch 161, Loss: 0.4439221813722748\n",
      "Epoch 162, Loss: 0.44363357870363757\n",
      "Epoch 163, Loss: 0.4433189071134186\n",
      "Epoch 164, Loss: 0.44295452976444744\n",
      "Epoch 165, Loss: 0.4426058108229731\n",
      "Epoch 166, Loss: 0.4422485787900922\n",
      "Epoch 167, Loss: 0.4419090985167976\n",
      "Epoch 168, Loss: 0.44158054387831963\n",
      "Epoch 169, Loss: 0.4412474388772268\n",
      "Epoch 170, Loss: 0.4409277694136744\n",
      "Epoch 171, Loss: 0.44068045571539705\n",
      "Epoch 172, Loss: 0.4405979418732049\n",
      "Epoch 173, Loss: 0.44094101686109266\n",
      "Epoch 174, Loss: 0.4417368233339534\n",
      "Epoch 175, Loss: 0.4416582519101522\n",
      "Epoch 176, Loss: 0.4396788970821407\n",
      "Epoch 177, Loss: 0.4390195954561405\n",
      "Epoch 178, Loss: 0.4400830988007806\n",
      "Epoch 179, Loss: 0.43931775869233325\n",
      "Epoch 180, Loss: 0.4380032505977321\n",
      "Epoch 181, Loss: 0.4384470974340077\n",
      "Epoch 182, Loss: 0.4385167636581309\n",
      "Epoch 183, Loss: 0.43733043450040754\n",
      "Epoch 184, Loss: 0.4373176399006318\n",
      "Epoch 185, Loss: 0.4375256973621567\n",
      "Epoch 186, Loss: 0.43659322296038683\n",
      "Epoch 187, Loss: 0.43644391045908093\n",
      "Epoch 188, Loss: 0.43663844358015674\n",
      "Epoch 189, Loss: 0.43586286705793914\n",
      "Epoch 190, Loss: 0.4355663072844155\n",
      "Epoch 191, Loss: 0.4357040532214924\n",
      "Epoch 192, Loss: 0.435134007760397\n",
      "Epoch 193, Loss: 0.43474241127906293\n",
      "Epoch 194, Loss: 0.43480482310319635\n",
      "Epoch 195, Loss: 0.43443826104982264\n",
      "Epoch 196, Loss: 0.43390767873021246\n",
      "Epoch 197, Loss: 0.4338897691233524\n",
      "Epoch 198, Loss: 0.43377856877811044\n",
      "Epoch 199, Loss: 0.4331595506415052\n",
      "Epoch 200, Loss: 0.43289415091975497\n",
      "Epoch 201, Loss: 0.43294048069736957\n",
      "Epoch 202, Loss: 0.43262240576772243\n",
      "Epoch 203, Loss: 0.43206541812705346\n",
      "Epoch 204, Loss: 0.43172500614102416\n",
      "Epoch 205, Loss: 0.4316686388133938\n",
      "Epoch 206, Loss: 0.4315179158466364\n",
      "Epoch 207, Loss: 0.4310712381632941\n",
      "Epoch 208, Loss: 0.43059470035496\n",
      "Epoch 209, Loss: 0.4303711221416647\n",
      "Epoch 210, Loss: 0.43033088719595897\n",
      "Epoch 211, Loss: 0.43016580489992484\n",
      "Epoch 212, Loss: 0.429691817522032\n",
      "Epoch 213, Loss: 0.4292099292794459\n",
      "Epoch 214, Loss: 0.4289207205910941\n",
      "Epoch 215, Loss: 0.4287978238495365\n",
      "Epoch 216, Loss: 0.42870603658529494\n",
      "Epoch 217, Loss: 0.42841980940051866\n",
      "Epoch 218, Loss: 0.4278918633922788\n",
      "Epoch 219, Loss: 0.42748307113702383\n",
      "Epoch 220, Loss: 0.4271688511370021\n",
      "Epoch 221, Loss: 0.4269432345934848\n",
      "Epoch 222, Loss: 0.4267651275776002\n",
      "Epoch 223, Loss: 0.42668376428587257\n",
      "Epoch 224, Loss: 0.42653554195413756\n",
      "Epoch 225, Loss: 0.42613293183996703\n",
      "Epoch 226, Loss: 0.4255451632780942\n",
      "Epoch 227, Loss: 0.4251305043179084\n",
      "Epoch 228, Loss: 0.42475934718412794\n",
      "Epoch 229, Loss: 0.42452283256309487\n",
      "Epoch 230, Loss: 0.4244024016195508\n",
      "Epoch 231, Loss: 0.4242604946441205\n",
      "Epoch 232, Loss: 0.424134367950937\n",
      "Epoch 233, Loss: 0.42372608274856827\n",
      "Epoch 234, Loss: 0.4231492155631465\n",
      "Epoch 235, Loss: 0.4226291943984805\n",
      "Epoch 236, Loss: 0.4221723179942825\n",
      "Epoch 237, Loss: 0.4218955917984354\n",
      "Epoch 238, Loss: 0.4216767834434851\n",
      "Epoch 239, Loss: 0.42145169178243785\n",
      "Epoch 240, Loss: 0.4212605805865247\n",
      "Epoch 241, Loss: 0.42123380253494425\n",
      "Epoch 242, Loss: 0.42125483246926426\n",
      "Epoch 243, Loss: 0.42124070061060576\n",
      "Epoch 244, Loss: 0.42073760533336746\n",
      "Epoch 245, Loss: 0.42002867294257734\n",
      "Epoch 246, Loss: 0.41900436945460057\n",
      "Epoch 247, Loss: 0.41841940783018283\n",
      "Epoch 248, Loss: 0.4184544673382656\n",
      "Epoch 249, Loss: 0.4187823781263808\n",
      "Epoch 250, Loss: 0.41914970907160026\n",
      "Epoch 251, Loss: 0.418702737788617\n",
      "Epoch 252, Loss: 0.41762532180686956\n",
      "Epoch 253, Loss: 0.41665732452727816\n",
      "Epoch 254, Loss: 0.4164608109104539\n",
      "Epoch 255, Loss: 0.4169362050698102\n",
      "Epoch 256, Loss: 0.4170439641107097\n",
      "Epoch 257, Loss: 0.41618098523090385\n",
      "Epoch 258, Loss: 0.41521870035352226\n",
      "Epoch 259, Loss: 0.41507164322372797\n",
      "Epoch 260, Loss: 0.4154855816987037\n",
      "Epoch 261, Loss: 0.4152875627561342\n",
      "Epoch 262, Loss: 0.4146051629547779\n",
      "Epoch 263, Loss: 0.4138602536580612\n",
      "Epoch 264, Loss: 0.4135535008669373\n",
      "Epoch 265, Loss: 0.4136485063474015\n",
      "Epoch 266, Loss: 0.4135539332507534\n",
      "Epoch 267, Loss: 0.4133421191440525\n",
      "Epoch 268, Loss: 0.412685660282317\n",
      "Epoch 269, Loss: 0.4122655129807526\n",
      "Epoch 270, Loss: 0.4118217259860175\n",
      "Epoch 271, Loss: 0.41165256945543577\n",
      "Epoch 272, Loss: 0.41146929239414193\n",
      "Epoch 273, Loss: 0.4115028618809221\n",
      "Epoch 274, Loss: 0.4117686232412549\n",
      "Epoch 275, Loss: 0.4121111105932055\n",
      "Epoch 276, Loss: 0.4130580342565522\n",
      "Epoch 277, Loss: 0.4121949346593443\n",
      "Epoch 278, Loss: 0.41057389204015954\n",
      "Epoch 279, Loss: 0.4095484089667836\n",
      "Epoch 280, Loss: 0.4098107108594438\n",
      "Epoch 281, Loss: 0.41080122599126295\n",
      "Epoch 282, Loss: 0.41057013564631983\n",
      "Epoch 283, Loss: 0.4096005808315313\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2510584876276694\n",
      "Valence RMSE: 0.26142112233520887\n",
      "Arousal RMSE: 0.24024929805679976\n",
      "Test R^2 score: tensor([0.2697, 0.5743], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.42201466468925397\n",
      "Num of epochs: 284\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Epoch 121, Loss: 0.4596922103619757\n",
      "Epoch 122, Loss: 0.45943389956138125\n",
      "Epoch 123, Loss: 0.45877818894117006\n",
      "Epoch 124, Loss: 0.45746538132874076\n",
      "Epoch 125, Loss: 0.4570462803121619\n",
      "Epoch 126, Loss: 0.45696189549769234\n",
      "Epoch 127, Loss: 0.45594920341421274\n",
      "Epoch 128, Loss: 0.45536359747685007\n",
      "Epoch 129, Loss: 0.4553290890752543\n",
      "Epoch 130, Loss: 0.4547195823074053\n",
      "Epoch 131, Loss: 0.45392838323998064\n",
      "Epoch 132, Loss: 0.4537853493595631\n",
      "Epoch 133, Loss: 0.45347437050767975\n",
      "Epoch 134, Loss: 0.45267431273812453\n",
      "Epoch 135, Loss: 0.45228737659373064\n",
      "Epoch 136, Loss: 0.4522154657432981\n",
      "Epoch 137, Loss: 0.45165964487407706\n",
      "Epoch 138, Loss: 0.4510751265462795\n",
      "Epoch 139, Loss: 0.45082650445054473\n",
      "Epoch 140, Loss: 0.4506281098405297\n",
      "Epoch 141, Loss: 0.4501911419656675\n",
      "Epoch 142, Loss: 0.4497001887659326\n",
      "Epoch 143, Loss: 0.44943436133718806\n",
      "Epoch 144, Loss: 0.449226296402944\n",
      "Epoch 145, Loss: 0.44886073897867573\n",
      "Epoch 146, Loss: 0.44844710376304864\n",
      "Epoch 147, Loss: 0.44810507013116724\n",
      "Epoch 148, Loss: 0.44788545862049767\n",
      "Epoch 149, Loss: 0.4476741441009651\n",
      "Epoch 150, Loss: 0.44735861731902055\n",
      "Epoch 151, Loss: 0.4469288720157105\n",
      "Epoch 152, Loss: 0.44655235754178146\n",
      "Epoch 153, Loss: 0.4462857402786373\n",
      "Epoch 154, Loss: 0.4460517369142754\n",
      "Epoch 155, Loss: 0.4457881796184935\n",
      "Epoch 156, Loss: 0.44542210886348255\n",
      "Epoch 157, Loss: 0.4450579467827662\n",
      "Epoch 158, Loss: 0.44473329320345206\n",
      "Epoch 159, Loss: 0.4444482514678638\n",
      "Epoch 160, Loss: 0.4441745674630603\n",
      "Epoch 161, Loss: 0.4439221813722748\n",
      "Epoch 162, Loss: 0.44363357870363757\n",
      "Epoch 163, Loss: 0.4433189071134186\n",
      "Epoch 164, Loss: 0.44295452976444744\n",
      "Epoch 165, Loss: 0.4426058108229731\n",
      "Epoch 166, Loss: 0.4422485787900922\n",
      "Epoch 167, Loss: 0.4419090985167976\n",
      "Epoch 168, Loss: 0.44158054387831963\n",
      "Epoch 169, Loss: 0.4412474388772268\n",
      "Epoch 170, Loss: 0.4409277694136744\n",
      "Epoch 171, Loss: 0.44068045571539705\n",
      "Epoch 172, Loss: 0.4405979418732049\n",
      "Epoch 173, Loss: 0.44094101686109266\n",
      "Epoch 174, Loss: 0.4417368233339534\n",
      "Epoch 175, Loss: 0.4416582519101522\n",
      "Epoch 176, Loss: 0.4396788970821407\n",
      "Epoch 177, Loss: 0.4390195954561405\n",
      "Epoch 178, Loss: 0.4400830988007806\n",
      "Epoch 179, Loss: 0.43931775869233325\n",
      "Epoch 180, Loss: 0.4380032505977321\n",
      "Epoch 181, Loss: 0.4384470974340077\n",
      "Epoch 182, Loss: 0.4385167636581309\n",
      "Epoch 183, Loss: 0.43733043450040754\n",
      "Epoch 184, Loss: 0.4373176399006318\n",
      "Epoch 185, Loss: 0.4375256973621567\n",
      "Epoch 186, Loss: 0.43659322296038683\n",
      "Epoch 187, Loss: 0.43644391045908093\n",
      "Epoch 188, Loss: 0.43663844358015674\n",
      "Epoch 189, Loss: 0.43586286705793914\n",
      "Epoch 190, Loss: 0.4355663072844155\n",
      "Epoch 191, Loss: 0.4357040532214924\n",
      "Epoch 192, Loss: 0.435134007760397\n",
      "Epoch 193, Loss: 0.43474241127906293\n",
      "Epoch 194, Loss: 0.43480482310319635\n",
      "Epoch 195, Loss: 0.43443826104982264\n",
      "Epoch 196, Loss: 0.43390767873021246\n",
      "Epoch 197, Loss: 0.4338897691233524\n",
      "Epoch 198, Loss: 0.43377856877811044\n",
      "Epoch 199, Loss: 0.4331595506415052\n",
      "Epoch 200, Loss: 0.43289415091975497\n",
      "Epoch 201, Loss: 0.43294048069736957\n",
      "Epoch 202, Loss: 0.43262240576772243\n",
      "Epoch 203, Loss: 0.43206541812705346\n",
      "Epoch 204, Loss: 0.43172500614102416\n",
      "Epoch 205, Loss: 0.4316686388133938\n",
      "Epoch 206, Loss: 0.4315179158466364\n",
      "Epoch 207, Loss: 0.4310712381632941\n",
      "Epoch 208, Loss: 0.43059470035496\n",
      "Epoch 209, Loss: 0.4303711221416647\n",
      "Epoch 210, Loss: 0.43033088719595897\n",
      "Epoch 211, Loss: 0.43016580489992484\n",
      "Epoch 212, Loss: 0.429691817522032\n",
      "Epoch 213, Loss: 0.4292099292794459\n",
      "Epoch 214, Loss: 0.4289207205910941\n",
      "Epoch 215, Loss: 0.4287978238495365\n",
      "Epoch 216, Loss: 0.42870603658529494\n",
      "Epoch 217, Loss: 0.42841980940051866\n",
      "Epoch 218, Loss: 0.4278918633922788\n",
      "Epoch 219, Loss: 0.42748307113702383\n",
      "Epoch 220, Loss: 0.4271688511370021\n",
      "Epoch 221, Loss: 0.4269432345934848\n",
      "Epoch 222, Loss: 0.4267651275776002\n",
      "Epoch 223, Loss: 0.42668376428587257\n",
      "Epoch 224, Loss: 0.42653554195413756\n",
      "Epoch 225, Loss: 0.42613293183996703\n",
      "Epoch 226, Loss: 0.4255451632780942\n",
      "Epoch 227, Loss: 0.4251305043179084\n",
      "Epoch 228, Loss: 0.42475934718412794\n",
      "Epoch 229, Loss: 0.42452283256309487\n",
      "Epoch 230, Loss: 0.4244024016195508\n",
      "Epoch 231, Loss: 0.4242604946441205\n",
      "Epoch 232, Loss: 0.424134367950937\n",
      "Epoch 233, Loss: 0.42372608274856827\n",
      "Epoch 234, Loss: 0.4231492155631465\n",
      "Epoch 235, Loss: 0.4226291943984805\n",
      "Epoch 236, Loss: 0.4221723179942825\n",
      "Epoch 237, Loss: 0.4218955917984354\n",
      "Epoch 238, Loss: 0.4216767834434851\n",
      "Epoch 239, Loss: 0.42145169178243785\n",
      "Epoch 240, Loss: 0.4212605805865247\n",
      "Epoch 241, Loss: 0.42123380253494425\n",
      "Epoch 242, Loss: 0.42125483246926426\n",
      "Epoch 243, Loss: 0.42124070061060576\n",
      "Epoch 244, Loss: 0.42073760533336746\n",
      "Epoch 245, Loss: 0.42002867294257734\n",
      "Epoch 246, Loss: 0.41900436945460057\n",
      "Epoch 247, Loss: 0.41841940783018283\n",
      "Epoch 248, Loss: 0.4184544673382656\n",
      "Epoch 249, Loss: 0.4187823781263808\n",
      "Epoch 250, Loss: 0.41914970907160026\n",
      "Epoch 251, Loss: 0.418702737788617\n",
      "Epoch 252, Loss: 0.41762532180686956\n",
      "Epoch 253, Loss: 0.41665732452727816\n",
      "Epoch 254, Loss: 0.4164608109104539\n",
      "Epoch 255, Loss: 0.4169362050698102\n",
      "Epoch 256, Loss: 0.4170439641107097\n",
      "Epoch 257, Loss: 0.41618098523090385\n",
      "Epoch 258, Loss: 0.41521870035352226\n",
      "Epoch 259, Loss: 0.41507164322372797\n",
      "Epoch 260, Loss: 0.4154855816987037\n",
      "Epoch 261, Loss: 0.4152875627561342\n",
      "Epoch 262, Loss: 0.4146051629547779\n",
      "Epoch 263, Loss: 0.4138602536580612\n",
      "Epoch 264, Loss: 0.4135535008669373\n",
      "Epoch 265, Loss: 0.4136485063474015\n",
      "Epoch 266, Loss: 0.4135539332507534\n",
      "Epoch 267, Loss: 0.4133421191440525\n",
      "Epoch 268, Loss: 0.412685660282317\n",
      "Epoch 269, Loss: 0.4122655129807526\n",
      "Epoch 270, Loss: 0.4118217259860175\n",
      "Epoch 271, Loss: 0.41165256945543577\n",
      "Epoch 272, Loss: 0.41146929239414193\n",
      "Epoch 273, Loss: 0.4115028618809221\n",
      "Epoch 274, Loss: 0.4117686232412549\n",
      "Epoch 275, Loss: 0.4121111105932055\n",
      "Epoch 276, Loss: 0.4130580342565522\n",
      "Epoch 277, Loss: 0.4121949346593443\n",
      "Epoch 278, Loss: 0.41057389204015954\n",
      "Epoch 279, Loss: 0.4095484089667836\n",
      "Epoch 280, Loss: 0.4098107108594438\n",
      "Epoch 281, Loss: 0.41080122599126295\n",
      "Epoch 282, Loss: 0.41057013564631983\n",
      "Epoch 283, Loss: 0.4096005808315313\n",
      "Epoch 284, Loss: 0.4083534584470671\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.25105074914315145\n",
      "Valence RMSE: 0.2614159762945221\n",
      "Arousal RMSE: 0.24023872424872922\n",
      "Test R^2 score: tensor([0.2697, 0.5744], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.422047774776748\n",
      "Num of epochs: 285\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Epoch 121, Loss: 0.4596922103619757\n",
      "Epoch 122, Loss: 0.45943389956138125\n",
      "Epoch 123, Loss: 0.45877818894117006\n",
      "Epoch 124, Loss: 0.45746538132874076\n",
      "Epoch 125, Loss: 0.4570462803121619\n",
      "Epoch 126, Loss: 0.45696189549769234\n",
      "Epoch 127, Loss: 0.45594920341421274\n",
      "Epoch 128, Loss: 0.45536359747685007\n",
      "Epoch 129, Loss: 0.4553290890752543\n",
      "Epoch 130, Loss: 0.4547195823074053\n",
      "Epoch 131, Loss: 0.45392838323998064\n",
      "Epoch 132, Loss: 0.4537853493595631\n",
      "Epoch 133, Loss: 0.45347437050767975\n",
      "Epoch 134, Loss: 0.45267431273812453\n",
      "Epoch 135, Loss: 0.45228737659373064\n",
      "Epoch 136, Loss: 0.4522154657432981\n",
      "Epoch 137, Loss: 0.45165964487407706\n",
      "Epoch 138, Loss: 0.4510751265462795\n",
      "Epoch 139, Loss: 0.45082650445054473\n",
      "Epoch 140, Loss: 0.4506281098405297\n",
      "Epoch 141, Loss: 0.4501911419656675\n",
      "Epoch 142, Loss: 0.4497001887659326\n",
      "Epoch 143, Loss: 0.44943436133718806\n",
      "Epoch 144, Loss: 0.449226296402944\n",
      "Epoch 145, Loss: 0.44886073897867573\n",
      "Epoch 146, Loss: 0.44844710376304864\n",
      "Epoch 147, Loss: 0.44810507013116724\n",
      "Epoch 148, Loss: 0.44788545862049767\n",
      "Epoch 149, Loss: 0.4476741441009651\n",
      "Epoch 150, Loss: 0.44735861731902055\n",
      "Epoch 151, Loss: 0.4469288720157105\n",
      "Epoch 152, Loss: 0.44655235754178146\n",
      "Epoch 153, Loss: 0.4462857402786373\n",
      "Epoch 154, Loss: 0.4460517369142754\n",
      "Epoch 155, Loss: 0.4457881796184935\n",
      "Epoch 156, Loss: 0.44542210886348255\n",
      "Epoch 157, Loss: 0.4450579467827662\n",
      "Epoch 158, Loss: 0.44473329320345206\n",
      "Epoch 159, Loss: 0.4444482514678638\n",
      "Epoch 160, Loss: 0.4441745674630603\n",
      "Epoch 161, Loss: 0.4439221813722748\n",
      "Epoch 162, Loss: 0.44363357870363757\n",
      "Epoch 163, Loss: 0.4433189071134186\n",
      "Epoch 164, Loss: 0.44295452976444744\n",
      "Epoch 165, Loss: 0.4426058108229731\n",
      "Epoch 166, Loss: 0.4422485787900922\n",
      "Epoch 167, Loss: 0.4419090985167976\n",
      "Epoch 168, Loss: 0.44158054387831963\n",
      "Epoch 169, Loss: 0.4412474388772268\n",
      "Epoch 170, Loss: 0.4409277694136744\n",
      "Epoch 171, Loss: 0.44068045571539705\n",
      "Epoch 172, Loss: 0.4405979418732049\n",
      "Epoch 173, Loss: 0.44094101686109266\n",
      "Epoch 174, Loss: 0.4417368233339534\n",
      "Epoch 175, Loss: 0.4416582519101522\n",
      "Epoch 176, Loss: 0.4396788970821407\n",
      "Epoch 177, Loss: 0.4390195954561405\n",
      "Epoch 178, Loss: 0.4400830988007806\n",
      "Epoch 179, Loss: 0.43931775869233325\n",
      "Epoch 180, Loss: 0.4380032505977321\n",
      "Epoch 181, Loss: 0.4384470974340077\n",
      "Epoch 182, Loss: 0.4385167636581309\n",
      "Epoch 183, Loss: 0.43733043450040754\n",
      "Epoch 184, Loss: 0.4373176399006318\n",
      "Epoch 185, Loss: 0.4375256973621567\n",
      "Epoch 186, Loss: 0.43659322296038683\n",
      "Epoch 187, Loss: 0.43644391045908093\n",
      "Epoch 188, Loss: 0.43663844358015674\n",
      "Epoch 189, Loss: 0.43586286705793914\n",
      "Epoch 190, Loss: 0.4355663072844155\n",
      "Epoch 191, Loss: 0.4357040532214924\n",
      "Epoch 192, Loss: 0.435134007760397\n",
      "Epoch 193, Loss: 0.43474241127906293\n",
      "Epoch 194, Loss: 0.43480482310319635\n",
      "Epoch 195, Loss: 0.43443826104982264\n",
      "Epoch 196, Loss: 0.43390767873021246\n",
      "Epoch 197, Loss: 0.4338897691233524\n",
      "Epoch 198, Loss: 0.43377856877811044\n",
      "Epoch 199, Loss: 0.4331595506415052\n",
      "Epoch 200, Loss: 0.43289415091975497\n",
      "Epoch 201, Loss: 0.43294048069736957\n",
      "Epoch 202, Loss: 0.43262240576772243\n",
      "Epoch 203, Loss: 0.43206541812705346\n",
      "Epoch 204, Loss: 0.43172500614102416\n",
      "Epoch 205, Loss: 0.4316686388133938\n",
      "Epoch 206, Loss: 0.4315179158466364\n",
      "Epoch 207, Loss: 0.4310712381632941\n",
      "Epoch 208, Loss: 0.43059470035496\n",
      "Epoch 209, Loss: 0.4303711221416647\n",
      "Epoch 210, Loss: 0.43033088719595897\n",
      "Epoch 211, Loss: 0.43016580489992484\n",
      "Epoch 212, Loss: 0.429691817522032\n",
      "Epoch 213, Loss: 0.4292099292794459\n",
      "Epoch 214, Loss: 0.4289207205910941\n",
      "Epoch 215, Loss: 0.4287978238495365\n",
      "Epoch 216, Loss: 0.42870603658529494\n",
      "Epoch 217, Loss: 0.42841980940051866\n",
      "Epoch 218, Loss: 0.4278918633922788\n",
      "Epoch 219, Loss: 0.42748307113702383\n",
      "Epoch 220, Loss: 0.4271688511370021\n",
      "Epoch 221, Loss: 0.4269432345934848\n",
      "Epoch 222, Loss: 0.4267651275776002\n",
      "Epoch 223, Loss: 0.42668376428587257\n",
      "Epoch 224, Loss: 0.42653554195413756\n",
      "Epoch 225, Loss: 0.42613293183996703\n",
      "Epoch 226, Loss: 0.4255451632780942\n",
      "Epoch 227, Loss: 0.4251305043179084\n",
      "Epoch 228, Loss: 0.42475934718412794\n",
      "Epoch 229, Loss: 0.42452283256309487\n",
      "Epoch 230, Loss: 0.4244024016195508\n",
      "Epoch 231, Loss: 0.4242604946441205\n",
      "Epoch 232, Loss: 0.424134367950937\n",
      "Epoch 233, Loss: 0.42372608274856827\n",
      "Epoch 234, Loss: 0.4231492155631465\n",
      "Epoch 235, Loss: 0.4226291943984805\n",
      "Epoch 236, Loss: 0.4221723179942825\n",
      "Epoch 237, Loss: 0.4218955917984354\n",
      "Epoch 238, Loss: 0.4216767834434851\n",
      "Epoch 239, Loss: 0.42145169178243785\n",
      "Epoch 240, Loss: 0.4212605805865247\n",
      "Epoch 241, Loss: 0.42123380253494425\n",
      "Epoch 242, Loss: 0.42125483246926426\n",
      "Epoch 243, Loss: 0.42124070061060576\n",
      "Epoch 244, Loss: 0.42073760533336746\n",
      "Epoch 245, Loss: 0.42002867294257734\n",
      "Epoch 246, Loss: 0.41900436945460057\n",
      "Epoch 247, Loss: 0.41841940783018283\n",
      "Epoch 248, Loss: 0.4184544673382656\n",
      "Epoch 249, Loss: 0.4187823781263808\n",
      "Epoch 250, Loss: 0.41914970907160026\n",
      "Epoch 251, Loss: 0.418702737788617\n",
      "Epoch 252, Loss: 0.41762532180686956\n",
      "Epoch 253, Loss: 0.41665732452727816\n",
      "Epoch 254, Loss: 0.4164608109104539\n",
      "Epoch 255, Loss: 0.4169362050698102\n",
      "Epoch 256, Loss: 0.4170439641107097\n",
      "Epoch 257, Loss: 0.41618098523090385\n",
      "Epoch 258, Loss: 0.41521870035352226\n",
      "Epoch 259, Loss: 0.41507164322372797\n",
      "Epoch 260, Loss: 0.4154855816987037\n",
      "Epoch 261, Loss: 0.4152875627561342\n",
      "Epoch 262, Loss: 0.4146051629547779\n",
      "Epoch 263, Loss: 0.4138602536580612\n",
      "Epoch 264, Loss: 0.4135535008669373\n",
      "Epoch 265, Loss: 0.4136485063474015\n",
      "Epoch 266, Loss: 0.4135539332507534\n",
      "Epoch 267, Loss: 0.4133421191440525\n",
      "Epoch 268, Loss: 0.412685660282317\n",
      "Epoch 269, Loss: 0.4122655129807526\n",
      "Epoch 270, Loss: 0.4118217259860175\n",
      "Epoch 271, Loss: 0.41165256945543577\n",
      "Epoch 272, Loss: 0.41146929239414193\n",
      "Epoch 273, Loss: 0.4115028618809221\n",
      "Epoch 274, Loss: 0.4117686232412549\n",
      "Epoch 275, Loss: 0.4121111105932055\n",
      "Epoch 276, Loss: 0.4130580342565522\n",
      "Epoch 277, Loss: 0.4121949346593443\n",
      "Epoch 278, Loss: 0.41057389204015954\n",
      "Epoch 279, Loss: 0.4095484089667836\n",
      "Epoch 280, Loss: 0.4098107108594438\n",
      "Epoch 281, Loss: 0.41080122599126295\n",
      "Epoch 282, Loss: 0.41057013564631983\n",
      "Epoch 283, Loss: 0.4096005808315313\n",
      "Epoch 284, Loss: 0.4083534584470671\n",
      "Epoch 285, Loss: 0.4080981251863069\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2518115504036324\n",
      "Valence RMSE: 0.2627314353652882\n",
      "Arousal RMSE: 0.24039614536064766\n",
      "Test R^2 score: tensor([0.2624, 0.5738], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.41808481301576017\n",
      "Num of epochs: 286\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Epoch 121, Loss: 0.4596922103619757\n",
      "Epoch 122, Loss: 0.45943389956138125\n",
      "Epoch 123, Loss: 0.45877818894117006\n",
      "Epoch 124, Loss: 0.45746538132874076\n",
      "Epoch 125, Loss: 0.4570462803121619\n",
      "Epoch 126, Loss: 0.45696189549769234\n",
      "Epoch 127, Loss: 0.45594920341421274\n",
      "Epoch 128, Loss: 0.45536359747685007\n",
      "Epoch 129, Loss: 0.4553290890752543\n",
      "Epoch 130, Loss: 0.4547195823074053\n",
      "Epoch 131, Loss: 0.45392838323998064\n",
      "Epoch 132, Loss: 0.4537853493595631\n",
      "Epoch 133, Loss: 0.45347437050767975\n",
      "Epoch 134, Loss: 0.45267431273812453\n",
      "Epoch 135, Loss: 0.45228737659373064\n",
      "Epoch 136, Loss: 0.4522154657432981\n",
      "Epoch 137, Loss: 0.45165964487407706\n",
      "Epoch 138, Loss: 0.4510751265462795\n",
      "Epoch 139, Loss: 0.45082650445054473\n",
      "Epoch 140, Loss: 0.4506281098405297\n",
      "Epoch 141, Loss: 0.4501911419656675\n",
      "Epoch 142, Loss: 0.4497001887659326\n",
      "Epoch 143, Loss: 0.44943436133718806\n",
      "Epoch 144, Loss: 0.449226296402944\n",
      "Epoch 145, Loss: 0.44886073897867573\n",
      "Epoch 146, Loss: 0.44844710376304864\n",
      "Epoch 147, Loss: 0.44810507013116724\n",
      "Epoch 148, Loss: 0.44788545862049767\n",
      "Epoch 149, Loss: 0.4476741441009651\n",
      "Epoch 150, Loss: 0.44735861731902055\n",
      "Epoch 151, Loss: 0.4469288720157105\n",
      "Epoch 152, Loss: 0.44655235754178146\n",
      "Epoch 153, Loss: 0.4462857402786373\n",
      "Epoch 154, Loss: 0.4460517369142754\n",
      "Epoch 155, Loss: 0.4457881796184935\n",
      "Epoch 156, Loss: 0.44542210886348255\n",
      "Epoch 157, Loss: 0.4450579467827662\n",
      "Epoch 158, Loss: 0.44473329320345206\n",
      "Epoch 159, Loss: 0.4444482514678638\n",
      "Epoch 160, Loss: 0.4441745674630603\n",
      "Epoch 161, Loss: 0.4439221813722748\n",
      "Epoch 162, Loss: 0.44363357870363757\n",
      "Epoch 163, Loss: 0.4433189071134186\n",
      "Epoch 164, Loss: 0.44295452976444744\n",
      "Epoch 165, Loss: 0.4426058108229731\n",
      "Epoch 166, Loss: 0.4422485787900922\n",
      "Epoch 167, Loss: 0.4419090985167976\n",
      "Epoch 168, Loss: 0.44158054387831963\n",
      "Epoch 169, Loss: 0.4412474388772268\n",
      "Epoch 170, Loss: 0.4409277694136744\n",
      "Epoch 171, Loss: 0.44068045571539705\n",
      "Epoch 172, Loss: 0.4405979418732049\n",
      "Epoch 173, Loss: 0.44094101686109266\n",
      "Epoch 174, Loss: 0.4417368233339534\n",
      "Epoch 175, Loss: 0.4416582519101522\n",
      "Epoch 176, Loss: 0.4396788970821407\n",
      "Epoch 177, Loss: 0.4390195954561405\n",
      "Epoch 178, Loss: 0.4400830988007806\n",
      "Epoch 179, Loss: 0.43931775869233325\n",
      "Epoch 180, Loss: 0.4380032505977321\n",
      "Epoch 181, Loss: 0.4384470974340077\n",
      "Epoch 182, Loss: 0.4385167636581309\n",
      "Epoch 183, Loss: 0.43733043450040754\n",
      "Epoch 184, Loss: 0.4373176399006318\n",
      "Epoch 185, Loss: 0.4375256973621567\n",
      "Epoch 186, Loss: 0.43659322296038683\n",
      "Epoch 187, Loss: 0.43644391045908093\n",
      "Epoch 188, Loss: 0.43663844358015674\n",
      "Epoch 189, Loss: 0.43586286705793914\n",
      "Epoch 190, Loss: 0.4355663072844155\n",
      "Epoch 191, Loss: 0.4357040532214924\n",
      "Epoch 192, Loss: 0.435134007760397\n",
      "Epoch 193, Loss: 0.43474241127906293\n",
      "Epoch 194, Loss: 0.43480482310319635\n",
      "Epoch 195, Loss: 0.43443826104982264\n",
      "Epoch 196, Loss: 0.43390767873021246\n",
      "Epoch 197, Loss: 0.4338897691233524\n",
      "Epoch 198, Loss: 0.43377856877811044\n",
      "Epoch 199, Loss: 0.4331595506415052\n",
      "Epoch 200, Loss: 0.43289415091975497\n",
      "Epoch 201, Loss: 0.43294048069736957\n",
      "Epoch 202, Loss: 0.43262240576772243\n",
      "Epoch 203, Loss: 0.43206541812705346\n",
      "Epoch 204, Loss: 0.43172500614102416\n",
      "Epoch 205, Loss: 0.4316686388133938\n",
      "Epoch 206, Loss: 0.4315179158466364\n",
      "Epoch 207, Loss: 0.4310712381632941\n",
      "Epoch 208, Loss: 0.43059470035496\n",
      "Epoch 209, Loss: 0.4303711221416647\n",
      "Epoch 210, Loss: 0.43033088719595897\n",
      "Epoch 211, Loss: 0.43016580489992484\n",
      "Epoch 212, Loss: 0.429691817522032\n",
      "Epoch 213, Loss: 0.4292099292794459\n",
      "Epoch 214, Loss: 0.4289207205910941\n",
      "Epoch 215, Loss: 0.4287978238495365\n",
      "Epoch 216, Loss: 0.42870603658529494\n",
      "Epoch 217, Loss: 0.42841980940051866\n",
      "Epoch 218, Loss: 0.4278918633922788\n",
      "Epoch 219, Loss: 0.42748307113702383\n",
      "Epoch 220, Loss: 0.4271688511370021\n",
      "Epoch 221, Loss: 0.4269432345934848\n",
      "Epoch 222, Loss: 0.4267651275776002\n",
      "Epoch 223, Loss: 0.42668376428587257\n",
      "Epoch 224, Loss: 0.42653554195413756\n",
      "Epoch 225, Loss: 0.42613293183996703\n",
      "Epoch 226, Loss: 0.4255451632780942\n",
      "Epoch 227, Loss: 0.4251305043179084\n",
      "Epoch 228, Loss: 0.42475934718412794\n",
      "Epoch 229, Loss: 0.42452283256309487\n",
      "Epoch 230, Loss: 0.4244024016195508\n",
      "Epoch 231, Loss: 0.4242604946441205\n",
      "Epoch 232, Loss: 0.424134367950937\n",
      "Epoch 233, Loss: 0.42372608274856827\n",
      "Epoch 234, Loss: 0.4231492155631465\n",
      "Epoch 235, Loss: 0.4226291943984805\n",
      "Epoch 236, Loss: 0.4221723179942825\n",
      "Epoch 237, Loss: 0.4218955917984354\n",
      "Epoch 238, Loss: 0.4216767834434851\n",
      "Epoch 239, Loss: 0.42145169178243785\n",
      "Epoch 240, Loss: 0.4212605805865247\n",
      "Epoch 241, Loss: 0.42123380253494425\n",
      "Epoch 242, Loss: 0.42125483246926426\n",
      "Epoch 243, Loss: 0.42124070061060576\n",
      "Epoch 244, Loss: 0.42073760533336746\n",
      "Epoch 245, Loss: 0.42002867294257734\n",
      "Epoch 246, Loss: 0.41900436945460057\n",
      "Epoch 247, Loss: 0.41841940783018283\n",
      "Epoch 248, Loss: 0.4184544673382656\n",
      "Epoch 249, Loss: 0.4187823781263808\n",
      "Epoch 250, Loss: 0.41914970907160026\n",
      "Epoch 251, Loss: 0.418702737788617\n",
      "Epoch 252, Loss: 0.41762532180686956\n",
      "Epoch 253, Loss: 0.41665732452727816\n",
      "Epoch 254, Loss: 0.4164608109104539\n",
      "Epoch 255, Loss: 0.4169362050698102\n",
      "Epoch 256, Loss: 0.4170439641107097\n",
      "Epoch 257, Loss: 0.41618098523090385\n",
      "Epoch 258, Loss: 0.41521870035352226\n",
      "Epoch 259, Loss: 0.41507164322372797\n",
      "Epoch 260, Loss: 0.4154855816987037\n",
      "Epoch 261, Loss: 0.4152875627561342\n",
      "Epoch 262, Loss: 0.4146051629547779\n",
      "Epoch 263, Loss: 0.4138602536580612\n",
      "Epoch 264, Loss: 0.4135535008669373\n",
      "Epoch 265, Loss: 0.4136485063474015\n",
      "Epoch 266, Loss: 0.4135539332507534\n",
      "Epoch 267, Loss: 0.4133421191440525\n",
      "Epoch 268, Loss: 0.412685660282317\n",
      "Epoch 269, Loss: 0.4122655129807526\n",
      "Epoch 270, Loss: 0.4118217259860175\n",
      "Epoch 271, Loss: 0.41165256945543577\n",
      "Epoch 272, Loss: 0.41146929239414193\n",
      "Epoch 273, Loss: 0.4115028618809221\n",
      "Epoch 274, Loss: 0.4117686232412549\n",
      "Epoch 275, Loss: 0.4121111105932055\n",
      "Epoch 276, Loss: 0.4130580342565522\n",
      "Epoch 277, Loss: 0.4121949346593443\n",
      "Epoch 278, Loss: 0.41057389204015954\n",
      "Epoch 279, Loss: 0.4095484089667836\n",
      "Epoch 280, Loss: 0.4098107108594438\n",
      "Epoch 281, Loss: 0.41080122599126295\n",
      "Epoch 282, Loss: 0.41057013564631983\n",
      "Epoch 283, Loss: 0.4096005808315313\n",
      "Epoch 284, Loss: 0.4083534584470671\n",
      "Epoch 285, Loss: 0.4080981251863069\n",
      "Epoch 286, Loss: 0.40860646229911785\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.25185966848154834\n",
      "Valence RMSE: 0.26157630254980146\n",
      "Arousal RMSE: 0.24175281417109837\n",
      "Test R^2 score: tensor([0.2688, 0.5690], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4189087253069388\n",
      "Num of epochs: 287\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Epoch 121, Loss: 0.4596922103619757\n",
      "Epoch 122, Loss: 0.45943389956138125\n",
      "Epoch 123, Loss: 0.45877818894117006\n",
      "Epoch 124, Loss: 0.45746538132874076\n",
      "Epoch 125, Loss: 0.4570462803121619\n",
      "Epoch 126, Loss: 0.45696189549769234\n",
      "Epoch 127, Loss: 0.45594920341421274\n",
      "Epoch 128, Loss: 0.45536359747685007\n",
      "Epoch 129, Loss: 0.4553290890752543\n",
      "Epoch 130, Loss: 0.4547195823074053\n",
      "Epoch 131, Loss: 0.45392838323998064\n",
      "Epoch 132, Loss: 0.4537853493595631\n",
      "Epoch 133, Loss: 0.45347437050767975\n",
      "Epoch 134, Loss: 0.45267431273812453\n",
      "Epoch 135, Loss: 0.45228737659373064\n",
      "Epoch 136, Loss: 0.4522154657432981\n",
      "Epoch 137, Loss: 0.45165964487407706\n",
      "Epoch 138, Loss: 0.4510751265462795\n",
      "Epoch 139, Loss: 0.45082650445054473\n",
      "Epoch 140, Loss: 0.4506281098405297\n",
      "Epoch 141, Loss: 0.4501911419656675\n",
      "Epoch 142, Loss: 0.4497001887659326\n",
      "Epoch 143, Loss: 0.44943436133718806\n",
      "Epoch 144, Loss: 0.449226296402944\n",
      "Epoch 145, Loss: 0.44886073897867573\n",
      "Epoch 146, Loss: 0.44844710376304864\n",
      "Epoch 147, Loss: 0.44810507013116724\n",
      "Epoch 148, Loss: 0.44788545862049767\n",
      "Epoch 149, Loss: 0.4476741441009651\n",
      "Epoch 150, Loss: 0.44735861731902055\n",
      "Epoch 151, Loss: 0.4469288720157105\n",
      "Epoch 152, Loss: 0.44655235754178146\n",
      "Epoch 153, Loss: 0.4462857402786373\n",
      "Epoch 154, Loss: 0.4460517369142754\n",
      "Epoch 155, Loss: 0.4457881796184935\n",
      "Epoch 156, Loss: 0.44542210886348255\n",
      "Epoch 157, Loss: 0.4450579467827662\n",
      "Epoch 158, Loss: 0.44473329320345206\n",
      "Epoch 159, Loss: 0.4444482514678638\n",
      "Epoch 160, Loss: 0.4441745674630603\n",
      "Epoch 161, Loss: 0.4439221813722748\n",
      "Epoch 162, Loss: 0.44363357870363757\n",
      "Epoch 163, Loss: 0.4433189071134186\n",
      "Epoch 164, Loss: 0.44295452976444744\n",
      "Epoch 165, Loss: 0.4426058108229731\n",
      "Epoch 166, Loss: 0.4422485787900922\n",
      "Epoch 167, Loss: 0.4419090985167976\n",
      "Epoch 168, Loss: 0.44158054387831963\n",
      "Epoch 169, Loss: 0.4412474388772268\n",
      "Epoch 170, Loss: 0.4409277694136744\n",
      "Epoch 171, Loss: 0.44068045571539705\n",
      "Epoch 172, Loss: 0.4405979418732049\n",
      "Epoch 173, Loss: 0.44094101686109266\n",
      "Epoch 174, Loss: 0.4417368233339534\n",
      "Epoch 175, Loss: 0.4416582519101522\n",
      "Epoch 176, Loss: 0.4396788970821407\n",
      "Epoch 177, Loss: 0.4390195954561405\n",
      "Epoch 178, Loss: 0.4400830988007806\n",
      "Epoch 179, Loss: 0.43931775869233325\n",
      "Epoch 180, Loss: 0.4380032505977321\n",
      "Epoch 181, Loss: 0.4384470974340077\n",
      "Epoch 182, Loss: 0.4385167636581309\n",
      "Epoch 183, Loss: 0.43733043450040754\n",
      "Epoch 184, Loss: 0.4373176399006318\n",
      "Epoch 185, Loss: 0.4375256973621567\n",
      "Epoch 186, Loss: 0.43659322296038683\n",
      "Epoch 187, Loss: 0.43644391045908093\n",
      "Epoch 188, Loss: 0.43663844358015674\n",
      "Epoch 189, Loss: 0.43586286705793914\n",
      "Epoch 190, Loss: 0.4355663072844155\n",
      "Epoch 191, Loss: 0.4357040532214924\n",
      "Epoch 192, Loss: 0.435134007760397\n",
      "Epoch 193, Loss: 0.43474241127906293\n",
      "Epoch 194, Loss: 0.43480482310319635\n",
      "Epoch 195, Loss: 0.43443826104982264\n",
      "Epoch 196, Loss: 0.43390767873021246\n",
      "Epoch 197, Loss: 0.4338897691233524\n",
      "Epoch 198, Loss: 0.43377856877811044\n",
      "Epoch 199, Loss: 0.4331595506415052\n",
      "Epoch 200, Loss: 0.43289415091975497\n",
      "Epoch 201, Loss: 0.43294048069736957\n",
      "Epoch 202, Loss: 0.43262240576772243\n",
      "Epoch 203, Loss: 0.43206541812705346\n",
      "Epoch 204, Loss: 0.43172500614102416\n",
      "Epoch 205, Loss: 0.4316686388133938\n",
      "Epoch 206, Loss: 0.4315179158466364\n",
      "Epoch 207, Loss: 0.4310712381632941\n",
      "Epoch 208, Loss: 0.43059470035496\n",
      "Epoch 209, Loss: 0.4303711221416647\n",
      "Epoch 210, Loss: 0.43033088719595897\n",
      "Epoch 211, Loss: 0.43016580489992484\n",
      "Epoch 212, Loss: 0.429691817522032\n",
      "Epoch 213, Loss: 0.4292099292794459\n",
      "Epoch 214, Loss: 0.4289207205910941\n",
      "Epoch 215, Loss: 0.4287978238495365\n",
      "Epoch 216, Loss: 0.42870603658529494\n",
      "Epoch 217, Loss: 0.42841980940051866\n",
      "Epoch 218, Loss: 0.4278918633922788\n",
      "Epoch 219, Loss: 0.42748307113702383\n",
      "Epoch 220, Loss: 0.4271688511370021\n",
      "Epoch 221, Loss: 0.4269432345934848\n",
      "Epoch 222, Loss: 0.4267651275776002\n",
      "Epoch 223, Loss: 0.42668376428587257\n",
      "Epoch 224, Loss: 0.42653554195413756\n",
      "Epoch 225, Loss: 0.42613293183996703\n",
      "Epoch 226, Loss: 0.4255451632780942\n",
      "Epoch 227, Loss: 0.4251305043179084\n",
      "Epoch 228, Loss: 0.42475934718412794\n",
      "Epoch 229, Loss: 0.42452283256309487\n",
      "Epoch 230, Loss: 0.4244024016195508\n",
      "Epoch 231, Loss: 0.4242604946441205\n",
      "Epoch 232, Loss: 0.424134367950937\n",
      "Epoch 233, Loss: 0.42372608274856827\n",
      "Epoch 234, Loss: 0.4231492155631465\n",
      "Epoch 235, Loss: 0.4226291943984805\n",
      "Epoch 236, Loss: 0.4221723179942825\n",
      "Epoch 237, Loss: 0.4218955917984354\n",
      "Epoch 238, Loss: 0.4216767834434851\n",
      "Epoch 239, Loss: 0.42145169178243785\n",
      "Epoch 240, Loss: 0.4212605805865247\n",
      "Epoch 241, Loss: 0.42123380253494425\n",
      "Epoch 242, Loss: 0.42125483246926426\n",
      "Epoch 243, Loss: 0.42124070061060576\n",
      "Epoch 244, Loss: 0.42073760533336746\n",
      "Epoch 245, Loss: 0.42002867294257734\n",
      "Epoch 246, Loss: 0.41900436945460057\n",
      "Epoch 247, Loss: 0.41841940783018283\n",
      "Epoch 248, Loss: 0.4184544673382656\n",
      "Epoch 249, Loss: 0.4187823781263808\n",
      "Epoch 250, Loss: 0.41914970907160026\n",
      "Epoch 251, Loss: 0.418702737788617\n",
      "Epoch 252, Loss: 0.41762532180686956\n",
      "Epoch 253, Loss: 0.41665732452727816\n",
      "Epoch 254, Loss: 0.4164608109104539\n",
      "Epoch 255, Loss: 0.4169362050698102\n",
      "Epoch 256, Loss: 0.4170439641107097\n",
      "Epoch 257, Loss: 0.41618098523090385\n",
      "Epoch 258, Loss: 0.41521870035352226\n",
      "Epoch 259, Loss: 0.41507164322372797\n",
      "Epoch 260, Loss: 0.4154855816987037\n",
      "Epoch 261, Loss: 0.4152875627561342\n",
      "Epoch 262, Loss: 0.4146051629547779\n",
      "Epoch 263, Loss: 0.4138602536580612\n",
      "Epoch 264, Loss: 0.4135535008669373\n",
      "Epoch 265, Loss: 0.4136485063474015\n",
      "Epoch 266, Loss: 0.4135539332507534\n",
      "Epoch 267, Loss: 0.4133421191440525\n",
      "Epoch 268, Loss: 0.412685660282317\n",
      "Epoch 269, Loss: 0.4122655129807526\n",
      "Epoch 270, Loss: 0.4118217259860175\n",
      "Epoch 271, Loss: 0.41165256945543577\n",
      "Epoch 272, Loss: 0.41146929239414193\n",
      "Epoch 273, Loss: 0.4115028618809221\n",
      "Epoch 274, Loss: 0.4117686232412549\n",
      "Epoch 275, Loss: 0.4121111105932055\n",
      "Epoch 276, Loss: 0.4130580342565522\n",
      "Epoch 277, Loss: 0.4121949346593443\n",
      "Epoch 278, Loss: 0.41057389204015954\n",
      "Epoch 279, Loss: 0.4095484089667836\n",
      "Epoch 280, Loss: 0.4098107108594438\n",
      "Epoch 281, Loss: 0.41080122599126295\n",
      "Epoch 282, Loss: 0.41057013564631983\n",
      "Epoch 283, Loss: 0.4096005808315313\n",
      "Epoch 284, Loss: 0.4083534584470671\n",
      "Epoch 285, Loss: 0.4080981251863069\n",
      "Epoch 286, Loss: 0.40860646229911785\n",
      "Epoch 287, Loss: 0.40884595202638996\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2520829636490197\n",
      "Valence RMSE: 0.262562663951134\n",
      "Arousal RMSE: 0.2411482710347098\n",
      "Test R^2 score: tensor([0.2633, 0.5711], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.41722295509491675\n",
      "Num of epochs: 288\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Epoch 121, Loss: 0.4596922103619757\n",
      "Epoch 122, Loss: 0.45943389956138125\n",
      "Epoch 123, Loss: 0.45877818894117006\n",
      "Epoch 124, Loss: 0.45746538132874076\n",
      "Epoch 125, Loss: 0.4570462803121619\n",
      "Epoch 126, Loss: 0.45696189549769234\n",
      "Epoch 127, Loss: 0.45594920341421274\n",
      "Epoch 128, Loss: 0.45536359747685007\n",
      "Epoch 129, Loss: 0.4553290890752543\n",
      "Epoch 130, Loss: 0.4547195823074053\n",
      "Epoch 131, Loss: 0.45392838323998064\n",
      "Epoch 132, Loss: 0.4537853493595631\n",
      "Epoch 133, Loss: 0.45347437050767975\n",
      "Epoch 134, Loss: 0.45267431273812453\n",
      "Epoch 135, Loss: 0.45228737659373064\n",
      "Epoch 136, Loss: 0.4522154657432981\n",
      "Epoch 137, Loss: 0.45165964487407706\n",
      "Epoch 138, Loss: 0.4510751265462795\n",
      "Epoch 139, Loss: 0.45082650445054473\n",
      "Epoch 140, Loss: 0.4506281098405297\n",
      "Epoch 141, Loss: 0.4501911419656675\n",
      "Epoch 142, Loss: 0.4497001887659326\n",
      "Epoch 143, Loss: 0.44943436133718806\n",
      "Epoch 144, Loss: 0.449226296402944\n",
      "Epoch 145, Loss: 0.44886073897867573\n",
      "Epoch 146, Loss: 0.44844710376304864\n",
      "Epoch 147, Loss: 0.44810507013116724\n",
      "Epoch 148, Loss: 0.44788545862049767\n",
      "Epoch 149, Loss: 0.4476741441009651\n",
      "Epoch 150, Loss: 0.44735861731902055\n",
      "Epoch 151, Loss: 0.4469288720157105\n",
      "Epoch 152, Loss: 0.44655235754178146\n",
      "Epoch 153, Loss: 0.4462857402786373\n",
      "Epoch 154, Loss: 0.4460517369142754\n",
      "Epoch 155, Loss: 0.4457881796184935\n",
      "Epoch 156, Loss: 0.44542210886348255\n",
      "Epoch 157, Loss: 0.4450579467827662\n",
      "Epoch 158, Loss: 0.44473329320345206\n",
      "Epoch 159, Loss: 0.4444482514678638\n",
      "Epoch 160, Loss: 0.4441745674630603\n",
      "Epoch 161, Loss: 0.4439221813722748\n",
      "Epoch 162, Loss: 0.44363357870363757\n",
      "Epoch 163, Loss: 0.4433189071134186\n",
      "Epoch 164, Loss: 0.44295452976444744\n",
      "Epoch 165, Loss: 0.4426058108229731\n",
      "Epoch 166, Loss: 0.4422485787900922\n",
      "Epoch 167, Loss: 0.4419090985167976\n",
      "Epoch 168, Loss: 0.44158054387831963\n",
      "Epoch 169, Loss: 0.4412474388772268\n",
      "Epoch 170, Loss: 0.4409277694136744\n",
      "Epoch 171, Loss: 0.44068045571539705\n",
      "Epoch 172, Loss: 0.4405979418732049\n",
      "Epoch 173, Loss: 0.44094101686109266\n",
      "Epoch 174, Loss: 0.4417368233339534\n",
      "Epoch 175, Loss: 0.4416582519101522\n",
      "Epoch 176, Loss: 0.4396788970821407\n",
      "Epoch 177, Loss: 0.4390195954561405\n",
      "Epoch 178, Loss: 0.4400830988007806\n",
      "Epoch 179, Loss: 0.43931775869233325\n",
      "Epoch 180, Loss: 0.4380032505977321\n",
      "Epoch 181, Loss: 0.4384470974340077\n",
      "Epoch 182, Loss: 0.4385167636581309\n",
      "Epoch 183, Loss: 0.43733043450040754\n",
      "Epoch 184, Loss: 0.4373176399006318\n",
      "Epoch 185, Loss: 0.4375256973621567\n",
      "Epoch 186, Loss: 0.43659322296038683\n",
      "Epoch 187, Loss: 0.43644391045908093\n",
      "Epoch 188, Loss: 0.43663844358015674\n",
      "Epoch 189, Loss: 0.43586286705793914\n",
      "Epoch 190, Loss: 0.4355663072844155\n",
      "Epoch 191, Loss: 0.4357040532214924\n",
      "Epoch 192, Loss: 0.435134007760397\n",
      "Epoch 193, Loss: 0.43474241127906293\n",
      "Epoch 194, Loss: 0.43480482310319635\n",
      "Epoch 195, Loss: 0.43443826104982264\n",
      "Epoch 196, Loss: 0.43390767873021246\n",
      "Epoch 197, Loss: 0.4338897691233524\n",
      "Epoch 198, Loss: 0.43377856877811044\n",
      "Epoch 199, Loss: 0.4331595506415052\n",
      "Epoch 200, Loss: 0.43289415091975497\n",
      "Epoch 201, Loss: 0.43294048069736957\n",
      "Epoch 202, Loss: 0.43262240576772243\n",
      "Epoch 203, Loss: 0.43206541812705346\n",
      "Epoch 204, Loss: 0.43172500614102416\n",
      "Epoch 205, Loss: 0.4316686388133938\n",
      "Epoch 206, Loss: 0.4315179158466364\n",
      "Epoch 207, Loss: 0.4310712381632941\n",
      "Epoch 208, Loss: 0.43059470035496\n",
      "Epoch 209, Loss: 0.4303711221416647\n",
      "Epoch 210, Loss: 0.43033088719595897\n",
      "Epoch 211, Loss: 0.43016580489992484\n",
      "Epoch 212, Loss: 0.429691817522032\n",
      "Epoch 213, Loss: 0.4292099292794459\n",
      "Epoch 214, Loss: 0.4289207205910941\n",
      "Epoch 215, Loss: 0.4287978238495365\n",
      "Epoch 216, Loss: 0.42870603658529494\n",
      "Epoch 217, Loss: 0.42841980940051866\n",
      "Epoch 218, Loss: 0.4278918633922788\n",
      "Epoch 219, Loss: 0.42748307113702383\n",
      "Epoch 220, Loss: 0.4271688511370021\n",
      "Epoch 221, Loss: 0.4269432345934848\n",
      "Epoch 222, Loss: 0.4267651275776002\n",
      "Epoch 223, Loss: 0.42668376428587257\n",
      "Epoch 224, Loss: 0.42653554195413756\n",
      "Epoch 225, Loss: 0.42613293183996703\n",
      "Epoch 226, Loss: 0.4255451632780942\n",
      "Epoch 227, Loss: 0.4251305043179084\n",
      "Epoch 228, Loss: 0.42475934718412794\n",
      "Epoch 229, Loss: 0.42452283256309487\n",
      "Epoch 230, Loss: 0.4244024016195508\n",
      "Epoch 231, Loss: 0.4242604946441205\n",
      "Epoch 232, Loss: 0.424134367950937\n",
      "Epoch 233, Loss: 0.42372608274856827\n",
      "Epoch 234, Loss: 0.4231492155631465\n",
      "Epoch 235, Loss: 0.4226291943984805\n",
      "Epoch 236, Loss: 0.4221723179942825\n",
      "Epoch 237, Loss: 0.4218955917984354\n",
      "Epoch 238, Loss: 0.4216767834434851\n",
      "Epoch 239, Loss: 0.42145169178243785\n",
      "Epoch 240, Loss: 0.4212605805865247\n",
      "Epoch 241, Loss: 0.42123380253494425\n",
      "Epoch 242, Loss: 0.42125483246926426\n",
      "Epoch 243, Loss: 0.42124070061060576\n",
      "Epoch 244, Loss: 0.42073760533336746\n",
      "Epoch 245, Loss: 0.42002867294257734\n",
      "Epoch 246, Loss: 0.41900436945460057\n",
      "Epoch 247, Loss: 0.41841940783018283\n",
      "Epoch 248, Loss: 0.4184544673382656\n",
      "Epoch 249, Loss: 0.4187823781263808\n",
      "Epoch 250, Loss: 0.41914970907160026\n",
      "Epoch 251, Loss: 0.418702737788617\n",
      "Epoch 252, Loss: 0.41762532180686956\n",
      "Epoch 253, Loss: 0.41665732452727816\n",
      "Epoch 254, Loss: 0.4164608109104539\n",
      "Epoch 255, Loss: 0.4169362050698102\n",
      "Epoch 256, Loss: 0.4170439641107097\n",
      "Epoch 257, Loss: 0.41618098523090385\n",
      "Epoch 258, Loss: 0.41521870035352226\n",
      "Epoch 259, Loss: 0.41507164322372797\n",
      "Epoch 260, Loss: 0.4154855816987037\n",
      "Epoch 261, Loss: 0.4152875627561342\n",
      "Epoch 262, Loss: 0.4146051629547779\n",
      "Epoch 263, Loss: 0.4138602536580612\n",
      "Epoch 264, Loss: 0.4135535008669373\n",
      "Epoch 265, Loss: 0.4136485063474015\n",
      "Epoch 266, Loss: 0.4135539332507534\n",
      "Epoch 267, Loss: 0.4133421191440525\n",
      "Epoch 268, Loss: 0.412685660282317\n",
      "Epoch 269, Loss: 0.4122655129807526\n",
      "Epoch 270, Loss: 0.4118217259860175\n",
      "Epoch 271, Loss: 0.41165256945543577\n",
      "Epoch 272, Loss: 0.41146929239414193\n",
      "Epoch 273, Loss: 0.4115028618809221\n",
      "Epoch 274, Loss: 0.4117686232412549\n",
      "Epoch 275, Loss: 0.4121111105932055\n",
      "Epoch 276, Loss: 0.4130580342565522\n",
      "Epoch 277, Loss: 0.4121949346593443\n",
      "Epoch 278, Loss: 0.41057389204015954\n",
      "Epoch 279, Loss: 0.4095484089667836\n",
      "Epoch 280, Loss: 0.4098107108594438\n",
      "Epoch 281, Loss: 0.41080122599126295\n",
      "Epoch 282, Loss: 0.41057013564631983\n",
      "Epoch 283, Loss: 0.4096005808315313\n",
      "Epoch 284, Loss: 0.4083534584470671\n",
      "Epoch 285, Loss: 0.4080981251863069\n",
      "Epoch 286, Loss: 0.40860646229911785\n",
      "Epoch 287, Loss: 0.40884595202638996\n",
      "Epoch 288, Loss: 0.40838367174584433\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.25159767585741555\n",
      "Valence RMSE: 0.26112269749124667\n",
      "Arousal RMSE: 0.24169757518146756\n",
      "Test R^2 score: tensor([0.2714, 0.5692], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4202740167959192\n",
      "Num of epochs: 289\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Epoch 121, Loss: 0.4596922103619757\n",
      "Epoch 122, Loss: 0.45943389956138125\n",
      "Epoch 123, Loss: 0.45877818894117006\n",
      "Epoch 124, Loss: 0.45746538132874076\n",
      "Epoch 125, Loss: 0.4570462803121619\n",
      "Epoch 126, Loss: 0.45696189549769234\n",
      "Epoch 127, Loss: 0.45594920341421274\n",
      "Epoch 128, Loss: 0.45536359747685007\n",
      "Epoch 129, Loss: 0.4553290890752543\n",
      "Epoch 130, Loss: 0.4547195823074053\n",
      "Epoch 131, Loss: 0.45392838323998064\n",
      "Epoch 132, Loss: 0.4537853493595631\n",
      "Epoch 133, Loss: 0.45347437050767975\n",
      "Epoch 134, Loss: 0.45267431273812453\n",
      "Epoch 135, Loss: 0.45228737659373064\n",
      "Epoch 136, Loss: 0.4522154657432981\n",
      "Epoch 137, Loss: 0.45165964487407706\n",
      "Epoch 138, Loss: 0.4510751265462795\n",
      "Epoch 139, Loss: 0.45082650445054473\n",
      "Epoch 140, Loss: 0.4506281098405297\n",
      "Epoch 141, Loss: 0.4501911419656675\n",
      "Epoch 142, Loss: 0.4497001887659326\n",
      "Epoch 143, Loss: 0.44943436133718806\n",
      "Epoch 144, Loss: 0.449226296402944\n",
      "Epoch 145, Loss: 0.44886073897867573\n",
      "Epoch 146, Loss: 0.44844710376304864\n",
      "Epoch 147, Loss: 0.44810507013116724\n",
      "Epoch 148, Loss: 0.44788545862049767\n",
      "Epoch 149, Loss: 0.4476741441009651\n",
      "Epoch 150, Loss: 0.44735861731902055\n",
      "Epoch 151, Loss: 0.4469288720157105\n",
      "Epoch 152, Loss: 0.44655235754178146\n",
      "Epoch 153, Loss: 0.4462857402786373\n",
      "Epoch 154, Loss: 0.4460517369142754\n",
      "Epoch 155, Loss: 0.4457881796184935\n",
      "Epoch 156, Loss: 0.44542210886348255\n",
      "Epoch 157, Loss: 0.4450579467827662\n",
      "Epoch 158, Loss: 0.44473329320345206\n",
      "Epoch 159, Loss: 0.4444482514678638\n",
      "Epoch 160, Loss: 0.4441745674630603\n",
      "Epoch 161, Loss: 0.4439221813722748\n",
      "Epoch 162, Loss: 0.44363357870363757\n",
      "Epoch 163, Loss: 0.4433189071134186\n",
      "Epoch 164, Loss: 0.44295452976444744\n",
      "Epoch 165, Loss: 0.4426058108229731\n",
      "Epoch 166, Loss: 0.4422485787900922\n",
      "Epoch 167, Loss: 0.4419090985167976\n",
      "Epoch 168, Loss: 0.44158054387831963\n",
      "Epoch 169, Loss: 0.4412474388772268\n",
      "Epoch 170, Loss: 0.4409277694136744\n",
      "Epoch 171, Loss: 0.44068045571539705\n",
      "Epoch 172, Loss: 0.4405979418732049\n",
      "Epoch 173, Loss: 0.44094101686109266\n",
      "Epoch 174, Loss: 0.4417368233339534\n",
      "Epoch 175, Loss: 0.4416582519101522\n",
      "Epoch 176, Loss: 0.4396788970821407\n",
      "Epoch 177, Loss: 0.4390195954561405\n",
      "Epoch 178, Loss: 0.4400830988007806\n",
      "Epoch 179, Loss: 0.43931775869233325\n",
      "Epoch 180, Loss: 0.4380032505977321\n",
      "Epoch 181, Loss: 0.4384470974340077\n",
      "Epoch 182, Loss: 0.4385167636581309\n",
      "Epoch 183, Loss: 0.43733043450040754\n",
      "Epoch 184, Loss: 0.4373176399006318\n",
      "Epoch 185, Loss: 0.4375256973621567\n",
      "Epoch 186, Loss: 0.43659322296038683\n",
      "Epoch 187, Loss: 0.43644391045908093\n",
      "Epoch 188, Loss: 0.43663844358015674\n",
      "Epoch 189, Loss: 0.43586286705793914\n",
      "Epoch 190, Loss: 0.4355663072844155\n",
      "Epoch 191, Loss: 0.4357040532214924\n",
      "Epoch 192, Loss: 0.435134007760397\n",
      "Epoch 193, Loss: 0.43474241127906293\n",
      "Epoch 194, Loss: 0.43480482310319635\n",
      "Epoch 195, Loss: 0.43443826104982264\n",
      "Epoch 196, Loss: 0.43390767873021246\n",
      "Epoch 197, Loss: 0.4338897691233524\n",
      "Epoch 198, Loss: 0.43377856877811044\n",
      "Epoch 199, Loss: 0.4331595506415052\n",
      "Epoch 200, Loss: 0.43289415091975497\n",
      "Epoch 201, Loss: 0.43294048069736957\n",
      "Epoch 202, Loss: 0.43262240576772243\n",
      "Epoch 203, Loss: 0.43206541812705346\n",
      "Epoch 204, Loss: 0.43172500614102416\n",
      "Epoch 205, Loss: 0.4316686388133938\n",
      "Epoch 206, Loss: 0.4315179158466364\n",
      "Epoch 207, Loss: 0.4310712381632941\n",
      "Epoch 208, Loss: 0.43059470035496\n",
      "Epoch 209, Loss: 0.4303711221416647\n",
      "Epoch 210, Loss: 0.43033088719595897\n",
      "Epoch 211, Loss: 0.43016580489992484\n",
      "Epoch 212, Loss: 0.429691817522032\n",
      "Epoch 213, Loss: 0.4292099292794459\n",
      "Epoch 214, Loss: 0.4289207205910941\n",
      "Epoch 215, Loss: 0.4287978238495365\n",
      "Epoch 216, Loss: 0.42870603658529494\n",
      "Epoch 217, Loss: 0.42841980940051866\n",
      "Epoch 218, Loss: 0.4278918633922788\n",
      "Epoch 219, Loss: 0.42748307113702383\n",
      "Epoch 220, Loss: 0.4271688511370021\n",
      "Epoch 221, Loss: 0.4269432345934848\n",
      "Epoch 222, Loss: 0.4267651275776002\n",
      "Epoch 223, Loss: 0.42668376428587257\n",
      "Epoch 224, Loss: 0.42653554195413756\n",
      "Epoch 225, Loss: 0.42613293183996703\n",
      "Epoch 226, Loss: 0.4255451632780942\n",
      "Epoch 227, Loss: 0.4251305043179084\n",
      "Epoch 228, Loss: 0.42475934718412794\n",
      "Epoch 229, Loss: 0.42452283256309487\n",
      "Epoch 230, Loss: 0.4244024016195508\n",
      "Epoch 231, Loss: 0.4242604946441205\n",
      "Epoch 232, Loss: 0.424134367950937\n",
      "Epoch 233, Loss: 0.42372608274856827\n",
      "Epoch 234, Loss: 0.4231492155631465\n",
      "Epoch 235, Loss: 0.4226291943984805\n",
      "Epoch 236, Loss: 0.4221723179942825\n",
      "Epoch 237, Loss: 0.4218955917984354\n",
      "Epoch 238, Loss: 0.4216767834434851\n",
      "Epoch 239, Loss: 0.42145169178243785\n",
      "Epoch 240, Loss: 0.4212605805865247\n",
      "Epoch 241, Loss: 0.42123380253494425\n",
      "Epoch 242, Loss: 0.42125483246926426\n",
      "Epoch 243, Loss: 0.42124070061060576\n",
      "Epoch 244, Loss: 0.42073760533336746\n",
      "Epoch 245, Loss: 0.42002867294257734\n",
      "Epoch 246, Loss: 0.41900436945460057\n",
      "Epoch 247, Loss: 0.41841940783018283\n",
      "Epoch 248, Loss: 0.4184544673382656\n",
      "Epoch 249, Loss: 0.4187823781263808\n",
      "Epoch 250, Loss: 0.41914970907160026\n",
      "Epoch 251, Loss: 0.418702737788617\n",
      "Epoch 252, Loss: 0.41762532180686956\n",
      "Epoch 253, Loss: 0.41665732452727816\n",
      "Epoch 254, Loss: 0.4164608109104539\n",
      "Epoch 255, Loss: 0.4169362050698102\n",
      "Epoch 256, Loss: 0.4170439641107097\n",
      "Epoch 257, Loss: 0.41618098523090385\n",
      "Epoch 258, Loss: 0.41521870035352226\n",
      "Epoch 259, Loss: 0.41507164322372797\n",
      "Epoch 260, Loss: 0.4154855816987037\n",
      "Epoch 261, Loss: 0.4152875627561342\n",
      "Epoch 262, Loss: 0.4146051629547779\n",
      "Epoch 263, Loss: 0.4138602536580612\n",
      "Epoch 264, Loss: 0.4135535008669373\n",
      "Epoch 265, Loss: 0.4136485063474015\n",
      "Epoch 266, Loss: 0.4135539332507534\n",
      "Epoch 267, Loss: 0.4133421191440525\n",
      "Epoch 268, Loss: 0.412685660282317\n",
      "Epoch 269, Loss: 0.4122655129807526\n",
      "Epoch 270, Loss: 0.4118217259860175\n",
      "Epoch 271, Loss: 0.41165256945543577\n",
      "Epoch 272, Loss: 0.41146929239414193\n",
      "Epoch 273, Loss: 0.4115028618809221\n",
      "Epoch 274, Loss: 0.4117686232412549\n",
      "Epoch 275, Loss: 0.4121111105932055\n",
      "Epoch 276, Loss: 0.4130580342565522\n",
      "Epoch 277, Loss: 0.4121949346593443\n",
      "Epoch 278, Loss: 0.41057389204015954\n",
      "Epoch 279, Loss: 0.4095484089667836\n",
      "Epoch 280, Loss: 0.4098107108594438\n",
      "Epoch 281, Loss: 0.41080122599126295\n",
      "Epoch 282, Loss: 0.41057013564631983\n",
      "Epoch 283, Loss: 0.4096005808315313\n",
      "Epoch 284, Loss: 0.4083534584470671\n",
      "Epoch 285, Loss: 0.4080981251863069\n",
      "Epoch 286, Loss: 0.40860646229911785\n",
      "Epoch 287, Loss: 0.40884595202638996\n",
      "Epoch 288, Loss: 0.40838367174584433\n",
      "Epoch 289, Loss: 0.40727524500698936\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.25223291602093506\n",
      "Valence RMSE: 0.26227878109764247\n",
      "Arousal RMSE: 0.24176999159280102\n",
      "Test R^2 score: tensor([0.2649, 0.5689], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4169118956348007\n",
      "Num of epochs: 290\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Epoch 121, Loss: 0.4596922103619757\n",
      "Epoch 122, Loss: 0.45943389956138125\n",
      "Epoch 123, Loss: 0.45877818894117006\n",
      "Epoch 124, Loss: 0.45746538132874076\n",
      "Epoch 125, Loss: 0.4570462803121619\n",
      "Epoch 126, Loss: 0.45696189549769234\n",
      "Epoch 127, Loss: 0.45594920341421274\n",
      "Epoch 128, Loss: 0.45536359747685007\n",
      "Epoch 129, Loss: 0.4553290890752543\n",
      "Epoch 130, Loss: 0.4547195823074053\n",
      "Epoch 131, Loss: 0.45392838323998064\n",
      "Epoch 132, Loss: 0.4537853493595631\n",
      "Epoch 133, Loss: 0.45347437050767975\n",
      "Epoch 134, Loss: 0.45267431273812453\n",
      "Epoch 135, Loss: 0.45228737659373064\n",
      "Epoch 136, Loss: 0.4522154657432981\n",
      "Epoch 137, Loss: 0.45165964487407706\n",
      "Epoch 138, Loss: 0.4510751265462795\n",
      "Epoch 139, Loss: 0.45082650445054473\n",
      "Epoch 140, Loss: 0.4506281098405297\n",
      "Epoch 141, Loss: 0.4501911419656675\n",
      "Epoch 142, Loss: 0.4497001887659326\n",
      "Epoch 143, Loss: 0.44943436133718806\n",
      "Epoch 144, Loss: 0.449226296402944\n",
      "Epoch 145, Loss: 0.44886073897867573\n",
      "Epoch 146, Loss: 0.44844710376304864\n",
      "Epoch 147, Loss: 0.44810507013116724\n",
      "Epoch 148, Loss: 0.44788545862049767\n",
      "Epoch 149, Loss: 0.4476741441009651\n",
      "Epoch 150, Loss: 0.44735861731902055\n",
      "Epoch 151, Loss: 0.4469288720157105\n",
      "Epoch 152, Loss: 0.44655235754178146\n",
      "Epoch 153, Loss: 0.4462857402786373\n",
      "Epoch 154, Loss: 0.4460517369142754\n",
      "Epoch 155, Loss: 0.4457881796184935\n",
      "Epoch 156, Loss: 0.44542210886348255\n",
      "Epoch 157, Loss: 0.4450579467827662\n",
      "Epoch 158, Loss: 0.44473329320345206\n",
      "Epoch 159, Loss: 0.4444482514678638\n",
      "Epoch 160, Loss: 0.4441745674630603\n",
      "Epoch 161, Loss: 0.4439221813722748\n",
      "Epoch 162, Loss: 0.44363357870363757\n",
      "Epoch 163, Loss: 0.4433189071134186\n",
      "Epoch 164, Loss: 0.44295452976444744\n",
      "Epoch 165, Loss: 0.4426058108229731\n",
      "Epoch 166, Loss: 0.4422485787900922\n",
      "Epoch 167, Loss: 0.4419090985167976\n",
      "Epoch 168, Loss: 0.44158054387831963\n",
      "Epoch 169, Loss: 0.4412474388772268\n",
      "Epoch 170, Loss: 0.4409277694136744\n",
      "Epoch 171, Loss: 0.44068045571539705\n",
      "Epoch 172, Loss: 0.4405979418732049\n",
      "Epoch 173, Loss: 0.44094101686109266\n",
      "Epoch 174, Loss: 0.4417368233339534\n",
      "Epoch 175, Loss: 0.4416582519101522\n",
      "Epoch 176, Loss: 0.4396788970821407\n",
      "Epoch 177, Loss: 0.4390195954561405\n",
      "Epoch 178, Loss: 0.4400830988007806\n",
      "Epoch 179, Loss: 0.43931775869233325\n",
      "Epoch 180, Loss: 0.4380032505977321\n",
      "Epoch 181, Loss: 0.4384470974340077\n",
      "Epoch 182, Loss: 0.4385167636581309\n",
      "Epoch 183, Loss: 0.43733043450040754\n",
      "Epoch 184, Loss: 0.4373176399006318\n",
      "Epoch 185, Loss: 0.4375256973621567\n",
      "Epoch 186, Loss: 0.43659322296038683\n",
      "Epoch 187, Loss: 0.43644391045908093\n",
      "Epoch 188, Loss: 0.43663844358015674\n",
      "Epoch 189, Loss: 0.43586286705793914\n",
      "Epoch 190, Loss: 0.4355663072844155\n",
      "Epoch 191, Loss: 0.4357040532214924\n",
      "Epoch 192, Loss: 0.435134007760397\n",
      "Epoch 193, Loss: 0.43474241127906293\n",
      "Epoch 194, Loss: 0.43480482310319635\n",
      "Epoch 195, Loss: 0.43443826104982264\n",
      "Epoch 196, Loss: 0.43390767873021246\n",
      "Epoch 197, Loss: 0.4338897691233524\n",
      "Epoch 198, Loss: 0.43377856877811044\n",
      "Epoch 199, Loss: 0.4331595506415052\n",
      "Epoch 200, Loss: 0.43289415091975497\n",
      "Epoch 201, Loss: 0.43294048069736957\n",
      "Epoch 202, Loss: 0.43262240576772243\n",
      "Epoch 203, Loss: 0.43206541812705346\n",
      "Epoch 204, Loss: 0.43172500614102416\n",
      "Epoch 205, Loss: 0.4316686388133938\n",
      "Epoch 206, Loss: 0.4315179158466364\n",
      "Epoch 207, Loss: 0.4310712381632941\n",
      "Epoch 208, Loss: 0.43059470035496\n",
      "Epoch 209, Loss: 0.4303711221416647\n",
      "Epoch 210, Loss: 0.43033088719595897\n",
      "Epoch 211, Loss: 0.43016580489992484\n",
      "Epoch 212, Loss: 0.429691817522032\n",
      "Epoch 213, Loss: 0.4292099292794459\n",
      "Epoch 214, Loss: 0.4289207205910941\n",
      "Epoch 215, Loss: 0.4287978238495365\n",
      "Epoch 216, Loss: 0.42870603658529494\n",
      "Epoch 217, Loss: 0.42841980940051866\n",
      "Epoch 218, Loss: 0.4278918633922788\n",
      "Epoch 219, Loss: 0.42748307113702383\n",
      "Epoch 220, Loss: 0.4271688511370021\n",
      "Epoch 221, Loss: 0.4269432345934848\n",
      "Epoch 222, Loss: 0.4267651275776002\n",
      "Epoch 223, Loss: 0.42668376428587257\n",
      "Epoch 224, Loss: 0.42653554195413756\n",
      "Epoch 225, Loss: 0.42613293183996703\n",
      "Epoch 226, Loss: 0.4255451632780942\n",
      "Epoch 227, Loss: 0.4251305043179084\n",
      "Epoch 228, Loss: 0.42475934718412794\n",
      "Epoch 229, Loss: 0.42452283256309487\n",
      "Epoch 230, Loss: 0.4244024016195508\n",
      "Epoch 231, Loss: 0.4242604946441205\n",
      "Epoch 232, Loss: 0.424134367950937\n",
      "Epoch 233, Loss: 0.42372608274856827\n",
      "Epoch 234, Loss: 0.4231492155631465\n",
      "Epoch 235, Loss: 0.4226291943984805\n",
      "Epoch 236, Loss: 0.4221723179942825\n",
      "Epoch 237, Loss: 0.4218955917984354\n",
      "Epoch 238, Loss: 0.4216767834434851\n",
      "Epoch 239, Loss: 0.42145169178243785\n",
      "Epoch 240, Loss: 0.4212605805865247\n",
      "Epoch 241, Loss: 0.42123380253494425\n",
      "Epoch 242, Loss: 0.42125483246926426\n",
      "Epoch 243, Loss: 0.42124070061060576\n",
      "Epoch 244, Loss: 0.42073760533336746\n",
      "Epoch 245, Loss: 0.42002867294257734\n",
      "Epoch 246, Loss: 0.41900436945460057\n",
      "Epoch 247, Loss: 0.41841940783018283\n",
      "Epoch 248, Loss: 0.4184544673382656\n",
      "Epoch 249, Loss: 0.4187823781263808\n",
      "Epoch 250, Loss: 0.41914970907160026\n",
      "Epoch 251, Loss: 0.418702737788617\n",
      "Epoch 252, Loss: 0.41762532180686956\n",
      "Epoch 253, Loss: 0.41665732452727816\n",
      "Epoch 254, Loss: 0.4164608109104539\n",
      "Epoch 255, Loss: 0.4169362050698102\n",
      "Epoch 256, Loss: 0.4170439641107097\n",
      "Epoch 257, Loss: 0.41618098523090385\n",
      "Epoch 258, Loss: 0.41521870035352226\n",
      "Epoch 259, Loss: 0.41507164322372797\n",
      "Epoch 260, Loss: 0.4154855816987037\n",
      "Epoch 261, Loss: 0.4152875627561342\n",
      "Epoch 262, Loss: 0.4146051629547779\n",
      "Epoch 263, Loss: 0.4138602536580612\n",
      "Epoch 264, Loss: 0.4135535008669373\n",
      "Epoch 265, Loss: 0.4136485063474015\n",
      "Epoch 266, Loss: 0.4135539332507534\n",
      "Epoch 267, Loss: 0.4133421191440525\n",
      "Epoch 268, Loss: 0.412685660282317\n",
      "Epoch 269, Loss: 0.4122655129807526\n",
      "Epoch 270, Loss: 0.4118217259860175\n",
      "Epoch 271, Loss: 0.41165256945543577\n",
      "Epoch 272, Loss: 0.41146929239414193\n",
      "Epoch 273, Loss: 0.4115028618809221\n",
      "Epoch 274, Loss: 0.4117686232412549\n",
      "Epoch 275, Loss: 0.4121111105932055\n",
      "Epoch 276, Loss: 0.4130580342565522\n",
      "Epoch 277, Loss: 0.4121949346593443\n",
      "Epoch 278, Loss: 0.41057389204015954\n",
      "Epoch 279, Loss: 0.4095484089667836\n",
      "Epoch 280, Loss: 0.4098107108594438\n",
      "Epoch 281, Loss: 0.41080122599126295\n",
      "Epoch 282, Loss: 0.41057013564631983\n",
      "Epoch 283, Loss: 0.4096005808315313\n",
      "Epoch 284, Loss: 0.4083534584470671\n",
      "Epoch 285, Loss: 0.4080981251863069\n",
      "Epoch 286, Loss: 0.40860646229911785\n",
      "Epoch 287, Loss: 0.40884595202638996\n",
      "Epoch 288, Loss: 0.40838367174584433\n",
      "Epoch 289, Loss: 0.40727524500698936\n",
      "Epoch 290, Loss: 0.4066578761535602\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.25270904826402146\n",
      "Valence RMSE: 0.2631671017547708\n",
      "Arousal RMSE: 0.24179909574481284\n",
      "Test R^2 score: tensor([0.2599, 0.5688], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4143660826226459\n",
      "Num of epochs: 291\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Epoch 121, Loss: 0.4596922103619757\n",
      "Epoch 122, Loss: 0.45943389956138125\n",
      "Epoch 123, Loss: 0.45877818894117006\n",
      "Epoch 124, Loss: 0.45746538132874076\n",
      "Epoch 125, Loss: 0.4570462803121619\n",
      "Epoch 126, Loss: 0.45696189549769234\n",
      "Epoch 127, Loss: 0.45594920341421274\n",
      "Epoch 128, Loss: 0.45536359747685007\n",
      "Epoch 129, Loss: 0.4553290890752543\n",
      "Epoch 130, Loss: 0.4547195823074053\n",
      "Epoch 131, Loss: 0.45392838323998064\n",
      "Epoch 132, Loss: 0.4537853493595631\n",
      "Epoch 133, Loss: 0.45347437050767975\n",
      "Epoch 134, Loss: 0.45267431273812453\n",
      "Epoch 135, Loss: 0.45228737659373064\n",
      "Epoch 136, Loss: 0.4522154657432981\n",
      "Epoch 137, Loss: 0.45165964487407706\n",
      "Epoch 138, Loss: 0.4510751265462795\n",
      "Epoch 139, Loss: 0.45082650445054473\n",
      "Epoch 140, Loss: 0.4506281098405297\n",
      "Epoch 141, Loss: 0.4501911419656675\n",
      "Epoch 142, Loss: 0.4497001887659326\n",
      "Epoch 143, Loss: 0.44943436133718806\n",
      "Epoch 144, Loss: 0.449226296402944\n",
      "Epoch 145, Loss: 0.44886073897867573\n",
      "Epoch 146, Loss: 0.44844710376304864\n",
      "Epoch 147, Loss: 0.44810507013116724\n",
      "Epoch 148, Loss: 0.44788545862049767\n",
      "Epoch 149, Loss: 0.4476741441009651\n",
      "Epoch 150, Loss: 0.44735861731902055\n",
      "Epoch 151, Loss: 0.4469288720157105\n",
      "Epoch 152, Loss: 0.44655235754178146\n",
      "Epoch 153, Loss: 0.4462857402786373\n",
      "Epoch 154, Loss: 0.4460517369142754\n",
      "Epoch 155, Loss: 0.4457881796184935\n",
      "Epoch 156, Loss: 0.44542210886348255\n",
      "Epoch 157, Loss: 0.4450579467827662\n",
      "Epoch 158, Loss: 0.44473329320345206\n",
      "Epoch 159, Loss: 0.4444482514678638\n",
      "Epoch 160, Loss: 0.4441745674630603\n",
      "Epoch 161, Loss: 0.4439221813722748\n",
      "Epoch 162, Loss: 0.44363357870363757\n",
      "Epoch 163, Loss: 0.4433189071134186\n",
      "Epoch 164, Loss: 0.44295452976444744\n",
      "Epoch 165, Loss: 0.4426058108229731\n",
      "Epoch 166, Loss: 0.4422485787900922\n",
      "Epoch 167, Loss: 0.4419090985167976\n",
      "Epoch 168, Loss: 0.44158054387831963\n",
      "Epoch 169, Loss: 0.4412474388772268\n",
      "Epoch 170, Loss: 0.4409277694136744\n",
      "Epoch 171, Loss: 0.44068045571539705\n",
      "Epoch 172, Loss: 0.4405979418732049\n",
      "Epoch 173, Loss: 0.44094101686109266\n",
      "Epoch 174, Loss: 0.4417368233339534\n",
      "Epoch 175, Loss: 0.4416582519101522\n",
      "Epoch 176, Loss: 0.4396788970821407\n",
      "Epoch 177, Loss: 0.4390195954561405\n",
      "Epoch 178, Loss: 0.4400830988007806\n",
      "Epoch 179, Loss: 0.43931775869233325\n",
      "Epoch 180, Loss: 0.4380032505977321\n",
      "Epoch 181, Loss: 0.4384470974340077\n",
      "Epoch 182, Loss: 0.4385167636581309\n",
      "Epoch 183, Loss: 0.43733043450040754\n",
      "Epoch 184, Loss: 0.4373176399006318\n",
      "Epoch 185, Loss: 0.4375256973621567\n",
      "Epoch 186, Loss: 0.43659322296038683\n",
      "Epoch 187, Loss: 0.43644391045908093\n",
      "Epoch 188, Loss: 0.43663844358015674\n",
      "Epoch 189, Loss: 0.43586286705793914\n",
      "Epoch 190, Loss: 0.4355663072844155\n",
      "Epoch 191, Loss: 0.4357040532214924\n",
      "Epoch 192, Loss: 0.435134007760397\n",
      "Epoch 193, Loss: 0.43474241127906293\n",
      "Epoch 194, Loss: 0.43480482310319635\n",
      "Epoch 195, Loss: 0.43443826104982264\n",
      "Epoch 196, Loss: 0.43390767873021246\n",
      "Epoch 197, Loss: 0.4338897691233524\n",
      "Epoch 198, Loss: 0.43377856877811044\n",
      "Epoch 199, Loss: 0.4331595506415052\n",
      "Epoch 200, Loss: 0.43289415091975497\n",
      "Epoch 201, Loss: 0.43294048069736957\n",
      "Epoch 202, Loss: 0.43262240576772243\n",
      "Epoch 203, Loss: 0.43206541812705346\n",
      "Epoch 204, Loss: 0.43172500614102416\n",
      "Epoch 205, Loss: 0.4316686388133938\n",
      "Epoch 206, Loss: 0.4315179158466364\n",
      "Epoch 207, Loss: 0.4310712381632941\n",
      "Epoch 208, Loss: 0.43059470035496\n",
      "Epoch 209, Loss: 0.4303711221416647\n",
      "Epoch 210, Loss: 0.43033088719595897\n",
      "Epoch 211, Loss: 0.43016580489992484\n",
      "Epoch 212, Loss: 0.429691817522032\n",
      "Epoch 213, Loss: 0.4292099292794459\n",
      "Epoch 214, Loss: 0.4289207205910941\n",
      "Epoch 215, Loss: 0.4287978238495365\n",
      "Epoch 216, Loss: 0.42870603658529494\n",
      "Epoch 217, Loss: 0.42841980940051866\n",
      "Epoch 218, Loss: 0.4278918633922788\n",
      "Epoch 219, Loss: 0.42748307113702383\n",
      "Epoch 220, Loss: 0.4271688511370021\n",
      "Epoch 221, Loss: 0.4269432345934848\n",
      "Epoch 222, Loss: 0.4267651275776002\n",
      "Epoch 223, Loss: 0.42668376428587257\n",
      "Epoch 224, Loss: 0.42653554195413756\n",
      "Epoch 225, Loss: 0.42613293183996703\n",
      "Epoch 226, Loss: 0.4255451632780942\n",
      "Epoch 227, Loss: 0.4251305043179084\n",
      "Epoch 228, Loss: 0.42475934718412794\n",
      "Epoch 229, Loss: 0.42452283256309487\n",
      "Epoch 230, Loss: 0.4244024016195508\n",
      "Epoch 231, Loss: 0.4242604946441205\n",
      "Epoch 232, Loss: 0.424134367950937\n",
      "Epoch 233, Loss: 0.42372608274856827\n",
      "Epoch 234, Loss: 0.4231492155631465\n",
      "Epoch 235, Loss: 0.4226291943984805\n",
      "Epoch 236, Loss: 0.4221723179942825\n",
      "Epoch 237, Loss: 0.4218955917984354\n",
      "Epoch 238, Loss: 0.4216767834434851\n",
      "Epoch 239, Loss: 0.42145169178243785\n",
      "Epoch 240, Loss: 0.4212605805865247\n",
      "Epoch 241, Loss: 0.42123380253494425\n",
      "Epoch 242, Loss: 0.42125483246926426\n",
      "Epoch 243, Loss: 0.42124070061060576\n",
      "Epoch 244, Loss: 0.42073760533336746\n",
      "Epoch 245, Loss: 0.42002867294257734\n",
      "Epoch 246, Loss: 0.41900436945460057\n",
      "Epoch 247, Loss: 0.41841940783018283\n",
      "Epoch 248, Loss: 0.4184544673382656\n",
      "Epoch 249, Loss: 0.4187823781263808\n",
      "Epoch 250, Loss: 0.41914970907160026\n",
      "Epoch 251, Loss: 0.418702737788617\n",
      "Epoch 252, Loss: 0.41762532180686956\n",
      "Epoch 253, Loss: 0.41665732452727816\n",
      "Epoch 254, Loss: 0.4164608109104539\n",
      "Epoch 255, Loss: 0.4169362050698102\n",
      "Epoch 256, Loss: 0.4170439641107097\n",
      "Epoch 257, Loss: 0.41618098523090385\n",
      "Epoch 258, Loss: 0.41521870035352226\n",
      "Epoch 259, Loss: 0.41507164322372797\n",
      "Epoch 260, Loss: 0.4154855816987037\n",
      "Epoch 261, Loss: 0.4152875627561342\n",
      "Epoch 262, Loss: 0.4146051629547779\n",
      "Epoch 263, Loss: 0.4138602536580612\n",
      "Epoch 264, Loss: 0.4135535008669373\n",
      "Epoch 265, Loss: 0.4136485063474015\n",
      "Epoch 266, Loss: 0.4135539332507534\n",
      "Epoch 267, Loss: 0.4133421191440525\n",
      "Epoch 268, Loss: 0.412685660282317\n",
      "Epoch 269, Loss: 0.4122655129807526\n",
      "Epoch 270, Loss: 0.4118217259860175\n",
      "Epoch 271, Loss: 0.41165256945543577\n",
      "Epoch 272, Loss: 0.41146929239414193\n",
      "Epoch 273, Loss: 0.4115028618809221\n",
      "Epoch 274, Loss: 0.4117686232412549\n",
      "Epoch 275, Loss: 0.4121111105932055\n",
      "Epoch 276, Loss: 0.4130580342565522\n",
      "Epoch 277, Loss: 0.4121949346593443\n",
      "Epoch 278, Loss: 0.41057389204015954\n",
      "Epoch 279, Loss: 0.4095484089667836\n",
      "Epoch 280, Loss: 0.4098107108594438\n",
      "Epoch 281, Loss: 0.41080122599126295\n",
      "Epoch 282, Loss: 0.41057013564631983\n",
      "Epoch 283, Loss: 0.4096005808315313\n",
      "Epoch 284, Loss: 0.4083534584470671\n",
      "Epoch 285, Loss: 0.4080981251863069\n",
      "Epoch 286, Loss: 0.40860646229911785\n",
      "Epoch 287, Loss: 0.40884595202638996\n",
      "Epoch 288, Loss: 0.40838367174584433\n",
      "Epoch 289, Loss: 0.40727524500698936\n",
      "Epoch 290, Loss: 0.4066578761535602\n",
      "Epoch 291, Loss: 0.4068821060860988\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2522678389461942\n",
      "Valence RMSE: 0.2615826980629526\n",
      "Arousal RMSE: 0.24259558365162406\n",
      "Test R^2 score: tensor([0.2688, 0.5660], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4173856412162142\n",
      "Num of epochs: 292\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Epoch 121, Loss: 0.4596922103619757\n",
      "Epoch 122, Loss: 0.45943389956138125\n",
      "Epoch 123, Loss: 0.45877818894117006\n",
      "Epoch 124, Loss: 0.45746538132874076\n",
      "Epoch 125, Loss: 0.4570462803121619\n",
      "Epoch 126, Loss: 0.45696189549769234\n",
      "Epoch 127, Loss: 0.45594920341421274\n",
      "Epoch 128, Loss: 0.45536359747685007\n",
      "Epoch 129, Loss: 0.4553290890752543\n",
      "Epoch 130, Loss: 0.4547195823074053\n",
      "Epoch 131, Loss: 0.45392838323998064\n",
      "Epoch 132, Loss: 0.4537853493595631\n",
      "Epoch 133, Loss: 0.45347437050767975\n",
      "Epoch 134, Loss: 0.45267431273812453\n",
      "Epoch 135, Loss: 0.45228737659373064\n",
      "Epoch 136, Loss: 0.4522154657432981\n",
      "Epoch 137, Loss: 0.45165964487407706\n",
      "Epoch 138, Loss: 0.4510751265462795\n",
      "Epoch 139, Loss: 0.45082650445054473\n",
      "Epoch 140, Loss: 0.4506281098405297\n",
      "Epoch 141, Loss: 0.4501911419656675\n",
      "Epoch 142, Loss: 0.4497001887659326\n",
      "Epoch 143, Loss: 0.44943436133718806\n",
      "Epoch 144, Loss: 0.449226296402944\n",
      "Epoch 145, Loss: 0.44886073897867573\n",
      "Epoch 146, Loss: 0.44844710376304864\n",
      "Epoch 147, Loss: 0.44810507013116724\n",
      "Epoch 148, Loss: 0.44788545862049767\n",
      "Epoch 149, Loss: 0.4476741441009651\n",
      "Epoch 150, Loss: 0.44735861731902055\n",
      "Epoch 151, Loss: 0.4469288720157105\n",
      "Epoch 152, Loss: 0.44655235754178146\n",
      "Epoch 153, Loss: 0.4462857402786373\n",
      "Epoch 154, Loss: 0.4460517369142754\n",
      "Epoch 155, Loss: 0.4457881796184935\n",
      "Epoch 156, Loss: 0.44542210886348255\n",
      "Epoch 157, Loss: 0.4450579467827662\n",
      "Epoch 158, Loss: 0.44473329320345206\n",
      "Epoch 159, Loss: 0.4444482514678638\n",
      "Epoch 160, Loss: 0.4441745674630603\n",
      "Epoch 161, Loss: 0.4439221813722748\n",
      "Epoch 162, Loss: 0.44363357870363757\n",
      "Epoch 163, Loss: 0.4433189071134186\n",
      "Epoch 164, Loss: 0.44295452976444744\n",
      "Epoch 165, Loss: 0.4426058108229731\n",
      "Epoch 166, Loss: 0.4422485787900922\n",
      "Epoch 167, Loss: 0.4419090985167976\n",
      "Epoch 168, Loss: 0.44158054387831963\n",
      "Epoch 169, Loss: 0.4412474388772268\n",
      "Epoch 170, Loss: 0.4409277694136744\n",
      "Epoch 171, Loss: 0.44068045571539705\n",
      "Epoch 172, Loss: 0.4405979418732049\n",
      "Epoch 173, Loss: 0.44094101686109266\n",
      "Epoch 174, Loss: 0.4417368233339534\n",
      "Epoch 175, Loss: 0.4416582519101522\n",
      "Epoch 176, Loss: 0.4396788970821407\n",
      "Epoch 177, Loss: 0.4390195954561405\n",
      "Epoch 178, Loss: 0.4400830988007806\n",
      "Epoch 179, Loss: 0.43931775869233325\n",
      "Epoch 180, Loss: 0.4380032505977321\n",
      "Epoch 181, Loss: 0.4384470974340077\n",
      "Epoch 182, Loss: 0.4385167636581309\n",
      "Epoch 183, Loss: 0.43733043450040754\n",
      "Epoch 184, Loss: 0.4373176399006318\n",
      "Epoch 185, Loss: 0.4375256973621567\n",
      "Epoch 186, Loss: 0.43659322296038683\n",
      "Epoch 187, Loss: 0.43644391045908093\n",
      "Epoch 188, Loss: 0.43663844358015674\n",
      "Epoch 189, Loss: 0.43586286705793914\n",
      "Epoch 190, Loss: 0.4355663072844155\n",
      "Epoch 191, Loss: 0.4357040532214924\n",
      "Epoch 192, Loss: 0.435134007760397\n",
      "Epoch 193, Loss: 0.43474241127906293\n",
      "Epoch 194, Loss: 0.43480482310319635\n",
      "Epoch 195, Loss: 0.43443826104982264\n",
      "Epoch 196, Loss: 0.43390767873021246\n",
      "Epoch 197, Loss: 0.4338897691233524\n",
      "Epoch 198, Loss: 0.43377856877811044\n",
      "Epoch 199, Loss: 0.4331595506415052\n",
      "Epoch 200, Loss: 0.43289415091975497\n",
      "Epoch 201, Loss: 0.43294048069736957\n",
      "Epoch 202, Loss: 0.43262240576772243\n",
      "Epoch 203, Loss: 0.43206541812705346\n",
      "Epoch 204, Loss: 0.43172500614102416\n",
      "Epoch 205, Loss: 0.4316686388133938\n",
      "Epoch 206, Loss: 0.4315179158466364\n",
      "Epoch 207, Loss: 0.4310712381632941\n",
      "Epoch 208, Loss: 0.43059470035496\n",
      "Epoch 209, Loss: 0.4303711221416647\n",
      "Epoch 210, Loss: 0.43033088719595897\n",
      "Epoch 211, Loss: 0.43016580489992484\n",
      "Epoch 212, Loss: 0.429691817522032\n",
      "Epoch 213, Loss: 0.4292099292794459\n",
      "Epoch 214, Loss: 0.4289207205910941\n",
      "Epoch 215, Loss: 0.4287978238495365\n",
      "Epoch 216, Loss: 0.42870603658529494\n",
      "Epoch 217, Loss: 0.42841980940051866\n",
      "Epoch 218, Loss: 0.4278918633922788\n",
      "Epoch 219, Loss: 0.42748307113702383\n",
      "Epoch 220, Loss: 0.4271688511370021\n",
      "Epoch 221, Loss: 0.4269432345934848\n",
      "Epoch 222, Loss: 0.4267651275776002\n",
      "Epoch 223, Loss: 0.42668376428587257\n",
      "Epoch 224, Loss: 0.42653554195413756\n",
      "Epoch 225, Loss: 0.42613293183996703\n",
      "Epoch 226, Loss: 0.4255451632780942\n",
      "Epoch 227, Loss: 0.4251305043179084\n",
      "Epoch 228, Loss: 0.42475934718412794\n",
      "Epoch 229, Loss: 0.42452283256309487\n",
      "Epoch 230, Loss: 0.4244024016195508\n",
      "Epoch 231, Loss: 0.4242604946441205\n",
      "Epoch 232, Loss: 0.424134367950937\n",
      "Epoch 233, Loss: 0.42372608274856827\n",
      "Epoch 234, Loss: 0.4231492155631465\n",
      "Epoch 235, Loss: 0.4226291943984805\n",
      "Epoch 236, Loss: 0.4221723179942825\n",
      "Epoch 237, Loss: 0.4218955917984354\n",
      "Epoch 238, Loss: 0.4216767834434851\n",
      "Epoch 239, Loss: 0.42145169178243785\n",
      "Epoch 240, Loss: 0.4212605805865247\n",
      "Epoch 241, Loss: 0.42123380253494425\n",
      "Epoch 242, Loss: 0.42125483246926426\n",
      "Epoch 243, Loss: 0.42124070061060576\n",
      "Epoch 244, Loss: 0.42073760533336746\n",
      "Epoch 245, Loss: 0.42002867294257734\n",
      "Epoch 246, Loss: 0.41900436945460057\n",
      "Epoch 247, Loss: 0.41841940783018283\n",
      "Epoch 248, Loss: 0.4184544673382656\n",
      "Epoch 249, Loss: 0.4187823781263808\n",
      "Epoch 250, Loss: 0.41914970907160026\n",
      "Epoch 251, Loss: 0.418702737788617\n",
      "Epoch 252, Loss: 0.41762532180686956\n",
      "Epoch 253, Loss: 0.41665732452727816\n",
      "Epoch 254, Loss: 0.4164608109104539\n",
      "Epoch 255, Loss: 0.4169362050698102\n",
      "Epoch 256, Loss: 0.4170439641107097\n",
      "Epoch 257, Loss: 0.41618098523090385\n",
      "Epoch 258, Loss: 0.41521870035352226\n",
      "Epoch 259, Loss: 0.41507164322372797\n",
      "Epoch 260, Loss: 0.4154855816987037\n",
      "Epoch 261, Loss: 0.4152875627561342\n",
      "Epoch 262, Loss: 0.4146051629547779\n",
      "Epoch 263, Loss: 0.4138602536580612\n",
      "Epoch 264, Loss: 0.4135535008669373\n",
      "Epoch 265, Loss: 0.4136485063474015\n",
      "Epoch 266, Loss: 0.4135539332507534\n",
      "Epoch 267, Loss: 0.4133421191440525\n",
      "Epoch 268, Loss: 0.412685660282317\n",
      "Epoch 269, Loss: 0.4122655129807526\n",
      "Epoch 270, Loss: 0.4118217259860175\n",
      "Epoch 271, Loss: 0.41165256945543577\n",
      "Epoch 272, Loss: 0.41146929239414193\n",
      "Epoch 273, Loss: 0.4115028618809221\n",
      "Epoch 274, Loss: 0.4117686232412549\n",
      "Epoch 275, Loss: 0.4121111105932055\n",
      "Epoch 276, Loss: 0.4130580342565522\n",
      "Epoch 277, Loss: 0.4121949346593443\n",
      "Epoch 278, Loss: 0.41057389204015954\n",
      "Epoch 279, Loss: 0.4095484089667836\n",
      "Epoch 280, Loss: 0.4098107108594438\n",
      "Epoch 281, Loss: 0.41080122599126295\n",
      "Epoch 282, Loss: 0.41057013564631983\n",
      "Epoch 283, Loss: 0.4096005808315313\n",
      "Epoch 284, Loss: 0.4083534584470671\n",
      "Epoch 285, Loss: 0.4080981251863069\n",
      "Epoch 286, Loss: 0.40860646229911785\n",
      "Epoch 287, Loss: 0.40884595202638996\n",
      "Epoch 288, Loss: 0.40838367174584433\n",
      "Epoch 289, Loss: 0.40727524500698936\n",
      "Epoch 290, Loss: 0.4066578761535602\n",
      "Epoch 291, Loss: 0.4068821060860988\n",
      "Epoch 292, Loss: 0.4073894550009826\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.25299775206106845\n",
      "Valence RMSE: 0.263294133383738\n",
      "Arousal RMSE: 0.24226416247892318\n",
      "Test R^2 score: tensor([0.2592, 0.5671], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4131786268101396\n",
      "Num of epochs: 293\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Epoch 121, Loss: 0.4596922103619757\n",
      "Epoch 122, Loss: 0.45943389956138125\n",
      "Epoch 123, Loss: 0.45877818894117006\n",
      "Epoch 124, Loss: 0.45746538132874076\n",
      "Epoch 125, Loss: 0.4570462803121619\n",
      "Epoch 126, Loss: 0.45696189549769234\n",
      "Epoch 127, Loss: 0.45594920341421274\n",
      "Epoch 128, Loss: 0.45536359747685007\n",
      "Epoch 129, Loss: 0.4553290890752543\n",
      "Epoch 130, Loss: 0.4547195823074053\n",
      "Epoch 131, Loss: 0.45392838323998064\n",
      "Epoch 132, Loss: 0.4537853493595631\n",
      "Epoch 133, Loss: 0.45347437050767975\n",
      "Epoch 134, Loss: 0.45267431273812453\n",
      "Epoch 135, Loss: 0.45228737659373064\n",
      "Epoch 136, Loss: 0.4522154657432981\n",
      "Epoch 137, Loss: 0.45165964487407706\n",
      "Epoch 138, Loss: 0.4510751265462795\n",
      "Epoch 139, Loss: 0.45082650445054473\n",
      "Epoch 140, Loss: 0.4506281098405297\n",
      "Epoch 141, Loss: 0.4501911419656675\n",
      "Epoch 142, Loss: 0.4497001887659326\n",
      "Epoch 143, Loss: 0.44943436133718806\n",
      "Epoch 144, Loss: 0.449226296402944\n",
      "Epoch 145, Loss: 0.44886073897867573\n",
      "Epoch 146, Loss: 0.44844710376304864\n",
      "Epoch 147, Loss: 0.44810507013116724\n",
      "Epoch 148, Loss: 0.44788545862049767\n",
      "Epoch 149, Loss: 0.4476741441009651\n",
      "Epoch 150, Loss: 0.44735861731902055\n",
      "Epoch 151, Loss: 0.4469288720157105\n",
      "Epoch 152, Loss: 0.44655235754178146\n",
      "Epoch 153, Loss: 0.4462857402786373\n",
      "Epoch 154, Loss: 0.4460517369142754\n",
      "Epoch 155, Loss: 0.4457881796184935\n",
      "Epoch 156, Loss: 0.44542210886348255\n",
      "Epoch 157, Loss: 0.4450579467827662\n",
      "Epoch 158, Loss: 0.44473329320345206\n",
      "Epoch 159, Loss: 0.4444482514678638\n",
      "Epoch 160, Loss: 0.4441745674630603\n",
      "Epoch 161, Loss: 0.4439221813722748\n",
      "Epoch 162, Loss: 0.44363357870363757\n",
      "Epoch 163, Loss: 0.4433189071134186\n",
      "Epoch 164, Loss: 0.44295452976444744\n",
      "Epoch 165, Loss: 0.4426058108229731\n",
      "Epoch 166, Loss: 0.4422485787900922\n",
      "Epoch 167, Loss: 0.4419090985167976\n",
      "Epoch 168, Loss: 0.44158054387831963\n",
      "Epoch 169, Loss: 0.4412474388772268\n",
      "Epoch 170, Loss: 0.4409277694136744\n",
      "Epoch 171, Loss: 0.44068045571539705\n",
      "Epoch 172, Loss: 0.4405979418732049\n",
      "Epoch 173, Loss: 0.44094101686109266\n",
      "Epoch 174, Loss: 0.4417368233339534\n",
      "Epoch 175, Loss: 0.4416582519101522\n",
      "Epoch 176, Loss: 0.4396788970821407\n",
      "Epoch 177, Loss: 0.4390195954561405\n",
      "Epoch 178, Loss: 0.4400830988007806\n",
      "Epoch 179, Loss: 0.43931775869233325\n",
      "Epoch 180, Loss: 0.4380032505977321\n",
      "Epoch 181, Loss: 0.4384470974340077\n",
      "Epoch 182, Loss: 0.4385167636581309\n",
      "Epoch 183, Loss: 0.43733043450040754\n",
      "Epoch 184, Loss: 0.4373176399006318\n",
      "Epoch 185, Loss: 0.4375256973621567\n",
      "Epoch 186, Loss: 0.43659322296038683\n",
      "Epoch 187, Loss: 0.43644391045908093\n",
      "Epoch 188, Loss: 0.43663844358015674\n",
      "Epoch 189, Loss: 0.43586286705793914\n",
      "Epoch 190, Loss: 0.4355663072844155\n",
      "Epoch 191, Loss: 0.4357040532214924\n",
      "Epoch 192, Loss: 0.435134007760397\n",
      "Epoch 193, Loss: 0.43474241127906293\n",
      "Epoch 194, Loss: 0.43480482310319635\n",
      "Epoch 195, Loss: 0.43443826104982264\n",
      "Epoch 196, Loss: 0.43390767873021246\n",
      "Epoch 197, Loss: 0.4338897691233524\n",
      "Epoch 198, Loss: 0.43377856877811044\n",
      "Epoch 199, Loss: 0.4331595506415052\n",
      "Epoch 200, Loss: 0.43289415091975497\n",
      "Epoch 201, Loss: 0.43294048069736957\n",
      "Epoch 202, Loss: 0.43262240576772243\n",
      "Epoch 203, Loss: 0.43206541812705346\n",
      "Epoch 204, Loss: 0.43172500614102416\n",
      "Epoch 205, Loss: 0.4316686388133938\n",
      "Epoch 206, Loss: 0.4315179158466364\n",
      "Epoch 207, Loss: 0.4310712381632941\n",
      "Epoch 208, Loss: 0.43059470035496\n",
      "Epoch 209, Loss: 0.4303711221416647\n",
      "Epoch 210, Loss: 0.43033088719595897\n",
      "Epoch 211, Loss: 0.43016580489992484\n",
      "Epoch 212, Loss: 0.429691817522032\n",
      "Epoch 213, Loss: 0.4292099292794459\n",
      "Epoch 214, Loss: 0.4289207205910941\n",
      "Epoch 215, Loss: 0.4287978238495365\n",
      "Epoch 216, Loss: 0.42870603658529494\n",
      "Epoch 217, Loss: 0.42841980940051866\n",
      "Epoch 218, Loss: 0.4278918633922788\n",
      "Epoch 219, Loss: 0.42748307113702383\n",
      "Epoch 220, Loss: 0.4271688511370021\n",
      "Epoch 221, Loss: 0.4269432345934848\n",
      "Epoch 222, Loss: 0.4267651275776002\n",
      "Epoch 223, Loss: 0.42668376428587257\n",
      "Epoch 224, Loss: 0.42653554195413756\n",
      "Epoch 225, Loss: 0.42613293183996703\n",
      "Epoch 226, Loss: 0.4255451632780942\n",
      "Epoch 227, Loss: 0.4251305043179084\n",
      "Epoch 228, Loss: 0.42475934718412794\n",
      "Epoch 229, Loss: 0.42452283256309487\n",
      "Epoch 230, Loss: 0.4244024016195508\n",
      "Epoch 231, Loss: 0.4242604946441205\n",
      "Epoch 232, Loss: 0.424134367950937\n",
      "Epoch 233, Loss: 0.42372608274856827\n",
      "Epoch 234, Loss: 0.4231492155631465\n",
      "Epoch 235, Loss: 0.4226291943984805\n",
      "Epoch 236, Loss: 0.4221723179942825\n",
      "Epoch 237, Loss: 0.4218955917984354\n",
      "Epoch 238, Loss: 0.4216767834434851\n",
      "Epoch 239, Loss: 0.42145169178243785\n",
      "Epoch 240, Loss: 0.4212605805865247\n",
      "Epoch 241, Loss: 0.42123380253494425\n",
      "Epoch 242, Loss: 0.42125483246926426\n",
      "Epoch 243, Loss: 0.42124070061060576\n",
      "Epoch 244, Loss: 0.42073760533336746\n",
      "Epoch 245, Loss: 0.42002867294257734\n",
      "Epoch 246, Loss: 0.41900436945460057\n",
      "Epoch 247, Loss: 0.41841940783018283\n",
      "Epoch 248, Loss: 0.4184544673382656\n",
      "Epoch 249, Loss: 0.4187823781263808\n",
      "Epoch 250, Loss: 0.41914970907160026\n",
      "Epoch 251, Loss: 0.418702737788617\n",
      "Epoch 252, Loss: 0.41762532180686956\n",
      "Epoch 253, Loss: 0.41665732452727816\n",
      "Epoch 254, Loss: 0.4164608109104539\n",
      "Epoch 255, Loss: 0.4169362050698102\n",
      "Epoch 256, Loss: 0.4170439641107097\n",
      "Epoch 257, Loss: 0.41618098523090385\n",
      "Epoch 258, Loss: 0.41521870035352226\n",
      "Epoch 259, Loss: 0.41507164322372797\n",
      "Epoch 260, Loss: 0.4154855816987037\n",
      "Epoch 261, Loss: 0.4152875627561342\n",
      "Epoch 262, Loss: 0.4146051629547779\n",
      "Epoch 263, Loss: 0.4138602536580612\n",
      "Epoch 264, Loss: 0.4135535008669373\n",
      "Epoch 265, Loss: 0.4136485063474015\n",
      "Epoch 266, Loss: 0.4135539332507534\n",
      "Epoch 267, Loss: 0.4133421191440525\n",
      "Epoch 268, Loss: 0.412685660282317\n",
      "Epoch 269, Loss: 0.4122655129807526\n",
      "Epoch 270, Loss: 0.4118217259860175\n",
      "Epoch 271, Loss: 0.41165256945543577\n",
      "Epoch 272, Loss: 0.41146929239414193\n",
      "Epoch 273, Loss: 0.4115028618809221\n",
      "Epoch 274, Loss: 0.4117686232412549\n",
      "Epoch 275, Loss: 0.4121111105932055\n",
      "Epoch 276, Loss: 0.4130580342565522\n",
      "Epoch 277, Loss: 0.4121949346593443\n",
      "Epoch 278, Loss: 0.41057389204015954\n",
      "Epoch 279, Loss: 0.4095484089667836\n",
      "Epoch 280, Loss: 0.4098107108594438\n",
      "Epoch 281, Loss: 0.41080122599126295\n",
      "Epoch 282, Loss: 0.41057013564631983\n",
      "Epoch 283, Loss: 0.4096005808315313\n",
      "Epoch 284, Loss: 0.4083534584470671\n",
      "Epoch 285, Loss: 0.4080981251863069\n",
      "Epoch 286, Loss: 0.40860646229911785\n",
      "Epoch 287, Loss: 0.40884595202638996\n",
      "Epoch 288, Loss: 0.40838367174584433\n",
      "Epoch 289, Loss: 0.40727524500698936\n",
      "Epoch 290, Loss: 0.4066578761535602\n",
      "Epoch 291, Loss: 0.4068821060860988\n",
      "Epoch 292, Loss: 0.4073894550009826\n",
      "Epoch 293, Loss: 0.4073556196958488\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2527840282401003\n",
      "Valence RMSE: 0.26213404380900135\n",
      "Arousal RMSE: 0.24307462422664414\n",
      "Test R^2 score: tensor([0.2657, 0.5642], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4149849460711589\n",
      "Num of epochs: 294\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Epoch 121, Loss: 0.4596922103619757\n",
      "Epoch 122, Loss: 0.45943389956138125\n",
      "Epoch 123, Loss: 0.45877818894117006\n",
      "Epoch 124, Loss: 0.45746538132874076\n",
      "Epoch 125, Loss: 0.4570462803121619\n",
      "Epoch 126, Loss: 0.45696189549769234\n",
      "Epoch 127, Loss: 0.45594920341421274\n",
      "Epoch 128, Loss: 0.45536359747685007\n",
      "Epoch 129, Loss: 0.4553290890752543\n",
      "Epoch 130, Loss: 0.4547195823074053\n",
      "Epoch 131, Loss: 0.45392838323998064\n",
      "Epoch 132, Loss: 0.4537853493595631\n",
      "Epoch 133, Loss: 0.45347437050767975\n",
      "Epoch 134, Loss: 0.45267431273812453\n",
      "Epoch 135, Loss: 0.45228737659373064\n",
      "Epoch 136, Loss: 0.4522154657432981\n",
      "Epoch 137, Loss: 0.45165964487407706\n",
      "Epoch 138, Loss: 0.4510751265462795\n",
      "Epoch 139, Loss: 0.45082650445054473\n",
      "Epoch 140, Loss: 0.4506281098405297\n",
      "Epoch 141, Loss: 0.4501911419656675\n",
      "Epoch 142, Loss: 0.4497001887659326\n",
      "Epoch 143, Loss: 0.44943436133718806\n",
      "Epoch 144, Loss: 0.449226296402944\n",
      "Epoch 145, Loss: 0.44886073897867573\n",
      "Epoch 146, Loss: 0.44844710376304864\n",
      "Epoch 147, Loss: 0.44810507013116724\n",
      "Epoch 148, Loss: 0.44788545862049767\n",
      "Epoch 149, Loss: 0.4476741441009651\n",
      "Epoch 150, Loss: 0.44735861731902055\n",
      "Epoch 151, Loss: 0.4469288720157105\n",
      "Epoch 152, Loss: 0.44655235754178146\n",
      "Epoch 153, Loss: 0.4462857402786373\n",
      "Epoch 154, Loss: 0.4460517369142754\n",
      "Epoch 155, Loss: 0.4457881796184935\n",
      "Epoch 156, Loss: 0.44542210886348255\n",
      "Epoch 157, Loss: 0.4450579467827662\n",
      "Epoch 158, Loss: 0.44473329320345206\n",
      "Epoch 159, Loss: 0.4444482514678638\n",
      "Epoch 160, Loss: 0.4441745674630603\n",
      "Epoch 161, Loss: 0.4439221813722748\n",
      "Epoch 162, Loss: 0.44363357870363757\n",
      "Epoch 163, Loss: 0.4433189071134186\n",
      "Epoch 164, Loss: 0.44295452976444744\n",
      "Epoch 165, Loss: 0.4426058108229731\n",
      "Epoch 166, Loss: 0.4422485787900922\n",
      "Epoch 167, Loss: 0.4419090985167976\n",
      "Epoch 168, Loss: 0.44158054387831963\n",
      "Epoch 169, Loss: 0.4412474388772268\n",
      "Epoch 170, Loss: 0.4409277694136744\n",
      "Epoch 171, Loss: 0.44068045571539705\n",
      "Epoch 172, Loss: 0.4405979418732049\n",
      "Epoch 173, Loss: 0.44094101686109266\n",
      "Epoch 174, Loss: 0.4417368233339534\n",
      "Epoch 175, Loss: 0.4416582519101522\n",
      "Epoch 176, Loss: 0.4396788970821407\n",
      "Epoch 177, Loss: 0.4390195954561405\n",
      "Epoch 178, Loss: 0.4400830988007806\n",
      "Epoch 179, Loss: 0.43931775869233325\n",
      "Epoch 180, Loss: 0.4380032505977321\n",
      "Epoch 181, Loss: 0.4384470974340077\n",
      "Epoch 182, Loss: 0.4385167636581309\n",
      "Epoch 183, Loss: 0.43733043450040754\n",
      "Epoch 184, Loss: 0.4373176399006318\n",
      "Epoch 185, Loss: 0.4375256973621567\n",
      "Epoch 186, Loss: 0.43659322296038683\n",
      "Epoch 187, Loss: 0.43644391045908093\n",
      "Epoch 188, Loss: 0.43663844358015674\n",
      "Epoch 189, Loss: 0.43586286705793914\n",
      "Epoch 190, Loss: 0.4355663072844155\n",
      "Epoch 191, Loss: 0.4357040532214924\n",
      "Epoch 192, Loss: 0.435134007760397\n",
      "Epoch 193, Loss: 0.43474241127906293\n",
      "Epoch 194, Loss: 0.43480482310319635\n",
      "Epoch 195, Loss: 0.43443826104982264\n",
      "Epoch 196, Loss: 0.43390767873021246\n",
      "Epoch 197, Loss: 0.4338897691233524\n",
      "Epoch 198, Loss: 0.43377856877811044\n",
      "Epoch 199, Loss: 0.4331595506415052\n",
      "Epoch 200, Loss: 0.43289415091975497\n",
      "Epoch 201, Loss: 0.43294048069736957\n",
      "Epoch 202, Loss: 0.43262240576772243\n",
      "Epoch 203, Loss: 0.43206541812705346\n",
      "Epoch 204, Loss: 0.43172500614102416\n",
      "Epoch 205, Loss: 0.4316686388133938\n",
      "Epoch 206, Loss: 0.4315179158466364\n",
      "Epoch 207, Loss: 0.4310712381632941\n",
      "Epoch 208, Loss: 0.43059470035496\n",
      "Epoch 209, Loss: 0.4303711221416647\n",
      "Epoch 210, Loss: 0.43033088719595897\n",
      "Epoch 211, Loss: 0.43016580489992484\n",
      "Epoch 212, Loss: 0.429691817522032\n",
      "Epoch 213, Loss: 0.4292099292794459\n",
      "Epoch 214, Loss: 0.4289207205910941\n",
      "Epoch 215, Loss: 0.4287978238495365\n",
      "Epoch 216, Loss: 0.42870603658529494\n",
      "Epoch 217, Loss: 0.42841980940051866\n",
      "Epoch 218, Loss: 0.4278918633922788\n",
      "Epoch 219, Loss: 0.42748307113702383\n",
      "Epoch 220, Loss: 0.4271688511370021\n",
      "Epoch 221, Loss: 0.4269432345934848\n",
      "Epoch 222, Loss: 0.4267651275776002\n",
      "Epoch 223, Loss: 0.42668376428587257\n",
      "Epoch 224, Loss: 0.42653554195413756\n",
      "Epoch 225, Loss: 0.42613293183996703\n",
      "Epoch 226, Loss: 0.4255451632780942\n",
      "Epoch 227, Loss: 0.4251305043179084\n",
      "Epoch 228, Loss: 0.42475934718412794\n",
      "Epoch 229, Loss: 0.42452283256309487\n",
      "Epoch 230, Loss: 0.4244024016195508\n",
      "Epoch 231, Loss: 0.4242604946441205\n",
      "Epoch 232, Loss: 0.424134367950937\n",
      "Epoch 233, Loss: 0.42372608274856827\n",
      "Epoch 234, Loss: 0.4231492155631465\n",
      "Epoch 235, Loss: 0.4226291943984805\n",
      "Epoch 236, Loss: 0.4221723179942825\n",
      "Epoch 237, Loss: 0.4218955917984354\n",
      "Epoch 238, Loss: 0.4216767834434851\n",
      "Epoch 239, Loss: 0.42145169178243785\n",
      "Epoch 240, Loss: 0.4212605805865247\n",
      "Epoch 241, Loss: 0.42123380253494425\n",
      "Epoch 242, Loss: 0.42125483246926426\n",
      "Epoch 243, Loss: 0.42124070061060576\n",
      "Epoch 244, Loss: 0.42073760533336746\n",
      "Epoch 245, Loss: 0.42002867294257734\n",
      "Epoch 246, Loss: 0.41900436945460057\n",
      "Epoch 247, Loss: 0.41841940783018283\n",
      "Epoch 248, Loss: 0.4184544673382656\n",
      "Epoch 249, Loss: 0.4187823781263808\n",
      "Epoch 250, Loss: 0.41914970907160026\n",
      "Epoch 251, Loss: 0.418702737788617\n",
      "Epoch 252, Loss: 0.41762532180686956\n",
      "Epoch 253, Loss: 0.41665732452727816\n",
      "Epoch 254, Loss: 0.4164608109104539\n",
      "Epoch 255, Loss: 0.4169362050698102\n",
      "Epoch 256, Loss: 0.4170439641107097\n",
      "Epoch 257, Loss: 0.41618098523090385\n",
      "Epoch 258, Loss: 0.41521870035352226\n",
      "Epoch 259, Loss: 0.41507164322372797\n",
      "Epoch 260, Loss: 0.4154855816987037\n",
      "Epoch 261, Loss: 0.4152875627561342\n",
      "Epoch 262, Loss: 0.4146051629547779\n",
      "Epoch 263, Loss: 0.4138602536580612\n",
      "Epoch 264, Loss: 0.4135535008669373\n",
      "Epoch 265, Loss: 0.4136485063474015\n",
      "Epoch 266, Loss: 0.4135539332507534\n",
      "Epoch 267, Loss: 0.4133421191440525\n",
      "Epoch 268, Loss: 0.412685660282317\n",
      "Epoch 269, Loss: 0.4122655129807526\n",
      "Epoch 270, Loss: 0.4118217259860175\n",
      "Epoch 271, Loss: 0.41165256945543577\n",
      "Epoch 272, Loss: 0.41146929239414193\n",
      "Epoch 273, Loss: 0.4115028618809221\n",
      "Epoch 274, Loss: 0.4117686232412549\n",
      "Epoch 275, Loss: 0.4121111105932055\n",
      "Epoch 276, Loss: 0.4130580342565522\n",
      "Epoch 277, Loss: 0.4121949346593443\n",
      "Epoch 278, Loss: 0.41057389204015954\n",
      "Epoch 279, Loss: 0.4095484089667836\n",
      "Epoch 280, Loss: 0.4098107108594438\n",
      "Epoch 281, Loss: 0.41080122599126295\n",
      "Epoch 282, Loss: 0.41057013564631983\n",
      "Epoch 283, Loss: 0.4096005808315313\n",
      "Epoch 284, Loss: 0.4083534584470671\n",
      "Epoch 285, Loss: 0.4080981251863069\n",
      "Epoch 286, Loss: 0.40860646229911785\n",
      "Epoch 287, Loss: 0.40884595202638996\n",
      "Epoch 288, Loss: 0.40838367174584433\n",
      "Epoch 289, Loss: 0.40727524500698936\n",
      "Epoch 290, Loss: 0.4066578761535602\n",
      "Epoch 291, Loss: 0.4068821060860988\n",
      "Epoch 292, Loss: 0.4073894550009826\n",
      "Epoch 293, Loss: 0.4073556196958488\n",
      "Epoch 294, Loss: 0.4063989292938854\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.252845382270677\n",
      "Valence RMSE: 0.2627326342977476\n",
      "Arousal RMSE: 0.2425554319040816\n",
      "Test R^2 score: tensor([0.2624, 0.5661], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.41423602968271855\n",
      "Num of epochs: 295\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Epoch 121, Loss: 0.4596922103619757\n",
      "Epoch 122, Loss: 0.45943389956138125\n",
      "Epoch 123, Loss: 0.45877818894117006\n",
      "Epoch 124, Loss: 0.45746538132874076\n",
      "Epoch 125, Loss: 0.4570462803121619\n",
      "Epoch 126, Loss: 0.45696189549769234\n",
      "Epoch 127, Loss: 0.45594920341421274\n",
      "Epoch 128, Loss: 0.45536359747685007\n",
      "Epoch 129, Loss: 0.4553290890752543\n",
      "Epoch 130, Loss: 0.4547195823074053\n",
      "Epoch 131, Loss: 0.45392838323998064\n",
      "Epoch 132, Loss: 0.4537853493595631\n",
      "Epoch 133, Loss: 0.45347437050767975\n",
      "Epoch 134, Loss: 0.45267431273812453\n",
      "Epoch 135, Loss: 0.45228737659373064\n",
      "Epoch 136, Loss: 0.4522154657432981\n",
      "Epoch 137, Loss: 0.45165964487407706\n",
      "Epoch 138, Loss: 0.4510751265462795\n",
      "Epoch 139, Loss: 0.45082650445054473\n",
      "Epoch 140, Loss: 0.4506281098405297\n",
      "Epoch 141, Loss: 0.4501911419656675\n",
      "Epoch 142, Loss: 0.4497001887659326\n",
      "Epoch 143, Loss: 0.44943436133718806\n",
      "Epoch 144, Loss: 0.449226296402944\n",
      "Epoch 145, Loss: 0.44886073897867573\n",
      "Epoch 146, Loss: 0.44844710376304864\n",
      "Epoch 147, Loss: 0.44810507013116724\n",
      "Epoch 148, Loss: 0.44788545862049767\n",
      "Epoch 149, Loss: 0.4476741441009651\n",
      "Epoch 150, Loss: 0.44735861731902055\n",
      "Epoch 151, Loss: 0.4469288720157105\n",
      "Epoch 152, Loss: 0.44655235754178146\n",
      "Epoch 153, Loss: 0.4462857402786373\n",
      "Epoch 154, Loss: 0.4460517369142754\n",
      "Epoch 155, Loss: 0.4457881796184935\n",
      "Epoch 156, Loss: 0.44542210886348255\n",
      "Epoch 157, Loss: 0.4450579467827662\n",
      "Epoch 158, Loss: 0.44473329320345206\n",
      "Epoch 159, Loss: 0.4444482514678638\n",
      "Epoch 160, Loss: 0.4441745674630603\n",
      "Epoch 161, Loss: 0.4439221813722748\n",
      "Epoch 162, Loss: 0.44363357870363757\n",
      "Epoch 163, Loss: 0.4433189071134186\n",
      "Epoch 164, Loss: 0.44295452976444744\n",
      "Epoch 165, Loss: 0.4426058108229731\n",
      "Epoch 166, Loss: 0.4422485787900922\n",
      "Epoch 167, Loss: 0.4419090985167976\n",
      "Epoch 168, Loss: 0.44158054387831963\n",
      "Epoch 169, Loss: 0.4412474388772268\n",
      "Epoch 170, Loss: 0.4409277694136744\n",
      "Epoch 171, Loss: 0.44068045571539705\n",
      "Epoch 172, Loss: 0.4405979418732049\n",
      "Epoch 173, Loss: 0.44094101686109266\n",
      "Epoch 174, Loss: 0.4417368233339534\n",
      "Epoch 175, Loss: 0.4416582519101522\n",
      "Epoch 176, Loss: 0.4396788970821407\n",
      "Epoch 177, Loss: 0.4390195954561405\n",
      "Epoch 178, Loss: 0.4400830988007806\n",
      "Epoch 179, Loss: 0.43931775869233325\n",
      "Epoch 180, Loss: 0.4380032505977321\n",
      "Epoch 181, Loss: 0.4384470974340077\n",
      "Epoch 182, Loss: 0.4385167636581309\n",
      "Epoch 183, Loss: 0.43733043450040754\n",
      "Epoch 184, Loss: 0.4373176399006318\n",
      "Epoch 185, Loss: 0.4375256973621567\n",
      "Epoch 186, Loss: 0.43659322296038683\n",
      "Epoch 187, Loss: 0.43644391045908093\n",
      "Epoch 188, Loss: 0.43663844358015674\n",
      "Epoch 189, Loss: 0.43586286705793914\n",
      "Epoch 190, Loss: 0.4355663072844155\n",
      "Epoch 191, Loss: 0.4357040532214924\n",
      "Epoch 192, Loss: 0.435134007760397\n",
      "Epoch 193, Loss: 0.43474241127906293\n",
      "Epoch 194, Loss: 0.43480482310319635\n",
      "Epoch 195, Loss: 0.43443826104982264\n",
      "Epoch 196, Loss: 0.43390767873021246\n",
      "Epoch 197, Loss: 0.4338897691233524\n",
      "Epoch 198, Loss: 0.43377856877811044\n",
      "Epoch 199, Loss: 0.4331595506415052\n",
      "Epoch 200, Loss: 0.43289415091975497\n",
      "Epoch 201, Loss: 0.43294048069736957\n",
      "Epoch 202, Loss: 0.43262240576772243\n",
      "Epoch 203, Loss: 0.43206541812705346\n",
      "Epoch 204, Loss: 0.43172500614102416\n",
      "Epoch 205, Loss: 0.4316686388133938\n",
      "Epoch 206, Loss: 0.4315179158466364\n",
      "Epoch 207, Loss: 0.4310712381632941\n",
      "Epoch 208, Loss: 0.43059470035496\n",
      "Epoch 209, Loss: 0.4303711221416647\n",
      "Epoch 210, Loss: 0.43033088719595897\n",
      "Epoch 211, Loss: 0.43016580489992484\n",
      "Epoch 212, Loss: 0.429691817522032\n",
      "Epoch 213, Loss: 0.4292099292794459\n",
      "Epoch 214, Loss: 0.4289207205910941\n",
      "Epoch 215, Loss: 0.4287978238495365\n",
      "Epoch 216, Loss: 0.42870603658529494\n",
      "Epoch 217, Loss: 0.42841980940051866\n",
      "Epoch 218, Loss: 0.4278918633922788\n",
      "Epoch 219, Loss: 0.42748307113702383\n",
      "Epoch 220, Loss: 0.4271688511370021\n",
      "Epoch 221, Loss: 0.4269432345934848\n",
      "Epoch 222, Loss: 0.4267651275776002\n",
      "Epoch 223, Loss: 0.42668376428587257\n",
      "Epoch 224, Loss: 0.42653554195413756\n",
      "Epoch 225, Loss: 0.42613293183996703\n",
      "Epoch 226, Loss: 0.4255451632780942\n",
      "Epoch 227, Loss: 0.4251305043179084\n",
      "Epoch 228, Loss: 0.42475934718412794\n",
      "Epoch 229, Loss: 0.42452283256309487\n",
      "Epoch 230, Loss: 0.4244024016195508\n",
      "Epoch 231, Loss: 0.4242604946441205\n",
      "Epoch 232, Loss: 0.424134367950937\n",
      "Epoch 233, Loss: 0.42372608274856827\n",
      "Epoch 234, Loss: 0.4231492155631465\n",
      "Epoch 235, Loss: 0.4226291943984805\n",
      "Epoch 236, Loss: 0.4221723179942825\n",
      "Epoch 237, Loss: 0.4218955917984354\n",
      "Epoch 238, Loss: 0.4216767834434851\n",
      "Epoch 239, Loss: 0.42145169178243785\n",
      "Epoch 240, Loss: 0.4212605805865247\n",
      "Epoch 241, Loss: 0.42123380253494425\n",
      "Epoch 242, Loss: 0.42125483246926426\n",
      "Epoch 243, Loss: 0.42124070061060576\n",
      "Epoch 244, Loss: 0.42073760533336746\n",
      "Epoch 245, Loss: 0.42002867294257734\n",
      "Epoch 246, Loss: 0.41900436945460057\n",
      "Epoch 247, Loss: 0.41841940783018283\n",
      "Epoch 248, Loss: 0.4184544673382656\n",
      "Epoch 249, Loss: 0.4187823781263808\n",
      "Epoch 250, Loss: 0.41914970907160026\n",
      "Epoch 251, Loss: 0.418702737788617\n",
      "Epoch 252, Loss: 0.41762532180686956\n",
      "Epoch 253, Loss: 0.41665732452727816\n",
      "Epoch 254, Loss: 0.4164608109104539\n",
      "Epoch 255, Loss: 0.4169362050698102\n",
      "Epoch 256, Loss: 0.4170439641107097\n",
      "Epoch 257, Loss: 0.41618098523090385\n",
      "Epoch 258, Loss: 0.41521870035352226\n",
      "Epoch 259, Loss: 0.41507164322372797\n",
      "Epoch 260, Loss: 0.4154855816987037\n",
      "Epoch 261, Loss: 0.4152875627561342\n",
      "Epoch 262, Loss: 0.4146051629547779\n",
      "Epoch 263, Loss: 0.4138602536580612\n",
      "Epoch 264, Loss: 0.4135535008669373\n",
      "Epoch 265, Loss: 0.4136485063474015\n",
      "Epoch 266, Loss: 0.4135539332507534\n",
      "Epoch 267, Loss: 0.4133421191440525\n",
      "Epoch 268, Loss: 0.412685660282317\n",
      "Epoch 269, Loss: 0.4122655129807526\n",
      "Epoch 270, Loss: 0.4118217259860175\n",
      "Epoch 271, Loss: 0.41165256945543577\n",
      "Epoch 272, Loss: 0.41146929239414193\n",
      "Epoch 273, Loss: 0.4115028618809221\n",
      "Epoch 274, Loss: 0.4117686232412549\n",
      "Epoch 275, Loss: 0.4121111105932055\n",
      "Epoch 276, Loss: 0.4130580342565522\n",
      "Epoch 277, Loss: 0.4121949346593443\n",
      "Epoch 278, Loss: 0.41057389204015954\n",
      "Epoch 279, Loss: 0.4095484089667836\n",
      "Epoch 280, Loss: 0.4098107108594438\n",
      "Epoch 281, Loss: 0.41080122599126295\n",
      "Epoch 282, Loss: 0.41057013564631983\n",
      "Epoch 283, Loss: 0.4096005808315313\n",
      "Epoch 284, Loss: 0.4083534584470671\n",
      "Epoch 285, Loss: 0.4080981251863069\n",
      "Epoch 286, Loss: 0.40860646229911785\n",
      "Epoch 287, Loss: 0.40884595202638996\n",
      "Epoch 288, Loss: 0.40838367174584433\n",
      "Epoch 289, Loss: 0.40727524500698936\n",
      "Epoch 290, Loss: 0.4066578761535602\n",
      "Epoch 291, Loss: 0.4068821060860988\n",
      "Epoch 292, Loss: 0.4073894550009826\n",
      "Epoch 293, Loss: 0.4073556196958488\n",
      "Epoch 294, Loss: 0.4063989292938854\n",
      "Epoch 295, Loss: 0.40550530476516333\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.25270437039939536\n",
      "Valence RMSE: 0.26236980825418366\n",
      "Arousal RMSE: 0.24265424239970032\n",
      "Test R^2 score: tensor([0.2644, 0.5658], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.41507718810129324\n",
      "Num of epochs: 296\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Epoch 121, Loss: 0.4596922103619757\n",
      "Epoch 122, Loss: 0.45943389956138125\n",
      "Epoch 123, Loss: 0.45877818894117006\n",
      "Epoch 124, Loss: 0.45746538132874076\n",
      "Epoch 125, Loss: 0.4570462803121619\n",
      "Epoch 126, Loss: 0.45696189549769234\n",
      "Epoch 127, Loss: 0.45594920341421274\n",
      "Epoch 128, Loss: 0.45536359747685007\n",
      "Epoch 129, Loss: 0.4553290890752543\n",
      "Epoch 130, Loss: 0.4547195823074053\n",
      "Epoch 131, Loss: 0.45392838323998064\n",
      "Epoch 132, Loss: 0.4537853493595631\n",
      "Epoch 133, Loss: 0.45347437050767975\n",
      "Epoch 134, Loss: 0.45267431273812453\n",
      "Epoch 135, Loss: 0.45228737659373064\n",
      "Epoch 136, Loss: 0.4522154657432981\n",
      "Epoch 137, Loss: 0.45165964487407706\n",
      "Epoch 138, Loss: 0.4510751265462795\n",
      "Epoch 139, Loss: 0.45082650445054473\n",
      "Epoch 140, Loss: 0.4506281098405297\n",
      "Epoch 141, Loss: 0.4501911419656675\n",
      "Epoch 142, Loss: 0.4497001887659326\n",
      "Epoch 143, Loss: 0.44943436133718806\n",
      "Epoch 144, Loss: 0.449226296402944\n",
      "Epoch 145, Loss: 0.44886073897867573\n",
      "Epoch 146, Loss: 0.44844710376304864\n",
      "Epoch 147, Loss: 0.44810507013116724\n",
      "Epoch 148, Loss: 0.44788545862049767\n",
      "Epoch 149, Loss: 0.4476741441009651\n",
      "Epoch 150, Loss: 0.44735861731902055\n",
      "Epoch 151, Loss: 0.4469288720157105\n",
      "Epoch 152, Loss: 0.44655235754178146\n",
      "Epoch 153, Loss: 0.4462857402786373\n",
      "Epoch 154, Loss: 0.4460517369142754\n",
      "Epoch 155, Loss: 0.4457881796184935\n",
      "Epoch 156, Loss: 0.44542210886348255\n",
      "Epoch 157, Loss: 0.4450579467827662\n",
      "Epoch 158, Loss: 0.44473329320345206\n",
      "Epoch 159, Loss: 0.4444482514678638\n",
      "Epoch 160, Loss: 0.4441745674630603\n",
      "Epoch 161, Loss: 0.4439221813722748\n",
      "Epoch 162, Loss: 0.44363357870363757\n",
      "Epoch 163, Loss: 0.4433189071134186\n",
      "Epoch 164, Loss: 0.44295452976444744\n",
      "Epoch 165, Loss: 0.4426058108229731\n",
      "Epoch 166, Loss: 0.4422485787900922\n",
      "Epoch 167, Loss: 0.4419090985167976\n",
      "Epoch 168, Loss: 0.44158054387831963\n",
      "Epoch 169, Loss: 0.4412474388772268\n",
      "Epoch 170, Loss: 0.4409277694136744\n",
      "Epoch 171, Loss: 0.44068045571539705\n",
      "Epoch 172, Loss: 0.4405979418732049\n",
      "Epoch 173, Loss: 0.44094101686109266\n",
      "Epoch 174, Loss: 0.4417368233339534\n",
      "Epoch 175, Loss: 0.4416582519101522\n",
      "Epoch 176, Loss: 0.4396788970821407\n",
      "Epoch 177, Loss: 0.4390195954561405\n",
      "Epoch 178, Loss: 0.4400830988007806\n",
      "Epoch 179, Loss: 0.43931775869233325\n",
      "Epoch 180, Loss: 0.4380032505977321\n",
      "Epoch 181, Loss: 0.4384470974340077\n",
      "Epoch 182, Loss: 0.4385167636581309\n",
      "Epoch 183, Loss: 0.43733043450040754\n",
      "Epoch 184, Loss: 0.4373176399006318\n",
      "Epoch 185, Loss: 0.4375256973621567\n",
      "Epoch 186, Loss: 0.43659322296038683\n",
      "Epoch 187, Loss: 0.43644391045908093\n",
      "Epoch 188, Loss: 0.43663844358015674\n",
      "Epoch 189, Loss: 0.43586286705793914\n",
      "Epoch 190, Loss: 0.4355663072844155\n",
      "Epoch 191, Loss: 0.4357040532214924\n",
      "Epoch 192, Loss: 0.435134007760397\n",
      "Epoch 193, Loss: 0.43474241127906293\n",
      "Epoch 194, Loss: 0.43480482310319635\n",
      "Epoch 195, Loss: 0.43443826104982264\n",
      "Epoch 196, Loss: 0.43390767873021246\n",
      "Epoch 197, Loss: 0.4338897691233524\n",
      "Epoch 198, Loss: 0.43377856877811044\n",
      "Epoch 199, Loss: 0.4331595506415052\n",
      "Epoch 200, Loss: 0.43289415091975497\n",
      "Epoch 201, Loss: 0.43294048069736957\n",
      "Epoch 202, Loss: 0.43262240576772243\n",
      "Epoch 203, Loss: 0.43206541812705346\n",
      "Epoch 204, Loss: 0.43172500614102416\n",
      "Epoch 205, Loss: 0.4316686388133938\n",
      "Epoch 206, Loss: 0.4315179158466364\n",
      "Epoch 207, Loss: 0.4310712381632941\n",
      "Epoch 208, Loss: 0.43059470035496\n",
      "Epoch 209, Loss: 0.4303711221416647\n",
      "Epoch 210, Loss: 0.43033088719595897\n",
      "Epoch 211, Loss: 0.43016580489992484\n",
      "Epoch 212, Loss: 0.429691817522032\n",
      "Epoch 213, Loss: 0.4292099292794459\n",
      "Epoch 214, Loss: 0.4289207205910941\n",
      "Epoch 215, Loss: 0.4287978238495365\n",
      "Epoch 216, Loss: 0.42870603658529494\n",
      "Epoch 217, Loss: 0.42841980940051866\n",
      "Epoch 218, Loss: 0.4278918633922788\n",
      "Epoch 219, Loss: 0.42748307113702383\n",
      "Epoch 220, Loss: 0.4271688511370021\n",
      "Epoch 221, Loss: 0.4269432345934848\n",
      "Epoch 222, Loss: 0.4267651275776002\n",
      "Epoch 223, Loss: 0.42668376428587257\n",
      "Epoch 224, Loss: 0.42653554195413756\n",
      "Epoch 225, Loss: 0.42613293183996703\n",
      "Epoch 226, Loss: 0.4255451632780942\n",
      "Epoch 227, Loss: 0.4251305043179084\n",
      "Epoch 228, Loss: 0.42475934718412794\n",
      "Epoch 229, Loss: 0.42452283256309487\n",
      "Epoch 230, Loss: 0.4244024016195508\n",
      "Epoch 231, Loss: 0.4242604946441205\n",
      "Epoch 232, Loss: 0.424134367950937\n",
      "Epoch 233, Loss: 0.42372608274856827\n",
      "Epoch 234, Loss: 0.4231492155631465\n",
      "Epoch 235, Loss: 0.4226291943984805\n",
      "Epoch 236, Loss: 0.4221723179942825\n",
      "Epoch 237, Loss: 0.4218955917984354\n",
      "Epoch 238, Loss: 0.4216767834434851\n",
      "Epoch 239, Loss: 0.42145169178243785\n",
      "Epoch 240, Loss: 0.4212605805865247\n",
      "Epoch 241, Loss: 0.42123380253494425\n",
      "Epoch 242, Loss: 0.42125483246926426\n",
      "Epoch 243, Loss: 0.42124070061060576\n",
      "Epoch 244, Loss: 0.42073760533336746\n",
      "Epoch 245, Loss: 0.42002867294257734\n",
      "Epoch 246, Loss: 0.41900436945460057\n",
      "Epoch 247, Loss: 0.41841940783018283\n",
      "Epoch 248, Loss: 0.4184544673382656\n",
      "Epoch 249, Loss: 0.4187823781263808\n",
      "Epoch 250, Loss: 0.41914970907160026\n",
      "Epoch 251, Loss: 0.418702737788617\n",
      "Epoch 252, Loss: 0.41762532180686956\n",
      "Epoch 253, Loss: 0.41665732452727816\n",
      "Epoch 254, Loss: 0.4164608109104539\n",
      "Epoch 255, Loss: 0.4169362050698102\n",
      "Epoch 256, Loss: 0.4170439641107097\n",
      "Epoch 257, Loss: 0.41618098523090385\n",
      "Epoch 258, Loss: 0.41521870035352226\n",
      "Epoch 259, Loss: 0.41507164322372797\n",
      "Epoch 260, Loss: 0.4154855816987037\n",
      "Epoch 261, Loss: 0.4152875627561342\n",
      "Epoch 262, Loss: 0.4146051629547779\n",
      "Epoch 263, Loss: 0.4138602536580612\n",
      "Epoch 264, Loss: 0.4135535008669373\n",
      "Epoch 265, Loss: 0.4136485063474015\n",
      "Epoch 266, Loss: 0.4135539332507534\n",
      "Epoch 267, Loss: 0.4133421191440525\n",
      "Epoch 268, Loss: 0.412685660282317\n",
      "Epoch 269, Loss: 0.4122655129807526\n",
      "Epoch 270, Loss: 0.4118217259860175\n",
      "Epoch 271, Loss: 0.41165256945543577\n",
      "Epoch 272, Loss: 0.41146929239414193\n",
      "Epoch 273, Loss: 0.4115028618809221\n",
      "Epoch 274, Loss: 0.4117686232412549\n",
      "Epoch 275, Loss: 0.4121111105932055\n",
      "Epoch 276, Loss: 0.4130580342565522\n",
      "Epoch 277, Loss: 0.4121949346593443\n",
      "Epoch 278, Loss: 0.41057389204015954\n",
      "Epoch 279, Loss: 0.4095484089667836\n",
      "Epoch 280, Loss: 0.4098107108594438\n",
      "Epoch 281, Loss: 0.41080122599126295\n",
      "Epoch 282, Loss: 0.41057013564631983\n",
      "Epoch 283, Loss: 0.4096005808315313\n",
      "Epoch 284, Loss: 0.4083534584470671\n",
      "Epoch 285, Loss: 0.4080981251863069\n",
      "Epoch 286, Loss: 0.40860646229911785\n",
      "Epoch 287, Loss: 0.40884595202638996\n",
      "Epoch 288, Loss: 0.40838367174584433\n",
      "Epoch 289, Loss: 0.40727524500698936\n",
      "Epoch 290, Loss: 0.4066578761535602\n",
      "Epoch 291, Loss: 0.4068821060860988\n",
      "Epoch 292, Loss: 0.4073894550009826\n",
      "Epoch 293, Loss: 0.4073556196958488\n",
      "Epoch 294, Loss: 0.4063989292938854\n",
      "Epoch 295, Loss: 0.40550530476516333\n",
      "Epoch 296, Loss: 0.40506262754189026\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.252724819077112\n",
      "Valence RMSE: 0.2621920760379204\n",
      "Arousal RMSE: 0.24288882975147974\n",
      "Test R^2 score: tensor([0.2654, 0.5649], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4151553107121779\n",
      "Num of epochs: 297\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Epoch 121, Loss: 0.4596922103619757\n",
      "Epoch 122, Loss: 0.45943389956138125\n",
      "Epoch 123, Loss: 0.45877818894117006\n",
      "Epoch 124, Loss: 0.45746538132874076\n",
      "Epoch 125, Loss: 0.4570462803121619\n",
      "Epoch 126, Loss: 0.45696189549769234\n",
      "Epoch 127, Loss: 0.45594920341421274\n",
      "Epoch 128, Loss: 0.45536359747685007\n",
      "Epoch 129, Loss: 0.4553290890752543\n",
      "Epoch 130, Loss: 0.4547195823074053\n",
      "Epoch 131, Loss: 0.45392838323998064\n",
      "Epoch 132, Loss: 0.4537853493595631\n",
      "Epoch 133, Loss: 0.45347437050767975\n",
      "Epoch 134, Loss: 0.45267431273812453\n",
      "Epoch 135, Loss: 0.45228737659373064\n",
      "Epoch 136, Loss: 0.4522154657432981\n",
      "Epoch 137, Loss: 0.45165964487407706\n",
      "Epoch 138, Loss: 0.4510751265462795\n",
      "Epoch 139, Loss: 0.45082650445054473\n",
      "Epoch 140, Loss: 0.4506281098405297\n",
      "Epoch 141, Loss: 0.4501911419656675\n",
      "Epoch 142, Loss: 0.4497001887659326\n",
      "Epoch 143, Loss: 0.44943436133718806\n",
      "Epoch 144, Loss: 0.449226296402944\n",
      "Epoch 145, Loss: 0.44886073897867573\n",
      "Epoch 146, Loss: 0.44844710376304864\n",
      "Epoch 147, Loss: 0.44810507013116724\n",
      "Epoch 148, Loss: 0.44788545862049767\n",
      "Epoch 149, Loss: 0.4476741441009651\n",
      "Epoch 150, Loss: 0.44735861731902055\n",
      "Epoch 151, Loss: 0.4469288720157105\n",
      "Epoch 152, Loss: 0.44655235754178146\n",
      "Epoch 153, Loss: 0.4462857402786373\n",
      "Epoch 154, Loss: 0.4460517369142754\n",
      "Epoch 155, Loss: 0.4457881796184935\n",
      "Epoch 156, Loss: 0.44542210886348255\n",
      "Epoch 157, Loss: 0.4450579467827662\n",
      "Epoch 158, Loss: 0.44473329320345206\n",
      "Epoch 159, Loss: 0.4444482514678638\n",
      "Epoch 160, Loss: 0.4441745674630603\n",
      "Epoch 161, Loss: 0.4439221813722748\n",
      "Epoch 162, Loss: 0.44363357870363757\n",
      "Epoch 163, Loss: 0.4433189071134186\n",
      "Epoch 164, Loss: 0.44295452976444744\n",
      "Epoch 165, Loss: 0.4426058108229731\n",
      "Epoch 166, Loss: 0.4422485787900922\n",
      "Epoch 167, Loss: 0.4419090985167976\n",
      "Epoch 168, Loss: 0.44158054387831963\n",
      "Epoch 169, Loss: 0.4412474388772268\n",
      "Epoch 170, Loss: 0.4409277694136744\n",
      "Epoch 171, Loss: 0.44068045571539705\n",
      "Epoch 172, Loss: 0.4405979418732049\n",
      "Epoch 173, Loss: 0.44094101686109266\n",
      "Epoch 174, Loss: 0.4417368233339534\n",
      "Epoch 175, Loss: 0.4416582519101522\n",
      "Epoch 176, Loss: 0.4396788970821407\n",
      "Epoch 177, Loss: 0.4390195954561405\n",
      "Epoch 178, Loss: 0.4400830988007806\n",
      "Epoch 179, Loss: 0.43931775869233325\n",
      "Epoch 180, Loss: 0.4380032505977321\n",
      "Epoch 181, Loss: 0.4384470974340077\n",
      "Epoch 182, Loss: 0.4385167636581309\n",
      "Epoch 183, Loss: 0.43733043450040754\n",
      "Epoch 184, Loss: 0.4373176399006318\n",
      "Epoch 185, Loss: 0.4375256973621567\n",
      "Epoch 186, Loss: 0.43659322296038683\n",
      "Epoch 187, Loss: 0.43644391045908093\n",
      "Epoch 188, Loss: 0.43663844358015674\n",
      "Epoch 189, Loss: 0.43586286705793914\n",
      "Epoch 190, Loss: 0.4355663072844155\n",
      "Epoch 191, Loss: 0.4357040532214924\n",
      "Epoch 192, Loss: 0.435134007760397\n",
      "Epoch 193, Loss: 0.43474241127906293\n",
      "Epoch 194, Loss: 0.43480482310319635\n",
      "Epoch 195, Loss: 0.43443826104982264\n",
      "Epoch 196, Loss: 0.43390767873021246\n",
      "Epoch 197, Loss: 0.4338897691233524\n",
      "Epoch 198, Loss: 0.43377856877811044\n",
      "Epoch 199, Loss: 0.4331595506415052\n",
      "Epoch 200, Loss: 0.43289415091975497\n",
      "Epoch 201, Loss: 0.43294048069736957\n",
      "Epoch 202, Loss: 0.43262240576772243\n",
      "Epoch 203, Loss: 0.43206541812705346\n",
      "Epoch 204, Loss: 0.43172500614102416\n",
      "Epoch 205, Loss: 0.4316686388133938\n",
      "Epoch 206, Loss: 0.4315179158466364\n",
      "Epoch 207, Loss: 0.4310712381632941\n",
      "Epoch 208, Loss: 0.43059470035496\n",
      "Epoch 209, Loss: 0.4303711221416647\n",
      "Epoch 210, Loss: 0.43033088719595897\n",
      "Epoch 211, Loss: 0.43016580489992484\n",
      "Epoch 212, Loss: 0.429691817522032\n",
      "Epoch 213, Loss: 0.4292099292794459\n",
      "Epoch 214, Loss: 0.4289207205910941\n",
      "Epoch 215, Loss: 0.4287978238495365\n",
      "Epoch 216, Loss: 0.42870603658529494\n",
      "Epoch 217, Loss: 0.42841980940051866\n",
      "Epoch 218, Loss: 0.4278918633922788\n",
      "Epoch 219, Loss: 0.42748307113702383\n",
      "Epoch 220, Loss: 0.4271688511370021\n",
      "Epoch 221, Loss: 0.4269432345934848\n",
      "Epoch 222, Loss: 0.4267651275776002\n",
      "Epoch 223, Loss: 0.42668376428587257\n",
      "Epoch 224, Loss: 0.42653554195413756\n",
      "Epoch 225, Loss: 0.42613293183996703\n",
      "Epoch 226, Loss: 0.4255451632780942\n",
      "Epoch 227, Loss: 0.4251305043179084\n",
      "Epoch 228, Loss: 0.42475934718412794\n",
      "Epoch 229, Loss: 0.42452283256309487\n",
      "Epoch 230, Loss: 0.4244024016195508\n",
      "Epoch 231, Loss: 0.4242604946441205\n",
      "Epoch 232, Loss: 0.424134367950937\n",
      "Epoch 233, Loss: 0.42372608274856827\n",
      "Epoch 234, Loss: 0.4231492155631465\n",
      "Epoch 235, Loss: 0.4226291943984805\n",
      "Epoch 236, Loss: 0.4221723179942825\n",
      "Epoch 237, Loss: 0.4218955917984354\n",
      "Epoch 238, Loss: 0.4216767834434851\n",
      "Epoch 239, Loss: 0.42145169178243785\n",
      "Epoch 240, Loss: 0.4212605805865247\n",
      "Epoch 241, Loss: 0.42123380253494425\n",
      "Epoch 242, Loss: 0.42125483246926426\n",
      "Epoch 243, Loss: 0.42124070061060576\n",
      "Epoch 244, Loss: 0.42073760533336746\n",
      "Epoch 245, Loss: 0.42002867294257734\n",
      "Epoch 246, Loss: 0.41900436945460057\n",
      "Epoch 247, Loss: 0.41841940783018283\n",
      "Epoch 248, Loss: 0.4184544673382656\n",
      "Epoch 249, Loss: 0.4187823781263808\n",
      "Epoch 250, Loss: 0.41914970907160026\n",
      "Epoch 251, Loss: 0.418702737788617\n",
      "Epoch 252, Loss: 0.41762532180686956\n",
      "Epoch 253, Loss: 0.41665732452727816\n",
      "Epoch 254, Loss: 0.4164608109104539\n",
      "Epoch 255, Loss: 0.4169362050698102\n",
      "Epoch 256, Loss: 0.4170439641107097\n",
      "Epoch 257, Loss: 0.41618098523090385\n",
      "Epoch 258, Loss: 0.41521870035352226\n",
      "Epoch 259, Loss: 0.41507164322372797\n",
      "Epoch 260, Loss: 0.4154855816987037\n",
      "Epoch 261, Loss: 0.4152875627561342\n",
      "Epoch 262, Loss: 0.4146051629547779\n",
      "Epoch 263, Loss: 0.4138602536580612\n",
      "Epoch 264, Loss: 0.4135535008669373\n",
      "Epoch 265, Loss: 0.4136485063474015\n",
      "Epoch 266, Loss: 0.4135539332507534\n",
      "Epoch 267, Loss: 0.4133421191440525\n",
      "Epoch 268, Loss: 0.412685660282317\n",
      "Epoch 269, Loss: 0.4122655129807526\n",
      "Epoch 270, Loss: 0.4118217259860175\n",
      "Epoch 271, Loss: 0.41165256945543577\n",
      "Epoch 272, Loss: 0.41146929239414193\n",
      "Epoch 273, Loss: 0.4115028618809221\n",
      "Epoch 274, Loss: 0.4117686232412549\n",
      "Epoch 275, Loss: 0.4121111105932055\n",
      "Epoch 276, Loss: 0.4130580342565522\n",
      "Epoch 277, Loss: 0.4121949346593443\n",
      "Epoch 278, Loss: 0.41057389204015954\n",
      "Epoch 279, Loss: 0.4095484089667836\n",
      "Epoch 280, Loss: 0.4098107108594438\n",
      "Epoch 281, Loss: 0.41080122599126295\n",
      "Epoch 282, Loss: 0.41057013564631983\n",
      "Epoch 283, Loss: 0.4096005808315313\n",
      "Epoch 284, Loss: 0.4083534584470671\n",
      "Epoch 285, Loss: 0.4080981251863069\n",
      "Epoch 286, Loss: 0.40860646229911785\n",
      "Epoch 287, Loss: 0.40884595202638996\n",
      "Epoch 288, Loss: 0.40838367174584433\n",
      "Epoch 289, Loss: 0.40727524500698936\n",
      "Epoch 290, Loss: 0.4066578761535602\n",
      "Epoch 291, Loss: 0.4068821060860988\n",
      "Epoch 292, Loss: 0.4073894550009826\n",
      "Epoch 293, Loss: 0.4073556196958488\n",
      "Epoch 294, Loss: 0.4063989292938854\n",
      "Epoch 295, Loss: 0.40550530476516333\n",
      "Epoch 296, Loss: 0.40506262754189026\n",
      "Epoch 297, Loss: 0.40498378468065027\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.25294268878397036\n",
      "Valence RMSE: 0.2629136380264869\n",
      "Arousal RMSE: 0.24256221172764403\n",
      "Test R^2 score: tensor([0.2613, 0.5661], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.41371554878646205\n",
      "Num of epochs: 298\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Epoch 121, Loss: 0.4596922103619757\n",
      "Epoch 122, Loss: 0.45943389956138125\n",
      "Epoch 123, Loss: 0.45877818894117006\n",
      "Epoch 124, Loss: 0.45746538132874076\n",
      "Epoch 125, Loss: 0.4570462803121619\n",
      "Epoch 126, Loss: 0.45696189549769234\n",
      "Epoch 127, Loss: 0.45594920341421274\n",
      "Epoch 128, Loss: 0.45536359747685007\n",
      "Epoch 129, Loss: 0.4553290890752543\n",
      "Epoch 130, Loss: 0.4547195823074053\n",
      "Epoch 131, Loss: 0.45392838323998064\n",
      "Epoch 132, Loss: 0.4537853493595631\n",
      "Epoch 133, Loss: 0.45347437050767975\n",
      "Epoch 134, Loss: 0.45267431273812453\n",
      "Epoch 135, Loss: 0.45228737659373064\n",
      "Epoch 136, Loss: 0.4522154657432981\n",
      "Epoch 137, Loss: 0.45165964487407706\n",
      "Epoch 138, Loss: 0.4510751265462795\n",
      "Epoch 139, Loss: 0.45082650445054473\n",
      "Epoch 140, Loss: 0.4506281098405297\n",
      "Epoch 141, Loss: 0.4501911419656675\n",
      "Epoch 142, Loss: 0.4497001887659326\n",
      "Epoch 143, Loss: 0.44943436133718806\n",
      "Epoch 144, Loss: 0.449226296402944\n",
      "Epoch 145, Loss: 0.44886073897867573\n",
      "Epoch 146, Loss: 0.44844710376304864\n",
      "Epoch 147, Loss: 0.44810507013116724\n",
      "Epoch 148, Loss: 0.44788545862049767\n",
      "Epoch 149, Loss: 0.4476741441009651\n",
      "Epoch 150, Loss: 0.44735861731902055\n",
      "Epoch 151, Loss: 0.4469288720157105\n",
      "Epoch 152, Loss: 0.44655235754178146\n",
      "Epoch 153, Loss: 0.4462857402786373\n",
      "Epoch 154, Loss: 0.4460517369142754\n",
      "Epoch 155, Loss: 0.4457881796184935\n",
      "Epoch 156, Loss: 0.44542210886348255\n",
      "Epoch 157, Loss: 0.4450579467827662\n",
      "Epoch 158, Loss: 0.44473329320345206\n",
      "Epoch 159, Loss: 0.4444482514678638\n",
      "Epoch 160, Loss: 0.4441745674630603\n",
      "Epoch 161, Loss: 0.4439221813722748\n",
      "Epoch 162, Loss: 0.44363357870363757\n",
      "Epoch 163, Loss: 0.4433189071134186\n",
      "Epoch 164, Loss: 0.44295452976444744\n",
      "Epoch 165, Loss: 0.4426058108229731\n",
      "Epoch 166, Loss: 0.4422485787900922\n",
      "Epoch 167, Loss: 0.4419090985167976\n",
      "Epoch 168, Loss: 0.44158054387831963\n",
      "Epoch 169, Loss: 0.4412474388772268\n",
      "Epoch 170, Loss: 0.4409277694136744\n",
      "Epoch 171, Loss: 0.44068045571539705\n",
      "Epoch 172, Loss: 0.4405979418732049\n",
      "Epoch 173, Loss: 0.44094101686109266\n",
      "Epoch 174, Loss: 0.4417368233339534\n",
      "Epoch 175, Loss: 0.4416582519101522\n",
      "Epoch 176, Loss: 0.4396788970821407\n",
      "Epoch 177, Loss: 0.4390195954561405\n",
      "Epoch 178, Loss: 0.4400830988007806\n",
      "Epoch 179, Loss: 0.43931775869233325\n",
      "Epoch 180, Loss: 0.4380032505977321\n",
      "Epoch 181, Loss: 0.4384470974340077\n",
      "Epoch 182, Loss: 0.4385167636581309\n",
      "Epoch 183, Loss: 0.43733043450040754\n",
      "Epoch 184, Loss: 0.4373176399006318\n",
      "Epoch 185, Loss: 0.4375256973621567\n",
      "Epoch 186, Loss: 0.43659322296038683\n",
      "Epoch 187, Loss: 0.43644391045908093\n",
      "Epoch 188, Loss: 0.43663844358015674\n",
      "Epoch 189, Loss: 0.43586286705793914\n",
      "Epoch 190, Loss: 0.4355663072844155\n",
      "Epoch 191, Loss: 0.4357040532214924\n",
      "Epoch 192, Loss: 0.435134007760397\n",
      "Epoch 193, Loss: 0.43474241127906293\n",
      "Epoch 194, Loss: 0.43480482310319635\n",
      "Epoch 195, Loss: 0.43443826104982264\n",
      "Epoch 196, Loss: 0.43390767873021246\n",
      "Epoch 197, Loss: 0.4338897691233524\n",
      "Epoch 198, Loss: 0.43377856877811044\n",
      "Epoch 199, Loss: 0.4331595506415052\n",
      "Epoch 200, Loss: 0.43289415091975497\n",
      "Epoch 201, Loss: 0.43294048069736957\n",
      "Epoch 202, Loss: 0.43262240576772243\n",
      "Epoch 203, Loss: 0.43206541812705346\n",
      "Epoch 204, Loss: 0.43172500614102416\n",
      "Epoch 205, Loss: 0.4316686388133938\n",
      "Epoch 206, Loss: 0.4315179158466364\n",
      "Epoch 207, Loss: 0.4310712381632941\n",
      "Epoch 208, Loss: 0.43059470035496\n",
      "Epoch 209, Loss: 0.4303711221416647\n",
      "Epoch 210, Loss: 0.43033088719595897\n",
      "Epoch 211, Loss: 0.43016580489992484\n",
      "Epoch 212, Loss: 0.429691817522032\n",
      "Epoch 213, Loss: 0.4292099292794459\n",
      "Epoch 214, Loss: 0.4289207205910941\n",
      "Epoch 215, Loss: 0.4287978238495365\n",
      "Epoch 216, Loss: 0.42870603658529494\n",
      "Epoch 217, Loss: 0.42841980940051866\n",
      "Epoch 218, Loss: 0.4278918633922788\n",
      "Epoch 219, Loss: 0.42748307113702383\n",
      "Epoch 220, Loss: 0.4271688511370021\n",
      "Epoch 221, Loss: 0.4269432345934848\n",
      "Epoch 222, Loss: 0.4267651275776002\n",
      "Epoch 223, Loss: 0.42668376428587257\n",
      "Epoch 224, Loss: 0.42653554195413756\n",
      "Epoch 225, Loss: 0.42613293183996703\n",
      "Epoch 226, Loss: 0.4255451632780942\n",
      "Epoch 227, Loss: 0.4251305043179084\n",
      "Epoch 228, Loss: 0.42475934718412794\n",
      "Epoch 229, Loss: 0.42452283256309487\n",
      "Epoch 230, Loss: 0.4244024016195508\n",
      "Epoch 231, Loss: 0.4242604946441205\n",
      "Epoch 232, Loss: 0.424134367950937\n",
      "Epoch 233, Loss: 0.42372608274856827\n",
      "Epoch 234, Loss: 0.4231492155631465\n",
      "Epoch 235, Loss: 0.4226291943984805\n",
      "Epoch 236, Loss: 0.4221723179942825\n",
      "Epoch 237, Loss: 0.4218955917984354\n",
      "Epoch 238, Loss: 0.4216767834434851\n",
      "Epoch 239, Loss: 0.42145169178243785\n",
      "Epoch 240, Loss: 0.4212605805865247\n",
      "Epoch 241, Loss: 0.42123380253494425\n",
      "Epoch 242, Loss: 0.42125483246926426\n",
      "Epoch 243, Loss: 0.42124070061060576\n",
      "Epoch 244, Loss: 0.42073760533336746\n",
      "Epoch 245, Loss: 0.42002867294257734\n",
      "Epoch 246, Loss: 0.41900436945460057\n",
      "Epoch 247, Loss: 0.41841940783018283\n",
      "Epoch 248, Loss: 0.4184544673382656\n",
      "Epoch 249, Loss: 0.4187823781263808\n",
      "Epoch 250, Loss: 0.41914970907160026\n",
      "Epoch 251, Loss: 0.418702737788617\n",
      "Epoch 252, Loss: 0.41762532180686956\n",
      "Epoch 253, Loss: 0.41665732452727816\n",
      "Epoch 254, Loss: 0.4164608109104539\n",
      "Epoch 255, Loss: 0.4169362050698102\n",
      "Epoch 256, Loss: 0.4170439641107097\n",
      "Epoch 257, Loss: 0.41618098523090385\n",
      "Epoch 258, Loss: 0.41521870035352226\n",
      "Epoch 259, Loss: 0.41507164322372797\n",
      "Epoch 260, Loss: 0.4154855816987037\n",
      "Epoch 261, Loss: 0.4152875627561342\n",
      "Epoch 262, Loss: 0.4146051629547779\n",
      "Epoch 263, Loss: 0.4138602536580612\n",
      "Epoch 264, Loss: 0.4135535008669373\n",
      "Epoch 265, Loss: 0.4136485063474015\n",
      "Epoch 266, Loss: 0.4135539332507534\n",
      "Epoch 267, Loss: 0.4133421191440525\n",
      "Epoch 268, Loss: 0.412685660282317\n",
      "Epoch 269, Loss: 0.4122655129807526\n",
      "Epoch 270, Loss: 0.4118217259860175\n",
      "Epoch 271, Loss: 0.41165256945543577\n",
      "Epoch 272, Loss: 0.41146929239414193\n",
      "Epoch 273, Loss: 0.4115028618809221\n",
      "Epoch 274, Loss: 0.4117686232412549\n",
      "Epoch 275, Loss: 0.4121111105932055\n",
      "Epoch 276, Loss: 0.4130580342565522\n",
      "Epoch 277, Loss: 0.4121949346593443\n",
      "Epoch 278, Loss: 0.41057389204015954\n",
      "Epoch 279, Loss: 0.4095484089667836\n",
      "Epoch 280, Loss: 0.4098107108594438\n",
      "Epoch 281, Loss: 0.41080122599126295\n",
      "Epoch 282, Loss: 0.41057013564631983\n",
      "Epoch 283, Loss: 0.4096005808315313\n",
      "Epoch 284, Loss: 0.4083534584470671\n",
      "Epoch 285, Loss: 0.4080981251863069\n",
      "Epoch 286, Loss: 0.40860646229911785\n",
      "Epoch 287, Loss: 0.40884595202638996\n",
      "Epoch 288, Loss: 0.40838367174584433\n",
      "Epoch 289, Loss: 0.40727524500698936\n",
      "Epoch 290, Loss: 0.4066578761535602\n",
      "Epoch 291, Loss: 0.4068821060860988\n",
      "Epoch 292, Loss: 0.4073894550009826\n",
      "Epoch 293, Loss: 0.4073556196958488\n",
      "Epoch 294, Loss: 0.4063989292938854\n",
      "Epoch 295, Loss: 0.40550530476516333\n",
      "Epoch 296, Loss: 0.40506262754189026\n",
      "Epoch 297, Loss: 0.40498378468065027\n",
      "Epoch 298, Loss: 0.40512210824199785\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.25281820166881885\n",
      "Valence RMSE: 0.2618563703866127\n",
      "Arousal RMSE: 0.2434447113372156\n",
      "Test R^2 score: tensor([0.2673, 0.5629], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4150983936123572\n",
      "Num of epochs: 299\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Epoch 121, Loss: 0.4596922103619757\n",
      "Epoch 122, Loss: 0.45943389956138125\n",
      "Epoch 123, Loss: 0.45877818894117006\n",
      "Epoch 124, Loss: 0.45746538132874076\n",
      "Epoch 125, Loss: 0.4570462803121619\n",
      "Epoch 126, Loss: 0.45696189549769234\n",
      "Epoch 127, Loss: 0.45594920341421274\n",
      "Epoch 128, Loss: 0.45536359747685007\n",
      "Epoch 129, Loss: 0.4553290890752543\n",
      "Epoch 130, Loss: 0.4547195823074053\n",
      "Epoch 131, Loss: 0.45392838323998064\n",
      "Epoch 132, Loss: 0.4537853493595631\n",
      "Epoch 133, Loss: 0.45347437050767975\n",
      "Epoch 134, Loss: 0.45267431273812453\n",
      "Epoch 135, Loss: 0.45228737659373064\n",
      "Epoch 136, Loss: 0.4522154657432981\n",
      "Epoch 137, Loss: 0.45165964487407706\n",
      "Epoch 138, Loss: 0.4510751265462795\n",
      "Epoch 139, Loss: 0.45082650445054473\n",
      "Epoch 140, Loss: 0.4506281098405297\n",
      "Epoch 141, Loss: 0.4501911419656675\n",
      "Epoch 142, Loss: 0.4497001887659326\n",
      "Epoch 143, Loss: 0.44943436133718806\n",
      "Epoch 144, Loss: 0.449226296402944\n",
      "Epoch 145, Loss: 0.44886073897867573\n",
      "Epoch 146, Loss: 0.44844710376304864\n",
      "Epoch 147, Loss: 0.44810507013116724\n",
      "Epoch 148, Loss: 0.44788545862049767\n",
      "Epoch 149, Loss: 0.4476741441009651\n",
      "Epoch 150, Loss: 0.44735861731902055\n",
      "Epoch 151, Loss: 0.4469288720157105\n",
      "Epoch 152, Loss: 0.44655235754178146\n",
      "Epoch 153, Loss: 0.4462857402786373\n",
      "Epoch 154, Loss: 0.4460517369142754\n",
      "Epoch 155, Loss: 0.4457881796184935\n",
      "Epoch 156, Loss: 0.44542210886348255\n",
      "Epoch 157, Loss: 0.4450579467827662\n",
      "Epoch 158, Loss: 0.44473329320345206\n",
      "Epoch 159, Loss: 0.4444482514678638\n",
      "Epoch 160, Loss: 0.4441745674630603\n",
      "Epoch 161, Loss: 0.4439221813722748\n",
      "Epoch 162, Loss: 0.44363357870363757\n",
      "Epoch 163, Loss: 0.4433189071134186\n",
      "Epoch 164, Loss: 0.44295452976444744\n",
      "Epoch 165, Loss: 0.4426058108229731\n",
      "Epoch 166, Loss: 0.4422485787900922\n",
      "Epoch 167, Loss: 0.4419090985167976\n",
      "Epoch 168, Loss: 0.44158054387831963\n",
      "Epoch 169, Loss: 0.4412474388772268\n",
      "Epoch 170, Loss: 0.4409277694136744\n",
      "Epoch 171, Loss: 0.44068045571539705\n",
      "Epoch 172, Loss: 0.4405979418732049\n",
      "Epoch 173, Loss: 0.44094101686109266\n",
      "Epoch 174, Loss: 0.4417368233339534\n",
      "Epoch 175, Loss: 0.4416582519101522\n",
      "Epoch 176, Loss: 0.4396788970821407\n",
      "Epoch 177, Loss: 0.4390195954561405\n",
      "Epoch 178, Loss: 0.4400830988007806\n",
      "Epoch 179, Loss: 0.43931775869233325\n",
      "Epoch 180, Loss: 0.4380032505977321\n",
      "Epoch 181, Loss: 0.4384470974340077\n",
      "Epoch 182, Loss: 0.4385167636581309\n",
      "Epoch 183, Loss: 0.43733043450040754\n",
      "Epoch 184, Loss: 0.4373176399006318\n",
      "Epoch 185, Loss: 0.4375256973621567\n",
      "Epoch 186, Loss: 0.43659322296038683\n",
      "Epoch 187, Loss: 0.43644391045908093\n",
      "Epoch 188, Loss: 0.43663844358015674\n",
      "Epoch 189, Loss: 0.43586286705793914\n",
      "Epoch 190, Loss: 0.4355663072844155\n",
      "Epoch 191, Loss: 0.4357040532214924\n",
      "Epoch 192, Loss: 0.435134007760397\n",
      "Epoch 193, Loss: 0.43474241127906293\n",
      "Epoch 194, Loss: 0.43480482310319635\n",
      "Epoch 195, Loss: 0.43443826104982264\n",
      "Epoch 196, Loss: 0.43390767873021246\n",
      "Epoch 197, Loss: 0.4338897691233524\n",
      "Epoch 198, Loss: 0.43377856877811044\n",
      "Epoch 199, Loss: 0.4331595506415052\n",
      "Epoch 200, Loss: 0.43289415091975497\n",
      "Epoch 201, Loss: 0.43294048069736957\n",
      "Epoch 202, Loss: 0.43262240576772243\n",
      "Epoch 203, Loss: 0.43206541812705346\n",
      "Epoch 204, Loss: 0.43172500614102416\n",
      "Epoch 205, Loss: 0.4316686388133938\n",
      "Epoch 206, Loss: 0.4315179158466364\n",
      "Epoch 207, Loss: 0.4310712381632941\n",
      "Epoch 208, Loss: 0.43059470035496\n",
      "Epoch 209, Loss: 0.4303711221416647\n",
      "Epoch 210, Loss: 0.43033088719595897\n",
      "Epoch 211, Loss: 0.43016580489992484\n",
      "Epoch 212, Loss: 0.429691817522032\n",
      "Epoch 213, Loss: 0.4292099292794459\n",
      "Epoch 214, Loss: 0.4289207205910941\n",
      "Epoch 215, Loss: 0.4287978238495365\n",
      "Epoch 216, Loss: 0.42870603658529494\n",
      "Epoch 217, Loss: 0.42841980940051866\n",
      "Epoch 218, Loss: 0.4278918633922788\n",
      "Epoch 219, Loss: 0.42748307113702383\n",
      "Epoch 220, Loss: 0.4271688511370021\n",
      "Epoch 221, Loss: 0.4269432345934848\n",
      "Epoch 222, Loss: 0.4267651275776002\n",
      "Epoch 223, Loss: 0.42668376428587257\n",
      "Epoch 224, Loss: 0.42653554195413756\n",
      "Epoch 225, Loss: 0.42613293183996703\n",
      "Epoch 226, Loss: 0.4255451632780942\n",
      "Epoch 227, Loss: 0.4251305043179084\n",
      "Epoch 228, Loss: 0.42475934718412794\n",
      "Epoch 229, Loss: 0.42452283256309487\n",
      "Epoch 230, Loss: 0.4244024016195508\n",
      "Epoch 231, Loss: 0.4242604946441205\n",
      "Epoch 232, Loss: 0.424134367950937\n",
      "Epoch 233, Loss: 0.42372608274856827\n",
      "Epoch 234, Loss: 0.4231492155631465\n",
      "Epoch 235, Loss: 0.4226291943984805\n",
      "Epoch 236, Loss: 0.4221723179942825\n",
      "Epoch 237, Loss: 0.4218955917984354\n",
      "Epoch 238, Loss: 0.4216767834434851\n",
      "Epoch 239, Loss: 0.42145169178243785\n",
      "Epoch 240, Loss: 0.4212605805865247\n",
      "Epoch 241, Loss: 0.42123380253494425\n",
      "Epoch 242, Loss: 0.42125483246926426\n",
      "Epoch 243, Loss: 0.42124070061060576\n",
      "Epoch 244, Loss: 0.42073760533336746\n",
      "Epoch 245, Loss: 0.42002867294257734\n",
      "Epoch 246, Loss: 0.41900436945460057\n",
      "Epoch 247, Loss: 0.41841940783018283\n",
      "Epoch 248, Loss: 0.4184544673382656\n",
      "Epoch 249, Loss: 0.4187823781263808\n",
      "Epoch 250, Loss: 0.41914970907160026\n",
      "Epoch 251, Loss: 0.418702737788617\n",
      "Epoch 252, Loss: 0.41762532180686956\n",
      "Epoch 253, Loss: 0.41665732452727816\n",
      "Epoch 254, Loss: 0.4164608109104539\n",
      "Epoch 255, Loss: 0.4169362050698102\n",
      "Epoch 256, Loss: 0.4170439641107097\n",
      "Epoch 257, Loss: 0.41618098523090385\n",
      "Epoch 258, Loss: 0.41521870035352226\n",
      "Epoch 259, Loss: 0.41507164322372797\n",
      "Epoch 260, Loss: 0.4154855816987037\n",
      "Epoch 261, Loss: 0.4152875627561342\n",
      "Epoch 262, Loss: 0.4146051629547779\n",
      "Epoch 263, Loss: 0.4138602536580612\n",
      "Epoch 264, Loss: 0.4135535008669373\n",
      "Epoch 265, Loss: 0.4136485063474015\n",
      "Epoch 266, Loss: 0.4135539332507534\n",
      "Epoch 267, Loss: 0.4133421191440525\n",
      "Epoch 268, Loss: 0.412685660282317\n",
      "Epoch 269, Loss: 0.4122655129807526\n",
      "Epoch 270, Loss: 0.4118217259860175\n",
      "Epoch 271, Loss: 0.41165256945543577\n",
      "Epoch 272, Loss: 0.41146929239414193\n",
      "Epoch 273, Loss: 0.4115028618809221\n",
      "Epoch 274, Loss: 0.4117686232412549\n",
      "Epoch 275, Loss: 0.4121111105932055\n",
      "Epoch 276, Loss: 0.4130580342565522\n",
      "Epoch 277, Loss: 0.4121949346593443\n",
      "Epoch 278, Loss: 0.41057389204015954\n",
      "Epoch 279, Loss: 0.4095484089667836\n",
      "Epoch 280, Loss: 0.4098107108594438\n",
      "Epoch 281, Loss: 0.41080122599126295\n",
      "Epoch 282, Loss: 0.41057013564631983\n",
      "Epoch 283, Loss: 0.4096005808315313\n",
      "Epoch 284, Loss: 0.4083534584470671\n",
      "Epoch 285, Loss: 0.4080981251863069\n",
      "Epoch 286, Loss: 0.40860646229911785\n",
      "Epoch 287, Loss: 0.40884595202638996\n",
      "Epoch 288, Loss: 0.40838367174584433\n",
      "Epoch 289, Loss: 0.40727524500698936\n",
      "Epoch 290, Loss: 0.4066578761535602\n",
      "Epoch 291, Loss: 0.4068821060860988\n",
      "Epoch 292, Loss: 0.4073894550009826\n",
      "Epoch 293, Loss: 0.4073556196958488\n",
      "Epoch 294, Loss: 0.4063989292938854\n",
      "Epoch 295, Loss: 0.40550530476516333\n",
      "Epoch 296, Loss: 0.40506262754189026\n",
      "Epoch 297, Loss: 0.40498378468065027\n",
      "Epoch 298, Loss: 0.40512210824199785\n",
      "Epoch 299, Loss: 0.4051975225103147\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2532379261830193\n",
      "Valence RMSE: 0.2629814792954195\n",
      "Arousal RMSE: 0.2431041671023875\n",
      "Test R^2 score: tensor([0.2610, 0.5641], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.41255434534159435\n",
      "Num of epochs: 300\n",
      "Epoch 1, Loss: 0.6117602639952692\n",
      "Epoch 2, Loss: 0.6101464254880237\n",
      "Epoch 3, Loss: 0.6086213023617328\n",
      "Epoch 4, Loss: 0.6072139322728687\n",
      "Epoch 5, Loss: 0.6059059041576254\n",
      "Epoch 6, Loss: 0.6046711251656307\n",
      "Epoch 7, Loss: 0.6035107609327528\n",
      "Epoch 8, Loss: 0.6024208138295384\n",
      "Epoch 9, Loss: 0.601443811573193\n",
      "Epoch 10, Loss: 0.6005301100672485\n",
      "Epoch 11, Loss: 0.5996730410297331\n",
      "Epoch 12, Loss: 0.5988706082219051\n",
      "Epoch 13, Loss: 0.5981198924692613\n",
      "Epoch 14, Loss: 0.5974230342438203\n",
      "Epoch 15, Loss: 0.5967813208598403\n",
      "Epoch 16, Loss: 0.5961819834671394\n",
      "Epoch 17, Loss: 0.5956223229811433\n",
      "Epoch 18, Loss: 0.5951022259866258\n",
      "Epoch 19, Loss: 0.5946245278209634\n",
      "Epoch 20, Loss: 0.5941769921723986\n",
      "Epoch 21, Loss: 0.5937608416721933\n",
      "Epoch 22, Loss: 0.5933797585437393\n",
      "Epoch 23, Loss: 0.5930243123073795\n",
      "Epoch 24, Loss: 0.5927319331462616\n",
      "Epoch 25, Loss: 0.5924678310079513\n",
      "Epoch 26, Loss: 0.5922268102074322\n",
      "Epoch 27, Loss: 0.5920063315388031\n",
      "Epoch 28, Loss: 0.5918112523312218\n",
      "Epoch 29, Loss: 0.5916378953367778\n",
      "Epoch 30, Loss: 0.591491494529562\n",
      "Epoch 31, Loss: 0.5913687186382279\n",
      "Epoch 32, Loss: 0.591285106592528\n",
      "Epoch 33, Loss: 0.591195988032881\n",
      "Epoch 34, Loss: 0.5911439875995429\n",
      "Epoch 35, Loss: 0.5911168890993741\n",
      "Epoch 36, Loss: 0.5911096794287155\n",
      "Epoch 37, Loss: 0.5911307283956\n",
      "Epoch 38, Loss: 0.5911297704948227\n",
      "Epoch 39, Loss: 0.5911022175790682\n",
      "Epoch 40, Loss: 0.5910778650765612\n",
      "Epoch 41, Loss: 0.5910509400239167\n",
      "Epoch 42, Loss: 0.5910167273027689\n",
      "Epoch 43, Loss: 0.5909679133891559\n",
      "Epoch 44, Loss: 0.5908954919320636\n",
      "Epoch 45, Loss: 0.5907984958120089\n",
      "Epoch 46, Loss: 0.5906717413113092\n",
      "Epoch 47, Loss: 0.590527624348138\n",
      "Epoch 48, Loss: 0.5903612607624893\n",
      "Epoch 49, Loss: 0.5901519020976843\n",
      "Epoch 50, Loss: 0.5898878573669333\n",
      "Epoch 51, Loss: 0.5895574013586506\n",
      "Epoch 52, Loss: 0.589148104895495\n",
      "Epoch 53, Loss: 0.5886434000136315\n",
      "Epoch 54, Loss: 0.5880622739918133\n",
      "Epoch 55, Loss: 0.5873901670340141\n",
      "Epoch 56, Loss: 0.5865883836029777\n",
      "Epoch 57, Loss: 0.5856215833322037\n",
      "Epoch 58, Loss: 0.5845016943937205\n",
      "Epoch 59, Loss: 0.5831606103995632\n",
      "Epoch 60, Loss: 0.5814975108997869\n",
      "Epoch 61, Loss: 0.5794439991971324\n",
      "Epoch 62, Loss: 0.5770026473045774\n",
      "Epoch 63, Loss: 0.5741798491503366\n",
      "Epoch 64, Loss: 0.570919575986668\n",
      "Epoch 65, Loss: 0.5672697916575286\n",
      "Epoch 66, Loss: 0.5632545919895717\n",
      "Epoch 67, Loss: 0.5587920257159121\n",
      "Epoch 68, Loss: 0.5539013032463652\n",
      "Epoch 69, Loss: 0.5486390680789626\n",
      "Epoch 70, Loss: 0.5432541735103024\n",
      "Epoch 71, Loss: 0.5378810818004984\n",
      "Epoch 72, Loss: 0.5329361411357282\n",
      "Epoch 73, Loss: 0.5287502605356427\n",
      "Epoch 74, Loss: 0.5257270331579124\n",
      "Epoch 75, Loss: 0.52345611766899\n",
      "Epoch 76, Loss: 0.5214510846316833\n",
      "Epoch 77, Loss: 0.5188763441630363\n",
      "Epoch 78, Loss: 0.5155629785401159\n",
      "Epoch 79, Loss: 0.5115299021984938\n",
      "Epoch 80, Loss: 0.5073941194550649\n",
      "Epoch 81, Loss: 0.5040897951818161\n",
      "Epoch 82, Loss: 0.5017768164303005\n",
      "Epoch 83, Loss: 0.5000121890012849\n",
      "Epoch 84, Loss: 0.4980856038911349\n",
      "Epoch 85, Loss: 0.4957433349076689\n",
      "Epoch 86, Loss: 0.4932817677448928\n",
      "Epoch 87, Loss: 0.4911335757179917\n",
      "Epoch 88, Loss: 0.48967332384906737\n",
      "Epoch 89, Loss: 0.4887405066775534\n",
      "Epoch 90, Loss: 0.4875983383568169\n",
      "Epoch 91, Loss: 0.4859970988434278\n",
      "Epoch 92, Loss: 0.4841904750174677\n",
      "Epoch 93, Loss: 0.48269647049709014\n",
      "Epoch 94, Loss: 0.4816942480576571\n",
      "Epoch 95, Loss: 0.48075039679976583\n",
      "Epoch 96, Loss: 0.47956218325125827\n",
      "Epoch 97, Loss: 0.478282466838277\n",
      "Epoch 98, Loss: 0.4771795655385937\n",
      "Epoch 99, Loss: 0.4762481215332311\n",
      "Epoch 100, Loss: 0.4753653858916849\n",
      "Epoch 101, Loss: 0.4745014680240782\n",
      "Epoch 102, Loss: 0.4736831480984736\n",
      "Epoch 103, Loss: 0.47296220949437623\n",
      "Epoch 104, Loss: 0.47223254988010666\n",
      "Epoch 105, Loss: 0.4714026575454215\n",
      "Epoch 106, Loss: 0.4705079493160215\n",
      "Epoch 107, Loss: 0.46967876928746666\n",
      "Epoch 108, Loss: 0.46901476058628677\n",
      "Epoch 109, Loss: 0.4683434789917324\n",
      "Epoch 110, Loss: 0.46750070668743393\n",
      "Epoch 111, Loss: 0.4665993468087093\n",
      "Epoch 112, Loss: 0.4658408330400088\n",
      "Epoch 113, Loss: 0.4652086957095019\n",
      "Epoch 114, Loss: 0.4645724586816371\n",
      "Epoch 115, Loss: 0.463772444837378\n",
      "Epoch 116, Loss: 0.46297748571594044\n",
      "Epoch 117, Loss: 0.46222643945480046\n",
      "Epoch 118, Loss: 0.46148185591093294\n",
      "Epoch 119, Loss: 0.4607256386288545\n",
      "Epoch 120, Loss: 0.4601132733882693\n",
      "Epoch 121, Loss: 0.4596922103619757\n",
      "Epoch 122, Loss: 0.45943389956138125\n",
      "Epoch 123, Loss: 0.45877818894117006\n",
      "Epoch 124, Loss: 0.45746538132874076\n",
      "Epoch 125, Loss: 0.4570462803121619\n",
      "Epoch 126, Loss: 0.45696189549769234\n",
      "Epoch 127, Loss: 0.45594920341421274\n",
      "Epoch 128, Loss: 0.45536359747685007\n",
      "Epoch 129, Loss: 0.4553290890752543\n",
      "Epoch 130, Loss: 0.4547195823074053\n",
      "Epoch 131, Loss: 0.45392838323998064\n",
      "Epoch 132, Loss: 0.4537853493595631\n",
      "Epoch 133, Loss: 0.45347437050767975\n",
      "Epoch 134, Loss: 0.45267431273812453\n",
      "Epoch 135, Loss: 0.45228737659373064\n",
      "Epoch 136, Loss: 0.4522154657432981\n",
      "Epoch 137, Loss: 0.45165964487407706\n",
      "Epoch 138, Loss: 0.4510751265462795\n",
      "Epoch 139, Loss: 0.45082650445054473\n",
      "Epoch 140, Loss: 0.4506281098405297\n",
      "Epoch 141, Loss: 0.4501911419656675\n",
      "Epoch 142, Loss: 0.4497001887659326\n",
      "Epoch 143, Loss: 0.44943436133718806\n",
      "Epoch 144, Loss: 0.449226296402944\n",
      "Epoch 145, Loss: 0.44886073897867573\n",
      "Epoch 146, Loss: 0.44844710376304864\n",
      "Epoch 147, Loss: 0.44810507013116724\n",
      "Epoch 148, Loss: 0.44788545862049767\n",
      "Epoch 149, Loss: 0.4476741441009651\n",
      "Epoch 150, Loss: 0.44735861731902055\n",
      "Epoch 151, Loss: 0.4469288720157105\n",
      "Epoch 152, Loss: 0.44655235754178146\n",
      "Epoch 153, Loss: 0.4462857402786373\n",
      "Epoch 154, Loss: 0.4460517369142754\n",
      "Epoch 155, Loss: 0.4457881796184935\n",
      "Epoch 156, Loss: 0.44542210886348255\n",
      "Epoch 157, Loss: 0.4450579467827662\n",
      "Epoch 158, Loss: 0.44473329320345206\n",
      "Epoch 159, Loss: 0.4444482514678638\n",
      "Epoch 160, Loss: 0.4441745674630603\n",
      "Epoch 161, Loss: 0.4439221813722748\n",
      "Epoch 162, Loss: 0.44363357870363757\n",
      "Epoch 163, Loss: 0.4433189071134186\n",
      "Epoch 164, Loss: 0.44295452976444744\n",
      "Epoch 165, Loss: 0.4426058108229731\n",
      "Epoch 166, Loss: 0.4422485787900922\n",
      "Epoch 167, Loss: 0.4419090985167976\n",
      "Epoch 168, Loss: 0.44158054387831963\n",
      "Epoch 169, Loss: 0.4412474388772268\n",
      "Epoch 170, Loss: 0.4409277694136744\n",
      "Epoch 171, Loss: 0.44068045571539705\n",
      "Epoch 172, Loss: 0.4405979418732049\n",
      "Epoch 173, Loss: 0.44094101686109266\n",
      "Epoch 174, Loss: 0.4417368233339534\n",
      "Epoch 175, Loss: 0.4416582519101522\n",
      "Epoch 176, Loss: 0.4396788970821407\n",
      "Epoch 177, Loss: 0.4390195954561405\n",
      "Epoch 178, Loss: 0.4400830988007806\n",
      "Epoch 179, Loss: 0.43931775869233325\n",
      "Epoch 180, Loss: 0.4380032505977321\n",
      "Epoch 181, Loss: 0.4384470974340077\n",
      "Epoch 182, Loss: 0.4385167636581309\n",
      "Epoch 183, Loss: 0.43733043450040754\n",
      "Epoch 184, Loss: 0.4373176399006318\n",
      "Epoch 185, Loss: 0.4375256973621567\n",
      "Epoch 186, Loss: 0.43659322296038683\n",
      "Epoch 187, Loss: 0.43644391045908093\n",
      "Epoch 188, Loss: 0.43663844358015674\n",
      "Epoch 189, Loss: 0.43586286705793914\n",
      "Epoch 190, Loss: 0.4355663072844155\n",
      "Epoch 191, Loss: 0.4357040532214924\n",
      "Epoch 192, Loss: 0.435134007760397\n",
      "Epoch 193, Loss: 0.43474241127906293\n",
      "Epoch 194, Loss: 0.43480482310319635\n",
      "Epoch 195, Loss: 0.43443826104982264\n",
      "Epoch 196, Loss: 0.43390767873021246\n",
      "Epoch 197, Loss: 0.4338897691233524\n",
      "Epoch 198, Loss: 0.43377856877811044\n",
      "Epoch 199, Loss: 0.4331595506415052\n",
      "Epoch 200, Loss: 0.43289415091975497\n",
      "Epoch 201, Loss: 0.43294048069736957\n",
      "Epoch 202, Loss: 0.43262240576772243\n",
      "Epoch 203, Loss: 0.43206541812705346\n",
      "Epoch 204, Loss: 0.43172500614102416\n",
      "Epoch 205, Loss: 0.4316686388133938\n",
      "Epoch 206, Loss: 0.4315179158466364\n",
      "Epoch 207, Loss: 0.4310712381632941\n",
      "Epoch 208, Loss: 0.43059470035496\n",
      "Epoch 209, Loss: 0.4303711221416647\n",
      "Epoch 210, Loss: 0.43033088719595897\n",
      "Epoch 211, Loss: 0.43016580489992484\n",
      "Epoch 212, Loss: 0.429691817522032\n",
      "Epoch 213, Loss: 0.4292099292794459\n",
      "Epoch 214, Loss: 0.4289207205910941\n",
      "Epoch 215, Loss: 0.4287978238495365\n",
      "Epoch 216, Loss: 0.42870603658529494\n",
      "Epoch 217, Loss: 0.42841980940051866\n",
      "Epoch 218, Loss: 0.4278918633922788\n",
      "Epoch 219, Loss: 0.42748307113702383\n",
      "Epoch 220, Loss: 0.4271688511370021\n",
      "Epoch 221, Loss: 0.4269432345934848\n",
      "Epoch 222, Loss: 0.4267651275776002\n",
      "Epoch 223, Loss: 0.42668376428587257\n",
      "Epoch 224, Loss: 0.42653554195413756\n",
      "Epoch 225, Loss: 0.42613293183996703\n",
      "Epoch 226, Loss: 0.4255451632780942\n",
      "Epoch 227, Loss: 0.4251305043179084\n",
      "Epoch 228, Loss: 0.42475934718412794\n",
      "Epoch 229, Loss: 0.42452283256309487\n",
      "Epoch 230, Loss: 0.4244024016195508\n",
      "Epoch 231, Loss: 0.4242604946441205\n",
      "Epoch 232, Loss: 0.424134367950937\n",
      "Epoch 233, Loss: 0.42372608274856827\n",
      "Epoch 234, Loss: 0.4231492155631465\n",
      "Epoch 235, Loss: 0.4226291943984805\n",
      "Epoch 236, Loss: 0.4221723179942825\n",
      "Epoch 237, Loss: 0.4218955917984354\n",
      "Epoch 238, Loss: 0.4216767834434851\n",
      "Epoch 239, Loss: 0.42145169178243785\n",
      "Epoch 240, Loss: 0.4212605805865247\n",
      "Epoch 241, Loss: 0.42123380253494425\n",
      "Epoch 242, Loss: 0.42125483246926426\n",
      "Epoch 243, Loss: 0.42124070061060576\n",
      "Epoch 244, Loss: 0.42073760533336746\n",
      "Epoch 245, Loss: 0.42002867294257734\n",
      "Epoch 246, Loss: 0.41900436945460057\n",
      "Epoch 247, Loss: 0.41841940783018283\n",
      "Epoch 248, Loss: 0.4184544673382656\n",
      "Epoch 249, Loss: 0.4187823781263808\n",
      "Epoch 250, Loss: 0.41914970907160026\n",
      "Epoch 251, Loss: 0.418702737788617\n",
      "Epoch 252, Loss: 0.41762532180686956\n",
      "Epoch 253, Loss: 0.41665732452727816\n",
      "Epoch 254, Loss: 0.4164608109104539\n",
      "Epoch 255, Loss: 0.4169362050698102\n",
      "Epoch 256, Loss: 0.4170439641107097\n",
      "Epoch 257, Loss: 0.41618098523090385\n",
      "Epoch 258, Loss: 0.41521870035352226\n",
      "Epoch 259, Loss: 0.41507164322372797\n",
      "Epoch 260, Loss: 0.4154855816987037\n",
      "Epoch 261, Loss: 0.4152875627561342\n",
      "Epoch 262, Loss: 0.4146051629547779\n",
      "Epoch 263, Loss: 0.4138602536580612\n",
      "Epoch 264, Loss: 0.4135535008669373\n",
      "Epoch 265, Loss: 0.4136485063474015\n",
      "Epoch 266, Loss: 0.4135539332507534\n",
      "Epoch 267, Loss: 0.4133421191440525\n",
      "Epoch 268, Loss: 0.412685660282317\n",
      "Epoch 269, Loss: 0.4122655129807526\n",
      "Epoch 270, Loss: 0.4118217259860175\n",
      "Epoch 271, Loss: 0.41165256945543577\n",
      "Epoch 272, Loss: 0.41146929239414193\n",
      "Epoch 273, Loss: 0.4115028618809221\n",
      "Epoch 274, Loss: 0.4117686232412549\n",
      "Epoch 275, Loss: 0.4121111105932055\n",
      "Epoch 276, Loss: 0.4130580342565522\n",
      "Epoch 277, Loss: 0.4121949346593443\n",
      "Epoch 278, Loss: 0.41057389204015954\n",
      "Epoch 279, Loss: 0.4095484089667836\n",
      "Epoch 280, Loss: 0.4098107108594438\n",
      "Epoch 281, Loss: 0.41080122599126295\n",
      "Epoch 282, Loss: 0.41057013564631983\n",
      "Epoch 283, Loss: 0.4096005808315313\n",
      "Epoch 284, Loss: 0.4083534584470671\n",
      "Epoch 285, Loss: 0.4080981251863069\n",
      "Epoch 286, Loss: 0.40860646229911785\n",
      "Epoch 287, Loss: 0.40884595202638996\n",
      "Epoch 288, Loss: 0.40838367174584433\n",
      "Epoch 289, Loss: 0.40727524500698936\n",
      "Epoch 290, Loss: 0.4066578761535602\n",
      "Epoch 291, Loss: 0.4068821060860988\n",
      "Epoch 292, Loss: 0.4073894550009826\n",
      "Epoch 293, Loss: 0.4073556196958488\n",
      "Epoch 294, Loss: 0.4063989292938854\n",
      "Epoch 295, Loss: 0.40550530476516333\n",
      "Epoch 296, Loss: 0.40506262754189026\n",
      "Epoch 297, Loss: 0.40498378468065027\n",
      "Epoch 298, Loss: 0.40512210824199785\n",
      "Epoch 299, Loss: 0.4051975225103147\n",
      "Epoch 300, Loss: 0.4051584471302062\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2532310950925696\n",
      "Valence RMSE: 0.2620278013566254\n",
      "Arousal RMSE: 0.24411760763979506\n",
      "Test R^2 score: tensor([0.2663, 0.5605], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4134087560477187\n",
      "Completed.\n"
     ]
    }
   ],
   "source": [
    "for num_epochs in num_epochs_list:\n",
    "  # Set the seed\n",
    "  torch.manual_seed(seed)\n",
    "\n",
    "  print(f'Num of epochs: {num_epochs}')\n",
    "  \n",
    "  model = train_model(num_epochs)\n",
    "\n",
    "  print(\"Training completed.\")\n",
    "  print(\"Testing model...\")\n",
    "\n",
    "  test_pred, rmse, adjusted_r2_score, r2_score = test_model(model)\n",
    "  adjusted_r2_scores_valence_list.append(adjusted_r2_score[0])\n",
    "  adjusted_r2_scores_arousal_list.append(adjusted_r2_score[1])\n",
    "  r2_scores_list.append(r2_score)\n",
    "  rmse_list.append(rmse)\n",
    "\n",
    "print(\"Completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the graph to visualise the relationship between the evaluation metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import relevant libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RMSE vs. num_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(num_epochs_list, rmse_list, color='b', linestyle='-')\n",
    "plt.title('num_epochs vs. Test RMSE')\n",
    "plt.xlabel('num_epochs')\n",
    "plt.ylabel('Tets RMSE') \n",
    "plt.grid(True)\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min RMSE score: 0.2313020158721768\n",
      "Corresponding R^2 SCore: 0.5067090666334924\n",
      "Corresponding num_epochs: 126\n"
     ]
    }
   ],
   "source": [
    "min_rmse = min(rmse_list)\n",
    "corresponding_r2_score = r2_scores_list[rmse_list.index(min_rmse)]\n",
    "corresponding_num_epochs = num_epochs_list[rmse_list.index(min_rmse)]\n",
    "\n",
    "print(f'Min RMSE score: {min_rmse}')\n",
    "print(f'Corresponding R^2 SCore: {corresponding_r2_score}')\n",
    "print(f'Corresponding num_epochs: {corresponding_num_epochs}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test R^2 Score vs. num_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(num_epochs_list, r2_scores_list, color='b', linestyle='-')\n",
    "plt.title('num_epochs vs. Test R^2 Score')\n",
    "plt.xlabel('num_epochs')\n",
    "plt.ylabel('Test R^2 SCore') \n",
    "plt.grid(True)\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max R^2 score: 0.5067090666334924\n",
      "Corresponding RMSE: 0.2313020158721768\n",
      "Corresponding num_epochs: 126\n"
     ]
    }
   ],
   "source": [
    "max_r2_score = max(r2_scores_list)\n",
    "corresponding_rmse = rmse_list[r2_scores_list.index(max_r2_score)]\n",
    "corresponding_num_epochs = num_epochs_list[r2_scores_list.index(max_r2_score)]\n",
    "\n",
    "print(f'Max R^2 score: {max_r2_score}')\n",
    "print(f'Corresponding RMSE: {corresponding_rmse}')\n",
    "print(f'Corresponding num_epochs: {corresponding_num_epochs}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Adjsuted R^2 Score (Valence) vs. num_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(num_epochs_list, adjusted_r2_scores_valence_list, color='b', linestyle='-')\n",
    "plt.title('num_epochs vs. Test Adjusted R^2 Score (Valence)')\n",
    "plt.xlabel('num_epochs')\n",
    "plt.ylabel('Test Adjusted R^2 Score (Valence)') \n",
    "plt.grid(True)\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max R^2 score: 0.3720297325121601\n",
      "Corresponding RMSE: 0.2329043355494024\n",
      "Corresponding num_epochs: 104\n"
     ]
    }
   ],
   "source": [
    "max_r2_score_valence = max(adjusted_r2_scores_valence_list)\n",
    "corresponding_rmse = rmse_list[adjusted_r2_scores_valence_list.index(max_r2_score_valence)]\n",
    "corresponding_num_epochs = num_epochs_list[adjusted_r2_scores_valence_list.index(max_r2_score_valence)]\n",
    "\n",
    "print(f'Max R^2 score: {max_r2_score_valence}')\n",
    "print(f'Corresponding RMSE: {corresponding_rmse}')\n",
    "print(f'Corresponding num_epochs: {corresponding_num_epochs}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Adjusted R^2 Score (Arousal) vs. num_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(num_epochs_list, adjusted_r2_scores_arousal_list, color='b', linestyle='-')\n",
    "plt.title('num_epochs vs. Test Adjusted R^2 Score (Arousal)')\n",
    "plt.xlabel('num_epochs')\n",
    "plt.ylabel('Test Adjusted R^2 SCore (Arousal)') \n",
    "plt.grid(True)\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max R^2 score: 0.6515897514027089\n",
      "Corresponding RMSE: 0.23177721399443038\n",
      "Corresponding num_epochs: 130\n"
     ]
    }
   ],
   "source": [
    "max_r2_score_arousal = max(adjusted_r2_scores_arousal_list)\n",
    "corresponding_rmse = rmse_list[adjusted_r2_scores_arousal_list.index(max_r2_score_arousal)]\n",
    "corresponding_num_epochs = num_epochs_list[adjusted_r2_scores_arousal_list.index(max_r2_score_arousal)]\n",
    "\n",
    "print(f'Max R^2 score: {max_r2_score_arousal}')\n",
    "print(f'Corresponding RMSE: {corresponding_rmse}')\n",
    "print(f'Corresponding num_epochs: {corresponding_num_epochs}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
