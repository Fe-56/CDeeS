{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PMEmo Dataset - Feed Forward Neural Network\n",
    "## Essentia Best Valence Mean Featureset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import relevant libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torcheval.metrics import R2Score\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import math\n",
    "\n",
    "import sys\n",
    "sys.path.insert(1, '../../utils')\n",
    "from paths import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import annotations dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>song_id</th>\n",
       "      <th>valence_mean_mapped</th>\n",
       "      <th>arousal_mean_mapped</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.150</td>\n",
       "      <td>-0.200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>-0.425</td>\n",
       "      <td>-0.475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>-0.600</td>\n",
       "      <td>-0.700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>-0.300</td>\n",
       "      <td>0.025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>0.450</td>\n",
       "      <td>0.400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>762</th>\n",
       "      <td>993</td>\n",
       "      <td>0.525</td>\n",
       "      <td>0.725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>763</th>\n",
       "      <td>996</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>764</th>\n",
       "      <td>997</td>\n",
       "      <td>0.325</td>\n",
       "      <td>0.425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>765</th>\n",
       "      <td>999</td>\n",
       "      <td>0.550</td>\n",
       "      <td>0.750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>766</th>\n",
       "      <td>1000</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.325</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>767 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     song_id  valence_mean_mapped  arousal_mean_mapped\n",
       "0          1                0.150               -0.200\n",
       "1          4               -0.425               -0.475\n",
       "2          5               -0.600               -0.700\n",
       "3          6               -0.300                0.025\n",
       "4          7                0.450                0.400\n",
       "..       ...                  ...                  ...\n",
       "762      993                0.525                0.725\n",
       "763      996                0.125                0.750\n",
       "764      997                0.325                0.425\n",
       "765      999                0.550                0.750\n",
       "766     1000                0.150                0.325\n",
       "\n",
       "[767 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_annotations = pd.read_csv(get_pmemo_path('processed/annotations/pmemo_static_annotations.csv'))\n",
    "df_annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the featureset\n",
    "\n",
    "This is where you should change between normalised and standardised, and untouched featuresets!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>song_id</th>\n",
       "      <th>lowlevel.melbands_kurtosis.mean</th>\n",
       "      <th>lowlevel.melbands_skewness.mean</th>\n",
       "      <th>lowlevel.spectral_energy.mean</th>\n",
       "      <th>lowlevel.zerocrossingrate.mean</th>\n",
       "      <th>rhythm.beats_loudness.mean</th>\n",
       "      <th>rhythm.onset_rate</th>\n",
       "      <th>tonal.chords_strength.mean</th>\n",
       "      <th>tonal.hpcp_entropy.mean</th>\n",
       "      <th>tonal.key_edma.strength</th>\n",
       "      <th>...</th>\n",
       "      <th>tonal.chords_histogram_14</th>\n",
       "      <th>tonal.chords_histogram_15</th>\n",
       "      <th>tonal.chords_histogram_16</th>\n",
       "      <th>tonal.chords_histogram_17</th>\n",
       "      <th>tonal.chords_histogram_18</th>\n",
       "      <th>tonal.chords_histogram_19</th>\n",
       "      <th>tonal.chords_histogram_20</th>\n",
       "      <th>tonal.chords_histogram_21</th>\n",
       "      <th>tonal.chords_histogram_22</th>\n",
       "      <th>tonal.chords_histogram_23</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.064362</td>\n",
       "      <td>0.214289</td>\n",
       "      <td>0.125964</td>\n",
       "      <td>0.416985</td>\n",
       "      <td>0.086404</td>\n",
       "      <td>0.810704</td>\n",
       "      <td>0.267193</td>\n",
       "      <td>0.482615</td>\n",
       "      <td>0.533418</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.089160</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.223899</td>\n",
       "      <td>0.098657</td>\n",
       "      <td>0.288181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>0.345452</td>\n",
       "      <td>0.539877</td>\n",
       "      <td>0.949078</td>\n",
       "      <td>0.050339</td>\n",
       "      <td>0.462009</td>\n",
       "      <td>0.795216</td>\n",
       "      <td>0.304507</td>\n",
       "      <td>0.461919</td>\n",
       "      <td>0.163914</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016972</td>\n",
       "      <td>0.077021</td>\n",
       "      <td>0.402838</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.030457</td>\n",
       "      <td>0.117746</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.107674</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.155779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>0.566046</td>\n",
       "      <td>0.636537</td>\n",
       "      <td>0.525953</td>\n",
       "      <td>0.171160</td>\n",
       "      <td>0.308063</td>\n",
       "      <td>0.844622</td>\n",
       "      <td>0.464434</td>\n",
       "      <td>0.609828</td>\n",
       "      <td>0.714130</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008505</td>\n",
       "      <td>0.007329</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>0.168166</td>\n",
       "      <td>0.426521</td>\n",
       "      <td>0.278434</td>\n",
       "      <td>0.265749</td>\n",
       "      <td>0.139343</td>\n",
       "      <td>0.765399</td>\n",
       "      <td>0.362823</td>\n",
       "      <td>0.392684</td>\n",
       "      <td>0.381464</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.063161</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.051213</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>0.050890</td>\n",
       "      <td>0.193447</td>\n",
       "      <td>0.201743</td>\n",
       "      <td>0.447316</td>\n",
       "      <td>0.231818</td>\n",
       "      <td>0.578654</td>\n",
       "      <td>0.445732</td>\n",
       "      <td>0.675341</td>\n",
       "      <td>0.746939</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.165104</td>\n",
       "      <td>0.096976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>762</th>\n",
       "      <td>993</td>\n",
       "      <td>0.058467</td>\n",
       "      <td>0.186511</td>\n",
       "      <td>0.184759</td>\n",
       "      <td>0.536035</td>\n",
       "      <td>0.124013</td>\n",
       "      <td>0.551773</td>\n",
       "      <td>0.555560</td>\n",
       "      <td>0.580548</td>\n",
       "      <td>0.612516</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.031499</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013240</td>\n",
       "      <td>0.156009</td>\n",
       "      <td>0.863862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>763</th>\n",
       "      <td>996</td>\n",
       "      <td>0.121857</td>\n",
       "      <td>0.234562</td>\n",
       "      <td>0.179638</td>\n",
       "      <td>0.633059</td>\n",
       "      <td>0.073642</td>\n",
       "      <td>0.674339</td>\n",
       "      <td>0.552154</td>\n",
       "      <td>0.660622</td>\n",
       "      <td>0.638703</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.044460</td>\n",
       "      <td>0.016906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>764</th>\n",
       "      <td>997</td>\n",
       "      <td>0.046206</td>\n",
       "      <td>0.200557</td>\n",
       "      <td>0.195031</td>\n",
       "      <td>0.620730</td>\n",
       "      <td>0.097882</td>\n",
       "      <td>0.520823</td>\n",
       "      <td>0.144097</td>\n",
       "      <td>0.571115</td>\n",
       "      <td>0.554702</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008208</td>\n",
       "      <td>0.025688</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.053602</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.027702</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.149709</td>\n",
       "      <td>0.035320</td>\n",
       "      <td>0.149458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>765</th>\n",
       "      <td>999</td>\n",
       "      <td>0.063658</td>\n",
       "      <td>0.238118</td>\n",
       "      <td>0.238011</td>\n",
       "      <td>0.658759</td>\n",
       "      <td>0.140357</td>\n",
       "      <td>0.433236</td>\n",
       "      <td>0.136955</td>\n",
       "      <td>0.490124</td>\n",
       "      <td>0.476657</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005471</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.051231</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.112450</td>\n",
       "      <td>0.019859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>766</th>\n",
       "      <td>1000</td>\n",
       "      <td>0.028755</td>\n",
       "      <td>0.176146</td>\n",
       "      <td>0.162377</td>\n",
       "      <td>0.698038</td>\n",
       "      <td>0.080558</td>\n",
       "      <td>0.338497</td>\n",
       "      <td>0.335940</td>\n",
       "      <td>0.598790</td>\n",
       "      <td>0.402670</td>\n",
       "      <td>...</td>\n",
       "      <td>0.155668</td>\n",
       "      <td>0.333946</td>\n",
       "      <td>0.483745</td>\n",
       "      <td>0.073610</td>\n",
       "      <td>0.174831</td>\n",
       "      <td>0.019717</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000761</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>767 rows Ã— 743 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     song_id  lowlevel.melbands_kurtosis.mean  \\\n",
       "0          1                         0.064362   \n",
       "1          4                         0.345452   \n",
       "2          5                         0.566046   \n",
       "3          6                         0.168166   \n",
       "4          7                         0.050890   \n",
       "..       ...                              ...   \n",
       "762      993                         0.058467   \n",
       "763      996                         0.121857   \n",
       "764      997                         0.046206   \n",
       "765      999                         0.063658   \n",
       "766     1000                         0.028755   \n",
       "\n",
       "     lowlevel.melbands_skewness.mean  lowlevel.spectral_energy.mean  \\\n",
       "0                           0.214289                       0.125964   \n",
       "1                           0.539877                       0.949078   \n",
       "2                           0.636537                       0.525953   \n",
       "3                           0.426521                       0.278434   \n",
       "4                           0.193447                       0.201743   \n",
       "..                               ...                            ...   \n",
       "762                         0.186511                       0.184759   \n",
       "763                         0.234562                       0.179638   \n",
       "764                         0.200557                       0.195031   \n",
       "765                         0.238118                       0.238011   \n",
       "766                         0.176146                       0.162377   \n",
       "\n",
       "     lowlevel.zerocrossingrate.mean  rhythm.beats_loudness.mean  \\\n",
       "0                          0.416985                    0.086404   \n",
       "1                          0.050339                    0.462009   \n",
       "2                          0.171160                    0.308063   \n",
       "3                          0.265749                    0.139343   \n",
       "4                          0.447316                    0.231818   \n",
       "..                              ...                         ...   \n",
       "762                        0.536035                    0.124013   \n",
       "763                        0.633059                    0.073642   \n",
       "764                        0.620730                    0.097882   \n",
       "765                        0.658759                    0.140357   \n",
       "766                        0.698038                    0.080558   \n",
       "\n",
       "     rhythm.onset_rate  tonal.chords_strength.mean  tonal.hpcp_entropy.mean  \\\n",
       "0             0.810704                    0.267193                 0.482615   \n",
       "1             0.795216                    0.304507                 0.461919   \n",
       "2             0.844622                    0.464434                 0.609828   \n",
       "3             0.765399                    0.362823                 0.392684   \n",
       "4             0.578654                    0.445732                 0.675341   \n",
       "..                 ...                         ...                      ...   \n",
       "762           0.551773                    0.555560                 0.580548   \n",
       "763           0.674339                    0.552154                 0.660622   \n",
       "764           0.520823                    0.144097                 0.571115   \n",
       "765           0.433236                    0.136955                 0.490124   \n",
       "766           0.338497                    0.335940                 0.598790   \n",
       "\n",
       "     tonal.key_edma.strength  ...  tonal.chords_histogram_14  \\\n",
       "0                   0.533418  ...                   0.000000   \n",
       "1                   0.163914  ...                   0.016972   \n",
       "2                   0.714130  ...                   0.000000   \n",
       "3                   0.381464  ...                   0.000000   \n",
       "4                   0.746939  ...                   0.000000   \n",
       "..                       ...  ...                        ...   \n",
       "762                 0.612516  ...                   0.000000   \n",
       "763                 0.638703  ...                   0.000000   \n",
       "764                 0.554702  ...                   0.008208   \n",
       "765                 0.476657  ...                   0.000000   \n",
       "766                 0.402670  ...                   0.155668   \n",
       "\n",
       "     tonal.chords_histogram_15  tonal.chords_histogram_16  \\\n",
       "0                     0.000000                   0.000000   \n",
       "1                     0.077021                   0.402838   \n",
       "2                     0.000000                   0.000000   \n",
       "3                     0.063161                   0.000000   \n",
       "4                     0.000000                   0.000000   \n",
       "..                         ...                        ...   \n",
       "762                   0.000000                   0.000000   \n",
       "763                   0.000000                   0.000000   \n",
       "764                   0.025688                   0.000000   \n",
       "765                   0.000000                   0.000000   \n",
       "766                   0.333946                   0.483745   \n",
       "\n",
       "     tonal.chords_histogram_17  tonal.chords_histogram_18  \\\n",
       "0                     0.000000                   0.000000   \n",
       "1                     0.000000                   0.030457   \n",
       "2                     0.000000                   0.000000   \n",
       "3                     0.000000                   0.000000   \n",
       "4                     0.000000                   0.000000   \n",
       "..                         ...                        ...   \n",
       "762                   0.031499                   0.000000   \n",
       "763                   0.000000                   0.000000   \n",
       "764                   0.053602                   0.000000   \n",
       "765                   0.000000                   0.005471   \n",
       "766                   0.073610                   0.174831   \n",
       "\n",
       "     tonal.chords_histogram_19  tonal.chords_histogram_20  \\\n",
       "0                     0.089160                   0.000000   \n",
       "1                     0.117746                   0.000000   \n",
       "2                     0.008505                   0.007329   \n",
       "3                     0.000000                   0.000000   \n",
       "4                     0.000000                   0.000000   \n",
       "..                         ...                        ...   \n",
       "762                   0.000000                   0.000000   \n",
       "763                   0.000000                   0.000000   \n",
       "764                   0.027702                   0.000000   \n",
       "765                   0.000000                   0.051231   \n",
       "766                   0.019717                   0.000000   \n",
       "\n",
       "     tonal.chords_histogram_21  tonal.chords_histogram_22  \\\n",
       "0                     0.223899                   0.098657   \n",
       "1                     0.107674                   0.000000   \n",
       "2                     0.000000                   0.000000   \n",
       "3                     0.051213                   0.000000   \n",
       "4                     0.142857                   0.165104   \n",
       "..                         ...                        ...   \n",
       "762                   0.013240                   0.156009   \n",
       "763                   0.000000                   0.044460   \n",
       "764                   0.149709                   0.035320   \n",
       "765                   0.000000                   0.112450   \n",
       "766                   0.000000                   0.000761   \n",
       "\n",
       "     tonal.chords_histogram_23  \n",
       "0                     0.288181  \n",
       "1                     0.155779  \n",
       "2                     0.000000  \n",
       "3                     0.000000  \n",
       "4                     0.096976  \n",
       "..                         ...  \n",
       "762                   0.863862  \n",
       "763                   0.016906  \n",
       "764                   0.149458  \n",
       "765                   0.019859  \n",
       "766                   0.000000  \n",
       "\n",
       "[767 rows x 743 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_essentia_best_valence_features_mean = pd.read_csv(get_pmemo_path('processed/features/normalised_essentia_best_valence_features_mean.csv'))\n",
    "\n",
    "# drop Unnamed:0 column\n",
    "df_essentia_best_valence_features_mean = df_essentia_best_valence_features_mean[df_essentia_best_valence_features_mean.columns[1:]]\n",
    "\n",
    "df_essentia_best_valence_features_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 767 entries, 0 to 766\n",
      "Data columns (total 743 columns):\n",
      " #    Column                                   Dtype  \n",
      "---   ------                                   -----  \n",
      " 0    song_id                                  int64  \n",
      " 1    lowlevel.melbands_kurtosis.mean          float64\n",
      " 2    lowlevel.melbands_skewness.mean          float64\n",
      " 3    lowlevel.spectral_energy.mean            float64\n",
      " 4    lowlevel.zerocrossingrate.mean           float64\n",
      " 5    rhythm.beats_loudness.mean               float64\n",
      " 6    rhythm.onset_rate                        float64\n",
      " 7    tonal.chords_strength.mean               float64\n",
      " 8    tonal.hpcp_entropy.mean                  float64\n",
      " 9    tonal.key_edma.strength                  float64\n",
      " 10   tonal.key_temperley.strength             float64\n",
      " 11   lowlevel.gfcc.cov_0                      float64\n",
      " 12   lowlevel.gfcc.cov_1                      float64\n",
      " 13   lowlevel.gfcc.cov_2                      float64\n",
      " 14   lowlevel.gfcc.cov_3                      float64\n",
      " 15   lowlevel.gfcc.cov_4                      float64\n",
      " 16   lowlevel.gfcc.cov_5                      float64\n",
      " 17   lowlevel.gfcc.cov_6                      float64\n",
      " 18   lowlevel.gfcc.cov_7                      float64\n",
      " 19   lowlevel.gfcc.cov_8                      float64\n",
      " 20   lowlevel.gfcc.cov_9                      float64\n",
      " 21   lowlevel.gfcc.cov_10                     float64\n",
      " 22   lowlevel.gfcc.cov_11                     float64\n",
      " 23   lowlevel.gfcc.cov_12                     float64\n",
      " 24   lowlevel.gfcc.cov_13                     float64\n",
      " 25   lowlevel.gfcc.cov_14                     float64\n",
      " 26   lowlevel.gfcc.cov_15                     float64\n",
      " 27   lowlevel.gfcc.cov_16                     float64\n",
      " 28   lowlevel.gfcc.cov_17                     float64\n",
      " 29   lowlevel.gfcc.cov_18                     float64\n",
      " 30   lowlevel.gfcc.cov_19                     float64\n",
      " 31   lowlevel.gfcc.cov_20                     float64\n",
      " 32   lowlevel.gfcc.cov_21                     float64\n",
      " 33   lowlevel.gfcc.cov_22                     float64\n",
      " 34   lowlevel.gfcc.cov_23                     float64\n",
      " 35   lowlevel.gfcc.cov_24                     float64\n",
      " 36   lowlevel.gfcc.cov_25                     float64\n",
      " 37   lowlevel.gfcc.cov_26                     float64\n",
      " 38   lowlevel.gfcc.cov_27                     float64\n",
      " 39   lowlevel.gfcc.cov_28                     float64\n",
      " 40   lowlevel.gfcc.cov_29                     float64\n",
      " 41   lowlevel.gfcc.cov_30                     float64\n",
      " 42   lowlevel.gfcc.cov_31                     float64\n",
      " 43   lowlevel.gfcc.cov_32                     float64\n",
      " 44   lowlevel.gfcc.cov_33                     float64\n",
      " 45   lowlevel.gfcc.cov_34                     float64\n",
      " 46   lowlevel.gfcc.cov_35                     float64\n",
      " 47   lowlevel.gfcc.cov_36                     float64\n",
      " 48   lowlevel.gfcc.cov_37                     float64\n",
      " 49   lowlevel.gfcc.cov_38                     float64\n",
      " 50   lowlevel.gfcc.cov_39                     float64\n",
      " 51   lowlevel.gfcc.cov_40                     float64\n",
      " 52   lowlevel.gfcc.cov_41                     float64\n",
      " 53   lowlevel.gfcc.cov_42                     float64\n",
      " 54   lowlevel.gfcc.cov_43                     float64\n",
      " 55   lowlevel.gfcc.cov_44                     float64\n",
      " 56   lowlevel.gfcc.cov_45                     float64\n",
      " 57   lowlevel.gfcc.cov_46                     float64\n",
      " 58   lowlevel.gfcc.cov_47                     float64\n",
      " 59   lowlevel.gfcc.cov_48                     float64\n",
      " 60   lowlevel.gfcc.cov_49                     float64\n",
      " 61   lowlevel.gfcc.cov_50                     float64\n",
      " 62   lowlevel.gfcc.cov_51                     float64\n",
      " 63   lowlevel.gfcc.cov_52                     float64\n",
      " 64   lowlevel.gfcc.cov_53                     float64\n",
      " 65   lowlevel.gfcc.cov_54                     float64\n",
      " 66   lowlevel.gfcc.cov_55                     float64\n",
      " 67   lowlevel.gfcc.cov_56                     float64\n",
      " 68   lowlevel.gfcc.cov_57                     float64\n",
      " 69   lowlevel.gfcc.cov_58                     float64\n",
      " 70   lowlevel.gfcc.cov_59                     float64\n",
      " 71   lowlevel.gfcc.cov_60                     float64\n",
      " 72   lowlevel.gfcc.cov_61                     float64\n",
      " 73   lowlevel.gfcc.cov_62                     float64\n",
      " 74   lowlevel.gfcc.cov_63                     float64\n",
      " 75   lowlevel.gfcc.cov_64                     float64\n",
      " 76   lowlevel.gfcc.cov_65                     float64\n",
      " 77   lowlevel.gfcc.cov_66                     float64\n",
      " 78   lowlevel.gfcc.cov_67                     float64\n",
      " 79   lowlevel.gfcc.cov_68                     float64\n",
      " 80   lowlevel.gfcc.cov_69                     float64\n",
      " 81   lowlevel.gfcc.cov_70                     float64\n",
      " 82   lowlevel.gfcc.cov_71                     float64\n",
      " 83   lowlevel.gfcc.cov_72                     float64\n",
      " 84   lowlevel.gfcc.cov_73                     float64\n",
      " 85   lowlevel.gfcc.cov_74                     float64\n",
      " 86   lowlevel.gfcc.cov_75                     float64\n",
      " 87   lowlevel.gfcc.cov_76                     float64\n",
      " 88   lowlevel.gfcc.cov_77                     float64\n",
      " 89   lowlevel.gfcc.cov_78                     float64\n",
      " 90   lowlevel.gfcc.cov_79                     float64\n",
      " 91   lowlevel.gfcc.cov_80                     float64\n",
      " 92   lowlevel.gfcc.cov_81                     float64\n",
      " 93   lowlevel.gfcc.cov_82                     float64\n",
      " 94   lowlevel.gfcc.cov_83                     float64\n",
      " 95   lowlevel.gfcc.cov_84                     float64\n",
      " 96   lowlevel.gfcc.cov_85                     float64\n",
      " 97   lowlevel.gfcc.cov_86                     float64\n",
      " 98   lowlevel.gfcc.cov_87                     float64\n",
      " 99   lowlevel.gfcc.cov_88                     float64\n",
      " 100  lowlevel.gfcc.cov_89                     float64\n",
      " 101  lowlevel.gfcc.cov_90                     float64\n",
      " 102  lowlevel.gfcc.cov_91                     float64\n",
      " 103  lowlevel.gfcc.cov_92                     float64\n",
      " 104  lowlevel.gfcc.cov_93                     float64\n",
      " 105  lowlevel.gfcc.cov_94                     float64\n",
      " 106  lowlevel.gfcc.cov_95                     float64\n",
      " 107  lowlevel.gfcc.cov_96                     float64\n",
      " 108  lowlevel.gfcc.cov_97                     float64\n",
      " 109  lowlevel.gfcc.cov_98                     float64\n",
      " 110  lowlevel.gfcc.cov_99                     float64\n",
      " 111  lowlevel.gfcc.cov_100                    float64\n",
      " 112  lowlevel.gfcc.cov_101                    float64\n",
      " 113  lowlevel.gfcc.cov_102                    float64\n",
      " 114  lowlevel.gfcc.cov_103                    float64\n",
      " 115  lowlevel.gfcc.cov_104                    float64\n",
      " 116  lowlevel.gfcc.cov_105                    float64\n",
      " 117  lowlevel.gfcc.cov_106                    float64\n",
      " 118  lowlevel.gfcc.cov_107                    float64\n",
      " 119  lowlevel.gfcc.cov_108                    float64\n",
      " 120  lowlevel.gfcc.cov_109                    float64\n",
      " 121  lowlevel.gfcc.cov_110                    float64\n",
      " 122  lowlevel.gfcc.cov_111                    float64\n",
      " 123  lowlevel.gfcc.cov_112                    float64\n",
      " 124  lowlevel.gfcc.cov_113                    float64\n",
      " 125  lowlevel.gfcc.cov_114                    float64\n",
      " 126  lowlevel.gfcc.cov_115                    float64\n",
      " 127  lowlevel.gfcc.cov_116                    float64\n",
      " 128  lowlevel.gfcc.cov_117                    float64\n",
      " 129  lowlevel.gfcc.cov_118                    float64\n",
      " 130  lowlevel.gfcc.cov_119                    float64\n",
      " 131  lowlevel.gfcc.cov_120                    float64\n",
      " 132  lowlevel.gfcc.cov_121                    float64\n",
      " 133  lowlevel.gfcc.cov_122                    float64\n",
      " 134  lowlevel.gfcc.cov_123                    float64\n",
      " 135  lowlevel.gfcc.cov_124                    float64\n",
      " 136  lowlevel.gfcc.cov_125                    float64\n",
      " 137  lowlevel.gfcc.cov_126                    float64\n",
      " 138  lowlevel.gfcc.cov_127                    float64\n",
      " 139  lowlevel.gfcc.cov_128                    float64\n",
      " 140  lowlevel.gfcc.cov_129                    float64\n",
      " 141  lowlevel.gfcc.cov_130                    float64\n",
      " 142  lowlevel.gfcc.cov_131                    float64\n",
      " 143  lowlevel.gfcc.cov_132                    float64\n",
      " 144  lowlevel.gfcc.cov_133                    float64\n",
      " 145  lowlevel.gfcc.cov_134                    float64\n",
      " 146  lowlevel.gfcc.cov_135                    float64\n",
      " 147  lowlevel.gfcc.cov_136                    float64\n",
      " 148  lowlevel.gfcc.cov_137                    float64\n",
      " 149  lowlevel.gfcc.cov_138                    float64\n",
      " 150  lowlevel.gfcc.cov_139                    float64\n",
      " 151  lowlevel.gfcc.cov_140                    float64\n",
      " 152  lowlevel.gfcc.cov_141                    float64\n",
      " 153  lowlevel.gfcc.cov_142                    float64\n",
      " 154  lowlevel.gfcc.cov_143                    float64\n",
      " 155  lowlevel.gfcc.cov_144                    float64\n",
      " 156  lowlevel.gfcc.cov_145                    float64\n",
      " 157  lowlevel.gfcc.cov_146                    float64\n",
      " 158  lowlevel.gfcc.cov_147                    float64\n",
      " 159  lowlevel.gfcc.cov_148                    float64\n",
      " 160  lowlevel.gfcc.cov_149                    float64\n",
      " 161  lowlevel.gfcc.cov_150                    float64\n",
      " 162  lowlevel.gfcc.cov_151                    float64\n",
      " 163  lowlevel.gfcc.cov_152                    float64\n",
      " 164  lowlevel.gfcc.cov_153                    float64\n",
      " 165  lowlevel.gfcc.cov_154                    float64\n",
      " 166  lowlevel.gfcc.cov_155                    float64\n",
      " 167  lowlevel.gfcc.cov_156                    float64\n",
      " 168  lowlevel.gfcc.cov_157                    float64\n",
      " 169  lowlevel.gfcc.cov_158                    float64\n",
      " 170  lowlevel.gfcc.cov_159                    float64\n",
      " 171  lowlevel.gfcc.cov_160                    float64\n",
      " 172  lowlevel.gfcc.cov_161                    float64\n",
      " 173  lowlevel.gfcc.cov_162                    float64\n",
      " 174  lowlevel.gfcc.cov_163                    float64\n",
      " 175  lowlevel.gfcc.cov_164                    float64\n",
      " 176  lowlevel.gfcc.cov_165                    float64\n",
      " 177  lowlevel.gfcc.cov_166                    float64\n",
      " 178  lowlevel.gfcc.cov_167                    float64\n",
      " 179  lowlevel.gfcc.cov_168                    float64\n",
      " 180  lowlevel.gfcc.icov_0                     float64\n",
      " 181  lowlevel.gfcc.icov_1                     float64\n",
      " 182  lowlevel.gfcc.icov_2                     float64\n",
      " 183  lowlevel.gfcc.icov_3                     float64\n",
      " 184  lowlevel.gfcc.icov_4                     float64\n",
      " 185  lowlevel.gfcc.icov_5                     float64\n",
      " 186  lowlevel.gfcc.icov_6                     float64\n",
      " 187  lowlevel.gfcc.icov_7                     float64\n",
      " 188  lowlevel.gfcc.icov_8                     float64\n",
      " 189  lowlevel.gfcc.icov_9                     float64\n",
      " 190  lowlevel.gfcc.icov_10                    float64\n",
      " 191  lowlevel.gfcc.icov_11                    float64\n",
      " 192  lowlevel.gfcc.icov_12                    float64\n",
      " 193  lowlevel.gfcc.icov_13                    float64\n",
      " 194  lowlevel.gfcc.icov_14                    float64\n",
      " 195  lowlevel.gfcc.icov_15                    float64\n",
      " 196  lowlevel.gfcc.icov_16                    float64\n",
      " 197  lowlevel.gfcc.icov_17                    float64\n",
      " 198  lowlevel.gfcc.icov_18                    float64\n",
      " 199  lowlevel.gfcc.icov_19                    float64\n",
      " 200  lowlevel.gfcc.icov_20                    float64\n",
      " 201  lowlevel.gfcc.icov_21                    float64\n",
      " 202  lowlevel.gfcc.icov_22                    float64\n",
      " 203  lowlevel.gfcc.icov_23                    float64\n",
      " 204  lowlevel.gfcc.icov_24                    float64\n",
      " 205  lowlevel.gfcc.icov_25                    float64\n",
      " 206  lowlevel.gfcc.icov_26                    float64\n",
      " 207  lowlevel.gfcc.icov_27                    float64\n",
      " 208  lowlevel.gfcc.icov_28                    float64\n",
      " 209  lowlevel.gfcc.icov_29                    float64\n",
      " 210  lowlevel.gfcc.icov_30                    float64\n",
      " 211  lowlevel.gfcc.icov_31                    float64\n",
      " 212  lowlevel.gfcc.icov_32                    float64\n",
      " 213  lowlevel.gfcc.icov_33                    float64\n",
      " 214  lowlevel.gfcc.icov_34                    float64\n",
      " 215  lowlevel.gfcc.icov_35                    float64\n",
      " 216  lowlevel.gfcc.icov_36                    float64\n",
      " 217  lowlevel.gfcc.icov_37                    float64\n",
      " 218  lowlevel.gfcc.icov_38                    float64\n",
      " 219  lowlevel.gfcc.icov_39                    float64\n",
      " 220  lowlevel.gfcc.icov_40                    float64\n",
      " 221  lowlevel.gfcc.icov_41                    float64\n",
      " 222  lowlevel.gfcc.icov_42                    float64\n",
      " 223  lowlevel.gfcc.icov_43                    float64\n",
      " 224  lowlevel.gfcc.icov_44                    float64\n",
      " 225  lowlevel.gfcc.icov_45                    float64\n",
      " 226  lowlevel.gfcc.icov_46                    float64\n",
      " 227  lowlevel.gfcc.icov_47                    float64\n",
      " 228  lowlevel.gfcc.icov_48                    float64\n",
      " 229  lowlevel.gfcc.icov_49                    float64\n",
      " 230  lowlevel.gfcc.icov_50                    float64\n",
      " 231  lowlevel.gfcc.icov_51                    float64\n",
      " 232  lowlevel.gfcc.icov_52                    float64\n",
      " 233  lowlevel.gfcc.icov_53                    float64\n",
      " 234  lowlevel.gfcc.icov_54                    float64\n",
      " 235  lowlevel.gfcc.icov_55                    float64\n",
      " 236  lowlevel.gfcc.icov_56                    float64\n",
      " 237  lowlevel.gfcc.icov_57                    float64\n",
      " 238  lowlevel.gfcc.icov_58                    float64\n",
      " 239  lowlevel.gfcc.icov_59                    float64\n",
      " 240  lowlevel.gfcc.icov_60                    float64\n",
      " 241  lowlevel.gfcc.icov_61                    float64\n",
      " 242  lowlevel.gfcc.icov_62                    float64\n",
      " 243  lowlevel.gfcc.icov_63                    float64\n",
      " 244  lowlevel.gfcc.icov_64                    float64\n",
      " 245  lowlevel.gfcc.icov_65                    float64\n",
      " 246  lowlevel.gfcc.icov_66                    float64\n",
      " 247  lowlevel.gfcc.icov_67                    float64\n",
      " 248  lowlevel.gfcc.icov_68                    float64\n",
      " 249  lowlevel.gfcc.icov_69                    float64\n",
      " 250  lowlevel.gfcc.icov_70                    float64\n",
      " 251  lowlevel.gfcc.icov_71                    float64\n",
      " 252  lowlevel.gfcc.icov_72                    float64\n",
      " 253  lowlevel.gfcc.icov_73                    float64\n",
      " 254  lowlevel.gfcc.icov_74                    float64\n",
      " 255  lowlevel.gfcc.icov_75                    float64\n",
      " 256  lowlevel.gfcc.icov_76                    float64\n",
      " 257  lowlevel.gfcc.icov_77                    float64\n",
      " 258  lowlevel.gfcc.icov_78                    float64\n",
      " 259  lowlevel.gfcc.icov_79                    float64\n",
      " 260  lowlevel.gfcc.icov_80                    float64\n",
      " 261  lowlevel.gfcc.icov_81                    float64\n",
      " 262  lowlevel.gfcc.icov_82                    float64\n",
      " 263  lowlevel.gfcc.icov_83                    float64\n",
      " 264  lowlevel.gfcc.icov_84                    float64\n",
      " 265  lowlevel.gfcc.icov_85                    float64\n",
      " 266  lowlevel.gfcc.icov_86                    float64\n",
      " 267  lowlevel.gfcc.icov_87                    float64\n",
      " 268  lowlevel.gfcc.icov_88                    float64\n",
      " 269  lowlevel.gfcc.icov_89                    float64\n",
      " 270  lowlevel.gfcc.icov_90                    float64\n",
      " 271  lowlevel.gfcc.icov_91                    float64\n",
      " 272  lowlevel.gfcc.icov_92                    float64\n",
      " 273  lowlevel.gfcc.icov_93                    float64\n",
      " 274  lowlevel.gfcc.icov_94                    float64\n",
      " 275  lowlevel.gfcc.icov_95                    float64\n",
      " 276  lowlevel.gfcc.icov_96                    float64\n",
      " 277  lowlevel.gfcc.icov_97                    float64\n",
      " 278  lowlevel.gfcc.icov_98                    float64\n",
      " 279  lowlevel.gfcc.icov_99                    float64\n",
      " 280  lowlevel.gfcc.icov_100                   float64\n",
      " 281  lowlevel.gfcc.icov_101                   float64\n",
      " 282  lowlevel.gfcc.icov_102                   float64\n",
      " 283  lowlevel.gfcc.icov_103                   float64\n",
      " 284  lowlevel.gfcc.icov_104                   float64\n",
      " 285  lowlevel.gfcc.icov_105                   float64\n",
      " 286  lowlevel.gfcc.icov_106                   float64\n",
      " 287  lowlevel.gfcc.icov_107                   float64\n",
      " 288  lowlevel.gfcc.icov_108                   float64\n",
      " 289  lowlevel.gfcc.icov_109                   float64\n",
      " 290  lowlevel.gfcc.icov_110                   float64\n",
      " 291  lowlevel.gfcc.icov_111                   float64\n",
      " 292  lowlevel.gfcc.icov_112                   float64\n",
      " 293  lowlevel.gfcc.icov_113                   float64\n",
      " 294  lowlevel.gfcc.icov_114                   float64\n",
      " 295  lowlevel.gfcc.icov_115                   float64\n",
      " 296  lowlevel.gfcc.icov_116                   float64\n",
      " 297  lowlevel.gfcc.icov_117                   float64\n",
      " 298  lowlevel.gfcc.icov_118                   float64\n",
      " 299  lowlevel.gfcc.icov_119                   float64\n",
      " 300  lowlevel.gfcc.icov_120                   float64\n",
      " 301  lowlevel.gfcc.icov_121                   float64\n",
      " 302  lowlevel.gfcc.icov_122                   float64\n",
      " 303  lowlevel.gfcc.icov_123                   float64\n",
      " 304  lowlevel.gfcc.icov_124                   float64\n",
      " 305  lowlevel.gfcc.icov_125                   float64\n",
      " 306  lowlevel.gfcc.icov_126                   float64\n",
      " 307  lowlevel.gfcc.icov_127                   float64\n",
      " 308  lowlevel.gfcc.icov_128                   float64\n",
      " 309  lowlevel.gfcc.icov_129                   float64\n",
      " 310  lowlevel.gfcc.icov_130                   float64\n",
      " 311  lowlevel.gfcc.icov_131                   float64\n",
      " 312  lowlevel.gfcc.icov_132                   float64\n",
      " 313  lowlevel.gfcc.icov_133                   float64\n",
      " 314  lowlevel.gfcc.icov_134                   float64\n",
      " 315  lowlevel.gfcc.icov_135                   float64\n",
      " 316  lowlevel.gfcc.icov_136                   float64\n",
      " 317  lowlevel.gfcc.icov_137                   float64\n",
      " 318  lowlevel.gfcc.icov_138                   float64\n",
      " 319  lowlevel.gfcc.icov_139                   float64\n",
      " 320  lowlevel.gfcc.icov_140                   float64\n",
      " 321  lowlevel.gfcc.icov_141                   float64\n",
      " 322  lowlevel.gfcc.icov_142                   float64\n",
      " 323  lowlevel.gfcc.icov_143                   float64\n",
      " 324  lowlevel.gfcc.icov_144                   float64\n",
      " 325  lowlevel.gfcc.icov_145                   float64\n",
      " 326  lowlevel.gfcc.icov_146                   float64\n",
      " 327  lowlevel.gfcc.icov_147                   float64\n",
      " 328  lowlevel.gfcc.icov_148                   float64\n",
      " 329  lowlevel.gfcc.icov_149                   float64\n",
      " 330  lowlevel.gfcc.icov_150                   float64\n",
      " 331  lowlevel.gfcc.icov_151                   float64\n",
      " 332  lowlevel.gfcc.icov_152                   float64\n",
      " 333  lowlevel.gfcc.icov_153                   float64\n",
      " 334  lowlevel.gfcc.icov_154                   float64\n",
      " 335  lowlevel.gfcc.icov_155                   float64\n",
      " 336  lowlevel.gfcc.icov_156                   float64\n",
      " 337  lowlevel.gfcc.icov_157                   float64\n",
      " 338  lowlevel.gfcc.icov_158                   float64\n",
      " 339  lowlevel.gfcc.icov_159                   float64\n",
      " 340  lowlevel.gfcc.icov_160                   float64\n",
      " 341  lowlevel.gfcc.icov_161                   float64\n",
      " 342  lowlevel.gfcc.icov_162                   float64\n",
      " 343  lowlevel.gfcc.icov_163                   float64\n",
      " 344  lowlevel.gfcc.icov_164                   float64\n",
      " 345  lowlevel.gfcc.icov_165                   float64\n",
      " 346  lowlevel.gfcc.icov_166                   float64\n",
      " 347  lowlevel.gfcc.icov_167                   float64\n",
      " 348  lowlevel.gfcc.icov_168                   float64\n",
      " 349  lowlevel.gfcc.mean_0                     float64\n",
      " 350  lowlevel.gfcc.mean_1                     float64\n",
      " 351  lowlevel.gfcc.mean_2                     float64\n",
      " 352  lowlevel.gfcc.mean_3                     float64\n",
      " 353  lowlevel.gfcc.mean_4                     float64\n",
      " 354  lowlevel.gfcc.mean_5                     float64\n",
      " 355  lowlevel.gfcc.mean_6                     float64\n",
      " 356  lowlevel.gfcc.mean_7                     float64\n",
      " 357  lowlevel.gfcc.mean_8                     float64\n",
      " 358  lowlevel.gfcc.mean_9                     float64\n",
      " 359  lowlevel.gfcc.mean_10                    float64\n",
      " 360  lowlevel.gfcc.mean_11                    float64\n",
      " 361  lowlevel.gfcc.mean_12                    float64\n",
      " 362  lowlevel.mfcc.cov_0                      float64\n",
      " 363  lowlevel.mfcc.cov_1                      float64\n",
      " 364  lowlevel.mfcc.cov_2                      float64\n",
      " 365  lowlevel.mfcc.cov_3                      float64\n",
      " 366  lowlevel.mfcc.cov_4                      float64\n",
      " 367  lowlevel.mfcc.cov_5                      float64\n",
      " 368  lowlevel.mfcc.cov_6                      float64\n",
      " 369  lowlevel.mfcc.cov_7                      float64\n",
      " 370  lowlevel.mfcc.cov_8                      float64\n",
      " 371  lowlevel.mfcc.cov_9                      float64\n",
      " 372  lowlevel.mfcc.cov_10                     float64\n",
      " 373  lowlevel.mfcc.cov_11                     float64\n",
      " 374  lowlevel.mfcc.cov_12                     float64\n",
      " 375  lowlevel.mfcc.cov_13                     float64\n",
      " 376  lowlevel.mfcc.cov_14                     float64\n",
      " 377  lowlevel.mfcc.cov_15                     float64\n",
      " 378  lowlevel.mfcc.cov_16                     float64\n",
      " 379  lowlevel.mfcc.cov_17                     float64\n",
      " 380  lowlevel.mfcc.cov_18                     float64\n",
      " 381  lowlevel.mfcc.cov_19                     float64\n",
      " 382  lowlevel.mfcc.cov_20                     float64\n",
      " 383  lowlevel.mfcc.cov_21                     float64\n",
      " 384  lowlevel.mfcc.cov_22                     float64\n",
      " 385  lowlevel.mfcc.cov_23                     float64\n",
      " 386  lowlevel.mfcc.cov_24                     float64\n",
      " 387  lowlevel.mfcc.cov_25                     float64\n",
      " 388  lowlevel.mfcc.cov_26                     float64\n",
      " 389  lowlevel.mfcc.cov_27                     float64\n",
      " 390  lowlevel.mfcc.cov_28                     float64\n",
      " 391  lowlevel.mfcc.cov_29                     float64\n",
      " 392  lowlevel.mfcc.cov_30                     float64\n",
      " 393  lowlevel.mfcc.cov_31                     float64\n",
      " 394  lowlevel.mfcc.cov_32                     float64\n",
      " 395  lowlevel.mfcc.cov_33                     float64\n",
      " 396  lowlevel.mfcc.cov_34                     float64\n",
      " 397  lowlevel.mfcc.cov_35                     float64\n",
      " 398  lowlevel.mfcc.cov_36                     float64\n",
      " 399  lowlevel.mfcc.cov_37                     float64\n",
      " 400  lowlevel.mfcc.cov_38                     float64\n",
      " 401  lowlevel.mfcc.cov_39                     float64\n",
      " 402  lowlevel.mfcc.cov_40                     float64\n",
      " 403  lowlevel.mfcc.cov_41                     float64\n",
      " 404  lowlevel.mfcc.cov_42                     float64\n",
      " 405  lowlevel.mfcc.cov_43                     float64\n",
      " 406  lowlevel.mfcc.cov_44                     float64\n",
      " 407  lowlevel.mfcc.cov_45                     float64\n",
      " 408  lowlevel.mfcc.cov_46                     float64\n",
      " 409  lowlevel.mfcc.cov_47                     float64\n",
      " 410  lowlevel.mfcc.cov_48                     float64\n",
      " 411  lowlevel.mfcc.cov_49                     float64\n",
      " 412  lowlevel.mfcc.cov_50                     float64\n",
      " 413  lowlevel.mfcc.cov_51                     float64\n",
      " 414  lowlevel.mfcc.cov_52                     float64\n",
      " 415  lowlevel.mfcc.cov_53                     float64\n",
      " 416  lowlevel.mfcc.cov_54                     float64\n",
      " 417  lowlevel.mfcc.cov_55                     float64\n",
      " 418  lowlevel.mfcc.cov_56                     float64\n",
      " 419  lowlevel.mfcc.cov_57                     float64\n",
      " 420  lowlevel.mfcc.cov_58                     float64\n",
      " 421  lowlevel.mfcc.cov_59                     float64\n",
      " 422  lowlevel.mfcc.cov_60                     float64\n",
      " 423  lowlevel.mfcc.cov_61                     float64\n",
      " 424  lowlevel.mfcc.cov_62                     float64\n",
      " 425  lowlevel.mfcc.cov_63                     float64\n",
      " 426  lowlevel.mfcc.cov_64                     float64\n",
      " 427  lowlevel.mfcc.cov_65                     float64\n",
      " 428  lowlevel.mfcc.cov_66                     float64\n",
      " 429  lowlevel.mfcc.cov_67                     float64\n",
      " 430  lowlevel.mfcc.cov_68                     float64\n",
      " 431  lowlevel.mfcc.cov_69                     float64\n",
      " 432  lowlevel.mfcc.cov_70                     float64\n",
      " 433  lowlevel.mfcc.cov_71                     float64\n",
      " 434  lowlevel.mfcc.cov_72                     float64\n",
      " 435  lowlevel.mfcc.cov_73                     float64\n",
      " 436  lowlevel.mfcc.cov_74                     float64\n",
      " 437  lowlevel.mfcc.cov_75                     float64\n",
      " 438  lowlevel.mfcc.cov_76                     float64\n",
      " 439  lowlevel.mfcc.cov_77                     float64\n",
      " 440  lowlevel.mfcc.cov_78                     float64\n",
      " 441  lowlevel.mfcc.cov_79                     float64\n",
      " 442  lowlevel.mfcc.cov_80                     float64\n",
      " 443  lowlevel.mfcc.cov_81                     float64\n",
      " 444  lowlevel.mfcc.cov_82                     float64\n",
      " 445  lowlevel.mfcc.cov_83                     float64\n",
      " 446  lowlevel.mfcc.cov_84                     float64\n",
      " 447  lowlevel.mfcc.cov_85                     float64\n",
      " 448  lowlevel.mfcc.cov_86                     float64\n",
      " 449  lowlevel.mfcc.cov_87                     float64\n",
      " 450  lowlevel.mfcc.cov_88                     float64\n",
      " 451  lowlevel.mfcc.cov_89                     float64\n",
      " 452  lowlevel.mfcc.cov_90                     float64\n",
      " 453  lowlevel.mfcc.cov_91                     float64\n",
      " 454  lowlevel.mfcc.cov_92                     float64\n",
      " 455  lowlevel.mfcc.cov_93                     float64\n",
      " 456  lowlevel.mfcc.cov_94                     float64\n",
      " 457  lowlevel.mfcc.cov_95                     float64\n",
      " 458  lowlevel.mfcc.cov_96                     float64\n",
      " 459  lowlevel.mfcc.cov_97                     float64\n",
      " 460  lowlevel.mfcc.cov_98                     float64\n",
      " 461  lowlevel.mfcc.cov_99                     float64\n",
      " 462  lowlevel.mfcc.cov_100                    float64\n",
      " 463  lowlevel.mfcc.cov_101                    float64\n",
      " 464  lowlevel.mfcc.cov_102                    float64\n",
      " 465  lowlevel.mfcc.cov_103                    float64\n",
      " 466  lowlevel.mfcc.cov_104                    float64\n",
      " 467  lowlevel.mfcc.cov_105                    float64\n",
      " 468  lowlevel.mfcc.cov_106                    float64\n",
      " 469  lowlevel.mfcc.cov_107                    float64\n",
      " 470  lowlevel.mfcc.cov_108                    float64\n",
      " 471  lowlevel.mfcc.cov_109                    float64\n",
      " 472  lowlevel.mfcc.cov_110                    float64\n",
      " 473  lowlevel.mfcc.cov_111                    float64\n",
      " 474  lowlevel.mfcc.cov_112                    float64\n",
      " 475  lowlevel.mfcc.cov_113                    float64\n",
      " 476  lowlevel.mfcc.cov_114                    float64\n",
      " 477  lowlevel.mfcc.cov_115                    float64\n",
      " 478  lowlevel.mfcc.cov_116                    float64\n",
      " 479  lowlevel.mfcc.cov_117                    float64\n",
      " 480  lowlevel.mfcc.cov_118                    float64\n",
      " 481  lowlevel.mfcc.cov_119                    float64\n",
      " 482  lowlevel.mfcc.cov_120                    float64\n",
      " 483  lowlevel.mfcc.cov_121                    float64\n",
      " 484  lowlevel.mfcc.cov_122                    float64\n",
      " 485  lowlevel.mfcc.cov_123                    float64\n",
      " 486  lowlevel.mfcc.cov_124                    float64\n",
      " 487  lowlevel.mfcc.cov_125                    float64\n",
      " 488  lowlevel.mfcc.cov_126                    float64\n",
      " 489  lowlevel.mfcc.cov_127                    float64\n",
      " 490  lowlevel.mfcc.cov_128                    float64\n",
      " 491  lowlevel.mfcc.cov_129                    float64\n",
      " 492  lowlevel.mfcc.cov_130                    float64\n",
      " 493  lowlevel.mfcc.cov_131                    float64\n",
      " 494  lowlevel.mfcc.cov_132                    float64\n",
      " 495  lowlevel.mfcc.cov_133                    float64\n",
      " 496  lowlevel.mfcc.cov_134                    float64\n",
      " 497  lowlevel.mfcc.cov_135                    float64\n",
      " 498  lowlevel.mfcc.cov_136                    float64\n",
      " 499  lowlevel.mfcc.cov_137                    float64\n",
      " 500  lowlevel.mfcc.cov_138                    float64\n",
      " 501  lowlevel.mfcc.cov_139                    float64\n",
      " 502  lowlevel.mfcc.cov_140                    float64\n",
      " 503  lowlevel.mfcc.cov_141                    float64\n",
      " 504  lowlevel.mfcc.cov_142                    float64\n",
      " 505  lowlevel.mfcc.cov_143                    float64\n",
      " 506  lowlevel.mfcc.cov_144                    float64\n",
      " 507  lowlevel.mfcc.cov_145                    float64\n",
      " 508  lowlevel.mfcc.cov_146                    float64\n",
      " 509  lowlevel.mfcc.cov_147                    float64\n",
      " 510  lowlevel.mfcc.cov_148                    float64\n",
      " 511  lowlevel.mfcc.cov_149                    float64\n",
      " 512  lowlevel.mfcc.cov_150                    float64\n",
      " 513  lowlevel.mfcc.cov_151                    float64\n",
      " 514  lowlevel.mfcc.cov_152                    float64\n",
      " 515  lowlevel.mfcc.cov_153                    float64\n",
      " 516  lowlevel.mfcc.cov_154                    float64\n",
      " 517  lowlevel.mfcc.cov_155                    float64\n",
      " 518  lowlevel.mfcc.cov_156                    float64\n",
      " 519  lowlevel.mfcc.cov_157                    float64\n",
      " 520  lowlevel.mfcc.cov_158                    float64\n",
      " 521  lowlevel.mfcc.cov_159                    float64\n",
      " 522  lowlevel.mfcc.cov_160                    float64\n",
      " 523  lowlevel.mfcc.cov_161                    float64\n",
      " 524  lowlevel.mfcc.cov_162                    float64\n",
      " 525  lowlevel.mfcc.cov_163                    float64\n",
      " 526  lowlevel.mfcc.cov_164                    float64\n",
      " 527  lowlevel.mfcc.cov_165                    float64\n",
      " 528  lowlevel.mfcc.cov_166                    float64\n",
      " 529  lowlevel.mfcc.cov_167                    float64\n",
      " 530  lowlevel.mfcc.cov_168                    float64\n",
      " 531  lowlevel.mfcc.icov_0                     float64\n",
      " 532  lowlevel.mfcc.icov_1                     float64\n",
      " 533  lowlevel.mfcc.icov_2                     float64\n",
      " 534  lowlevel.mfcc.icov_3                     float64\n",
      " 535  lowlevel.mfcc.icov_4                     float64\n",
      " 536  lowlevel.mfcc.icov_5                     float64\n",
      " 537  lowlevel.mfcc.icov_6                     float64\n",
      " 538  lowlevel.mfcc.icov_7                     float64\n",
      " 539  lowlevel.mfcc.icov_8                     float64\n",
      " 540  lowlevel.mfcc.icov_9                     float64\n",
      " 541  lowlevel.mfcc.icov_10                    float64\n",
      " 542  lowlevel.mfcc.icov_11                    float64\n",
      " 543  lowlevel.mfcc.icov_12                    float64\n",
      " 544  lowlevel.mfcc.icov_13                    float64\n",
      " 545  lowlevel.mfcc.icov_14                    float64\n",
      " 546  lowlevel.mfcc.icov_15                    float64\n",
      " 547  lowlevel.mfcc.icov_16                    float64\n",
      " 548  lowlevel.mfcc.icov_17                    float64\n",
      " 549  lowlevel.mfcc.icov_18                    float64\n",
      " 550  lowlevel.mfcc.icov_19                    float64\n",
      " 551  lowlevel.mfcc.icov_20                    float64\n",
      " 552  lowlevel.mfcc.icov_21                    float64\n",
      " 553  lowlevel.mfcc.icov_22                    float64\n",
      " 554  lowlevel.mfcc.icov_23                    float64\n",
      " 555  lowlevel.mfcc.icov_24                    float64\n",
      " 556  lowlevel.mfcc.icov_25                    float64\n",
      " 557  lowlevel.mfcc.icov_26                    float64\n",
      " 558  lowlevel.mfcc.icov_27                    float64\n",
      " 559  lowlevel.mfcc.icov_28                    float64\n",
      " 560  lowlevel.mfcc.icov_29                    float64\n",
      " 561  lowlevel.mfcc.icov_30                    float64\n",
      " 562  lowlevel.mfcc.icov_31                    float64\n",
      " 563  lowlevel.mfcc.icov_32                    float64\n",
      " 564  lowlevel.mfcc.icov_33                    float64\n",
      " 565  lowlevel.mfcc.icov_34                    float64\n",
      " 566  lowlevel.mfcc.icov_35                    float64\n",
      " 567  lowlevel.mfcc.icov_36                    float64\n",
      " 568  lowlevel.mfcc.icov_37                    float64\n",
      " 569  lowlevel.mfcc.icov_38                    float64\n",
      " 570  lowlevel.mfcc.icov_39                    float64\n",
      " 571  lowlevel.mfcc.icov_40                    float64\n",
      " 572  lowlevel.mfcc.icov_41                    float64\n",
      " 573  lowlevel.mfcc.icov_42                    float64\n",
      " 574  lowlevel.mfcc.icov_43                    float64\n",
      " 575  lowlevel.mfcc.icov_44                    float64\n",
      " 576  lowlevel.mfcc.icov_45                    float64\n",
      " 577  lowlevel.mfcc.icov_46                    float64\n",
      " 578  lowlevel.mfcc.icov_47                    float64\n",
      " 579  lowlevel.mfcc.icov_48                    float64\n",
      " 580  lowlevel.mfcc.icov_49                    float64\n",
      " 581  lowlevel.mfcc.icov_50                    float64\n",
      " 582  lowlevel.mfcc.icov_51                    float64\n",
      " 583  lowlevel.mfcc.icov_52                    float64\n",
      " 584  lowlevel.mfcc.icov_53                    float64\n",
      " 585  lowlevel.mfcc.icov_54                    float64\n",
      " 586  lowlevel.mfcc.icov_55                    float64\n",
      " 587  lowlevel.mfcc.icov_56                    float64\n",
      " 588  lowlevel.mfcc.icov_57                    float64\n",
      " 589  lowlevel.mfcc.icov_58                    float64\n",
      " 590  lowlevel.mfcc.icov_59                    float64\n",
      " 591  lowlevel.mfcc.icov_60                    float64\n",
      " 592  lowlevel.mfcc.icov_61                    float64\n",
      " 593  lowlevel.mfcc.icov_62                    float64\n",
      " 594  lowlevel.mfcc.icov_63                    float64\n",
      " 595  lowlevel.mfcc.icov_64                    float64\n",
      " 596  lowlevel.mfcc.icov_65                    float64\n",
      " 597  lowlevel.mfcc.icov_66                    float64\n",
      " 598  lowlevel.mfcc.icov_67                    float64\n",
      " 599  lowlevel.mfcc.icov_68                    float64\n",
      " 600  lowlevel.mfcc.icov_69                    float64\n",
      " 601  lowlevel.mfcc.icov_70                    float64\n",
      " 602  lowlevel.mfcc.icov_71                    float64\n",
      " 603  lowlevel.mfcc.icov_72                    float64\n",
      " 604  lowlevel.mfcc.icov_73                    float64\n",
      " 605  lowlevel.mfcc.icov_74                    float64\n",
      " 606  lowlevel.mfcc.icov_75                    float64\n",
      " 607  lowlevel.mfcc.icov_76                    float64\n",
      " 608  lowlevel.mfcc.icov_77                    float64\n",
      " 609  lowlevel.mfcc.icov_78                    float64\n",
      " 610  lowlevel.mfcc.icov_79                    float64\n",
      " 611  lowlevel.mfcc.icov_80                    float64\n",
      " 612  lowlevel.mfcc.icov_81                    float64\n",
      " 613  lowlevel.mfcc.icov_82                    float64\n",
      " 614  lowlevel.mfcc.icov_83                    float64\n",
      " 615  lowlevel.mfcc.icov_84                    float64\n",
      " 616  lowlevel.mfcc.icov_85                    float64\n",
      " 617  lowlevel.mfcc.icov_86                    float64\n",
      " 618  lowlevel.mfcc.icov_87                    float64\n",
      " 619  lowlevel.mfcc.icov_88                    float64\n",
      " 620  lowlevel.mfcc.icov_89                    float64\n",
      " 621  lowlevel.mfcc.icov_90                    float64\n",
      " 622  lowlevel.mfcc.icov_91                    float64\n",
      " 623  lowlevel.mfcc.icov_92                    float64\n",
      " 624  lowlevel.mfcc.icov_93                    float64\n",
      " 625  lowlevel.mfcc.icov_94                    float64\n",
      " 626  lowlevel.mfcc.icov_95                    float64\n",
      " 627  lowlevel.mfcc.icov_96                    float64\n",
      " 628  lowlevel.mfcc.icov_97                    float64\n",
      " 629  lowlevel.mfcc.icov_98                    float64\n",
      " 630  lowlevel.mfcc.icov_99                    float64\n",
      " 631  lowlevel.mfcc.icov_100                   float64\n",
      " 632  lowlevel.mfcc.icov_101                   float64\n",
      " 633  lowlevel.mfcc.icov_102                   float64\n",
      " 634  lowlevel.mfcc.icov_103                   float64\n",
      " 635  lowlevel.mfcc.icov_104                   float64\n",
      " 636  lowlevel.mfcc.icov_105                   float64\n",
      " 637  lowlevel.mfcc.icov_106                   float64\n",
      " 638  lowlevel.mfcc.icov_107                   float64\n",
      " 639  lowlevel.mfcc.icov_108                   float64\n",
      " 640  lowlevel.mfcc.icov_109                   float64\n",
      " 641  lowlevel.mfcc.icov_110                   float64\n",
      " 642  lowlevel.mfcc.icov_111                   float64\n",
      " 643  lowlevel.mfcc.icov_112                   float64\n",
      " 644  lowlevel.mfcc.icov_113                   float64\n",
      " 645  lowlevel.mfcc.icov_114                   float64\n",
      " 646  lowlevel.mfcc.icov_115                   float64\n",
      " 647  lowlevel.mfcc.icov_116                   float64\n",
      " 648  lowlevel.mfcc.icov_117                   float64\n",
      " 649  lowlevel.mfcc.icov_118                   float64\n",
      " 650  lowlevel.mfcc.icov_119                   float64\n",
      " 651  lowlevel.mfcc.icov_120                   float64\n",
      " 652  lowlevel.mfcc.icov_121                   float64\n",
      " 653  lowlevel.mfcc.icov_122                   float64\n",
      " 654  lowlevel.mfcc.icov_123                   float64\n",
      " 655  lowlevel.mfcc.icov_124                   float64\n",
      " 656  lowlevel.mfcc.icov_125                   float64\n",
      " 657  lowlevel.mfcc.icov_126                   float64\n",
      " 658  lowlevel.mfcc.icov_127                   float64\n",
      " 659  lowlevel.mfcc.icov_128                   float64\n",
      " 660  lowlevel.mfcc.icov_129                   float64\n",
      " 661  lowlevel.mfcc.icov_130                   float64\n",
      " 662  lowlevel.mfcc.icov_131                   float64\n",
      " 663  lowlevel.mfcc.icov_132                   float64\n",
      " 664  lowlevel.mfcc.icov_133                   float64\n",
      " 665  lowlevel.mfcc.icov_134                   float64\n",
      " 666  lowlevel.mfcc.icov_135                   float64\n",
      " 667  lowlevel.mfcc.icov_136                   float64\n",
      " 668  lowlevel.mfcc.icov_137                   float64\n",
      " 669  lowlevel.mfcc.icov_138                   float64\n",
      " 670  lowlevel.mfcc.icov_139                   float64\n",
      " 671  lowlevel.mfcc.icov_140                   float64\n",
      " 672  lowlevel.mfcc.icov_141                   float64\n",
      " 673  lowlevel.mfcc.icov_142                   float64\n",
      " 674  lowlevel.mfcc.icov_143                   float64\n",
      " 675  lowlevel.mfcc.icov_144                   float64\n",
      " 676  lowlevel.mfcc.icov_145                   float64\n",
      " 677  lowlevel.mfcc.icov_146                   float64\n",
      " 678  lowlevel.mfcc.icov_147                   float64\n",
      " 679  lowlevel.mfcc.icov_148                   float64\n",
      " 680  lowlevel.mfcc.icov_149                   float64\n",
      " 681  lowlevel.mfcc.icov_150                   float64\n",
      " 682  lowlevel.mfcc.icov_151                   float64\n",
      " 683  lowlevel.mfcc.icov_152                   float64\n",
      " 684  lowlevel.mfcc.icov_153                   float64\n",
      " 685  lowlevel.mfcc.icov_154                   float64\n",
      " 686  lowlevel.mfcc.icov_155                   float64\n",
      " 687  lowlevel.mfcc.icov_156                   float64\n",
      " 688  lowlevel.mfcc.icov_157                   float64\n",
      " 689  lowlevel.mfcc.icov_158                   float64\n",
      " 690  lowlevel.mfcc.icov_159                   float64\n",
      " 691  lowlevel.mfcc.icov_160                   float64\n",
      " 692  lowlevel.mfcc.icov_161                   float64\n",
      " 693  lowlevel.mfcc.icov_162                   float64\n",
      " 694  lowlevel.mfcc.icov_163                   float64\n",
      " 695  lowlevel.mfcc.icov_164                   float64\n",
      " 696  lowlevel.mfcc.icov_165                   float64\n",
      " 697  lowlevel.mfcc.icov_166                   float64\n",
      " 698  lowlevel.mfcc.icov_167                   float64\n",
      " 699  lowlevel.mfcc.icov_168                   float64\n",
      " 700  lowlevel.mfcc.mean_0                     float64\n",
      " 701  lowlevel.mfcc.mean_1                     float64\n",
      " 702  lowlevel.mfcc.mean_2                     float64\n",
      " 703  lowlevel.mfcc.mean_3                     float64\n",
      " 704  lowlevel.mfcc.mean_4                     float64\n",
      " 705  lowlevel.mfcc.mean_5                     float64\n",
      " 706  lowlevel.mfcc.mean_6                     float64\n",
      " 707  lowlevel.mfcc.mean_7                     float64\n",
      " 708  lowlevel.mfcc.mean_8                     float64\n",
      " 709  lowlevel.mfcc.mean_9                     float64\n",
      " 710  lowlevel.mfcc.mean_10                    float64\n",
      " 711  lowlevel.mfcc.mean_11                    float64\n",
      " 712  lowlevel.mfcc.mean_12                    float64\n",
      " 713  rhythm.beats_loudness_band_ratio.mean_0  float64\n",
      " 714  rhythm.beats_loudness_band_ratio.mean_1  float64\n",
      " 715  rhythm.beats_loudness_band_ratio.mean_2  float64\n",
      " 716  rhythm.beats_loudness_band_ratio.mean_3  float64\n",
      " 717  rhythm.beats_loudness_band_ratio.mean_4  float64\n",
      " 718  rhythm.beats_loudness_band_ratio.mean_5  float64\n",
      " 719  tonal.chords_histogram_0                 float64\n",
      " 720  tonal.chords_histogram_1                 float64\n",
      " 721  tonal.chords_histogram_2                 float64\n",
      " 722  tonal.chords_histogram_3                 float64\n",
      " 723  tonal.chords_histogram_4                 float64\n",
      " 724  tonal.chords_histogram_5                 float64\n",
      " 725  tonal.chords_histogram_6                 float64\n",
      " 726  tonal.chords_histogram_7                 float64\n",
      " 727  tonal.chords_histogram_8                 float64\n",
      " 728  tonal.chords_histogram_9                 float64\n",
      " 729  tonal.chords_histogram_10                float64\n",
      " 730  tonal.chords_histogram_11                float64\n",
      " 731  tonal.chords_histogram_12                float64\n",
      " 732  tonal.chords_histogram_13                float64\n",
      " 733  tonal.chords_histogram_14                float64\n",
      " 734  tonal.chords_histogram_15                float64\n",
      " 735  tonal.chords_histogram_16                float64\n",
      " 736  tonal.chords_histogram_17                float64\n",
      " 737  tonal.chords_histogram_18                float64\n",
      " 738  tonal.chords_histogram_19                float64\n",
      " 739  tonal.chords_histogram_20                float64\n",
      " 740  tonal.chords_histogram_21                float64\n",
      " 741  tonal.chords_histogram_22                float64\n",
      " 742  tonal.chords_histogram_23                float64\n",
      "dtypes: float64(742), int64(1)\n",
      "memory usage: 4.3 MB\n"
     ]
    }
   ],
   "source": [
    "df_essentia_best_valence_features_mean.info(verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join both the featureset and annotation set together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lowlevel.melbands_kurtosis.mean</th>\n",
       "      <th>lowlevel.melbands_skewness.mean</th>\n",
       "      <th>lowlevel.spectral_energy.mean</th>\n",
       "      <th>lowlevel.zerocrossingrate.mean</th>\n",
       "      <th>rhythm.beats_loudness.mean</th>\n",
       "      <th>rhythm.onset_rate</th>\n",
       "      <th>tonal.chords_strength.mean</th>\n",
       "      <th>tonal.hpcp_entropy.mean</th>\n",
       "      <th>tonal.key_edma.strength</th>\n",
       "      <th>tonal.key_temperley.strength</th>\n",
       "      <th>...</th>\n",
       "      <th>tonal.chords_histogram_16</th>\n",
       "      <th>tonal.chords_histogram_17</th>\n",
       "      <th>tonal.chords_histogram_18</th>\n",
       "      <th>tonal.chords_histogram_19</th>\n",
       "      <th>tonal.chords_histogram_20</th>\n",
       "      <th>tonal.chords_histogram_21</th>\n",
       "      <th>tonal.chords_histogram_22</th>\n",
       "      <th>tonal.chords_histogram_23</th>\n",
       "      <th>valence_mean_mapped</th>\n",
       "      <th>arousal_mean_mapped</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.064362</td>\n",
       "      <td>0.214289</td>\n",
       "      <td>0.125964</td>\n",
       "      <td>0.416985</td>\n",
       "      <td>0.086404</td>\n",
       "      <td>0.810704</td>\n",
       "      <td>0.267193</td>\n",
       "      <td>0.482615</td>\n",
       "      <td>0.533418</td>\n",
       "      <td>0.570785</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.089160</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.223899</td>\n",
       "      <td>0.098657</td>\n",
       "      <td>0.288181</td>\n",
       "      <td>0.150</td>\n",
       "      <td>-0.200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.345452</td>\n",
       "      <td>0.539877</td>\n",
       "      <td>0.949078</td>\n",
       "      <td>0.050339</td>\n",
       "      <td>0.462009</td>\n",
       "      <td>0.795216</td>\n",
       "      <td>0.304507</td>\n",
       "      <td>0.461919</td>\n",
       "      <td>0.163914</td>\n",
       "      <td>0.158169</td>\n",
       "      <td>...</td>\n",
       "      <td>0.402838</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.030457</td>\n",
       "      <td>0.117746</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.107674</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.155779</td>\n",
       "      <td>-0.425</td>\n",
       "      <td>-0.475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.566046</td>\n",
       "      <td>0.636537</td>\n",
       "      <td>0.525953</td>\n",
       "      <td>0.171160</td>\n",
       "      <td>0.308063</td>\n",
       "      <td>0.844622</td>\n",
       "      <td>0.464434</td>\n",
       "      <td>0.609828</td>\n",
       "      <td>0.714130</td>\n",
       "      <td>0.649047</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008505</td>\n",
       "      <td>0.007329</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.600</td>\n",
       "      <td>-0.700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.168166</td>\n",
       "      <td>0.426521</td>\n",
       "      <td>0.278434</td>\n",
       "      <td>0.265749</td>\n",
       "      <td>0.139343</td>\n",
       "      <td>0.765399</td>\n",
       "      <td>0.362823</td>\n",
       "      <td>0.392684</td>\n",
       "      <td>0.381464</td>\n",
       "      <td>0.311019</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.051213</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.300</td>\n",
       "      <td>0.025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.050890</td>\n",
       "      <td>0.193447</td>\n",
       "      <td>0.201743</td>\n",
       "      <td>0.447316</td>\n",
       "      <td>0.231818</td>\n",
       "      <td>0.578654</td>\n",
       "      <td>0.445732</td>\n",
       "      <td>0.675341</td>\n",
       "      <td>0.746939</td>\n",
       "      <td>0.781565</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.165104</td>\n",
       "      <td>0.096976</td>\n",
       "      <td>0.450</td>\n",
       "      <td>0.400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>762</th>\n",
       "      <td>0.058467</td>\n",
       "      <td>0.186511</td>\n",
       "      <td>0.184759</td>\n",
       "      <td>0.536035</td>\n",
       "      <td>0.124013</td>\n",
       "      <td>0.551773</td>\n",
       "      <td>0.555560</td>\n",
       "      <td>0.580548</td>\n",
       "      <td>0.612516</td>\n",
       "      <td>0.656733</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.031499</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013240</td>\n",
       "      <td>0.156009</td>\n",
       "      <td>0.863862</td>\n",
       "      <td>0.525</td>\n",
       "      <td>0.725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>763</th>\n",
       "      <td>0.121857</td>\n",
       "      <td>0.234562</td>\n",
       "      <td>0.179638</td>\n",
       "      <td>0.633059</td>\n",
       "      <td>0.073642</td>\n",
       "      <td>0.674339</td>\n",
       "      <td>0.552154</td>\n",
       "      <td>0.660622</td>\n",
       "      <td>0.638703</td>\n",
       "      <td>0.659746</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.044460</td>\n",
       "      <td>0.016906</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>764</th>\n",
       "      <td>0.046206</td>\n",
       "      <td>0.200557</td>\n",
       "      <td>0.195031</td>\n",
       "      <td>0.620730</td>\n",
       "      <td>0.097882</td>\n",
       "      <td>0.520823</td>\n",
       "      <td>0.144097</td>\n",
       "      <td>0.571115</td>\n",
       "      <td>0.554702</td>\n",
       "      <td>0.580168</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.053602</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.027702</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.149709</td>\n",
       "      <td>0.035320</td>\n",
       "      <td>0.149458</td>\n",
       "      <td>0.325</td>\n",
       "      <td>0.425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>765</th>\n",
       "      <td>0.063658</td>\n",
       "      <td>0.238118</td>\n",
       "      <td>0.238011</td>\n",
       "      <td>0.658759</td>\n",
       "      <td>0.140357</td>\n",
       "      <td>0.433236</td>\n",
       "      <td>0.136955</td>\n",
       "      <td>0.490124</td>\n",
       "      <td>0.476657</td>\n",
       "      <td>0.512086</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005471</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.051231</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.112450</td>\n",
       "      <td>0.019859</td>\n",
       "      <td>0.550</td>\n",
       "      <td>0.750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>766</th>\n",
       "      <td>0.028755</td>\n",
       "      <td>0.176146</td>\n",
       "      <td>0.162377</td>\n",
       "      <td>0.698038</td>\n",
       "      <td>0.080558</td>\n",
       "      <td>0.338497</td>\n",
       "      <td>0.335940</td>\n",
       "      <td>0.598790</td>\n",
       "      <td>0.402670</td>\n",
       "      <td>0.382374</td>\n",
       "      <td>...</td>\n",
       "      <td>0.483745</td>\n",
       "      <td>0.073610</td>\n",
       "      <td>0.174831</td>\n",
       "      <td>0.019717</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000761</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.325</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>767 rows Ã— 744 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     lowlevel.melbands_kurtosis.mean  lowlevel.melbands_skewness.mean  \\\n",
       "0                           0.064362                         0.214289   \n",
       "1                           0.345452                         0.539877   \n",
       "2                           0.566046                         0.636537   \n",
       "3                           0.168166                         0.426521   \n",
       "4                           0.050890                         0.193447   \n",
       "..                               ...                              ...   \n",
       "762                         0.058467                         0.186511   \n",
       "763                         0.121857                         0.234562   \n",
       "764                         0.046206                         0.200557   \n",
       "765                         0.063658                         0.238118   \n",
       "766                         0.028755                         0.176146   \n",
       "\n",
       "     lowlevel.spectral_energy.mean  lowlevel.zerocrossingrate.mean  \\\n",
       "0                         0.125964                        0.416985   \n",
       "1                         0.949078                        0.050339   \n",
       "2                         0.525953                        0.171160   \n",
       "3                         0.278434                        0.265749   \n",
       "4                         0.201743                        0.447316   \n",
       "..                             ...                             ...   \n",
       "762                       0.184759                        0.536035   \n",
       "763                       0.179638                        0.633059   \n",
       "764                       0.195031                        0.620730   \n",
       "765                       0.238011                        0.658759   \n",
       "766                       0.162377                        0.698038   \n",
       "\n",
       "     rhythm.beats_loudness.mean  rhythm.onset_rate  \\\n",
       "0                      0.086404           0.810704   \n",
       "1                      0.462009           0.795216   \n",
       "2                      0.308063           0.844622   \n",
       "3                      0.139343           0.765399   \n",
       "4                      0.231818           0.578654   \n",
       "..                          ...                ...   \n",
       "762                    0.124013           0.551773   \n",
       "763                    0.073642           0.674339   \n",
       "764                    0.097882           0.520823   \n",
       "765                    0.140357           0.433236   \n",
       "766                    0.080558           0.338497   \n",
       "\n",
       "     tonal.chords_strength.mean  tonal.hpcp_entropy.mean  \\\n",
       "0                      0.267193                 0.482615   \n",
       "1                      0.304507                 0.461919   \n",
       "2                      0.464434                 0.609828   \n",
       "3                      0.362823                 0.392684   \n",
       "4                      0.445732                 0.675341   \n",
       "..                          ...                      ...   \n",
       "762                    0.555560                 0.580548   \n",
       "763                    0.552154                 0.660622   \n",
       "764                    0.144097                 0.571115   \n",
       "765                    0.136955                 0.490124   \n",
       "766                    0.335940                 0.598790   \n",
       "\n",
       "     tonal.key_edma.strength  tonal.key_temperley.strength  ...  \\\n",
       "0                   0.533418                      0.570785  ...   \n",
       "1                   0.163914                      0.158169  ...   \n",
       "2                   0.714130                      0.649047  ...   \n",
       "3                   0.381464                      0.311019  ...   \n",
       "4                   0.746939                      0.781565  ...   \n",
       "..                       ...                           ...  ...   \n",
       "762                 0.612516                      0.656733  ...   \n",
       "763                 0.638703                      0.659746  ...   \n",
       "764                 0.554702                      0.580168  ...   \n",
       "765                 0.476657                      0.512086  ...   \n",
       "766                 0.402670                      0.382374  ...   \n",
       "\n",
       "     tonal.chords_histogram_16  tonal.chords_histogram_17  \\\n",
       "0                     0.000000                   0.000000   \n",
       "1                     0.402838                   0.000000   \n",
       "2                     0.000000                   0.000000   \n",
       "3                     0.000000                   0.000000   \n",
       "4                     0.000000                   0.000000   \n",
       "..                         ...                        ...   \n",
       "762                   0.000000                   0.031499   \n",
       "763                   0.000000                   0.000000   \n",
       "764                   0.000000                   0.053602   \n",
       "765                   0.000000                   0.000000   \n",
       "766                   0.483745                   0.073610   \n",
       "\n",
       "     tonal.chords_histogram_18  tonal.chords_histogram_19  \\\n",
       "0                     0.000000                   0.089160   \n",
       "1                     0.030457                   0.117746   \n",
       "2                     0.000000                   0.008505   \n",
       "3                     0.000000                   0.000000   \n",
       "4                     0.000000                   0.000000   \n",
       "..                         ...                        ...   \n",
       "762                   0.000000                   0.000000   \n",
       "763                   0.000000                   0.000000   \n",
       "764                   0.000000                   0.027702   \n",
       "765                   0.005471                   0.000000   \n",
       "766                   0.174831                   0.019717   \n",
       "\n",
       "     tonal.chords_histogram_20  tonal.chords_histogram_21  \\\n",
       "0                     0.000000                   0.223899   \n",
       "1                     0.000000                   0.107674   \n",
       "2                     0.007329                   0.000000   \n",
       "3                     0.000000                   0.051213   \n",
       "4                     0.000000                   0.142857   \n",
       "..                         ...                        ...   \n",
       "762                   0.000000                   0.013240   \n",
       "763                   0.000000                   0.000000   \n",
       "764                   0.000000                   0.149709   \n",
       "765                   0.051231                   0.000000   \n",
       "766                   0.000000                   0.000000   \n",
       "\n",
       "     tonal.chords_histogram_22  tonal.chords_histogram_23  \\\n",
       "0                     0.098657                   0.288181   \n",
       "1                     0.000000                   0.155779   \n",
       "2                     0.000000                   0.000000   \n",
       "3                     0.000000                   0.000000   \n",
       "4                     0.165104                   0.096976   \n",
       "..                         ...                        ...   \n",
       "762                   0.156009                   0.863862   \n",
       "763                   0.044460                   0.016906   \n",
       "764                   0.035320                   0.149458   \n",
       "765                   0.112450                   0.019859   \n",
       "766                   0.000761                   0.000000   \n",
       "\n",
       "     valence_mean_mapped  arousal_mean_mapped  \n",
       "0                  0.150               -0.200  \n",
       "1                 -0.425               -0.475  \n",
       "2                 -0.600               -0.700  \n",
       "3                 -0.300                0.025  \n",
       "4                  0.450                0.400  \n",
       "..                   ...                  ...  \n",
       "762                0.525                0.725  \n",
       "763                0.125                0.750  \n",
       "764                0.325                0.425  \n",
       "765                0.550                0.750  \n",
       "766                0.150                0.325  \n",
       "\n",
       "[767 rows x 744 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_essentia_best_valence_features_mean_whole = pd.merge(df_essentia_best_valence_features_mean, df_annotations, how='inner', on='song_id')\n",
    "df_essentia_best_valence_features_mean_whole = df_essentia_best_valence_features_mean_whole.drop('song_id', axis=1)\n",
    "df_essentia_best_valence_features_mean_whole"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare dataframes for the neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform splitting of the dataframe into training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lowlevel.melbands_kurtosis.mean</th>\n",
       "      <th>lowlevel.melbands_skewness.mean</th>\n",
       "      <th>lowlevel.spectral_energy.mean</th>\n",
       "      <th>lowlevel.zerocrossingrate.mean</th>\n",
       "      <th>rhythm.beats_loudness.mean</th>\n",
       "      <th>rhythm.onset_rate</th>\n",
       "      <th>tonal.chords_strength.mean</th>\n",
       "      <th>tonal.hpcp_entropy.mean</th>\n",
       "      <th>tonal.key_edma.strength</th>\n",
       "      <th>tonal.key_temperley.strength</th>\n",
       "      <th>...</th>\n",
       "      <th>tonal.chords_histogram_14</th>\n",
       "      <th>tonal.chords_histogram_15</th>\n",
       "      <th>tonal.chords_histogram_16</th>\n",
       "      <th>tonal.chords_histogram_17</th>\n",
       "      <th>tonal.chords_histogram_18</th>\n",
       "      <th>tonal.chords_histogram_19</th>\n",
       "      <th>tonal.chords_histogram_20</th>\n",
       "      <th>tonal.chords_histogram_21</th>\n",
       "      <th>tonal.chords_histogram_22</th>\n",
       "      <th>tonal.chords_histogram_23</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.064362</td>\n",
       "      <td>0.214289</td>\n",
       "      <td>0.125964</td>\n",
       "      <td>0.416985</td>\n",
       "      <td>0.086404</td>\n",
       "      <td>0.810704</td>\n",
       "      <td>0.267193</td>\n",
       "      <td>0.482615</td>\n",
       "      <td>0.533418</td>\n",
       "      <td>0.570785</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.089160</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.223899</td>\n",
       "      <td>0.098657</td>\n",
       "      <td>0.288181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.345452</td>\n",
       "      <td>0.539877</td>\n",
       "      <td>0.949078</td>\n",
       "      <td>0.050339</td>\n",
       "      <td>0.462009</td>\n",
       "      <td>0.795216</td>\n",
       "      <td>0.304507</td>\n",
       "      <td>0.461919</td>\n",
       "      <td>0.163914</td>\n",
       "      <td>0.158169</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016972</td>\n",
       "      <td>0.077021</td>\n",
       "      <td>0.402838</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.030457</td>\n",
       "      <td>0.117746</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.107674</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.155779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.566046</td>\n",
       "      <td>0.636537</td>\n",
       "      <td>0.525953</td>\n",
       "      <td>0.171160</td>\n",
       "      <td>0.308063</td>\n",
       "      <td>0.844622</td>\n",
       "      <td>0.464434</td>\n",
       "      <td>0.609828</td>\n",
       "      <td>0.714130</td>\n",
       "      <td>0.649047</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008505</td>\n",
       "      <td>0.007329</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.168166</td>\n",
       "      <td>0.426521</td>\n",
       "      <td>0.278434</td>\n",
       "      <td>0.265749</td>\n",
       "      <td>0.139343</td>\n",
       "      <td>0.765399</td>\n",
       "      <td>0.362823</td>\n",
       "      <td>0.392684</td>\n",
       "      <td>0.381464</td>\n",
       "      <td>0.311019</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.063161</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.051213</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.050890</td>\n",
       "      <td>0.193447</td>\n",
       "      <td>0.201743</td>\n",
       "      <td>0.447316</td>\n",
       "      <td>0.231818</td>\n",
       "      <td>0.578654</td>\n",
       "      <td>0.445732</td>\n",
       "      <td>0.675341</td>\n",
       "      <td>0.746939</td>\n",
       "      <td>0.781565</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.165104</td>\n",
       "      <td>0.096976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>762</th>\n",
       "      <td>0.058467</td>\n",
       "      <td>0.186511</td>\n",
       "      <td>0.184759</td>\n",
       "      <td>0.536035</td>\n",
       "      <td>0.124013</td>\n",
       "      <td>0.551773</td>\n",
       "      <td>0.555560</td>\n",
       "      <td>0.580548</td>\n",
       "      <td>0.612516</td>\n",
       "      <td>0.656733</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.031499</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013240</td>\n",
       "      <td>0.156009</td>\n",
       "      <td>0.863862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>763</th>\n",
       "      <td>0.121857</td>\n",
       "      <td>0.234562</td>\n",
       "      <td>0.179638</td>\n",
       "      <td>0.633059</td>\n",
       "      <td>0.073642</td>\n",
       "      <td>0.674339</td>\n",
       "      <td>0.552154</td>\n",
       "      <td>0.660622</td>\n",
       "      <td>0.638703</td>\n",
       "      <td>0.659746</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.044460</td>\n",
       "      <td>0.016906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>764</th>\n",
       "      <td>0.046206</td>\n",
       "      <td>0.200557</td>\n",
       "      <td>0.195031</td>\n",
       "      <td>0.620730</td>\n",
       "      <td>0.097882</td>\n",
       "      <td>0.520823</td>\n",
       "      <td>0.144097</td>\n",
       "      <td>0.571115</td>\n",
       "      <td>0.554702</td>\n",
       "      <td>0.580168</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008208</td>\n",
       "      <td>0.025688</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.053602</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.027702</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.149709</td>\n",
       "      <td>0.035320</td>\n",
       "      <td>0.149458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>765</th>\n",
       "      <td>0.063658</td>\n",
       "      <td>0.238118</td>\n",
       "      <td>0.238011</td>\n",
       "      <td>0.658759</td>\n",
       "      <td>0.140357</td>\n",
       "      <td>0.433236</td>\n",
       "      <td>0.136955</td>\n",
       "      <td>0.490124</td>\n",
       "      <td>0.476657</td>\n",
       "      <td>0.512086</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005471</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.051231</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.112450</td>\n",
       "      <td>0.019859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>766</th>\n",
       "      <td>0.028755</td>\n",
       "      <td>0.176146</td>\n",
       "      <td>0.162377</td>\n",
       "      <td>0.698038</td>\n",
       "      <td>0.080558</td>\n",
       "      <td>0.338497</td>\n",
       "      <td>0.335940</td>\n",
       "      <td>0.598790</td>\n",
       "      <td>0.402670</td>\n",
       "      <td>0.382374</td>\n",
       "      <td>...</td>\n",
       "      <td>0.155668</td>\n",
       "      <td>0.333946</td>\n",
       "      <td>0.483745</td>\n",
       "      <td>0.073610</td>\n",
       "      <td>0.174831</td>\n",
       "      <td>0.019717</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000761</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>767 rows Ã— 742 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     lowlevel.melbands_kurtosis.mean  lowlevel.melbands_skewness.mean  \\\n",
       "0                           0.064362                         0.214289   \n",
       "1                           0.345452                         0.539877   \n",
       "2                           0.566046                         0.636537   \n",
       "3                           0.168166                         0.426521   \n",
       "4                           0.050890                         0.193447   \n",
       "..                               ...                              ...   \n",
       "762                         0.058467                         0.186511   \n",
       "763                         0.121857                         0.234562   \n",
       "764                         0.046206                         0.200557   \n",
       "765                         0.063658                         0.238118   \n",
       "766                         0.028755                         0.176146   \n",
       "\n",
       "     lowlevel.spectral_energy.mean  lowlevel.zerocrossingrate.mean  \\\n",
       "0                         0.125964                        0.416985   \n",
       "1                         0.949078                        0.050339   \n",
       "2                         0.525953                        0.171160   \n",
       "3                         0.278434                        0.265749   \n",
       "4                         0.201743                        0.447316   \n",
       "..                             ...                             ...   \n",
       "762                       0.184759                        0.536035   \n",
       "763                       0.179638                        0.633059   \n",
       "764                       0.195031                        0.620730   \n",
       "765                       0.238011                        0.658759   \n",
       "766                       0.162377                        0.698038   \n",
       "\n",
       "     rhythm.beats_loudness.mean  rhythm.onset_rate  \\\n",
       "0                      0.086404           0.810704   \n",
       "1                      0.462009           0.795216   \n",
       "2                      0.308063           0.844622   \n",
       "3                      0.139343           0.765399   \n",
       "4                      0.231818           0.578654   \n",
       "..                          ...                ...   \n",
       "762                    0.124013           0.551773   \n",
       "763                    0.073642           0.674339   \n",
       "764                    0.097882           0.520823   \n",
       "765                    0.140357           0.433236   \n",
       "766                    0.080558           0.338497   \n",
       "\n",
       "     tonal.chords_strength.mean  tonal.hpcp_entropy.mean  \\\n",
       "0                      0.267193                 0.482615   \n",
       "1                      0.304507                 0.461919   \n",
       "2                      0.464434                 0.609828   \n",
       "3                      0.362823                 0.392684   \n",
       "4                      0.445732                 0.675341   \n",
       "..                          ...                      ...   \n",
       "762                    0.555560                 0.580548   \n",
       "763                    0.552154                 0.660622   \n",
       "764                    0.144097                 0.571115   \n",
       "765                    0.136955                 0.490124   \n",
       "766                    0.335940                 0.598790   \n",
       "\n",
       "     tonal.key_edma.strength  tonal.key_temperley.strength  ...  \\\n",
       "0                   0.533418                      0.570785  ...   \n",
       "1                   0.163914                      0.158169  ...   \n",
       "2                   0.714130                      0.649047  ...   \n",
       "3                   0.381464                      0.311019  ...   \n",
       "4                   0.746939                      0.781565  ...   \n",
       "..                       ...                           ...  ...   \n",
       "762                 0.612516                      0.656733  ...   \n",
       "763                 0.638703                      0.659746  ...   \n",
       "764                 0.554702                      0.580168  ...   \n",
       "765                 0.476657                      0.512086  ...   \n",
       "766                 0.402670                      0.382374  ...   \n",
       "\n",
       "     tonal.chords_histogram_14  tonal.chords_histogram_15  \\\n",
       "0                     0.000000                   0.000000   \n",
       "1                     0.016972                   0.077021   \n",
       "2                     0.000000                   0.000000   \n",
       "3                     0.000000                   0.063161   \n",
       "4                     0.000000                   0.000000   \n",
       "..                         ...                        ...   \n",
       "762                   0.000000                   0.000000   \n",
       "763                   0.000000                   0.000000   \n",
       "764                   0.008208                   0.025688   \n",
       "765                   0.000000                   0.000000   \n",
       "766                   0.155668                   0.333946   \n",
       "\n",
       "     tonal.chords_histogram_16  tonal.chords_histogram_17  \\\n",
       "0                     0.000000                   0.000000   \n",
       "1                     0.402838                   0.000000   \n",
       "2                     0.000000                   0.000000   \n",
       "3                     0.000000                   0.000000   \n",
       "4                     0.000000                   0.000000   \n",
       "..                         ...                        ...   \n",
       "762                   0.000000                   0.031499   \n",
       "763                   0.000000                   0.000000   \n",
       "764                   0.000000                   0.053602   \n",
       "765                   0.000000                   0.000000   \n",
       "766                   0.483745                   0.073610   \n",
       "\n",
       "     tonal.chords_histogram_18  tonal.chords_histogram_19  \\\n",
       "0                     0.000000                   0.089160   \n",
       "1                     0.030457                   0.117746   \n",
       "2                     0.000000                   0.008505   \n",
       "3                     0.000000                   0.000000   \n",
       "4                     0.000000                   0.000000   \n",
       "..                         ...                        ...   \n",
       "762                   0.000000                   0.000000   \n",
       "763                   0.000000                   0.000000   \n",
       "764                   0.000000                   0.027702   \n",
       "765                   0.005471                   0.000000   \n",
       "766                   0.174831                   0.019717   \n",
       "\n",
       "     tonal.chords_histogram_20  tonal.chords_histogram_21  \\\n",
       "0                     0.000000                   0.223899   \n",
       "1                     0.000000                   0.107674   \n",
       "2                     0.007329                   0.000000   \n",
       "3                     0.000000                   0.051213   \n",
       "4                     0.000000                   0.142857   \n",
       "..                         ...                        ...   \n",
       "762                   0.000000                   0.013240   \n",
       "763                   0.000000                   0.000000   \n",
       "764                   0.000000                   0.149709   \n",
       "765                   0.051231                   0.000000   \n",
       "766                   0.000000                   0.000000   \n",
       "\n",
       "     tonal.chords_histogram_22  tonal.chords_histogram_23  \n",
       "0                     0.098657                   0.288181  \n",
       "1                     0.000000                   0.155779  \n",
       "2                     0.000000                   0.000000  \n",
       "3                     0.000000                   0.000000  \n",
       "4                     0.165104                   0.096976  \n",
       "..                         ...                        ...  \n",
       "762                   0.156009                   0.863862  \n",
       "763                   0.044460                   0.016906  \n",
       "764                   0.035320                   0.149458  \n",
       "765                   0.112450                   0.019859  \n",
       "766                   0.000761                   0.000000  \n",
       "\n",
       "[767 rows x 742 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = df_essentia_best_valence_features_mean.drop('song_id', axis=1)\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>valence_mean_mapped</th>\n",
       "      <th>arousal_mean_mapped</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.150</td>\n",
       "      <td>-0.200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.425</td>\n",
       "      <td>-0.475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.600</td>\n",
       "      <td>-0.700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.300</td>\n",
       "      <td>0.025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.450</td>\n",
       "      <td>0.400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>762</th>\n",
       "      <td>0.525</td>\n",
       "      <td>0.725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>763</th>\n",
       "      <td>0.125</td>\n",
       "      <td>0.750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>764</th>\n",
       "      <td>0.325</td>\n",
       "      <td>0.425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>765</th>\n",
       "      <td>0.550</td>\n",
       "      <td>0.750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>766</th>\n",
       "      <td>0.150</td>\n",
       "      <td>0.325</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>767 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     valence_mean_mapped  arousal_mean_mapped\n",
       "0                  0.150               -0.200\n",
       "1                 -0.425               -0.475\n",
       "2                 -0.600               -0.700\n",
       "3                 -0.300                0.025\n",
       "4                  0.450                0.400\n",
       "..                   ...                  ...\n",
       "762                0.525                0.725\n",
       "763                0.125                0.750\n",
       "764                0.325                0.425\n",
       "765                0.550                0.750\n",
       "766                0.150                0.325\n",
       "\n",
       "[767 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets = df_annotations.drop('song_id', axis=1)\n",
    "targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform 80-20 train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(features, targets, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create tensors for X_train and X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tensor = torch.tensor(X_train.values, dtype=torch.float64)\n",
    "X_test_tensor = torch.tensor(X_test.values, dtype=torch.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create tensors for Y_train and Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float64)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define neural network parameters and instantitate neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 1\n",
    "hidden_size = 20 \n",
    "output_size = 2  # Output size for valence and arousal\n",
    "learning_rate = 0.001\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 294"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set a random seed to ensure consistent initial weights of the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x117433e70>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the seed\n",
    "seed = 42\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_size, math.ceil((input_size**0.5) * 2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(math.ceil((input_size**0.5) * 2), math.ceil((input_size**0.5) * 2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(math.ceil((input_size**0.5) * 2), math.ceil((input_size**0.5) * 2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(math.ceil((input_size**0.5) * 2), math.ceil((input_size**0.5) * 2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(math.ceil((input_size**0.5) * 2), math.ceil((input_size**0.5) * 2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(math.ceil((input_size**0.5) * 2), math.ceil((input_size**0.5) * 2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(math.ceil((input_size**0.5) * 2), math.ceil((input_size**0.5) * 2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(math.ceil((input_size**0.5) * 2), math.ceil((input_size**0.5) * 2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(math.ceil((input_size**0.5) * 2), math.ceil((input_size**0.5) * 2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(math.ceil((input_size**0.5) * 2), math.ceil((input_size**0.5) * 2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(math.ceil((input_size**0.5) * 2), math.ceil((input_size**0.5) * 2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(math.ceil((input_size**0.5) * 2), 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare input_train_data and target_train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([613, 742])\n"
     ]
    }
   ],
   "source": [
    "input_train_data = X_train_tensor.float()\n",
    "\n",
    "# input_train_data = input_train_data.view(input_train_data.shape[1], -1)\n",
    "print(input_train_data.shape)\n",
    "\n",
    "target_train_labels = y_train_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(num_epochs):\n",
    "  model = NeuralNetwork(input_size=input_train_data.shape[1])\n",
    "  optimiser = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "  \n",
    "  for epoch in range(num_epochs):\n",
    "    optimiser.zero_grad()\n",
    "    \n",
    "    # forward pass\n",
    "    output = model(input_train_data)\n",
    "\n",
    "    # calculate loss\n",
    "    loss = torch.sqrt(criterion(output.float(), target_train_labels.float()))\n",
    "\n",
    "    # backward pass\n",
    "    loss.backward()\n",
    "    # update weights\n",
    "    optimiser.step()\n",
    "\n",
    "    print(f'Epoch {epoch + 1}, Loss: {math.sqrt(loss.item())}')\n",
    "\n",
    "  print(\"Training completed.\")\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Epoch 121, Loss: 0.4816228295571127\n",
      "Epoch 122, Loss: 0.4807618495484584\n",
      "Epoch 123, Loss: 0.48102030846138955\n",
      "Epoch 124, Loss: 0.4810176288364116\n",
      "Epoch 125, Loss: 0.48014338261661127\n",
      "Epoch 126, Loss: 0.47910314809270116\n",
      "Epoch 127, Loss: 0.47880795949683447\n",
      "Epoch 128, Loss: 0.47876177315142754\n",
      "Epoch 129, Loss: 0.47819597116449997\n",
      "Epoch 130, Loss: 0.47743214539542544\n",
      "Epoch 131, Loss: 0.47658476230340185\n",
      "Epoch 132, Loss: 0.4759644680184978\n",
      "Epoch 133, Loss: 0.4755635187712421\n",
      "Epoch 134, Loss: 0.47538264196584307\n",
      "Epoch 135, Loss: 0.4753673450596914\n",
      "Epoch 136, Loss: 0.4753697744168057\n",
      "Epoch 137, Loss: 0.4755973109583155\n",
      "Epoch 138, Loss: 0.4767626826605241\n",
      "Epoch 139, Loss: 0.47683528222287225\n",
      "Epoch 140, Loss: 0.47579445468261566\n",
      "Epoch 141, Loss: 0.47284533919249566\n",
      "Epoch 142, Loss: 0.472809286012082\n",
      "Epoch 143, Loss: 0.47458738111232796\n",
      "Epoch 144, Loss: 0.4733380058918715\n",
      "Epoch 145, Loss: 0.4715229034484008\n",
      "Epoch 146, Loss: 0.47096730966267386\n",
      "Epoch 147, Loss: 0.47161469890467383\n",
      "Epoch 148, Loss: 0.47183531411129226\n",
      "Epoch 149, Loss: 0.470294933688076\n",
      "Epoch 150, Loss: 0.4697114144973765\n",
      "Epoch 151, Loss: 0.47018789053701304\n",
      "Epoch 152, Loss: 0.4700460953124117\n",
      "Epoch 153, Loss: 0.4693645740016508\n",
      "Epoch 154, Loss: 0.4684558104180481\n",
      "Epoch 155, Loss: 0.4680960384979934\n",
      "Epoch 156, Loss: 0.4682454254529793\n",
      "Epoch 157, Loss: 0.4682974379105488\n",
      "Epoch 158, Loss: 0.4681590010361841\n",
      "Epoch 159, Loss: 0.4673225124817593\n",
      "Epoch 160, Loss: 0.4666701428107949\n",
      "Epoch 161, Loss: 0.4663166784414499\n",
      "Epoch 162, Loss: 0.46624854535037\n",
      "Epoch 163, Loss: 0.4664611566834947\n",
      "Epoch 164, Loss: 0.46654532450127006\n",
      "Epoch 165, Loss: 0.46721845607201035\n",
      "Epoch 166, Loss: 0.4668208160006968\n",
      "Epoch 167, Loss: 0.466573749669782\n",
      "Epoch 168, Loss: 0.46506387643568103\n",
      "Epoch 169, Loss: 0.4641220516950567\n",
      "Epoch 170, Loss: 0.4644550172656552\n",
      "Epoch 171, Loss: 0.4654613047116713\n",
      "Epoch 172, Loss: 0.46880176576246846\n",
      "Epoch 173, Loss: 0.4685649824581709\n",
      "Epoch 174, Loss: 0.46606115996454733\n",
      "Epoch 175, Loss: 0.46328263242364126\n",
      "Epoch 176, Loss: 0.4660484187313888\n",
      "Epoch 177, Loss: 0.4667669789465077\n",
      "Epoch 178, Loss: 0.46222975993575777\n",
      "Epoch 179, Loss: 0.46412214801342555\n",
      "Epoch 180, Loss: 0.46867384291927394\n",
      "Epoch 181, Loss: 0.46316269230838986\n",
      "Epoch 182, Loss: 0.4619791093960296\n",
      "Epoch 183, Loss: 0.4650783107301587\n",
      "Epoch 184, Loss: 0.4617210775403838\n",
      "Epoch 185, Loss: 0.4621011464022392\n",
      "Epoch 186, Loss: 0.4641367239628427\n",
      "Epoch 187, Loss: 0.46071131054180564\n",
      "Epoch 188, Loss: 0.46145892957673257\n",
      "Epoch 189, Loss: 0.464304908222076\n",
      "Epoch 190, Loss: 0.4606605441278432\n",
      "Epoch 191, Loss: 0.4601386308130386\n",
      "Epoch 192, Loss: 0.46193430495315263\n",
      "Epoch 193, Loss: 0.45977696912799226\n",
      "Epoch 194, Loss: 0.4597173803227511\n",
      "Epoch 195, Loss: 0.46096589922299847\n",
      "Epoch 196, Loss: 0.4592170612502741\n",
      "Epoch 197, Loss: 0.45864347419013624\n",
      "Epoch 198, Loss: 0.4594650511157848\n",
      "Epoch 199, Loss: 0.45924705942486493\n",
      "Epoch 200, Loss: 0.4584931199827434\n",
      "Epoch 201, Loss: 0.4578552480015059\n",
      "Epoch 202, Loss: 0.458251953125\n",
      "Epoch 203, Loss: 0.4586848316367108\n",
      "Epoch 204, Loss: 0.4577025349799131\n",
      "Epoch 205, Loss: 0.45707273702530193\n",
      "Epoch 206, Loss: 0.4571336158814618\n",
      "Epoch 207, Loss: 0.45746758002181026\n",
      "Epoch 208, Loss: 0.4580334166179955\n",
      "Epoch 209, Loss: 0.457551138813026\n",
      "Epoch 210, Loss: 0.4572461918817771\n",
      "Epoch 211, Loss: 0.45629613394883295\n",
      "Epoch 212, Loss: 0.4557945440718634\n",
      "Epoch 213, Loss: 0.455843155547024\n",
      "Epoch 214, Loss: 0.456219547378948\n",
      "Epoch 215, Loss: 0.4577116995236092\n",
      "Epoch 216, Loss: 0.4589176697652806\n",
      "Epoch 217, Loss: 0.46105034303609993\n",
      "Epoch 218, Loss: 0.4567326442394303\n",
      "Epoch 219, Loss: 0.45493112899589366\n",
      "Epoch 220, Loss: 0.45527199475335134\n",
      "Epoch 221, Loss: 0.45623630281032773\n",
      "Epoch 222, Loss: 0.45802054967026307\n",
      "Epoch 223, Loss: 0.45677224995978094\n",
      "Epoch 224, Loss: 0.45565914342704716\n",
      "Epoch 225, Loss: 0.45401674524485375\n",
      "Epoch 226, Loss: 0.4535057342791329\n",
      "Epoch 227, Loss: 0.45416164205845616\n",
      "Epoch 228, Loss: 0.45450513400796655\n",
      "Epoch 229, Loss: 0.4548238117351997\n",
      "Epoch 230, Loss: 0.4537990916311975\n",
      "Epoch 231, Loss: 0.4532521496977959\n",
      "Epoch 232, Loss: 0.4526084553640119\n",
      "Epoch 233, Loss: 0.45228981460773116\n",
      "Epoch 234, Loss: 0.4520949132230242\n",
      "Epoch 235, Loss: 0.45205797975469736\n",
      "Epoch 236, Loss: 0.45230557901617613\n",
      "Epoch 237, Loss: 0.4528345953247846\n",
      "Epoch 238, Loss: 0.4549386133976144\n",
      "Epoch 239, Loss: 0.4564022887834084\n",
      "Epoch 240, Loss: 0.45805755540321125\n",
      "Epoch 241, Loss: 0.4541498302115434\n",
      "Epoch 242, Loss: 0.45158727109343993\n",
      "Epoch 243, Loss: 0.45105347173746146\n",
      "Epoch 244, Loss: 0.45263971453254503\n",
      "Epoch 245, Loss: 0.4550743757756552\n",
      "Epoch 246, Loss: 0.4549471294161167\n",
      "Epoch 247, Loss: 0.45361945699341477\n",
      "Epoch 248, Loss: 0.4506913801287277\n",
      "Epoch 249, Loss: 0.44996455105922556\n",
      "Epoch 250, Loss: 0.45070657227233163\n",
      "Epoch 251, Loss: 0.45134097806864715\n",
      "Epoch 252, Loss: 0.4518641160838718\n",
      "Epoch 253, Loss: 0.45124870735997746\n",
      "Epoch 254, Loss: 0.4507253179113287\n",
      "Epoch 255, Loss: 0.44965336548788515\n",
      "Epoch 256, Loss: 0.4490457284512466\n",
      "Epoch 257, Loss: 0.44867439459283065\n",
      "Epoch 258, Loss: 0.4485724236213288\n",
      "Epoch 259, Loss: 0.4487178165615502\n",
      "Epoch 260, Loss: 0.44913423821312026\n",
      "Epoch 261, Loss: 0.45068397397968624\n",
      "Epoch 262, Loss: 0.4523160224282965\n",
      "Epoch 263, Loss: 0.4554805695192658\n",
      "Epoch 264, Loss: 0.4521284819909996\n",
      "Epoch 265, Loss: 0.4491460824324\n",
      "Epoch 266, Loss: 0.4475917876267889\n",
      "Epoch 267, Loss: 0.4489836201844808\n",
      "Epoch 268, Loss: 0.4522696841254308\n",
      "Epoch 269, Loss: 0.45230492011773493\n",
      "Epoch 270, Loss: 0.45030259118174343\n",
      "Epoch 271, Loss: 0.44750189058874124\n",
      "Epoch 272, Loss: 0.4466950387656147\n",
      "Epoch 273, Loss: 0.44782488644087964\n",
      "Epoch 274, Loss: 0.4490013591686314\n",
      "Epoch 275, Loss: 0.44932258069431275\n",
      "Epoch 276, Loss: 0.4477355520631983\n",
      "Epoch 277, Loss: 0.4464409066948383\n",
      "Epoch 278, Loss: 0.4458417591594104\n",
      "Epoch 279, Loss: 0.4457414301360274\n",
      "Epoch 280, Loss: 0.4460887501130773\n",
      "Epoch 281, Loss: 0.44690611604251484\n",
      "Epoch 282, Loss: 0.4483696750030082\n",
      "Epoch 283, Loss: 0.4491296928695683\n",
      "Epoch 284, Loss: 0.4498625247292945\n",
      "Epoch 285, Loss: 0.44791530084032105\n",
      "Epoch 286, Loss: 0.4460872803334146\n",
      "Epoch 287, Loss: 0.44485744850497494\n",
      "Epoch 288, Loss: 0.4448821514773518\n",
      "Epoch 289, Loss: 0.4460718473545706\n",
      "Epoch 290, Loss: 0.4475995445619453\n",
      "Epoch 291, Loss: 0.44939647973391844\n",
      "Epoch 292, Loss: 0.44907386765397006\n",
      "Epoch 293, Loss: 0.4480021879332198\n",
      "Epoch 294, Loss: 0.44517745931865826\n",
      "Training completed.\n"
     ]
    }
   ],
   "source": [
    "model = train_model(num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare input_test_data and target_test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([154, 742])\n"
     ]
    }
   ],
   "source": [
    "input_test_data = X_test_tensor.float()\n",
    "\n",
    "# input_test_data = input_test_data.view(input_test_data.shape[1], -1)\n",
    "print(input_test_data.shape)\n",
    "\n",
    "target_test_labels = y_test_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(trained_model):\n",
    "  with torch.no_grad():\n",
    "    test_pred = trained_model(input_test_data)\n",
    "    test_loss = criterion(test_pred.float(), target_test_labels)\n",
    "\n",
    "    # Separate the output into valence and arousal\n",
    "    valence_pred = test_pred[:, 0]\n",
    "    arousal_pred = test_pred[:, 1]\n",
    "        \n",
    "    valence_target = target_test_labels[:, 0]\n",
    "    arousal_target = target_test_labels[:, 1]\n",
    "\n",
    "     # Calculate RMSE for valence and arousal separately\n",
    "    valence_rmse = math.sqrt(mean_squared_error(valence_pred, valence_target))\n",
    "    arousal_rmse = math.sqrt(mean_squared_error(arousal_pred, arousal_target))\n",
    "\n",
    "  rmse = math.sqrt(test_loss.item())\n",
    "  print(f'Test RMSE: {rmse}')\n",
    "\n",
    "  print(f'Valence RMSE: {valence_rmse}')\n",
    "  print(f'Arousal RMSE: {arousal_rmse}')\n",
    "\n",
    "  metric = R2Score(multioutput=\"raw_values\")\n",
    "  metric.update(test_pred, target_test_labels)\n",
    "  adjusted_r2_score = metric.compute()\n",
    "  print(f'Test R^2 score: {adjusted_r2_score}')\n",
    "\n",
    "  # metric = R2Score(multioutput=\"raw_values\", num_regressors=input_test_data.shape[1])\n",
    "  # metric.update(test_pred, target_test_labels)\n",
    "  # adjusted_r2_score = metric.compute()\n",
    "  # print(f'Test Adjusted R^2 score: {adjusted_r2_score}')\n",
    "\n",
    "  metric = R2Score()\n",
    "  metric.update(test_pred, target_test_labels)\n",
    "  r2_score = metric.compute()\n",
    "  print(f'Test R^2 score (overall): {r2_score}')\n",
    "  return test_pred, rmse, adjusted_r2_score, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE: 0.2332381147157868\n",
      "Valence RMSE: 0.24267499277215512\n",
      "Arousal RMSE: 0.2234029637121752\n",
      "Test R^2 score: tensor([0.3707, 0.6319], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.5013074040747145\n"
     ]
    }
   ],
   "source": [
    "test_pred, rmse, adjusted_r2_score, r2_score = test_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), '../../models/pmemo_feedforward_nn_essentia_best_valence_mean_normalised.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "True values (test set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5750,  0.3500],\n",
       "        [ 0.1250, -0.0250],\n",
       "        [ 0.2000,  0.4750],\n",
       "        [ 0.3500,  0.3500],\n",
       "        [ 0.3000,  0.4500],\n",
       "        [ 0.3500,  0.0250],\n",
       "        [ 0.3250, -0.0250],\n",
       "        [ 0.3750,  0.3500],\n",
       "        [ 0.1500,  0.1000],\n",
       "        [ 0.2750,  0.6500],\n",
       "        [ 0.5000,  0.5250],\n",
       "        [ 0.0500, -0.3500],\n",
       "        [ 0.0500,  0.2250],\n",
       "        [-0.3250, -0.4500],\n",
       "        [-0.1000,  0.4500],\n",
       "        [ 0.1250, -0.4000],\n",
       "        [ 0.3750,  0.5500],\n",
       "        [ 0.2000, -0.2250],\n",
       "        [-0.4500, -0.3000],\n",
       "        [ 0.0500,  0.0750],\n",
       "        [ 0.2750,  0.4250],\n",
       "        [-0.0250,  0.4000],\n",
       "        [ 0.6500,  0.6750],\n",
       "        [-0.1750, -0.3250],\n",
       "        [-0.6500,  0.6500],\n",
       "        [ 0.0250,  0.3000],\n",
       "        [-0.0500,  0.6750],\n",
       "        [-0.7250, -0.4500],\n",
       "        [ 0.0000, -0.2750],\n",
       "        [ 0.2750,  0.4500],\n",
       "        [ 0.0000, -0.2000],\n",
       "        [ 0.3250,  0.2250],\n",
       "        [-0.3750, -0.1250],\n",
       "        [-0.1000,  0.2250],\n",
       "        [ 0.4000,  0.2250],\n",
       "        [ 0.3500,  0.4000],\n",
       "        [ 0.4500,  0.7000],\n",
       "        [ 0.5250,  0.4500],\n",
       "        [ 0.5750,  0.3250],\n",
       "        [ 0.6000,  0.5250],\n",
       "        [ 0.5750,  0.7000],\n",
       "        [ 0.3000,  0.5000],\n",
       "        [ 0.6750,  0.7750],\n",
       "        [ 0.3500,  0.3500],\n",
       "        [ 0.2000,  0.5250],\n",
       "        [ 0.1818,  0.7500],\n",
       "        [ 0.4250,  0.5750],\n",
       "        [ 0.3000,  0.2250],\n",
       "        [-0.1250, -0.1250],\n",
       "        [ 0.1000,  0.1500],\n",
       "        [ 0.4500,  0.2250],\n",
       "        [-0.1500, -0.3750],\n",
       "        [ 0.1750,  0.1000],\n",
       "        [-0.5500, -0.4750],\n",
       "        [ 0.1500,  0.1500],\n",
       "        [ 0.7000,  0.6250],\n",
       "        [ 0.7000,  0.5250],\n",
       "        [ 0.3750,  0.5250],\n",
       "        [ 0.5750,  0.5750],\n",
       "        [ 0.4000,  0.6000],\n",
       "        [ 0.4500,  0.3750],\n",
       "        [ 0.1250,  0.7500],\n",
       "        [ 0.0500,  0.3500],\n",
       "        [ 0.5500,  0.5750],\n",
       "        [-0.0250, -0.4250],\n",
       "        [-0.0750, -0.2750],\n",
       "        [-0.2250, -0.6000],\n",
       "        [ 0.6500,  0.4750],\n",
       "        [ 0.3000,  0.1250],\n",
       "        [ 0.3000,  0.2250],\n",
       "        [ 0.1750,  0.6000],\n",
       "        [-0.3250,  0.2000],\n",
       "        [ 0.3250,  0.1500],\n",
       "        [ 0.4000,  0.5250],\n",
       "        [ 0.0500,  0.1750],\n",
       "        [ 0.5750,  0.7500],\n",
       "        [-0.2000, -0.1500],\n",
       "        [ 0.4750,  0.3750],\n",
       "        [ 0.2250,  0.4250],\n",
       "        [ 0.1500,  0.1250],\n",
       "        [ 0.3750,  0.2500],\n",
       "        [ 0.1000, -0.2750],\n",
       "        [ 0.5000,  0.6750],\n",
       "        [ 0.7500,  0.7750],\n",
       "        [-0.1500,  0.1000],\n",
       "        [-0.2000, -0.0250],\n",
       "        [ 0.0750,  0.8750],\n",
       "        [ 0.2750, -0.6500],\n",
       "        [ 0.2500,  0.8500],\n",
       "        [-0.3000, -0.5000],\n",
       "        [ 0.2000,  0.3500],\n",
       "        [ 0.0500,  0.4000],\n",
       "        [ 0.3000,  0.4750],\n",
       "        [ 0.3000, -0.1750],\n",
       "        [-0.2500,  0.0250],\n",
       "        [ 0.2000,  0.3000],\n",
       "        [ 0.4750,  0.5250],\n",
       "        [ 0.0250,  0.4250],\n",
       "        [ 0.1000,  0.4000],\n",
       "        [ 0.1500,  0.3000],\n",
       "        [ 0.0909,  0.0909],\n",
       "        [ 0.1750,  0.1250],\n",
       "        [ 0.1750,  0.2500],\n",
       "        [ 0.0000,  0.4750],\n",
       "        [ 0.0750, -0.2750],\n",
       "        [-0.1750, -0.0250],\n",
       "        [ 0.3000,  0.2750],\n",
       "        [-0.0500,  0.0500],\n",
       "        [ 0.0750,  0.8500],\n",
       "        [ 0.5500,  0.7250],\n",
       "        [ 0.4750,  0.3250],\n",
       "        [ 0.4500,  0.7250],\n",
       "        [-0.1818, -0.1591],\n",
       "        [ 0.5909,  0.8182],\n",
       "        [ 0.2250,  0.6750],\n",
       "        [ 0.5000,  0.2750],\n",
       "        [ 0.5750,  0.6500],\n",
       "        [ 0.3000,  0.3000],\n",
       "        [ 0.0750,  0.0000],\n",
       "        [-0.1500, -0.1250],\n",
       "        [-0.1000,  0.0750],\n",
       "        [-0.0750, -0.2000],\n",
       "        [ 0.0750,  0.2250],\n",
       "        [-0.0750,  0.6000],\n",
       "        [ 0.4000,  0.4000],\n",
       "        [ 0.5250,  0.7250],\n",
       "        [-0.2500, -0.4250],\n",
       "        [ 0.5750,  0.4500],\n",
       "        [ 0.1250,  0.0500],\n",
       "        [ 0.0750,  0.3000],\n",
       "        [-0.6000, -0.7000],\n",
       "        [-0.0250, -0.0750],\n",
       "        [ 0.5000,  0.4750],\n",
       "        [-0.1000, -0.0500],\n",
       "        [-0.0500, -0.3250],\n",
       "        [ 0.4750,  0.5250],\n",
       "        [-0.2000, -0.0250],\n",
       "        [ 0.5000,  0.6750],\n",
       "        [ 0.0500, -0.0750],\n",
       "        [ 0.4500,  0.3750],\n",
       "        [ 0.6500,  0.7500],\n",
       "        [-0.1500, -0.3000],\n",
       "        [ 0.7750,  0.6500],\n",
       "        [ 0.5500,  0.7000],\n",
       "        [ 0.2500,  0.4000],\n",
       "        [ 0.3500,  0.4750],\n",
       "        [ 0.7250,  0.9000],\n",
       "        [ 0.1500,  0.3000],\n",
       "        [-0.2273,  0.0227],\n",
       "        [ 0.3000, -0.1750],\n",
       "        [ 0.2250,  0.5500],\n",
       "        [ 0.4750,  0.4000],\n",
       "        [ 0.3500,  0.4250],\n",
       "        [ 0.0000, -0.1250]], dtype=torch.float64)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_test_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicted values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.5577e-01,  2.1835e-01],\n",
       "        [ 1.3380e-01,  1.6136e-01],\n",
       "        [ 4.5622e-01,  5.7208e-01],\n",
       "        [ 5.8336e-02,  2.1370e-02],\n",
       "        [ 4.4037e-01,  5.3074e-01],\n",
       "        [-2.0288e-01, -1.7777e-01],\n",
       "        [-7.8618e-02, -4.4369e-02],\n",
       "        [ 4.4832e-01,  5.5150e-01],\n",
       "        [-3.3905e-01, -3.5932e-01],\n",
       "        [ 4.6401e-01,  5.9250e-01],\n",
       "        [ 4.5448e-01,  5.6756e-01],\n",
       "        [-2.8570e-01, -5.0269e-01],\n",
       "        [-2.9286e-01, -1.9611e-01],\n",
       "        [-2.7245e-01, -2.3350e-01],\n",
       "        [ 2.5928e-01,  3.3261e-01],\n",
       "        [ 1.1035e-01,  9.4527e-02],\n",
       "        [ 2.3434e-01,  2.6040e-01],\n",
       "        [ 2.9592e-01,  3.3159e-01],\n",
       "        [-2.9470e-01, -2.8463e-01],\n",
       "        [ 3.0237e-01,  3.3258e-01],\n",
       "        [ 1.4560e-01,  1.3529e-01],\n",
       "        [-7.4763e-02, -3.2510e-02],\n",
       "        [ 2.7019e-01,  3.1831e-01],\n",
       "        [-3.2383e-01, -5.1126e-01],\n",
       "        [-1.5032e-01,  8.7258e-03],\n",
       "        [ 9.5408e-02,  1.6199e-01],\n",
       "        [ 2.9313e-01,  3.3277e-01],\n",
       "        [-3.1539e-01, -5.1626e-01],\n",
       "        [-1.4201e-01, -8.4338e-02],\n",
       "        [ 4.3107e-01,  5.0759e-01],\n",
       "        [-2.9045e-01, -2.7493e-01],\n",
       "        [ 3.2015e-01,  4.0066e-01],\n",
       "        [ 1.2035e-03, -7.7682e-03],\n",
       "        [-1.6526e-01, -9.4844e-02],\n",
       "        [ 3.8949e-01,  4.3176e-01],\n",
       "        [ 3.0290e-01,  3.4243e-01],\n",
       "        [ 4.3515e-01,  5.1713e-01],\n",
       "        [ 4.6076e-01,  5.8390e-01],\n",
       "        [ 2.1090e-01,  2.2524e-01],\n",
       "        [ 2.3447e-01,  2.7591e-01],\n",
       "        [ 4.7039e-01,  6.0896e-01],\n",
       "        [ 4.6608e-01,  5.9771e-01],\n",
       "        [ 4.4451e-01,  5.4185e-01],\n",
       "        [ 1.2373e-01,  1.3461e-01],\n",
       "        [ 4.1844e-01,  4.8734e-01],\n",
       "        [ 4.3544e-01,  5.1798e-01],\n",
       "        [ 2.2503e-01,  2.4490e-01],\n",
       "        [ 1.5459e-01,  1.6295e-01],\n",
       "        [-2.9513e-01, -3.8684e-01],\n",
       "        [ 1.7134e-01,  1.7142e-01],\n",
       "        [ 1.6609e-01,  1.8424e-01],\n",
       "        [-2.9452e-01, -2.7660e-01],\n",
       "        [ 1.5032e-01,  2.0786e-01],\n",
       "        [-7.0933e-02, -3.8195e-02],\n",
       "        [ 1.0435e-01,  7.6058e-02],\n",
       "        [ 4.4650e-01,  5.4674e-01],\n",
       "        [ 4.1391e-01,  4.7364e-01],\n",
       "        [ 4.4826e-01,  5.5880e-01],\n",
       "        [ 5.0406e-01,  6.9687e-01],\n",
       "        [ 5.0522e-01,  6.9993e-01],\n",
       "        [ 3.0183e-01,  3.5436e-01],\n",
       "        [ 4.7844e-01,  6.3010e-01],\n",
       "        [ 2.4127e-01,  2.7254e-01],\n",
       "        [ 4.4674e-01,  5.4749e-01],\n",
       "        [-7.8702e-02, -4.8844e-02],\n",
       "        [-4.4352e-02, -1.0078e-03],\n",
       "        [-7.3566e-02, -3.6756e-02],\n",
       "        [ 2.9196e-01,  3.1284e-01],\n",
       "        [ 2.8746e-01,  3.0536e-01],\n",
       "        [ 8.3574e-02,  9.1011e-02],\n",
       "        [ 4.6353e-01,  5.9114e-01],\n",
       "        [ 2.8438e-02,  4.0683e-02],\n",
       "        [ 3.8135e-01,  4.1631e-01],\n",
       "        [ 4.7561e-01,  6.2262e-01],\n",
       "        [ 4.5441e-03,  5.1461e-02],\n",
       "        [ 4.7140e-01,  6.1175e-01],\n",
       "        [-1.1398e-02,  4.5141e-03],\n",
       "        [ 4.6445e-01,  5.9353e-01],\n",
       "        [ 1.5970e-01,  2.0194e-01],\n",
       "        [-7.1381e-02, -4.4566e-02],\n",
       "        [ 1.0978e-01,  1.4864e-01],\n",
       "        [-2.3147e-01, -2.0692e-01],\n",
       "        [ 1.1762e-01,  9.1783e-02],\n",
       "        [ 4.5214e-01,  5.6140e-01],\n",
       "        [-7.0707e-02, -5.5657e-02],\n",
       "        [ 1.2864e-01,  1.3352e-01],\n",
       "        [ 5.0003e-01,  6.8651e-01],\n",
       "        [-3.1445e-01, -2.8558e-01],\n",
       "        [ 4.7002e-01,  6.0799e-01],\n",
       "        [-3.0118e-01, -4.6643e-01],\n",
       "        [ 2.5291e-01,  2.6900e-01],\n",
       "        [ 2.5727e-01,  2.8422e-01],\n",
       "        [ 4.5222e-01,  5.6355e-01],\n",
       "        [-7.1572e-02, -4.2890e-02],\n",
       "        [-1.1390e-01, -3.0625e-02],\n",
       "        [ 3.7665e-02,  1.0351e-01],\n",
       "        [ 4.9231e-01,  6.6628e-01],\n",
       "        [ 3.7016e-01,  4.4659e-01],\n",
       "        [ 3.1014e-01,  3.4401e-01],\n",
       "        [ 4.4800e-01,  5.5065e-01],\n",
       "        [ 2.6249e-01,  3.0172e-01],\n",
       "        [ 1.7127e-01,  2.0592e-01],\n",
       "        [ 1.5126e-01,  1.6003e-01],\n",
       "        [ 4.4078e-01,  5.3183e-01],\n",
       "        [-1.4753e-01, -1.4924e-01],\n",
       "        [-9.6690e-02,  4.8479e-02],\n",
       "        [ 1.6980e-01,  1.9467e-01],\n",
       "        [-4.9921e-02,  3.3560e-02],\n",
       "        [ 4.3037e-01,  5.4844e-01],\n",
       "        [ 4.7379e-01,  6.1784e-01],\n",
       "        [ 1.6221e-01,  1.6402e-01],\n",
       "        [ 4.8805e-01,  6.5494e-01],\n",
       "        [-3.2751e-01, -3.3156e-01],\n",
       "        [ 5.4176e-01,  7.9506e-01],\n",
       "        [ 4.6145e-01,  5.8577e-01],\n",
       "        [ 4.4116e-01,  5.3276e-01],\n",
       "        [ 1.2683e-01,  1.3780e-01],\n",
       "        [ 1.6548e-01,  1.7998e-01],\n",
       "        [-5.1534e-02,  1.4216e-02],\n",
       "        [ 2.2278e-01,  2.8125e-01],\n",
       "        [-3.5700e-01, -3.6209e-01],\n",
       "        [-2.8386e-01, -4.0485e-01],\n",
       "        [ 4.5412e-01,  5.6658e-01],\n",
       "        [ 1.8106e-01,  2.6221e-01],\n",
       "        [ 1.7462e-01,  2.0439e-01],\n",
       "        [ 3.9092e-01,  4.5737e-01],\n",
       "        [-2.9431e-01, -2.5589e-01],\n",
       "        [ 4.2367e-01,  4.9088e-01],\n",
       "        [ 1.7382e-01,  1.9503e-01],\n",
       "        [ 2.3018e-01,  2.6068e-01],\n",
       "        [-3.1050e-01, -3.4001e-01],\n",
       "        [ 1.5514e-01,  1.6276e-01],\n",
       "        [ 4.9545e-01,  6.7417e-01],\n",
       "        [ 2.5130e-01,  2.9874e-01],\n",
       "        [ 1.6432e-01,  1.6583e-01],\n",
       "        [ 2.0263e-01,  2.4244e-01],\n",
       "        [-6.3915e-02, -6.4987e-03],\n",
       "        [ 4.8983e-01,  6.5947e-01],\n",
       "        [-3.1964e-01, -2.5577e-01],\n",
       "        [ 4.1189e-01,  4.6119e-01],\n",
       "        [ 4.6177e-01,  5.8657e-01],\n",
       "        [-2.3793e-01, -2.1647e-01],\n",
       "        [ 4.6672e-01,  5.9943e-01],\n",
       "        [ 4.2331e-01,  4.8732e-01],\n",
       "        [ 1.3647e-01,  1.3540e-01],\n",
       "        [ 4.7419e-01,  6.1898e-01],\n",
       "        [ 4.6945e-01,  6.0664e-01],\n",
       "        [-1.2063e-01,  1.5813e-04],\n",
       "        [-9.0119e-02, -2.7658e-02],\n",
       "        [-2.8712e-01, -2.6167e-01],\n",
       "        [ 3.0588e-01,  3.3751e-01],\n",
       "        [ 3.9708e-01,  4.4000e-01],\n",
       "        [ 2.8231e-01,  3.2734e-01],\n",
       "        [ 1.9566e-01,  2.8982e-01]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3710, 0.6235], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "pred_valence = test_pred[:, 0]\n",
    "pred_arousal = test_pred[1]\n",
    "real_valence = target_test_labels[0]\n",
    "real_arousal = target_test_labels[1]\n",
    "\n",
    "\n",
    "metric = R2Score(multioutput='raw_values')\n",
    "metric.update(test_pred, target_test_labels)\n",
    "print(metric.compute())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse relationship between epochs and r^2 score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create lists to store the epochs and R^2 scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs_list = [i for i in range(1, 301)]\n",
    "adjusted_r2_scores_valence_list = []\n",
    "adjusted_r2_scores_arousal_list = []\n",
    "r2_scores_list = []\n",
    "rmse_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conduct training and testing for each num_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of epochs: 1\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.4071492850037976\n",
      "Valence RMSE: 0.3931229731524217\n",
      "Arousal RMSE: 0.4207082225699988\n",
      "Test R^2 score: tensor([-0.6515, -0.3053], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.47840231826086455\n",
      "Num of epochs: 2\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.40142422932138666\n",
      "Valence RMSE: 0.38622772545872075\n",
      "Arousal RMSE: 0.41606606189344675\n",
      "Test R^2 score: tensor([-0.5940, -0.2767], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.43536629794628656\n",
      "Num of epochs: 3\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.39613138557940497\n",
      "Valence RMSE: 0.37977878655769176\n",
      "Arousal RMSE: 0.4118351885923368\n",
      "Test R^2 score: tensor([-0.5413, -0.2509], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.3960560232841497\n",
      "Num of epochs: 4\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.39113237543313334\n",
      "Valence RMSE: 0.37360390945561145\n",
      "Arousal RMSE: 0.40790830962780644\n",
      "Test R^2 score: tensor([-0.4915, -0.2271], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.35933008832248525\n",
      "Num of epochs: 5\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.38638289202779985\n",
      "Valence RMSE: 0.36759133178911774\n",
      "Arousal RMSE: 0.40430198032786774\n",
      "Test R^2 score: tensor([-0.4439, -0.2055], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.32471811493403924\n",
      "Num of epochs: 6\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.38196646359202296\n",
      "Valence RMSE: 0.36200869008156417\n",
      "Arousal RMSE: 0.4009319978792194\n",
      "Test R^2 score: tensor([-0.4004, -0.1855], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.29294918782846224\n",
      "Num of epochs: 7\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.37766102729599677\n",
      "Valence RMSE: 0.3566836872647025\n",
      "Arousal RMSE: 0.3975329550059857\n",
      "Test R^2 score: tensor([-0.3595, -0.1655], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.2624934912661335\n",
      "Num of epochs: 8\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3734580887891889\n",
      "Valence RMSE: 0.35156730272331177\n",
      "Arousal RMSE: 0.3941349005353405\n",
      "Test R^2 score: tensor([-0.3208, -0.1456], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.23321239462646326\n",
      "Num of epochs: 9\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3693388888802903\n",
      "Valence RMSE: 0.346548825432228\n",
      "Arousal RMSE: 0.3908021766446524\n",
      "Test R^2 score: tensor([-0.2833, -0.1264], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.2048469826431193\n",
      "Num of epochs: 10\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3652656848447836\n",
      "Valence RMSE: 0.3415636278512363\n",
      "Arousal RMSE: 0.38752074677281434\n",
      "Test R^2 score: tensor([-0.2467, -0.1075], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.17710066114871947\n",
      "Num of epochs: 11\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.36137187090825257\n",
      "Valence RMSE: 0.33678826352006463\n",
      "Arousal RMSE: 0.3843864249978152\n",
      "Test R^2 score: tensor([-0.2121, -0.0897], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.15087120446264846\n",
      "Num of epochs: 12\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.357670757837753\n",
      "Valence RMSE: 0.3322728982899824\n",
      "Arousal RMSE: 0.3813809946581503\n",
      "Test R^2 score: tensor([-0.1798, -0.0727], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.12624315787454177\n",
      "Num of epochs: 13\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3540201693480002\n",
      "Valence RMSE: 0.32794478840790153\n",
      "Arousal RMSE: 0.3783024932067857\n",
      "Test R^2 score: tensor([-0.1493, -0.0555], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.10235174403144343\n",
      "Num of epochs: 14\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.35052032869318683\n",
      "Valence RMSE: 0.3237022704541689\n",
      "Arousal RMSE: 0.37542754528294214\n",
      "Test R^2 score: tensor([-0.1197, -0.0395], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.07958986754607922\n",
      "Num of epochs: 15\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.34715523637082574\n",
      "Valence RMSE: 0.3193409605230511\n",
      "Arousal RMSE: 0.37290061304801164\n",
      "Test R^2 score: tensor([-0.0897, -0.0255], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.05763247519175918\n",
      "Num of epochs: 16\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3439100236069089\n",
      "Valence RMSE: 0.31489304090147224\n",
      "Arousal RMSE: 0.3706623550705318\n",
      "Test R^2 score: tensor([-0.0596, -0.0132], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.03642277491863577\n",
      "Num of epochs: 17\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.341044113705886\n",
      "Valence RMSE: 0.3106327931983295\n",
      "Arousal RMSE: 0.3689572370569122\n",
      "Test R^2 score: tensor([-0.0311, -0.0039], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.017533854782732505\n",
      "Num of epochs: 18\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3392895678110953\n",
      "Valence RMSE: 0.30751254219619784\n",
      "Arousal RMSE: 0.3683352522402809\n",
      "Test R^2 score: tensor([-0.0105, -0.0006], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.005537449011871698\n",
      "Num of epochs: 19\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3397273832499409\n",
      "Valence RMSE: 0.3072586608958747\n",
      "Arousal RMSE: 0.36935281935322795\n",
      "Test R^2 score: tensor([-0.0088, -0.0061], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.007471517353771273\n",
      "Num of epochs: 20\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3417422714585898\n",
      "Valence RMSE: 0.30979920368173125\n",
      "Arousal RMSE: 0.3709447581534456\n",
      "Test R^2 score: tensor([-0.0256, -0.0148], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.0201932388807895\n",
      "Num of epochs: 21\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.34253714516093875\n",
      "Valence RMSE: 0.31125202564661114\n",
      "Arousal RMSE: 0.37119478466284156\n",
      "Test R^2 score: tensor([-0.0352, -0.0162], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.025698320339855862\n",
      "Num of epochs: 22\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.34200477353401\n",
      "Valence RMSE: 0.3109258627787907\n",
      "Arousal RMSE: 0.37048567866432236\n",
      "Test R^2 score: tensor([-0.0331, -0.0123], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.02267470478096789\n",
      "Num of epochs: 23\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3409142313775622\n",
      "Valence RMSE: 0.30967593800975046\n",
      "Arousal RMSE: 0.3695210951072878\n",
      "Test R^2 score: tensor([-0.0248, -0.0070], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.01589800388632623\n",
      "Num of epochs: 24\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.33988665726388073\n",
      "Valence RMSE: 0.30832136370323077\n",
      "Arousal RMSE: 0.3687598354704719\n",
      "Test R^2 score: tensor([-0.0158, -0.0029], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.009352815843351792\n",
      "Num of epochs: 25\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.339195012079049\n",
      "Valence RMSE: 0.307321560246928\n",
      "Arousal RMSE: 0.3683204732919502\n",
      "Test R^2 score: tensor([-0.0093, -0.0005], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.004869918160005815\n",
      "Num of epochs: 26\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.338805010634296\n",
      "Valence RMSE: 0.3066616625831401\n",
      "Arousal RMSE: 0.36815254333435643\n",
      "Test R^2 score: tensor([-0.0049,  0.0004], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.002249069042353391\n",
      "Num of epochs: 27\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.33861288007251245\n",
      "Valence RMSE: 0.30623495144608076\n",
      "Arousal RMSE: 0.36815420629787726\n",
      "Test R^2 score: tensor([-0.0021,  0.0004], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.0008562340130420232\n",
      "Num of epochs: 28\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3385922838074074\n",
      "Valence RMSE: 0.3060060669078381\n",
      "Arousal RMSE: 0.36830660640752505\n",
      "Test R^2 score: tensor([-0.0006, -0.0004], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.0005213785400220994\n",
      "Num of epochs: 29\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3386591969055975\n",
      "Valence RMSE: 0.30588683690943613\n",
      "Arousal RMSE: 0.3685286234514607\n",
      "Test R^2 score: tensor([ 0.0001, -0.0016], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.0007348103774008075\n",
      "Num of epochs: 30\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3387681741500024\n",
      "Valence RMSE: 0.30583975871462576\n",
      "Arousal RMSE: 0.36876794006967606\n",
      "Test R^2 score: tensor([ 0.0005, -0.0029], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.0012315836674099967\n",
      "Num of epochs: 31\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3388525504069834\n",
      "Valence RMSE: 0.30581289200988093\n",
      "Arousal RMSE: 0.3689452221064631\n",
      "Test R^2 score: tensor([ 0.0006, -0.0039], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.0016260425864931816\n",
      "Num of epochs: 32\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3389123989939645\n",
      "Valence RMSE: 0.30580094071210967\n",
      "Arousal RMSE: 0.3690650525900241\n",
      "Test R^2 score: tensor([ 0.0007, -0.0045], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.0019130940094405635\n",
      "Num of epochs: 33\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3389404822430017\n",
      "Valence RMSE: 0.3058015064700358\n",
      "Arousal RMSE: 0.36911616009989834\n",
      "Test R^2 score: tensor([ 0.0007, -0.0048], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.0020540588584327946\n",
      "Num of epochs: 34\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3389309994291558\n",
      "Valence RMSE: 0.3058020862584147\n",
      "Arousal RMSE: 0.3690982644067754\n",
      "Test R^2 score: tensor([ 0.0007, -0.0047], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.002007238714824222\n",
      "Num of epochs: 35\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3388780979933187\n",
      "Valence RMSE: 0.30579200682522517\n",
      "Arousal RMSE: 0.3690094567364637\n",
      "Test R^2 score: tensor([ 0.0008, -0.0042], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.0017325888585363325\n",
      "Num of epochs: 36\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3388223656457082\n",
      "Valence RMSE: 0.3057861189586994\n",
      "Arousal RMSE: 0.36891196832806084\n",
      "Test R^2 score: tensor([ 0.0008, -0.0037], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.0014480765604997226\n",
      "Num of epochs: 37\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.33875834118549025\n",
      "Valence RMSE: 0.30578368862340993\n",
      "Arousal RMSE: 0.3687963709384238\n",
      "Test R^2 score: tensor([ 0.0008, -0.0031], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.0011256774081636611\n",
      "Num of epochs: 38\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.33870872077203856\n",
      "Valence RMSE: 0.3058016950929507\n",
      "Arousal RMSE: 0.3686902742578648\n",
      "Test R^2 score: tensor([ 0.0007, -0.0025], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.0008959906056636435\n",
      "Num of epochs: 39\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.338680036194202\n",
      "Valence RMSE: 0.30584336763685427\n",
      "Arousal RMSE: 0.3686029955189728\n",
      "Test R^2 score: tensor([ 0.0004, -0.0020], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.0007948875428545055\n",
      "Num of epochs: 40\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3386406986428302\n",
      "Valence RMSE: 0.3058829693431731\n",
      "Arousal RMSE: 0.3684978352994922\n",
      "Test R^2 score: tensor([ 0.0002, -0.0015], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.0006384934373230844\n",
      "Num of epochs: 41\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3386039578651618\n",
      "Valence RMSE: 0.3059261091625728\n",
      "Arousal RMSE: 0.36839448461744667\n",
      "Test R^2 score: tensor([-0.0001, -0.0009], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.0004986803995247202\n",
      "Num of epochs: 42\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.33857974600720103\n",
      "Valence RMSE: 0.30597039611016297\n",
      "Arousal RMSE: 0.36831318944180036\n",
      "Test R^2 score: tensor([-0.0004, -0.0004], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.00042262411614568673\n",
      "Num of epochs: 43\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.33856189490960004\n",
      "Valence RMSE: 0.3060124514643458\n",
      "Arousal RMSE: 0.3682454248437304\n",
      "Test R^2 score: tensor([-6.7314e-04, -7.9030e-05], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.0003760857021490871\n",
      "Num of epochs: 44\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3385481545747182\n",
      "Valence RMSE: 0.3060478290567223\n",
      "Arousal RMSE: 0.36819075526357514\n",
      "Test R^2 score: tensor([-0.0009,  0.0002], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.0003433182092781317\n",
      "Num of epochs: 45\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.33853584382435825\n",
      "Valence RMSE: 0.3060721465771\n",
      "Arousal RMSE: 0.3681478998954452\n",
      "Test R^2 score: tensor([-0.0011,  0.0005], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.0003064875036563053\n",
      "Num of epochs: 46\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.33852124007997403\n",
      "Valence RMSE: 0.30608070674275417\n",
      "Arousal RMSE: 0.36811392384752706\n",
      "Test R^2 score: tensor([-0.0011,  0.0006], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.0002422421625673521\n",
      "Num of epochs: 47\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3385011153158689\n",
      "Valence RMSE: 0.3060699950119593\n",
      "Arousal RMSE: 0.36808581647973554\n",
      "Test R^2 score: tensor([-0.0010,  0.0008], dtype=torch.float64)\n",
      "Test R^2 score (overall): -0.00013090349543726232\n",
      "Num of epochs: 48\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3384732426754973\n",
      "Valence RMSE: 0.3060381504203423\n",
      "Arousal RMSE: 0.3680610309470809\n",
      "Test R^2 score: tensor([-0.0008,  0.0009], dtype=torch.float64)\n",
      "Test R^2 score (overall): 4.0524757023707725e-05\n",
      "Num of epochs: 49\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.33843699356148294\n",
      "Valence RMSE: 0.3059862375188993\n",
      "Arousal RMSE: 0.3680375248135687\n",
      "Test R^2 score: tensor([-0.0005,  0.0010], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.00027408574275500364\n",
      "Num of epochs: 50\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.33839156337211757\n",
      "Valence RMSE: 0.30591461270732734\n",
      "Arousal RMSE: 0.3680135188481229\n",
      "Test R^2 score: tensor([-3.3370e-05,  1.1802e-03], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.0005734106561381402\n",
      "Num of epochs: 51\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.33833928427761195\n",
      "Valence RMSE: 0.30582781283625887\n",
      "Arousal RMSE: 0.36798952630036047\n",
      "Test R^2 score: tensor([0.0005, 0.0013], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.0009222344348765454\n",
      "Num of epochs: 52\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3382851777199399\n",
      "Valence RMSE: 0.30573532868751724\n",
      "Arousal RMSE: 0.36796688943756667\n",
      "Test R^2 score: tensor([0.0011, 0.0014], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.001285865645604345\n",
      "Num of epochs: 53\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3382296664223657\n",
      "Valence RMSE: 0.3056397608852111\n",
      "Arousal RMSE: 0.36794422275991845\n",
      "Test R^2 score: tensor([0.0018, 0.0016], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.0016595540238123485\n",
      "Num of epochs: 54\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.33817354171513625\n",
      "Valence RMSE: 0.30554338431410666\n",
      "Arousal RMSE: 0.36792109063520684\n",
      "Test R^2 score: tensor([0.0024, 0.0017], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.0020370446183636814\n",
      "Num of epochs: 55\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.338113864026536\n",
      "Valence RMSE: 0.3054430369564717\n",
      "Arousal RMSE: 0.36789471492360043\n",
      "Test R^2 score: tensor([0.0030, 0.0018], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.00243619313270127\n",
      "Num of epochs: 56\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3380484357974425\n",
      "Valence RMSE: 0.30533635715508\n",
      "Arousal RMSE: 0.36786301647414976\n",
      "Test R^2 score: tensor([0.0037, 0.0020], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.0028703312824367444\n",
      "Num of epochs: 57\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3379732624751027\n",
      "Valence RMSE: 0.3052190484247762\n",
      "Arousal RMSE: 0.36782221897922834\n",
      "Test R^2 score: tensor([0.0045, 0.0022], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.003363690973875011\n",
      "Num of epochs: 58\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3378854278080528\n",
      "Valence RMSE: 0.30508818137651994\n",
      "Arousal RMSE: 0.367769392737391\n",
      "Test R^2 score: tensor([0.0054, 0.0025], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.003933720570518273\n",
      "Num of epochs: 59\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3377808009806137\n",
      "Valence RMSE: 0.3049397729378297\n",
      "Arousal RMSE: 0.3677002500717599\n",
      "Test R^2 score: tensor([0.0063, 0.0029], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.004604955648398523\n",
      "Num of epochs: 60\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3376542079470813\n",
      "Valence RMSE: 0.30476784301549387\n",
      "Arousal RMSE: 0.3676102421756295\n",
      "Test R^2 score: tensor([0.0075, 0.0034], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.005409095988864743\n",
      "Num of epochs: 61\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.33750419264882053\n",
      "Valence RMSE: 0.30457005908709134\n",
      "Arousal RMSE: 0.36749862478484197\n",
      "Test R^2 score: tensor([0.0087, 0.0040], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.006355579088656527\n",
      "Num of epochs: 62\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.33732241770569915\n",
      "Valence RMSE: 0.30433342942611175\n",
      "Arousal RMSE: 0.3673608453651236\n",
      "Test R^2 score: tensor([0.0103, 0.0047], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.007498773058005781\n",
      "Num of epochs: 63\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.33709091801274327\n",
      "Valence RMSE: 0.30403195657659976\n",
      "Arousal RMSE: 0.3671854346151989\n",
      "Test R^2 score: tensor([0.0122, 0.0057], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.008953828712469913\n",
      "Num of epochs: 64\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.33680741575221124\n",
      "Valence RMSE: 0.30366205203547847\n",
      "Arousal RMSE: 0.3669711552219984\n",
      "Test R^2 score: tensor([0.0146, 0.0068], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.010734966480003105\n",
      "Num of epochs: 65\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3364702160118254\n",
      "Valence RMSE: 0.3032290595424753\n",
      "Arousal RMSE: 0.366710444322322\n",
      "Test R^2 score: tensor([0.0174, 0.0082], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.012844329227778062\n",
      "Num of epochs: 66\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3360516069509647\n",
      "Valence RMSE: 0.3027053844933555\n",
      "Arousal RMSE: 0.36637523833821134\n",
      "Test R^2 score: tensor([0.0208, 0.0101], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.015445868545419628\n",
      "Num of epochs: 67\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3355292205938908\n",
      "Valence RMSE: 0.30206756280478575\n",
      "Arousal RMSE: 0.3659438525865197\n",
      "Test R^2 score: tensor([0.0250, 0.0124], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.018671776703869658\n",
      "Num of epochs: 68\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.33489340919473926\n",
      "Valence RMSE: 0.30130801019509845\n",
      "Arousal RMSE: 0.3654048084473171\n",
      "Test R^2 score: tensor([0.0299, 0.0153], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.02257415528116069\n",
      "Num of epochs: 69\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3340853249133366\n",
      "Valence RMSE: 0.30033305084504514\n",
      "Arousal RMSE: 0.3647273875307514\n",
      "Test R^2 score: tensor([0.0361, 0.0189], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.0275320729689279\n",
      "Num of epochs: 70\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3330488472677779\n",
      "Valence RMSE: 0.29907022472374845\n",
      "Arousal RMSE: 0.3638682041845342\n",
      "Test R^2 score: tensor([0.0442, 0.0236], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.03388475601623986\n",
      "Num of epochs: 71\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3317360055793718\n",
      "Valence RMSE: 0.2974897927447516\n",
      "Arousal RMSE: 0.3627635290491572\n",
      "Test R^2 score: tensor([0.0543, 0.0295], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.04188215403113377\n",
      "Num of epochs: 72\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3300348785171976\n",
      "Valence RMSE: 0.295542543335164\n",
      "Arousal RMSE: 0.36124873308387595\n",
      "Test R^2 score: tensor([0.0666, 0.0376], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.05209631004943954\n",
      "Num of epochs: 73\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3278454721094067\n",
      "Valence RMSE: 0.29312746571597836\n",
      "Arousal RMSE: 0.35922360168592404\n",
      "Test R^2 score: tensor([0.0818, 0.0483], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.0650725678460436\n",
      "Num of epochs: 74\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.32505655712858167\n",
      "Valence RMSE: 0.2901105079489374\n",
      "Arousal RMSE: 0.3565942005167545\n",
      "Test R^2 score: tensor([0.1006, 0.0622], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.08141458063839135\n",
      "Num of epochs: 75\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3215615977823252\n",
      "Valence RMSE: 0.2863009650483911\n",
      "Arousal RMSE: 0.35332064721581685\n",
      "Test R^2 score: tensor([0.1241, 0.0793], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.10171654894293036\n",
      "Num of epochs: 76\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.31733162727376185\n",
      "Valence RMSE: 0.28211978352468153\n",
      "Arousal RMSE: 0.3490088123248671\n",
      "Test R^2 score: tensor([0.1495, 0.1017], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.12558196535490462\n",
      "Num of epochs: 77\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.312662414187875\n",
      "Valence RMSE: 0.27824261627171604\n",
      "Arousal RMSE: 0.34365188342543845\n",
      "Test R^2 score: tensor([0.1727, 0.1290], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.15087275545162793\n",
      "Num of epochs: 78\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.30778460979860894\n",
      "Valence RMSE: 0.27410946660076013\n",
      "Arousal RMSE: 0.3381223630250012\n",
      "Test R^2 score: tensor([0.1971, 0.1568], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.17697191814292867\n",
      "Num of epochs: 79\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.30414010542741615\n",
      "Valence RMSE: 0.27308123292800734\n",
      "Arousal RMSE: 0.33230866326552344\n",
      "Test R^2 score: tensor([0.2031, 0.1856], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.19435072749472976\n",
      "Num of epochs: 80\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.3007484162488072\n",
      "Valence RMSE: 0.26962768557225847\n",
      "Arousal RMSE: 0.32893788308019334\n",
      "Test R^2 score: tensor([0.2231, 0.2020], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.2125840159726372\n",
      "Num of epochs: 81\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2974739738305176\n",
      "Valence RMSE: 0.26747662677586365\n",
      "Arousal RMSE: 0.3247118481694875\n",
      "Test R^2 score: tensor([0.2355, 0.2224], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.22894309361875986\n",
      "Num of epochs: 82\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.29287423473303764\n",
      "Valence RMSE: 0.26220910920811835\n",
      "Arousal RMSE: 0.320619740173948\n",
      "Test R^2 score: tensor([0.2653, 0.2419], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.2535884862264778\n",
      "Num of epochs: 83\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2879277582772512\n",
      "Valence RMSE: 0.25777139029404966\n",
      "Arousal RMSE: 0.315212148114566\n",
      "Test R^2 score: tensor([0.2900, 0.2672], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.27859630500810684\n",
      "Num of epochs: 84\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.283554085115751\n",
      "Valence RMSE: 0.25414473620746836\n",
      "Arousal RMSE: 0.310187510112347\n",
      "Test R^2 score: tensor([0.3098, 0.2904], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.30010334782532794\n",
      "Num of epochs: 85\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.28144603815122377\n",
      "Valence RMSE: 0.2536570023163454\n",
      "Arousal RMSE: 0.30672768045601256\n",
      "Test R^2 score: tensor([0.3124, 0.3062], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.30929729179177917\n",
      "Num of epochs: 86\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.27863215046724815\n",
      "Valence RMSE: 0.2527921262262446\n",
      "Arousal RMSE: 0.3022712216967088\n",
      "Test R^2 score: tensor([0.3171, 0.3262], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.32164534370106773\n",
      "Num of epochs: 87\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.27601581261797364\n",
      "Valence RMSE: 0.25145713777252676\n",
      "Arousal RMSE: 0.29856115871554556\n",
      "Test R^2 score: tensor([0.3243, 0.3426], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.33346192155078247\n",
      "Num of epochs: 88\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2724636870045158\n",
      "Valence RMSE: 0.2485915716197123\n",
      "Arousal RMSE: 0.2944064401330769\n",
      "Test R^2 score: tensor([0.3396, 0.3608], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.3502025316107331\n",
      "Num of epochs: 89\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2702781108673411\n",
      "Valence RMSE: 0.24893544958815886\n",
      "Arousal RMSE: 0.2900545748068433\n",
      "Test R^2 score: tensor([0.3378, 0.3795], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.3586674954708559\n",
      "Num of epochs: 90\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2686514288551495\n",
      "Valence RMSE: 0.24895606979978702\n",
      "Arousal RMSE: 0.28699835498077414\n",
      "Test R^2 score: tensor([0.3377, 0.3925], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.3651158819606598\n",
      "Num of epochs: 91\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2672638199517927\n",
      "Valence RMSE: 0.2488075801727134\n",
      "Arousal RMSE: 0.2845253713802116\n",
      "Test R^2 score: tensor([0.3385, 0.4030], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.3707225650827366\n",
      "Num of epochs: 92\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.26613302112114573\n",
      "Valence RMSE: 0.2510369132575743\n",
      "Arousal RMSE: 0.2804176136483683\n",
      "Test R^2 score: tensor([0.3266, 0.4201], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.3733261145540617\n",
      "Num of epochs: 93\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2646905267232212\n",
      "Valence RMSE: 0.2444259194171908\n",
      "Arousal RMSE: 0.28351035217623616\n",
      "Test R^2 score: tensor([0.3616, 0.4072], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.38439580952348434\n",
      "Num of epochs: 94\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.26303821550987944\n",
      "Valence RMSE: 0.24950534721650572\n",
      "Arousal RMSE: 0.2759081139575538\n",
      "Test R^2 score: tensor([0.3348, 0.4386], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.3866730866233323\n",
      "Num of epochs: 95\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2588171200708062\n",
      "Valence RMSE: 0.24374675333188486\n",
      "Arousal RMSE: 0.2730569968406917\n",
      "Test R^2 score: tensor([0.3651, 0.4501], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4076210198520829\n",
      "Num of epochs: 96\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2588486475354642\n",
      "Valence RMSE: 0.2430593900312123\n",
      "Arousal RMSE: 0.2737286568474938\n",
      "Test R^2 score: tensor([0.3687, 0.4474], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4080546089911993\n",
      "Num of epochs: 97\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2560347678705867\n",
      "Valence RMSE: 0.24282200566802542\n",
      "Arousal RMSE: 0.26859835867042803\n",
      "Test R^2 score: tensor([0.3699, 0.4679], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.41893054388804396\n",
      "Num of epochs: 98\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.25559182164422856\n",
      "Valence RMSE: 0.24358936079039992\n",
      "Arousal RMSE: 0.2670553910569016\n",
      "Test R^2 score: tensor([0.3659, 0.4740], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.41998396136375554\n",
      "Num of epochs: 99\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.25537037647744926\n",
      "Valence RMSE: 0.2403493605117252\n",
      "Arousal RMSE: 0.2695556403900187\n",
      "Test R^2 score: tensor([0.3827, 0.4641], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.42341420978119865\n",
      "Num of epochs: 100\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.25397848221804004\n",
      "Valence RMSE: 0.2426701206773724\n",
      "Arousal RMSE: 0.2648043643711109\n",
      "Test R^2 score: tensor([0.3707, 0.4829], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.42678698144129473\n",
      "Num of epochs: 101\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.25445332742572435\n",
      "Valence RMSE: 0.2465536324650144\n",
      "Arousal RMSE: 0.2621150472490131\n",
      "Test R^2 score: tensor([0.3504, 0.4933], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.42186116403328555\n",
      "Num of epochs: 102\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.25165524627397634\n",
      "Valence RMSE: 0.23744874355706885\n",
      "Arousal RMSE: 0.26510152798050923\n",
      "Test R^2 score: tensor([0.3975, 0.4817], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.43960055271845355\n",
      "Num of epochs: 103\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.24828387605080698\n",
      "Valence RMSE: 0.23733832003647432\n",
      "Arousal RMSE: 0.25876686042824154\n",
      "Test R^2 score: tensor([0.3981, 0.5062], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.45211769174987465\n",
      "Num of epochs: 104\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.248918824405494\n",
      "Valence RMSE: 0.24021293056610024\n",
      "Arousal RMSE: 0.25733035241819463\n",
      "Test R^2 score: tensor([0.3834, 0.5116], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.44751679532342875\n",
      "Num of epochs: 105\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.24665597410373874\n",
      "Valence RMSE: 0.2347383127367482\n",
      "Arousal RMSE: 0.2580237656798945\n",
      "Test R^2 score: tensor([0.4112, 0.5090], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.46009176945027763\n",
      "Num of epochs: 106\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.24472369433258123\n",
      "Valence RMSE: 0.23454270603104996\n",
      "Arousal RMSE: 0.25449772530065135\n",
      "Test R^2 score: tensor([0.4122, 0.5223], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4672461338936525\n",
      "Num of epochs: 107\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.24709911846043936\n",
      "Valence RMSE: 0.2415349083199049\n",
      "Arousal RMSE: 0.2525407625527959\n",
      "Test R^2 score: tensor([0.3766, 0.5296], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.45311915741363545\n",
      "Num of epochs: 108\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.24674300248120834\n",
      "Valence RMSE: 0.23630730590333515\n",
      "Arousal RMSE: 0.25675489425440506\n",
      "Test R^2 score: tensor([0.4033, 0.5138], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.45855155983150586\n",
      "Num of epochs: 109\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.24459083077529936\n",
      "Valence RMSE: 0.24062378379645186\n",
      "Arousal RMSE: 0.24849455460870903\n",
      "Test R^2 score: tensor([0.3813, 0.5446], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.46294196807869453\n",
      "Num of epochs: 110\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.24313676465532333\n",
      "Valence RMSE: 0.23977094295435925\n",
      "Arousal RMSE: 0.2464566241124258\n",
      "Test R^2 score: tensor([0.3857, 0.5520], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.46885046318976886\n",
      "Num of epochs: 111\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.24410361609055864\n",
      "Valence RMSE: 0.23632462550899377\n",
      "Arousal RMSE: 0.2516422503376719\n",
      "Test R^2 score: tensor([0.4032, 0.5330], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4680925172771787\n",
      "Num of epochs: 112\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.24261705099724953\n",
      "Valence RMSE: 0.2399960976140948\n",
      "Arousal RMSE: 0.24520999163820761\n",
      "Test R^2 score: tensor([0.3845, 0.5566], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.47053346208811503\n",
      "Num of epochs: 113\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.24172573268278122\n",
      "Valence RMSE: 0.23832646974372304\n",
      "Arousal RMSE: 0.24507785191964834\n",
      "Test R^2 score: tensor([0.3930, 0.5570], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4750393802781916\n",
      "Num of epochs: 114\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2433600151901778\n",
      "Valence RMSE: 0.23913362315036385\n",
      "Arousal RMSE: 0.24751425063156848\n",
      "Test R^2 score: tensor([0.3889, 0.5482], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.46855475009444375\n",
      "Num of epochs: 115\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.24685460657662334\n",
      "Valence RMSE: 0.24778317309867048\n",
      "Arousal RMSE: 0.2459225339519608\n",
      "Test R^2 score: tensor([0.3439, 0.5540], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4489483588632269\n",
      "Num of epochs: 116\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.24431402323235996\n",
      "Valence RMSE: 0.2396494855456626\n",
      "Arousal RMSE: 0.24889115688120275\n",
      "Test R^2 score: tensor([0.3863, 0.5431], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.46471470037240925\n",
      "Num of epochs: 117\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2419464377042838\n",
      "Valence RMSE: 0.24119977756190206\n",
      "Arousal RMSE: 0.24269080069025017\n",
      "Test R^2 score: tensor([0.3783, 0.5656], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.47197111059125146\n",
      "Num of epochs: 118\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.24117479544056983\n",
      "Valence RMSE: 0.24026445912083136\n",
      "Arousal RMSE: 0.24208170850970082\n",
      "Test R^2 score: tensor([0.3831, 0.5678], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4754659831145798\n",
      "Num of epochs: 119\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.24034434223076118\n",
      "Valence RMSE: 0.2371730153821857\n",
      "Arousal RMSE: 0.24347436509660722\n",
      "Test R^2 score: tensor([0.3989, 0.5628], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4808585552437711\n",
      "Num of epochs: 120\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.24014513456980774\n",
      "Valence RMSE: 0.23868819963719426\n",
      "Arousal RMSE: 0.2415932835760486\n",
      "Test R^2 score: tensor([0.3912, 0.5695], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.48037081632738055\n",
      "Num of epochs: 121\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Epoch 121, Loss: 0.4816228295571127\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23986699658110433\n",
      "Valence RMSE: 0.23843040130250667\n",
      "Arousal RMSE: 0.24129503897180452\n",
      "Test R^2 score: tensor([0.3925, 0.5706], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4815590721598769\n",
      "Num of epochs: 122\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Epoch 121, Loss: 0.4816228295571127\n",
      "Epoch 122, Loss: 0.4807618495484584\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2401468014244062\n",
      "Valence RMSE: 0.23762332371456552\n",
      "Arousal RMSE: 0.24264403659597567\n",
      "Test R^2 score: tensor([0.3966, 0.5658], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4812046061161318\n",
      "Num of epochs: 123\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Epoch 121, Loss: 0.4816228295571127\n",
      "Epoch 122, Loss: 0.4807618495484584\n",
      "Epoch 123, Loss: 0.48102030846138955\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.24139922314399928\n",
      "Valence RMSE: 0.24210695772143903\n",
      "Arousal RMSE: 0.2406894075191554\n",
      "Test R^2 score: tensor([0.3736, 0.5728], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.47319589057555883\n",
      "Num of epochs: 124\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Epoch 121, Loss: 0.4816228295571127\n",
      "Epoch 122, Loss: 0.4807618495484584\n",
      "Epoch 123, Loss: 0.48102030846138955\n",
      "Epoch 124, Loss: 0.4810176288364116\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23938289373051083\n",
      "Valence RMSE: 0.23709051277616686\n",
      "Arousal RMSE: 0.24165352961030867\n",
      "Test R^2 score: tensor([0.3993, 0.5693], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4843249065023544\n",
      "Num of epochs: 125\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Epoch 121, Loss: 0.4816228295571127\n",
      "Epoch 122, Loss: 0.4807618495484584\n",
      "Epoch 123, Loss: 0.48102030846138955\n",
      "Epoch 124, Loss: 0.4810176288364116\n",
      "Epoch 125, Loss: 0.48014338261661127\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23883774990622827\n",
      "Valence RMSE: 0.2380804954237269\n",
      "Arousal RMSE: 0.23959261102824694\n",
      "Test R^2 score: tensor([0.3943, 0.5766], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4854687824459226\n",
      "Num of epochs: 126\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Epoch 121, Loss: 0.4816228295571127\n",
      "Epoch 122, Loss: 0.4807618495484584\n",
      "Epoch 123, Loss: 0.48102030846138955\n",
      "Epoch 124, Loss: 0.4810176288364116\n",
      "Epoch 125, Loss: 0.48014338261661127\n",
      "Epoch 126, Loss: 0.47910314809270116\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2387354484772426\n",
      "Valence RMSE: 0.2383644194540563\n",
      "Arousal RMSE: 0.23910590176235932\n",
      "Test R^2 score: tensor([0.3928, 0.5784], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.48560514925031134\n",
      "Num of epochs: 127\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Epoch 121, Loss: 0.4816228295571127\n",
      "Epoch 122, Loss: 0.4807618495484584\n",
      "Epoch 123, Loss: 0.48102030846138955\n",
      "Epoch 124, Loss: 0.4810176288364116\n",
      "Epoch 125, Loss: 0.48014338261661127\n",
      "Epoch 126, Loss: 0.47910314809270116\n",
      "Epoch 127, Loss: 0.47880795949683447\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2383956441924214\n",
      "Valence RMSE: 0.2366894014936212\n",
      "Arousal RMSE: 0.24008976146522898\n",
      "Test R^2 score: tensor([0.4014, 0.5749], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4881181860396543\n",
      "Num of epochs: 128\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Epoch 121, Loss: 0.4816228295571127\n",
      "Epoch 122, Loss: 0.4807618495484584\n",
      "Epoch 123, Loss: 0.48102030846138955\n",
      "Epoch 124, Loss: 0.4810176288364116\n",
      "Epoch 125, Loss: 0.48014338261661127\n",
      "Epoch 126, Loss: 0.47910314809270116\n",
      "Epoch 127, Loss: 0.47880795949683447\n",
      "Epoch 128, Loss: 0.47876177315142754\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2397587227636973\n",
      "Valence RMSE: 0.24077851084179572\n",
      "Arousal RMSE: 0.23873457855821015\n",
      "Test R^2 score: tensor([0.3805, 0.5797], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4800792262848939\n",
      "Num of epochs: 129\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Epoch 121, Loss: 0.4816228295571127\n",
      "Epoch 122, Loss: 0.4807618495484584\n",
      "Epoch 123, Loss: 0.48102030846138955\n",
      "Epoch 124, Loss: 0.4810176288364116\n",
      "Epoch 125, Loss: 0.48014338261661127\n",
      "Epoch 126, Loss: 0.47910314809270116\n",
      "Epoch 127, Loss: 0.47880795949683447\n",
      "Epoch 128, Loss: 0.47876177315142754\n",
      "Epoch 129, Loss: 0.47819597116449997\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23901495203168585\n",
      "Valence RMSE: 0.23860340406971287\n",
      "Arousal RMSE: 0.23942579258668753\n",
      "Test R^2 score: tensor([0.3916, 0.5772], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.48443164149050383\n",
      "Num of epochs: 130\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Epoch 121, Loss: 0.4816228295571127\n",
      "Epoch 122, Loss: 0.4807618495484584\n",
      "Epoch 123, Loss: 0.48102030846138955\n",
      "Epoch 124, Loss: 0.4810176288364116\n",
      "Epoch 125, Loss: 0.48014338261661127\n",
      "Epoch 126, Loss: 0.47910314809270116\n",
      "Epoch 127, Loss: 0.47880795949683447\n",
      "Epoch 128, Loss: 0.47876177315142754\n",
      "Epoch 129, Loss: 0.47819597116449997\n",
      "Epoch 130, Loss: 0.47743214539542544\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23982617202896292\n",
      "Valence RMSE: 0.2417293175556956\n",
      "Arousal RMSE: 0.23790780276020607\n",
      "Test R^2 score: tensor([0.3756, 0.5826], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.47908116408306456\n",
      "Num of epochs: 131\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Epoch 121, Loss: 0.4816228295571127\n",
      "Epoch 122, Loss: 0.4807618495484584\n",
      "Epoch 123, Loss: 0.48102030846138955\n",
      "Epoch 124, Loss: 0.4810176288364116\n",
      "Epoch 125, Loss: 0.48014338261661127\n",
      "Epoch 126, Loss: 0.47910314809270116\n",
      "Epoch 127, Loss: 0.47880795949683447\n",
      "Epoch 128, Loss: 0.47876177315142754\n",
      "Epoch 129, Loss: 0.47819597116449997\n",
      "Epoch 130, Loss: 0.47743214539542544\n",
      "Epoch 131, Loss: 0.47658476230340185\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2394520438306258\n",
      "Valence RMSE: 0.24095577124369927\n",
      "Arousal RMSE: 0.23793881334007272\n",
      "Test R^2 score: tensor([0.3796, 0.5825], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4810217108899056\n",
      "Num of epochs: 132\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Epoch 121, Loss: 0.4816228295571127\n",
      "Epoch 122, Loss: 0.4807618495484584\n",
      "Epoch 123, Loss: 0.48102030846138955\n",
      "Epoch 124, Loss: 0.4810176288364116\n",
      "Epoch 125, Loss: 0.48014338261661127\n",
      "Epoch 126, Loss: 0.47910314809270116\n",
      "Epoch 127, Loss: 0.47880795949683447\n",
      "Epoch 128, Loss: 0.47876177315142754\n",
      "Epoch 129, Loss: 0.47819597116449997\n",
      "Epoch 130, Loss: 0.47743214539542544\n",
      "Epoch 131, Loss: 0.47658476230340185\n",
      "Epoch 132, Loss: 0.4759644680184978\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23916221547290406\n",
      "Valence RMSE: 0.24039216483526038\n",
      "Arousal RMSE: 0.237925908016829\n",
      "Test R^2 score: tensor([0.3825, 0.5825], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.48249385929261457\n",
      "Num of epochs: 133\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Epoch 121, Loss: 0.4816228295571127\n",
      "Epoch 122, Loss: 0.4807618495484584\n",
      "Epoch 123, Loss: 0.48102030846138955\n",
      "Epoch 124, Loss: 0.4810176288364116\n",
      "Epoch 125, Loss: 0.48014338261661127\n",
      "Epoch 126, Loss: 0.47910314809270116\n",
      "Epoch 127, Loss: 0.47880795949683447\n",
      "Epoch 128, Loss: 0.47876177315142754\n",
      "Epoch 129, Loss: 0.47819597116449997\n",
      "Epoch 130, Loss: 0.47743214539542544\n",
      "Epoch 131, Loss: 0.47658476230340185\n",
      "Epoch 132, Loss: 0.4759644680184978\n",
      "Epoch 133, Loss: 0.4755635187712421\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23958110694222243\n",
      "Valence RMSE: 0.2417278329313822\n",
      "Arousal RMSE: 0.23741497087087624\n",
      "Test R^2 score: tensor([0.3756, 0.5843], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.47994880664197825\n",
      "Num of epochs: 134\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Epoch 121, Loss: 0.4816228295571127\n",
      "Epoch 122, Loss: 0.4807618495484584\n",
      "Epoch 123, Loss: 0.48102030846138955\n",
      "Epoch 124, Loss: 0.4810176288364116\n",
      "Epoch 125, Loss: 0.48014338261661127\n",
      "Epoch 126, Loss: 0.47910314809270116\n",
      "Epoch 127, Loss: 0.47880795949683447\n",
      "Epoch 128, Loss: 0.47876177315142754\n",
      "Epoch 129, Loss: 0.47819597116449997\n",
      "Epoch 130, Loss: 0.47743214539542544\n",
      "Epoch 131, Loss: 0.47658476230340185\n",
      "Epoch 132, Loss: 0.4759644680184978\n",
      "Epoch 133, Loss: 0.4755635187712421\n",
      "Epoch 134, Loss: 0.47538264196584307\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23919797981537108\n",
      "Valence RMSE: 0.23978561232885384\n",
      "Arousal RMSE: 0.2386089001181347\n",
      "Test R^2 score: tensor([0.3856, 0.5801], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.48284985811023845\n",
      "Num of epochs: 135\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Epoch 121, Loss: 0.4816228295571127\n",
      "Epoch 122, Loss: 0.4807618495484584\n",
      "Epoch 123, Loss: 0.48102030846138955\n",
      "Epoch 124, Loss: 0.4810176288364116\n",
      "Epoch 125, Loss: 0.48014338261661127\n",
      "Epoch 126, Loss: 0.47910314809270116\n",
      "Epoch 127, Loss: 0.47880795949683447\n",
      "Epoch 128, Loss: 0.47876177315142754\n",
      "Epoch 129, Loss: 0.47819597116449997\n",
      "Epoch 130, Loss: 0.47743214539542544\n",
      "Epoch 131, Loss: 0.47658476230340185\n",
      "Epoch 132, Loss: 0.4759644680184978\n",
      "Epoch 133, Loss: 0.4755635187712421\n",
      "Epoch 134, Loss: 0.47538264196584307\n",
      "Epoch 135, Loss: 0.4753673450596914\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.24078738269951636\n",
      "Valence RMSE: 0.24396457248817352\n",
      "Arousal RMSE: 0.23756770551829934\n",
      "Test R^2 score: tensor([0.3640, 0.5838], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.47387684751434245\n",
      "Num of epochs: 136\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Epoch 121, Loss: 0.4816228295571127\n",
      "Epoch 122, Loss: 0.4807618495484584\n",
      "Epoch 123, Loss: 0.48102030846138955\n",
      "Epoch 124, Loss: 0.4810176288364116\n",
      "Epoch 125, Loss: 0.48014338261661127\n",
      "Epoch 126, Loss: 0.47910314809270116\n",
      "Epoch 127, Loss: 0.47880795949683447\n",
      "Epoch 128, Loss: 0.47876177315142754\n",
      "Epoch 129, Loss: 0.47819597116449997\n",
      "Epoch 130, Loss: 0.47743214539542544\n",
      "Epoch 131, Loss: 0.47658476230340185\n",
      "Epoch 132, Loss: 0.4759644680184978\n",
      "Epoch 133, Loss: 0.4755635187712421\n",
      "Epoch 134, Loss: 0.47538264196584307\n",
      "Epoch 135, Loss: 0.4753673450596914\n",
      "Epoch 136, Loss: 0.4753697744168057\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.24021355244708936\n",
      "Valence RMSE: 0.2402897803673618\n",
      "Arousal RMSE: 0.24013730032942848\n",
      "Test R^2 score: tensor([0.3830, 0.5747], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.47885846190942344\n",
      "Num of epochs: 137\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Epoch 121, Loss: 0.4816228295571127\n",
      "Epoch 122, Loss: 0.4807618495484584\n",
      "Epoch 123, Loss: 0.48102030846138955\n",
      "Epoch 124, Loss: 0.4810176288364116\n",
      "Epoch 125, Loss: 0.48014338261661127\n",
      "Epoch 126, Loss: 0.47910314809270116\n",
      "Epoch 127, Loss: 0.47880795949683447\n",
      "Epoch 128, Loss: 0.47876177315142754\n",
      "Epoch 129, Loss: 0.47819597116449997\n",
      "Epoch 130, Loss: 0.47743214539542544\n",
      "Epoch 131, Loss: 0.47658476230340185\n",
      "Epoch 132, Loss: 0.4759644680184978\n",
      "Epoch 133, Loss: 0.4755635187712421\n",
      "Epoch 134, Loss: 0.47538264196584307\n",
      "Epoch 135, Loss: 0.4753673450596914\n",
      "Epoch 136, Loss: 0.4753697744168057\n",
      "Epoch 137, Loss: 0.4755973109583155\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.24336299518218985\n",
      "Valence RMSE: 0.24815098859531767\n",
      "Arousal RMSE: 0.23847889153394614\n",
      "Test R^2 score: tensor([0.3420, 0.5806], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4612697177428033\n",
      "Num of epochs: 138\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Epoch 121, Loss: 0.4816228295571127\n",
      "Epoch 122, Loss: 0.4807618495484584\n",
      "Epoch 123, Loss: 0.48102030846138955\n",
      "Epoch 124, Loss: 0.4810176288364116\n",
      "Epoch 125, Loss: 0.48014338261661127\n",
      "Epoch 126, Loss: 0.47910314809270116\n",
      "Epoch 127, Loss: 0.47880795949683447\n",
      "Epoch 128, Loss: 0.47876177315142754\n",
      "Epoch 129, Loss: 0.47819597116449997\n",
      "Epoch 130, Loss: 0.47743214539542544\n",
      "Epoch 131, Loss: 0.47658476230340185\n",
      "Epoch 132, Loss: 0.4759644680184978\n",
      "Epoch 133, Loss: 0.4755635187712421\n",
      "Epoch 134, Loss: 0.47538264196584307\n",
      "Epoch 135, Loss: 0.4753673450596914\n",
      "Epoch 136, Loss: 0.4753697744168057\n",
      "Epoch 137, Loss: 0.4755973109583155\n",
      "Epoch 138, Loss: 0.4767626826605241\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.24109141493191233\n",
      "Valence RMSE: 0.24008158299275886\n",
      "Arousal RMSE: 0.24209703471012214\n",
      "Test R^2 score: tensor([0.3841, 0.5677], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.47590796806929203\n",
      "Num of epochs: 139\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Epoch 121, Loss: 0.4816228295571127\n",
      "Epoch 122, Loss: 0.4807618495484584\n",
      "Epoch 123, Loss: 0.48102030846138955\n",
      "Epoch 124, Loss: 0.4810176288364116\n",
      "Epoch 125, Loss: 0.48014338261661127\n",
      "Epoch 126, Loss: 0.47910314809270116\n",
      "Epoch 127, Loss: 0.47880795949683447\n",
      "Epoch 128, Loss: 0.47876177315142754\n",
      "Epoch 129, Loss: 0.47819597116449997\n",
      "Epoch 130, Loss: 0.47743214539542544\n",
      "Epoch 131, Loss: 0.47658476230340185\n",
      "Epoch 132, Loss: 0.4759644680184978\n",
      "Epoch 133, Loss: 0.4755635187712421\n",
      "Epoch 134, Loss: 0.47538264196584307\n",
      "Epoch 135, Loss: 0.4753673450596914\n",
      "Epoch 136, Loss: 0.4753697744168057\n",
      "Epoch 137, Loss: 0.4755973109583155\n",
      "Epoch 138, Loss: 0.4767626826605241\n",
      "Epoch 139, Loss: 0.47683528222287225\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.24221151615140532\n",
      "Valence RMSE: 0.24651463519778152\n",
      "Arousal RMSE: 0.2378305525916081\n",
      "Test R^2 score: tensor([0.3506, 0.5828], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.46673331639778093\n",
      "Num of epochs: 140\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Epoch 121, Loss: 0.4816228295571127\n",
      "Epoch 122, Loss: 0.4807618495484584\n",
      "Epoch 123, Loss: 0.48102030846138955\n",
      "Epoch 124, Loss: 0.4810176288364116\n",
      "Epoch 125, Loss: 0.48014338261661127\n",
      "Epoch 126, Loss: 0.47910314809270116\n",
      "Epoch 127, Loss: 0.47880795949683447\n",
      "Epoch 128, Loss: 0.47876177315142754\n",
      "Epoch 129, Loss: 0.47819597116449997\n",
      "Epoch 130, Loss: 0.47743214539542544\n",
      "Epoch 131, Loss: 0.47658476230340185\n",
      "Epoch 132, Loss: 0.4759644680184978\n",
      "Epoch 133, Loss: 0.4755635187712421\n",
      "Epoch 134, Loss: 0.47538264196584307\n",
      "Epoch 135, Loss: 0.4753673450596914\n",
      "Epoch 136, Loss: 0.4753697744168057\n",
      "Epoch 137, Loss: 0.4755973109583155\n",
      "Epoch 138, Loss: 0.4767626826605241\n",
      "Epoch 139, Loss: 0.47683528222287225\n",
      "Epoch 140, Loss: 0.47579445468261566\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2382035756592392\n",
      "Valence RMSE: 0.23950032141361985\n",
      "Arousal RMSE: 0.23689973186237803\n",
      "Test R^2 score: tensor([0.3870, 0.5861], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.48657734024035787\n",
      "Num of epochs: 141\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Epoch 121, Loss: 0.4816228295571127\n",
      "Epoch 122, Loss: 0.4807618495484584\n",
      "Epoch 123, Loss: 0.48102030846138955\n",
      "Epoch 124, Loss: 0.4810176288364116\n",
      "Epoch 125, Loss: 0.48014338261661127\n",
      "Epoch 126, Loss: 0.47910314809270116\n",
      "Epoch 127, Loss: 0.47880795949683447\n",
      "Epoch 128, Loss: 0.47876177315142754\n",
      "Epoch 129, Loss: 0.47819597116449997\n",
      "Epoch 130, Loss: 0.47743214539542544\n",
      "Epoch 131, Loss: 0.47658476230340185\n",
      "Epoch 132, Loss: 0.4759644680184978\n",
      "Epoch 133, Loss: 0.4755635187712421\n",
      "Epoch 134, Loss: 0.47538264196584307\n",
      "Epoch 135, Loss: 0.4753673450596914\n",
      "Epoch 136, Loss: 0.4753697744168057\n",
      "Epoch 137, Loss: 0.4755973109583155\n",
      "Epoch 138, Loss: 0.4767626826605241\n",
      "Epoch 139, Loss: 0.47683528222287225\n",
      "Epoch 140, Loss: 0.47579445468261566\n",
      "Epoch 141, Loss: 0.47284533919249566\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23833754691082631\n",
      "Valence RMSE: 0.23933235049524884\n",
      "Arousal RMSE: 0.23733857364819572\n",
      "Test R^2 score: tensor([0.3879, 0.5846], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4862396544878269\n",
      "Num of epochs: 142\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Epoch 121, Loss: 0.4816228295571127\n",
      "Epoch 122, Loss: 0.4807618495484584\n",
      "Epoch 123, Loss: 0.48102030846138955\n",
      "Epoch 124, Loss: 0.4810176288364116\n",
      "Epoch 125, Loss: 0.48014338261661127\n",
      "Epoch 126, Loss: 0.47910314809270116\n",
      "Epoch 127, Loss: 0.47880795949683447\n",
      "Epoch 128, Loss: 0.47876177315142754\n",
      "Epoch 129, Loss: 0.47819597116449997\n",
      "Epoch 130, Loss: 0.47743214539542544\n",
      "Epoch 131, Loss: 0.47658476230340185\n",
      "Epoch 132, Loss: 0.4759644680184978\n",
      "Epoch 133, Loss: 0.4755635187712421\n",
      "Epoch 134, Loss: 0.47538264196584307\n",
      "Epoch 135, Loss: 0.4753673450596914\n",
      "Epoch 136, Loss: 0.4753697744168057\n",
      "Epoch 137, Loss: 0.4755973109583155\n",
      "Epoch 138, Loss: 0.4767626826605241\n",
      "Epoch 139, Loss: 0.47683528222287225\n",
      "Epoch 140, Loss: 0.47579445468261566\n",
      "Epoch 141, Loss: 0.47284533919249566\n",
      "Epoch 142, Loss: 0.472809286012082\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.24197996569061178\n",
      "Valence RMSE: 0.24605829438697688\n",
      "Arousal RMSE: 0.23783171225602173\n",
      "Test R^2 score: tensor([0.3530, 0.5828], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4679322848255537\n",
      "Num of epochs: 143\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Epoch 121, Loss: 0.4816228295571127\n",
      "Epoch 122, Loss: 0.4807618495484584\n",
      "Epoch 123, Loss: 0.48102030846138955\n",
      "Epoch 124, Loss: 0.4810176288364116\n",
      "Epoch 125, Loss: 0.48014338261661127\n",
      "Epoch 126, Loss: 0.47910314809270116\n",
      "Epoch 127, Loss: 0.47880795949683447\n",
      "Epoch 128, Loss: 0.47876177315142754\n",
      "Epoch 129, Loss: 0.47819597116449997\n",
      "Epoch 130, Loss: 0.47743214539542544\n",
      "Epoch 131, Loss: 0.47658476230340185\n",
      "Epoch 132, Loss: 0.4759644680184978\n",
      "Epoch 133, Loss: 0.4755635187712421\n",
      "Epoch 134, Loss: 0.47538264196584307\n",
      "Epoch 135, Loss: 0.4753673450596914\n",
      "Epoch 136, Loss: 0.4753697744168057\n",
      "Epoch 137, Loss: 0.4755973109583155\n",
      "Epoch 138, Loss: 0.4767626826605241\n",
      "Epoch 139, Loss: 0.47683528222287225\n",
      "Epoch 140, Loss: 0.47579445468261566\n",
      "Epoch 141, Loss: 0.47284533919249566\n",
      "Epoch 142, Loss: 0.472809286012082\n",
      "Epoch 143, Loss: 0.47458738111232796\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2396252487539475\n",
      "Valence RMSE: 0.24006157072828713\n",
      "Arousal RMSE: 0.2391881308515329\n",
      "Test R^2 score: tensor([0.3842, 0.5781], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4811218239570227\n",
      "Num of epochs: 144\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Epoch 121, Loss: 0.4816228295571127\n",
      "Epoch 122, Loss: 0.4807618495484584\n",
      "Epoch 123, Loss: 0.48102030846138955\n",
      "Epoch 124, Loss: 0.4810176288364116\n",
      "Epoch 125, Loss: 0.48014338261661127\n",
      "Epoch 126, Loss: 0.47910314809270116\n",
      "Epoch 127, Loss: 0.47880795949683447\n",
      "Epoch 128, Loss: 0.47876177315142754\n",
      "Epoch 129, Loss: 0.47819597116449997\n",
      "Epoch 130, Loss: 0.47743214539542544\n",
      "Epoch 131, Loss: 0.47658476230340185\n",
      "Epoch 132, Loss: 0.4759644680184978\n",
      "Epoch 133, Loss: 0.4755635187712421\n",
      "Epoch 134, Loss: 0.47538264196584307\n",
      "Epoch 135, Loss: 0.4753673450596914\n",
      "Epoch 136, Loss: 0.4753697744168057\n",
      "Epoch 137, Loss: 0.4755973109583155\n",
      "Epoch 138, Loss: 0.4767626826605241\n",
      "Epoch 139, Loss: 0.47683528222287225\n",
      "Epoch 140, Loss: 0.47579445468261566\n",
      "Epoch 141, Loss: 0.47284533919249566\n",
      "Epoch 142, Loss: 0.472809286012082\n",
      "Epoch 143, Loss: 0.47458738111232796\n",
      "Epoch 144, Loss: 0.4733380058918715\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23979701718137203\n",
      "Valence RMSE: 0.2433923597578617\n",
      "Arousal RMSE: 0.2361469417749595\n",
      "Test R^2 score: tensor([0.3670, 0.5887], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.47784866450651264\n",
      "Num of epochs: 145\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Epoch 121, Loss: 0.4816228295571127\n",
      "Epoch 122, Loss: 0.4807618495484584\n",
      "Epoch 123, Loss: 0.48102030846138955\n",
      "Epoch 124, Loss: 0.4810176288364116\n",
      "Epoch 125, Loss: 0.48014338261661127\n",
      "Epoch 126, Loss: 0.47910314809270116\n",
      "Epoch 127, Loss: 0.47880795949683447\n",
      "Epoch 128, Loss: 0.47876177315142754\n",
      "Epoch 129, Loss: 0.47819597116449997\n",
      "Epoch 130, Loss: 0.47743214539542544\n",
      "Epoch 131, Loss: 0.47658476230340185\n",
      "Epoch 132, Loss: 0.4759644680184978\n",
      "Epoch 133, Loss: 0.4755635187712421\n",
      "Epoch 134, Loss: 0.47538264196584307\n",
      "Epoch 135, Loss: 0.4753673450596914\n",
      "Epoch 136, Loss: 0.4753697744168057\n",
      "Epoch 137, Loss: 0.4755973109583155\n",
      "Epoch 138, Loss: 0.4767626826605241\n",
      "Epoch 139, Loss: 0.47683528222287225\n",
      "Epoch 140, Loss: 0.47579445468261566\n",
      "Epoch 141, Loss: 0.47284533919249566\n",
      "Epoch 142, Loss: 0.472809286012082\n",
      "Epoch 143, Loss: 0.47458738111232796\n",
      "Epoch 144, Loss: 0.4733380058918715\n",
      "Epoch 145, Loss: 0.4715229034484008\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2394864629092468\n",
      "Valence RMSE: 0.2428381881011207\n",
      "Arousal RMSE: 0.2360871581287911\n",
      "Test R^2 score: tensor([0.3698, 0.5889], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.47939246431866905\n",
      "Num of epochs: 146\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Epoch 121, Loss: 0.4816228295571127\n",
      "Epoch 122, Loss: 0.4807618495484584\n",
      "Epoch 123, Loss: 0.48102030846138955\n",
      "Epoch 124, Loss: 0.4810176288364116\n",
      "Epoch 125, Loss: 0.48014338261661127\n",
      "Epoch 126, Loss: 0.47910314809270116\n",
      "Epoch 127, Loss: 0.47880795949683447\n",
      "Epoch 128, Loss: 0.47876177315142754\n",
      "Epoch 129, Loss: 0.47819597116449997\n",
      "Epoch 130, Loss: 0.47743214539542544\n",
      "Epoch 131, Loss: 0.47658476230340185\n",
      "Epoch 132, Loss: 0.4759644680184978\n",
      "Epoch 133, Loss: 0.4755635187712421\n",
      "Epoch 134, Loss: 0.47538264196584307\n",
      "Epoch 135, Loss: 0.4753673450596914\n",
      "Epoch 136, Loss: 0.4753697744168057\n",
      "Epoch 137, Loss: 0.4755973109583155\n",
      "Epoch 138, Loss: 0.4767626826605241\n",
      "Epoch 139, Loss: 0.47683528222287225\n",
      "Epoch 140, Loss: 0.47579445468261566\n",
      "Epoch 141, Loss: 0.47284533919249566\n",
      "Epoch 142, Loss: 0.472809286012082\n",
      "Epoch 143, Loss: 0.47458738111232796\n",
      "Epoch 144, Loss: 0.4733380058918715\n",
      "Epoch 145, Loss: 0.4715229034484008\n",
      "Epoch 146, Loss: 0.47096730966267386\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2393226049119067\n",
      "Valence RMSE: 0.2405943095370292\n",
      "Arousal RMSE: 0.238044106547592\n",
      "Test R^2 score: tensor([0.3814, 0.5821], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.48176691305274977\n",
      "Num of epochs: 147\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Epoch 121, Loss: 0.4816228295571127\n",
      "Epoch 122, Loss: 0.4807618495484584\n",
      "Epoch 123, Loss: 0.48102030846138955\n",
      "Epoch 124, Loss: 0.4810176288364116\n",
      "Epoch 125, Loss: 0.48014338261661127\n",
      "Epoch 126, Loss: 0.47910314809270116\n",
      "Epoch 127, Loss: 0.47880795949683447\n",
      "Epoch 128, Loss: 0.47876177315142754\n",
      "Epoch 129, Loss: 0.47819597116449997\n",
      "Epoch 130, Loss: 0.47743214539542544\n",
      "Epoch 131, Loss: 0.47658476230340185\n",
      "Epoch 132, Loss: 0.4759644680184978\n",
      "Epoch 133, Loss: 0.4755635187712421\n",
      "Epoch 134, Loss: 0.47538264196584307\n",
      "Epoch 135, Loss: 0.4753673450596914\n",
      "Epoch 136, Loss: 0.4753697744168057\n",
      "Epoch 137, Loss: 0.4755973109583155\n",
      "Epoch 138, Loss: 0.4767626826605241\n",
      "Epoch 139, Loss: 0.47683528222287225\n",
      "Epoch 140, Loss: 0.47579445468261566\n",
      "Epoch 141, Loss: 0.47284533919249566\n",
      "Epoch 142, Loss: 0.472809286012082\n",
      "Epoch 143, Loss: 0.47458738111232796\n",
      "Epoch 144, Loss: 0.4733380058918715\n",
      "Epoch 145, Loss: 0.4715229034484008\n",
      "Epoch 146, Loss: 0.47096730966267386\n",
      "Epoch 147, Loss: 0.47161469890467383\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.24067458828116967\n",
      "Valence RMSE: 0.24442871858155088\n",
      "Arousal RMSE: 0.23686096432549214\n",
      "Test R^2 score: tensor([0.3616, 0.5862], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4739020737362298\n",
      "Num of epochs: 148\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Epoch 121, Loss: 0.4816228295571127\n",
      "Epoch 122, Loss: 0.4807618495484584\n",
      "Epoch 123, Loss: 0.48102030846138955\n",
      "Epoch 124, Loss: 0.4810176288364116\n",
      "Epoch 125, Loss: 0.48014338261661127\n",
      "Epoch 126, Loss: 0.47910314809270116\n",
      "Epoch 127, Loss: 0.47880795949683447\n",
      "Epoch 128, Loss: 0.47876177315142754\n",
      "Epoch 129, Loss: 0.47819597116449997\n",
      "Epoch 130, Loss: 0.47743214539542544\n",
      "Epoch 131, Loss: 0.47658476230340185\n",
      "Epoch 132, Loss: 0.4759644680184978\n",
      "Epoch 133, Loss: 0.4755635187712421\n",
      "Epoch 134, Loss: 0.47538264196584307\n",
      "Epoch 135, Loss: 0.4753673450596914\n",
      "Epoch 136, Loss: 0.4753697744168057\n",
      "Epoch 137, Loss: 0.4755973109583155\n",
      "Epoch 138, Loss: 0.4767626826605241\n",
      "Epoch 139, Loss: 0.47683528222287225\n",
      "Epoch 140, Loss: 0.47579445468261566\n",
      "Epoch 141, Loss: 0.47284533919249566\n",
      "Epoch 142, Loss: 0.472809286012082\n",
      "Epoch 143, Loss: 0.47458738111232796\n",
      "Epoch 144, Loss: 0.4733380058918715\n",
      "Epoch 145, Loss: 0.4715229034484008\n",
      "Epoch 146, Loss: 0.47096730966267386\n",
      "Epoch 147, Loss: 0.47161469890467383\n",
      "Epoch 148, Loss: 0.47183531411129226\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2389975850833008\n",
      "Valence RMSE: 0.24118413918080475\n",
      "Arousal RMSE: 0.23679084095233374\n",
      "Test R^2 score: tensor([0.3784, 0.5865], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4824430095694156\n",
      "Num of epochs: 149\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Epoch 121, Loss: 0.4816228295571127\n",
      "Epoch 122, Loss: 0.4807618495484584\n",
      "Epoch 123, Loss: 0.48102030846138955\n",
      "Epoch 124, Loss: 0.4810176288364116\n",
      "Epoch 125, Loss: 0.48014338261661127\n",
      "Epoch 126, Loss: 0.47910314809270116\n",
      "Epoch 127, Loss: 0.47880795949683447\n",
      "Epoch 128, Loss: 0.47876177315142754\n",
      "Epoch 129, Loss: 0.47819597116449997\n",
      "Epoch 130, Loss: 0.47743214539542544\n",
      "Epoch 131, Loss: 0.47658476230340185\n",
      "Epoch 132, Loss: 0.4759644680184978\n",
      "Epoch 133, Loss: 0.4755635187712421\n",
      "Epoch 134, Loss: 0.47538264196584307\n",
      "Epoch 135, Loss: 0.4753673450596914\n",
      "Epoch 136, Loss: 0.4753697744168057\n",
      "Epoch 137, Loss: 0.4755973109583155\n",
      "Epoch 138, Loss: 0.4767626826605241\n",
      "Epoch 139, Loss: 0.47683528222287225\n",
      "Epoch 140, Loss: 0.47579445468261566\n",
      "Epoch 141, Loss: 0.47284533919249566\n",
      "Epoch 142, Loss: 0.472809286012082\n",
      "Epoch 143, Loss: 0.47458738111232796\n",
      "Epoch 144, Loss: 0.4733380058918715\n",
      "Epoch 145, Loss: 0.4715229034484008\n",
      "Epoch 146, Loss: 0.47096730966267386\n",
      "Epoch 147, Loss: 0.47161469890467383\n",
      "Epoch 148, Loss: 0.47183531411129226\n",
      "Epoch 149, Loss: 0.470294933688076\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23930878513680554\n",
      "Valence RMSE: 0.24251762867801158\n",
      "Arousal RMSE: 0.23605632604889376\n",
      "Test R^2 score: tensor([0.3715, 0.5890], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.48027743410707524\n",
      "Num of epochs: 150\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Epoch 121, Loss: 0.4816228295571127\n",
      "Epoch 122, Loss: 0.4807618495484584\n",
      "Epoch 123, Loss: 0.48102030846138955\n",
      "Epoch 124, Loss: 0.4810176288364116\n",
      "Epoch 125, Loss: 0.48014338261661127\n",
      "Epoch 126, Loss: 0.47910314809270116\n",
      "Epoch 127, Loss: 0.47880795949683447\n",
      "Epoch 128, Loss: 0.47876177315142754\n",
      "Epoch 129, Loss: 0.47819597116449997\n",
      "Epoch 130, Loss: 0.47743214539542544\n",
      "Epoch 131, Loss: 0.47658476230340185\n",
      "Epoch 132, Loss: 0.4759644680184978\n",
      "Epoch 133, Loss: 0.4755635187712421\n",
      "Epoch 134, Loss: 0.47538264196584307\n",
      "Epoch 135, Loss: 0.4753673450596914\n",
      "Epoch 136, Loss: 0.4753697744168057\n",
      "Epoch 137, Loss: 0.4755973109583155\n",
      "Epoch 138, Loss: 0.4767626826605241\n",
      "Epoch 139, Loss: 0.47683528222287225\n",
      "Epoch 140, Loss: 0.47579445468261566\n",
      "Epoch 141, Loss: 0.47284533919249566\n",
      "Epoch 142, Loss: 0.472809286012082\n",
      "Epoch 143, Loss: 0.47458738111232796\n",
      "Epoch 144, Loss: 0.4733380058918715\n",
      "Epoch 145, Loss: 0.4715229034484008\n",
      "Epoch 146, Loss: 0.47096730966267386\n",
      "Epoch 147, Loss: 0.47161469890467383\n",
      "Epoch 148, Loss: 0.47183531411129226\n",
      "Epoch 149, Loss: 0.470294933688076\n",
      "Epoch 150, Loss: 0.4697114144973765\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.24050310435081002\n",
      "Valence RMSE: 0.24553703209406055\n",
      "Arousal RMSE: 0.2353615352499076\n",
      "Test R^2 score: tensor([0.3558, 0.5915], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4736116136642342\n",
      "Num of epochs: 151\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Epoch 121, Loss: 0.4816228295571127\n",
      "Epoch 122, Loss: 0.4807618495484584\n",
      "Epoch 123, Loss: 0.48102030846138955\n",
      "Epoch 124, Loss: 0.4810176288364116\n",
      "Epoch 125, Loss: 0.48014338261661127\n",
      "Epoch 126, Loss: 0.47910314809270116\n",
      "Epoch 127, Loss: 0.47880795949683447\n",
      "Epoch 128, Loss: 0.47876177315142754\n",
      "Epoch 129, Loss: 0.47819597116449997\n",
      "Epoch 130, Loss: 0.47743214539542544\n",
      "Epoch 131, Loss: 0.47658476230340185\n",
      "Epoch 132, Loss: 0.4759644680184978\n",
      "Epoch 133, Loss: 0.4755635187712421\n",
      "Epoch 134, Loss: 0.47538264196584307\n",
      "Epoch 135, Loss: 0.4753673450596914\n",
      "Epoch 136, Loss: 0.4753697744168057\n",
      "Epoch 137, Loss: 0.4755973109583155\n",
      "Epoch 138, Loss: 0.4767626826605241\n",
      "Epoch 139, Loss: 0.47683528222287225\n",
      "Epoch 140, Loss: 0.47579445468261566\n",
      "Epoch 141, Loss: 0.47284533919249566\n",
      "Epoch 142, Loss: 0.472809286012082\n",
      "Epoch 143, Loss: 0.47458738111232796\n",
      "Epoch 144, Loss: 0.4733380058918715\n",
      "Epoch 145, Loss: 0.4715229034484008\n",
      "Epoch 146, Loss: 0.47096730966267386\n",
      "Epoch 147, Loss: 0.47161469890467383\n",
      "Epoch 148, Loss: 0.47183531411129226\n",
      "Epoch 149, Loss: 0.470294933688076\n",
      "Epoch 150, Loss: 0.4697114144973765\n",
      "Epoch 151, Loss: 0.47018789053701304\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23942786967821225\n",
      "Valence RMSE: 0.24162166795083573\n",
      "Arousal RMSE: 0.2372137836086893\n",
      "Test R^2 score: tensor([0.3761, 0.5850], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.48057509553842587\n",
      "Num of epochs: 152\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Epoch 121, Loss: 0.4816228295571127\n",
      "Epoch 122, Loss: 0.4807618495484584\n",
      "Epoch 123, Loss: 0.48102030846138955\n",
      "Epoch 124, Loss: 0.4810176288364116\n",
      "Epoch 125, Loss: 0.48014338261661127\n",
      "Epoch 126, Loss: 0.47910314809270116\n",
      "Epoch 127, Loss: 0.47880795949683447\n",
      "Epoch 128, Loss: 0.47876177315142754\n",
      "Epoch 129, Loss: 0.47819597116449997\n",
      "Epoch 130, Loss: 0.47743214539542544\n",
      "Epoch 131, Loss: 0.47658476230340185\n",
      "Epoch 132, Loss: 0.4759644680184978\n",
      "Epoch 133, Loss: 0.4755635187712421\n",
      "Epoch 134, Loss: 0.47538264196584307\n",
      "Epoch 135, Loss: 0.4753673450596914\n",
      "Epoch 136, Loss: 0.4753697744168057\n",
      "Epoch 137, Loss: 0.4755973109583155\n",
      "Epoch 138, Loss: 0.4767626826605241\n",
      "Epoch 139, Loss: 0.47683528222287225\n",
      "Epoch 140, Loss: 0.47579445468261566\n",
      "Epoch 141, Loss: 0.47284533919249566\n",
      "Epoch 142, Loss: 0.472809286012082\n",
      "Epoch 143, Loss: 0.47458738111232796\n",
      "Epoch 144, Loss: 0.4733380058918715\n",
      "Epoch 145, Loss: 0.4715229034484008\n",
      "Epoch 146, Loss: 0.47096730966267386\n",
      "Epoch 147, Loss: 0.47161469890467383\n",
      "Epoch 148, Loss: 0.47183531411129226\n",
      "Epoch 149, Loss: 0.470294933688076\n",
      "Epoch 150, Loss: 0.4697114144973765\n",
      "Epoch 151, Loss: 0.47018789053701304\n",
      "Epoch 152, Loss: 0.4700460953124117\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2399970799387081\n",
      "Valence RMSE: 0.24458155649313412\n",
      "Arousal RMSE: 0.23532330734886664\n",
      "Test R^2 score: tensor([0.3608, 0.5916], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.47618006450957795\n",
      "Num of epochs: 153\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Epoch 121, Loss: 0.4816228295571127\n",
      "Epoch 122, Loss: 0.4807618495484584\n",
      "Epoch 123, Loss: 0.48102030846138955\n",
      "Epoch 124, Loss: 0.4810176288364116\n",
      "Epoch 125, Loss: 0.48014338261661127\n",
      "Epoch 126, Loss: 0.47910314809270116\n",
      "Epoch 127, Loss: 0.47880795949683447\n",
      "Epoch 128, Loss: 0.47876177315142754\n",
      "Epoch 129, Loss: 0.47819597116449997\n",
      "Epoch 130, Loss: 0.47743214539542544\n",
      "Epoch 131, Loss: 0.47658476230340185\n",
      "Epoch 132, Loss: 0.4759644680184978\n",
      "Epoch 133, Loss: 0.4755635187712421\n",
      "Epoch 134, Loss: 0.47538264196584307\n",
      "Epoch 135, Loss: 0.4753673450596914\n",
      "Epoch 136, Loss: 0.4753697744168057\n",
      "Epoch 137, Loss: 0.4755973109583155\n",
      "Epoch 138, Loss: 0.4767626826605241\n",
      "Epoch 139, Loss: 0.47683528222287225\n",
      "Epoch 140, Loss: 0.47579445468261566\n",
      "Epoch 141, Loss: 0.47284533919249566\n",
      "Epoch 142, Loss: 0.472809286012082\n",
      "Epoch 143, Loss: 0.47458738111232796\n",
      "Epoch 144, Loss: 0.4733380058918715\n",
      "Epoch 145, Loss: 0.4715229034484008\n",
      "Epoch 146, Loss: 0.47096730966267386\n",
      "Epoch 147, Loss: 0.47161469890467383\n",
      "Epoch 148, Loss: 0.47183531411129226\n",
      "Epoch 149, Loss: 0.470294933688076\n",
      "Epoch 150, Loss: 0.4697114144973765\n",
      "Epoch 151, Loss: 0.47018789053701304\n",
      "Epoch 152, Loss: 0.4700460953124117\n",
      "Epoch 153, Loss: 0.4693645740016508\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23914842587400603\n",
      "Valence RMSE: 0.24295431450277719\n",
      "Arousal RMSE: 0.235280981510439\n",
      "Test R^2 score: tensor([0.3692, 0.5917], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.48049231285367283\n",
      "Num of epochs: 154\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Epoch 121, Loss: 0.4816228295571127\n",
      "Epoch 122, Loss: 0.4807618495484584\n",
      "Epoch 123, Loss: 0.48102030846138955\n",
      "Epoch 124, Loss: 0.4810176288364116\n",
      "Epoch 125, Loss: 0.48014338261661127\n",
      "Epoch 126, Loss: 0.47910314809270116\n",
      "Epoch 127, Loss: 0.47880795949683447\n",
      "Epoch 128, Loss: 0.47876177315142754\n",
      "Epoch 129, Loss: 0.47819597116449997\n",
      "Epoch 130, Loss: 0.47743214539542544\n",
      "Epoch 131, Loss: 0.47658476230340185\n",
      "Epoch 132, Loss: 0.4759644680184978\n",
      "Epoch 133, Loss: 0.4755635187712421\n",
      "Epoch 134, Loss: 0.47538264196584307\n",
      "Epoch 135, Loss: 0.4753673450596914\n",
      "Epoch 136, Loss: 0.4753697744168057\n",
      "Epoch 137, Loss: 0.4755973109583155\n",
      "Epoch 138, Loss: 0.4767626826605241\n",
      "Epoch 139, Loss: 0.47683528222287225\n",
      "Epoch 140, Loss: 0.47579445468261566\n",
      "Epoch 141, Loss: 0.47284533919249566\n",
      "Epoch 142, Loss: 0.472809286012082\n",
      "Epoch 143, Loss: 0.47458738111232796\n",
      "Epoch 144, Loss: 0.4733380058918715\n",
      "Epoch 145, Loss: 0.4715229034484008\n",
      "Epoch 146, Loss: 0.47096730966267386\n",
      "Epoch 147, Loss: 0.47161469890467383\n",
      "Epoch 148, Loss: 0.47183531411129226\n",
      "Epoch 149, Loss: 0.470294933688076\n",
      "Epoch 150, Loss: 0.4697114144973765\n",
      "Epoch 151, Loss: 0.47018789053701304\n",
      "Epoch 152, Loss: 0.4700460953124117\n",
      "Epoch 153, Loss: 0.4693645740016508\n",
      "Epoch 154, Loss: 0.4684558104180481\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23933387494264882\n",
      "Valence RMSE: 0.2438744987227342\n",
      "Arousal RMSE: 0.23470542444277262\n",
      "Test R^2 score: tensor([0.3645, 0.5937], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4790962815465717\n",
      "Num of epochs: 155\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Epoch 121, Loss: 0.4816228295571127\n",
      "Epoch 122, Loss: 0.4807618495484584\n",
      "Epoch 123, Loss: 0.48102030846138955\n",
      "Epoch 124, Loss: 0.4810176288364116\n",
      "Epoch 125, Loss: 0.48014338261661127\n",
      "Epoch 126, Loss: 0.47910314809270116\n",
      "Epoch 127, Loss: 0.47880795949683447\n",
      "Epoch 128, Loss: 0.47876177315142754\n",
      "Epoch 129, Loss: 0.47819597116449997\n",
      "Epoch 130, Loss: 0.47743214539542544\n",
      "Epoch 131, Loss: 0.47658476230340185\n",
      "Epoch 132, Loss: 0.4759644680184978\n",
      "Epoch 133, Loss: 0.4755635187712421\n",
      "Epoch 134, Loss: 0.47538264196584307\n",
      "Epoch 135, Loss: 0.4753673450596914\n",
      "Epoch 136, Loss: 0.4753697744168057\n",
      "Epoch 137, Loss: 0.4755973109583155\n",
      "Epoch 138, Loss: 0.4767626826605241\n",
      "Epoch 139, Loss: 0.47683528222287225\n",
      "Epoch 140, Loss: 0.47579445468261566\n",
      "Epoch 141, Loss: 0.47284533919249566\n",
      "Epoch 142, Loss: 0.472809286012082\n",
      "Epoch 143, Loss: 0.47458738111232796\n",
      "Epoch 144, Loss: 0.4733380058918715\n",
      "Epoch 145, Loss: 0.4715229034484008\n",
      "Epoch 146, Loss: 0.47096730966267386\n",
      "Epoch 147, Loss: 0.47161469890467383\n",
      "Epoch 148, Loss: 0.47183531411129226\n",
      "Epoch 149, Loss: 0.470294933688076\n",
      "Epoch 150, Loss: 0.4697114144973765\n",
      "Epoch 151, Loss: 0.47018789053701304\n",
      "Epoch 152, Loss: 0.4700460953124117\n",
      "Epoch 153, Loss: 0.4693645740016508\n",
      "Epoch 154, Loss: 0.4684558104180481\n",
      "Epoch 155, Loss: 0.4680960384979934\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.24006616416454576\n",
      "Valence RMSE: 0.2460735270984281\n",
      "Arousal RMSE: 0.2339045651856681\n",
      "Test R^2 score: tensor([0.3529, 0.5965], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4747235734182544\n",
      "Num of epochs: 156\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Epoch 121, Loss: 0.4816228295571127\n",
      "Epoch 122, Loss: 0.4807618495484584\n",
      "Epoch 123, Loss: 0.48102030846138955\n",
      "Epoch 124, Loss: 0.4810176288364116\n",
      "Epoch 125, Loss: 0.48014338261661127\n",
      "Epoch 126, Loss: 0.47910314809270116\n",
      "Epoch 127, Loss: 0.47880795949683447\n",
      "Epoch 128, Loss: 0.47876177315142754\n",
      "Epoch 129, Loss: 0.47819597116449997\n",
      "Epoch 130, Loss: 0.47743214539542544\n",
      "Epoch 131, Loss: 0.47658476230340185\n",
      "Epoch 132, Loss: 0.4759644680184978\n",
      "Epoch 133, Loss: 0.4755635187712421\n",
      "Epoch 134, Loss: 0.47538264196584307\n",
      "Epoch 135, Loss: 0.4753673450596914\n",
      "Epoch 136, Loss: 0.4753697744168057\n",
      "Epoch 137, Loss: 0.4755973109583155\n",
      "Epoch 138, Loss: 0.4767626826605241\n",
      "Epoch 139, Loss: 0.47683528222287225\n",
      "Epoch 140, Loss: 0.47579445468261566\n",
      "Epoch 141, Loss: 0.47284533919249566\n",
      "Epoch 142, Loss: 0.472809286012082\n",
      "Epoch 143, Loss: 0.47458738111232796\n",
      "Epoch 144, Loss: 0.4733380058918715\n",
      "Epoch 145, Loss: 0.4715229034484008\n",
      "Epoch 146, Loss: 0.47096730966267386\n",
      "Epoch 147, Loss: 0.47161469890467383\n",
      "Epoch 148, Loss: 0.47183531411129226\n",
      "Epoch 149, Loss: 0.470294933688076\n",
      "Epoch 150, Loss: 0.4697114144973765\n",
      "Epoch 151, Loss: 0.47018789053701304\n",
      "Epoch 152, Loss: 0.4700460953124117\n",
      "Epoch 153, Loss: 0.4693645740016508\n",
      "Epoch 154, Loss: 0.4684558104180481\n",
      "Epoch 155, Loss: 0.4680960384979934\n",
      "Epoch 156, Loss: 0.4682454254529793\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2389387223762368\n",
      "Valence RMSE: 0.24246296145854854\n",
      "Arousal RMSE: 0.23536171826005853\n",
      "Test R^2 score: tensor([0.3718, 0.5915], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.48162655776156554\n",
      "Num of epochs: 157\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Epoch 121, Loss: 0.4816228295571127\n",
      "Epoch 122, Loss: 0.4807618495484584\n",
      "Epoch 123, Loss: 0.48102030846138955\n",
      "Epoch 124, Loss: 0.4810176288364116\n",
      "Epoch 125, Loss: 0.48014338261661127\n",
      "Epoch 126, Loss: 0.47910314809270116\n",
      "Epoch 127, Loss: 0.47880795949683447\n",
      "Epoch 128, Loss: 0.47876177315142754\n",
      "Epoch 129, Loss: 0.47819597116449997\n",
      "Epoch 130, Loss: 0.47743214539542544\n",
      "Epoch 131, Loss: 0.47658476230340185\n",
      "Epoch 132, Loss: 0.4759644680184978\n",
      "Epoch 133, Loss: 0.4755635187712421\n",
      "Epoch 134, Loss: 0.47538264196584307\n",
      "Epoch 135, Loss: 0.4753673450596914\n",
      "Epoch 136, Loss: 0.4753697744168057\n",
      "Epoch 137, Loss: 0.4755973109583155\n",
      "Epoch 138, Loss: 0.4767626826605241\n",
      "Epoch 139, Loss: 0.47683528222287225\n",
      "Epoch 140, Loss: 0.47579445468261566\n",
      "Epoch 141, Loss: 0.47284533919249566\n",
      "Epoch 142, Loss: 0.472809286012082\n",
      "Epoch 143, Loss: 0.47458738111232796\n",
      "Epoch 144, Loss: 0.4733380058918715\n",
      "Epoch 145, Loss: 0.4715229034484008\n",
      "Epoch 146, Loss: 0.47096730966267386\n",
      "Epoch 147, Loss: 0.47161469890467383\n",
      "Epoch 148, Loss: 0.47183531411129226\n",
      "Epoch 149, Loss: 0.470294933688076\n",
      "Epoch 150, Loss: 0.4697114144973765\n",
      "Epoch 151, Loss: 0.47018789053701304\n",
      "Epoch 152, Loss: 0.4700460953124117\n",
      "Epoch 153, Loss: 0.4693645740016508\n",
      "Epoch 154, Loss: 0.4684558104180481\n",
      "Epoch 155, Loss: 0.4680960384979934\n",
      "Epoch 156, Loss: 0.4682454254529793\n",
      "Epoch 157, Loss: 0.4682974379105488\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23993319095576326\n",
      "Valence RMSE: 0.24572517588530335\n",
      "Arousal RMSE: 0.23399788499165172\n",
      "Test R^2 score: tensor([0.3548, 0.5962], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.47547791454309146\n",
      "Num of epochs: 158\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Epoch 121, Loss: 0.4816228295571127\n",
      "Epoch 122, Loss: 0.4807618495484584\n",
      "Epoch 123, Loss: 0.48102030846138955\n",
      "Epoch 124, Loss: 0.4810176288364116\n",
      "Epoch 125, Loss: 0.48014338261661127\n",
      "Epoch 126, Loss: 0.47910314809270116\n",
      "Epoch 127, Loss: 0.47880795949683447\n",
      "Epoch 128, Loss: 0.47876177315142754\n",
      "Epoch 129, Loss: 0.47819597116449997\n",
      "Epoch 130, Loss: 0.47743214539542544\n",
      "Epoch 131, Loss: 0.47658476230340185\n",
      "Epoch 132, Loss: 0.4759644680184978\n",
      "Epoch 133, Loss: 0.4755635187712421\n",
      "Epoch 134, Loss: 0.47538264196584307\n",
      "Epoch 135, Loss: 0.4753673450596914\n",
      "Epoch 136, Loss: 0.4753697744168057\n",
      "Epoch 137, Loss: 0.4755973109583155\n",
      "Epoch 138, Loss: 0.4767626826605241\n",
      "Epoch 139, Loss: 0.47683528222287225\n",
      "Epoch 140, Loss: 0.47579445468261566\n",
      "Epoch 141, Loss: 0.47284533919249566\n",
      "Epoch 142, Loss: 0.472809286012082\n",
      "Epoch 143, Loss: 0.47458738111232796\n",
      "Epoch 144, Loss: 0.4733380058918715\n",
      "Epoch 145, Loss: 0.4715229034484008\n",
      "Epoch 146, Loss: 0.47096730966267386\n",
      "Epoch 147, Loss: 0.47161469890467383\n",
      "Epoch 148, Loss: 0.47183531411129226\n",
      "Epoch 149, Loss: 0.470294933688076\n",
      "Epoch 150, Loss: 0.4697114144973765\n",
      "Epoch 151, Loss: 0.47018789053701304\n",
      "Epoch 152, Loss: 0.4700460953124117\n",
      "Epoch 153, Loss: 0.4693645740016508\n",
      "Epoch 154, Loss: 0.4684558104180481\n",
      "Epoch 155, Loss: 0.4680960384979934\n",
      "Epoch 156, Loss: 0.4682454254529793\n",
      "Epoch 157, Loss: 0.4682974379105488\n",
      "Epoch 158, Loss: 0.4681590010361841\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2385535648885658\n",
      "Valence RMSE: 0.24266271832425063\n",
      "Arousal RMSE: 0.23437237844415829\n",
      "Test R^2 score: tensor([0.3708, 0.5949], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.48282245395009116\n",
      "Num of epochs: 159\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Epoch 121, Loss: 0.4816228295571127\n",
      "Epoch 122, Loss: 0.4807618495484584\n",
      "Epoch 123, Loss: 0.48102030846138955\n",
      "Epoch 124, Loss: 0.4810176288364116\n",
      "Epoch 125, Loss: 0.48014338261661127\n",
      "Epoch 126, Loss: 0.47910314809270116\n",
      "Epoch 127, Loss: 0.47880795949683447\n",
      "Epoch 128, Loss: 0.47876177315142754\n",
      "Epoch 129, Loss: 0.47819597116449997\n",
      "Epoch 130, Loss: 0.47743214539542544\n",
      "Epoch 131, Loss: 0.47658476230340185\n",
      "Epoch 132, Loss: 0.4759644680184978\n",
      "Epoch 133, Loss: 0.4755635187712421\n",
      "Epoch 134, Loss: 0.47538264196584307\n",
      "Epoch 135, Loss: 0.4753673450596914\n",
      "Epoch 136, Loss: 0.4753697744168057\n",
      "Epoch 137, Loss: 0.4755973109583155\n",
      "Epoch 138, Loss: 0.4767626826605241\n",
      "Epoch 139, Loss: 0.47683528222287225\n",
      "Epoch 140, Loss: 0.47579445468261566\n",
      "Epoch 141, Loss: 0.47284533919249566\n",
      "Epoch 142, Loss: 0.472809286012082\n",
      "Epoch 143, Loss: 0.47458738111232796\n",
      "Epoch 144, Loss: 0.4733380058918715\n",
      "Epoch 145, Loss: 0.4715229034484008\n",
      "Epoch 146, Loss: 0.47096730966267386\n",
      "Epoch 147, Loss: 0.47161469890467383\n",
      "Epoch 148, Loss: 0.47183531411129226\n",
      "Epoch 149, Loss: 0.470294933688076\n",
      "Epoch 150, Loss: 0.4697114144973765\n",
      "Epoch 151, Loss: 0.47018789053701304\n",
      "Epoch 152, Loss: 0.4700460953124117\n",
      "Epoch 153, Loss: 0.4693645740016508\n",
      "Epoch 154, Loss: 0.4684558104180481\n",
      "Epoch 155, Loss: 0.4680960384979934\n",
      "Epoch 156, Loss: 0.4682454254529793\n",
      "Epoch 157, Loss: 0.4682974379105488\n",
      "Epoch 158, Loss: 0.4681590010361841\n",
      "Epoch 159, Loss: 0.4673225124817593\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2390583314684908\n",
      "Valence RMSE: 0.2448675651286919\n",
      "Arousal RMSE: 0.23310436983665372\n",
      "Test R^2 score: tensor([0.3593, 0.5993], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.47926492705894824\n",
      "Num of epochs: 160\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Epoch 121, Loss: 0.4816228295571127\n",
      "Epoch 122, Loss: 0.4807618495484584\n",
      "Epoch 123, Loss: 0.48102030846138955\n",
      "Epoch 124, Loss: 0.4810176288364116\n",
      "Epoch 125, Loss: 0.48014338261661127\n",
      "Epoch 126, Loss: 0.47910314809270116\n",
      "Epoch 127, Loss: 0.47880795949683447\n",
      "Epoch 128, Loss: 0.47876177315142754\n",
      "Epoch 129, Loss: 0.47819597116449997\n",
      "Epoch 130, Loss: 0.47743214539542544\n",
      "Epoch 131, Loss: 0.47658476230340185\n",
      "Epoch 132, Loss: 0.4759644680184978\n",
      "Epoch 133, Loss: 0.4755635187712421\n",
      "Epoch 134, Loss: 0.47538264196584307\n",
      "Epoch 135, Loss: 0.4753673450596914\n",
      "Epoch 136, Loss: 0.4753697744168057\n",
      "Epoch 137, Loss: 0.4755973109583155\n",
      "Epoch 138, Loss: 0.4767626826605241\n",
      "Epoch 139, Loss: 0.47683528222287225\n",
      "Epoch 140, Loss: 0.47579445468261566\n",
      "Epoch 141, Loss: 0.47284533919249566\n",
      "Epoch 142, Loss: 0.472809286012082\n",
      "Epoch 143, Loss: 0.47458738111232796\n",
      "Epoch 144, Loss: 0.4733380058918715\n",
      "Epoch 145, Loss: 0.4715229034484008\n",
      "Epoch 146, Loss: 0.47096730966267386\n",
      "Epoch 147, Loss: 0.47161469890467383\n",
      "Epoch 148, Loss: 0.47183531411129226\n",
      "Epoch 149, Loss: 0.470294933688076\n",
      "Epoch 150, Loss: 0.4697114144973765\n",
      "Epoch 151, Loss: 0.47018789053701304\n",
      "Epoch 152, Loss: 0.4700460953124117\n",
      "Epoch 153, Loss: 0.4693645740016508\n",
      "Epoch 154, Loss: 0.4684558104180481\n",
      "Epoch 155, Loss: 0.4680960384979934\n",
      "Epoch 156, Loss: 0.4682454254529793\n",
      "Epoch 157, Loss: 0.4682974379105488\n",
      "Epoch 158, Loss: 0.4681590010361841\n",
      "Epoch 159, Loss: 0.4673225124817593\n",
      "Epoch 160, Loss: 0.4666701428107949\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2391374714032394\n",
      "Valence RMSE: 0.24492078241728918\n",
      "Arousal RMSE: 0.23321078619646468\n",
      "Test R^2 score: tensor([0.3590, 0.5989], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.478942675151696\n",
      "Num of epochs: 161\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Epoch 121, Loss: 0.4816228295571127\n",
      "Epoch 122, Loss: 0.4807618495484584\n",
      "Epoch 123, Loss: 0.48102030846138955\n",
      "Epoch 124, Loss: 0.4810176288364116\n",
      "Epoch 125, Loss: 0.48014338261661127\n",
      "Epoch 126, Loss: 0.47910314809270116\n",
      "Epoch 127, Loss: 0.47880795949683447\n",
      "Epoch 128, Loss: 0.47876177315142754\n",
      "Epoch 129, Loss: 0.47819597116449997\n",
      "Epoch 130, Loss: 0.47743214539542544\n",
      "Epoch 131, Loss: 0.47658476230340185\n",
      "Epoch 132, Loss: 0.4759644680184978\n",
      "Epoch 133, Loss: 0.4755635187712421\n",
      "Epoch 134, Loss: 0.47538264196584307\n",
      "Epoch 135, Loss: 0.4753673450596914\n",
      "Epoch 136, Loss: 0.4753697744168057\n",
      "Epoch 137, Loss: 0.4755973109583155\n",
      "Epoch 138, Loss: 0.4767626826605241\n",
      "Epoch 139, Loss: 0.47683528222287225\n",
      "Epoch 140, Loss: 0.47579445468261566\n",
      "Epoch 141, Loss: 0.47284533919249566\n",
      "Epoch 142, Loss: 0.472809286012082\n",
      "Epoch 143, Loss: 0.47458738111232796\n",
      "Epoch 144, Loss: 0.4733380058918715\n",
      "Epoch 145, Loss: 0.4715229034484008\n",
      "Epoch 146, Loss: 0.47096730966267386\n",
      "Epoch 147, Loss: 0.47161469890467383\n",
      "Epoch 148, Loss: 0.47183531411129226\n",
      "Epoch 149, Loss: 0.470294933688076\n",
      "Epoch 150, Loss: 0.4697114144973765\n",
      "Epoch 151, Loss: 0.47018789053701304\n",
      "Epoch 152, Loss: 0.4700460953124117\n",
      "Epoch 153, Loss: 0.4693645740016508\n",
      "Epoch 154, Loss: 0.4684558104180481\n",
      "Epoch 155, Loss: 0.4680960384979934\n",
      "Epoch 156, Loss: 0.4682454254529793\n",
      "Epoch 157, Loss: 0.4682974379105488\n",
      "Epoch 158, Loss: 0.4681590010361841\n",
      "Epoch 159, Loss: 0.4673225124817593\n",
      "Epoch 160, Loss: 0.4666701428107949\n",
      "Epoch 161, Loss: 0.4663166784414499\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2389566143307587\n",
      "Valence RMSE: 0.24359059264872832\n",
      "Arousal RMSE: 0.23423097625608616\n",
      "Test R^2 score: tensor([0.3659, 0.5954], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4806561330210348\n",
      "Num of epochs: 162\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Epoch 121, Loss: 0.4816228295571127\n",
      "Epoch 122, Loss: 0.4807618495484584\n",
      "Epoch 123, Loss: 0.48102030846138955\n",
      "Epoch 124, Loss: 0.4810176288364116\n",
      "Epoch 125, Loss: 0.48014338261661127\n",
      "Epoch 126, Loss: 0.47910314809270116\n",
      "Epoch 127, Loss: 0.47880795949683447\n",
      "Epoch 128, Loss: 0.47876177315142754\n",
      "Epoch 129, Loss: 0.47819597116449997\n",
      "Epoch 130, Loss: 0.47743214539542544\n",
      "Epoch 131, Loss: 0.47658476230340185\n",
      "Epoch 132, Loss: 0.4759644680184978\n",
      "Epoch 133, Loss: 0.4755635187712421\n",
      "Epoch 134, Loss: 0.47538264196584307\n",
      "Epoch 135, Loss: 0.4753673450596914\n",
      "Epoch 136, Loss: 0.4753697744168057\n",
      "Epoch 137, Loss: 0.4755973109583155\n",
      "Epoch 138, Loss: 0.4767626826605241\n",
      "Epoch 139, Loss: 0.47683528222287225\n",
      "Epoch 140, Loss: 0.47579445468261566\n",
      "Epoch 141, Loss: 0.47284533919249566\n",
      "Epoch 142, Loss: 0.472809286012082\n",
      "Epoch 143, Loss: 0.47458738111232796\n",
      "Epoch 144, Loss: 0.4733380058918715\n",
      "Epoch 145, Loss: 0.4715229034484008\n",
      "Epoch 146, Loss: 0.47096730966267386\n",
      "Epoch 147, Loss: 0.47161469890467383\n",
      "Epoch 148, Loss: 0.47183531411129226\n",
      "Epoch 149, Loss: 0.470294933688076\n",
      "Epoch 150, Loss: 0.4697114144973765\n",
      "Epoch 151, Loss: 0.47018789053701304\n",
      "Epoch 152, Loss: 0.4700460953124117\n",
      "Epoch 153, Loss: 0.4693645740016508\n",
      "Epoch 154, Loss: 0.4684558104180481\n",
      "Epoch 155, Loss: 0.4680960384979934\n",
      "Epoch 156, Loss: 0.4682454254529793\n",
      "Epoch 157, Loss: 0.4682974379105488\n",
      "Epoch 158, Loss: 0.4681590010361841\n",
      "Epoch 159, Loss: 0.4673225124817593\n",
      "Epoch 160, Loss: 0.4666701428107949\n",
      "Epoch 161, Loss: 0.4663166784414499\n",
      "Epoch 162, Loss: 0.46624854535037\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.24014002107212193\n",
      "Valence RMSE: 0.2463665841716633\n",
      "Arousal RMSE: 0.23374765377351941\n",
      "Test R^2 score: tensor([0.3514, 0.5970], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.47422309787387584\n",
      "Num of epochs: 163\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Epoch 121, Loss: 0.4816228295571127\n",
      "Epoch 122, Loss: 0.4807618495484584\n",
      "Epoch 123, Loss: 0.48102030846138955\n",
      "Epoch 124, Loss: 0.4810176288364116\n",
      "Epoch 125, Loss: 0.48014338261661127\n",
      "Epoch 126, Loss: 0.47910314809270116\n",
      "Epoch 127, Loss: 0.47880795949683447\n",
      "Epoch 128, Loss: 0.47876177315142754\n",
      "Epoch 129, Loss: 0.47819597116449997\n",
      "Epoch 130, Loss: 0.47743214539542544\n",
      "Epoch 131, Loss: 0.47658476230340185\n",
      "Epoch 132, Loss: 0.4759644680184978\n",
      "Epoch 133, Loss: 0.4755635187712421\n",
      "Epoch 134, Loss: 0.47538264196584307\n",
      "Epoch 135, Loss: 0.4753673450596914\n",
      "Epoch 136, Loss: 0.4753697744168057\n",
      "Epoch 137, Loss: 0.4755973109583155\n",
      "Epoch 138, Loss: 0.4767626826605241\n",
      "Epoch 139, Loss: 0.47683528222287225\n",
      "Epoch 140, Loss: 0.47579445468261566\n",
      "Epoch 141, Loss: 0.47284533919249566\n",
      "Epoch 142, Loss: 0.472809286012082\n",
      "Epoch 143, Loss: 0.47458738111232796\n",
      "Epoch 144, Loss: 0.4733380058918715\n",
      "Epoch 145, Loss: 0.4715229034484008\n",
      "Epoch 146, Loss: 0.47096730966267386\n",
      "Epoch 147, Loss: 0.47161469890467383\n",
      "Epoch 148, Loss: 0.47183531411129226\n",
      "Epoch 149, Loss: 0.470294933688076\n",
      "Epoch 150, Loss: 0.4697114144973765\n",
      "Epoch 151, Loss: 0.47018789053701304\n",
      "Epoch 152, Loss: 0.4700460953124117\n",
      "Epoch 153, Loss: 0.4693645740016508\n",
      "Epoch 154, Loss: 0.4684558104180481\n",
      "Epoch 155, Loss: 0.4680960384979934\n",
      "Epoch 156, Loss: 0.4682454254529793\n",
      "Epoch 157, Loss: 0.4682974379105488\n",
      "Epoch 158, Loss: 0.4681590010361841\n",
      "Epoch 159, Loss: 0.4673225124817593\n",
      "Epoch 160, Loss: 0.4666701428107949\n",
      "Epoch 161, Loss: 0.4663166784414499\n",
      "Epoch 162, Loss: 0.46624854535037\n",
      "Epoch 163, Loss: 0.4664611566834947\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23916506444721553\n",
      "Valence RMSE: 0.24292778830495138\n",
      "Arousal RMSE: 0.23534218868138848\n",
      "Test R^2 score: tensor([0.3694, 0.5915], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4804549567550882\n",
      "Num of epochs: 164\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Epoch 121, Loss: 0.4816228295571127\n",
      "Epoch 122, Loss: 0.4807618495484584\n",
      "Epoch 123, Loss: 0.48102030846138955\n",
      "Epoch 124, Loss: 0.4810176288364116\n",
      "Epoch 125, Loss: 0.48014338261661127\n",
      "Epoch 126, Loss: 0.47910314809270116\n",
      "Epoch 127, Loss: 0.47880795949683447\n",
      "Epoch 128, Loss: 0.47876177315142754\n",
      "Epoch 129, Loss: 0.47819597116449997\n",
      "Epoch 130, Loss: 0.47743214539542544\n",
      "Epoch 131, Loss: 0.47658476230340185\n",
      "Epoch 132, Loss: 0.4759644680184978\n",
      "Epoch 133, Loss: 0.4755635187712421\n",
      "Epoch 134, Loss: 0.47538264196584307\n",
      "Epoch 135, Loss: 0.4753673450596914\n",
      "Epoch 136, Loss: 0.4753697744168057\n",
      "Epoch 137, Loss: 0.4755973109583155\n",
      "Epoch 138, Loss: 0.4767626826605241\n",
      "Epoch 139, Loss: 0.47683528222287225\n",
      "Epoch 140, Loss: 0.47579445468261566\n",
      "Epoch 141, Loss: 0.47284533919249566\n",
      "Epoch 142, Loss: 0.472809286012082\n",
      "Epoch 143, Loss: 0.47458738111232796\n",
      "Epoch 144, Loss: 0.4733380058918715\n",
      "Epoch 145, Loss: 0.4715229034484008\n",
      "Epoch 146, Loss: 0.47096730966267386\n",
      "Epoch 147, Loss: 0.47161469890467383\n",
      "Epoch 148, Loss: 0.47183531411129226\n",
      "Epoch 149, Loss: 0.470294933688076\n",
      "Epoch 150, Loss: 0.4697114144973765\n",
      "Epoch 151, Loss: 0.47018789053701304\n",
      "Epoch 152, Loss: 0.4700460953124117\n",
      "Epoch 153, Loss: 0.4693645740016508\n",
      "Epoch 154, Loss: 0.4684558104180481\n",
      "Epoch 155, Loss: 0.4680960384979934\n",
      "Epoch 156, Loss: 0.4682454254529793\n",
      "Epoch 157, Loss: 0.4682974379105488\n",
      "Epoch 158, Loss: 0.4681590010361841\n",
      "Epoch 159, Loss: 0.4673225124817593\n",
      "Epoch 160, Loss: 0.4666701428107949\n",
      "Epoch 161, Loss: 0.4663166784414499\n",
      "Epoch 162, Loss: 0.46624854535037\n",
      "Epoch 163, Loss: 0.4664611566834947\n",
      "Epoch 164, Loss: 0.46654532450127006\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.24133955137979035\n",
      "Valence RMSE: 0.24859264607529624\n",
      "Arousal RMSE: 0.23386161386101706\n",
      "Test R^2 score: tensor([0.3396, 0.5967], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4681396415422097\n",
      "Num of epochs: 165\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Epoch 121, Loss: 0.4816228295571127\n",
      "Epoch 122, Loss: 0.4807618495484584\n",
      "Epoch 123, Loss: 0.48102030846138955\n",
      "Epoch 124, Loss: 0.4810176288364116\n",
      "Epoch 125, Loss: 0.48014338261661127\n",
      "Epoch 126, Loss: 0.47910314809270116\n",
      "Epoch 127, Loss: 0.47880795949683447\n",
      "Epoch 128, Loss: 0.47876177315142754\n",
      "Epoch 129, Loss: 0.47819597116449997\n",
      "Epoch 130, Loss: 0.47743214539542544\n",
      "Epoch 131, Loss: 0.47658476230340185\n",
      "Epoch 132, Loss: 0.4759644680184978\n",
      "Epoch 133, Loss: 0.4755635187712421\n",
      "Epoch 134, Loss: 0.47538264196584307\n",
      "Epoch 135, Loss: 0.4753673450596914\n",
      "Epoch 136, Loss: 0.4753697744168057\n",
      "Epoch 137, Loss: 0.4755973109583155\n",
      "Epoch 138, Loss: 0.4767626826605241\n",
      "Epoch 139, Loss: 0.47683528222287225\n",
      "Epoch 140, Loss: 0.47579445468261566\n",
      "Epoch 141, Loss: 0.47284533919249566\n",
      "Epoch 142, Loss: 0.472809286012082\n",
      "Epoch 143, Loss: 0.47458738111232796\n",
      "Epoch 144, Loss: 0.4733380058918715\n",
      "Epoch 145, Loss: 0.4715229034484008\n",
      "Epoch 146, Loss: 0.47096730966267386\n",
      "Epoch 147, Loss: 0.47161469890467383\n",
      "Epoch 148, Loss: 0.47183531411129226\n",
      "Epoch 149, Loss: 0.470294933688076\n",
      "Epoch 150, Loss: 0.4697114144973765\n",
      "Epoch 151, Loss: 0.47018789053701304\n",
      "Epoch 152, Loss: 0.4700460953124117\n",
      "Epoch 153, Loss: 0.4693645740016508\n",
      "Epoch 154, Loss: 0.4684558104180481\n",
      "Epoch 155, Loss: 0.4680960384979934\n",
      "Epoch 156, Loss: 0.4682454254529793\n",
      "Epoch 157, Loss: 0.4682974379105488\n",
      "Epoch 158, Loss: 0.4681590010361841\n",
      "Epoch 159, Loss: 0.4673225124817593\n",
      "Epoch 160, Loss: 0.4666701428107949\n",
      "Epoch 161, Loss: 0.4663166784414499\n",
      "Epoch 162, Loss: 0.46624854535037\n",
      "Epoch 163, Loss: 0.4664611566834947\n",
      "Epoch 164, Loss: 0.46654532450127006\n",
      "Epoch 165, Loss: 0.46721845607201035\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23942375780613565\n",
      "Valence RMSE: 0.24278671643843974\n",
      "Arousal RMSE: 0.23601288508270657\n",
      "Test R^2 score: tensor([0.3701, 0.5892], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4796553160960763\n",
      "Num of epochs: 166\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Epoch 121, Loss: 0.4816228295571127\n",
      "Epoch 122, Loss: 0.4807618495484584\n",
      "Epoch 123, Loss: 0.48102030846138955\n",
      "Epoch 124, Loss: 0.4810176288364116\n",
      "Epoch 125, Loss: 0.48014338261661127\n",
      "Epoch 126, Loss: 0.47910314809270116\n",
      "Epoch 127, Loss: 0.47880795949683447\n",
      "Epoch 128, Loss: 0.47876177315142754\n",
      "Epoch 129, Loss: 0.47819597116449997\n",
      "Epoch 130, Loss: 0.47743214539542544\n",
      "Epoch 131, Loss: 0.47658476230340185\n",
      "Epoch 132, Loss: 0.4759644680184978\n",
      "Epoch 133, Loss: 0.4755635187712421\n",
      "Epoch 134, Loss: 0.47538264196584307\n",
      "Epoch 135, Loss: 0.4753673450596914\n",
      "Epoch 136, Loss: 0.4753697744168057\n",
      "Epoch 137, Loss: 0.4755973109583155\n",
      "Epoch 138, Loss: 0.4767626826605241\n",
      "Epoch 139, Loss: 0.47683528222287225\n",
      "Epoch 140, Loss: 0.47579445468261566\n",
      "Epoch 141, Loss: 0.47284533919249566\n",
      "Epoch 142, Loss: 0.472809286012082\n",
      "Epoch 143, Loss: 0.47458738111232796\n",
      "Epoch 144, Loss: 0.4733380058918715\n",
      "Epoch 145, Loss: 0.4715229034484008\n",
      "Epoch 146, Loss: 0.47096730966267386\n",
      "Epoch 147, Loss: 0.47161469890467383\n",
      "Epoch 148, Loss: 0.47183531411129226\n",
      "Epoch 149, Loss: 0.470294933688076\n",
      "Epoch 150, Loss: 0.4697114144973765\n",
      "Epoch 151, Loss: 0.47018789053701304\n",
      "Epoch 152, Loss: 0.4700460953124117\n",
      "Epoch 153, Loss: 0.4693645740016508\n",
      "Epoch 154, Loss: 0.4684558104180481\n",
      "Epoch 155, Loss: 0.4680960384979934\n",
      "Epoch 156, Loss: 0.4682454254529793\n",
      "Epoch 157, Loss: 0.4682974379105488\n",
      "Epoch 158, Loss: 0.4681590010361841\n",
      "Epoch 159, Loss: 0.4673225124817593\n",
      "Epoch 160, Loss: 0.4666701428107949\n",
      "Epoch 161, Loss: 0.4663166784414499\n",
      "Epoch 162, Loss: 0.46624854535037\n",
      "Epoch 163, Loss: 0.4664611566834947\n",
      "Epoch 164, Loss: 0.46654532450127006\n",
      "Epoch 165, Loss: 0.46721845607201035\n",
      "Epoch 166, Loss: 0.4668208160006968\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.24057464800172124\n",
      "Valence RMSE: 0.24799301763788967\n",
      "Arousal RMSE: 0.23292012735089576\n",
      "Test R^2 score: tensor([0.3428, 0.5999], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.47135113773431137\n",
      "Num of epochs: 167\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Epoch 121, Loss: 0.4816228295571127\n",
      "Epoch 122, Loss: 0.4807618495484584\n",
      "Epoch 123, Loss: 0.48102030846138955\n",
      "Epoch 124, Loss: 0.4810176288364116\n",
      "Epoch 125, Loss: 0.48014338261661127\n",
      "Epoch 126, Loss: 0.47910314809270116\n",
      "Epoch 127, Loss: 0.47880795949683447\n",
      "Epoch 128, Loss: 0.47876177315142754\n",
      "Epoch 129, Loss: 0.47819597116449997\n",
      "Epoch 130, Loss: 0.47743214539542544\n",
      "Epoch 131, Loss: 0.47658476230340185\n",
      "Epoch 132, Loss: 0.4759644680184978\n",
      "Epoch 133, Loss: 0.4755635187712421\n",
      "Epoch 134, Loss: 0.47538264196584307\n",
      "Epoch 135, Loss: 0.4753673450596914\n",
      "Epoch 136, Loss: 0.4753697744168057\n",
      "Epoch 137, Loss: 0.4755973109583155\n",
      "Epoch 138, Loss: 0.4767626826605241\n",
      "Epoch 139, Loss: 0.47683528222287225\n",
      "Epoch 140, Loss: 0.47579445468261566\n",
      "Epoch 141, Loss: 0.47284533919249566\n",
      "Epoch 142, Loss: 0.472809286012082\n",
      "Epoch 143, Loss: 0.47458738111232796\n",
      "Epoch 144, Loss: 0.4733380058918715\n",
      "Epoch 145, Loss: 0.4715229034484008\n",
      "Epoch 146, Loss: 0.47096730966267386\n",
      "Epoch 147, Loss: 0.47161469890467383\n",
      "Epoch 148, Loss: 0.47183531411129226\n",
      "Epoch 149, Loss: 0.470294933688076\n",
      "Epoch 150, Loss: 0.4697114144973765\n",
      "Epoch 151, Loss: 0.47018789053701304\n",
      "Epoch 152, Loss: 0.4700460953124117\n",
      "Epoch 153, Loss: 0.4693645740016508\n",
      "Epoch 154, Loss: 0.4684558104180481\n",
      "Epoch 155, Loss: 0.4680960384979934\n",
      "Epoch 156, Loss: 0.4682454254529793\n",
      "Epoch 157, Loss: 0.4682974379105488\n",
      "Epoch 158, Loss: 0.4681590010361841\n",
      "Epoch 159, Loss: 0.4673225124817593\n",
      "Epoch 160, Loss: 0.4666701428107949\n",
      "Epoch 161, Loss: 0.4663166784414499\n",
      "Epoch 162, Loss: 0.46624854535037\n",
      "Epoch 163, Loss: 0.4664611566834947\n",
      "Epoch 164, Loss: 0.46654532450127006\n",
      "Epoch 165, Loss: 0.46721845607201035\n",
      "Epoch 166, Loss: 0.4668208160006968\n",
      "Epoch 167, Loss: 0.466573749669782\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2383737987085245\n",
      "Valence RMSE: 0.24289478795607058\n",
      "Arousal RMSE: 0.23376539052058112\n",
      "Test R^2 score: tensor([0.3696, 0.5970], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4832682013535013\n",
      "Num of epochs: 168\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Epoch 121, Loss: 0.4816228295571127\n",
      "Epoch 122, Loss: 0.4807618495484584\n",
      "Epoch 123, Loss: 0.48102030846138955\n",
      "Epoch 124, Loss: 0.4810176288364116\n",
      "Epoch 125, Loss: 0.48014338261661127\n",
      "Epoch 126, Loss: 0.47910314809270116\n",
      "Epoch 127, Loss: 0.47880795949683447\n",
      "Epoch 128, Loss: 0.47876177315142754\n",
      "Epoch 129, Loss: 0.47819597116449997\n",
      "Epoch 130, Loss: 0.47743214539542544\n",
      "Epoch 131, Loss: 0.47658476230340185\n",
      "Epoch 132, Loss: 0.4759644680184978\n",
      "Epoch 133, Loss: 0.4755635187712421\n",
      "Epoch 134, Loss: 0.47538264196584307\n",
      "Epoch 135, Loss: 0.4753673450596914\n",
      "Epoch 136, Loss: 0.4753697744168057\n",
      "Epoch 137, Loss: 0.4755973109583155\n",
      "Epoch 138, Loss: 0.4767626826605241\n",
      "Epoch 139, Loss: 0.47683528222287225\n",
      "Epoch 140, Loss: 0.47579445468261566\n",
      "Epoch 141, Loss: 0.47284533919249566\n",
      "Epoch 142, Loss: 0.472809286012082\n",
      "Epoch 143, Loss: 0.47458738111232796\n",
      "Epoch 144, Loss: 0.4733380058918715\n",
      "Epoch 145, Loss: 0.4715229034484008\n",
      "Epoch 146, Loss: 0.47096730966267386\n",
      "Epoch 147, Loss: 0.47161469890467383\n",
      "Epoch 148, Loss: 0.47183531411129226\n",
      "Epoch 149, Loss: 0.470294933688076\n",
      "Epoch 150, Loss: 0.4697114144973765\n",
      "Epoch 151, Loss: 0.47018789053701304\n",
      "Epoch 152, Loss: 0.4700460953124117\n",
      "Epoch 153, Loss: 0.4693645740016508\n",
      "Epoch 154, Loss: 0.4684558104180481\n",
      "Epoch 155, Loss: 0.4680960384979934\n",
      "Epoch 156, Loss: 0.4682454254529793\n",
      "Epoch 157, Loss: 0.4682974379105488\n",
      "Epoch 158, Loss: 0.4681590010361841\n",
      "Epoch 159, Loss: 0.4673225124817593\n",
      "Epoch 160, Loss: 0.4666701428107949\n",
      "Epoch 161, Loss: 0.4663166784414499\n",
      "Epoch 162, Loss: 0.46624854535037\n",
      "Epoch 163, Loss: 0.4664611566834947\n",
      "Epoch 164, Loss: 0.46654532450127006\n",
      "Epoch 165, Loss: 0.46721845607201035\n",
      "Epoch 166, Loss: 0.4668208160006968\n",
      "Epoch 167, Loss: 0.466573749669782\n",
      "Epoch 168, Loss: 0.46506387643568103\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23845970138148248\n",
      "Valence RMSE: 0.24422429671252813\n",
      "Arousal RMSE: 0.2325522549044894\n",
      "Test R^2 score: tensor([0.3626, 0.6012], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4818939593210123\n",
      "Num of epochs: 169\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Epoch 121, Loss: 0.4816228295571127\n",
      "Epoch 122, Loss: 0.4807618495484584\n",
      "Epoch 123, Loss: 0.48102030846138955\n",
      "Epoch 124, Loss: 0.4810176288364116\n",
      "Epoch 125, Loss: 0.48014338261661127\n",
      "Epoch 126, Loss: 0.47910314809270116\n",
      "Epoch 127, Loss: 0.47880795949683447\n",
      "Epoch 128, Loss: 0.47876177315142754\n",
      "Epoch 129, Loss: 0.47819597116449997\n",
      "Epoch 130, Loss: 0.47743214539542544\n",
      "Epoch 131, Loss: 0.47658476230340185\n",
      "Epoch 132, Loss: 0.4759644680184978\n",
      "Epoch 133, Loss: 0.4755635187712421\n",
      "Epoch 134, Loss: 0.47538264196584307\n",
      "Epoch 135, Loss: 0.4753673450596914\n",
      "Epoch 136, Loss: 0.4753697744168057\n",
      "Epoch 137, Loss: 0.4755973109583155\n",
      "Epoch 138, Loss: 0.4767626826605241\n",
      "Epoch 139, Loss: 0.47683528222287225\n",
      "Epoch 140, Loss: 0.47579445468261566\n",
      "Epoch 141, Loss: 0.47284533919249566\n",
      "Epoch 142, Loss: 0.472809286012082\n",
      "Epoch 143, Loss: 0.47458738111232796\n",
      "Epoch 144, Loss: 0.4733380058918715\n",
      "Epoch 145, Loss: 0.4715229034484008\n",
      "Epoch 146, Loss: 0.47096730966267386\n",
      "Epoch 147, Loss: 0.47161469890467383\n",
      "Epoch 148, Loss: 0.47183531411129226\n",
      "Epoch 149, Loss: 0.470294933688076\n",
      "Epoch 150, Loss: 0.4697114144973765\n",
      "Epoch 151, Loss: 0.47018789053701304\n",
      "Epoch 152, Loss: 0.4700460953124117\n",
      "Epoch 153, Loss: 0.4693645740016508\n",
      "Epoch 154, Loss: 0.4684558104180481\n",
      "Epoch 155, Loss: 0.4680960384979934\n",
      "Epoch 156, Loss: 0.4682454254529793\n",
      "Epoch 157, Loss: 0.4682974379105488\n",
      "Epoch 158, Loss: 0.4681590010361841\n",
      "Epoch 159, Loss: 0.4673225124817593\n",
      "Epoch 160, Loss: 0.4666701428107949\n",
      "Epoch 161, Loss: 0.4663166784414499\n",
      "Epoch 162, Loss: 0.46624854535037\n",
      "Epoch 163, Loss: 0.4664611566834947\n",
      "Epoch 164, Loss: 0.46654532450127006\n",
      "Epoch 165, Loss: 0.46721845607201035\n",
      "Epoch 166, Loss: 0.4668208160006968\n",
      "Epoch 167, Loss: 0.466573749669782\n",
      "Epoch 168, Loss: 0.46506387643568103\n",
      "Epoch 169, Loss: 0.4641220516950567\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.239988584045657\n",
      "Valence RMSE: 0.247069344358974\n",
      "Arousal RMSE: 0.23269245802669558\n",
      "Test R^2 score: tensor([0.3477, 0.6007], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.47418525026668185\n",
      "Num of epochs: 170\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Epoch 121, Loss: 0.4816228295571127\n",
      "Epoch 122, Loss: 0.4807618495484584\n",
      "Epoch 123, Loss: 0.48102030846138955\n",
      "Epoch 124, Loss: 0.4810176288364116\n",
      "Epoch 125, Loss: 0.48014338261661127\n",
      "Epoch 126, Loss: 0.47910314809270116\n",
      "Epoch 127, Loss: 0.47880795949683447\n",
      "Epoch 128, Loss: 0.47876177315142754\n",
      "Epoch 129, Loss: 0.47819597116449997\n",
      "Epoch 130, Loss: 0.47743214539542544\n",
      "Epoch 131, Loss: 0.47658476230340185\n",
      "Epoch 132, Loss: 0.4759644680184978\n",
      "Epoch 133, Loss: 0.4755635187712421\n",
      "Epoch 134, Loss: 0.47538264196584307\n",
      "Epoch 135, Loss: 0.4753673450596914\n",
      "Epoch 136, Loss: 0.4753697744168057\n",
      "Epoch 137, Loss: 0.4755973109583155\n",
      "Epoch 138, Loss: 0.4767626826605241\n",
      "Epoch 139, Loss: 0.47683528222287225\n",
      "Epoch 140, Loss: 0.47579445468261566\n",
      "Epoch 141, Loss: 0.47284533919249566\n",
      "Epoch 142, Loss: 0.472809286012082\n",
      "Epoch 143, Loss: 0.47458738111232796\n",
      "Epoch 144, Loss: 0.4733380058918715\n",
      "Epoch 145, Loss: 0.4715229034484008\n",
      "Epoch 146, Loss: 0.47096730966267386\n",
      "Epoch 147, Loss: 0.47161469890467383\n",
      "Epoch 148, Loss: 0.47183531411129226\n",
      "Epoch 149, Loss: 0.470294933688076\n",
      "Epoch 150, Loss: 0.4697114144973765\n",
      "Epoch 151, Loss: 0.47018789053701304\n",
      "Epoch 152, Loss: 0.4700460953124117\n",
      "Epoch 153, Loss: 0.4693645740016508\n",
      "Epoch 154, Loss: 0.4684558104180481\n",
      "Epoch 155, Loss: 0.4680960384979934\n",
      "Epoch 156, Loss: 0.4682454254529793\n",
      "Epoch 157, Loss: 0.4682974379105488\n",
      "Epoch 158, Loss: 0.4681590010361841\n",
      "Epoch 159, Loss: 0.4673225124817593\n",
      "Epoch 160, Loss: 0.4666701428107949\n",
      "Epoch 161, Loss: 0.4663166784414499\n",
      "Epoch 162, Loss: 0.46624854535037\n",
      "Epoch 163, Loss: 0.4664611566834947\n",
      "Epoch 164, Loss: 0.46654532450127006\n",
      "Epoch 165, Loss: 0.46721845607201035\n",
      "Epoch 166, Loss: 0.4668208160006968\n",
      "Epoch 167, Loss: 0.466573749669782\n",
      "Epoch 168, Loss: 0.46506387643568103\n",
      "Epoch 169, Loss: 0.4641220516950567\n",
      "Epoch 170, Loss: 0.4644550172656552\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.24005337029212795\n",
      "Valence RMSE: 0.24425039769690604\n",
      "Arousal RMSE: 0.2357816456005901\n",
      "Test R^2 score: tensor([0.3625, 0.5900], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.476248771919127\n",
      "Num of epochs: 171\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Epoch 121, Loss: 0.4816228295571127\n",
      "Epoch 122, Loss: 0.4807618495484584\n",
      "Epoch 123, Loss: 0.48102030846138955\n",
      "Epoch 124, Loss: 0.4810176288364116\n",
      "Epoch 125, Loss: 0.48014338261661127\n",
      "Epoch 126, Loss: 0.47910314809270116\n",
      "Epoch 127, Loss: 0.47880795949683447\n",
      "Epoch 128, Loss: 0.47876177315142754\n",
      "Epoch 129, Loss: 0.47819597116449997\n",
      "Epoch 130, Loss: 0.47743214539542544\n",
      "Epoch 131, Loss: 0.47658476230340185\n",
      "Epoch 132, Loss: 0.4759644680184978\n",
      "Epoch 133, Loss: 0.4755635187712421\n",
      "Epoch 134, Loss: 0.47538264196584307\n",
      "Epoch 135, Loss: 0.4753673450596914\n",
      "Epoch 136, Loss: 0.4753697744168057\n",
      "Epoch 137, Loss: 0.4755973109583155\n",
      "Epoch 138, Loss: 0.4767626826605241\n",
      "Epoch 139, Loss: 0.47683528222287225\n",
      "Epoch 140, Loss: 0.47579445468261566\n",
      "Epoch 141, Loss: 0.47284533919249566\n",
      "Epoch 142, Loss: 0.472809286012082\n",
      "Epoch 143, Loss: 0.47458738111232796\n",
      "Epoch 144, Loss: 0.4733380058918715\n",
      "Epoch 145, Loss: 0.4715229034484008\n",
      "Epoch 146, Loss: 0.47096730966267386\n",
      "Epoch 147, Loss: 0.47161469890467383\n",
      "Epoch 148, Loss: 0.47183531411129226\n",
      "Epoch 149, Loss: 0.470294933688076\n",
      "Epoch 150, Loss: 0.4697114144973765\n",
      "Epoch 151, Loss: 0.47018789053701304\n",
      "Epoch 152, Loss: 0.4700460953124117\n",
      "Epoch 153, Loss: 0.4693645740016508\n",
      "Epoch 154, Loss: 0.4684558104180481\n",
      "Epoch 155, Loss: 0.4680960384979934\n",
      "Epoch 156, Loss: 0.4682454254529793\n",
      "Epoch 157, Loss: 0.4682974379105488\n",
      "Epoch 158, Loss: 0.4681590010361841\n",
      "Epoch 159, Loss: 0.4673225124817593\n",
      "Epoch 160, Loss: 0.4666701428107949\n",
      "Epoch 161, Loss: 0.4663166784414499\n",
      "Epoch 162, Loss: 0.46624854535037\n",
      "Epoch 163, Loss: 0.4664611566834947\n",
      "Epoch 164, Loss: 0.46654532450127006\n",
      "Epoch 165, Loss: 0.46721845607201035\n",
      "Epoch 166, Loss: 0.4668208160006968\n",
      "Epoch 167, Loss: 0.466573749669782\n",
      "Epoch 168, Loss: 0.46506387643568103\n",
      "Epoch 169, Loss: 0.4641220516950567\n",
      "Epoch 170, Loss: 0.4644550172656552\n",
      "Epoch 171, Loss: 0.4654613047116713\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.24474207058161712\n",
      "Valence RMSE: 0.2528281265174966\n",
      "Arousal RMSE: 0.23637956905538024\n",
      "Test R^2 score: tensor([0.3169, 0.5879], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4524262864389503\n",
      "Num of epochs: 172\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Epoch 121, Loss: 0.4816228295571127\n",
      "Epoch 122, Loss: 0.4807618495484584\n",
      "Epoch 123, Loss: 0.48102030846138955\n",
      "Epoch 124, Loss: 0.4810176288364116\n",
      "Epoch 125, Loss: 0.48014338261661127\n",
      "Epoch 126, Loss: 0.47910314809270116\n",
      "Epoch 127, Loss: 0.47880795949683447\n",
      "Epoch 128, Loss: 0.47876177315142754\n",
      "Epoch 129, Loss: 0.47819597116449997\n",
      "Epoch 130, Loss: 0.47743214539542544\n",
      "Epoch 131, Loss: 0.47658476230340185\n",
      "Epoch 132, Loss: 0.4759644680184978\n",
      "Epoch 133, Loss: 0.4755635187712421\n",
      "Epoch 134, Loss: 0.47538264196584307\n",
      "Epoch 135, Loss: 0.4753673450596914\n",
      "Epoch 136, Loss: 0.4753697744168057\n",
      "Epoch 137, Loss: 0.4755973109583155\n",
      "Epoch 138, Loss: 0.4767626826605241\n",
      "Epoch 139, Loss: 0.47683528222287225\n",
      "Epoch 140, Loss: 0.47579445468261566\n",
      "Epoch 141, Loss: 0.47284533919249566\n",
      "Epoch 142, Loss: 0.472809286012082\n",
      "Epoch 143, Loss: 0.47458738111232796\n",
      "Epoch 144, Loss: 0.4733380058918715\n",
      "Epoch 145, Loss: 0.4715229034484008\n",
      "Epoch 146, Loss: 0.47096730966267386\n",
      "Epoch 147, Loss: 0.47161469890467383\n",
      "Epoch 148, Loss: 0.47183531411129226\n",
      "Epoch 149, Loss: 0.470294933688076\n",
      "Epoch 150, Loss: 0.4697114144973765\n",
      "Epoch 151, Loss: 0.47018789053701304\n",
      "Epoch 152, Loss: 0.4700460953124117\n",
      "Epoch 153, Loss: 0.4693645740016508\n",
      "Epoch 154, Loss: 0.4684558104180481\n",
      "Epoch 155, Loss: 0.4680960384979934\n",
      "Epoch 156, Loss: 0.4682454254529793\n",
      "Epoch 157, Loss: 0.4682974379105488\n",
      "Epoch 158, Loss: 0.4681590010361841\n",
      "Epoch 159, Loss: 0.4673225124817593\n",
      "Epoch 160, Loss: 0.4666701428107949\n",
      "Epoch 161, Loss: 0.4663166784414499\n",
      "Epoch 162, Loss: 0.46624854535037\n",
      "Epoch 163, Loss: 0.4664611566834947\n",
      "Epoch 164, Loss: 0.46654532450127006\n",
      "Epoch 165, Loss: 0.46721845607201035\n",
      "Epoch 166, Loss: 0.4668208160006968\n",
      "Epoch 167, Loss: 0.466573749669782\n",
      "Epoch 168, Loss: 0.46506387643568103\n",
      "Epoch 169, Loss: 0.4641220516950567\n",
      "Epoch 170, Loss: 0.4644550172656552\n",
      "Epoch 171, Loss: 0.4654613047116713\n",
      "Epoch 172, Loss: 0.46880176576246846\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.24100396066689853\n",
      "Valence RMSE: 0.24348155167719657\n",
      "Arousal RMSE: 0.23850063334743707\n",
      "Test R^2 score: tensor([0.3665, 0.5805], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4734970880331955\n",
      "Num of epochs: 173\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Epoch 121, Loss: 0.4816228295571127\n",
      "Epoch 122, Loss: 0.4807618495484584\n",
      "Epoch 123, Loss: 0.48102030846138955\n",
      "Epoch 124, Loss: 0.4810176288364116\n",
      "Epoch 125, Loss: 0.48014338261661127\n",
      "Epoch 126, Loss: 0.47910314809270116\n",
      "Epoch 127, Loss: 0.47880795949683447\n",
      "Epoch 128, Loss: 0.47876177315142754\n",
      "Epoch 129, Loss: 0.47819597116449997\n",
      "Epoch 130, Loss: 0.47743214539542544\n",
      "Epoch 131, Loss: 0.47658476230340185\n",
      "Epoch 132, Loss: 0.4759644680184978\n",
      "Epoch 133, Loss: 0.4755635187712421\n",
      "Epoch 134, Loss: 0.47538264196584307\n",
      "Epoch 135, Loss: 0.4753673450596914\n",
      "Epoch 136, Loss: 0.4753697744168057\n",
      "Epoch 137, Loss: 0.4755973109583155\n",
      "Epoch 138, Loss: 0.4767626826605241\n",
      "Epoch 139, Loss: 0.47683528222287225\n",
      "Epoch 140, Loss: 0.47579445468261566\n",
      "Epoch 141, Loss: 0.47284533919249566\n",
      "Epoch 142, Loss: 0.472809286012082\n",
      "Epoch 143, Loss: 0.47458738111232796\n",
      "Epoch 144, Loss: 0.4733380058918715\n",
      "Epoch 145, Loss: 0.4715229034484008\n",
      "Epoch 146, Loss: 0.47096730966267386\n",
      "Epoch 147, Loss: 0.47161469890467383\n",
      "Epoch 148, Loss: 0.47183531411129226\n",
      "Epoch 149, Loss: 0.470294933688076\n",
      "Epoch 150, Loss: 0.4697114144973765\n",
      "Epoch 151, Loss: 0.47018789053701304\n",
      "Epoch 152, Loss: 0.4700460953124117\n",
      "Epoch 153, Loss: 0.4693645740016508\n",
      "Epoch 154, Loss: 0.4684558104180481\n",
      "Epoch 155, Loss: 0.4680960384979934\n",
      "Epoch 156, Loss: 0.4682454254529793\n",
      "Epoch 157, Loss: 0.4682974379105488\n",
      "Epoch 158, Loss: 0.4681590010361841\n",
      "Epoch 159, Loss: 0.4673225124817593\n",
      "Epoch 160, Loss: 0.4666701428107949\n",
      "Epoch 161, Loss: 0.4663166784414499\n",
      "Epoch 162, Loss: 0.46624854535037\n",
      "Epoch 163, Loss: 0.4664611566834947\n",
      "Epoch 164, Loss: 0.46654532450127006\n",
      "Epoch 165, Loss: 0.46721845607201035\n",
      "Epoch 166, Loss: 0.4668208160006968\n",
      "Epoch 167, Loss: 0.466573749669782\n",
      "Epoch 168, Loss: 0.46506387643568103\n",
      "Epoch 169, Loss: 0.4641220516950567\n",
      "Epoch 170, Loss: 0.4644550172656552\n",
      "Epoch 171, Loss: 0.4654613047116713\n",
      "Epoch 172, Loss: 0.46880176576246846\n",
      "Epoch 173, Loss: 0.4685649824581709\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.24074162111863584\n",
      "Valence RMSE: 0.24847995706547094\n",
      "Arousal RMSE: 0.23274614328576823\n",
      "Test R^2 score: tensor([0.3402, 0.6005], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.47035821280717693\n",
      "Num of epochs: 174\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Epoch 121, Loss: 0.4816228295571127\n",
      "Epoch 122, Loss: 0.4807618495484584\n",
      "Epoch 123, Loss: 0.48102030846138955\n",
      "Epoch 124, Loss: 0.4810176288364116\n",
      "Epoch 125, Loss: 0.48014338261661127\n",
      "Epoch 126, Loss: 0.47910314809270116\n",
      "Epoch 127, Loss: 0.47880795949683447\n",
      "Epoch 128, Loss: 0.47876177315142754\n",
      "Epoch 129, Loss: 0.47819597116449997\n",
      "Epoch 130, Loss: 0.47743214539542544\n",
      "Epoch 131, Loss: 0.47658476230340185\n",
      "Epoch 132, Loss: 0.4759644680184978\n",
      "Epoch 133, Loss: 0.4755635187712421\n",
      "Epoch 134, Loss: 0.47538264196584307\n",
      "Epoch 135, Loss: 0.4753673450596914\n",
      "Epoch 136, Loss: 0.4753697744168057\n",
      "Epoch 137, Loss: 0.4755973109583155\n",
      "Epoch 138, Loss: 0.4767626826605241\n",
      "Epoch 139, Loss: 0.47683528222287225\n",
      "Epoch 140, Loss: 0.47579445468261566\n",
      "Epoch 141, Loss: 0.47284533919249566\n",
      "Epoch 142, Loss: 0.472809286012082\n",
      "Epoch 143, Loss: 0.47458738111232796\n",
      "Epoch 144, Loss: 0.4733380058918715\n",
      "Epoch 145, Loss: 0.4715229034484008\n",
      "Epoch 146, Loss: 0.47096730966267386\n",
      "Epoch 147, Loss: 0.47161469890467383\n",
      "Epoch 148, Loss: 0.47183531411129226\n",
      "Epoch 149, Loss: 0.470294933688076\n",
      "Epoch 150, Loss: 0.4697114144973765\n",
      "Epoch 151, Loss: 0.47018789053701304\n",
      "Epoch 152, Loss: 0.4700460953124117\n",
      "Epoch 153, Loss: 0.4693645740016508\n",
      "Epoch 154, Loss: 0.4684558104180481\n",
      "Epoch 155, Loss: 0.4680960384979934\n",
      "Epoch 156, Loss: 0.4682454254529793\n",
      "Epoch 157, Loss: 0.4682974379105488\n",
      "Epoch 158, Loss: 0.4681590010361841\n",
      "Epoch 159, Loss: 0.4673225124817593\n",
      "Epoch 160, Loss: 0.4666701428107949\n",
      "Epoch 161, Loss: 0.4663166784414499\n",
      "Epoch 162, Loss: 0.46624854535037\n",
      "Epoch 163, Loss: 0.4664611566834947\n",
      "Epoch 164, Loss: 0.46654532450127006\n",
      "Epoch 165, Loss: 0.46721845607201035\n",
      "Epoch 166, Loss: 0.4668208160006968\n",
      "Epoch 167, Loss: 0.466573749669782\n",
      "Epoch 168, Loss: 0.46506387643568103\n",
      "Epoch 169, Loss: 0.4641220516950567\n",
      "Epoch 170, Loss: 0.4644550172656552\n",
      "Epoch 171, Loss: 0.4654613047116713\n",
      "Epoch 172, Loss: 0.46880176576246846\n",
      "Epoch 173, Loss: 0.4685649824581709\n",
      "Epoch 174, Loss: 0.46606115996454733\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2376483367972036\n",
      "Valence RMSE: 0.24426134045254844\n",
      "Arousal RMSE: 0.23084596926365952\n",
      "Test R^2 score: tensor([0.3624, 0.6070], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4847129283290915\n",
      "Num of epochs: 175\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Epoch 121, Loss: 0.4816228295571127\n",
      "Epoch 122, Loss: 0.4807618495484584\n",
      "Epoch 123, Loss: 0.48102030846138955\n",
      "Epoch 124, Loss: 0.4810176288364116\n",
      "Epoch 125, Loss: 0.48014338261661127\n",
      "Epoch 126, Loss: 0.47910314809270116\n",
      "Epoch 127, Loss: 0.47880795949683447\n",
      "Epoch 128, Loss: 0.47876177315142754\n",
      "Epoch 129, Loss: 0.47819597116449997\n",
      "Epoch 130, Loss: 0.47743214539542544\n",
      "Epoch 131, Loss: 0.47658476230340185\n",
      "Epoch 132, Loss: 0.4759644680184978\n",
      "Epoch 133, Loss: 0.4755635187712421\n",
      "Epoch 134, Loss: 0.47538264196584307\n",
      "Epoch 135, Loss: 0.4753673450596914\n",
      "Epoch 136, Loss: 0.4753697744168057\n",
      "Epoch 137, Loss: 0.4755973109583155\n",
      "Epoch 138, Loss: 0.4767626826605241\n",
      "Epoch 139, Loss: 0.47683528222287225\n",
      "Epoch 140, Loss: 0.47579445468261566\n",
      "Epoch 141, Loss: 0.47284533919249566\n",
      "Epoch 142, Loss: 0.472809286012082\n",
      "Epoch 143, Loss: 0.47458738111232796\n",
      "Epoch 144, Loss: 0.4733380058918715\n",
      "Epoch 145, Loss: 0.4715229034484008\n",
      "Epoch 146, Loss: 0.47096730966267386\n",
      "Epoch 147, Loss: 0.47161469890467383\n",
      "Epoch 148, Loss: 0.47183531411129226\n",
      "Epoch 149, Loss: 0.470294933688076\n",
      "Epoch 150, Loss: 0.4697114144973765\n",
      "Epoch 151, Loss: 0.47018789053701304\n",
      "Epoch 152, Loss: 0.4700460953124117\n",
      "Epoch 153, Loss: 0.4693645740016508\n",
      "Epoch 154, Loss: 0.4684558104180481\n",
      "Epoch 155, Loss: 0.4680960384979934\n",
      "Epoch 156, Loss: 0.4682454254529793\n",
      "Epoch 157, Loss: 0.4682974379105488\n",
      "Epoch 158, Loss: 0.4681590010361841\n",
      "Epoch 159, Loss: 0.4673225124817593\n",
      "Epoch 160, Loss: 0.4666701428107949\n",
      "Epoch 161, Loss: 0.4663166784414499\n",
      "Epoch 162, Loss: 0.46624854535037\n",
      "Epoch 163, Loss: 0.4664611566834947\n",
      "Epoch 164, Loss: 0.46654532450127006\n",
      "Epoch 165, Loss: 0.46721845607201035\n",
      "Epoch 166, Loss: 0.4668208160006968\n",
      "Epoch 167, Loss: 0.466573749669782\n",
      "Epoch 168, Loss: 0.46506387643568103\n",
      "Epoch 169, Loss: 0.4641220516950567\n",
      "Epoch 170, Loss: 0.4644550172656552\n",
      "Epoch 171, Loss: 0.4654613047116713\n",
      "Epoch 172, Loss: 0.46880176576246846\n",
      "Epoch 173, Loss: 0.4685649824581709\n",
      "Epoch 174, Loss: 0.46606115996454733\n",
      "Epoch 175, Loss: 0.46328263242364126\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23932832652893413\n",
      "Valence RMSE: 0.2422939111051904\n",
      "Arousal RMSE: 0.2363255305709274\n",
      "Test R^2 score: tensor([0.3727, 0.5881], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4803880120689994\n",
      "Num of epochs: 176\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Epoch 121, Loss: 0.4816228295571127\n",
      "Epoch 122, Loss: 0.4807618495484584\n",
      "Epoch 123, Loss: 0.48102030846138955\n",
      "Epoch 124, Loss: 0.4810176288364116\n",
      "Epoch 125, Loss: 0.48014338261661127\n",
      "Epoch 126, Loss: 0.47910314809270116\n",
      "Epoch 127, Loss: 0.47880795949683447\n",
      "Epoch 128, Loss: 0.47876177315142754\n",
      "Epoch 129, Loss: 0.47819597116449997\n",
      "Epoch 130, Loss: 0.47743214539542544\n",
      "Epoch 131, Loss: 0.47658476230340185\n",
      "Epoch 132, Loss: 0.4759644680184978\n",
      "Epoch 133, Loss: 0.4755635187712421\n",
      "Epoch 134, Loss: 0.47538264196584307\n",
      "Epoch 135, Loss: 0.4753673450596914\n",
      "Epoch 136, Loss: 0.4753697744168057\n",
      "Epoch 137, Loss: 0.4755973109583155\n",
      "Epoch 138, Loss: 0.4767626826605241\n",
      "Epoch 139, Loss: 0.47683528222287225\n",
      "Epoch 140, Loss: 0.47579445468261566\n",
      "Epoch 141, Loss: 0.47284533919249566\n",
      "Epoch 142, Loss: 0.472809286012082\n",
      "Epoch 143, Loss: 0.47458738111232796\n",
      "Epoch 144, Loss: 0.4733380058918715\n",
      "Epoch 145, Loss: 0.4715229034484008\n",
      "Epoch 146, Loss: 0.47096730966267386\n",
      "Epoch 147, Loss: 0.47161469890467383\n",
      "Epoch 148, Loss: 0.47183531411129226\n",
      "Epoch 149, Loss: 0.470294933688076\n",
      "Epoch 150, Loss: 0.4697114144973765\n",
      "Epoch 151, Loss: 0.47018789053701304\n",
      "Epoch 152, Loss: 0.4700460953124117\n",
      "Epoch 153, Loss: 0.4693645740016508\n",
      "Epoch 154, Loss: 0.4684558104180481\n",
      "Epoch 155, Loss: 0.4680960384979934\n",
      "Epoch 156, Loss: 0.4682454254529793\n",
      "Epoch 157, Loss: 0.4682974379105488\n",
      "Epoch 158, Loss: 0.4681590010361841\n",
      "Epoch 159, Loss: 0.4673225124817593\n",
      "Epoch 160, Loss: 0.4666701428107949\n",
      "Epoch 161, Loss: 0.4663166784414499\n",
      "Epoch 162, Loss: 0.46624854535037\n",
      "Epoch 163, Loss: 0.4664611566834947\n",
      "Epoch 164, Loss: 0.46654532450127006\n",
      "Epoch 165, Loss: 0.46721845607201035\n",
      "Epoch 166, Loss: 0.4668208160006968\n",
      "Epoch 167, Loss: 0.466573749669782\n",
      "Epoch 168, Loss: 0.46506387643568103\n",
      "Epoch 169, Loss: 0.4641220516950567\n",
      "Epoch 170, Loss: 0.4644550172656552\n",
      "Epoch 171, Loss: 0.4654613047116713\n",
      "Epoch 172, Loss: 0.46880176576246846\n",
      "Epoch 173, Loss: 0.4685649824581709\n",
      "Epoch 174, Loss: 0.46606115996454733\n",
      "Epoch 175, Loss: 0.46328263242364126\n",
      "Epoch 176, Loss: 0.4660484187313888\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.24227988268038686\n",
      "Valence RMSE: 0.250289326011562\n",
      "Arousal RMSE: 0.23399644524633717\n",
      "Test R^2 score: tensor([0.3306, 0.5962], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4633844947169475\n",
      "Num of epochs: 177\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Epoch 121, Loss: 0.4816228295571127\n",
      "Epoch 122, Loss: 0.4807618495484584\n",
      "Epoch 123, Loss: 0.48102030846138955\n",
      "Epoch 124, Loss: 0.4810176288364116\n",
      "Epoch 125, Loss: 0.48014338261661127\n",
      "Epoch 126, Loss: 0.47910314809270116\n",
      "Epoch 127, Loss: 0.47880795949683447\n",
      "Epoch 128, Loss: 0.47876177315142754\n",
      "Epoch 129, Loss: 0.47819597116449997\n",
      "Epoch 130, Loss: 0.47743214539542544\n",
      "Epoch 131, Loss: 0.47658476230340185\n",
      "Epoch 132, Loss: 0.4759644680184978\n",
      "Epoch 133, Loss: 0.4755635187712421\n",
      "Epoch 134, Loss: 0.47538264196584307\n",
      "Epoch 135, Loss: 0.4753673450596914\n",
      "Epoch 136, Loss: 0.4753697744168057\n",
      "Epoch 137, Loss: 0.4755973109583155\n",
      "Epoch 138, Loss: 0.4767626826605241\n",
      "Epoch 139, Loss: 0.47683528222287225\n",
      "Epoch 140, Loss: 0.47579445468261566\n",
      "Epoch 141, Loss: 0.47284533919249566\n",
      "Epoch 142, Loss: 0.472809286012082\n",
      "Epoch 143, Loss: 0.47458738111232796\n",
      "Epoch 144, Loss: 0.4733380058918715\n",
      "Epoch 145, Loss: 0.4715229034484008\n",
      "Epoch 146, Loss: 0.47096730966267386\n",
      "Epoch 147, Loss: 0.47161469890467383\n",
      "Epoch 148, Loss: 0.47183531411129226\n",
      "Epoch 149, Loss: 0.470294933688076\n",
      "Epoch 150, Loss: 0.4697114144973765\n",
      "Epoch 151, Loss: 0.47018789053701304\n",
      "Epoch 152, Loss: 0.4700460953124117\n",
      "Epoch 153, Loss: 0.4693645740016508\n",
      "Epoch 154, Loss: 0.4684558104180481\n",
      "Epoch 155, Loss: 0.4680960384979934\n",
      "Epoch 156, Loss: 0.4682454254529793\n",
      "Epoch 157, Loss: 0.4682974379105488\n",
      "Epoch 158, Loss: 0.4681590010361841\n",
      "Epoch 159, Loss: 0.4673225124817593\n",
      "Epoch 160, Loss: 0.4666701428107949\n",
      "Epoch 161, Loss: 0.4663166784414499\n",
      "Epoch 162, Loss: 0.46624854535037\n",
      "Epoch 163, Loss: 0.4664611566834947\n",
      "Epoch 164, Loss: 0.46654532450127006\n",
      "Epoch 165, Loss: 0.46721845607201035\n",
      "Epoch 166, Loss: 0.4668208160006968\n",
      "Epoch 167, Loss: 0.466573749669782\n",
      "Epoch 168, Loss: 0.46506387643568103\n",
      "Epoch 169, Loss: 0.4641220516950567\n",
      "Epoch 170, Loss: 0.4644550172656552\n",
      "Epoch 171, Loss: 0.4654613047116713\n",
      "Epoch 172, Loss: 0.46880176576246846\n",
      "Epoch 173, Loss: 0.4685649824581709\n",
      "Epoch 174, Loss: 0.46606115996454733\n",
      "Epoch 175, Loss: 0.46328263242364126\n",
      "Epoch 176, Loss: 0.4660484187313888\n",
      "Epoch 177, Loss: 0.4667669789465077\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23837071963255438\n",
      "Valence RMSE: 0.24419618838917095\n",
      "Arousal RMSE: 0.232399271798524\n",
      "Test R^2 score: tensor([0.3628, 0.6017], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.48222960088460887\n",
      "Num of epochs: 178\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Epoch 121, Loss: 0.4816228295571127\n",
      "Epoch 122, Loss: 0.4807618495484584\n",
      "Epoch 123, Loss: 0.48102030846138955\n",
      "Epoch 124, Loss: 0.4810176288364116\n",
      "Epoch 125, Loss: 0.48014338261661127\n",
      "Epoch 126, Loss: 0.47910314809270116\n",
      "Epoch 127, Loss: 0.47880795949683447\n",
      "Epoch 128, Loss: 0.47876177315142754\n",
      "Epoch 129, Loss: 0.47819597116449997\n",
      "Epoch 130, Loss: 0.47743214539542544\n",
      "Epoch 131, Loss: 0.47658476230340185\n",
      "Epoch 132, Loss: 0.4759644680184978\n",
      "Epoch 133, Loss: 0.4755635187712421\n",
      "Epoch 134, Loss: 0.47538264196584307\n",
      "Epoch 135, Loss: 0.4753673450596914\n",
      "Epoch 136, Loss: 0.4753697744168057\n",
      "Epoch 137, Loss: 0.4755973109583155\n",
      "Epoch 138, Loss: 0.4767626826605241\n",
      "Epoch 139, Loss: 0.47683528222287225\n",
      "Epoch 140, Loss: 0.47579445468261566\n",
      "Epoch 141, Loss: 0.47284533919249566\n",
      "Epoch 142, Loss: 0.472809286012082\n",
      "Epoch 143, Loss: 0.47458738111232796\n",
      "Epoch 144, Loss: 0.4733380058918715\n",
      "Epoch 145, Loss: 0.4715229034484008\n",
      "Epoch 146, Loss: 0.47096730966267386\n",
      "Epoch 147, Loss: 0.47161469890467383\n",
      "Epoch 148, Loss: 0.47183531411129226\n",
      "Epoch 149, Loss: 0.470294933688076\n",
      "Epoch 150, Loss: 0.4697114144973765\n",
      "Epoch 151, Loss: 0.47018789053701304\n",
      "Epoch 152, Loss: 0.4700460953124117\n",
      "Epoch 153, Loss: 0.4693645740016508\n",
      "Epoch 154, Loss: 0.4684558104180481\n",
      "Epoch 155, Loss: 0.4680960384979934\n",
      "Epoch 156, Loss: 0.4682454254529793\n",
      "Epoch 157, Loss: 0.4682974379105488\n",
      "Epoch 158, Loss: 0.4681590010361841\n",
      "Epoch 159, Loss: 0.4673225124817593\n",
      "Epoch 160, Loss: 0.4666701428107949\n",
      "Epoch 161, Loss: 0.4663166784414499\n",
      "Epoch 162, Loss: 0.46624854535037\n",
      "Epoch 163, Loss: 0.4664611566834947\n",
      "Epoch 164, Loss: 0.46654532450127006\n",
      "Epoch 165, Loss: 0.46721845607201035\n",
      "Epoch 166, Loss: 0.4668208160006968\n",
      "Epoch 167, Loss: 0.466573749669782\n",
      "Epoch 168, Loss: 0.46506387643568103\n",
      "Epoch 169, Loss: 0.4641220516950567\n",
      "Epoch 170, Loss: 0.4644550172656552\n",
      "Epoch 171, Loss: 0.4654613047116713\n",
      "Epoch 172, Loss: 0.46880176576246846\n",
      "Epoch 173, Loss: 0.4685649824581709\n",
      "Epoch 174, Loss: 0.46606115996454733\n",
      "Epoch 175, Loss: 0.46328263242364126\n",
      "Epoch 176, Loss: 0.4660484187313888\n",
      "Epoch 177, Loss: 0.4667669789465077\n",
      "Epoch 178, Loss: 0.46222975993575777\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2398013187553705\n",
      "Valence RMSE: 0.2439056914304577\n",
      "Arousal RMSE: 0.23562546263394357\n",
      "Test R^2 score: tensor([0.3643, 0.5905], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4774193321559012\n",
      "Num of epochs: 179\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Epoch 121, Loss: 0.4816228295571127\n",
      "Epoch 122, Loss: 0.4807618495484584\n",
      "Epoch 123, Loss: 0.48102030846138955\n",
      "Epoch 124, Loss: 0.4810176288364116\n",
      "Epoch 125, Loss: 0.48014338261661127\n",
      "Epoch 126, Loss: 0.47910314809270116\n",
      "Epoch 127, Loss: 0.47880795949683447\n",
      "Epoch 128, Loss: 0.47876177315142754\n",
      "Epoch 129, Loss: 0.47819597116449997\n",
      "Epoch 130, Loss: 0.47743214539542544\n",
      "Epoch 131, Loss: 0.47658476230340185\n",
      "Epoch 132, Loss: 0.4759644680184978\n",
      "Epoch 133, Loss: 0.4755635187712421\n",
      "Epoch 134, Loss: 0.47538264196584307\n",
      "Epoch 135, Loss: 0.4753673450596914\n",
      "Epoch 136, Loss: 0.4753697744168057\n",
      "Epoch 137, Loss: 0.4755973109583155\n",
      "Epoch 138, Loss: 0.4767626826605241\n",
      "Epoch 139, Loss: 0.47683528222287225\n",
      "Epoch 140, Loss: 0.47579445468261566\n",
      "Epoch 141, Loss: 0.47284533919249566\n",
      "Epoch 142, Loss: 0.472809286012082\n",
      "Epoch 143, Loss: 0.47458738111232796\n",
      "Epoch 144, Loss: 0.4733380058918715\n",
      "Epoch 145, Loss: 0.4715229034484008\n",
      "Epoch 146, Loss: 0.47096730966267386\n",
      "Epoch 147, Loss: 0.47161469890467383\n",
      "Epoch 148, Loss: 0.47183531411129226\n",
      "Epoch 149, Loss: 0.470294933688076\n",
      "Epoch 150, Loss: 0.4697114144973765\n",
      "Epoch 151, Loss: 0.47018789053701304\n",
      "Epoch 152, Loss: 0.4700460953124117\n",
      "Epoch 153, Loss: 0.4693645740016508\n",
      "Epoch 154, Loss: 0.4684558104180481\n",
      "Epoch 155, Loss: 0.4680960384979934\n",
      "Epoch 156, Loss: 0.4682454254529793\n",
      "Epoch 157, Loss: 0.4682974379105488\n",
      "Epoch 158, Loss: 0.4681590010361841\n",
      "Epoch 159, Loss: 0.4673225124817593\n",
      "Epoch 160, Loss: 0.4666701428107949\n",
      "Epoch 161, Loss: 0.4663166784414499\n",
      "Epoch 162, Loss: 0.46624854535037\n",
      "Epoch 163, Loss: 0.4664611566834947\n",
      "Epoch 164, Loss: 0.46654532450127006\n",
      "Epoch 165, Loss: 0.46721845607201035\n",
      "Epoch 166, Loss: 0.4668208160006968\n",
      "Epoch 167, Loss: 0.466573749669782\n",
      "Epoch 168, Loss: 0.46506387643568103\n",
      "Epoch 169, Loss: 0.4641220516950567\n",
      "Epoch 170, Loss: 0.4644550172656552\n",
      "Epoch 171, Loss: 0.4654613047116713\n",
      "Epoch 172, Loss: 0.46880176576246846\n",
      "Epoch 173, Loss: 0.4685649824581709\n",
      "Epoch 174, Loss: 0.46606115996454733\n",
      "Epoch 175, Loss: 0.46328263242364126\n",
      "Epoch 176, Loss: 0.4660484187313888\n",
      "Epoch 177, Loss: 0.4667669789465077\n",
      "Epoch 178, Loss: 0.46222975993575777\n",
      "Epoch 179, Loss: 0.46412214801342555\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2459938920985596\n",
      "Valence RMSE: 0.25460953189082\n",
      "Arousal RMSE: 0.23706534156205344\n",
      "Test R^2 score: tensor([0.3073, 0.5855], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4463992482358953\n",
      "Num of epochs: 180\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Epoch 121, Loss: 0.4816228295571127\n",
      "Epoch 122, Loss: 0.4807618495484584\n",
      "Epoch 123, Loss: 0.48102030846138955\n",
      "Epoch 124, Loss: 0.4810176288364116\n",
      "Epoch 125, Loss: 0.48014338261661127\n",
      "Epoch 126, Loss: 0.47910314809270116\n",
      "Epoch 127, Loss: 0.47880795949683447\n",
      "Epoch 128, Loss: 0.47876177315142754\n",
      "Epoch 129, Loss: 0.47819597116449997\n",
      "Epoch 130, Loss: 0.47743214539542544\n",
      "Epoch 131, Loss: 0.47658476230340185\n",
      "Epoch 132, Loss: 0.4759644680184978\n",
      "Epoch 133, Loss: 0.4755635187712421\n",
      "Epoch 134, Loss: 0.47538264196584307\n",
      "Epoch 135, Loss: 0.4753673450596914\n",
      "Epoch 136, Loss: 0.4753697744168057\n",
      "Epoch 137, Loss: 0.4755973109583155\n",
      "Epoch 138, Loss: 0.4767626826605241\n",
      "Epoch 139, Loss: 0.47683528222287225\n",
      "Epoch 140, Loss: 0.47579445468261566\n",
      "Epoch 141, Loss: 0.47284533919249566\n",
      "Epoch 142, Loss: 0.472809286012082\n",
      "Epoch 143, Loss: 0.47458738111232796\n",
      "Epoch 144, Loss: 0.4733380058918715\n",
      "Epoch 145, Loss: 0.4715229034484008\n",
      "Epoch 146, Loss: 0.47096730966267386\n",
      "Epoch 147, Loss: 0.47161469890467383\n",
      "Epoch 148, Loss: 0.47183531411129226\n",
      "Epoch 149, Loss: 0.470294933688076\n",
      "Epoch 150, Loss: 0.4697114144973765\n",
      "Epoch 151, Loss: 0.47018789053701304\n",
      "Epoch 152, Loss: 0.4700460953124117\n",
      "Epoch 153, Loss: 0.4693645740016508\n",
      "Epoch 154, Loss: 0.4684558104180481\n",
      "Epoch 155, Loss: 0.4680960384979934\n",
      "Epoch 156, Loss: 0.4682454254529793\n",
      "Epoch 157, Loss: 0.4682974379105488\n",
      "Epoch 158, Loss: 0.4681590010361841\n",
      "Epoch 159, Loss: 0.4673225124817593\n",
      "Epoch 160, Loss: 0.4666701428107949\n",
      "Epoch 161, Loss: 0.4663166784414499\n",
      "Epoch 162, Loss: 0.46624854535037\n",
      "Epoch 163, Loss: 0.4664611566834947\n",
      "Epoch 164, Loss: 0.46654532450127006\n",
      "Epoch 165, Loss: 0.46721845607201035\n",
      "Epoch 166, Loss: 0.4668208160006968\n",
      "Epoch 167, Loss: 0.466573749669782\n",
      "Epoch 168, Loss: 0.46506387643568103\n",
      "Epoch 169, Loss: 0.4641220516950567\n",
      "Epoch 170, Loss: 0.4644550172656552\n",
      "Epoch 171, Loss: 0.4654613047116713\n",
      "Epoch 172, Loss: 0.46880176576246846\n",
      "Epoch 173, Loss: 0.4685649824581709\n",
      "Epoch 174, Loss: 0.46606115996454733\n",
      "Epoch 175, Loss: 0.46328263242364126\n",
      "Epoch 176, Loss: 0.4660484187313888\n",
      "Epoch 177, Loss: 0.4667669789465077\n",
      "Epoch 178, Loss: 0.46222975993575777\n",
      "Epoch 179, Loss: 0.46412214801342555\n",
      "Epoch 180, Loss: 0.46867384291927394\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2391954871543241\n",
      "Valence RMSE: 0.24389548147516718\n",
      "Arousal RMSE: 0.23440127189498183\n",
      "Test R^2 score: tensor([0.3643, 0.5948], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4795677269355501\n",
      "Num of epochs: 181\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Epoch 121, Loss: 0.4816228295571127\n",
      "Epoch 122, Loss: 0.4807618495484584\n",
      "Epoch 123, Loss: 0.48102030846138955\n",
      "Epoch 124, Loss: 0.4810176288364116\n",
      "Epoch 125, Loss: 0.48014338261661127\n",
      "Epoch 126, Loss: 0.47910314809270116\n",
      "Epoch 127, Loss: 0.47880795949683447\n",
      "Epoch 128, Loss: 0.47876177315142754\n",
      "Epoch 129, Loss: 0.47819597116449997\n",
      "Epoch 130, Loss: 0.47743214539542544\n",
      "Epoch 131, Loss: 0.47658476230340185\n",
      "Epoch 132, Loss: 0.4759644680184978\n",
      "Epoch 133, Loss: 0.4755635187712421\n",
      "Epoch 134, Loss: 0.47538264196584307\n",
      "Epoch 135, Loss: 0.4753673450596914\n",
      "Epoch 136, Loss: 0.4753697744168057\n",
      "Epoch 137, Loss: 0.4755973109583155\n",
      "Epoch 138, Loss: 0.4767626826605241\n",
      "Epoch 139, Loss: 0.47683528222287225\n",
      "Epoch 140, Loss: 0.47579445468261566\n",
      "Epoch 141, Loss: 0.47284533919249566\n",
      "Epoch 142, Loss: 0.472809286012082\n",
      "Epoch 143, Loss: 0.47458738111232796\n",
      "Epoch 144, Loss: 0.4733380058918715\n",
      "Epoch 145, Loss: 0.4715229034484008\n",
      "Epoch 146, Loss: 0.47096730966267386\n",
      "Epoch 147, Loss: 0.47161469890467383\n",
      "Epoch 148, Loss: 0.47183531411129226\n",
      "Epoch 149, Loss: 0.470294933688076\n",
      "Epoch 150, Loss: 0.4697114144973765\n",
      "Epoch 151, Loss: 0.47018789053701304\n",
      "Epoch 152, Loss: 0.4700460953124117\n",
      "Epoch 153, Loss: 0.4693645740016508\n",
      "Epoch 154, Loss: 0.4684558104180481\n",
      "Epoch 155, Loss: 0.4680960384979934\n",
      "Epoch 156, Loss: 0.4682454254529793\n",
      "Epoch 157, Loss: 0.4682974379105488\n",
      "Epoch 158, Loss: 0.4681590010361841\n",
      "Epoch 159, Loss: 0.4673225124817593\n",
      "Epoch 160, Loss: 0.4666701428107949\n",
      "Epoch 161, Loss: 0.4663166784414499\n",
      "Epoch 162, Loss: 0.46624854535037\n",
      "Epoch 163, Loss: 0.4664611566834947\n",
      "Epoch 164, Loss: 0.46654532450127006\n",
      "Epoch 165, Loss: 0.46721845607201035\n",
      "Epoch 166, Loss: 0.4668208160006968\n",
      "Epoch 167, Loss: 0.466573749669782\n",
      "Epoch 168, Loss: 0.46506387643568103\n",
      "Epoch 169, Loss: 0.4641220516950567\n",
      "Epoch 170, Loss: 0.4644550172656552\n",
      "Epoch 171, Loss: 0.4654613047116713\n",
      "Epoch 172, Loss: 0.46880176576246846\n",
      "Epoch 173, Loss: 0.4685649824581709\n",
      "Epoch 174, Loss: 0.46606115996454733\n",
      "Epoch 175, Loss: 0.46328263242364126\n",
      "Epoch 176, Loss: 0.4660484187313888\n",
      "Epoch 177, Loss: 0.4667669789465077\n",
      "Epoch 178, Loss: 0.46222975993575777\n",
      "Epoch 179, Loss: 0.46412214801342555\n",
      "Epoch 180, Loss: 0.46867384291927394\n",
      "Epoch 181, Loss: 0.46316269230838986\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23814447305963268\n",
      "Valence RMSE: 0.24386871603811583\n",
      "Arousal RMSE: 0.2322792057753363\n",
      "Test R^2 score: tensor([0.3645, 0.6021], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4832892906171161\n",
      "Num of epochs: 182\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Epoch 121, Loss: 0.4816228295571127\n",
      "Epoch 122, Loss: 0.4807618495484584\n",
      "Epoch 123, Loss: 0.48102030846138955\n",
      "Epoch 124, Loss: 0.4810176288364116\n",
      "Epoch 125, Loss: 0.48014338261661127\n",
      "Epoch 126, Loss: 0.47910314809270116\n",
      "Epoch 127, Loss: 0.47880795949683447\n",
      "Epoch 128, Loss: 0.47876177315142754\n",
      "Epoch 129, Loss: 0.47819597116449997\n",
      "Epoch 130, Loss: 0.47743214539542544\n",
      "Epoch 131, Loss: 0.47658476230340185\n",
      "Epoch 132, Loss: 0.4759644680184978\n",
      "Epoch 133, Loss: 0.4755635187712421\n",
      "Epoch 134, Loss: 0.47538264196584307\n",
      "Epoch 135, Loss: 0.4753673450596914\n",
      "Epoch 136, Loss: 0.4753697744168057\n",
      "Epoch 137, Loss: 0.4755973109583155\n",
      "Epoch 138, Loss: 0.4767626826605241\n",
      "Epoch 139, Loss: 0.47683528222287225\n",
      "Epoch 140, Loss: 0.47579445468261566\n",
      "Epoch 141, Loss: 0.47284533919249566\n",
      "Epoch 142, Loss: 0.472809286012082\n",
      "Epoch 143, Loss: 0.47458738111232796\n",
      "Epoch 144, Loss: 0.4733380058918715\n",
      "Epoch 145, Loss: 0.4715229034484008\n",
      "Epoch 146, Loss: 0.47096730966267386\n",
      "Epoch 147, Loss: 0.47161469890467383\n",
      "Epoch 148, Loss: 0.47183531411129226\n",
      "Epoch 149, Loss: 0.470294933688076\n",
      "Epoch 150, Loss: 0.4697114144973765\n",
      "Epoch 151, Loss: 0.47018789053701304\n",
      "Epoch 152, Loss: 0.4700460953124117\n",
      "Epoch 153, Loss: 0.4693645740016508\n",
      "Epoch 154, Loss: 0.4684558104180481\n",
      "Epoch 155, Loss: 0.4680960384979934\n",
      "Epoch 156, Loss: 0.4682454254529793\n",
      "Epoch 157, Loss: 0.4682974379105488\n",
      "Epoch 158, Loss: 0.4681590010361841\n",
      "Epoch 159, Loss: 0.4673225124817593\n",
      "Epoch 160, Loss: 0.4666701428107949\n",
      "Epoch 161, Loss: 0.4663166784414499\n",
      "Epoch 162, Loss: 0.46624854535037\n",
      "Epoch 163, Loss: 0.4664611566834947\n",
      "Epoch 164, Loss: 0.46654532450127006\n",
      "Epoch 165, Loss: 0.46721845607201035\n",
      "Epoch 166, Loss: 0.4668208160006968\n",
      "Epoch 167, Loss: 0.466573749669782\n",
      "Epoch 168, Loss: 0.46506387643568103\n",
      "Epoch 169, Loss: 0.4641220516950567\n",
      "Epoch 170, Loss: 0.4644550172656552\n",
      "Epoch 171, Loss: 0.4654613047116713\n",
      "Epoch 172, Loss: 0.46880176576246846\n",
      "Epoch 173, Loss: 0.4685649824581709\n",
      "Epoch 174, Loss: 0.46606115996454733\n",
      "Epoch 175, Loss: 0.46328263242364126\n",
      "Epoch 176, Loss: 0.4660484187313888\n",
      "Epoch 177, Loss: 0.4667669789465077\n",
      "Epoch 178, Loss: 0.46222975993575777\n",
      "Epoch 179, Loss: 0.46412214801342555\n",
      "Epoch 180, Loss: 0.46867384291927394\n",
      "Epoch 181, Loss: 0.46316269230838986\n",
      "Epoch 182, Loss: 0.4619791093960296\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.24137439952451734\n",
      "Valence RMSE: 0.24992891464797748\n",
      "Arousal RMSE: 0.2325053528728459\n",
      "Test R^2 score: tensor([0.3325, 0.6013], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.46691275207963207\n",
      "Num of epochs: 183\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Epoch 121, Loss: 0.4816228295571127\n",
      "Epoch 122, Loss: 0.4807618495484584\n",
      "Epoch 123, Loss: 0.48102030846138955\n",
      "Epoch 124, Loss: 0.4810176288364116\n",
      "Epoch 125, Loss: 0.48014338261661127\n",
      "Epoch 126, Loss: 0.47910314809270116\n",
      "Epoch 127, Loss: 0.47880795949683447\n",
      "Epoch 128, Loss: 0.47876177315142754\n",
      "Epoch 129, Loss: 0.47819597116449997\n",
      "Epoch 130, Loss: 0.47743214539542544\n",
      "Epoch 131, Loss: 0.47658476230340185\n",
      "Epoch 132, Loss: 0.4759644680184978\n",
      "Epoch 133, Loss: 0.4755635187712421\n",
      "Epoch 134, Loss: 0.47538264196584307\n",
      "Epoch 135, Loss: 0.4753673450596914\n",
      "Epoch 136, Loss: 0.4753697744168057\n",
      "Epoch 137, Loss: 0.4755973109583155\n",
      "Epoch 138, Loss: 0.4767626826605241\n",
      "Epoch 139, Loss: 0.47683528222287225\n",
      "Epoch 140, Loss: 0.47579445468261566\n",
      "Epoch 141, Loss: 0.47284533919249566\n",
      "Epoch 142, Loss: 0.472809286012082\n",
      "Epoch 143, Loss: 0.47458738111232796\n",
      "Epoch 144, Loss: 0.4733380058918715\n",
      "Epoch 145, Loss: 0.4715229034484008\n",
      "Epoch 146, Loss: 0.47096730966267386\n",
      "Epoch 147, Loss: 0.47161469890467383\n",
      "Epoch 148, Loss: 0.47183531411129226\n",
      "Epoch 149, Loss: 0.470294933688076\n",
      "Epoch 150, Loss: 0.4697114144973765\n",
      "Epoch 151, Loss: 0.47018789053701304\n",
      "Epoch 152, Loss: 0.4700460953124117\n",
      "Epoch 153, Loss: 0.4693645740016508\n",
      "Epoch 154, Loss: 0.4684558104180481\n",
      "Epoch 155, Loss: 0.4680960384979934\n",
      "Epoch 156, Loss: 0.4682454254529793\n",
      "Epoch 157, Loss: 0.4682974379105488\n",
      "Epoch 158, Loss: 0.4681590010361841\n",
      "Epoch 159, Loss: 0.4673225124817593\n",
      "Epoch 160, Loss: 0.4666701428107949\n",
      "Epoch 161, Loss: 0.4663166784414499\n",
      "Epoch 162, Loss: 0.46624854535037\n",
      "Epoch 163, Loss: 0.4664611566834947\n",
      "Epoch 164, Loss: 0.46654532450127006\n",
      "Epoch 165, Loss: 0.46721845607201035\n",
      "Epoch 166, Loss: 0.4668208160006968\n",
      "Epoch 167, Loss: 0.466573749669782\n",
      "Epoch 168, Loss: 0.46506387643568103\n",
      "Epoch 169, Loss: 0.4641220516950567\n",
      "Epoch 170, Loss: 0.4644550172656552\n",
      "Epoch 171, Loss: 0.4654613047116713\n",
      "Epoch 172, Loss: 0.46880176576246846\n",
      "Epoch 173, Loss: 0.4685649824581709\n",
      "Epoch 174, Loss: 0.46606115996454733\n",
      "Epoch 175, Loss: 0.46328263242364126\n",
      "Epoch 176, Loss: 0.4660484187313888\n",
      "Epoch 177, Loss: 0.4667669789465077\n",
      "Epoch 178, Loss: 0.46222975993575777\n",
      "Epoch 179, Loss: 0.46412214801342555\n",
      "Epoch 180, Loss: 0.46867384291927394\n",
      "Epoch 181, Loss: 0.46316269230838986\n",
      "Epoch 182, Loss: 0.4619791093960296\n",
      "Epoch 183, Loss: 0.4650783107301587\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23773985432314487\n",
      "Valence RMSE: 0.24380742733386324\n",
      "Arousal RMSE: 0.23151331504693917\n",
      "Test R^2 score: tensor([0.3648, 0.6047], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.48475883309976925\n",
      "Num of epochs: 184\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Epoch 121, Loss: 0.4816228295571127\n",
      "Epoch 122, Loss: 0.4807618495484584\n",
      "Epoch 123, Loss: 0.48102030846138955\n",
      "Epoch 124, Loss: 0.4810176288364116\n",
      "Epoch 125, Loss: 0.48014338261661127\n",
      "Epoch 126, Loss: 0.47910314809270116\n",
      "Epoch 127, Loss: 0.47880795949683447\n",
      "Epoch 128, Loss: 0.47876177315142754\n",
      "Epoch 129, Loss: 0.47819597116449997\n",
      "Epoch 130, Loss: 0.47743214539542544\n",
      "Epoch 131, Loss: 0.47658476230340185\n",
      "Epoch 132, Loss: 0.4759644680184978\n",
      "Epoch 133, Loss: 0.4755635187712421\n",
      "Epoch 134, Loss: 0.47538264196584307\n",
      "Epoch 135, Loss: 0.4753673450596914\n",
      "Epoch 136, Loss: 0.4753697744168057\n",
      "Epoch 137, Loss: 0.4755973109583155\n",
      "Epoch 138, Loss: 0.4767626826605241\n",
      "Epoch 139, Loss: 0.47683528222287225\n",
      "Epoch 140, Loss: 0.47579445468261566\n",
      "Epoch 141, Loss: 0.47284533919249566\n",
      "Epoch 142, Loss: 0.472809286012082\n",
      "Epoch 143, Loss: 0.47458738111232796\n",
      "Epoch 144, Loss: 0.4733380058918715\n",
      "Epoch 145, Loss: 0.4715229034484008\n",
      "Epoch 146, Loss: 0.47096730966267386\n",
      "Epoch 147, Loss: 0.47161469890467383\n",
      "Epoch 148, Loss: 0.47183531411129226\n",
      "Epoch 149, Loss: 0.470294933688076\n",
      "Epoch 150, Loss: 0.4697114144973765\n",
      "Epoch 151, Loss: 0.47018789053701304\n",
      "Epoch 152, Loss: 0.4700460953124117\n",
      "Epoch 153, Loss: 0.4693645740016508\n",
      "Epoch 154, Loss: 0.4684558104180481\n",
      "Epoch 155, Loss: 0.4680960384979934\n",
      "Epoch 156, Loss: 0.4682454254529793\n",
      "Epoch 157, Loss: 0.4682974379105488\n",
      "Epoch 158, Loss: 0.4681590010361841\n",
      "Epoch 159, Loss: 0.4673225124817593\n",
      "Epoch 160, Loss: 0.4666701428107949\n",
      "Epoch 161, Loss: 0.4663166784414499\n",
      "Epoch 162, Loss: 0.46624854535037\n",
      "Epoch 163, Loss: 0.4664611566834947\n",
      "Epoch 164, Loss: 0.46654532450127006\n",
      "Epoch 165, Loss: 0.46721845607201035\n",
      "Epoch 166, Loss: 0.4668208160006968\n",
      "Epoch 167, Loss: 0.466573749669782\n",
      "Epoch 168, Loss: 0.46506387643568103\n",
      "Epoch 169, Loss: 0.4641220516950567\n",
      "Epoch 170, Loss: 0.4644550172656552\n",
      "Epoch 171, Loss: 0.4654613047116713\n",
      "Epoch 172, Loss: 0.46880176576246846\n",
      "Epoch 173, Loss: 0.4685649824581709\n",
      "Epoch 174, Loss: 0.46606115996454733\n",
      "Epoch 175, Loss: 0.46328263242364126\n",
      "Epoch 176, Loss: 0.4660484187313888\n",
      "Epoch 177, Loss: 0.4667669789465077\n",
      "Epoch 178, Loss: 0.46222975993575777\n",
      "Epoch 179, Loss: 0.46412214801342555\n",
      "Epoch 180, Loss: 0.46867384291927394\n",
      "Epoch 181, Loss: 0.46316269230838986\n",
      "Epoch 182, Loss: 0.4619791093960296\n",
      "Epoch 183, Loss: 0.4650783107301587\n",
      "Epoch 184, Loss: 0.4617210775403838\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23827065635259975\n",
      "Valence RMSE: 0.24381909015045647\n",
      "Arousal RMSE: 0.2325899022649112\n",
      "Test R^2 score: tensor([0.3647, 0.6010], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.48288600679652\n",
      "Num of epochs: 185\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Epoch 121, Loss: 0.4816228295571127\n",
      "Epoch 122, Loss: 0.4807618495484584\n",
      "Epoch 123, Loss: 0.48102030846138955\n",
      "Epoch 124, Loss: 0.4810176288364116\n",
      "Epoch 125, Loss: 0.48014338261661127\n",
      "Epoch 126, Loss: 0.47910314809270116\n",
      "Epoch 127, Loss: 0.47880795949683447\n",
      "Epoch 128, Loss: 0.47876177315142754\n",
      "Epoch 129, Loss: 0.47819597116449997\n",
      "Epoch 130, Loss: 0.47743214539542544\n",
      "Epoch 131, Loss: 0.47658476230340185\n",
      "Epoch 132, Loss: 0.4759644680184978\n",
      "Epoch 133, Loss: 0.4755635187712421\n",
      "Epoch 134, Loss: 0.47538264196584307\n",
      "Epoch 135, Loss: 0.4753673450596914\n",
      "Epoch 136, Loss: 0.4753697744168057\n",
      "Epoch 137, Loss: 0.4755973109583155\n",
      "Epoch 138, Loss: 0.4767626826605241\n",
      "Epoch 139, Loss: 0.47683528222287225\n",
      "Epoch 140, Loss: 0.47579445468261566\n",
      "Epoch 141, Loss: 0.47284533919249566\n",
      "Epoch 142, Loss: 0.472809286012082\n",
      "Epoch 143, Loss: 0.47458738111232796\n",
      "Epoch 144, Loss: 0.4733380058918715\n",
      "Epoch 145, Loss: 0.4715229034484008\n",
      "Epoch 146, Loss: 0.47096730966267386\n",
      "Epoch 147, Loss: 0.47161469890467383\n",
      "Epoch 148, Loss: 0.47183531411129226\n",
      "Epoch 149, Loss: 0.470294933688076\n",
      "Epoch 150, Loss: 0.4697114144973765\n",
      "Epoch 151, Loss: 0.47018789053701304\n",
      "Epoch 152, Loss: 0.4700460953124117\n",
      "Epoch 153, Loss: 0.4693645740016508\n",
      "Epoch 154, Loss: 0.4684558104180481\n",
      "Epoch 155, Loss: 0.4680960384979934\n",
      "Epoch 156, Loss: 0.4682454254529793\n",
      "Epoch 157, Loss: 0.4682974379105488\n",
      "Epoch 158, Loss: 0.4681590010361841\n",
      "Epoch 159, Loss: 0.4673225124817593\n",
      "Epoch 160, Loss: 0.4666701428107949\n",
      "Epoch 161, Loss: 0.4663166784414499\n",
      "Epoch 162, Loss: 0.46624854535037\n",
      "Epoch 163, Loss: 0.4664611566834947\n",
      "Epoch 164, Loss: 0.46654532450127006\n",
      "Epoch 165, Loss: 0.46721845607201035\n",
      "Epoch 166, Loss: 0.4668208160006968\n",
      "Epoch 167, Loss: 0.466573749669782\n",
      "Epoch 168, Loss: 0.46506387643568103\n",
      "Epoch 169, Loss: 0.4641220516950567\n",
      "Epoch 170, Loss: 0.4644550172656552\n",
      "Epoch 171, Loss: 0.4654613047116713\n",
      "Epoch 172, Loss: 0.46880176576246846\n",
      "Epoch 173, Loss: 0.4685649824581709\n",
      "Epoch 174, Loss: 0.46606115996454733\n",
      "Epoch 175, Loss: 0.46328263242364126\n",
      "Epoch 176, Loss: 0.4660484187313888\n",
      "Epoch 177, Loss: 0.4667669789465077\n",
      "Epoch 178, Loss: 0.46222975993575777\n",
      "Epoch 179, Loss: 0.46412214801342555\n",
      "Epoch 180, Loss: 0.46867384291927394\n",
      "Epoch 181, Loss: 0.46316269230838986\n",
      "Epoch 182, Loss: 0.4619791093960296\n",
      "Epoch 183, Loss: 0.4650783107301587\n",
      "Epoch 184, Loss: 0.4617210775403838\n",
      "Epoch 185, Loss: 0.4621011464022392\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.24157557683195183\n",
      "Valence RMSE: 0.25061991051703897\n",
      "Arousal RMSE: 0.23217919608744433\n",
      "Test R^2 score: tensor([0.3288, 0.6024], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4656236095860469\n",
      "Num of epochs: 186\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Epoch 121, Loss: 0.4816228295571127\n",
      "Epoch 122, Loss: 0.4807618495484584\n",
      "Epoch 123, Loss: 0.48102030846138955\n",
      "Epoch 124, Loss: 0.4810176288364116\n",
      "Epoch 125, Loss: 0.48014338261661127\n",
      "Epoch 126, Loss: 0.47910314809270116\n",
      "Epoch 127, Loss: 0.47880795949683447\n",
      "Epoch 128, Loss: 0.47876177315142754\n",
      "Epoch 129, Loss: 0.47819597116449997\n",
      "Epoch 130, Loss: 0.47743214539542544\n",
      "Epoch 131, Loss: 0.47658476230340185\n",
      "Epoch 132, Loss: 0.4759644680184978\n",
      "Epoch 133, Loss: 0.4755635187712421\n",
      "Epoch 134, Loss: 0.47538264196584307\n",
      "Epoch 135, Loss: 0.4753673450596914\n",
      "Epoch 136, Loss: 0.4753697744168057\n",
      "Epoch 137, Loss: 0.4755973109583155\n",
      "Epoch 138, Loss: 0.4767626826605241\n",
      "Epoch 139, Loss: 0.47683528222287225\n",
      "Epoch 140, Loss: 0.47579445468261566\n",
      "Epoch 141, Loss: 0.47284533919249566\n",
      "Epoch 142, Loss: 0.472809286012082\n",
      "Epoch 143, Loss: 0.47458738111232796\n",
      "Epoch 144, Loss: 0.4733380058918715\n",
      "Epoch 145, Loss: 0.4715229034484008\n",
      "Epoch 146, Loss: 0.47096730966267386\n",
      "Epoch 147, Loss: 0.47161469890467383\n",
      "Epoch 148, Loss: 0.47183531411129226\n",
      "Epoch 149, Loss: 0.470294933688076\n",
      "Epoch 150, Loss: 0.4697114144973765\n",
      "Epoch 151, Loss: 0.47018789053701304\n",
      "Epoch 152, Loss: 0.4700460953124117\n",
      "Epoch 153, Loss: 0.4693645740016508\n",
      "Epoch 154, Loss: 0.4684558104180481\n",
      "Epoch 155, Loss: 0.4680960384979934\n",
      "Epoch 156, Loss: 0.4682454254529793\n",
      "Epoch 157, Loss: 0.4682974379105488\n",
      "Epoch 158, Loss: 0.4681590010361841\n",
      "Epoch 159, Loss: 0.4673225124817593\n",
      "Epoch 160, Loss: 0.4666701428107949\n",
      "Epoch 161, Loss: 0.4663166784414499\n",
      "Epoch 162, Loss: 0.46624854535037\n",
      "Epoch 163, Loss: 0.4664611566834947\n",
      "Epoch 164, Loss: 0.46654532450127006\n",
      "Epoch 165, Loss: 0.46721845607201035\n",
      "Epoch 166, Loss: 0.4668208160006968\n",
      "Epoch 167, Loss: 0.466573749669782\n",
      "Epoch 168, Loss: 0.46506387643568103\n",
      "Epoch 169, Loss: 0.4641220516950567\n",
      "Epoch 170, Loss: 0.4644550172656552\n",
      "Epoch 171, Loss: 0.4654613047116713\n",
      "Epoch 172, Loss: 0.46880176576246846\n",
      "Epoch 173, Loss: 0.4685649824581709\n",
      "Epoch 174, Loss: 0.46606115996454733\n",
      "Epoch 175, Loss: 0.46328263242364126\n",
      "Epoch 176, Loss: 0.4660484187313888\n",
      "Epoch 177, Loss: 0.4667669789465077\n",
      "Epoch 178, Loss: 0.46222975993575777\n",
      "Epoch 179, Loss: 0.46412214801342555\n",
      "Epoch 180, Loss: 0.46867384291927394\n",
      "Epoch 181, Loss: 0.46316269230838986\n",
      "Epoch 182, Loss: 0.4619791093960296\n",
      "Epoch 183, Loss: 0.4650783107301587\n",
      "Epoch 184, Loss: 0.4617210775403838\n",
      "Epoch 185, Loss: 0.4621011464022392\n",
      "Epoch 186, Loss: 0.4641367239628427\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2383891642136923\n",
      "Valence RMSE: 0.24497882361609702\n",
      "Arousal RMSE: 0.2316120964213198\n",
      "Test R^2 score: tensor([0.3587, 0.6044], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4815309447043494\n",
      "Num of epochs: 187\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Epoch 121, Loss: 0.4816228295571127\n",
      "Epoch 122, Loss: 0.4807618495484584\n",
      "Epoch 123, Loss: 0.48102030846138955\n",
      "Epoch 124, Loss: 0.4810176288364116\n",
      "Epoch 125, Loss: 0.48014338261661127\n",
      "Epoch 126, Loss: 0.47910314809270116\n",
      "Epoch 127, Loss: 0.47880795949683447\n",
      "Epoch 128, Loss: 0.47876177315142754\n",
      "Epoch 129, Loss: 0.47819597116449997\n",
      "Epoch 130, Loss: 0.47743214539542544\n",
      "Epoch 131, Loss: 0.47658476230340185\n",
      "Epoch 132, Loss: 0.4759644680184978\n",
      "Epoch 133, Loss: 0.4755635187712421\n",
      "Epoch 134, Loss: 0.47538264196584307\n",
      "Epoch 135, Loss: 0.4753673450596914\n",
      "Epoch 136, Loss: 0.4753697744168057\n",
      "Epoch 137, Loss: 0.4755973109583155\n",
      "Epoch 138, Loss: 0.4767626826605241\n",
      "Epoch 139, Loss: 0.47683528222287225\n",
      "Epoch 140, Loss: 0.47579445468261566\n",
      "Epoch 141, Loss: 0.47284533919249566\n",
      "Epoch 142, Loss: 0.472809286012082\n",
      "Epoch 143, Loss: 0.47458738111232796\n",
      "Epoch 144, Loss: 0.4733380058918715\n",
      "Epoch 145, Loss: 0.4715229034484008\n",
      "Epoch 146, Loss: 0.47096730966267386\n",
      "Epoch 147, Loss: 0.47161469890467383\n",
      "Epoch 148, Loss: 0.47183531411129226\n",
      "Epoch 149, Loss: 0.470294933688076\n",
      "Epoch 150, Loss: 0.4697114144973765\n",
      "Epoch 151, Loss: 0.47018789053701304\n",
      "Epoch 152, Loss: 0.4700460953124117\n",
      "Epoch 153, Loss: 0.4693645740016508\n",
      "Epoch 154, Loss: 0.4684558104180481\n",
      "Epoch 155, Loss: 0.4680960384979934\n",
      "Epoch 156, Loss: 0.4682454254529793\n",
      "Epoch 157, Loss: 0.4682974379105488\n",
      "Epoch 158, Loss: 0.4681590010361841\n",
      "Epoch 159, Loss: 0.4673225124817593\n",
      "Epoch 160, Loss: 0.4666701428107949\n",
      "Epoch 161, Loss: 0.4663166784414499\n",
      "Epoch 162, Loss: 0.46624854535037\n",
      "Epoch 163, Loss: 0.4664611566834947\n",
      "Epoch 164, Loss: 0.46654532450127006\n",
      "Epoch 165, Loss: 0.46721845607201035\n",
      "Epoch 166, Loss: 0.4668208160006968\n",
      "Epoch 167, Loss: 0.466573749669782\n",
      "Epoch 168, Loss: 0.46506387643568103\n",
      "Epoch 169, Loss: 0.4641220516950567\n",
      "Epoch 170, Loss: 0.4644550172656552\n",
      "Epoch 171, Loss: 0.4654613047116713\n",
      "Epoch 172, Loss: 0.46880176576246846\n",
      "Epoch 173, Loss: 0.4685649824581709\n",
      "Epoch 174, Loss: 0.46606115996454733\n",
      "Epoch 175, Loss: 0.46328263242364126\n",
      "Epoch 176, Loss: 0.4660484187313888\n",
      "Epoch 177, Loss: 0.4667669789465077\n",
      "Epoch 178, Loss: 0.46222975993575777\n",
      "Epoch 179, Loss: 0.46412214801342555\n",
      "Epoch 180, Loss: 0.46867384291927394\n",
      "Epoch 181, Loss: 0.46316269230838986\n",
      "Epoch 182, Loss: 0.4619791093960296\n",
      "Epoch 183, Loss: 0.4650783107301587\n",
      "Epoch 184, Loss: 0.4617210775403838\n",
      "Epoch 185, Loss: 0.4621011464022392\n",
      "Epoch 186, Loss: 0.4641367239628427\n",
      "Epoch 187, Loss: 0.46071131054180564\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2392267020050709\n",
      "Valence RMSE: 0.24511040318438437\n",
      "Arousal RMSE: 0.23319459718277036\n",
      "Test R^2 score: tensor([0.3580, 0.5990], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.478474047239176\n",
      "Num of epochs: 188\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Epoch 121, Loss: 0.4816228295571127\n",
      "Epoch 122, Loss: 0.4807618495484584\n",
      "Epoch 123, Loss: 0.48102030846138955\n",
      "Epoch 124, Loss: 0.4810176288364116\n",
      "Epoch 125, Loss: 0.48014338261661127\n",
      "Epoch 126, Loss: 0.47910314809270116\n",
      "Epoch 127, Loss: 0.47880795949683447\n",
      "Epoch 128, Loss: 0.47876177315142754\n",
      "Epoch 129, Loss: 0.47819597116449997\n",
      "Epoch 130, Loss: 0.47743214539542544\n",
      "Epoch 131, Loss: 0.47658476230340185\n",
      "Epoch 132, Loss: 0.4759644680184978\n",
      "Epoch 133, Loss: 0.4755635187712421\n",
      "Epoch 134, Loss: 0.47538264196584307\n",
      "Epoch 135, Loss: 0.4753673450596914\n",
      "Epoch 136, Loss: 0.4753697744168057\n",
      "Epoch 137, Loss: 0.4755973109583155\n",
      "Epoch 138, Loss: 0.4767626826605241\n",
      "Epoch 139, Loss: 0.47683528222287225\n",
      "Epoch 140, Loss: 0.47579445468261566\n",
      "Epoch 141, Loss: 0.47284533919249566\n",
      "Epoch 142, Loss: 0.472809286012082\n",
      "Epoch 143, Loss: 0.47458738111232796\n",
      "Epoch 144, Loss: 0.4733380058918715\n",
      "Epoch 145, Loss: 0.4715229034484008\n",
      "Epoch 146, Loss: 0.47096730966267386\n",
      "Epoch 147, Loss: 0.47161469890467383\n",
      "Epoch 148, Loss: 0.47183531411129226\n",
      "Epoch 149, Loss: 0.470294933688076\n",
      "Epoch 150, Loss: 0.4697114144973765\n",
      "Epoch 151, Loss: 0.47018789053701304\n",
      "Epoch 152, Loss: 0.4700460953124117\n",
      "Epoch 153, Loss: 0.4693645740016508\n",
      "Epoch 154, Loss: 0.4684558104180481\n",
      "Epoch 155, Loss: 0.4680960384979934\n",
      "Epoch 156, Loss: 0.4682454254529793\n",
      "Epoch 157, Loss: 0.4682974379105488\n",
      "Epoch 158, Loss: 0.4681590010361841\n",
      "Epoch 159, Loss: 0.4673225124817593\n",
      "Epoch 160, Loss: 0.4666701428107949\n",
      "Epoch 161, Loss: 0.4663166784414499\n",
      "Epoch 162, Loss: 0.46624854535037\n",
      "Epoch 163, Loss: 0.4664611566834947\n",
      "Epoch 164, Loss: 0.46654532450127006\n",
      "Epoch 165, Loss: 0.46721845607201035\n",
      "Epoch 166, Loss: 0.4668208160006968\n",
      "Epoch 167, Loss: 0.466573749669782\n",
      "Epoch 168, Loss: 0.46506387643568103\n",
      "Epoch 169, Loss: 0.4641220516950567\n",
      "Epoch 170, Loss: 0.4644550172656552\n",
      "Epoch 171, Loss: 0.4654613047116713\n",
      "Epoch 172, Loss: 0.46880176576246846\n",
      "Epoch 173, Loss: 0.4685649824581709\n",
      "Epoch 174, Loss: 0.46606115996454733\n",
      "Epoch 175, Loss: 0.46328263242364126\n",
      "Epoch 176, Loss: 0.4660484187313888\n",
      "Epoch 177, Loss: 0.4667669789465077\n",
      "Epoch 178, Loss: 0.46222975993575777\n",
      "Epoch 179, Loss: 0.46412214801342555\n",
      "Epoch 180, Loss: 0.46867384291927394\n",
      "Epoch 181, Loss: 0.46316269230838986\n",
      "Epoch 182, Loss: 0.4619791093960296\n",
      "Epoch 183, Loss: 0.4650783107301587\n",
      "Epoch 184, Loss: 0.4617210775403838\n",
      "Epoch 185, Loss: 0.4621011464022392\n",
      "Epoch 186, Loss: 0.4641367239628427\n",
      "Epoch 187, Loss: 0.46071131054180564\n",
      "Epoch 188, Loss: 0.46145892957673257\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.24315134206105843\n",
      "Valence RMSE: 0.2529965555188419\n",
      "Arousal RMSE: 0.2328903029063022\n",
      "Test R^2 score: tensor([0.3160, 0.6000], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4580089942026482\n",
      "Num of epochs: 189\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Epoch 121, Loss: 0.4816228295571127\n",
      "Epoch 122, Loss: 0.4807618495484584\n",
      "Epoch 123, Loss: 0.48102030846138955\n",
      "Epoch 124, Loss: 0.4810176288364116\n",
      "Epoch 125, Loss: 0.48014338261661127\n",
      "Epoch 126, Loss: 0.47910314809270116\n",
      "Epoch 127, Loss: 0.47880795949683447\n",
      "Epoch 128, Loss: 0.47876177315142754\n",
      "Epoch 129, Loss: 0.47819597116449997\n",
      "Epoch 130, Loss: 0.47743214539542544\n",
      "Epoch 131, Loss: 0.47658476230340185\n",
      "Epoch 132, Loss: 0.4759644680184978\n",
      "Epoch 133, Loss: 0.4755635187712421\n",
      "Epoch 134, Loss: 0.47538264196584307\n",
      "Epoch 135, Loss: 0.4753673450596914\n",
      "Epoch 136, Loss: 0.4753697744168057\n",
      "Epoch 137, Loss: 0.4755973109583155\n",
      "Epoch 138, Loss: 0.4767626826605241\n",
      "Epoch 139, Loss: 0.47683528222287225\n",
      "Epoch 140, Loss: 0.47579445468261566\n",
      "Epoch 141, Loss: 0.47284533919249566\n",
      "Epoch 142, Loss: 0.472809286012082\n",
      "Epoch 143, Loss: 0.47458738111232796\n",
      "Epoch 144, Loss: 0.4733380058918715\n",
      "Epoch 145, Loss: 0.4715229034484008\n",
      "Epoch 146, Loss: 0.47096730966267386\n",
      "Epoch 147, Loss: 0.47161469890467383\n",
      "Epoch 148, Loss: 0.47183531411129226\n",
      "Epoch 149, Loss: 0.470294933688076\n",
      "Epoch 150, Loss: 0.4697114144973765\n",
      "Epoch 151, Loss: 0.47018789053701304\n",
      "Epoch 152, Loss: 0.4700460953124117\n",
      "Epoch 153, Loss: 0.4693645740016508\n",
      "Epoch 154, Loss: 0.4684558104180481\n",
      "Epoch 155, Loss: 0.4680960384979934\n",
      "Epoch 156, Loss: 0.4682454254529793\n",
      "Epoch 157, Loss: 0.4682974379105488\n",
      "Epoch 158, Loss: 0.4681590010361841\n",
      "Epoch 159, Loss: 0.4673225124817593\n",
      "Epoch 160, Loss: 0.4666701428107949\n",
      "Epoch 161, Loss: 0.4663166784414499\n",
      "Epoch 162, Loss: 0.46624854535037\n",
      "Epoch 163, Loss: 0.4664611566834947\n",
      "Epoch 164, Loss: 0.46654532450127006\n",
      "Epoch 165, Loss: 0.46721845607201035\n",
      "Epoch 166, Loss: 0.4668208160006968\n",
      "Epoch 167, Loss: 0.466573749669782\n",
      "Epoch 168, Loss: 0.46506387643568103\n",
      "Epoch 169, Loss: 0.4641220516950567\n",
      "Epoch 170, Loss: 0.4644550172656552\n",
      "Epoch 171, Loss: 0.4654613047116713\n",
      "Epoch 172, Loss: 0.46880176576246846\n",
      "Epoch 173, Loss: 0.4685649824581709\n",
      "Epoch 174, Loss: 0.46606115996454733\n",
      "Epoch 175, Loss: 0.46328263242364126\n",
      "Epoch 176, Loss: 0.4660484187313888\n",
      "Epoch 177, Loss: 0.4667669789465077\n",
      "Epoch 178, Loss: 0.46222975993575777\n",
      "Epoch 179, Loss: 0.46412214801342555\n",
      "Epoch 180, Loss: 0.46867384291927394\n",
      "Epoch 181, Loss: 0.46316269230838986\n",
      "Epoch 182, Loss: 0.4619791093960296\n",
      "Epoch 183, Loss: 0.4650783107301587\n",
      "Epoch 184, Loss: 0.4617210775403838\n",
      "Epoch 185, Loss: 0.4621011464022392\n",
      "Epoch 186, Loss: 0.4641367239628427\n",
      "Epoch 187, Loss: 0.46071131054180564\n",
      "Epoch 188, Loss: 0.46145892957673257\n",
      "Epoch 189, Loss: 0.464304908222076\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2385071839120708\n",
      "Valence RMSE: 0.24472258790484597\n",
      "Arousal RMSE: 0.23212541550740978\n",
      "Test R^2 score: tensor([0.3600, 0.6026], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.481323590007661\n",
      "Num of epochs: 190\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Epoch 121, Loss: 0.4816228295571127\n",
      "Epoch 122, Loss: 0.4807618495484584\n",
      "Epoch 123, Loss: 0.48102030846138955\n",
      "Epoch 124, Loss: 0.4810176288364116\n",
      "Epoch 125, Loss: 0.48014338261661127\n",
      "Epoch 126, Loss: 0.47910314809270116\n",
      "Epoch 127, Loss: 0.47880795949683447\n",
      "Epoch 128, Loss: 0.47876177315142754\n",
      "Epoch 129, Loss: 0.47819597116449997\n",
      "Epoch 130, Loss: 0.47743214539542544\n",
      "Epoch 131, Loss: 0.47658476230340185\n",
      "Epoch 132, Loss: 0.4759644680184978\n",
      "Epoch 133, Loss: 0.4755635187712421\n",
      "Epoch 134, Loss: 0.47538264196584307\n",
      "Epoch 135, Loss: 0.4753673450596914\n",
      "Epoch 136, Loss: 0.4753697744168057\n",
      "Epoch 137, Loss: 0.4755973109583155\n",
      "Epoch 138, Loss: 0.4767626826605241\n",
      "Epoch 139, Loss: 0.47683528222287225\n",
      "Epoch 140, Loss: 0.47579445468261566\n",
      "Epoch 141, Loss: 0.47284533919249566\n",
      "Epoch 142, Loss: 0.472809286012082\n",
      "Epoch 143, Loss: 0.47458738111232796\n",
      "Epoch 144, Loss: 0.4733380058918715\n",
      "Epoch 145, Loss: 0.4715229034484008\n",
      "Epoch 146, Loss: 0.47096730966267386\n",
      "Epoch 147, Loss: 0.47161469890467383\n",
      "Epoch 148, Loss: 0.47183531411129226\n",
      "Epoch 149, Loss: 0.470294933688076\n",
      "Epoch 150, Loss: 0.4697114144973765\n",
      "Epoch 151, Loss: 0.47018789053701304\n",
      "Epoch 152, Loss: 0.4700460953124117\n",
      "Epoch 153, Loss: 0.4693645740016508\n",
      "Epoch 154, Loss: 0.4684558104180481\n",
      "Epoch 155, Loss: 0.4680960384979934\n",
      "Epoch 156, Loss: 0.4682454254529793\n",
      "Epoch 157, Loss: 0.4682974379105488\n",
      "Epoch 158, Loss: 0.4681590010361841\n",
      "Epoch 159, Loss: 0.4673225124817593\n",
      "Epoch 160, Loss: 0.4666701428107949\n",
      "Epoch 161, Loss: 0.4663166784414499\n",
      "Epoch 162, Loss: 0.46624854535037\n",
      "Epoch 163, Loss: 0.4664611566834947\n",
      "Epoch 164, Loss: 0.46654532450127006\n",
      "Epoch 165, Loss: 0.46721845607201035\n",
      "Epoch 166, Loss: 0.4668208160006968\n",
      "Epoch 167, Loss: 0.466573749669782\n",
      "Epoch 168, Loss: 0.46506387643568103\n",
      "Epoch 169, Loss: 0.4641220516950567\n",
      "Epoch 170, Loss: 0.4644550172656552\n",
      "Epoch 171, Loss: 0.4654613047116713\n",
      "Epoch 172, Loss: 0.46880176576246846\n",
      "Epoch 173, Loss: 0.4685649824581709\n",
      "Epoch 174, Loss: 0.46606115996454733\n",
      "Epoch 175, Loss: 0.46328263242364126\n",
      "Epoch 176, Loss: 0.4660484187313888\n",
      "Epoch 177, Loss: 0.4667669789465077\n",
      "Epoch 178, Loss: 0.46222975993575777\n",
      "Epoch 179, Loss: 0.46412214801342555\n",
      "Epoch 180, Loss: 0.46867384291927394\n",
      "Epoch 181, Loss: 0.46316269230838986\n",
      "Epoch 182, Loss: 0.4619791093960296\n",
      "Epoch 183, Loss: 0.4650783107301587\n",
      "Epoch 184, Loss: 0.4617210775403838\n",
      "Epoch 185, Loss: 0.4621011464022392\n",
      "Epoch 186, Loss: 0.4641367239628427\n",
      "Epoch 187, Loss: 0.46071131054180564\n",
      "Epoch 188, Loss: 0.46145892957673257\n",
      "Epoch 189, Loss: 0.464304908222076\n",
      "Epoch 190, Loss: 0.4606605441278432\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23800741493734145\n",
      "Valence RMSE: 0.24465315463007647\n",
      "Arousal RMSE: 0.23117070112768087\n",
      "Test R^2 score: tensor([0.3604, 0.6059], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4831361682989553\n",
      "Num of epochs: 191\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Epoch 121, Loss: 0.4816228295571127\n",
      "Epoch 122, Loss: 0.4807618495484584\n",
      "Epoch 123, Loss: 0.48102030846138955\n",
      "Epoch 124, Loss: 0.4810176288364116\n",
      "Epoch 125, Loss: 0.48014338261661127\n",
      "Epoch 126, Loss: 0.47910314809270116\n",
      "Epoch 127, Loss: 0.47880795949683447\n",
      "Epoch 128, Loss: 0.47876177315142754\n",
      "Epoch 129, Loss: 0.47819597116449997\n",
      "Epoch 130, Loss: 0.47743214539542544\n",
      "Epoch 131, Loss: 0.47658476230340185\n",
      "Epoch 132, Loss: 0.4759644680184978\n",
      "Epoch 133, Loss: 0.4755635187712421\n",
      "Epoch 134, Loss: 0.47538264196584307\n",
      "Epoch 135, Loss: 0.4753673450596914\n",
      "Epoch 136, Loss: 0.4753697744168057\n",
      "Epoch 137, Loss: 0.4755973109583155\n",
      "Epoch 138, Loss: 0.4767626826605241\n",
      "Epoch 139, Loss: 0.47683528222287225\n",
      "Epoch 140, Loss: 0.47579445468261566\n",
      "Epoch 141, Loss: 0.47284533919249566\n",
      "Epoch 142, Loss: 0.472809286012082\n",
      "Epoch 143, Loss: 0.47458738111232796\n",
      "Epoch 144, Loss: 0.4733380058918715\n",
      "Epoch 145, Loss: 0.4715229034484008\n",
      "Epoch 146, Loss: 0.47096730966267386\n",
      "Epoch 147, Loss: 0.47161469890467383\n",
      "Epoch 148, Loss: 0.47183531411129226\n",
      "Epoch 149, Loss: 0.470294933688076\n",
      "Epoch 150, Loss: 0.4697114144973765\n",
      "Epoch 151, Loss: 0.47018789053701304\n",
      "Epoch 152, Loss: 0.4700460953124117\n",
      "Epoch 153, Loss: 0.4693645740016508\n",
      "Epoch 154, Loss: 0.4684558104180481\n",
      "Epoch 155, Loss: 0.4680960384979934\n",
      "Epoch 156, Loss: 0.4682454254529793\n",
      "Epoch 157, Loss: 0.4682974379105488\n",
      "Epoch 158, Loss: 0.4681590010361841\n",
      "Epoch 159, Loss: 0.4673225124817593\n",
      "Epoch 160, Loss: 0.4666701428107949\n",
      "Epoch 161, Loss: 0.4663166784414499\n",
      "Epoch 162, Loss: 0.46624854535037\n",
      "Epoch 163, Loss: 0.4664611566834947\n",
      "Epoch 164, Loss: 0.46654532450127006\n",
      "Epoch 165, Loss: 0.46721845607201035\n",
      "Epoch 166, Loss: 0.4668208160006968\n",
      "Epoch 167, Loss: 0.466573749669782\n",
      "Epoch 168, Loss: 0.46506387643568103\n",
      "Epoch 169, Loss: 0.4641220516950567\n",
      "Epoch 170, Loss: 0.4644550172656552\n",
      "Epoch 171, Loss: 0.4654613047116713\n",
      "Epoch 172, Loss: 0.46880176576246846\n",
      "Epoch 173, Loss: 0.4685649824581709\n",
      "Epoch 174, Loss: 0.46606115996454733\n",
      "Epoch 175, Loss: 0.46328263242364126\n",
      "Epoch 176, Loss: 0.4660484187313888\n",
      "Epoch 177, Loss: 0.4667669789465077\n",
      "Epoch 178, Loss: 0.46222975993575777\n",
      "Epoch 179, Loss: 0.46412214801342555\n",
      "Epoch 180, Loss: 0.46867384291927394\n",
      "Epoch 181, Loss: 0.46316269230838986\n",
      "Epoch 182, Loss: 0.4619791093960296\n",
      "Epoch 183, Loss: 0.4650783107301587\n",
      "Epoch 184, Loss: 0.4617210775403838\n",
      "Epoch 185, Loss: 0.4621011464022392\n",
      "Epoch 186, Loss: 0.4641367239628427\n",
      "Epoch 187, Loss: 0.46071131054180564\n",
      "Epoch 188, Loss: 0.46145892957673257\n",
      "Epoch 189, Loss: 0.464304908222076\n",
      "Epoch 190, Loss: 0.4606605441278432\n",
      "Epoch 191, Loss: 0.4601386308130386\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.24029938602757284\n",
      "Valence RMSE: 0.250005288105508\n",
      "Arousal RMSE: 0.23018459064355043\n",
      "Test R^2 score: tensor([0.3321, 0.6092], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4706683386304508\n",
      "Num of epochs: 192\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Epoch 121, Loss: 0.4816228295571127\n",
      "Epoch 122, Loss: 0.4807618495484584\n",
      "Epoch 123, Loss: 0.48102030846138955\n",
      "Epoch 124, Loss: 0.4810176288364116\n",
      "Epoch 125, Loss: 0.48014338261661127\n",
      "Epoch 126, Loss: 0.47910314809270116\n",
      "Epoch 127, Loss: 0.47880795949683447\n",
      "Epoch 128, Loss: 0.47876177315142754\n",
      "Epoch 129, Loss: 0.47819597116449997\n",
      "Epoch 130, Loss: 0.47743214539542544\n",
      "Epoch 131, Loss: 0.47658476230340185\n",
      "Epoch 132, Loss: 0.4759644680184978\n",
      "Epoch 133, Loss: 0.4755635187712421\n",
      "Epoch 134, Loss: 0.47538264196584307\n",
      "Epoch 135, Loss: 0.4753673450596914\n",
      "Epoch 136, Loss: 0.4753697744168057\n",
      "Epoch 137, Loss: 0.4755973109583155\n",
      "Epoch 138, Loss: 0.4767626826605241\n",
      "Epoch 139, Loss: 0.47683528222287225\n",
      "Epoch 140, Loss: 0.47579445468261566\n",
      "Epoch 141, Loss: 0.47284533919249566\n",
      "Epoch 142, Loss: 0.472809286012082\n",
      "Epoch 143, Loss: 0.47458738111232796\n",
      "Epoch 144, Loss: 0.4733380058918715\n",
      "Epoch 145, Loss: 0.4715229034484008\n",
      "Epoch 146, Loss: 0.47096730966267386\n",
      "Epoch 147, Loss: 0.47161469890467383\n",
      "Epoch 148, Loss: 0.47183531411129226\n",
      "Epoch 149, Loss: 0.470294933688076\n",
      "Epoch 150, Loss: 0.4697114144973765\n",
      "Epoch 151, Loss: 0.47018789053701304\n",
      "Epoch 152, Loss: 0.4700460953124117\n",
      "Epoch 153, Loss: 0.4693645740016508\n",
      "Epoch 154, Loss: 0.4684558104180481\n",
      "Epoch 155, Loss: 0.4680960384979934\n",
      "Epoch 156, Loss: 0.4682454254529793\n",
      "Epoch 157, Loss: 0.4682974379105488\n",
      "Epoch 158, Loss: 0.4681590010361841\n",
      "Epoch 159, Loss: 0.4673225124817593\n",
      "Epoch 160, Loss: 0.4666701428107949\n",
      "Epoch 161, Loss: 0.4663166784414499\n",
      "Epoch 162, Loss: 0.46624854535037\n",
      "Epoch 163, Loss: 0.4664611566834947\n",
      "Epoch 164, Loss: 0.46654532450127006\n",
      "Epoch 165, Loss: 0.46721845607201035\n",
      "Epoch 166, Loss: 0.4668208160006968\n",
      "Epoch 167, Loss: 0.466573749669782\n",
      "Epoch 168, Loss: 0.46506387643568103\n",
      "Epoch 169, Loss: 0.4641220516950567\n",
      "Epoch 170, Loss: 0.4644550172656552\n",
      "Epoch 171, Loss: 0.4654613047116713\n",
      "Epoch 172, Loss: 0.46880176576246846\n",
      "Epoch 173, Loss: 0.4685649824581709\n",
      "Epoch 174, Loss: 0.46606115996454733\n",
      "Epoch 175, Loss: 0.46328263242364126\n",
      "Epoch 176, Loss: 0.4660484187313888\n",
      "Epoch 177, Loss: 0.4667669789465077\n",
      "Epoch 178, Loss: 0.46222975993575777\n",
      "Epoch 179, Loss: 0.46412214801342555\n",
      "Epoch 180, Loss: 0.46867384291927394\n",
      "Epoch 181, Loss: 0.46316269230838986\n",
      "Epoch 182, Loss: 0.4619791093960296\n",
      "Epoch 183, Loss: 0.4650783107301587\n",
      "Epoch 184, Loss: 0.4617210775403838\n",
      "Epoch 185, Loss: 0.4621011464022392\n",
      "Epoch 186, Loss: 0.4641367239628427\n",
      "Epoch 187, Loss: 0.46071131054180564\n",
      "Epoch 188, Loss: 0.46145892957673257\n",
      "Epoch 189, Loss: 0.464304908222076\n",
      "Epoch 190, Loss: 0.4606605441278432\n",
      "Epoch 191, Loss: 0.4601386308130386\n",
      "Epoch 192, Loss: 0.46193430495315263\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23765886801441927\n",
      "Valence RMSE: 0.24446903344566553\n",
      "Arousal RMSE: 0.23064771140839957\n",
      "Test R^2 score: tensor([0.3614, 0.6077], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.484507968843743\n",
      "Num of epochs: 193\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Epoch 121, Loss: 0.4816228295571127\n",
      "Epoch 122, Loss: 0.4807618495484584\n",
      "Epoch 123, Loss: 0.48102030846138955\n",
      "Epoch 124, Loss: 0.4810176288364116\n",
      "Epoch 125, Loss: 0.48014338261661127\n",
      "Epoch 126, Loss: 0.47910314809270116\n",
      "Epoch 127, Loss: 0.47880795949683447\n",
      "Epoch 128, Loss: 0.47876177315142754\n",
      "Epoch 129, Loss: 0.47819597116449997\n",
      "Epoch 130, Loss: 0.47743214539542544\n",
      "Epoch 131, Loss: 0.47658476230340185\n",
      "Epoch 132, Loss: 0.4759644680184978\n",
      "Epoch 133, Loss: 0.4755635187712421\n",
      "Epoch 134, Loss: 0.47538264196584307\n",
      "Epoch 135, Loss: 0.4753673450596914\n",
      "Epoch 136, Loss: 0.4753697744168057\n",
      "Epoch 137, Loss: 0.4755973109583155\n",
      "Epoch 138, Loss: 0.4767626826605241\n",
      "Epoch 139, Loss: 0.47683528222287225\n",
      "Epoch 140, Loss: 0.47579445468261566\n",
      "Epoch 141, Loss: 0.47284533919249566\n",
      "Epoch 142, Loss: 0.472809286012082\n",
      "Epoch 143, Loss: 0.47458738111232796\n",
      "Epoch 144, Loss: 0.4733380058918715\n",
      "Epoch 145, Loss: 0.4715229034484008\n",
      "Epoch 146, Loss: 0.47096730966267386\n",
      "Epoch 147, Loss: 0.47161469890467383\n",
      "Epoch 148, Loss: 0.47183531411129226\n",
      "Epoch 149, Loss: 0.470294933688076\n",
      "Epoch 150, Loss: 0.4697114144973765\n",
      "Epoch 151, Loss: 0.47018789053701304\n",
      "Epoch 152, Loss: 0.4700460953124117\n",
      "Epoch 153, Loss: 0.4693645740016508\n",
      "Epoch 154, Loss: 0.4684558104180481\n",
      "Epoch 155, Loss: 0.4680960384979934\n",
      "Epoch 156, Loss: 0.4682454254529793\n",
      "Epoch 157, Loss: 0.4682974379105488\n",
      "Epoch 158, Loss: 0.4681590010361841\n",
      "Epoch 159, Loss: 0.4673225124817593\n",
      "Epoch 160, Loss: 0.4666701428107949\n",
      "Epoch 161, Loss: 0.4663166784414499\n",
      "Epoch 162, Loss: 0.46624854535037\n",
      "Epoch 163, Loss: 0.4664611566834947\n",
      "Epoch 164, Loss: 0.46654532450127006\n",
      "Epoch 165, Loss: 0.46721845607201035\n",
      "Epoch 166, Loss: 0.4668208160006968\n",
      "Epoch 167, Loss: 0.466573749669782\n",
      "Epoch 168, Loss: 0.46506387643568103\n",
      "Epoch 169, Loss: 0.4641220516950567\n",
      "Epoch 170, Loss: 0.4644550172656552\n",
      "Epoch 171, Loss: 0.4654613047116713\n",
      "Epoch 172, Loss: 0.46880176576246846\n",
      "Epoch 173, Loss: 0.4685649824581709\n",
      "Epoch 174, Loss: 0.46606115996454733\n",
      "Epoch 175, Loss: 0.46328263242364126\n",
      "Epoch 176, Loss: 0.4660484187313888\n",
      "Epoch 177, Loss: 0.4667669789465077\n",
      "Epoch 178, Loss: 0.46222975993575777\n",
      "Epoch 179, Loss: 0.46412214801342555\n",
      "Epoch 180, Loss: 0.46867384291927394\n",
      "Epoch 181, Loss: 0.46316269230838986\n",
      "Epoch 182, Loss: 0.4619791093960296\n",
      "Epoch 183, Loss: 0.4650783107301587\n",
      "Epoch 184, Loss: 0.4617210775403838\n",
      "Epoch 185, Loss: 0.4621011464022392\n",
      "Epoch 186, Loss: 0.4641367239628427\n",
      "Epoch 187, Loss: 0.46071131054180564\n",
      "Epoch 188, Loss: 0.46145892957673257\n",
      "Epoch 189, Loss: 0.464304908222076\n",
      "Epoch 190, Loss: 0.4606605441278432\n",
      "Epoch 191, Loss: 0.4601386308130386\n",
      "Epoch 192, Loss: 0.46193430495315263\n",
      "Epoch 193, Loss: 0.45977696912799226\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23770551879025892\n",
      "Valence RMSE: 0.24428691905556407\n",
      "Arousal RMSE: 0.23093663309451912\n",
      "Test R^2 score: tensor([0.3623, 0.6067], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4844917767063218\n",
      "Num of epochs: 194\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Epoch 121, Loss: 0.4816228295571127\n",
      "Epoch 122, Loss: 0.4807618495484584\n",
      "Epoch 123, Loss: 0.48102030846138955\n",
      "Epoch 124, Loss: 0.4810176288364116\n",
      "Epoch 125, Loss: 0.48014338261661127\n",
      "Epoch 126, Loss: 0.47910314809270116\n",
      "Epoch 127, Loss: 0.47880795949683447\n",
      "Epoch 128, Loss: 0.47876177315142754\n",
      "Epoch 129, Loss: 0.47819597116449997\n",
      "Epoch 130, Loss: 0.47743214539542544\n",
      "Epoch 131, Loss: 0.47658476230340185\n",
      "Epoch 132, Loss: 0.4759644680184978\n",
      "Epoch 133, Loss: 0.4755635187712421\n",
      "Epoch 134, Loss: 0.47538264196584307\n",
      "Epoch 135, Loss: 0.4753673450596914\n",
      "Epoch 136, Loss: 0.4753697744168057\n",
      "Epoch 137, Loss: 0.4755973109583155\n",
      "Epoch 138, Loss: 0.4767626826605241\n",
      "Epoch 139, Loss: 0.47683528222287225\n",
      "Epoch 140, Loss: 0.47579445468261566\n",
      "Epoch 141, Loss: 0.47284533919249566\n",
      "Epoch 142, Loss: 0.472809286012082\n",
      "Epoch 143, Loss: 0.47458738111232796\n",
      "Epoch 144, Loss: 0.4733380058918715\n",
      "Epoch 145, Loss: 0.4715229034484008\n",
      "Epoch 146, Loss: 0.47096730966267386\n",
      "Epoch 147, Loss: 0.47161469890467383\n",
      "Epoch 148, Loss: 0.47183531411129226\n",
      "Epoch 149, Loss: 0.470294933688076\n",
      "Epoch 150, Loss: 0.4697114144973765\n",
      "Epoch 151, Loss: 0.47018789053701304\n",
      "Epoch 152, Loss: 0.4700460953124117\n",
      "Epoch 153, Loss: 0.4693645740016508\n",
      "Epoch 154, Loss: 0.4684558104180481\n",
      "Epoch 155, Loss: 0.4680960384979934\n",
      "Epoch 156, Loss: 0.4682454254529793\n",
      "Epoch 157, Loss: 0.4682974379105488\n",
      "Epoch 158, Loss: 0.4681590010361841\n",
      "Epoch 159, Loss: 0.4673225124817593\n",
      "Epoch 160, Loss: 0.4666701428107949\n",
      "Epoch 161, Loss: 0.4663166784414499\n",
      "Epoch 162, Loss: 0.46624854535037\n",
      "Epoch 163, Loss: 0.4664611566834947\n",
      "Epoch 164, Loss: 0.46654532450127006\n",
      "Epoch 165, Loss: 0.46721845607201035\n",
      "Epoch 166, Loss: 0.4668208160006968\n",
      "Epoch 167, Loss: 0.466573749669782\n",
      "Epoch 168, Loss: 0.46506387643568103\n",
      "Epoch 169, Loss: 0.4641220516950567\n",
      "Epoch 170, Loss: 0.4644550172656552\n",
      "Epoch 171, Loss: 0.4654613047116713\n",
      "Epoch 172, Loss: 0.46880176576246846\n",
      "Epoch 173, Loss: 0.4685649824581709\n",
      "Epoch 174, Loss: 0.46606115996454733\n",
      "Epoch 175, Loss: 0.46328263242364126\n",
      "Epoch 176, Loss: 0.4660484187313888\n",
      "Epoch 177, Loss: 0.4667669789465077\n",
      "Epoch 178, Loss: 0.46222975993575777\n",
      "Epoch 179, Loss: 0.46412214801342555\n",
      "Epoch 180, Loss: 0.46867384291927394\n",
      "Epoch 181, Loss: 0.46316269230838986\n",
      "Epoch 182, Loss: 0.4619791093960296\n",
      "Epoch 183, Loss: 0.4650783107301587\n",
      "Epoch 184, Loss: 0.4617210775403838\n",
      "Epoch 185, Loss: 0.4621011464022392\n",
      "Epoch 186, Loss: 0.4641367239628427\n",
      "Epoch 187, Loss: 0.46071131054180564\n",
      "Epoch 188, Loss: 0.46145892957673257\n",
      "Epoch 189, Loss: 0.464304908222076\n",
      "Epoch 190, Loss: 0.4606605441278432\n",
      "Epoch 191, Loss: 0.4601386308130386\n",
      "Epoch 192, Loss: 0.46193430495315263\n",
      "Epoch 193, Loss: 0.45977696912799226\n",
      "Epoch 194, Loss: 0.4597173803227511\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2398863601581413\n",
      "Valence RMSE: 0.2491130008464919\n",
      "Arousal RMSE: 0.23029034801549692\n",
      "Test R^2 score: tensor([0.3369, 0.6089], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4728682991851301\n",
      "Num of epochs: 195\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Epoch 121, Loss: 0.4816228295571127\n",
      "Epoch 122, Loss: 0.4807618495484584\n",
      "Epoch 123, Loss: 0.48102030846138955\n",
      "Epoch 124, Loss: 0.4810176288364116\n",
      "Epoch 125, Loss: 0.48014338261661127\n",
      "Epoch 126, Loss: 0.47910314809270116\n",
      "Epoch 127, Loss: 0.47880795949683447\n",
      "Epoch 128, Loss: 0.47876177315142754\n",
      "Epoch 129, Loss: 0.47819597116449997\n",
      "Epoch 130, Loss: 0.47743214539542544\n",
      "Epoch 131, Loss: 0.47658476230340185\n",
      "Epoch 132, Loss: 0.4759644680184978\n",
      "Epoch 133, Loss: 0.4755635187712421\n",
      "Epoch 134, Loss: 0.47538264196584307\n",
      "Epoch 135, Loss: 0.4753673450596914\n",
      "Epoch 136, Loss: 0.4753697744168057\n",
      "Epoch 137, Loss: 0.4755973109583155\n",
      "Epoch 138, Loss: 0.4767626826605241\n",
      "Epoch 139, Loss: 0.47683528222287225\n",
      "Epoch 140, Loss: 0.47579445468261566\n",
      "Epoch 141, Loss: 0.47284533919249566\n",
      "Epoch 142, Loss: 0.472809286012082\n",
      "Epoch 143, Loss: 0.47458738111232796\n",
      "Epoch 144, Loss: 0.4733380058918715\n",
      "Epoch 145, Loss: 0.4715229034484008\n",
      "Epoch 146, Loss: 0.47096730966267386\n",
      "Epoch 147, Loss: 0.47161469890467383\n",
      "Epoch 148, Loss: 0.47183531411129226\n",
      "Epoch 149, Loss: 0.470294933688076\n",
      "Epoch 150, Loss: 0.4697114144973765\n",
      "Epoch 151, Loss: 0.47018789053701304\n",
      "Epoch 152, Loss: 0.4700460953124117\n",
      "Epoch 153, Loss: 0.4693645740016508\n",
      "Epoch 154, Loss: 0.4684558104180481\n",
      "Epoch 155, Loss: 0.4680960384979934\n",
      "Epoch 156, Loss: 0.4682454254529793\n",
      "Epoch 157, Loss: 0.4682974379105488\n",
      "Epoch 158, Loss: 0.4681590010361841\n",
      "Epoch 159, Loss: 0.4673225124817593\n",
      "Epoch 160, Loss: 0.4666701428107949\n",
      "Epoch 161, Loss: 0.4663166784414499\n",
      "Epoch 162, Loss: 0.46624854535037\n",
      "Epoch 163, Loss: 0.4664611566834947\n",
      "Epoch 164, Loss: 0.46654532450127006\n",
      "Epoch 165, Loss: 0.46721845607201035\n",
      "Epoch 166, Loss: 0.4668208160006968\n",
      "Epoch 167, Loss: 0.466573749669782\n",
      "Epoch 168, Loss: 0.46506387643568103\n",
      "Epoch 169, Loss: 0.4641220516950567\n",
      "Epoch 170, Loss: 0.4644550172656552\n",
      "Epoch 171, Loss: 0.4654613047116713\n",
      "Epoch 172, Loss: 0.46880176576246846\n",
      "Epoch 173, Loss: 0.4685649824581709\n",
      "Epoch 174, Loss: 0.46606115996454733\n",
      "Epoch 175, Loss: 0.46328263242364126\n",
      "Epoch 176, Loss: 0.4660484187313888\n",
      "Epoch 177, Loss: 0.4667669789465077\n",
      "Epoch 178, Loss: 0.46222975993575777\n",
      "Epoch 179, Loss: 0.46412214801342555\n",
      "Epoch 180, Loss: 0.46867384291927394\n",
      "Epoch 181, Loss: 0.46316269230838986\n",
      "Epoch 182, Loss: 0.4619791093960296\n",
      "Epoch 183, Loss: 0.4650783107301587\n",
      "Epoch 184, Loss: 0.4617210775403838\n",
      "Epoch 185, Loss: 0.4621011464022392\n",
      "Epoch 186, Loss: 0.4641367239628427\n",
      "Epoch 187, Loss: 0.46071131054180564\n",
      "Epoch 188, Loss: 0.46145892957673257\n",
      "Epoch 189, Loss: 0.464304908222076\n",
      "Epoch 190, Loss: 0.4606605441278432\n",
      "Epoch 191, Loss: 0.4601386308130386\n",
      "Epoch 192, Loss: 0.46193430495315263\n",
      "Epoch 193, Loss: 0.45977696912799226\n",
      "Epoch 194, Loss: 0.4597173803227511\n",
      "Epoch 195, Loss: 0.46096589922299847\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23844517340322471\n",
      "Valence RMSE: 0.24543908460676053\n",
      "Arousal RMSE: 0.23123982612427138\n",
      "Test R^2 score: tensor([0.3563, 0.6056], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.48096029953458846\n",
      "Num of epochs: 196\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Epoch 121, Loss: 0.4816228295571127\n",
      "Epoch 122, Loss: 0.4807618495484584\n",
      "Epoch 123, Loss: 0.48102030846138955\n",
      "Epoch 124, Loss: 0.4810176288364116\n",
      "Epoch 125, Loss: 0.48014338261661127\n",
      "Epoch 126, Loss: 0.47910314809270116\n",
      "Epoch 127, Loss: 0.47880795949683447\n",
      "Epoch 128, Loss: 0.47876177315142754\n",
      "Epoch 129, Loss: 0.47819597116449997\n",
      "Epoch 130, Loss: 0.47743214539542544\n",
      "Epoch 131, Loss: 0.47658476230340185\n",
      "Epoch 132, Loss: 0.4759644680184978\n",
      "Epoch 133, Loss: 0.4755635187712421\n",
      "Epoch 134, Loss: 0.47538264196584307\n",
      "Epoch 135, Loss: 0.4753673450596914\n",
      "Epoch 136, Loss: 0.4753697744168057\n",
      "Epoch 137, Loss: 0.4755973109583155\n",
      "Epoch 138, Loss: 0.4767626826605241\n",
      "Epoch 139, Loss: 0.47683528222287225\n",
      "Epoch 140, Loss: 0.47579445468261566\n",
      "Epoch 141, Loss: 0.47284533919249566\n",
      "Epoch 142, Loss: 0.472809286012082\n",
      "Epoch 143, Loss: 0.47458738111232796\n",
      "Epoch 144, Loss: 0.4733380058918715\n",
      "Epoch 145, Loss: 0.4715229034484008\n",
      "Epoch 146, Loss: 0.47096730966267386\n",
      "Epoch 147, Loss: 0.47161469890467383\n",
      "Epoch 148, Loss: 0.47183531411129226\n",
      "Epoch 149, Loss: 0.470294933688076\n",
      "Epoch 150, Loss: 0.4697114144973765\n",
      "Epoch 151, Loss: 0.47018789053701304\n",
      "Epoch 152, Loss: 0.4700460953124117\n",
      "Epoch 153, Loss: 0.4693645740016508\n",
      "Epoch 154, Loss: 0.4684558104180481\n",
      "Epoch 155, Loss: 0.4680960384979934\n",
      "Epoch 156, Loss: 0.4682454254529793\n",
      "Epoch 157, Loss: 0.4682974379105488\n",
      "Epoch 158, Loss: 0.4681590010361841\n",
      "Epoch 159, Loss: 0.4673225124817593\n",
      "Epoch 160, Loss: 0.4666701428107949\n",
      "Epoch 161, Loss: 0.4663166784414499\n",
      "Epoch 162, Loss: 0.46624854535037\n",
      "Epoch 163, Loss: 0.4664611566834947\n",
      "Epoch 164, Loss: 0.46654532450127006\n",
      "Epoch 165, Loss: 0.46721845607201035\n",
      "Epoch 166, Loss: 0.4668208160006968\n",
      "Epoch 167, Loss: 0.466573749669782\n",
      "Epoch 168, Loss: 0.46506387643568103\n",
      "Epoch 169, Loss: 0.4641220516950567\n",
      "Epoch 170, Loss: 0.4644550172656552\n",
      "Epoch 171, Loss: 0.4654613047116713\n",
      "Epoch 172, Loss: 0.46880176576246846\n",
      "Epoch 173, Loss: 0.4685649824581709\n",
      "Epoch 174, Loss: 0.46606115996454733\n",
      "Epoch 175, Loss: 0.46328263242364126\n",
      "Epoch 176, Loss: 0.4660484187313888\n",
      "Epoch 177, Loss: 0.4667669789465077\n",
      "Epoch 178, Loss: 0.46222975993575777\n",
      "Epoch 179, Loss: 0.46412214801342555\n",
      "Epoch 180, Loss: 0.46867384291927394\n",
      "Epoch 181, Loss: 0.46316269230838986\n",
      "Epoch 182, Loss: 0.4619791093960296\n",
      "Epoch 183, Loss: 0.4650783107301587\n",
      "Epoch 184, Loss: 0.4617210775403838\n",
      "Epoch 185, Loss: 0.4621011464022392\n",
      "Epoch 186, Loss: 0.4641367239628427\n",
      "Epoch 187, Loss: 0.46071131054180564\n",
      "Epoch 188, Loss: 0.46145892957673257\n",
      "Epoch 189, Loss: 0.464304908222076\n",
      "Epoch 190, Loss: 0.4606605441278432\n",
      "Epoch 191, Loss: 0.4601386308130386\n",
      "Epoch 192, Loss: 0.46193430495315263\n",
      "Epoch 193, Loss: 0.45977696912799226\n",
      "Epoch 194, Loss: 0.4597173803227511\n",
      "Epoch 195, Loss: 0.46096589922299847\n",
      "Epoch 196, Loss: 0.4592170612502741\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23856282710182122\n",
      "Valence RMSE: 0.24634292705531874\n",
      "Arousal RMSE: 0.23052029680582312\n",
      "Test R^2 score: tensor([0.3515, 0.6081], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4798105419722234\n",
      "Num of epochs: 197\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Epoch 121, Loss: 0.4816228295571127\n",
      "Epoch 122, Loss: 0.4807618495484584\n",
      "Epoch 123, Loss: 0.48102030846138955\n",
      "Epoch 124, Loss: 0.4810176288364116\n",
      "Epoch 125, Loss: 0.48014338261661127\n",
      "Epoch 126, Loss: 0.47910314809270116\n",
      "Epoch 127, Loss: 0.47880795949683447\n",
      "Epoch 128, Loss: 0.47876177315142754\n",
      "Epoch 129, Loss: 0.47819597116449997\n",
      "Epoch 130, Loss: 0.47743214539542544\n",
      "Epoch 131, Loss: 0.47658476230340185\n",
      "Epoch 132, Loss: 0.4759644680184978\n",
      "Epoch 133, Loss: 0.4755635187712421\n",
      "Epoch 134, Loss: 0.47538264196584307\n",
      "Epoch 135, Loss: 0.4753673450596914\n",
      "Epoch 136, Loss: 0.4753697744168057\n",
      "Epoch 137, Loss: 0.4755973109583155\n",
      "Epoch 138, Loss: 0.4767626826605241\n",
      "Epoch 139, Loss: 0.47683528222287225\n",
      "Epoch 140, Loss: 0.47579445468261566\n",
      "Epoch 141, Loss: 0.47284533919249566\n",
      "Epoch 142, Loss: 0.472809286012082\n",
      "Epoch 143, Loss: 0.47458738111232796\n",
      "Epoch 144, Loss: 0.4733380058918715\n",
      "Epoch 145, Loss: 0.4715229034484008\n",
      "Epoch 146, Loss: 0.47096730966267386\n",
      "Epoch 147, Loss: 0.47161469890467383\n",
      "Epoch 148, Loss: 0.47183531411129226\n",
      "Epoch 149, Loss: 0.470294933688076\n",
      "Epoch 150, Loss: 0.4697114144973765\n",
      "Epoch 151, Loss: 0.47018789053701304\n",
      "Epoch 152, Loss: 0.4700460953124117\n",
      "Epoch 153, Loss: 0.4693645740016508\n",
      "Epoch 154, Loss: 0.4684558104180481\n",
      "Epoch 155, Loss: 0.4680960384979934\n",
      "Epoch 156, Loss: 0.4682454254529793\n",
      "Epoch 157, Loss: 0.4682974379105488\n",
      "Epoch 158, Loss: 0.4681590010361841\n",
      "Epoch 159, Loss: 0.4673225124817593\n",
      "Epoch 160, Loss: 0.4666701428107949\n",
      "Epoch 161, Loss: 0.4663166784414499\n",
      "Epoch 162, Loss: 0.46624854535037\n",
      "Epoch 163, Loss: 0.4664611566834947\n",
      "Epoch 164, Loss: 0.46654532450127006\n",
      "Epoch 165, Loss: 0.46721845607201035\n",
      "Epoch 166, Loss: 0.4668208160006968\n",
      "Epoch 167, Loss: 0.466573749669782\n",
      "Epoch 168, Loss: 0.46506387643568103\n",
      "Epoch 169, Loss: 0.4641220516950567\n",
      "Epoch 170, Loss: 0.4644550172656552\n",
      "Epoch 171, Loss: 0.4654613047116713\n",
      "Epoch 172, Loss: 0.46880176576246846\n",
      "Epoch 173, Loss: 0.4685649824581709\n",
      "Epoch 174, Loss: 0.46606115996454733\n",
      "Epoch 175, Loss: 0.46328263242364126\n",
      "Epoch 176, Loss: 0.4660484187313888\n",
      "Epoch 177, Loss: 0.4667669789465077\n",
      "Epoch 178, Loss: 0.46222975993575777\n",
      "Epoch 179, Loss: 0.46412214801342555\n",
      "Epoch 180, Loss: 0.46867384291927394\n",
      "Epoch 181, Loss: 0.46316269230838986\n",
      "Epoch 182, Loss: 0.4619791093960296\n",
      "Epoch 183, Loss: 0.4650783107301587\n",
      "Epoch 184, Loss: 0.4617210775403838\n",
      "Epoch 185, Loss: 0.4621011464022392\n",
      "Epoch 186, Loss: 0.4641367239628427\n",
      "Epoch 187, Loss: 0.46071131054180564\n",
      "Epoch 188, Loss: 0.46145892957673257\n",
      "Epoch 189, Loss: 0.464304908222076\n",
      "Epoch 190, Loss: 0.4606605441278432\n",
      "Epoch 191, Loss: 0.4601386308130386\n",
      "Epoch 192, Loss: 0.46193430495315263\n",
      "Epoch 193, Loss: 0.45977696912799226\n",
      "Epoch 194, Loss: 0.4597173803227511\n",
      "Epoch 195, Loss: 0.46096589922299847\n",
      "Epoch 196, Loss: 0.4592170612502741\n",
      "Epoch 197, Loss: 0.45864347419013624\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23960788449089146\n",
      "Valence RMSE: 0.2488637088117108\n",
      "Arousal RMSE: 0.22997984924092973\n",
      "Test R^2 score: tensor([0.3382, 0.6099], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.47405857615680763\n",
      "Num of epochs: 198\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Epoch 121, Loss: 0.4816228295571127\n",
      "Epoch 122, Loss: 0.4807618495484584\n",
      "Epoch 123, Loss: 0.48102030846138955\n",
      "Epoch 124, Loss: 0.4810176288364116\n",
      "Epoch 125, Loss: 0.48014338261661127\n",
      "Epoch 126, Loss: 0.47910314809270116\n",
      "Epoch 127, Loss: 0.47880795949683447\n",
      "Epoch 128, Loss: 0.47876177315142754\n",
      "Epoch 129, Loss: 0.47819597116449997\n",
      "Epoch 130, Loss: 0.47743214539542544\n",
      "Epoch 131, Loss: 0.47658476230340185\n",
      "Epoch 132, Loss: 0.4759644680184978\n",
      "Epoch 133, Loss: 0.4755635187712421\n",
      "Epoch 134, Loss: 0.47538264196584307\n",
      "Epoch 135, Loss: 0.4753673450596914\n",
      "Epoch 136, Loss: 0.4753697744168057\n",
      "Epoch 137, Loss: 0.4755973109583155\n",
      "Epoch 138, Loss: 0.4767626826605241\n",
      "Epoch 139, Loss: 0.47683528222287225\n",
      "Epoch 140, Loss: 0.47579445468261566\n",
      "Epoch 141, Loss: 0.47284533919249566\n",
      "Epoch 142, Loss: 0.472809286012082\n",
      "Epoch 143, Loss: 0.47458738111232796\n",
      "Epoch 144, Loss: 0.4733380058918715\n",
      "Epoch 145, Loss: 0.4715229034484008\n",
      "Epoch 146, Loss: 0.47096730966267386\n",
      "Epoch 147, Loss: 0.47161469890467383\n",
      "Epoch 148, Loss: 0.47183531411129226\n",
      "Epoch 149, Loss: 0.470294933688076\n",
      "Epoch 150, Loss: 0.4697114144973765\n",
      "Epoch 151, Loss: 0.47018789053701304\n",
      "Epoch 152, Loss: 0.4700460953124117\n",
      "Epoch 153, Loss: 0.4693645740016508\n",
      "Epoch 154, Loss: 0.4684558104180481\n",
      "Epoch 155, Loss: 0.4680960384979934\n",
      "Epoch 156, Loss: 0.4682454254529793\n",
      "Epoch 157, Loss: 0.4682974379105488\n",
      "Epoch 158, Loss: 0.4681590010361841\n",
      "Epoch 159, Loss: 0.4673225124817593\n",
      "Epoch 160, Loss: 0.4666701428107949\n",
      "Epoch 161, Loss: 0.4663166784414499\n",
      "Epoch 162, Loss: 0.46624854535037\n",
      "Epoch 163, Loss: 0.4664611566834947\n",
      "Epoch 164, Loss: 0.46654532450127006\n",
      "Epoch 165, Loss: 0.46721845607201035\n",
      "Epoch 166, Loss: 0.4668208160006968\n",
      "Epoch 167, Loss: 0.466573749669782\n",
      "Epoch 168, Loss: 0.46506387643568103\n",
      "Epoch 169, Loss: 0.4641220516950567\n",
      "Epoch 170, Loss: 0.4644550172656552\n",
      "Epoch 171, Loss: 0.4654613047116713\n",
      "Epoch 172, Loss: 0.46880176576246846\n",
      "Epoch 173, Loss: 0.4685649824581709\n",
      "Epoch 174, Loss: 0.46606115996454733\n",
      "Epoch 175, Loss: 0.46328263242364126\n",
      "Epoch 176, Loss: 0.4660484187313888\n",
      "Epoch 177, Loss: 0.4667669789465077\n",
      "Epoch 178, Loss: 0.46222975993575777\n",
      "Epoch 179, Loss: 0.46412214801342555\n",
      "Epoch 180, Loss: 0.46867384291927394\n",
      "Epoch 181, Loss: 0.46316269230838986\n",
      "Epoch 182, Loss: 0.4619791093960296\n",
      "Epoch 183, Loss: 0.4650783107301587\n",
      "Epoch 184, Loss: 0.4617210775403838\n",
      "Epoch 185, Loss: 0.4621011464022392\n",
      "Epoch 186, Loss: 0.4641367239628427\n",
      "Epoch 187, Loss: 0.46071131054180564\n",
      "Epoch 188, Loss: 0.46145892957673257\n",
      "Epoch 189, Loss: 0.464304908222076\n",
      "Epoch 190, Loss: 0.4606605441278432\n",
      "Epoch 191, Loss: 0.4601386308130386\n",
      "Epoch 192, Loss: 0.46193430495315263\n",
      "Epoch 193, Loss: 0.45977696912799226\n",
      "Epoch 194, Loss: 0.4597173803227511\n",
      "Epoch 195, Loss: 0.46096589922299847\n",
      "Epoch 196, Loss: 0.4592170612502741\n",
      "Epoch 197, Loss: 0.45864347419013624\n",
      "Epoch 198, Loss: 0.4594650511157848\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2384074165869643\n",
      "Valence RMSE: 0.2448080589523331\n",
      "Arousal RMSE: 0.23183012496078173\n",
      "Test R^2 score: tensor([0.3596, 0.6036], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4816052266358667\n",
      "Num of epochs: 199\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Epoch 121, Loss: 0.4816228295571127\n",
      "Epoch 122, Loss: 0.4807618495484584\n",
      "Epoch 123, Loss: 0.48102030846138955\n",
      "Epoch 124, Loss: 0.4810176288364116\n",
      "Epoch 125, Loss: 0.48014338261661127\n",
      "Epoch 126, Loss: 0.47910314809270116\n",
      "Epoch 127, Loss: 0.47880795949683447\n",
      "Epoch 128, Loss: 0.47876177315142754\n",
      "Epoch 129, Loss: 0.47819597116449997\n",
      "Epoch 130, Loss: 0.47743214539542544\n",
      "Epoch 131, Loss: 0.47658476230340185\n",
      "Epoch 132, Loss: 0.4759644680184978\n",
      "Epoch 133, Loss: 0.4755635187712421\n",
      "Epoch 134, Loss: 0.47538264196584307\n",
      "Epoch 135, Loss: 0.4753673450596914\n",
      "Epoch 136, Loss: 0.4753697744168057\n",
      "Epoch 137, Loss: 0.4755973109583155\n",
      "Epoch 138, Loss: 0.4767626826605241\n",
      "Epoch 139, Loss: 0.47683528222287225\n",
      "Epoch 140, Loss: 0.47579445468261566\n",
      "Epoch 141, Loss: 0.47284533919249566\n",
      "Epoch 142, Loss: 0.472809286012082\n",
      "Epoch 143, Loss: 0.47458738111232796\n",
      "Epoch 144, Loss: 0.4733380058918715\n",
      "Epoch 145, Loss: 0.4715229034484008\n",
      "Epoch 146, Loss: 0.47096730966267386\n",
      "Epoch 147, Loss: 0.47161469890467383\n",
      "Epoch 148, Loss: 0.47183531411129226\n",
      "Epoch 149, Loss: 0.470294933688076\n",
      "Epoch 150, Loss: 0.4697114144973765\n",
      "Epoch 151, Loss: 0.47018789053701304\n",
      "Epoch 152, Loss: 0.4700460953124117\n",
      "Epoch 153, Loss: 0.4693645740016508\n",
      "Epoch 154, Loss: 0.4684558104180481\n",
      "Epoch 155, Loss: 0.4680960384979934\n",
      "Epoch 156, Loss: 0.4682454254529793\n",
      "Epoch 157, Loss: 0.4682974379105488\n",
      "Epoch 158, Loss: 0.4681590010361841\n",
      "Epoch 159, Loss: 0.4673225124817593\n",
      "Epoch 160, Loss: 0.4666701428107949\n",
      "Epoch 161, Loss: 0.4663166784414499\n",
      "Epoch 162, Loss: 0.46624854535037\n",
      "Epoch 163, Loss: 0.4664611566834947\n",
      "Epoch 164, Loss: 0.46654532450127006\n",
      "Epoch 165, Loss: 0.46721845607201035\n",
      "Epoch 166, Loss: 0.4668208160006968\n",
      "Epoch 167, Loss: 0.466573749669782\n",
      "Epoch 168, Loss: 0.46506387643568103\n",
      "Epoch 169, Loss: 0.4641220516950567\n",
      "Epoch 170, Loss: 0.4644550172656552\n",
      "Epoch 171, Loss: 0.4654613047116713\n",
      "Epoch 172, Loss: 0.46880176576246846\n",
      "Epoch 173, Loss: 0.4685649824581709\n",
      "Epoch 174, Loss: 0.46606115996454733\n",
      "Epoch 175, Loss: 0.46328263242364126\n",
      "Epoch 176, Loss: 0.4660484187313888\n",
      "Epoch 177, Loss: 0.4667669789465077\n",
      "Epoch 178, Loss: 0.46222975993575777\n",
      "Epoch 179, Loss: 0.46412214801342555\n",
      "Epoch 180, Loss: 0.46867384291927394\n",
      "Epoch 181, Loss: 0.46316269230838986\n",
      "Epoch 182, Loss: 0.4619791093960296\n",
      "Epoch 183, Loss: 0.4650783107301587\n",
      "Epoch 184, Loss: 0.4617210775403838\n",
      "Epoch 185, Loss: 0.4621011464022392\n",
      "Epoch 186, Loss: 0.4641367239628427\n",
      "Epoch 187, Loss: 0.46071131054180564\n",
      "Epoch 188, Loss: 0.46145892957673257\n",
      "Epoch 189, Loss: 0.464304908222076\n",
      "Epoch 190, Loss: 0.4606605441278432\n",
      "Epoch 191, Loss: 0.4601386308130386\n",
      "Epoch 192, Loss: 0.46193430495315263\n",
      "Epoch 193, Loss: 0.45977696912799226\n",
      "Epoch 194, Loss: 0.4597173803227511\n",
      "Epoch 195, Loss: 0.46096589922299847\n",
      "Epoch 196, Loss: 0.4592170612502741\n",
      "Epoch 197, Loss: 0.45864347419013624\n",
      "Epoch 198, Loss: 0.4594650511157848\n",
      "Epoch 199, Loss: 0.45924705942486493\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23846108027809562\n",
      "Valence RMSE: 0.2473532452827748\n",
      "Arousal RMSE: 0.2292242257329538\n",
      "Test R^2 score: tensor([0.3462, 0.6125], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4793427399040487\n",
      "Num of epochs: 200\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Epoch 121, Loss: 0.4816228295571127\n",
      "Epoch 122, Loss: 0.4807618495484584\n",
      "Epoch 123, Loss: 0.48102030846138955\n",
      "Epoch 124, Loss: 0.4810176288364116\n",
      "Epoch 125, Loss: 0.48014338261661127\n",
      "Epoch 126, Loss: 0.47910314809270116\n",
      "Epoch 127, Loss: 0.47880795949683447\n",
      "Epoch 128, Loss: 0.47876177315142754\n",
      "Epoch 129, Loss: 0.47819597116449997\n",
      "Epoch 130, Loss: 0.47743214539542544\n",
      "Epoch 131, Loss: 0.47658476230340185\n",
      "Epoch 132, Loss: 0.4759644680184978\n",
      "Epoch 133, Loss: 0.4755635187712421\n",
      "Epoch 134, Loss: 0.47538264196584307\n",
      "Epoch 135, Loss: 0.4753673450596914\n",
      "Epoch 136, Loss: 0.4753697744168057\n",
      "Epoch 137, Loss: 0.4755973109583155\n",
      "Epoch 138, Loss: 0.4767626826605241\n",
      "Epoch 139, Loss: 0.47683528222287225\n",
      "Epoch 140, Loss: 0.47579445468261566\n",
      "Epoch 141, Loss: 0.47284533919249566\n",
      "Epoch 142, Loss: 0.472809286012082\n",
      "Epoch 143, Loss: 0.47458738111232796\n",
      "Epoch 144, Loss: 0.4733380058918715\n",
      "Epoch 145, Loss: 0.4715229034484008\n",
      "Epoch 146, Loss: 0.47096730966267386\n",
      "Epoch 147, Loss: 0.47161469890467383\n",
      "Epoch 148, Loss: 0.47183531411129226\n",
      "Epoch 149, Loss: 0.470294933688076\n",
      "Epoch 150, Loss: 0.4697114144973765\n",
      "Epoch 151, Loss: 0.47018789053701304\n",
      "Epoch 152, Loss: 0.4700460953124117\n",
      "Epoch 153, Loss: 0.4693645740016508\n",
      "Epoch 154, Loss: 0.4684558104180481\n",
      "Epoch 155, Loss: 0.4680960384979934\n",
      "Epoch 156, Loss: 0.4682454254529793\n",
      "Epoch 157, Loss: 0.4682974379105488\n",
      "Epoch 158, Loss: 0.4681590010361841\n",
      "Epoch 159, Loss: 0.4673225124817593\n",
      "Epoch 160, Loss: 0.4666701428107949\n",
      "Epoch 161, Loss: 0.4663166784414499\n",
      "Epoch 162, Loss: 0.46624854535037\n",
      "Epoch 163, Loss: 0.4664611566834947\n",
      "Epoch 164, Loss: 0.46654532450127006\n",
      "Epoch 165, Loss: 0.46721845607201035\n",
      "Epoch 166, Loss: 0.4668208160006968\n",
      "Epoch 167, Loss: 0.466573749669782\n",
      "Epoch 168, Loss: 0.46506387643568103\n",
      "Epoch 169, Loss: 0.4641220516950567\n",
      "Epoch 170, Loss: 0.4644550172656552\n",
      "Epoch 171, Loss: 0.4654613047116713\n",
      "Epoch 172, Loss: 0.46880176576246846\n",
      "Epoch 173, Loss: 0.4685649824581709\n",
      "Epoch 174, Loss: 0.46606115996454733\n",
      "Epoch 175, Loss: 0.46328263242364126\n",
      "Epoch 176, Loss: 0.4660484187313888\n",
      "Epoch 177, Loss: 0.4667669789465077\n",
      "Epoch 178, Loss: 0.46222975993575777\n",
      "Epoch 179, Loss: 0.46412214801342555\n",
      "Epoch 180, Loss: 0.46867384291927394\n",
      "Epoch 181, Loss: 0.46316269230838986\n",
      "Epoch 182, Loss: 0.4619791093960296\n",
      "Epoch 183, Loss: 0.4650783107301587\n",
      "Epoch 184, Loss: 0.4617210775403838\n",
      "Epoch 185, Loss: 0.4621011464022392\n",
      "Epoch 186, Loss: 0.4641367239628427\n",
      "Epoch 187, Loss: 0.46071131054180564\n",
      "Epoch 188, Loss: 0.46145892957673257\n",
      "Epoch 189, Loss: 0.464304908222076\n",
      "Epoch 190, Loss: 0.4606605441278432\n",
      "Epoch 191, Loss: 0.4601386308130386\n",
      "Epoch 192, Loss: 0.46193430495315263\n",
      "Epoch 193, Loss: 0.45977696912799226\n",
      "Epoch 194, Loss: 0.4597173803227511\n",
      "Epoch 195, Loss: 0.46096589922299847\n",
      "Epoch 196, Loss: 0.4592170612502741\n",
      "Epoch 197, Loss: 0.45864347419013624\n",
      "Epoch 198, Loss: 0.4594650511157848\n",
      "Epoch 199, Loss: 0.45924705942486493\n",
      "Epoch 200, Loss: 0.4584931199827434\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2378532908643929\n",
      "Valence RMSE: 0.24621506446254557\n",
      "Arousal RMSE: 0.2291866444227227\n",
      "Test R^2 score: tensor([0.3522, 0.6126], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.48240779588675514\n",
      "Num of epochs: 201\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Epoch 121, Loss: 0.4816228295571127\n",
      "Epoch 122, Loss: 0.4807618495484584\n",
      "Epoch 123, Loss: 0.48102030846138955\n",
      "Epoch 124, Loss: 0.4810176288364116\n",
      "Epoch 125, Loss: 0.48014338261661127\n",
      "Epoch 126, Loss: 0.47910314809270116\n",
      "Epoch 127, Loss: 0.47880795949683447\n",
      "Epoch 128, Loss: 0.47876177315142754\n",
      "Epoch 129, Loss: 0.47819597116449997\n",
      "Epoch 130, Loss: 0.47743214539542544\n",
      "Epoch 131, Loss: 0.47658476230340185\n",
      "Epoch 132, Loss: 0.4759644680184978\n",
      "Epoch 133, Loss: 0.4755635187712421\n",
      "Epoch 134, Loss: 0.47538264196584307\n",
      "Epoch 135, Loss: 0.4753673450596914\n",
      "Epoch 136, Loss: 0.4753697744168057\n",
      "Epoch 137, Loss: 0.4755973109583155\n",
      "Epoch 138, Loss: 0.4767626826605241\n",
      "Epoch 139, Loss: 0.47683528222287225\n",
      "Epoch 140, Loss: 0.47579445468261566\n",
      "Epoch 141, Loss: 0.47284533919249566\n",
      "Epoch 142, Loss: 0.472809286012082\n",
      "Epoch 143, Loss: 0.47458738111232796\n",
      "Epoch 144, Loss: 0.4733380058918715\n",
      "Epoch 145, Loss: 0.4715229034484008\n",
      "Epoch 146, Loss: 0.47096730966267386\n",
      "Epoch 147, Loss: 0.47161469890467383\n",
      "Epoch 148, Loss: 0.47183531411129226\n",
      "Epoch 149, Loss: 0.470294933688076\n",
      "Epoch 150, Loss: 0.4697114144973765\n",
      "Epoch 151, Loss: 0.47018789053701304\n",
      "Epoch 152, Loss: 0.4700460953124117\n",
      "Epoch 153, Loss: 0.4693645740016508\n",
      "Epoch 154, Loss: 0.4684558104180481\n",
      "Epoch 155, Loss: 0.4680960384979934\n",
      "Epoch 156, Loss: 0.4682454254529793\n",
      "Epoch 157, Loss: 0.4682974379105488\n",
      "Epoch 158, Loss: 0.4681590010361841\n",
      "Epoch 159, Loss: 0.4673225124817593\n",
      "Epoch 160, Loss: 0.4666701428107949\n",
      "Epoch 161, Loss: 0.4663166784414499\n",
      "Epoch 162, Loss: 0.46624854535037\n",
      "Epoch 163, Loss: 0.4664611566834947\n",
      "Epoch 164, Loss: 0.46654532450127006\n",
      "Epoch 165, Loss: 0.46721845607201035\n",
      "Epoch 166, Loss: 0.4668208160006968\n",
      "Epoch 167, Loss: 0.466573749669782\n",
      "Epoch 168, Loss: 0.46506387643568103\n",
      "Epoch 169, Loss: 0.4641220516950567\n",
      "Epoch 170, Loss: 0.4644550172656552\n",
      "Epoch 171, Loss: 0.4654613047116713\n",
      "Epoch 172, Loss: 0.46880176576246846\n",
      "Epoch 173, Loss: 0.4685649824581709\n",
      "Epoch 174, Loss: 0.46606115996454733\n",
      "Epoch 175, Loss: 0.46328263242364126\n",
      "Epoch 176, Loss: 0.4660484187313888\n",
      "Epoch 177, Loss: 0.4667669789465077\n",
      "Epoch 178, Loss: 0.46222975993575777\n",
      "Epoch 179, Loss: 0.46412214801342555\n",
      "Epoch 180, Loss: 0.46867384291927394\n",
      "Epoch 181, Loss: 0.46316269230838986\n",
      "Epoch 182, Loss: 0.4619791093960296\n",
      "Epoch 183, Loss: 0.4650783107301587\n",
      "Epoch 184, Loss: 0.4617210775403838\n",
      "Epoch 185, Loss: 0.4621011464022392\n",
      "Epoch 186, Loss: 0.4641367239628427\n",
      "Epoch 187, Loss: 0.46071131054180564\n",
      "Epoch 188, Loss: 0.46145892957673257\n",
      "Epoch 189, Loss: 0.464304908222076\n",
      "Epoch 190, Loss: 0.4606605441278432\n",
      "Epoch 191, Loss: 0.4601386308130386\n",
      "Epoch 192, Loss: 0.46193430495315263\n",
      "Epoch 193, Loss: 0.45977696912799226\n",
      "Epoch 194, Loss: 0.4597173803227511\n",
      "Epoch 195, Loss: 0.46096589922299847\n",
      "Epoch 196, Loss: 0.4592170612502741\n",
      "Epoch 197, Loss: 0.45864347419013624\n",
      "Epoch 198, Loss: 0.4594650511157848\n",
      "Epoch 199, Loss: 0.45924705942486493\n",
      "Epoch 200, Loss: 0.4584931199827434\n",
      "Epoch 201, Loss: 0.4578552480015059\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23786456341960782\n",
      "Valence RMSE: 0.24479037103641524\n",
      "Arousal RMSE: 0.2307309587148101\n",
      "Test R^2 score: tensor([0.3597, 0.6074], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4835263275728074\n",
      "Num of epochs: 202\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Epoch 121, Loss: 0.4816228295571127\n",
      "Epoch 122, Loss: 0.4807618495484584\n",
      "Epoch 123, Loss: 0.48102030846138955\n",
      "Epoch 124, Loss: 0.4810176288364116\n",
      "Epoch 125, Loss: 0.48014338261661127\n",
      "Epoch 126, Loss: 0.47910314809270116\n",
      "Epoch 127, Loss: 0.47880795949683447\n",
      "Epoch 128, Loss: 0.47876177315142754\n",
      "Epoch 129, Loss: 0.47819597116449997\n",
      "Epoch 130, Loss: 0.47743214539542544\n",
      "Epoch 131, Loss: 0.47658476230340185\n",
      "Epoch 132, Loss: 0.4759644680184978\n",
      "Epoch 133, Loss: 0.4755635187712421\n",
      "Epoch 134, Loss: 0.47538264196584307\n",
      "Epoch 135, Loss: 0.4753673450596914\n",
      "Epoch 136, Loss: 0.4753697744168057\n",
      "Epoch 137, Loss: 0.4755973109583155\n",
      "Epoch 138, Loss: 0.4767626826605241\n",
      "Epoch 139, Loss: 0.47683528222287225\n",
      "Epoch 140, Loss: 0.47579445468261566\n",
      "Epoch 141, Loss: 0.47284533919249566\n",
      "Epoch 142, Loss: 0.472809286012082\n",
      "Epoch 143, Loss: 0.47458738111232796\n",
      "Epoch 144, Loss: 0.4733380058918715\n",
      "Epoch 145, Loss: 0.4715229034484008\n",
      "Epoch 146, Loss: 0.47096730966267386\n",
      "Epoch 147, Loss: 0.47161469890467383\n",
      "Epoch 148, Loss: 0.47183531411129226\n",
      "Epoch 149, Loss: 0.470294933688076\n",
      "Epoch 150, Loss: 0.4697114144973765\n",
      "Epoch 151, Loss: 0.47018789053701304\n",
      "Epoch 152, Loss: 0.4700460953124117\n",
      "Epoch 153, Loss: 0.4693645740016508\n",
      "Epoch 154, Loss: 0.4684558104180481\n",
      "Epoch 155, Loss: 0.4680960384979934\n",
      "Epoch 156, Loss: 0.4682454254529793\n",
      "Epoch 157, Loss: 0.4682974379105488\n",
      "Epoch 158, Loss: 0.4681590010361841\n",
      "Epoch 159, Loss: 0.4673225124817593\n",
      "Epoch 160, Loss: 0.4666701428107949\n",
      "Epoch 161, Loss: 0.4663166784414499\n",
      "Epoch 162, Loss: 0.46624854535037\n",
      "Epoch 163, Loss: 0.4664611566834947\n",
      "Epoch 164, Loss: 0.46654532450127006\n",
      "Epoch 165, Loss: 0.46721845607201035\n",
      "Epoch 166, Loss: 0.4668208160006968\n",
      "Epoch 167, Loss: 0.466573749669782\n",
      "Epoch 168, Loss: 0.46506387643568103\n",
      "Epoch 169, Loss: 0.4641220516950567\n",
      "Epoch 170, Loss: 0.4644550172656552\n",
      "Epoch 171, Loss: 0.4654613047116713\n",
      "Epoch 172, Loss: 0.46880176576246846\n",
      "Epoch 173, Loss: 0.4685649824581709\n",
      "Epoch 174, Loss: 0.46606115996454733\n",
      "Epoch 175, Loss: 0.46328263242364126\n",
      "Epoch 176, Loss: 0.4660484187313888\n",
      "Epoch 177, Loss: 0.4667669789465077\n",
      "Epoch 178, Loss: 0.46222975993575777\n",
      "Epoch 179, Loss: 0.46412214801342555\n",
      "Epoch 180, Loss: 0.46867384291927394\n",
      "Epoch 181, Loss: 0.46316269230838986\n",
      "Epoch 182, Loss: 0.4619791093960296\n",
      "Epoch 183, Loss: 0.4650783107301587\n",
      "Epoch 184, Loss: 0.4617210775403838\n",
      "Epoch 185, Loss: 0.4621011464022392\n",
      "Epoch 186, Loss: 0.4641367239628427\n",
      "Epoch 187, Loss: 0.46071131054180564\n",
      "Epoch 188, Loss: 0.46145892957673257\n",
      "Epoch 189, Loss: 0.464304908222076\n",
      "Epoch 190, Loss: 0.4606605441278432\n",
      "Epoch 191, Loss: 0.4601386308130386\n",
      "Epoch 192, Loss: 0.46193430495315263\n",
      "Epoch 193, Loss: 0.45977696912799226\n",
      "Epoch 194, Loss: 0.4597173803227511\n",
      "Epoch 195, Loss: 0.46096589922299847\n",
      "Epoch 196, Loss: 0.4592170612502741\n",
      "Epoch 197, Loss: 0.45864347419013624\n",
      "Epoch 198, Loss: 0.4594650511157848\n",
      "Epoch 199, Loss: 0.45924705942486493\n",
      "Epoch 200, Loss: 0.4584931199827434\n",
      "Epoch 201, Loss: 0.4578552480015059\n",
      "Epoch 202, Loss: 0.458251953125\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23870750796217216\n",
      "Valence RMSE: 0.2478566700660885\n",
      "Arousal RMSE: 0.229193411377315\n",
      "Test R^2 score: tensor([0.3435, 0.6126], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4780628171614721\n",
      "Num of epochs: 203\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Epoch 121, Loss: 0.4816228295571127\n",
      "Epoch 122, Loss: 0.4807618495484584\n",
      "Epoch 123, Loss: 0.48102030846138955\n",
      "Epoch 124, Loss: 0.4810176288364116\n",
      "Epoch 125, Loss: 0.48014338261661127\n",
      "Epoch 126, Loss: 0.47910314809270116\n",
      "Epoch 127, Loss: 0.47880795949683447\n",
      "Epoch 128, Loss: 0.47876177315142754\n",
      "Epoch 129, Loss: 0.47819597116449997\n",
      "Epoch 130, Loss: 0.47743214539542544\n",
      "Epoch 131, Loss: 0.47658476230340185\n",
      "Epoch 132, Loss: 0.4759644680184978\n",
      "Epoch 133, Loss: 0.4755635187712421\n",
      "Epoch 134, Loss: 0.47538264196584307\n",
      "Epoch 135, Loss: 0.4753673450596914\n",
      "Epoch 136, Loss: 0.4753697744168057\n",
      "Epoch 137, Loss: 0.4755973109583155\n",
      "Epoch 138, Loss: 0.4767626826605241\n",
      "Epoch 139, Loss: 0.47683528222287225\n",
      "Epoch 140, Loss: 0.47579445468261566\n",
      "Epoch 141, Loss: 0.47284533919249566\n",
      "Epoch 142, Loss: 0.472809286012082\n",
      "Epoch 143, Loss: 0.47458738111232796\n",
      "Epoch 144, Loss: 0.4733380058918715\n",
      "Epoch 145, Loss: 0.4715229034484008\n",
      "Epoch 146, Loss: 0.47096730966267386\n",
      "Epoch 147, Loss: 0.47161469890467383\n",
      "Epoch 148, Loss: 0.47183531411129226\n",
      "Epoch 149, Loss: 0.470294933688076\n",
      "Epoch 150, Loss: 0.4697114144973765\n",
      "Epoch 151, Loss: 0.47018789053701304\n",
      "Epoch 152, Loss: 0.4700460953124117\n",
      "Epoch 153, Loss: 0.4693645740016508\n",
      "Epoch 154, Loss: 0.4684558104180481\n",
      "Epoch 155, Loss: 0.4680960384979934\n",
      "Epoch 156, Loss: 0.4682454254529793\n",
      "Epoch 157, Loss: 0.4682974379105488\n",
      "Epoch 158, Loss: 0.4681590010361841\n",
      "Epoch 159, Loss: 0.4673225124817593\n",
      "Epoch 160, Loss: 0.4666701428107949\n",
      "Epoch 161, Loss: 0.4663166784414499\n",
      "Epoch 162, Loss: 0.46624854535037\n",
      "Epoch 163, Loss: 0.4664611566834947\n",
      "Epoch 164, Loss: 0.46654532450127006\n",
      "Epoch 165, Loss: 0.46721845607201035\n",
      "Epoch 166, Loss: 0.4668208160006968\n",
      "Epoch 167, Loss: 0.466573749669782\n",
      "Epoch 168, Loss: 0.46506387643568103\n",
      "Epoch 169, Loss: 0.4641220516950567\n",
      "Epoch 170, Loss: 0.4644550172656552\n",
      "Epoch 171, Loss: 0.4654613047116713\n",
      "Epoch 172, Loss: 0.46880176576246846\n",
      "Epoch 173, Loss: 0.4685649824581709\n",
      "Epoch 174, Loss: 0.46606115996454733\n",
      "Epoch 175, Loss: 0.46328263242364126\n",
      "Epoch 176, Loss: 0.4660484187313888\n",
      "Epoch 177, Loss: 0.4667669789465077\n",
      "Epoch 178, Loss: 0.46222975993575777\n",
      "Epoch 179, Loss: 0.46412214801342555\n",
      "Epoch 180, Loss: 0.46867384291927394\n",
      "Epoch 181, Loss: 0.46316269230838986\n",
      "Epoch 182, Loss: 0.4619791093960296\n",
      "Epoch 183, Loss: 0.4650783107301587\n",
      "Epoch 184, Loss: 0.4617210775403838\n",
      "Epoch 185, Loss: 0.4621011464022392\n",
      "Epoch 186, Loss: 0.4641367239628427\n",
      "Epoch 187, Loss: 0.46071131054180564\n",
      "Epoch 188, Loss: 0.46145892957673257\n",
      "Epoch 189, Loss: 0.464304908222076\n",
      "Epoch 190, Loss: 0.4606605441278432\n",
      "Epoch 191, Loss: 0.4601386308130386\n",
      "Epoch 192, Loss: 0.46193430495315263\n",
      "Epoch 193, Loss: 0.45977696912799226\n",
      "Epoch 194, Loss: 0.4597173803227511\n",
      "Epoch 195, Loss: 0.46096589922299847\n",
      "Epoch 196, Loss: 0.4592170612502741\n",
      "Epoch 197, Loss: 0.45864347419013624\n",
      "Epoch 198, Loss: 0.4594650511157848\n",
      "Epoch 199, Loss: 0.45924705942486493\n",
      "Epoch 200, Loss: 0.4584931199827434\n",
      "Epoch 201, Loss: 0.4578552480015059\n",
      "Epoch 202, Loss: 0.458251953125\n",
      "Epoch 203, Loss: 0.4586848316367108\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2377675414557761\n",
      "Valence RMSE: 0.2446418474970194\n",
      "Arousal RMSE: 0.23068847824088118\n",
      "Test R^2 score: tensor([0.3604, 0.6075], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4839870004729347\n",
      "Num of epochs: 204\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Epoch 121, Loss: 0.4816228295571127\n",
      "Epoch 122, Loss: 0.4807618495484584\n",
      "Epoch 123, Loss: 0.48102030846138955\n",
      "Epoch 124, Loss: 0.4810176288364116\n",
      "Epoch 125, Loss: 0.48014338261661127\n",
      "Epoch 126, Loss: 0.47910314809270116\n",
      "Epoch 127, Loss: 0.47880795949683447\n",
      "Epoch 128, Loss: 0.47876177315142754\n",
      "Epoch 129, Loss: 0.47819597116449997\n",
      "Epoch 130, Loss: 0.47743214539542544\n",
      "Epoch 131, Loss: 0.47658476230340185\n",
      "Epoch 132, Loss: 0.4759644680184978\n",
      "Epoch 133, Loss: 0.4755635187712421\n",
      "Epoch 134, Loss: 0.47538264196584307\n",
      "Epoch 135, Loss: 0.4753673450596914\n",
      "Epoch 136, Loss: 0.4753697744168057\n",
      "Epoch 137, Loss: 0.4755973109583155\n",
      "Epoch 138, Loss: 0.4767626826605241\n",
      "Epoch 139, Loss: 0.47683528222287225\n",
      "Epoch 140, Loss: 0.47579445468261566\n",
      "Epoch 141, Loss: 0.47284533919249566\n",
      "Epoch 142, Loss: 0.472809286012082\n",
      "Epoch 143, Loss: 0.47458738111232796\n",
      "Epoch 144, Loss: 0.4733380058918715\n",
      "Epoch 145, Loss: 0.4715229034484008\n",
      "Epoch 146, Loss: 0.47096730966267386\n",
      "Epoch 147, Loss: 0.47161469890467383\n",
      "Epoch 148, Loss: 0.47183531411129226\n",
      "Epoch 149, Loss: 0.470294933688076\n",
      "Epoch 150, Loss: 0.4697114144973765\n",
      "Epoch 151, Loss: 0.47018789053701304\n",
      "Epoch 152, Loss: 0.4700460953124117\n",
      "Epoch 153, Loss: 0.4693645740016508\n",
      "Epoch 154, Loss: 0.4684558104180481\n",
      "Epoch 155, Loss: 0.4680960384979934\n",
      "Epoch 156, Loss: 0.4682454254529793\n",
      "Epoch 157, Loss: 0.4682974379105488\n",
      "Epoch 158, Loss: 0.4681590010361841\n",
      "Epoch 159, Loss: 0.4673225124817593\n",
      "Epoch 160, Loss: 0.4666701428107949\n",
      "Epoch 161, Loss: 0.4663166784414499\n",
      "Epoch 162, Loss: 0.46624854535037\n",
      "Epoch 163, Loss: 0.4664611566834947\n",
      "Epoch 164, Loss: 0.46654532450127006\n",
      "Epoch 165, Loss: 0.46721845607201035\n",
      "Epoch 166, Loss: 0.4668208160006968\n",
      "Epoch 167, Loss: 0.466573749669782\n",
      "Epoch 168, Loss: 0.46506387643568103\n",
      "Epoch 169, Loss: 0.4641220516950567\n",
      "Epoch 170, Loss: 0.4644550172656552\n",
      "Epoch 171, Loss: 0.4654613047116713\n",
      "Epoch 172, Loss: 0.46880176576246846\n",
      "Epoch 173, Loss: 0.4685649824581709\n",
      "Epoch 174, Loss: 0.46606115996454733\n",
      "Epoch 175, Loss: 0.46328263242364126\n",
      "Epoch 176, Loss: 0.4660484187313888\n",
      "Epoch 177, Loss: 0.4667669789465077\n",
      "Epoch 178, Loss: 0.46222975993575777\n",
      "Epoch 179, Loss: 0.46412214801342555\n",
      "Epoch 180, Loss: 0.46867384291927394\n",
      "Epoch 181, Loss: 0.46316269230838986\n",
      "Epoch 182, Loss: 0.4619791093960296\n",
      "Epoch 183, Loss: 0.4650783107301587\n",
      "Epoch 184, Loss: 0.4617210775403838\n",
      "Epoch 185, Loss: 0.4621011464022392\n",
      "Epoch 186, Loss: 0.4641367239628427\n",
      "Epoch 187, Loss: 0.46071131054180564\n",
      "Epoch 188, Loss: 0.46145892957673257\n",
      "Epoch 189, Loss: 0.464304908222076\n",
      "Epoch 190, Loss: 0.4606605441278432\n",
      "Epoch 191, Loss: 0.4601386308130386\n",
      "Epoch 192, Loss: 0.46193430495315263\n",
      "Epoch 193, Loss: 0.45977696912799226\n",
      "Epoch 194, Loss: 0.4597173803227511\n",
      "Epoch 195, Loss: 0.46096589922299847\n",
      "Epoch 196, Loss: 0.4592170612502741\n",
      "Epoch 197, Loss: 0.45864347419013624\n",
      "Epoch 198, Loss: 0.4594650511157848\n",
      "Epoch 199, Loss: 0.45924705942486493\n",
      "Epoch 200, Loss: 0.4584931199827434\n",
      "Epoch 201, Loss: 0.4578552480015059\n",
      "Epoch 202, Loss: 0.458251953125\n",
      "Epoch 203, Loss: 0.4586848316367108\n",
      "Epoch 204, Loss: 0.4577025349799131\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23808068402253255\n",
      "Valence RMSE: 0.24629188083061715\n",
      "Arousal RMSE: 0.22957598664971707\n",
      "Test R^2 score: tensor([0.3518, 0.6113], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4815470152677251\n",
      "Num of epochs: 205\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Epoch 121, Loss: 0.4816228295571127\n",
      "Epoch 122, Loss: 0.4807618495484584\n",
      "Epoch 123, Loss: 0.48102030846138955\n",
      "Epoch 124, Loss: 0.4810176288364116\n",
      "Epoch 125, Loss: 0.48014338261661127\n",
      "Epoch 126, Loss: 0.47910314809270116\n",
      "Epoch 127, Loss: 0.47880795949683447\n",
      "Epoch 128, Loss: 0.47876177315142754\n",
      "Epoch 129, Loss: 0.47819597116449997\n",
      "Epoch 130, Loss: 0.47743214539542544\n",
      "Epoch 131, Loss: 0.47658476230340185\n",
      "Epoch 132, Loss: 0.4759644680184978\n",
      "Epoch 133, Loss: 0.4755635187712421\n",
      "Epoch 134, Loss: 0.47538264196584307\n",
      "Epoch 135, Loss: 0.4753673450596914\n",
      "Epoch 136, Loss: 0.4753697744168057\n",
      "Epoch 137, Loss: 0.4755973109583155\n",
      "Epoch 138, Loss: 0.4767626826605241\n",
      "Epoch 139, Loss: 0.47683528222287225\n",
      "Epoch 140, Loss: 0.47579445468261566\n",
      "Epoch 141, Loss: 0.47284533919249566\n",
      "Epoch 142, Loss: 0.472809286012082\n",
      "Epoch 143, Loss: 0.47458738111232796\n",
      "Epoch 144, Loss: 0.4733380058918715\n",
      "Epoch 145, Loss: 0.4715229034484008\n",
      "Epoch 146, Loss: 0.47096730966267386\n",
      "Epoch 147, Loss: 0.47161469890467383\n",
      "Epoch 148, Loss: 0.47183531411129226\n",
      "Epoch 149, Loss: 0.470294933688076\n",
      "Epoch 150, Loss: 0.4697114144973765\n",
      "Epoch 151, Loss: 0.47018789053701304\n",
      "Epoch 152, Loss: 0.4700460953124117\n",
      "Epoch 153, Loss: 0.4693645740016508\n",
      "Epoch 154, Loss: 0.4684558104180481\n",
      "Epoch 155, Loss: 0.4680960384979934\n",
      "Epoch 156, Loss: 0.4682454254529793\n",
      "Epoch 157, Loss: 0.4682974379105488\n",
      "Epoch 158, Loss: 0.4681590010361841\n",
      "Epoch 159, Loss: 0.4673225124817593\n",
      "Epoch 160, Loss: 0.4666701428107949\n",
      "Epoch 161, Loss: 0.4663166784414499\n",
      "Epoch 162, Loss: 0.46624854535037\n",
      "Epoch 163, Loss: 0.4664611566834947\n",
      "Epoch 164, Loss: 0.46654532450127006\n",
      "Epoch 165, Loss: 0.46721845607201035\n",
      "Epoch 166, Loss: 0.4668208160006968\n",
      "Epoch 167, Loss: 0.466573749669782\n",
      "Epoch 168, Loss: 0.46506387643568103\n",
      "Epoch 169, Loss: 0.4641220516950567\n",
      "Epoch 170, Loss: 0.4644550172656552\n",
      "Epoch 171, Loss: 0.4654613047116713\n",
      "Epoch 172, Loss: 0.46880176576246846\n",
      "Epoch 173, Loss: 0.4685649824581709\n",
      "Epoch 174, Loss: 0.46606115996454733\n",
      "Epoch 175, Loss: 0.46328263242364126\n",
      "Epoch 176, Loss: 0.4660484187313888\n",
      "Epoch 177, Loss: 0.4667669789465077\n",
      "Epoch 178, Loss: 0.46222975993575777\n",
      "Epoch 179, Loss: 0.46412214801342555\n",
      "Epoch 180, Loss: 0.46867384291927394\n",
      "Epoch 181, Loss: 0.46316269230838986\n",
      "Epoch 182, Loss: 0.4619791093960296\n",
      "Epoch 183, Loss: 0.4650783107301587\n",
      "Epoch 184, Loss: 0.4617210775403838\n",
      "Epoch 185, Loss: 0.4621011464022392\n",
      "Epoch 186, Loss: 0.4641367239628427\n",
      "Epoch 187, Loss: 0.46071131054180564\n",
      "Epoch 188, Loss: 0.46145892957673257\n",
      "Epoch 189, Loss: 0.464304908222076\n",
      "Epoch 190, Loss: 0.4606605441278432\n",
      "Epoch 191, Loss: 0.4601386308130386\n",
      "Epoch 192, Loss: 0.46193430495315263\n",
      "Epoch 193, Loss: 0.45977696912799226\n",
      "Epoch 194, Loss: 0.4597173803227511\n",
      "Epoch 195, Loss: 0.46096589922299847\n",
      "Epoch 196, Loss: 0.4592170612502741\n",
      "Epoch 197, Loss: 0.45864347419013624\n",
      "Epoch 198, Loss: 0.4594650511157848\n",
      "Epoch 199, Loss: 0.45924705942486493\n",
      "Epoch 200, Loss: 0.4584931199827434\n",
      "Epoch 201, Loss: 0.4578552480015059\n",
      "Epoch 202, Loss: 0.458251953125\n",
      "Epoch 203, Loss: 0.4586848316367108\n",
      "Epoch 204, Loss: 0.4577025349799131\n",
      "Epoch 205, Loss: 0.45707273702530193\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2386399843516593\n",
      "Valence RMSE: 0.2476147644810631\n",
      "Arousal RMSE: 0.2293142225717979\n",
      "Test R^2 score: tensor([0.3448, 0.6122], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.47849895308562274\n",
      "Num of epochs: 206\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Epoch 121, Loss: 0.4816228295571127\n",
      "Epoch 122, Loss: 0.4807618495484584\n",
      "Epoch 123, Loss: 0.48102030846138955\n",
      "Epoch 124, Loss: 0.4810176288364116\n",
      "Epoch 125, Loss: 0.48014338261661127\n",
      "Epoch 126, Loss: 0.47910314809270116\n",
      "Epoch 127, Loss: 0.47880795949683447\n",
      "Epoch 128, Loss: 0.47876177315142754\n",
      "Epoch 129, Loss: 0.47819597116449997\n",
      "Epoch 130, Loss: 0.47743214539542544\n",
      "Epoch 131, Loss: 0.47658476230340185\n",
      "Epoch 132, Loss: 0.4759644680184978\n",
      "Epoch 133, Loss: 0.4755635187712421\n",
      "Epoch 134, Loss: 0.47538264196584307\n",
      "Epoch 135, Loss: 0.4753673450596914\n",
      "Epoch 136, Loss: 0.4753697744168057\n",
      "Epoch 137, Loss: 0.4755973109583155\n",
      "Epoch 138, Loss: 0.4767626826605241\n",
      "Epoch 139, Loss: 0.47683528222287225\n",
      "Epoch 140, Loss: 0.47579445468261566\n",
      "Epoch 141, Loss: 0.47284533919249566\n",
      "Epoch 142, Loss: 0.472809286012082\n",
      "Epoch 143, Loss: 0.47458738111232796\n",
      "Epoch 144, Loss: 0.4733380058918715\n",
      "Epoch 145, Loss: 0.4715229034484008\n",
      "Epoch 146, Loss: 0.47096730966267386\n",
      "Epoch 147, Loss: 0.47161469890467383\n",
      "Epoch 148, Loss: 0.47183531411129226\n",
      "Epoch 149, Loss: 0.470294933688076\n",
      "Epoch 150, Loss: 0.4697114144973765\n",
      "Epoch 151, Loss: 0.47018789053701304\n",
      "Epoch 152, Loss: 0.4700460953124117\n",
      "Epoch 153, Loss: 0.4693645740016508\n",
      "Epoch 154, Loss: 0.4684558104180481\n",
      "Epoch 155, Loss: 0.4680960384979934\n",
      "Epoch 156, Loss: 0.4682454254529793\n",
      "Epoch 157, Loss: 0.4682974379105488\n",
      "Epoch 158, Loss: 0.4681590010361841\n",
      "Epoch 159, Loss: 0.4673225124817593\n",
      "Epoch 160, Loss: 0.4666701428107949\n",
      "Epoch 161, Loss: 0.4663166784414499\n",
      "Epoch 162, Loss: 0.46624854535037\n",
      "Epoch 163, Loss: 0.4664611566834947\n",
      "Epoch 164, Loss: 0.46654532450127006\n",
      "Epoch 165, Loss: 0.46721845607201035\n",
      "Epoch 166, Loss: 0.4668208160006968\n",
      "Epoch 167, Loss: 0.466573749669782\n",
      "Epoch 168, Loss: 0.46506387643568103\n",
      "Epoch 169, Loss: 0.4641220516950567\n",
      "Epoch 170, Loss: 0.4644550172656552\n",
      "Epoch 171, Loss: 0.4654613047116713\n",
      "Epoch 172, Loss: 0.46880176576246846\n",
      "Epoch 173, Loss: 0.4685649824581709\n",
      "Epoch 174, Loss: 0.46606115996454733\n",
      "Epoch 175, Loss: 0.46328263242364126\n",
      "Epoch 176, Loss: 0.4660484187313888\n",
      "Epoch 177, Loss: 0.4667669789465077\n",
      "Epoch 178, Loss: 0.46222975993575777\n",
      "Epoch 179, Loss: 0.46412214801342555\n",
      "Epoch 180, Loss: 0.46867384291927394\n",
      "Epoch 181, Loss: 0.46316269230838986\n",
      "Epoch 182, Loss: 0.4619791093960296\n",
      "Epoch 183, Loss: 0.4650783107301587\n",
      "Epoch 184, Loss: 0.4617210775403838\n",
      "Epoch 185, Loss: 0.4621011464022392\n",
      "Epoch 186, Loss: 0.4641367239628427\n",
      "Epoch 187, Loss: 0.46071131054180564\n",
      "Epoch 188, Loss: 0.46145892957673257\n",
      "Epoch 189, Loss: 0.464304908222076\n",
      "Epoch 190, Loss: 0.4606605441278432\n",
      "Epoch 191, Loss: 0.4601386308130386\n",
      "Epoch 192, Loss: 0.46193430495315263\n",
      "Epoch 193, Loss: 0.45977696912799226\n",
      "Epoch 194, Loss: 0.4597173803227511\n",
      "Epoch 195, Loss: 0.46096589922299847\n",
      "Epoch 196, Loss: 0.4592170612502741\n",
      "Epoch 197, Loss: 0.45864347419013624\n",
      "Epoch 198, Loss: 0.4594650511157848\n",
      "Epoch 199, Loss: 0.45924705942486493\n",
      "Epoch 200, Loss: 0.4584931199827434\n",
      "Epoch 201, Loss: 0.4578552480015059\n",
      "Epoch 202, Loss: 0.458251953125\n",
      "Epoch 203, Loss: 0.4586848316367108\n",
      "Epoch 204, Loss: 0.4577025349799131\n",
      "Epoch 205, Loss: 0.45707273702530193\n",
      "Epoch 206, Loss: 0.4571336158814618\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23843819165518557\n",
      "Valence RMSE: 0.2451638080276933\n",
      "Arousal RMSE: 0.2315172773530095\n",
      "Test R^2 score: tensor([0.3577, 0.6047], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.48120843295715415\n",
      "Num of epochs: 207\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Epoch 121, Loss: 0.4816228295571127\n",
      "Epoch 122, Loss: 0.4807618495484584\n",
      "Epoch 123, Loss: 0.48102030846138955\n",
      "Epoch 124, Loss: 0.4810176288364116\n",
      "Epoch 125, Loss: 0.48014338261661127\n",
      "Epoch 126, Loss: 0.47910314809270116\n",
      "Epoch 127, Loss: 0.47880795949683447\n",
      "Epoch 128, Loss: 0.47876177315142754\n",
      "Epoch 129, Loss: 0.47819597116449997\n",
      "Epoch 130, Loss: 0.47743214539542544\n",
      "Epoch 131, Loss: 0.47658476230340185\n",
      "Epoch 132, Loss: 0.4759644680184978\n",
      "Epoch 133, Loss: 0.4755635187712421\n",
      "Epoch 134, Loss: 0.47538264196584307\n",
      "Epoch 135, Loss: 0.4753673450596914\n",
      "Epoch 136, Loss: 0.4753697744168057\n",
      "Epoch 137, Loss: 0.4755973109583155\n",
      "Epoch 138, Loss: 0.4767626826605241\n",
      "Epoch 139, Loss: 0.47683528222287225\n",
      "Epoch 140, Loss: 0.47579445468261566\n",
      "Epoch 141, Loss: 0.47284533919249566\n",
      "Epoch 142, Loss: 0.472809286012082\n",
      "Epoch 143, Loss: 0.47458738111232796\n",
      "Epoch 144, Loss: 0.4733380058918715\n",
      "Epoch 145, Loss: 0.4715229034484008\n",
      "Epoch 146, Loss: 0.47096730966267386\n",
      "Epoch 147, Loss: 0.47161469890467383\n",
      "Epoch 148, Loss: 0.47183531411129226\n",
      "Epoch 149, Loss: 0.470294933688076\n",
      "Epoch 150, Loss: 0.4697114144973765\n",
      "Epoch 151, Loss: 0.47018789053701304\n",
      "Epoch 152, Loss: 0.4700460953124117\n",
      "Epoch 153, Loss: 0.4693645740016508\n",
      "Epoch 154, Loss: 0.4684558104180481\n",
      "Epoch 155, Loss: 0.4680960384979934\n",
      "Epoch 156, Loss: 0.4682454254529793\n",
      "Epoch 157, Loss: 0.4682974379105488\n",
      "Epoch 158, Loss: 0.4681590010361841\n",
      "Epoch 159, Loss: 0.4673225124817593\n",
      "Epoch 160, Loss: 0.4666701428107949\n",
      "Epoch 161, Loss: 0.4663166784414499\n",
      "Epoch 162, Loss: 0.46624854535037\n",
      "Epoch 163, Loss: 0.4664611566834947\n",
      "Epoch 164, Loss: 0.46654532450127006\n",
      "Epoch 165, Loss: 0.46721845607201035\n",
      "Epoch 166, Loss: 0.4668208160006968\n",
      "Epoch 167, Loss: 0.466573749669782\n",
      "Epoch 168, Loss: 0.46506387643568103\n",
      "Epoch 169, Loss: 0.4641220516950567\n",
      "Epoch 170, Loss: 0.4644550172656552\n",
      "Epoch 171, Loss: 0.4654613047116713\n",
      "Epoch 172, Loss: 0.46880176576246846\n",
      "Epoch 173, Loss: 0.4685649824581709\n",
      "Epoch 174, Loss: 0.46606115996454733\n",
      "Epoch 175, Loss: 0.46328263242364126\n",
      "Epoch 176, Loss: 0.4660484187313888\n",
      "Epoch 177, Loss: 0.4667669789465077\n",
      "Epoch 178, Loss: 0.46222975993575777\n",
      "Epoch 179, Loss: 0.46412214801342555\n",
      "Epoch 180, Loss: 0.46867384291927394\n",
      "Epoch 181, Loss: 0.46316269230838986\n",
      "Epoch 182, Loss: 0.4619791093960296\n",
      "Epoch 183, Loss: 0.4650783107301587\n",
      "Epoch 184, Loss: 0.4617210775403838\n",
      "Epoch 185, Loss: 0.4621011464022392\n",
      "Epoch 186, Loss: 0.4641367239628427\n",
      "Epoch 187, Loss: 0.46071131054180564\n",
      "Epoch 188, Loss: 0.46145892957673257\n",
      "Epoch 189, Loss: 0.464304908222076\n",
      "Epoch 190, Loss: 0.4606605441278432\n",
      "Epoch 191, Loss: 0.4601386308130386\n",
      "Epoch 192, Loss: 0.46193430495315263\n",
      "Epoch 193, Loss: 0.45977696912799226\n",
      "Epoch 194, Loss: 0.4597173803227511\n",
      "Epoch 195, Loss: 0.46096589922299847\n",
      "Epoch 196, Loss: 0.4592170612502741\n",
      "Epoch 197, Loss: 0.45864347419013624\n",
      "Epoch 198, Loss: 0.4594650511157848\n",
      "Epoch 199, Loss: 0.45924705942486493\n",
      "Epoch 200, Loss: 0.4584931199827434\n",
      "Epoch 201, Loss: 0.4578552480015059\n",
      "Epoch 202, Loss: 0.458251953125\n",
      "Epoch 203, Loss: 0.4586848316367108\n",
      "Epoch 204, Loss: 0.4577025349799131\n",
      "Epoch 205, Loss: 0.45707273702530193\n",
      "Epoch 206, Loss: 0.4571336158814618\n",
      "Epoch 207, Loss: 0.45746758002181026\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23932373420941042\n",
      "Valence RMSE: 0.24865354058661182\n",
      "Arousal RMSE: 0.22961514816234416\n",
      "Test R^2 score: tensor([0.3393, 0.6112], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.475235327267808\n",
      "Num of epochs: 208\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Epoch 121, Loss: 0.4816228295571127\n",
      "Epoch 122, Loss: 0.4807618495484584\n",
      "Epoch 123, Loss: 0.48102030846138955\n",
      "Epoch 124, Loss: 0.4810176288364116\n",
      "Epoch 125, Loss: 0.48014338261661127\n",
      "Epoch 126, Loss: 0.47910314809270116\n",
      "Epoch 127, Loss: 0.47880795949683447\n",
      "Epoch 128, Loss: 0.47876177315142754\n",
      "Epoch 129, Loss: 0.47819597116449997\n",
      "Epoch 130, Loss: 0.47743214539542544\n",
      "Epoch 131, Loss: 0.47658476230340185\n",
      "Epoch 132, Loss: 0.4759644680184978\n",
      "Epoch 133, Loss: 0.4755635187712421\n",
      "Epoch 134, Loss: 0.47538264196584307\n",
      "Epoch 135, Loss: 0.4753673450596914\n",
      "Epoch 136, Loss: 0.4753697744168057\n",
      "Epoch 137, Loss: 0.4755973109583155\n",
      "Epoch 138, Loss: 0.4767626826605241\n",
      "Epoch 139, Loss: 0.47683528222287225\n",
      "Epoch 140, Loss: 0.47579445468261566\n",
      "Epoch 141, Loss: 0.47284533919249566\n",
      "Epoch 142, Loss: 0.472809286012082\n",
      "Epoch 143, Loss: 0.47458738111232796\n",
      "Epoch 144, Loss: 0.4733380058918715\n",
      "Epoch 145, Loss: 0.4715229034484008\n",
      "Epoch 146, Loss: 0.47096730966267386\n",
      "Epoch 147, Loss: 0.47161469890467383\n",
      "Epoch 148, Loss: 0.47183531411129226\n",
      "Epoch 149, Loss: 0.470294933688076\n",
      "Epoch 150, Loss: 0.4697114144973765\n",
      "Epoch 151, Loss: 0.47018789053701304\n",
      "Epoch 152, Loss: 0.4700460953124117\n",
      "Epoch 153, Loss: 0.4693645740016508\n",
      "Epoch 154, Loss: 0.4684558104180481\n",
      "Epoch 155, Loss: 0.4680960384979934\n",
      "Epoch 156, Loss: 0.4682454254529793\n",
      "Epoch 157, Loss: 0.4682974379105488\n",
      "Epoch 158, Loss: 0.4681590010361841\n",
      "Epoch 159, Loss: 0.4673225124817593\n",
      "Epoch 160, Loss: 0.4666701428107949\n",
      "Epoch 161, Loss: 0.4663166784414499\n",
      "Epoch 162, Loss: 0.46624854535037\n",
      "Epoch 163, Loss: 0.4664611566834947\n",
      "Epoch 164, Loss: 0.46654532450127006\n",
      "Epoch 165, Loss: 0.46721845607201035\n",
      "Epoch 166, Loss: 0.4668208160006968\n",
      "Epoch 167, Loss: 0.466573749669782\n",
      "Epoch 168, Loss: 0.46506387643568103\n",
      "Epoch 169, Loss: 0.4641220516950567\n",
      "Epoch 170, Loss: 0.4644550172656552\n",
      "Epoch 171, Loss: 0.4654613047116713\n",
      "Epoch 172, Loss: 0.46880176576246846\n",
      "Epoch 173, Loss: 0.4685649824581709\n",
      "Epoch 174, Loss: 0.46606115996454733\n",
      "Epoch 175, Loss: 0.46328263242364126\n",
      "Epoch 176, Loss: 0.4660484187313888\n",
      "Epoch 177, Loss: 0.4667669789465077\n",
      "Epoch 178, Loss: 0.46222975993575777\n",
      "Epoch 179, Loss: 0.46412214801342555\n",
      "Epoch 180, Loss: 0.46867384291927394\n",
      "Epoch 181, Loss: 0.46316269230838986\n",
      "Epoch 182, Loss: 0.4619791093960296\n",
      "Epoch 183, Loss: 0.4650783107301587\n",
      "Epoch 184, Loss: 0.4617210775403838\n",
      "Epoch 185, Loss: 0.4621011464022392\n",
      "Epoch 186, Loss: 0.4641367239628427\n",
      "Epoch 187, Loss: 0.46071131054180564\n",
      "Epoch 188, Loss: 0.46145892957673257\n",
      "Epoch 189, Loss: 0.464304908222076\n",
      "Epoch 190, Loss: 0.4606605441278432\n",
      "Epoch 191, Loss: 0.4601386308130386\n",
      "Epoch 192, Loss: 0.46193430495315263\n",
      "Epoch 193, Loss: 0.45977696912799226\n",
      "Epoch 194, Loss: 0.4597173803227511\n",
      "Epoch 195, Loss: 0.46096589922299847\n",
      "Epoch 196, Loss: 0.4592170612502741\n",
      "Epoch 197, Loss: 0.45864347419013624\n",
      "Epoch 198, Loss: 0.4594650511157848\n",
      "Epoch 199, Loss: 0.45924705942486493\n",
      "Epoch 200, Loss: 0.4584931199827434\n",
      "Epoch 201, Loss: 0.4578552480015059\n",
      "Epoch 202, Loss: 0.458251953125\n",
      "Epoch 203, Loss: 0.4586848316367108\n",
      "Epoch 204, Loss: 0.4577025349799131\n",
      "Epoch 205, Loss: 0.45707273702530193\n",
      "Epoch 206, Loss: 0.4571336158814618\n",
      "Epoch 207, Loss: 0.45746758002181026\n",
      "Epoch 208, Loss: 0.4580334166179955\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2382928848851467\n",
      "Valence RMSE: 0.24491707983081948\n",
      "Arousal RMSE: 0.23147920420831658\n",
      "Test R^2 score: tensor([0.3590, 0.6048], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.48191949150849817\n",
      "Num of epochs: 209\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Epoch 121, Loss: 0.4816228295571127\n",
      "Epoch 122, Loss: 0.4807618495484584\n",
      "Epoch 123, Loss: 0.48102030846138955\n",
      "Epoch 124, Loss: 0.4810176288364116\n",
      "Epoch 125, Loss: 0.48014338261661127\n",
      "Epoch 126, Loss: 0.47910314809270116\n",
      "Epoch 127, Loss: 0.47880795949683447\n",
      "Epoch 128, Loss: 0.47876177315142754\n",
      "Epoch 129, Loss: 0.47819597116449997\n",
      "Epoch 130, Loss: 0.47743214539542544\n",
      "Epoch 131, Loss: 0.47658476230340185\n",
      "Epoch 132, Loss: 0.4759644680184978\n",
      "Epoch 133, Loss: 0.4755635187712421\n",
      "Epoch 134, Loss: 0.47538264196584307\n",
      "Epoch 135, Loss: 0.4753673450596914\n",
      "Epoch 136, Loss: 0.4753697744168057\n",
      "Epoch 137, Loss: 0.4755973109583155\n",
      "Epoch 138, Loss: 0.4767626826605241\n",
      "Epoch 139, Loss: 0.47683528222287225\n",
      "Epoch 140, Loss: 0.47579445468261566\n",
      "Epoch 141, Loss: 0.47284533919249566\n",
      "Epoch 142, Loss: 0.472809286012082\n",
      "Epoch 143, Loss: 0.47458738111232796\n",
      "Epoch 144, Loss: 0.4733380058918715\n",
      "Epoch 145, Loss: 0.4715229034484008\n",
      "Epoch 146, Loss: 0.47096730966267386\n",
      "Epoch 147, Loss: 0.47161469890467383\n",
      "Epoch 148, Loss: 0.47183531411129226\n",
      "Epoch 149, Loss: 0.470294933688076\n",
      "Epoch 150, Loss: 0.4697114144973765\n",
      "Epoch 151, Loss: 0.47018789053701304\n",
      "Epoch 152, Loss: 0.4700460953124117\n",
      "Epoch 153, Loss: 0.4693645740016508\n",
      "Epoch 154, Loss: 0.4684558104180481\n",
      "Epoch 155, Loss: 0.4680960384979934\n",
      "Epoch 156, Loss: 0.4682454254529793\n",
      "Epoch 157, Loss: 0.4682974379105488\n",
      "Epoch 158, Loss: 0.4681590010361841\n",
      "Epoch 159, Loss: 0.4673225124817593\n",
      "Epoch 160, Loss: 0.4666701428107949\n",
      "Epoch 161, Loss: 0.4663166784414499\n",
      "Epoch 162, Loss: 0.46624854535037\n",
      "Epoch 163, Loss: 0.4664611566834947\n",
      "Epoch 164, Loss: 0.46654532450127006\n",
      "Epoch 165, Loss: 0.46721845607201035\n",
      "Epoch 166, Loss: 0.4668208160006968\n",
      "Epoch 167, Loss: 0.466573749669782\n",
      "Epoch 168, Loss: 0.46506387643568103\n",
      "Epoch 169, Loss: 0.4641220516950567\n",
      "Epoch 170, Loss: 0.4644550172656552\n",
      "Epoch 171, Loss: 0.4654613047116713\n",
      "Epoch 172, Loss: 0.46880176576246846\n",
      "Epoch 173, Loss: 0.4685649824581709\n",
      "Epoch 174, Loss: 0.46606115996454733\n",
      "Epoch 175, Loss: 0.46328263242364126\n",
      "Epoch 176, Loss: 0.4660484187313888\n",
      "Epoch 177, Loss: 0.4667669789465077\n",
      "Epoch 178, Loss: 0.46222975993575777\n",
      "Epoch 179, Loss: 0.46412214801342555\n",
      "Epoch 180, Loss: 0.46867384291927394\n",
      "Epoch 181, Loss: 0.46316269230838986\n",
      "Epoch 182, Loss: 0.4619791093960296\n",
      "Epoch 183, Loss: 0.4650783107301587\n",
      "Epoch 184, Loss: 0.4617210775403838\n",
      "Epoch 185, Loss: 0.4621011464022392\n",
      "Epoch 186, Loss: 0.4641367239628427\n",
      "Epoch 187, Loss: 0.46071131054180564\n",
      "Epoch 188, Loss: 0.46145892957673257\n",
      "Epoch 189, Loss: 0.464304908222076\n",
      "Epoch 190, Loss: 0.4606605441278432\n",
      "Epoch 191, Loss: 0.4601386308130386\n",
      "Epoch 192, Loss: 0.46193430495315263\n",
      "Epoch 193, Loss: 0.45977696912799226\n",
      "Epoch 194, Loss: 0.4597173803227511\n",
      "Epoch 195, Loss: 0.46096589922299847\n",
      "Epoch 196, Loss: 0.4592170612502741\n",
      "Epoch 197, Loss: 0.45864347419013624\n",
      "Epoch 198, Loss: 0.4594650511157848\n",
      "Epoch 199, Loss: 0.45924705942486493\n",
      "Epoch 200, Loss: 0.4584931199827434\n",
      "Epoch 201, Loss: 0.4578552480015059\n",
      "Epoch 202, Loss: 0.458251953125\n",
      "Epoch 203, Loss: 0.4586848316367108\n",
      "Epoch 204, Loss: 0.4577025349799131\n",
      "Epoch 205, Loss: 0.45707273702530193\n",
      "Epoch 206, Loss: 0.4571336158814618\n",
      "Epoch 207, Loss: 0.45746758002181026\n",
      "Epoch 208, Loss: 0.4580334166179955\n",
      "Epoch 209, Loss: 0.457551138813026\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23858728994194534\n",
      "Valence RMSE: 0.24839204008524876\n",
      "Arousal RMSE: 0.22836195888538863\n",
      "Test R^2 score: tensor([0.3407, 0.6154], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4780461585956469\n",
      "Num of epochs: 210\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Epoch 121, Loss: 0.4816228295571127\n",
      "Epoch 122, Loss: 0.4807618495484584\n",
      "Epoch 123, Loss: 0.48102030846138955\n",
      "Epoch 124, Loss: 0.4810176288364116\n",
      "Epoch 125, Loss: 0.48014338261661127\n",
      "Epoch 126, Loss: 0.47910314809270116\n",
      "Epoch 127, Loss: 0.47880795949683447\n",
      "Epoch 128, Loss: 0.47876177315142754\n",
      "Epoch 129, Loss: 0.47819597116449997\n",
      "Epoch 130, Loss: 0.47743214539542544\n",
      "Epoch 131, Loss: 0.47658476230340185\n",
      "Epoch 132, Loss: 0.4759644680184978\n",
      "Epoch 133, Loss: 0.4755635187712421\n",
      "Epoch 134, Loss: 0.47538264196584307\n",
      "Epoch 135, Loss: 0.4753673450596914\n",
      "Epoch 136, Loss: 0.4753697744168057\n",
      "Epoch 137, Loss: 0.4755973109583155\n",
      "Epoch 138, Loss: 0.4767626826605241\n",
      "Epoch 139, Loss: 0.47683528222287225\n",
      "Epoch 140, Loss: 0.47579445468261566\n",
      "Epoch 141, Loss: 0.47284533919249566\n",
      "Epoch 142, Loss: 0.472809286012082\n",
      "Epoch 143, Loss: 0.47458738111232796\n",
      "Epoch 144, Loss: 0.4733380058918715\n",
      "Epoch 145, Loss: 0.4715229034484008\n",
      "Epoch 146, Loss: 0.47096730966267386\n",
      "Epoch 147, Loss: 0.47161469890467383\n",
      "Epoch 148, Loss: 0.47183531411129226\n",
      "Epoch 149, Loss: 0.470294933688076\n",
      "Epoch 150, Loss: 0.4697114144973765\n",
      "Epoch 151, Loss: 0.47018789053701304\n",
      "Epoch 152, Loss: 0.4700460953124117\n",
      "Epoch 153, Loss: 0.4693645740016508\n",
      "Epoch 154, Loss: 0.4684558104180481\n",
      "Epoch 155, Loss: 0.4680960384979934\n",
      "Epoch 156, Loss: 0.4682454254529793\n",
      "Epoch 157, Loss: 0.4682974379105488\n",
      "Epoch 158, Loss: 0.4681590010361841\n",
      "Epoch 159, Loss: 0.4673225124817593\n",
      "Epoch 160, Loss: 0.4666701428107949\n",
      "Epoch 161, Loss: 0.4663166784414499\n",
      "Epoch 162, Loss: 0.46624854535037\n",
      "Epoch 163, Loss: 0.4664611566834947\n",
      "Epoch 164, Loss: 0.46654532450127006\n",
      "Epoch 165, Loss: 0.46721845607201035\n",
      "Epoch 166, Loss: 0.4668208160006968\n",
      "Epoch 167, Loss: 0.466573749669782\n",
      "Epoch 168, Loss: 0.46506387643568103\n",
      "Epoch 169, Loss: 0.4641220516950567\n",
      "Epoch 170, Loss: 0.4644550172656552\n",
      "Epoch 171, Loss: 0.4654613047116713\n",
      "Epoch 172, Loss: 0.46880176576246846\n",
      "Epoch 173, Loss: 0.4685649824581709\n",
      "Epoch 174, Loss: 0.46606115996454733\n",
      "Epoch 175, Loss: 0.46328263242364126\n",
      "Epoch 176, Loss: 0.4660484187313888\n",
      "Epoch 177, Loss: 0.4667669789465077\n",
      "Epoch 178, Loss: 0.46222975993575777\n",
      "Epoch 179, Loss: 0.46412214801342555\n",
      "Epoch 180, Loss: 0.46867384291927394\n",
      "Epoch 181, Loss: 0.46316269230838986\n",
      "Epoch 182, Loss: 0.4619791093960296\n",
      "Epoch 183, Loss: 0.4650783107301587\n",
      "Epoch 184, Loss: 0.4617210775403838\n",
      "Epoch 185, Loss: 0.4621011464022392\n",
      "Epoch 186, Loss: 0.4641367239628427\n",
      "Epoch 187, Loss: 0.46071131054180564\n",
      "Epoch 188, Loss: 0.46145892957673257\n",
      "Epoch 189, Loss: 0.464304908222076\n",
      "Epoch 190, Loss: 0.4606605441278432\n",
      "Epoch 191, Loss: 0.4601386308130386\n",
      "Epoch 192, Loss: 0.46193430495315263\n",
      "Epoch 193, Loss: 0.45977696912799226\n",
      "Epoch 194, Loss: 0.4597173803227511\n",
      "Epoch 195, Loss: 0.46096589922299847\n",
      "Epoch 196, Loss: 0.4592170612502741\n",
      "Epoch 197, Loss: 0.45864347419013624\n",
      "Epoch 198, Loss: 0.4594650511157848\n",
      "Epoch 199, Loss: 0.45924705942486493\n",
      "Epoch 200, Loss: 0.4584931199827434\n",
      "Epoch 201, Loss: 0.4578552480015059\n",
      "Epoch 202, Loss: 0.458251953125\n",
      "Epoch 203, Loss: 0.4586848316367108\n",
      "Epoch 204, Loss: 0.4577025349799131\n",
      "Epoch 205, Loss: 0.45707273702530193\n",
      "Epoch 206, Loss: 0.4571336158814618\n",
      "Epoch 207, Loss: 0.45746758002181026\n",
      "Epoch 208, Loss: 0.4580334166179955\n",
      "Epoch 209, Loss: 0.457551138813026\n",
      "Epoch 210, Loss: 0.4572461918817771\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23739641463629657\n",
      "Valence RMSE: 0.24495314897441609\n",
      "Arousal RMSE: 0.22959109340707104\n",
      "Test R^2 score: tensor([0.3588, 0.6113], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4850352271033983\n",
      "Num of epochs: 211\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Epoch 121, Loss: 0.4816228295571127\n",
      "Epoch 122, Loss: 0.4807618495484584\n",
      "Epoch 123, Loss: 0.48102030846138955\n",
      "Epoch 124, Loss: 0.4810176288364116\n",
      "Epoch 125, Loss: 0.48014338261661127\n",
      "Epoch 126, Loss: 0.47910314809270116\n",
      "Epoch 127, Loss: 0.47880795949683447\n",
      "Epoch 128, Loss: 0.47876177315142754\n",
      "Epoch 129, Loss: 0.47819597116449997\n",
      "Epoch 130, Loss: 0.47743214539542544\n",
      "Epoch 131, Loss: 0.47658476230340185\n",
      "Epoch 132, Loss: 0.4759644680184978\n",
      "Epoch 133, Loss: 0.4755635187712421\n",
      "Epoch 134, Loss: 0.47538264196584307\n",
      "Epoch 135, Loss: 0.4753673450596914\n",
      "Epoch 136, Loss: 0.4753697744168057\n",
      "Epoch 137, Loss: 0.4755973109583155\n",
      "Epoch 138, Loss: 0.4767626826605241\n",
      "Epoch 139, Loss: 0.47683528222287225\n",
      "Epoch 140, Loss: 0.47579445468261566\n",
      "Epoch 141, Loss: 0.47284533919249566\n",
      "Epoch 142, Loss: 0.472809286012082\n",
      "Epoch 143, Loss: 0.47458738111232796\n",
      "Epoch 144, Loss: 0.4733380058918715\n",
      "Epoch 145, Loss: 0.4715229034484008\n",
      "Epoch 146, Loss: 0.47096730966267386\n",
      "Epoch 147, Loss: 0.47161469890467383\n",
      "Epoch 148, Loss: 0.47183531411129226\n",
      "Epoch 149, Loss: 0.470294933688076\n",
      "Epoch 150, Loss: 0.4697114144973765\n",
      "Epoch 151, Loss: 0.47018789053701304\n",
      "Epoch 152, Loss: 0.4700460953124117\n",
      "Epoch 153, Loss: 0.4693645740016508\n",
      "Epoch 154, Loss: 0.4684558104180481\n",
      "Epoch 155, Loss: 0.4680960384979934\n",
      "Epoch 156, Loss: 0.4682454254529793\n",
      "Epoch 157, Loss: 0.4682974379105488\n",
      "Epoch 158, Loss: 0.4681590010361841\n",
      "Epoch 159, Loss: 0.4673225124817593\n",
      "Epoch 160, Loss: 0.4666701428107949\n",
      "Epoch 161, Loss: 0.4663166784414499\n",
      "Epoch 162, Loss: 0.46624854535037\n",
      "Epoch 163, Loss: 0.4664611566834947\n",
      "Epoch 164, Loss: 0.46654532450127006\n",
      "Epoch 165, Loss: 0.46721845607201035\n",
      "Epoch 166, Loss: 0.4668208160006968\n",
      "Epoch 167, Loss: 0.466573749669782\n",
      "Epoch 168, Loss: 0.46506387643568103\n",
      "Epoch 169, Loss: 0.4641220516950567\n",
      "Epoch 170, Loss: 0.4644550172656552\n",
      "Epoch 171, Loss: 0.4654613047116713\n",
      "Epoch 172, Loss: 0.46880176576246846\n",
      "Epoch 173, Loss: 0.4685649824581709\n",
      "Epoch 174, Loss: 0.46606115996454733\n",
      "Epoch 175, Loss: 0.46328263242364126\n",
      "Epoch 176, Loss: 0.4660484187313888\n",
      "Epoch 177, Loss: 0.4667669789465077\n",
      "Epoch 178, Loss: 0.46222975993575777\n",
      "Epoch 179, Loss: 0.46412214801342555\n",
      "Epoch 180, Loss: 0.46867384291927394\n",
      "Epoch 181, Loss: 0.46316269230838986\n",
      "Epoch 182, Loss: 0.4619791093960296\n",
      "Epoch 183, Loss: 0.4650783107301587\n",
      "Epoch 184, Loss: 0.4617210775403838\n",
      "Epoch 185, Loss: 0.4621011464022392\n",
      "Epoch 186, Loss: 0.4641367239628427\n",
      "Epoch 187, Loss: 0.46071131054180564\n",
      "Epoch 188, Loss: 0.46145892957673257\n",
      "Epoch 189, Loss: 0.464304908222076\n",
      "Epoch 190, Loss: 0.4606605441278432\n",
      "Epoch 191, Loss: 0.4601386308130386\n",
      "Epoch 192, Loss: 0.46193430495315263\n",
      "Epoch 193, Loss: 0.45977696912799226\n",
      "Epoch 194, Loss: 0.4597173803227511\n",
      "Epoch 195, Loss: 0.46096589922299847\n",
      "Epoch 196, Loss: 0.4592170612502741\n",
      "Epoch 197, Loss: 0.45864347419013624\n",
      "Epoch 198, Loss: 0.4594650511157848\n",
      "Epoch 199, Loss: 0.45924705942486493\n",
      "Epoch 200, Loss: 0.4584931199827434\n",
      "Epoch 201, Loss: 0.4578552480015059\n",
      "Epoch 202, Loss: 0.458251953125\n",
      "Epoch 203, Loss: 0.4586848316367108\n",
      "Epoch 204, Loss: 0.4577025349799131\n",
      "Epoch 205, Loss: 0.45707273702530193\n",
      "Epoch 206, Loss: 0.4571336158814618\n",
      "Epoch 207, Loss: 0.45746758002181026\n",
      "Epoch 208, Loss: 0.4580334166179955\n",
      "Epoch 209, Loss: 0.457551138813026\n",
      "Epoch 210, Loss: 0.4572461918817771\n",
      "Epoch 211, Loss: 0.45629613394883295\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23734037970655256\n",
      "Valence RMSE: 0.24564568164942982\n",
      "Arousal RMSE: 0.2287337114757859\n",
      "Test R^2 score: tensor([0.3552, 0.6141], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.48466894670740496\n",
      "Num of epochs: 212\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Epoch 121, Loss: 0.4816228295571127\n",
      "Epoch 122, Loss: 0.4807618495484584\n",
      "Epoch 123, Loss: 0.48102030846138955\n",
      "Epoch 124, Loss: 0.4810176288364116\n",
      "Epoch 125, Loss: 0.48014338261661127\n",
      "Epoch 126, Loss: 0.47910314809270116\n",
      "Epoch 127, Loss: 0.47880795949683447\n",
      "Epoch 128, Loss: 0.47876177315142754\n",
      "Epoch 129, Loss: 0.47819597116449997\n",
      "Epoch 130, Loss: 0.47743214539542544\n",
      "Epoch 131, Loss: 0.47658476230340185\n",
      "Epoch 132, Loss: 0.4759644680184978\n",
      "Epoch 133, Loss: 0.4755635187712421\n",
      "Epoch 134, Loss: 0.47538264196584307\n",
      "Epoch 135, Loss: 0.4753673450596914\n",
      "Epoch 136, Loss: 0.4753697744168057\n",
      "Epoch 137, Loss: 0.4755973109583155\n",
      "Epoch 138, Loss: 0.4767626826605241\n",
      "Epoch 139, Loss: 0.47683528222287225\n",
      "Epoch 140, Loss: 0.47579445468261566\n",
      "Epoch 141, Loss: 0.47284533919249566\n",
      "Epoch 142, Loss: 0.472809286012082\n",
      "Epoch 143, Loss: 0.47458738111232796\n",
      "Epoch 144, Loss: 0.4733380058918715\n",
      "Epoch 145, Loss: 0.4715229034484008\n",
      "Epoch 146, Loss: 0.47096730966267386\n",
      "Epoch 147, Loss: 0.47161469890467383\n",
      "Epoch 148, Loss: 0.47183531411129226\n",
      "Epoch 149, Loss: 0.470294933688076\n",
      "Epoch 150, Loss: 0.4697114144973765\n",
      "Epoch 151, Loss: 0.47018789053701304\n",
      "Epoch 152, Loss: 0.4700460953124117\n",
      "Epoch 153, Loss: 0.4693645740016508\n",
      "Epoch 154, Loss: 0.4684558104180481\n",
      "Epoch 155, Loss: 0.4680960384979934\n",
      "Epoch 156, Loss: 0.4682454254529793\n",
      "Epoch 157, Loss: 0.4682974379105488\n",
      "Epoch 158, Loss: 0.4681590010361841\n",
      "Epoch 159, Loss: 0.4673225124817593\n",
      "Epoch 160, Loss: 0.4666701428107949\n",
      "Epoch 161, Loss: 0.4663166784414499\n",
      "Epoch 162, Loss: 0.46624854535037\n",
      "Epoch 163, Loss: 0.4664611566834947\n",
      "Epoch 164, Loss: 0.46654532450127006\n",
      "Epoch 165, Loss: 0.46721845607201035\n",
      "Epoch 166, Loss: 0.4668208160006968\n",
      "Epoch 167, Loss: 0.466573749669782\n",
      "Epoch 168, Loss: 0.46506387643568103\n",
      "Epoch 169, Loss: 0.4641220516950567\n",
      "Epoch 170, Loss: 0.4644550172656552\n",
      "Epoch 171, Loss: 0.4654613047116713\n",
      "Epoch 172, Loss: 0.46880176576246846\n",
      "Epoch 173, Loss: 0.4685649824581709\n",
      "Epoch 174, Loss: 0.46606115996454733\n",
      "Epoch 175, Loss: 0.46328263242364126\n",
      "Epoch 176, Loss: 0.4660484187313888\n",
      "Epoch 177, Loss: 0.4667669789465077\n",
      "Epoch 178, Loss: 0.46222975993575777\n",
      "Epoch 179, Loss: 0.46412214801342555\n",
      "Epoch 180, Loss: 0.46867384291927394\n",
      "Epoch 181, Loss: 0.46316269230838986\n",
      "Epoch 182, Loss: 0.4619791093960296\n",
      "Epoch 183, Loss: 0.4650783107301587\n",
      "Epoch 184, Loss: 0.4617210775403838\n",
      "Epoch 185, Loss: 0.4621011464022392\n",
      "Epoch 186, Loss: 0.4641367239628427\n",
      "Epoch 187, Loss: 0.46071131054180564\n",
      "Epoch 188, Loss: 0.46145892957673257\n",
      "Epoch 189, Loss: 0.464304908222076\n",
      "Epoch 190, Loss: 0.4606605441278432\n",
      "Epoch 191, Loss: 0.4601386308130386\n",
      "Epoch 192, Loss: 0.46193430495315263\n",
      "Epoch 193, Loss: 0.45977696912799226\n",
      "Epoch 194, Loss: 0.4597173803227511\n",
      "Epoch 195, Loss: 0.46096589922299847\n",
      "Epoch 196, Loss: 0.4592170612502741\n",
      "Epoch 197, Loss: 0.45864347419013624\n",
      "Epoch 198, Loss: 0.4594650511157848\n",
      "Epoch 199, Loss: 0.45924705942486493\n",
      "Epoch 200, Loss: 0.4584931199827434\n",
      "Epoch 201, Loss: 0.4578552480015059\n",
      "Epoch 202, Loss: 0.458251953125\n",
      "Epoch 203, Loss: 0.4586848316367108\n",
      "Epoch 204, Loss: 0.4577025349799131\n",
      "Epoch 205, Loss: 0.45707273702530193\n",
      "Epoch 206, Loss: 0.4571336158814618\n",
      "Epoch 207, Loss: 0.45746758002181026\n",
      "Epoch 208, Loss: 0.4580334166179955\n",
      "Epoch 209, Loss: 0.457551138813026\n",
      "Epoch 210, Loss: 0.4572461918817771\n",
      "Epoch 211, Loss: 0.45629613394883295\n",
      "Epoch 212, Loss: 0.4557945440718634\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23795154441982908\n",
      "Valence RMSE: 0.24695363243987076\n",
      "Arousal RMSE: 0.22859522831484694\n",
      "Test R^2 score: tensor([0.3483, 0.6146], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.48146002064749027\n",
      "Num of epochs: 213\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Epoch 121, Loss: 0.4816228295571127\n",
      "Epoch 122, Loss: 0.4807618495484584\n",
      "Epoch 123, Loss: 0.48102030846138955\n",
      "Epoch 124, Loss: 0.4810176288364116\n",
      "Epoch 125, Loss: 0.48014338261661127\n",
      "Epoch 126, Loss: 0.47910314809270116\n",
      "Epoch 127, Loss: 0.47880795949683447\n",
      "Epoch 128, Loss: 0.47876177315142754\n",
      "Epoch 129, Loss: 0.47819597116449997\n",
      "Epoch 130, Loss: 0.47743214539542544\n",
      "Epoch 131, Loss: 0.47658476230340185\n",
      "Epoch 132, Loss: 0.4759644680184978\n",
      "Epoch 133, Loss: 0.4755635187712421\n",
      "Epoch 134, Loss: 0.47538264196584307\n",
      "Epoch 135, Loss: 0.4753673450596914\n",
      "Epoch 136, Loss: 0.4753697744168057\n",
      "Epoch 137, Loss: 0.4755973109583155\n",
      "Epoch 138, Loss: 0.4767626826605241\n",
      "Epoch 139, Loss: 0.47683528222287225\n",
      "Epoch 140, Loss: 0.47579445468261566\n",
      "Epoch 141, Loss: 0.47284533919249566\n",
      "Epoch 142, Loss: 0.472809286012082\n",
      "Epoch 143, Loss: 0.47458738111232796\n",
      "Epoch 144, Loss: 0.4733380058918715\n",
      "Epoch 145, Loss: 0.4715229034484008\n",
      "Epoch 146, Loss: 0.47096730966267386\n",
      "Epoch 147, Loss: 0.47161469890467383\n",
      "Epoch 148, Loss: 0.47183531411129226\n",
      "Epoch 149, Loss: 0.470294933688076\n",
      "Epoch 150, Loss: 0.4697114144973765\n",
      "Epoch 151, Loss: 0.47018789053701304\n",
      "Epoch 152, Loss: 0.4700460953124117\n",
      "Epoch 153, Loss: 0.4693645740016508\n",
      "Epoch 154, Loss: 0.4684558104180481\n",
      "Epoch 155, Loss: 0.4680960384979934\n",
      "Epoch 156, Loss: 0.4682454254529793\n",
      "Epoch 157, Loss: 0.4682974379105488\n",
      "Epoch 158, Loss: 0.4681590010361841\n",
      "Epoch 159, Loss: 0.4673225124817593\n",
      "Epoch 160, Loss: 0.4666701428107949\n",
      "Epoch 161, Loss: 0.4663166784414499\n",
      "Epoch 162, Loss: 0.46624854535037\n",
      "Epoch 163, Loss: 0.4664611566834947\n",
      "Epoch 164, Loss: 0.46654532450127006\n",
      "Epoch 165, Loss: 0.46721845607201035\n",
      "Epoch 166, Loss: 0.4668208160006968\n",
      "Epoch 167, Loss: 0.466573749669782\n",
      "Epoch 168, Loss: 0.46506387643568103\n",
      "Epoch 169, Loss: 0.4641220516950567\n",
      "Epoch 170, Loss: 0.4644550172656552\n",
      "Epoch 171, Loss: 0.4654613047116713\n",
      "Epoch 172, Loss: 0.46880176576246846\n",
      "Epoch 173, Loss: 0.4685649824581709\n",
      "Epoch 174, Loss: 0.46606115996454733\n",
      "Epoch 175, Loss: 0.46328263242364126\n",
      "Epoch 176, Loss: 0.4660484187313888\n",
      "Epoch 177, Loss: 0.4667669789465077\n",
      "Epoch 178, Loss: 0.46222975993575777\n",
      "Epoch 179, Loss: 0.46412214801342555\n",
      "Epoch 180, Loss: 0.46867384291927394\n",
      "Epoch 181, Loss: 0.46316269230838986\n",
      "Epoch 182, Loss: 0.4619791093960296\n",
      "Epoch 183, Loss: 0.4650783107301587\n",
      "Epoch 184, Loss: 0.4617210775403838\n",
      "Epoch 185, Loss: 0.4621011464022392\n",
      "Epoch 186, Loss: 0.4641367239628427\n",
      "Epoch 187, Loss: 0.46071131054180564\n",
      "Epoch 188, Loss: 0.46145892957673257\n",
      "Epoch 189, Loss: 0.464304908222076\n",
      "Epoch 190, Loss: 0.4606605441278432\n",
      "Epoch 191, Loss: 0.4601386308130386\n",
      "Epoch 192, Loss: 0.46193430495315263\n",
      "Epoch 193, Loss: 0.45977696912799226\n",
      "Epoch 194, Loss: 0.4597173803227511\n",
      "Epoch 195, Loss: 0.46096589922299847\n",
      "Epoch 196, Loss: 0.4592170612502741\n",
      "Epoch 197, Loss: 0.45864347419013624\n",
      "Epoch 198, Loss: 0.4594650511157848\n",
      "Epoch 199, Loss: 0.45924705942486493\n",
      "Epoch 200, Loss: 0.4584931199827434\n",
      "Epoch 201, Loss: 0.4578552480015059\n",
      "Epoch 202, Loss: 0.458251953125\n",
      "Epoch 203, Loss: 0.4586848316367108\n",
      "Epoch 204, Loss: 0.4577025349799131\n",
      "Epoch 205, Loss: 0.45707273702530193\n",
      "Epoch 206, Loss: 0.4571336158814618\n",
      "Epoch 207, Loss: 0.45746758002181026\n",
      "Epoch 208, Loss: 0.4580334166179955\n",
      "Epoch 209, Loss: 0.457551138813026\n",
      "Epoch 210, Loss: 0.4572461918817771\n",
      "Epoch 211, Loss: 0.45629613394883295\n",
      "Epoch 212, Loss: 0.4557945440718634\n",
      "Epoch 213, Loss: 0.455843155547024\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23822360974179888\n",
      "Valence RMSE: 0.24527859171638608\n",
      "Arousal RMSE: 0.23095321803874516\n",
      "Test R^2 score: tensor([0.3571, 0.6066], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4818695694816604\n",
      "Num of epochs: 214\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Epoch 121, Loss: 0.4816228295571127\n",
      "Epoch 122, Loss: 0.4807618495484584\n",
      "Epoch 123, Loss: 0.48102030846138955\n",
      "Epoch 124, Loss: 0.4810176288364116\n",
      "Epoch 125, Loss: 0.48014338261661127\n",
      "Epoch 126, Loss: 0.47910314809270116\n",
      "Epoch 127, Loss: 0.47880795949683447\n",
      "Epoch 128, Loss: 0.47876177315142754\n",
      "Epoch 129, Loss: 0.47819597116449997\n",
      "Epoch 130, Loss: 0.47743214539542544\n",
      "Epoch 131, Loss: 0.47658476230340185\n",
      "Epoch 132, Loss: 0.4759644680184978\n",
      "Epoch 133, Loss: 0.4755635187712421\n",
      "Epoch 134, Loss: 0.47538264196584307\n",
      "Epoch 135, Loss: 0.4753673450596914\n",
      "Epoch 136, Loss: 0.4753697744168057\n",
      "Epoch 137, Loss: 0.4755973109583155\n",
      "Epoch 138, Loss: 0.4767626826605241\n",
      "Epoch 139, Loss: 0.47683528222287225\n",
      "Epoch 140, Loss: 0.47579445468261566\n",
      "Epoch 141, Loss: 0.47284533919249566\n",
      "Epoch 142, Loss: 0.472809286012082\n",
      "Epoch 143, Loss: 0.47458738111232796\n",
      "Epoch 144, Loss: 0.4733380058918715\n",
      "Epoch 145, Loss: 0.4715229034484008\n",
      "Epoch 146, Loss: 0.47096730966267386\n",
      "Epoch 147, Loss: 0.47161469890467383\n",
      "Epoch 148, Loss: 0.47183531411129226\n",
      "Epoch 149, Loss: 0.470294933688076\n",
      "Epoch 150, Loss: 0.4697114144973765\n",
      "Epoch 151, Loss: 0.47018789053701304\n",
      "Epoch 152, Loss: 0.4700460953124117\n",
      "Epoch 153, Loss: 0.4693645740016508\n",
      "Epoch 154, Loss: 0.4684558104180481\n",
      "Epoch 155, Loss: 0.4680960384979934\n",
      "Epoch 156, Loss: 0.4682454254529793\n",
      "Epoch 157, Loss: 0.4682974379105488\n",
      "Epoch 158, Loss: 0.4681590010361841\n",
      "Epoch 159, Loss: 0.4673225124817593\n",
      "Epoch 160, Loss: 0.4666701428107949\n",
      "Epoch 161, Loss: 0.4663166784414499\n",
      "Epoch 162, Loss: 0.46624854535037\n",
      "Epoch 163, Loss: 0.4664611566834947\n",
      "Epoch 164, Loss: 0.46654532450127006\n",
      "Epoch 165, Loss: 0.46721845607201035\n",
      "Epoch 166, Loss: 0.4668208160006968\n",
      "Epoch 167, Loss: 0.466573749669782\n",
      "Epoch 168, Loss: 0.46506387643568103\n",
      "Epoch 169, Loss: 0.4641220516950567\n",
      "Epoch 170, Loss: 0.4644550172656552\n",
      "Epoch 171, Loss: 0.4654613047116713\n",
      "Epoch 172, Loss: 0.46880176576246846\n",
      "Epoch 173, Loss: 0.4685649824581709\n",
      "Epoch 174, Loss: 0.46606115996454733\n",
      "Epoch 175, Loss: 0.46328263242364126\n",
      "Epoch 176, Loss: 0.4660484187313888\n",
      "Epoch 177, Loss: 0.4667669789465077\n",
      "Epoch 178, Loss: 0.46222975993575777\n",
      "Epoch 179, Loss: 0.46412214801342555\n",
      "Epoch 180, Loss: 0.46867384291927394\n",
      "Epoch 181, Loss: 0.46316269230838986\n",
      "Epoch 182, Loss: 0.4619791093960296\n",
      "Epoch 183, Loss: 0.4650783107301587\n",
      "Epoch 184, Loss: 0.4617210775403838\n",
      "Epoch 185, Loss: 0.4621011464022392\n",
      "Epoch 186, Loss: 0.4641367239628427\n",
      "Epoch 187, Loss: 0.46071131054180564\n",
      "Epoch 188, Loss: 0.46145892957673257\n",
      "Epoch 189, Loss: 0.464304908222076\n",
      "Epoch 190, Loss: 0.4606605441278432\n",
      "Epoch 191, Loss: 0.4601386308130386\n",
      "Epoch 192, Loss: 0.46193430495315263\n",
      "Epoch 193, Loss: 0.45977696912799226\n",
      "Epoch 194, Loss: 0.4597173803227511\n",
      "Epoch 195, Loss: 0.46096589922299847\n",
      "Epoch 196, Loss: 0.4592170612502741\n",
      "Epoch 197, Loss: 0.45864347419013624\n",
      "Epoch 198, Loss: 0.4594650511157848\n",
      "Epoch 199, Loss: 0.45924705942486493\n",
      "Epoch 200, Loss: 0.4584931199827434\n",
      "Epoch 201, Loss: 0.4578552480015059\n",
      "Epoch 202, Loss: 0.458251953125\n",
      "Epoch 203, Loss: 0.4586848316367108\n",
      "Epoch 204, Loss: 0.4577025349799131\n",
      "Epoch 205, Loss: 0.45707273702530193\n",
      "Epoch 206, Loss: 0.4571336158814618\n",
      "Epoch 207, Loss: 0.45746758002181026\n",
      "Epoch 208, Loss: 0.4580334166179955\n",
      "Epoch 209, Loss: 0.457551138813026\n",
      "Epoch 210, Loss: 0.4572461918817771\n",
      "Epoch 211, Loss: 0.45629613394883295\n",
      "Epoch 212, Loss: 0.4557945440718634\n",
      "Epoch 213, Loss: 0.455843155547024\n",
      "Epoch 214, Loss: 0.456219547378948\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2401257145253919\n",
      "Valence RMSE: 0.24996962485173546\n",
      "Arousal RMSE: 0.22986061908065644\n",
      "Test R^2 score: tensor([0.3323, 0.6103], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.47131319569294017\n",
      "Num of epochs: 215\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Epoch 121, Loss: 0.4816228295571127\n",
      "Epoch 122, Loss: 0.4807618495484584\n",
      "Epoch 123, Loss: 0.48102030846138955\n",
      "Epoch 124, Loss: 0.4810176288364116\n",
      "Epoch 125, Loss: 0.48014338261661127\n",
      "Epoch 126, Loss: 0.47910314809270116\n",
      "Epoch 127, Loss: 0.47880795949683447\n",
      "Epoch 128, Loss: 0.47876177315142754\n",
      "Epoch 129, Loss: 0.47819597116449997\n",
      "Epoch 130, Loss: 0.47743214539542544\n",
      "Epoch 131, Loss: 0.47658476230340185\n",
      "Epoch 132, Loss: 0.4759644680184978\n",
      "Epoch 133, Loss: 0.4755635187712421\n",
      "Epoch 134, Loss: 0.47538264196584307\n",
      "Epoch 135, Loss: 0.4753673450596914\n",
      "Epoch 136, Loss: 0.4753697744168057\n",
      "Epoch 137, Loss: 0.4755973109583155\n",
      "Epoch 138, Loss: 0.4767626826605241\n",
      "Epoch 139, Loss: 0.47683528222287225\n",
      "Epoch 140, Loss: 0.47579445468261566\n",
      "Epoch 141, Loss: 0.47284533919249566\n",
      "Epoch 142, Loss: 0.472809286012082\n",
      "Epoch 143, Loss: 0.47458738111232796\n",
      "Epoch 144, Loss: 0.4733380058918715\n",
      "Epoch 145, Loss: 0.4715229034484008\n",
      "Epoch 146, Loss: 0.47096730966267386\n",
      "Epoch 147, Loss: 0.47161469890467383\n",
      "Epoch 148, Loss: 0.47183531411129226\n",
      "Epoch 149, Loss: 0.470294933688076\n",
      "Epoch 150, Loss: 0.4697114144973765\n",
      "Epoch 151, Loss: 0.47018789053701304\n",
      "Epoch 152, Loss: 0.4700460953124117\n",
      "Epoch 153, Loss: 0.4693645740016508\n",
      "Epoch 154, Loss: 0.4684558104180481\n",
      "Epoch 155, Loss: 0.4680960384979934\n",
      "Epoch 156, Loss: 0.4682454254529793\n",
      "Epoch 157, Loss: 0.4682974379105488\n",
      "Epoch 158, Loss: 0.4681590010361841\n",
      "Epoch 159, Loss: 0.4673225124817593\n",
      "Epoch 160, Loss: 0.4666701428107949\n",
      "Epoch 161, Loss: 0.4663166784414499\n",
      "Epoch 162, Loss: 0.46624854535037\n",
      "Epoch 163, Loss: 0.4664611566834947\n",
      "Epoch 164, Loss: 0.46654532450127006\n",
      "Epoch 165, Loss: 0.46721845607201035\n",
      "Epoch 166, Loss: 0.4668208160006968\n",
      "Epoch 167, Loss: 0.466573749669782\n",
      "Epoch 168, Loss: 0.46506387643568103\n",
      "Epoch 169, Loss: 0.4641220516950567\n",
      "Epoch 170, Loss: 0.4644550172656552\n",
      "Epoch 171, Loss: 0.4654613047116713\n",
      "Epoch 172, Loss: 0.46880176576246846\n",
      "Epoch 173, Loss: 0.4685649824581709\n",
      "Epoch 174, Loss: 0.46606115996454733\n",
      "Epoch 175, Loss: 0.46328263242364126\n",
      "Epoch 176, Loss: 0.4660484187313888\n",
      "Epoch 177, Loss: 0.4667669789465077\n",
      "Epoch 178, Loss: 0.46222975993575777\n",
      "Epoch 179, Loss: 0.46412214801342555\n",
      "Epoch 180, Loss: 0.46867384291927394\n",
      "Epoch 181, Loss: 0.46316269230838986\n",
      "Epoch 182, Loss: 0.4619791093960296\n",
      "Epoch 183, Loss: 0.4650783107301587\n",
      "Epoch 184, Loss: 0.4617210775403838\n",
      "Epoch 185, Loss: 0.4621011464022392\n",
      "Epoch 186, Loss: 0.4641367239628427\n",
      "Epoch 187, Loss: 0.46071131054180564\n",
      "Epoch 188, Loss: 0.46145892957673257\n",
      "Epoch 189, Loss: 0.464304908222076\n",
      "Epoch 190, Loss: 0.4606605441278432\n",
      "Epoch 191, Loss: 0.4601386308130386\n",
      "Epoch 192, Loss: 0.46193430495315263\n",
      "Epoch 193, Loss: 0.45977696912799226\n",
      "Epoch 194, Loss: 0.4597173803227511\n",
      "Epoch 195, Loss: 0.46096589922299847\n",
      "Epoch 196, Loss: 0.4592170612502741\n",
      "Epoch 197, Loss: 0.45864347419013624\n",
      "Epoch 198, Loss: 0.4594650511157848\n",
      "Epoch 199, Loss: 0.45924705942486493\n",
      "Epoch 200, Loss: 0.4584931199827434\n",
      "Epoch 201, Loss: 0.4578552480015059\n",
      "Epoch 202, Loss: 0.458251953125\n",
      "Epoch 203, Loss: 0.4586848316367108\n",
      "Epoch 204, Loss: 0.4577025349799131\n",
      "Epoch 205, Loss: 0.45707273702530193\n",
      "Epoch 206, Loss: 0.4571336158814618\n",
      "Epoch 207, Loss: 0.45746758002181026\n",
      "Epoch 208, Loss: 0.4580334166179955\n",
      "Epoch 209, Loss: 0.457551138813026\n",
      "Epoch 210, Loss: 0.4572461918817771\n",
      "Epoch 211, Loss: 0.45629613394883295\n",
      "Epoch 212, Loss: 0.4557945440718634\n",
      "Epoch 213, Loss: 0.455843155547024\n",
      "Epoch 214, Loss: 0.456219547378948\n",
      "Epoch 215, Loss: 0.4577116995236092\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23931233859342665\n",
      "Valence RMSE: 0.24496662946907832\n",
      "Arousal RMSE: 0.233521179452034\n",
      "Test R^2 score: tensor([0.3587, 0.5978], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4782884654311922\n",
      "Num of epochs: 216\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Epoch 121, Loss: 0.4816228295571127\n",
      "Epoch 122, Loss: 0.4807618495484584\n",
      "Epoch 123, Loss: 0.48102030846138955\n",
      "Epoch 124, Loss: 0.4810176288364116\n",
      "Epoch 125, Loss: 0.48014338261661127\n",
      "Epoch 126, Loss: 0.47910314809270116\n",
      "Epoch 127, Loss: 0.47880795949683447\n",
      "Epoch 128, Loss: 0.47876177315142754\n",
      "Epoch 129, Loss: 0.47819597116449997\n",
      "Epoch 130, Loss: 0.47743214539542544\n",
      "Epoch 131, Loss: 0.47658476230340185\n",
      "Epoch 132, Loss: 0.4759644680184978\n",
      "Epoch 133, Loss: 0.4755635187712421\n",
      "Epoch 134, Loss: 0.47538264196584307\n",
      "Epoch 135, Loss: 0.4753673450596914\n",
      "Epoch 136, Loss: 0.4753697744168057\n",
      "Epoch 137, Loss: 0.4755973109583155\n",
      "Epoch 138, Loss: 0.4767626826605241\n",
      "Epoch 139, Loss: 0.47683528222287225\n",
      "Epoch 140, Loss: 0.47579445468261566\n",
      "Epoch 141, Loss: 0.47284533919249566\n",
      "Epoch 142, Loss: 0.472809286012082\n",
      "Epoch 143, Loss: 0.47458738111232796\n",
      "Epoch 144, Loss: 0.4733380058918715\n",
      "Epoch 145, Loss: 0.4715229034484008\n",
      "Epoch 146, Loss: 0.47096730966267386\n",
      "Epoch 147, Loss: 0.47161469890467383\n",
      "Epoch 148, Loss: 0.47183531411129226\n",
      "Epoch 149, Loss: 0.470294933688076\n",
      "Epoch 150, Loss: 0.4697114144973765\n",
      "Epoch 151, Loss: 0.47018789053701304\n",
      "Epoch 152, Loss: 0.4700460953124117\n",
      "Epoch 153, Loss: 0.4693645740016508\n",
      "Epoch 154, Loss: 0.4684558104180481\n",
      "Epoch 155, Loss: 0.4680960384979934\n",
      "Epoch 156, Loss: 0.4682454254529793\n",
      "Epoch 157, Loss: 0.4682974379105488\n",
      "Epoch 158, Loss: 0.4681590010361841\n",
      "Epoch 159, Loss: 0.4673225124817593\n",
      "Epoch 160, Loss: 0.4666701428107949\n",
      "Epoch 161, Loss: 0.4663166784414499\n",
      "Epoch 162, Loss: 0.46624854535037\n",
      "Epoch 163, Loss: 0.4664611566834947\n",
      "Epoch 164, Loss: 0.46654532450127006\n",
      "Epoch 165, Loss: 0.46721845607201035\n",
      "Epoch 166, Loss: 0.4668208160006968\n",
      "Epoch 167, Loss: 0.466573749669782\n",
      "Epoch 168, Loss: 0.46506387643568103\n",
      "Epoch 169, Loss: 0.4641220516950567\n",
      "Epoch 170, Loss: 0.4644550172656552\n",
      "Epoch 171, Loss: 0.4654613047116713\n",
      "Epoch 172, Loss: 0.46880176576246846\n",
      "Epoch 173, Loss: 0.4685649824581709\n",
      "Epoch 174, Loss: 0.46606115996454733\n",
      "Epoch 175, Loss: 0.46328263242364126\n",
      "Epoch 176, Loss: 0.4660484187313888\n",
      "Epoch 177, Loss: 0.4667669789465077\n",
      "Epoch 178, Loss: 0.46222975993575777\n",
      "Epoch 179, Loss: 0.46412214801342555\n",
      "Epoch 180, Loss: 0.46867384291927394\n",
      "Epoch 181, Loss: 0.46316269230838986\n",
      "Epoch 182, Loss: 0.4619791093960296\n",
      "Epoch 183, Loss: 0.4650783107301587\n",
      "Epoch 184, Loss: 0.4617210775403838\n",
      "Epoch 185, Loss: 0.4621011464022392\n",
      "Epoch 186, Loss: 0.4641367239628427\n",
      "Epoch 187, Loss: 0.46071131054180564\n",
      "Epoch 188, Loss: 0.46145892957673257\n",
      "Epoch 189, Loss: 0.464304908222076\n",
      "Epoch 190, Loss: 0.4606605441278432\n",
      "Epoch 191, Loss: 0.4601386308130386\n",
      "Epoch 192, Loss: 0.46193430495315263\n",
      "Epoch 193, Loss: 0.45977696912799226\n",
      "Epoch 194, Loss: 0.4597173803227511\n",
      "Epoch 195, Loss: 0.46096589922299847\n",
      "Epoch 196, Loss: 0.4592170612502741\n",
      "Epoch 197, Loss: 0.45864347419013624\n",
      "Epoch 198, Loss: 0.4594650511157848\n",
      "Epoch 199, Loss: 0.45924705942486493\n",
      "Epoch 200, Loss: 0.4584931199827434\n",
      "Epoch 201, Loss: 0.4578552480015059\n",
      "Epoch 202, Loss: 0.458251953125\n",
      "Epoch 203, Loss: 0.4586848316367108\n",
      "Epoch 204, Loss: 0.4577025349799131\n",
      "Epoch 205, Loss: 0.45707273702530193\n",
      "Epoch 206, Loss: 0.4571336158814618\n",
      "Epoch 207, Loss: 0.45746758002181026\n",
      "Epoch 208, Loss: 0.4580334166179955\n",
      "Epoch 209, Loss: 0.457551138813026\n",
      "Epoch 210, Loss: 0.4572461918817771\n",
      "Epoch 211, Loss: 0.45629613394883295\n",
      "Epoch 212, Loss: 0.4557945440718634\n",
      "Epoch 213, Loss: 0.455843155547024\n",
      "Epoch 214, Loss: 0.456219547378948\n",
      "Epoch 215, Loss: 0.4577116995236092\n",
      "Epoch 216, Loss: 0.4589176697652806\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.24223281406088398\n",
      "Valence RMSE: 0.25321242950374034\n",
      "Arousal RMSE: 0.2307313111836426\n",
      "Test R^2 score: tensor([0.3149, 0.6074], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.461116128632838\n",
      "Num of epochs: 217\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Epoch 121, Loss: 0.4816228295571127\n",
      "Epoch 122, Loss: 0.4807618495484584\n",
      "Epoch 123, Loss: 0.48102030846138955\n",
      "Epoch 124, Loss: 0.4810176288364116\n",
      "Epoch 125, Loss: 0.48014338261661127\n",
      "Epoch 126, Loss: 0.47910314809270116\n",
      "Epoch 127, Loss: 0.47880795949683447\n",
      "Epoch 128, Loss: 0.47876177315142754\n",
      "Epoch 129, Loss: 0.47819597116449997\n",
      "Epoch 130, Loss: 0.47743214539542544\n",
      "Epoch 131, Loss: 0.47658476230340185\n",
      "Epoch 132, Loss: 0.4759644680184978\n",
      "Epoch 133, Loss: 0.4755635187712421\n",
      "Epoch 134, Loss: 0.47538264196584307\n",
      "Epoch 135, Loss: 0.4753673450596914\n",
      "Epoch 136, Loss: 0.4753697744168057\n",
      "Epoch 137, Loss: 0.4755973109583155\n",
      "Epoch 138, Loss: 0.4767626826605241\n",
      "Epoch 139, Loss: 0.47683528222287225\n",
      "Epoch 140, Loss: 0.47579445468261566\n",
      "Epoch 141, Loss: 0.47284533919249566\n",
      "Epoch 142, Loss: 0.472809286012082\n",
      "Epoch 143, Loss: 0.47458738111232796\n",
      "Epoch 144, Loss: 0.4733380058918715\n",
      "Epoch 145, Loss: 0.4715229034484008\n",
      "Epoch 146, Loss: 0.47096730966267386\n",
      "Epoch 147, Loss: 0.47161469890467383\n",
      "Epoch 148, Loss: 0.47183531411129226\n",
      "Epoch 149, Loss: 0.470294933688076\n",
      "Epoch 150, Loss: 0.4697114144973765\n",
      "Epoch 151, Loss: 0.47018789053701304\n",
      "Epoch 152, Loss: 0.4700460953124117\n",
      "Epoch 153, Loss: 0.4693645740016508\n",
      "Epoch 154, Loss: 0.4684558104180481\n",
      "Epoch 155, Loss: 0.4680960384979934\n",
      "Epoch 156, Loss: 0.4682454254529793\n",
      "Epoch 157, Loss: 0.4682974379105488\n",
      "Epoch 158, Loss: 0.4681590010361841\n",
      "Epoch 159, Loss: 0.4673225124817593\n",
      "Epoch 160, Loss: 0.4666701428107949\n",
      "Epoch 161, Loss: 0.4663166784414499\n",
      "Epoch 162, Loss: 0.46624854535037\n",
      "Epoch 163, Loss: 0.4664611566834947\n",
      "Epoch 164, Loss: 0.46654532450127006\n",
      "Epoch 165, Loss: 0.46721845607201035\n",
      "Epoch 166, Loss: 0.4668208160006968\n",
      "Epoch 167, Loss: 0.466573749669782\n",
      "Epoch 168, Loss: 0.46506387643568103\n",
      "Epoch 169, Loss: 0.4641220516950567\n",
      "Epoch 170, Loss: 0.4644550172656552\n",
      "Epoch 171, Loss: 0.4654613047116713\n",
      "Epoch 172, Loss: 0.46880176576246846\n",
      "Epoch 173, Loss: 0.4685649824581709\n",
      "Epoch 174, Loss: 0.46606115996454733\n",
      "Epoch 175, Loss: 0.46328263242364126\n",
      "Epoch 176, Loss: 0.4660484187313888\n",
      "Epoch 177, Loss: 0.4667669789465077\n",
      "Epoch 178, Loss: 0.46222975993575777\n",
      "Epoch 179, Loss: 0.46412214801342555\n",
      "Epoch 180, Loss: 0.46867384291927394\n",
      "Epoch 181, Loss: 0.46316269230838986\n",
      "Epoch 182, Loss: 0.4619791093960296\n",
      "Epoch 183, Loss: 0.4650783107301587\n",
      "Epoch 184, Loss: 0.4617210775403838\n",
      "Epoch 185, Loss: 0.4621011464022392\n",
      "Epoch 186, Loss: 0.4641367239628427\n",
      "Epoch 187, Loss: 0.46071131054180564\n",
      "Epoch 188, Loss: 0.46145892957673257\n",
      "Epoch 189, Loss: 0.464304908222076\n",
      "Epoch 190, Loss: 0.4606605441278432\n",
      "Epoch 191, Loss: 0.4601386308130386\n",
      "Epoch 192, Loss: 0.46193430495315263\n",
      "Epoch 193, Loss: 0.45977696912799226\n",
      "Epoch 194, Loss: 0.4597173803227511\n",
      "Epoch 195, Loss: 0.46096589922299847\n",
      "Epoch 196, Loss: 0.4592170612502741\n",
      "Epoch 197, Loss: 0.45864347419013624\n",
      "Epoch 198, Loss: 0.4594650511157848\n",
      "Epoch 199, Loss: 0.45924705942486493\n",
      "Epoch 200, Loss: 0.4584931199827434\n",
      "Epoch 201, Loss: 0.4578552480015059\n",
      "Epoch 202, Loss: 0.458251953125\n",
      "Epoch 203, Loss: 0.4586848316367108\n",
      "Epoch 204, Loss: 0.4577025349799131\n",
      "Epoch 205, Loss: 0.45707273702530193\n",
      "Epoch 206, Loss: 0.4571336158814618\n",
      "Epoch 207, Loss: 0.45746758002181026\n",
      "Epoch 208, Loss: 0.4580334166179955\n",
      "Epoch 209, Loss: 0.457551138813026\n",
      "Epoch 210, Loss: 0.4572461918817771\n",
      "Epoch 211, Loss: 0.45629613394883295\n",
      "Epoch 212, Loss: 0.4557945440718634\n",
      "Epoch 213, Loss: 0.455843155547024\n",
      "Epoch 214, Loss: 0.456219547378948\n",
      "Epoch 215, Loss: 0.4577116995236092\n",
      "Epoch 216, Loss: 0.4589176697652806\n",
      "Epoch 217, Loss: 0.46105034303609993\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23758745041917542\n",
      "Valence RMSE: 0.24385622837810805\n",
      "Arousal RMSE: 0.23114872501178169\n",
      "Test R^2 score: tensor([0.3645, 0.6060], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.48525368935803465\n",
      "Num of epochs: 218\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Epoch 121, Loss: 0.4816228295571127\n",
      "Epoch 122, Loss: 0.4807618495484584\n",
      "Epoch 123, Loss: 0.48102030846138955\n",
      "Epoch 124, Loss: 0.4810176288364116\n",
      "Epoch 125, Loss: 0.48014338261661127\n",
      "Epoch 126, Loss: 0.47910314809270116\n",
      "Epoch 127, Loss: 0.47880795949683447\n",
      "Epoch 128, Loss: 0.47876177315142754\n",
      "Epoch 129, Loss: 0.47819597116449997\n",
      "Epoch 130, Loss: 0.47743214539542544\n",
      "Epoch 131, Loss: 0.47658476230340185\n",
      "Epoch 132, Loss: 0.4759644680184978\n",
      "Epoch 133, Loss: 0.4755635187712421\n",
      "Epoch 134, Loss: 0.47538264196584307\n",
      "Epoch 135, Loss: 0.4753673450596914\n",
      "Epoch 136, Loss: 0.4753697744168057\n",
      "Epoch 137, Loss: 0.4755973109583155\n",
      "Epoch 138, Loss: 0.4767626826605241\n",
      "Epoch 139, Loss: 0.47683528222287225\n",
      "Epoch 140, Loss: 0.47579445468261566\n",
      "Epoch 141, Loss: 0.47284533919249566\n",
      "Epoch 142, Loss: 0.472809286012082\n",
      "Epoch 143, Loss: 0.47458738111232796\n",
      "Epoch 144, Loss: 0.4733380058918715\n",
      "Epoch 145, Loss: 0.4715229034484008\n",
      "Epoch 146, Loss: 0.47096730966267386\n",
      "Epoch 147, Loss: 0.47161469890467383\n",
      "Epoch 148, Loss: 0.47183531411129226\n",
      "Epoch 149, Loss: 0.470294933688076\n",
      "Epoch 150, Loss: 0.4697114144973765\n",
      "Epoch 151, Loss: 0.47018789053701304\n",
      "Epoch 152, Loss: 0.4700460953124117\n",
      "Epoch 153, Loss: 0.4693645740016508\n",
      "Epoch 154, Loss: 0.4684558104180481\n",
      "Epoch 155, Loss: 0.4680960384979934\n",
      "Epoch 156, Loss: 0.4682454254529793\n",
      "Epoch 157, Loss: 0.4682974379105488\n",
      "Epoch 158, Loss: 0.4681590010361841\n",
      "Epoch 159, Loss: 0.4673225124817593\n",
      "Epoch 160, Loss: 0.4666701428107949\n",
      "Epoch 161, Loss: 0.4663166784414499\n",
      "Epoch 162, Loss: 0.46624854535037\n",
      "Epoch 163, Loss: 0.4664611566834947\n",
      "Epoch 164, Loss: 0.46654532450127006\n",
      "Epoch 165, Loss: 0.46721845607201035\n",
      "Epoch 166, Loss: 0.4668208160006968\n",
      "Epoch 167, Loss: 0.466573749669782\n",
      "Epoch 168, Loss: 0.46506387643568103\n",
      "Epoch 169, Loss: 0.4641220516950567\n",
      "Epoch 170, Loss: 0.4644550172656552\n",
      "Epoch 171, Loss: 0.4654613047116713\n",
      "Epoch 172, Loss: 0.46880176576246846\n",
      "Epoch 173, Loss: 0.4685649824581709\n",
      "Epoch 174, Loss: 0.46606115996454733\n",
      "Epoch 175, Loss: 0.46328263242364126\n",
      "Epoch 176, Loss: 0.4660484187313888\n",
      "Epoch 177, Loss: 0.4667669789465077\n",
      "Epoch 178, Loss: 0.46222975993575777\n",
      "Epoch 179, Loss: 0.46412214801342555\n",
      "Epoch 180, Loss: 0.46867384291927394\n",
      "Epoch 181, Loss: 0.46316269230838986\n",
      "Epoch 182, Loss: 0.4619791093960296\n",
      "Epoch 183, Loss: 0.4650783107301587\n",
      "Epoch 184, Loss: 0.4617210775403838\n",
      "Epoch 185, Loss: 0.4621011464022392\n",
      "Epoch 186, Loss: 0.4641367239628427\n",
      "Epoch 187, Loss: 0.46071131054180564\n",
      "Epoch 188, Loss: 0.46145892957673257\n",
      "Epoch 189, Loss: 0.464304908222076\n",
      "Epoch 190, Loss: 0.4606605441278432\n",
      "Epoch 191, Loss: 0.4601386308130386\n",
      "Epoch 192, Loss: 0.46193430495315263\n",
      "Epoch 193, Loss: 0.45977696912799226\n",
      "Epoch 194, Loss: 0.4597173803227511\n",
      "Epoch 195, Loss: 0.46096589922299847\n",
      "Epoch 196, Loss: 0.4592170612502741\n",
      "Epoch 197, Loss: 0.45864347419013624\n",
      "Epoch 198, Loss: 0.4594650511157848\n",
      "Epoch 199, Loss: 0.45924705942486493\n",
      "Epoch 200, Loss: 0.4584931199827434\n",
      "Epoch 201, Loss: 0.4578552480015059\n",
      "Epoch 202, Loss: 0.458251953125\n",
      "Epoch 203, Loss: 0.4586848316367108\n",
      "Epoch 204, Loss: 0.4577025349799131\n",
      "Epoch 205, Loss: 0.45707273702530193\n",
      "Epoch 206, Loss: 0.4571336158814618\n",
      "Epoch 207, Loss: 0.45746758002181026\n",
      "Epoch 208, Loss: 0.4580334166179955\n",
      "Epoch 209, Loss: 0.457551138813026\n",
      "Epoch 210, Loss: 0.4572461918817771\n",
      "Epoch 211, Loss: 0.45629613394883295\n",
      "Epoch 212, Loss: 0.4557945440718634\n",
      "Epoch 213, Loss: 0.455843155547024\n",
      "Epoch 214, Loss: 0.456219547378948\n",
      "Epoch 215, Loss: 0.4577116995236092\n",
      "Epoch 216, Loss: 0.4589176697652806\n",
      "Epoch 217, Loss: 0.46105034303609993\n",
      "Epoch 218, Loss: 0.4567326442394303\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23664524835137388\n",
      "Valence RMSE: 0.24525390033693262\n",
      "Arousal RMSE: 0.22771137763425164\n",
      "Test R^2 score: tensor([0.3572, 0.6176], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4874172607108618\n",
      "Num of epochs: 219\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Epoch 121, Loss: 0.4816228295571127\n",
      "Epoch 122, Loss: 0.4807618495484584\n",
      "Epoch 123, Loss: 0.48102030846138955\n",
      "Epoch 124, Loss: 0.4810176288364116\n",
      "Epoch 125, Loss: 0.48014338261661127\n",
      "Epoch 126, Loss: 0.47910314809270116\n",
      "Epoch 127, Loss: 0.47880795949683447\n",
      "Epoch 128, Loss: 0.47876177315142754\n",
      "Epoch 129, Loss: 0.47819597116449997\n",
      "Epoch 130, Loss: 0.47743214539542544\n",
      "Epoch 131, Loss: 0.47658476230340185\n",
      "Epoch 132, Loss: 0.4759644680184978\n",
      "Epoch 133, Loss: 0.4755635187712421\n",
      "Epoch 134, Loss: 0.47538264196584307\n",
      "Epoch 135, Loss: 0.4753673450596914\n",
      "Epoch 136, Loss: 0.4753697744168057\n",
      "Epoch 137, Loss: 0.4755973109583155\n",
      "Epoch 138, Loss: 0.4767626826605241\n",
      "Epoch 139, Loss: 0.47683528222287225\n",
      "Epoch 140, Loss: 0.47579445468261566\n",
      "Epoch 141, Loss: 0.47284533919249566\n",
      "Epoch 142, Loss: 0.472809286012082\n",
      "Epoch 143, Loss: 0.47458738111232796\n",
      "Epoch 144, Loss: 0.4733380058918715\n",
      "Epoch 145, Loss: 0.4715229034484008\n",
      "Epoch 146, Loss: 0.47096730966267386\n",
      "Epoch 147, Loss: 0.47161469890467383\n",
      "Epoch 148, Loss: 0.47183531411129226\n",
      "Epoch 149, Loss: 0.470294933688076\n",
      "Epoch 150, Loss: 0.4697114144973765\n",
      "Epoch 151, Loss: 0.47018789053701304\n",
      "Epoch 152, Loss: 0.4700460953124117\n",
      "Epoch 153, Loss: 0.4693645740016508\n",
      "Epoch 154, Loss: 0.4684558104180481\n",
      "Epoch 155, Loss: 0.4680960384979934\n",
      "Epoch 156, Loss: 0.4682454254529793\n",
      "Epoch 157, Loss: 0.4682974379105488\n",
      "Epoch 158, Loss: 0.4681590010361841\n",
      "Epoch 159, Loss: 0.4673225124817593\n",
      "Epoch 160, Loss: 0.4666701428107949\n",
      "Epoch 161, Loss: 0.4663166784414499\n",
      "Epoch 162, Loss: 0.46624854535037\n",
      "Epoch 163, Loss: 0.4664611566834947\n",
      "Epoch 164, Loss: 0.46654532450127006\n",
      "Epoch 165, Loss: 0.46721845607201035\n",
      "Epoch 166, Loss: 0.4668208160006968\n",
      "Epoch 167, Loss: 0.466573749669782\n",
      "Epoch 168, Loss: 0.46506387643568103\n",
      "Epoch 169, Loss: 0.4641220516950567\n",
      "Epoch 170, Loss: 0.4644550172656552\n",
      "Epoch 171, Loss: 0.4654613047116713\n",
      "Epoch 172, Loss: 0.46880176576246846\n",
      "Epoch 173, Loss: 0.4685649824581709\n",
      "Epoch 174, Loss: 0.46606115996454733\n",
      "Epoch 175, Loss: 0.46328263242364126\n",
      "Epoch 176, Loss: 0.4660484187313888\n",
      "Epoch 177, Loss: 0.4667669789465077\n",
      "Epoch 178, Loss: 0.46222975993575777\n",
      "Epoch 179, Loss: 0.46412214801342555\n",
      "Epoch 180, Loss: 0.46867384291927394\n",
      "Epoch 181, Loss: 0.46316269230838986\n",
      "Epoch 182, Loss: 0.4619791093960296\n",
      "Epoch 183, Loss: 0.4650783107301587\n",
      "Epoch 184, Loss: 0.4617210775403838\n",
      "Epoch 185, Loss: 0.4621011464022392\n",
      "Epoch 186, Loss: 0.4641367239628427\n",
      "Epoch 187, Loss: 0.46071131054180564\n",
      "Epoch 188, Loss: 0.46145892957673257\n",
      "Epoch 189, Loss: 0.464304908222076\n",
      "Epoch 190, Loss: 0.4606605441278432\n",
      "Epoch 191, Loss: 0.4601386308130386\n",
      "Epoch 192, Loss: 0.46193430495315263\n",
      "Epoch 193, Loss: 0.45977696912799226\n",
      "Epoch 194, Loss: 0.4597173803227511\n",
      "Epoch 195, Loss: 0.46096589922299847\n",
      "Epoch 196, Loss: 0.4592170612502741\n",
      "Epoch 197, Loss: 0.45864347419013624\n",
      "Epoch 198, Loss: 0.4594650511157848\n",
      "Epoch 199, Loss: 0.45924705942486493\n",
      "Epoch 200, Loss: 0.4584931199827434\n",
      "Epoch 201, Loss: 0.4578552480015059\n",
      "Epoch 202, Loss: 0.458251953125\n",
      "Epoch 203, Loss: 0.4586848316367108\n",
      "Epoch 204, Loss: 0.4577025349799131\n",
      "Epoch 205, Loss: 0.45707273702530193\n",
      "Epoch 206, Loss: 0.4571336158814618\n",
      "Epoch 207, Loss: 0.45746758002181026\n",
      "Epoch 208, Loss: 0.4580334166179955\n",
      "Epoch 209, Loss: 0.457551138813026\n",
      "Epoch 210, Loss: 0.4572461918817771\n",
      "Epoch 211, Loss: 0.45629613394883295\n",
      "Epoch 212, Loss: 0.4557945440718634\n",
      "Epoch 213, Loss: 0.455843155547024\n",
      "Epoch 214, Loss: 0.456219547378948\n",
      "Epoch 215, Loss: 0.4577116995236092\n",
      "Epoch 216, Loss: 0.4589176697652806\n",
      "Epoch 217, Loss: 0.46105034303609993\n",
      "Epoch 218, Loss: 0.4567326442394303\n",
      "Epoch 219, Loss: 0.45493112899589366\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23724524058809304\n",
      "Valence RMSE: 0.246574144501056\n",
      "Arousal RMSE: 0.22753417243784876\n",
      "Test R^2 score: tensor([0.3503, 0.6182], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4842453587817783\n",
      "Num of epochs: 220\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Epoch 121, Loss: 0.4816228295571127\n",
      "Epoch 122, Loss: 0.4807618495484584\n",
      "Epoch 123, Loss: 0.48102030846138955\n",
      "Epoch 124, Loss: 0.4810176288364116\n",
      "Epoch 125, Loss: 0.48014338261661127\n",
      "Epoch 126, Loss: 0.47910314809270116\n",
      "Epoch 127, Loss: 0.47880795949683447\n",
      "Epoch 128, Loss: 0.47876177315142754\n",
      "Epoch 129, Loss: 0.47819597116449997\n",
      "Epoch 130, Loss: 0.47743214539542544\n",
      "Epoch 131, Loss: 0.47658476230340185\n",
      "Epoch 132, Loss: 0.4759644680184978\n",
      "Epoch 133, Loss: 0.4755635187712421\n",
      "Epoch 134, Loss: 0.47538264196584307\n",
      "Epoch 135, Loss: 0.4753673450596914\n",
      "Epoch 136, Loss: 0.4753697744168057\n",
      "Epoch 137, Loss: 0.4755973109583155\n",
      "Epoch 138, Loss: 0.4767626826605241\n",
      "Epoch 139, Loss: 0.47683528222287225\n",
      "Epoch 140, Loss: 0.47579445468261566\n",
      "Epoch 141, Loss: 0.47284533919249566\n",
      "Epoch 142, Loss: 0.472809286012082\n",
      "Epoch 143, Loss: 0.47458738111232796\n",
      "Epoch 144, Loss: 0.4733380058918715\n",
      "Epoch 145, Loss: 0.4715229034484008\n",
      "Epoch 146, Loss: 0.47096730966267386\n",
      "Epoch 147, Loss: 0.47161469890467383\n",
      "Epoch 148, Loss: 0.47183531411129226\n",
      "Epoch 149, Loss: 0.470294933688076\n",
      "Epoch 150, Loss: 0.4697114144973765\n",
      "Epoch 151, Loss: 0.47018789053701304\n",
      "Epoch 152, Loss: 0.4700460953124117\n",
      "Epoch 153, Loss: 0.4693645740016508\n",
      "Epoch 154, Loss: 0.4684558104180481\n",
      "Epoch 155, Loss: 0.4680960384979934\n",
      "Epoch 156, Loss: 0.4682454254529793\n",
      "Epoch 157, Loss: 0.4682974379105488\n",
      "Epoch 158, Loss: 0.4681590010361841\n",
      "Epoch 159, Loss: 0.4673225124817593\n",
      "Epoch 160, Loss: 0.4666701428107949\n",
      "Epoch 161, Loss: 0.4663166784414499\n",
      "Epoch 162, Loss: 0.46624854535037\n",
      "Epoch 163, Loss: 0.4664611566834947\n",
      "Epoch 164, Loss: 0.46654532450127006\n",
      "Epoch 165, Loss: 0.46721845607201035\n",
      "Epoch 166, Loss: 0.4668208160006968\n",
      "Epoch 167, Loss: 0.466573749669782\n",
      "Epoch 168, Loss: 0.46506387643568103\n",
      "Epoch 169, Loss: 0.4641220516950567\n",
      "Epoch 170, Loss: 0.4644550172656552\n",
      "Epoch 171, Loss: 0.4654613047116713\n",
      "Epoch 172, Loss: 0.46880176576246846\n",
      "Epoch 173, Loss: 0.4685649824581709\n",
      "Epoch 174, Loss: 0.46606115996454733\n",
      "Epoch 175, Loss: 0.46328263242364126\n",
      "Epoch 176, Loss: 0.4660484187313888\n",
      "Epoch 177, Loss: 0.4667669789465077\n",
      "Epoch 178, Loss: 0.46222975993575777\n",
      "Epoch 179, Loss: 0.46412214801342555\n",
      "Epoch 180, Loss: 0.46867384291927394\n",
      "Epoch 181, Loss: 0.46316269230838986\n",
      "Epoch 182, Loss: 0.4619791093960296\n",
      "Epoch 183, Loss: 0.4650783107301587\n",
      "Epoch 184, Loss: 0.4617210775403838\n",
      "Epoch 185, Loss: 0.4621011464022392\n",
      "Epoch 186, Loss: 0.4641367239628427\n",
      "Epoch 187, Loss: 0.46071131054180564\n",
      "Epoch 188, Loss: 0.46145892957673257\n",
      "Epoch 189, Loss: 0.464304908222076\n",
      "Epoch 190, Loss: 0.4606605441278432\n",
      "Epoch 191, Loss: 0.4601386308130386\n",
      "Epoch 192, Loss: 0.46193430495315263\n",
      "Epoch 193, Loss: 0.45977696912799226\n",
      "Epoch 194, Loss: 0.4597173803227511\n",
      "Epoch 195, Loss: 0.46096589922299847\n",
      "Epoch 196, Loss: 0.4592170612502741\n",
      "Epoch 197, Loss: 0.45864347419013624\n",
      "Epoch 198, Loss: 0.4594650511157848\n",
      "Epoch 199, Loss: 0.45924705942486493\n",
      "Epoch 200, Loss: 0.4584931199827434\n",
      "Epoch 201, Loss: 0.4578552480015059\n",
      "Epoch 202, Loss: 0.458251953125\n",
      "Epoch 203, Loss: 0.4586848316367108\n",
      "Epoch 204, Loss: 0.4577025349799131\n",
      "Epoch 205, Loss: 0.45707273702530193\n",
      "Epoch 206, Loss: 0.4571336158814618\n",
      "Epoch 207, Loss: 0.45746758002181026\n",
      "Epoch 208, Loss: 0.4580334166179955\n",
      "Epoch 209, Loss: 0.457551138813026\n",
      "Epoch 210, Loss: 0.4572461918817771\n",
      "Epoch 211, Loss: 0.45629613394883295\n",
      "Epoch 212, Loss: 0.4557945440718634\n",
      "Epoch 213, Loss: 0.455843155547024\n",
      "Epoch 214, Loss: 0.456219547378948\n",
      "Epoch 215, Loss: 0.4577116995236092\n",
      "Epoch 216, Loss: 0.4589176697652806\n",
      "Epoch 217, Loss: 0.46105034303609993\n",
      "Epoch 218, Loss: 0.4567326442394303\n",
      "Epoch 219, Loss: 0.45493112899589366\n",
      "Epoch 220, Loss: 0.45527199475335134\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23795431711566584\n",
      "Valence RMSE: 0.24418463806486926\n",
      "Arousal RMSE: 0.23155642206834715\n",
      "Test R^2 score: tensor([0.3628, 0.6046], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.48370171057837247\n",
      "Num of epochs: 221\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Epoch 121, Loss: 0.4816228295571127\n",
      "Epoch 122, Loss: 0.4807618495484584\n",
      "Epoch 123, Loss: 0.48102030846138955\n",
      "Epoch 124, Loss: 0.4810176288364116\n",
      "Epoch 125, Loss: 0.48014338261661127\n",
      "Epoch 126, Loss: 0.47910314809270116\n",
      "Epoch 127, Loss: 0.47880795949683447\n",
      "Epoch 128, Loss: 0.47876177315142754\n",
      "Epoch 129, Loss: 0.47819597116449997\n",
      "Epoch 130, Loss: 0.47743214539542544\n",
      "Epoch 131, Loss: 0.47658476230340185\n",
      "Epoch 132, Loss: 0.4759644680184978\n",
      "Epoch 133, Loss: 0.4755635187712421\n",
      "Epoch 134, Loss: 0.47538264196584307\n",
      "Epoch 135, Loss: 0.4753673450596914\n",
      "Epoch 136, Loss: 0.4753697744168057\n",
      "Epoch 137, Loss: 0.4755973109583155\n",
      "Epoch 138, Loss: 0.4767626826605241\n",
      "Epoch 139, Loss: 0.47683528222287225\n",
      "Epoch 140, Loss: 0.47579445468261566\n",
      "Epoch 141, Loss: 0.47284533919249566\n",
      "Epoch 142, Loss: 0.472809286012082\n",
      "Epoch 143, Loss: 0.47458738111232796\n",
      "Epoch 144, Loss: 0.4733380058918715\n",
      "Epoch 145, Loss: 0.4715229034484008\n",
      "Epoch 146, Loss: 0.47096730966267386\n",
      "Epoch 147, Loss: 0.47161469890467383\n",
      "Epoch 148, Loss: 0.47183531411129226\n",
      "Epoch 149, Loss: 0.470294933688076\n",
      "Epoch 150, Loss: 0.4697114144973765\n",
      "Epoch 151, Loss: 0.47018789053701304\n",
      "Epoch 152, Loss: 0.4700460953124117\n",
      "Epoch 153, Loss: 0.4693645740016508\n",
      "Epoch 154, Loss: 0.4684558104180481\n",
      "Epoch 155, Loss: 0.4680960384979934\n",
      "Epoch 156, Loss: 0.4682454254529793\n",
      "Epoch 157, Loss: 0.4682974379105488\n",
      "Epoch 158, Loss: 0.4681590010361841\n",
      "Epoch 159, Loss: 0.4673225124817593\n",
      "Epoch 160, Loss: 0.4666701428107949\n",
      "Epoch 161, Loss: 0.4663166784414499\n",
      "Epoch 162, Loss: 0.46624854535037\n",
      "Epoch 163, Loss: 0.4664611566834947\n",
      "Epoch 164, Loss: 0.46654532450127006\n",
      "Epoch 165, Loss: 0.46721845607201035\n",
      "Epoch 166, Loss: 0.4668208160006968\n",
      "Epoch 167, Loss: 0.466573749669782\n",
      "Epoch 168, Loss: 0.46506387643568103\n",
      "Epoch 169, Loss: 0.4641220516950567\n",
      "Epoch 170, Loss: 0.4644550172656552\n",
      "Epoch 171, Loss: 0.4654613047116713\n",
      "Epoch 172, Loss: 0.46880176576246846\n",
      "Epoch 173, Loss: 0.4685649824581709\n",
      "Epoch 174, Loss: 0.46606115996454733\n",
      "Epoch 175, Loss: 0.46328263242364126\n",
      "Epoch 176, Loss: 0.4660484187313888\n",
      "Epoch 177, Loss: 0.4667669789465077\n",
      "Epoch 178, Loss: 0.46222975993575777\n",
      "Epoch 179, Loss: 0.46412214801342555\n",
      "Epoch 180, Loss: 0.46867384291927394\n",
      "Epoch 181, Loss: 0.46316269230838986\n",
      "Epoch 182, Loss: 0.4619791093960296\n",
      "Epoch 183, Loss: 0.4650783107301587\n",
      "Epoch 184, Loss: 0.4617210775403838\n",
      "Epoch 185, Loss: 0.4621011464022392\n",
      "Epoch 186, Loss: 0.4641367239628427\n",
      "Epoch 187, Loss: 0.46071131054180564\n",
      "Epoch 188, Loss: 0.46145892957673257\n",
      "Epoch 189, Loss: 0.464304908222076\n",
      "Epoch 190, Loss: 0.4606605441278432\n",
      "Epoch 191, Loss: 0.4601386308130386\n",
      "Epoch 192, Loss: 0.46193430495315263\n",
      "Epoch 193, Loss: 0.45977696912799226\n",
      "Epoch 194, Loss: 0.4597173803227511\n",
      "Epoch 195, Loss: 0.46096589922299847\n",
      "Epoch 196, Loss: 0.4592170612502741\n",
      "Epoch 197, Loss: 0.45864347419013624\n",
      "Epoch 198, Loss: 0.4594650511157848\n",
      "Epoch 199, Loss: 0.45924705942486493\n",
      "Epoch 200, Loss: 0.4584931199827434\n",
      "Epoch 201, Loss: 0.4578552480015059\n",
      "Epoch 202, Loss: 0.458251953125\n",
      "Epoch 203, Loss: 0.4586848316367108\n",
      "Epoch 204, Loss: 0.4577025349799131\n",
      "Epoch 205, Loss: 0.45707273702530193\n",
      "Epoch 206, Loss: 0.4571336158814618\n",
      "Epoch 207, Loss: 0.45746758002181026\n",
      "Epoch 208, Loss: 0.4580334166179955\n",
      "Epoch 209, Loss: 0.457551138813026\n",
      "Epoch 210, Loss: 0.4572461918817771\n",
      "Epoch 211, Loss: 0.45629613394883295\n",
      "Epoch 212, Loss: 0.4557945440718634\n",
      "Epoch 213, Loss: 0.455843155547024\n",
      "Epoch 214, Loss: 0.456219547378948\n",
      "Epoch 215, Loss: 0.4577116995236092\n",
      "Epoch 216, Loss: 0.4589176697652806\n",
      "Epoch 217, Loss: 0.46105034303609993\n",
      "Epoch 218, Loss: 0.4567326442394303\n",
      "Epoch 219, Loss: 0.45493112899589366\n",
      "Epoch 220, Loss: 0.45527199475335134\n",
      "Epoch 221, Loss: 0.45623630281032773\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.24076491838553774\n",
      "Valence RMSE: 0.2506530871372463\n",
      "Arousal RMSE: 0.2304528623362212\n",
      "Test R^2 score: tensor([0.3286, 0.6083], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4684797853172985\n",
      "Num of epochs: 222\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Epoch 121, Loss: 0.4816228295571127\n",
      "Epoch 122, Loss: 0.4807618495484584\n",
      "Epoch 123, Loss: 0.48102030846138955\n",
      "Epoch 124, Loss: 0.4810176288364116\n",
      "Epoch 125, Loss: 0.48014338261661127\n",
      "Epoch 126, Loss: 0.47910314809270116\n",
      "Epoch 127, Loss: 0.47880795949683447\n",
      "Epoch 128, Loss: 0.47876177315142754\n",
      "Epoch 129, Loss: 0.47819597116449997\n",
      "Epoch 130, Loss: 0.47743214539542544\n",
      "Epoch 131, Loss: 0.47658476230340185\n",
      "Epoch 132, Loss: 0.4759644680184978\n",
      "Epoch 133, Loss: 0.4755635187712421\n",
      "Epoch 134, Loss: 0.47538264196584307\n",
      "Epoch 135, Loss: 0.4753673450596914\n",
      "Epoch 136, Loss: 0.4753697744168057\n",
      "Epoch 137, Loss: 0.4755973109583155\n",
      "Epoch 138, Loss: 0.4767626826605241\n",
      "Epoch 139, Loss: 0.47683528222287225\n",
      "Epoch 140, Loss: 0.47579445468261566\n",
      "Epoch 141, Loss: 0.47284533919249566\n",
      "Epoch 142, Loss: 0.472809286012082\n",
      "Epoch 143, Loss: 0.47458738111232796\n",
      "Epoch 144, Loss: 0.4733380058918715\n",
      "Epoch 145, Loss: 0.4715229034484008\n",
      "Epoch 146, Loss: 0.47096730966267386\n",
      "Epoch 147, Loss: 0.47161469890467383\n",
      "Epoch 148, Loss: 0.47183531411129226\n",
      "Epoch 149, Loss: 0.470294933688076\n",
      "Epoch 150, Loss: 0.4697114144973765\n",
      "Epoch 151, Loss: 0.47018789053701304\n",
      "Epoch 152, Loss: 0.4700460953124117\n",
      "Epoch 153, Loss: 0.4693645740016508\n",
      "Epoch 154, Loss: 0.4684558104180481\n",
      "Epoch 155, Loss: 0.4680960384979934\n",
      "Epoch 156, Loss: 0.4682454254529793\n",
      "Epoch 157, Loss: 0.4682974379105488\n",
      "Epoch 158, Loss: 0.4681590010361841\n",
      "Epoch 159, Loss: 0.4673225124817593\n",
      "Epoch 160, Loss: 0.4666701428107949\n",
      "Epoch 161, Loss: 0.4663166784414499\n",
      "Epoch 162, Loss: 0.46624854535037\n",
      "Epoch 163, Loss: 0.4664611566834947\n",
      "Epoch 164, Loss: 0.46654532450127006\n",
      "Epoch 165, Loss: 0.46721845607201035\n",
      "Epoch 166, Loss: 0.4668208160006968\n",
      "Epoch 167, Loss: 0.466573749669782\n",
      "Epoch 168, Loss: 0.46506387643568103\n",
      "Epoch 169, Loss: 0.4641220516950567\n",
      "Epoch 170, Loss: 0.4644550172656552\n",
      "Epoch 171, Loss: 0.4654613047116713\n",
      "Epoch 172, Loss: 0.46880176576246846\n",
      "Epoch 173, Loss: 0.4685649824581709\n",
      "Epoch 174, Loss: 0.46606115996454733\n",
      "Epoch 175, Loss: 0.46328263242364126\n",
      "Epoch 176, Loss: 0.4660484187313888\n",
      "Epoch 177, Loss: 0.4667669789465077\n",
      "Epoch 178, Loss: 0.46222975993575777\n",
      "Epoch 179, Loss: 0.46412214801342555\n",
      "Epoch 180, Loss: 0.46867384291927394\n",
      "Epoch 181, Loss: 0.46316269230838986\n",
      "Epoch 182, Loss: 0.4619791093960296\n",
      "Epoch 183, Loss: 0.4650783107301587\n",
      "Epoch 184, Loss: 0.4617210775403838\n",
      "Epoch 185, Loss: 0.4621011464022392\n",
      "Epoch 186, Loss: 0.4641367239628427\n",
      "Epoch 187, Loss: 0.46071131054180564\n",
      "Epoch 188, Loss: 0.46145892957673257\n",
      "Epoch 189, Loss: 0.464304908222076\n",
      "Epoch 190, Loss: 0.4606605441278432\n",
      "Epoch 191, Loss: 0.4601386308130386\n",
      "Epoch 192, Loss: 0.46193430495315263\n",
      "Epoch 193, Loss: 0.45977696912799226\n",
      "Epoch 194, Loss: 0.4597173803227511\n",
      "Epoch 195, Loss: 0.46096589922299847\n",
      "Epoch 196, Loss: 0.4592170612502741\n",
      "Epoch 197, Loss: 0.45864347419013624\n",
      "Epoch 198, Loss: 0.4594650511157848\n",
      "Epoch 199, Loss: 0.45924705942486493\n",
      "Epoch 200, Loss: 0.4584931199827434\n",
      "Epoch 201, Loss: 0.4578552480015059\n",
      "Epoch 202, Loss: 0.458251953125\n",
      "Epoch 203, Loss: 0.4586848316367108\n",
      "Epoch 204, Loss: 0.4577025349799131\n",
      "Epoch 205, Loss: 0.45707273702530193\n",
      "Epoch 206, Loss: 0.4571336158814618\n",
      "Epoch 207, Loss: 0.45746758002181026\n",
      "Epoch 208, Loss: 0.4580334166179955\n",
      "Epoch 209, Loss: 0.457551138813026\n",
      "Epoch 210, Loss: 0.4572461918817771\n",
      "Epoch 211, Loss: 0.45629613394883295\n",
      "Epoch 212, Loss: 0.4557945440718634\n",
      "Epoch 213, Loss: 0.455843155547024\n",
      "Epoch 214, Loss: 0.456219547378948\n",
      "Epoch 215, Loss: 0.4577116995236092\n",
      "Epoch 216, Loss: 0.4589176697652806\n",
      "Epoch 217, Loss: 0.46105034303609993\n",
      "Epoch 218, Loss: 0.4567326442394303\n",
      "Epoch 219, Loss: 0.45493112899589366\n",
      "Epoch 220, Loss: 0.45527199475335134\n",
      "Epoch 221, Loss: 0.45623630281032773\n",
      "Epoch 222, Loss: 0.45802054967026307\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.238864824876877\n",
      "Valence RMSE: 0.24503159146266149\n",
      "Arousal RMSE: 0.23253457444474274\n",
      "Test R^2 score: tensor([0.3584, 0.6012], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.479813942199234\n",
      "Num of epochs: 223\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Epoch 121, Loss: 0.4816228295571127\n",
      "Epoch 122, Loss: 0.4807618495484584\n",
      "Epoch 123, Loss: 0.48102030846138955\n",
      "Epoch 124, Loss: 0.4810176288364116\n",
      "Epoch 125, Loss: 0.48014338261661127\n",
      "Epoch 126, Loss: 0.47910314809270116\n",
      "Epoch 127, Loss: 0.47880795949683447\n",
      "Epoch 128, Loss: 0.47876177315142754\n",
      "Epoch 129, Loss: 0.47819597116449997\n",
      "Epoch 130, Loss: 0.47743214539542544\n",
      "Epoch 131, Loss: 0.47658476230340185\n",
      "Epoch 132, Loss: 0.4759644680184978\n",
      "Epoch 133, Loss: 0.4755635187712421\n",
      "Epoch 134, Loss: 0.47538264196584307\n",
      "Epoch 135, Loss: 0.4753673450596914\n",
      "Epoch 136, Loss: 0.4753697744168057\n",
      "Epoch 137, Loss: 0.4755973109583155\n",
      "Epoch 138, Loss: 0.4767626826605241\n",
      "Epoch 139, Loss: 0.47683528222287225\n",
      "Epoch 140, Loss: 0.47579445468261566\n",
      "Epoch 141, Loss: 0.47284533919249566\n",
      "Epoch 142, Loss: 0.472809286012082\n",
      "Epoch 143, Loss: 0.47458738111232796\n",
      "Epoch 144, Loss: 0.4733380058918715\n",
      "Epoch 145, Loss: 0.4715229034484008\n",
      "Epoch 146, Loss: 0.47096730966267386\n",
      "Epoch 147, Loss: 0.47161469890467383\n",
      "Epoch 148, Loss: 0.47183531411129226\n",
      "Epoch 149, Loss: 0.470294933688076\n",
      "Epoch 150, Loss: 0.4697114144973765\n",
      "Epoch 151, Loss: 0.47018789053701304\n",
      "Epoch 152, Loss: 0.4700460953124117\n",
      "Epoch 153, Loss: 0.4693645740016508\n",
      "Epoch 154, Loss: 0.4684558104180481\n",
      "Epoch 155, Loss: 0.4680960384979934\n",
      "Epoch 156, Loss: 0.4682454254529793\n",
      "Epoch 157, Loss: 0.4682974379105488\n",
      "Epoch 158, Loss: 0.4681590010361841\n",
      "Epoch 159, Loss: 0.4673225124817593\n",
      "Epoch 160, Loss: 0.4666701428107949\n",
      "Epoch 161, Loss: 0.4663166784414499\n",
      "Epoch 162, Loss: 0.46624854535037\n",
      "Epoch 163, Loss: 0.4664611566834947\n",
      "Epoch 164, Loss: 0.46654532450127006\n",
      "Epoch 165, Loss: 0.46721845607201035\n",
      "Epoch 166, Loss: 0.4668208160006968\n",
      "Epoch 167, Loss: 0.466573749669782\n",
      "Epoch 168, Loss: 0.46506387643568103\n",
      "Epoch 169, Loss: 0.4641220516950567\n",
      "Epoch 170, Loss: 0.4644550172656552\n",
      "Epoch 171, Loss: 0.4654613047116713\n",
      "Epoch 172, Loss: 0.46880176576246846\n",
      "Epoch 173, Loss: 0.4685649824581709\n",
      "Epoch 174, Loss: 0.46606115996454733\n",
      "Epoch 175, Loss: 0.46328263242364126\n",
      "Epoch 176, Loss: 0.4660484187313888\n",
      "Epoch 177, Loss: 0.4667669789465077\n",
      "Epoch 178, Loss: 0.46222975993575777\n",
      "Epoch 179, Loss: 0.46412214801342555\n",
      "Epoch 180, Loss: 0.46867384291927394\n",
      "Epoch 181, Loss: 0.46316269230838986\n",
      "Epoch 182, Loss: 0.4619791093960296\n",
      "Epoch 183, Loss: 0.4650783107301587\n",
      "Epoch 184, Loss: 0.4617210775403838\n",
      "Epoch 185, Loss: 0.4621011464022392\n",
      "Epoch 186, Loss: 0.4641367239628427\n",
      "Epoch 187, Loss: 0.46071131054180564\n",
      "Epoch 188, Loss: 0.46145892957673257\n",
      "Epoch 189, Loss: 0.464304908222076\n",
      "Epoch 190, Loss: 0.4606605441278432\n",
      "Epoch 191, Loss: 0.4601386308130386\n",
      "Epoch 192, Loss: 0.46193430495315263\n",
      "Epoch 193, Loss: 0.45977696912799226\n",
      "Epoch 194, Loss: 0.4597173803227511\n",
      "Epoch 195, Loss: 0.46096589922299847\n",
      "Epoch 196, Loss: 0.4592170612502741\n",
      "Epoch 197, Loss: 0.45864347419013624\n",
      "Epoch 198, Loss: 0.4594650511157848\n",
      "Epoch 199, Loss: 0.45924705942486493\n",
      "Epoch 200, Loss: 0.4584931199827434\n",
      "Epoch 201, Loss: 0.4578552480015059\n",
      "Epoch 202, Loss: 0.458251953125\n",
      "Epoch 203, Loss: 0.4586848316367108\n",
      "Epoch 204, Loss: 0.4577025349799131\n",
      "Epoch 205, Loss: 0.45707273702530193\n",
      "Epoch 206, Loss: 0.4571336158814618\n",
      "Epoch 207, Loss: 0.45746758002181026\n",
      "Epoch 208, Loss: 0.4580334166179955\n",
      "Epoch 209, Loss: 0.457551138813026\n",
      "Epoch 210, Loss: 0.4572461918817771\n",
      "Epoch 211, Loss: 0.45629613394883295\n",
      "Epoch 212, Loss: 0.4557945440718634\n",
      "Epoch 213, Loss: 0.455843155547024\n",
      "Epoch 214, Loss: 0.456219547378948\n",
      "Epoch 215, Loss: 0.4577116995236092\n",
      "Epoch 216, Loss: 0.4589176697652806\n",
      "Epoch 217, Loss: 0.46105034303609993\n",
      "Epoch 218, Loss: 0.4567326442394303\n",
      "Epoch 219, Loss: 0.45493112899589366\n",
      "Epoch 220, Loss: 0.45527199475335134\n",
      "Epoch 221, Loss: 0.45623630281032773\n",
      "Epoch 222, Loss: 0.45802054967026307\n",
      "Epoch 223, Loss: 0.45677224995978094\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23927157453797576\n",
      "Valence RMSE: 0.24913097957455835\n",
      "Arousal RMSE: 0.22898805160965316\n",
      "Test R^2 score: tensor([0.3368, 0.6133], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4750259805106273\n",
      "Num of epochs: 224\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Epoch 121, Loss: 0.4816228295571127\n",
      "Epoch 122, Loss: 0.4807618495484584\n",
      "Epoch 123, Loss: 0.48102030846138955\n",
      "Epoch 124, Loss: 0.4810176288364116\n",
      "Epoch 125, Loss: 0.48014338261661127\n",
      "Epoch 126, Loss: 0.47910314809270116\n",
      "Epoch 127, Loss: 0.47880795949683447\n",
      "Epoch 128, Loss: 0.47876177315142754\n",
      "Epoch 129, Loss: 0.47819597116449997\n",
      "Epoch 130, Loss: 0.47743214539542544\n",
      "Epoch 131, Loss: 0.47658476230340185\n",
      "Epoch 132, Loss: 0.4759644680184978\n",
      "Epoch 133, Loss: 0.4755635187712421\n",
      "Epoch 134, Loss: 0.47538264196584307\n",
      "Epoch 135, Loss: 0.4753673450596914\n",
      "Epoch 136, Loss: 0.4753697744168057\n",
      "Epoch 137, Loss: 0.4755973109583155\n",
      "Epoch 138, Loss: 0.4767626826605241\n",
      "Epoch 139, Loss: 0.47683528222287225\n",
      "Epoch 140, Loss: 0.47579445468261566\n",
      "Epoch 141, Loss: 0.47284533919249566\n",
      "Epoch 142, Loss: 0.472809286012082\n",
      "Epoch 143, Loss: 0.47458738111232796\n",
      "Epoch 144, Loss: 0.4733380058918715\n",
      "Epoch 145, Loss: 0.4715229034484008\n",
      "Epoch 146, Loss: 0.47096730966267386\n",
      "Epoch 147, Loss: 0.47161469890467383\n",
      "Epoch 148, Loss: 0.47183531411129226\n",
      "Epoch 149, Loss: 0.470294933688076\n",
      "Epoch 150, Loss: 0.4697114144973765\n",
      "Epoch 151, Loss: 0.47018789053701304\n",
      "Epoch 152, Loss: 0.4700460953124117\n",
      "Epoch 153, Loss: 0.4693645740016508\n",
      "Epoch 154, Loss: 0.4684558104180481\n",
      "Epoch 155, Loss: 0.4680960384979934\n",
      "Epoch 156, Loss: 0.4682454254529793\n",
      "Epoch 157, Loss: 0.4682974379105488\n",
      "Epoch 158, Loss: 0.4681590010361841\n",
      "Epoch 159, Loss: 0.4673225124817593\n",
      "Epoch 160, Loss: 0.4666701428107949\n",
      "Epoch 161, Loss: 0.4663166784414499\n",
      "Epoch 162, Loss: 0.46624854535037\n",
      "Epoch 163, Loss: 0.4664611566834947\n",
      "Epoch 164, Loss: 0.46654532450127006\n",
      "Epoch 165, Loss: 0.46721845607201035\n",
      "Epoch 166, Loss: 0.4668208160006968\n",
      "Epoch 167, Loss: 0.466573749669782\n",
      "Epoch 168, Loss: 0.46506387643568103\n",
      "Epoch 169, Loss: 0.4641220516950567\n",
      "Epoch 170, Loss: 0.4644550172656552\n",
      "Epoch 171, Loss: 0.4654613047116713\n",
      "Epoch 172, Loss: 0.46880176576246846\n",
      "Epoch 173, Loss: 0.4685649824581709\n",
      "Epoch 174, Loss: 0.46606115996454733\n",
      "Epoch 175, Loss: 0.46328263242364126\n",
      "Epoch 176, Loss: 0.4660484187313888\n",
      "Epoch 177, Loss: 0.4667669789465077\n",
      "Epoch 178, Loss: 0.46222975993575777\n",
      "Epoch 179, Loss: 0.46412214801342555\n",
      "Epoch 180, Loss: 0.46867384291927394\n",
      "Epoch 181, Loss: 0.46316269230838986\n",
      "Epoch 182, Loss: 0.4619791093960296\n",
      "Epoch 183, Loss: 0.4650783107301587\n",
      "Epoch 184, Loss: 0.4617210775403838\n",
      "Epoch 185, Loss: 0.4621011464022392\n",
      "Epoch 186, Loss: 0.4641367239628427\n",
      "Epoch 187, Loss: 0.46071131054180564\n",
      "Epoch 188, Loss: 0.46145892957673257\n",
      "Epoch 189, Loss: 0.464304908222076\n",
      "Epoch 190, Loss: 0.4606605441278432\n",
      "Epoch 191, Loss: 0.4601386308130386\n",
      "Epoch 192, Loss: 0.46193430495315263\n",
      "Epoch 193, Loss: 0.45977696912799226\n",
      "Epoch 194, Loss: 0.4597173803227511\n",
      "Epoch 195, Loss: 0.46096589922299847\n",
      "Epoch 196, Loss: 0.4592170612502741\n",
      "Epoch 197, Loss: 0.45864347419013624\n",
      "Epoch 198, Loss: 0.4594650511157848\n",
      "Epoch 199, Loss: 0.45924705942486493\n",
      "Epoch 200, Loss: 0.4584931199827434\n",
      "Epoch 201, Loss: 0.4578552480015059\n",
      "Epoch 202, Loss: 0.458251953125\n",
      "Epoch 203, Loss: 0.4586848316367108\n",
      "Epoch 204, Loss: 0.4577025349799131\n",
      "Epoch 205, Loss: 0.45707273702530193\n",
      "Epoch 206, Loss: 0.4571336158814618\n",
      "Epoch 207, Loss: 0.45746758002181026\n",
      "Epoch 208, Loss: 0.4580334166179955\n",
      "Epoch 209, Loss: 0.457551138813026\n",
      "Epoch 210, Loss: 0.4572461918817771\n",
      "Epoch 211, Loss: 0.45629613394883295\n",
      "Epoch 212, Loss: 0.4557945440718634\n",
      "Epoch 213, Loss: 0.455843155547024\n",
      "Epoch 214, Loss: 0.456219547378948\n",
      "Epoch 215, Loss: 0.4577116995236092\n",
      "Epoch 216, Loss: 0.4589176697652806\n",
      "Epoch 217, Loss: 0.46105034303609993\n",
      "Epoch 218, Loss: 0.4567326442394303\n",
      "Epoch 219, Loss: 0.45493112899589366\n",
      "Epoch 220, Loss: 0.45527199475335134\n",
      "Epoch 221, Loss: 0.45623630281032773\n",
      "Epoch 222, Loss: 0.45802054967026307\n",
      "Epoch 223, Loss: 0.45677224995978094\n",
      "Epoch 224, Loss: 0.45565914342704716\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23726527343498743\n",
      "Valence RMSE: 0.24495061380203417\n",
      "Arousal RMSE: 0.2293225168934909\n",
      "Test R^2 score: tensor([0.3588, 0.6122], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4854963574824981\n",
      "Num of epochs: 225\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Epoch 121, Loss: 0.4816228295571127\n",
      "Epoch 122, Loss: 0.4807618495484584\n",
      "Epoch 123, Loss: 0.48102030846138955\n",
      "Epoch 124, Loss: 0.4810176288364116\n",
      "Epoch 125, Loss: 0.48014338261661127\n",
      "Epoch 126, Loss: 0.47910314809270116\n",
      "Epoch 127, Loss: 0.47880795949683447\n",
      "Epoch 128, Loss: 0.47876177315142754\n",
      "Epoch 129, Loss: 0.47819597116449997\n",
      "Epoch 130, Loss: 0.47743214539542544\n",
      "Epoch 131, Loss: 0.47658476230340185\n",
      "Epoch 132, Loss: 0.4759644680184978\n",
      "Epoch 133, Loss: 0.4755635187712421\n",
      "Epoch 134, Loss: 0.47538264196584307\n",
      "Epoch 135, Loss: 0.4753673450596914\n",
      "Epoch 136, Loss: 0.4753697744168057\n",
      "Epoch 137, Loss: 0.4755973109583155\n",
      "Epoch 138, Loss: 0.4767626826605241\n",
      "Epoch 139, Loss: 0.47683528222287225\n",
      "Epoch 140, Loss: 0.47579445468261566\n",
      "Epoch 141, Loss: 0.47284533919249566\n",
      "Epoch 142, Loss: 0.472809286012082\n",
      "Epoch 143, Loss: 0.47458738111232796\n",
      "Epoch 144, Loss: 0.4733380058918715\n",
      "Epoch 145, Loss: 0.4715229034484008\n",
      "Epoch 146, Loss: 0.47096730966267386\n",
      "Epoch 147, Loss: 0.47161469890467383\n",
      "Epoch 148, Loss: 0.47183531411129226\n",
      "Epoch 149, Loss: 0.470294933688076\n",
      "Epoch 150, Loss: 0.4697114144973765\n",
      "Epoch 151, Loss: 0.47018789053701304\n",
      "Epoch 152, Loss: 0.4700460953124117\n",
      "Epoch 153, Loss: 0.4693645740016508\n",
      "Epoch 154, Loss: 0.4684558104180481\n",
      "Epoch 155, Loss: 0.4680960384979934\n",
      "Epoch 156, Loss: 0.4682454254529793\n",
      "Epoch 157, Loss: 0.4682974379105488\n",
      "Epoch 158, Loss: 0.4681590010361841\n",
      "Epoch 159, Loss: 0.4673225124817593\n",
      "Epoch 160, Loss: 0.4666701428107949\n",
      "Epoch 161, Loss: 0.4663166784414499\n",
      "Epoch 162, Loss: 0.46624854535037\n",
      "Epoch 163, Loss: 0.4664611566834947\n",
      "Epoch 164, Loss: 0.46654532450127006\n",
      "Epoch 165, Loss: 0.46721845607201035\n",
      "Epoch 166, Loss: 0.4668208160006968\n",
      "Epoch 167, Loss: 0.466573749669782\n",
      "Epoch 168, Loss: 0.46506387643568103\n",
      "Epoch 169, Loss: 0.4641220516950567\n",
      "Epoch 170, Loss: 0.4644550172656552\n",
      "Epoch 171, Loss: 0.4654613047116713\n",
      "Epoch 172, Loss: 0.46880176576246846\n",
      "Epoch 173, Loss: 0.4685649824581709\n",
      "Epoch 174, Loss: 0.46606115996454733\n",
      "Epoch 175, Loss: 0.46328263242364126\n",
      "Epoch 176, Loss: 0.4660484187313888\n",
      "Epoch 177, Loss: 0.4667669789465077\n",
      "Epoch 178, Loss: 0.46222975993575777\n",
      "Epoch 179, Loss: 0.46412214801342555\n",
      "Epoch 180, Loss: 0.46867384291927394\n",
      "Epoch 181, Loss: 0.46316269230838986\n",
      "Epoch 182, Loss: 0.4619791093960296\n",
      "Epoch 183, Loss: 0.4650783107301587\n",
      "Epoch 184, Loss: 0.4617210775403838\n",
      "Epoch 185, Loss: 0.4621011464022392\n",
      "Epoch 186, Loss: 0.4641367239628427\n",
      "Epoch 187, Loss: 0.46071131054180564\n",
      "Epoch 188, Loss: 0.46145892957673257\n",
      "Epoch 189, Loss: 0.464304908222076\n",
      "Epoch 190, Loss: 0.4606605441278432\n",
      "Epoch 191, Loss: 0.4601386308130386\n",
      "Epoch 192, Loss: 0.46193430495315263\n",
      "Epoch 193, Loss: 0.45977696912799226\n",
      "Epoch 194, Loss: 0.4597173803227511\n",
      "Epoch 195, Loss: 0.46096589922299847\n",
      "Epoch 196, Loss: 0.4592170612502741\n",
      "Epoch 197, Loss: 0.45864347419013624\n",
      "Epoch 198, Loss: 0.4594650511157848\n",
      "Epoch 199, Loss: 0.45924705942486493\n",
      "Epoch 200, Loss: 0.4584931199827434\n",
      "Epoch 201, Loss: 0.4578552480015059\n",
      "Epoch 202, Loss: 0.458251953125\n",
      "Epoch 203, Loss: 0.4586848316367108\n",
      "Epoch 204, Loss: 0.4577025349799131\n",
      "Epoch 205, Loss: 0.45707273702530193\n",
      "Epoch 206, Loss: 0.4571336158814618\n",
      "Epoch 207, Loss: 0.45746758002181026\n",
      "Epoch 208, Loss: 0.4580334166179955\n",
      "Epoch 209, Loss: 0.457551138813026\n",
      "Epoch 210, Loss: 0.4572461918817771\n",
      "Epoch 211, Loss: 0.45629613394883295\n",
      "Epoch 212, Loss: 0.4557945440718634\n",
      "Epoch 213, Loss: 0.455843155547024\n",
      "Epoch 214, Loss: 0.456219547378948\n",
      "Epoch 215, Loss: 0.4577116995236092\n",
      "Epoch 216, Loss: 0.4589176697652806\n",
      "Epoch 217, Loss: 0.46105034303609993\n",
      "Epoch 218, Loss: 0.4567326442394303\n",
      "Epoch 219, Loss: 0.45493112899589366\n",
      "Epoch 220, Loss: 0.45527199475335134\n",
      "Epoch 221, Loss: 0.45623630281032773\n",
      "Epoch 222, Loss: 0.45802054967026307\n",
      "Epoch 223, Loss: 0.45677224995978094\n",
      "Epoch 224, Loss: 0.45565914342704716\n",
      "Epoch 225, Loss: 0.45401674524485375\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23674545766256022\n",
      "Valence RMSE: 0.2449873531038483\n",
      "Arousal RMSE: 0.2282060916515607\n",
      "Test R^2 score: tensor([0.3586, 0.6159], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.487283735382245\n",
      "Num of epochs: 226\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Epoch 121, Loss: 0.4816228295571127\n",
      "Epoch 122, Loss: 0.4807618495484584\n",
      "Epoch 123, Loss: 0.48102030846138955\n",
      "Epoch 124, Loss: 0.4810176288364116\n",
      "Epoch 125, Loss: 0.48014338261661127\n",
      "Epoch 126, Loss: 0.47910314809270116\n",
      "Epoch 127, Loss: 0.47880795949683447\n",
      "Epoch 128, Loss: 0.47876177315142754\n",
      "Epoch 129, Loss: 0.47819597116449997\n",
      "Epoch 130, Loss: 0.47743214539542544\n",
      "Epoch 131, Loss: 0.47658476230340185\n",
      "Epoch 132, Loss: 0.4759644680184978\n",
      "Epoch 133, Loss: 0.4755635187712421\n",
      "Epoch 134, Loss: 0.47538264196584307\n",
      "Epoch 135, Loss: 0.4753673450596914\n",
      "Epoch 136, Loss: 0.4753697744168057\n",
      "Epoch 137, Loss: 0.4755973109583155\n",
      "Epoch 138, Loss: 0.4767626826605241\n",
      "Epoch 139, Loss: 0.47683528222287225\n",
      "Epoch 140, Loss: 0.47579445468261566\n",
      "Epoch 141, Loss: 0.47284533919249566\n",
      "Epoch 142, Loss: 0.472809286012082\n",
      "Epoch 143, Loss: 0.47458738111232796\n",
      "Epoch 144, Loss: 0.4733380058918715\n",
      "Epoch 145, Loss: 0.4715229034484008\n",
      "Epoch 146, Loss: 0.47096730966267386\n",
      "Epoch 147, Loss: 0.47161469890467383\n",
      "Epoch 148, Loss: 0.47183531411129226\n",
      "Epoch 149, Loss: 0.470294933688076\n",
      "Epoch 150, Loss: 0.4697114144973765\n",
      "Epoch 151, Loss: 0.47018789053701304\n",
      "Epoch 152, Loss: 0.4700460953124117\n",
      "Epoch 153, Loss: 0.4693645740016508\n",
      "Epoch 154, Loss: 0.4684558104180481\n",
      "Epoch 155, Loss: 0.4680960384979934\n",
      "Epoch 156, Loss: 0.4682454254529793\n",
      "Epoch 157, Loss: 0.4682974379105488\n",
      "Epoch 158, Loss: 0.4681590010361841\n",
      "Epoch 159, Loss: 0.4673225124817593\n",
      "Epoch 160, Loss: 0.4666701428107949\n",
      "Epoch 161, Loss: 0.4663166784414499\n",
      "Epoch 162, Loss: 0.46624854535037\n",
      "Epoch 163, Loss: 0.4664611566834947\n",
      "Epoch 164, Loss: 0.46654532450127006\n",
      "Epoch 165, Loss: 0.46721845607201035\n",
      "Epoch 166, Loss: 0.4668208160006968\n",
      "Epoch 167, Loss: 0.466573749669782\n",
      "Epoch 168, Loss: 0.46506387643568103\n",
      "Epoch 169, Loss: 0.4641220516950567\n",
      "Epoch 170, Loss: 0.4644550172656552\n",
      "Epoch 171, Loss: 0.4654613047116713\n",
      "Epoch 172, Loss: 0.46880176576246846\n",
      "Epoch 173, Loss: 0.4685649824581709\n",
      "Epoch 174, Loss: 0.46606115996454733\n",
      "Epoch 175, Loss: 0.46328263242364126\n",
      "Epoch 176, Loss: 0.4660484187313888\n",
      "Epoch 177, Loss: 0.4667669789465077\n",
      "Epoch 178, Loss: 0.46222975993575777\n",
      "Epoch 179, Loss: 0.46412214801342555\n",
      "Epoch 180, Loss: 0.46867384291927394\n",
      "Epoch 181, Loss: 0.46316269230838986\n",
      "Epoch 182, Loss: 0.4619791093960296\n",
      "Epoch 183, Loss: 0.4650783107301587\n",
      "Epoch 184, Loss: 0.4617210775403838\n",
      "Epoch 185, Loss: 0.4621011464022392\n",
      "Epoch 186, Loss: 0.4641367239628427\n",
      "Epoch 187, Loss: 0.46071131054180564\n",
      "Epoch 188, Loss: 0.46145892957673257\n",
      "Epoch 189, Loss: 0.464304908222076\n",
      "Epoch 190, Loss: 0.4606605441278432\n",
      "Epoch 191, Loss: 0.4601386308130386\n",
      "Epoch 192, Loss: 0.46193430495315263\n",
      "Epoch 193, Loss: 0.45977696912799226\n",
      "Epoch 194, Loss: 0.4597173803227511\n",
      "Epoch 195, Loss: 0.46096589922299847\n",
      "Epoch 196, Loss: 0.4592170612502741\n",
      "Epoch 197, Loss: 0.45864347419013624\n",
      "Epoch 198, Loss: 0.4594650511157848\n",
      "Epoch 199, Loss: 0.45924705942486493\n",
      "Epoch 200, Loss: 0.4584931199827434\n",
      "Epoch 201, Loss: 0.4578552480015059\n",
      "Epoch 202, Loss: 0.458251953125\n",
      "Epoch 203, Loss: 0.4586848316367108\n",
      "Epoch 204, Loss: 0.4577025349799131\n",
      "Epoch 205, Loss: 0.45707273702530193\n",
      "Epoch 206, Loss: 0.4571336158814618\n",
      "Epoch 207, Loss: 0.45746758002181026\n",
      "Epoch 208, Loss: 0.4580334166179955\n",
      "Epoch 209, Loss: 0.457551138813026\n",
      "Epoch 210, Loss: 0.4572461918817771\n",
      "Epoch 211, Loss: 0.45629613394883295\n",
      "Epoch 212, Loss: 0.4557945440718634\n",
      "Epoch 213, Loss: 0.455843155547024\n",
      "Epoch 214, Loss: 0.456219547378948\n",
      "Epoch 215, Loss: 0.4577116995236092\n",
      "Epoch 216, Loss: 0.4589176697652806\n",
      "Epoch 217, Loss: 0.46105034303609993\n",
      "Epoch 218, Loss: 0.4567326442394303\n",
      "Epoch 219, Loss: 0.45493112899589366\n",
      "Epoch 220, Loss: 0.45527199475335134\n",
      "Epoch 221, Loss: 0.45623630281032773\n",
      "Epoch 222, Loss: 0.45802054967026307\n",
      "Epoch 223, Loss: 0.45677224995978094\n",
      "Epoch 224, Loss: 0.45565914342704716\n",
      "Epoch 225, Loss: 0.45401674524485375\n",
      "Epoch 226, Loss: 0.4535057342791329\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23733799522949214\n",
      "Valence RMSE: 0.24694145240164947\n",
      "Arousal RMSE: 0.22732920411788882\n",
      "Test R^2 score: tensor([0.3484, 0.6189], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4836206172678466\n",
      "Num of epochs: 227\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Epoch 121, Loss: 0.4816228295571127\n",
      "Epoch 122, Loss: 0.4807618495484584\n",
      "Epoch 123, Loss: 0.48102030846138955\n",
      "Epoch 124, Loss: 0.4810176288364116\n",
      "Epoch 125, Loss: 0.48014338261661127\n",
      "Epoch 126, Loss: 0.47910314809270116\n",
      "Epoch 127, Loss: 0.47880795949683447\n",
      "Epoch 128, Loss: 0.47876177315142754\n",
      "Epoch 129, Loss: 0.47819597116449997\n",
      "Epoch 130, Loss: 0.47743214539542544\n",
      "Epoch 131, Loss: 0.47658476230340185\n",
      "Epoch 132, Loss: 0.4759644680184978\n",
      "Epoch 133, Loss: 0.4755635187712421\n",
      "Epoch 134, Loss: 0.47538264196584307\n",
      "Epoch 135, Loss: 0.4753673450596914\n",
      "Epoch 136, Loss: 0.4753697744168057\n",
      "Epoch 137, Loss: 0.4755973109583155\n",
      "Epoch 138, Loss: 0.4767626826605241\n",
      "Epoch 139, Loss: 0.47683528222287225\n",
      "Epoch 140, Loss: 0.47579445468261566\n",
      "Epoch 141, Loss: 0.47284533919249566\n",
      "Epoch 142, Loss: 0.472809286012082\n",
      "Epoch 143, Loss: 0.47458738111232796\n",
      "Epoch 144, Loss: 0.4733380058918715\n",
      "Epoch 145, Loss: 0.4715229034484008\n",
      "Epoch 146, Loss: 0.47096730966267386\n",
      "Epoch 147, Loss: 0.47161469890467383\n",
      "Epoch 148, Loss: 0.47183531411129226\n",
      "Epoch 149, Loss: 0.470294933688076\n",
      "Epoch 150, Loss: 0.4697114144973765\n",
      "Epoch 151, Loss: 0.47018789053701304\n",
      "Epoch 152, Loss: 0.4700460953124117\n",
      "Epoch 153, Loss: 0.4693645740016508\n",
      "Epoch 154, Loss: 0.4684558104180481\n",
      "Epoch 155, Loss: 0.4680960384979934\n",
      "Epoch 156, Loss: 0.4682454254529793\n",
      "Epoch 157, Loss: 0.4682974379105488\n",
      "Epoch 158, Loss: 0.4681590010361841\n",
      "Epoch 159, Loss: 0.4673225124817593\n",
      "Epoch 160, Loss: 0.4666701428107949\n",
      "Epoch 161, Loss: 0.4663166784414499\n",
      "Epoch 162, Loss: 0.46624854535037\n",
      "Epoch 163, Loss: 0.4664611566834947\n",
      "Epoch 164, Loss: 0.46654532450127006\n",
      "Epoch 165, Loss: 0.46721845607201035\n",
      "Epoch 166, Loss: 0.4668208160006968\n",
      "Epoch 167, Loss: 0.466573749669782\n",
      "Epoch 168, Loss: 0.46506387643568103\n",
      "Epoch 169, Loss: 0.4641220516950567\n",
      "Epoch 170, Loss: 0.4644550172656552\n",
      "Epoch 171, Loss: 0.4654613047116713\n",
      "Epoch 172, Loss: 0.46880176576246846\n",
      "Epoch 173, Loss: 0.4685649824581709\n",
      "Epoch 174, Loss: 0.46606115996454733\n",
      "Epoch 175, Loss: 0.46328263242364126\n",
      "Epoch 176, Loss: 0.4660484187313888\n",
      "Epoch 177, Loss: 0.4667669789465077\n",
      "Epoch 178, Loss: 0.46222975993575777\n",
      "Epoch 179, Loss: 0.46412214801342555\n",
      "Epoch 180, Loss: 0.46867384291927394\n",
      "Epoch 181, Loss: 0.46316269230838986\n",
      "Epoch 182, Loss: 0.4619791093960296\n",
      "Epoch 183, Loss: 0.4650783107301587\n",
      "Epoch 184, Loss: 0.4617210775403838\n",
      "Epoch 185, Loss: 0.4621011464022392\n",
      "Epoch 186, Loss: 0.4641367239628427\n",
      "Epoch 187, Loss: 0.46071131054180564\n",
      "Epoch 188, Loss: 0.46145892957673257\n",
      "Epoch 189, Loss: 0.464304908222076\n",
      "Epoch 190, Loss: 0.4606605441278432\n",
      "Epoch 191, Loss: 0.4601386308130386\n",
      "Epoch 192, Loss: 0.46193430495315263\n",
      "Epoch 193, Loss: 0.45977696912799226\n",
      "Epoch 194, Loss: 0.4597173803227511\n",
      "Epoch 195, Loss: 0.46096589922299847\n",
      "Epoch 196, Loss: 0.4592170612502741\n",
      "Epoch 197, Loss: 0.45864347419013624\n",
      "Epoch 198, Loss: 0.4594650511157848\n",
      "Epoch 199, Loss: 0.45924705942486493\n",
      "Epoch 200, Loss: 0.4584931199827434\n",
      "Epoch 201, Loss: 0.4578552480015059\n",
      "Epoch 202, Loss: 0.458251953125\n",
      "Epoch 203, Loss: 0.4586848316367108\n",
      "Epoch 204, Loss: 0.4577025349799131\n",
      "Epoch 205, Loss: 0.45707273702530193\n",
      "Epoch 206, Loss: 0.4571336158814618\n",
      "Epoch 207, Loss: 0.45746758002181026\n",
      "Epoch 208, Loss: 0.4580334166179955\n",
      "Epoch 209, Loss: 0.457551138813026\n",
      "Epoch 210, Loss: 0.4572461918817771\n",
      "Epoch 211, Loss: 0.45629613394883295\n",
      "Epoch 212, Loss: 0.4557945440718634\n",
      "Epoch 213, Loss: 0.455843155547024\n",
      "Epoch 214, Loss: 0.456219547378948\n",
      "Epoch 215, Loss: 0.4577116995236092\n",
      "Epoch 216, Loss: 0.4589176697652806\n",
      "Epoch 217, Loss: 0.46105034303609993\n",
      "Epoch 218, Loss: 0.4567326442394303\n",
      "Epoch 219, Loss: 0.45493112899589366\n",
      "Epoch 220, Loss: 0.45527199475335134\n",
      "Epoch 221, Loss: 0.45623630281032773\n",
      "Epoch 222, Loss: 0.45802054967026307\n",
      "Epoch 223, Loss: 0.45677224995978094\n",
      "Epoch 224, Loss: 0.45565914342704716\n",
      "Epoch 225, Loss: 0.45401674524485375\n",
      "Epoch 226, Loss: 0.4535057342791329\n",
      "Epoch 227, Loss: 0.45416164205845616\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2371452265125313\n",
      "Valence RMSE: 0.24424575656330827\n",
      "Arousal RMSE: 0.22982542791470376\n",
      "Test R^2 score: tensor([0.3625, 0.6105], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.486487213853818\n",
      "Num of epochs: 228\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Epoch 121, Loss: 0.4816228295571127\n",
      "Epoch 122, Loss: 0.4807618495484584\n",
      "Epoch 123, Loss: 0.48102030846138955\n",
      "Epoch 124, Loss: 0.4810176288364116\n",
      "Epoch 125, Loss: 0.48014338261661127\n",
      "Epoch 126, Loss: 0.47910314809270116\n",
      "Epoch 127, Loss: 0.47880795949683447\n",
      "Epoch 128, Loss: 0.47876177315142754\n",
      "Epoch 129, Loss: 0.47819597116449997\n",
      "Epoch 130, Loss: 0.47743214539542544\n",
      "Epoch 131, Loss: 0.47658476230340185\n",
      "Epoch 132, Loss: 0.4759644680184978\n",
      "Epoch 133, Loss: 0.4755635187712421\n",
      "Epoch 134, Loss: 0.47538264196584307\n",
      "Epoch 135, Loss: 0.4753673450596914\n",
      "Epoch 136, Loss: 0.4753697744168057\n",
      "Epoch 137, Loss: 0.4755973109583155\n",
      "Epoch 138, Loss: 0.4767626826605241\n",
      "Epoch 139, Loss: 0.47683528222287225\n",
      "Epoch 140, Loss: 0.47579445468261566\n",
      "Epoch 141, Loss: 0.47284533919249566\n",
      "Epoch 142, Loss: 0.472809286012082\n",
      "Epoch 143, Loss: 0.47458738111232796\n",
      "Epoch 144, Loss: 0.4733380058918715\n",
      "Epoch 145, Loss: 0.4715229034484008\n",
      "Epoch 146, Loss: 0.47096730966267386\n",
      "Epoch 147, Loss: 0.47161469890467383\n",
      "Epoch 148, Loss: 0.47183531411129226\n",
      "Epoch 149, Loss: 0.470294933688076\n",
      "Epoch 150, Loss: 0.4697114144973765\n",
      "Epoch 151, Loss: 0.47018789053701304\n",
      "Epoch 152, Loss: 0.4700460953124117\n",
      "Epoch 153, Loss: 0.4693645740016508\n",
      "Epoch 154, Loss: 0.4684558104180481\n",
      "Epoch 155, Loss: 0.4680960384979934\n",
      "Epoch 156, Loss: 0.4682454254529793\n",
      "Epoch 157, Loss: 0.4682974379105488\n",
      "Epoch 158, Loss: 0.4681590010361841\n",
      "Epoch 159, Loss: 0.4673225124817593\n",
      "Epoch 160, Loss: 0.4666701428107949\n",
      "Epoch 161, Loss: 0.4663166784414499\n",
      "Epoch 162, Loss: 0.46624854535037\n",
      "Epoch 163, Loss: 0.4664611566834947\n",
      "Epoch 164, Loss: 0.46654532450127006\n",
      "Epoch 165, Loss: 0.46721845607201035\n",
      "Epoch 166, Loss: 0.4668208160006968\n",
      "Epoch 167, Loss: 0.466573749669782\n",
      "Epoch 168, Loss: 0.46506387643568103\n",
      "Epoch 169, Loss: 0.4641220516950567\n",
      "Epoch 170, Loss: 0.4644550172656552\n",
      "Epoch 171, Loss: 0.4654613047116713\n",
      "Epoch 172, Loss: 0.46880176576246846\n",
      "Epoch 173, Loss: 0.4685649824581709\n",
      "Epoch 174, Loss: 0.46606115996454733\n",
      "Epoch 175, Loss: 0.46328263242364126\n",
      "Epoch 176, Loss: 0.4660484187313888\n",
      "Epoch 177, Loss: 0.4667669789465077\n",
      "Epoch 178, Loss: 0.46222975993575777\n",
      "Epoch 179, Loss: 0.46412214801342555\n",
      "Epoch 180, Loss: 0.46867384291927394\n",
      "Epoch 181, Loss: 0.46316269230838986\n",
      "Epoch 182, Loss: 0.4619791093960296\n",
      "Epoch 183, Loss: 0.4650783107301587\n",
      "Epoch 184, Loss: 0.4617210775403838\n",
      "Epoch 185, Loss: 0.4621011464022392\n",
      "Epoch 186, Loss: 0.4641367239628427\n",
      "Epoch 187, Loss: 0.46071131054180564\n",
      "Epoch 188, Loss: 0.46145892957673257\n",
      "Epoch 189, Loss: 0.464304908222076\n",
      "Epoch 190, Loss: 0.4606605441278432\n",
      "Epoch 191, Loss: 0.4601386308130386\n",
      "Epoch 192, Loss: 0.46193430495315263\n",
      "Epoch 193, Loss: 0.45977696912799226\n",
      "Epoch 194, Loss: 0.4597173803227511\n",
      "Epoch 195, Loss: 0.46096589922299847\n",
      "Epoch 196, Loss: 0.4592170612502741\n",
      "Epoch 197, Loss: 0.45864347419013624\n",
      "Epoch 198, Loss: 0.4594650511157848\n",
      "Epoch 199, Loss: 0.45924705942486493\n",
      "Epoch 200, Loss: 0.4584931199827434\n",
      "Epoch 201, Loss: 0.4578552480015059\n",
      "Epoch 202, Loss: 0.458251953125\n",
      "Epoch 203, Loss: 0.4586848316367108\n",
      "Epoch 204, Loss: 0.4577025349799131\n",
      "Epoch 205, Loss: 0.45707273702530193\n",
      "Epoch 206, Loss: 0.4571336158814618\n",
      "Epoch 207, Loss: 0.45746758002181026\n",
      "Epoch 208, Loss: 0.4580334166179955\n",
      "Epoch 209, Loss: 0.457551138813026\n",
      "Epoch 210, Loss: 0.4572461918817771\n",
      "Epoch 211, Loss: 0.45629613394883295\n",
      "Epoch 212, Loss: 0.4557945440718634\n",
      "Epoch 213, Loss: 0.455843155547024\n",
      "Epoch 214, Loss: 0.456219547378948\n",
      "Epoch 215, Loss: 0.4577116995236092\n",
      "Epoch 216, Loss: 0.4589176697652806\n",
      "Epoch 217, Loss: 0.46105034303609993\n",
      "Epoch 218, Loss: 0.4567326442394303\n",
      "Epoch 219, Loss: 0.45493112899589366\n",
      "Epoch 220, Loss: 0.45527199475335134\n",
      "Epoch 221, Loss: 0.45623630281032773\n",
      "Epoch 222, Loss: 0.45802054967026307\n",
      "Epoch 223, Loss: 0.45677224995978094\n",
      "Epoch 224, Loss: 0.45565914342704716\n",
      "Epoch 225, Loss: 0.45401674524485375\n",
      "Epoch 226, Loss: 0.4535057342791329\n",
      "Epoch 227, Loss: 0.45416164205845616\n",
      "Epoch 228, Loss: 0.45450513400796655\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23860279092591538\n",
      "Valence RMSE: 0.24902125142664341\n",
      "Arousal RMSE: 0.22770814656744412\n",
      "Test R^2 score: tensor([0.3373, 0.6176], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4774734693109149\n",
      "Num of epochs: 229\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Epoch 121, Loss: 0.4816228295571127\n",
      "Epoch 122, Loss: 0.4807618495484584\n",
      "Epoch 123, Loss: 0.48102030846138955\n",
      "Epoch 124, Loss: 0.4810176288364116\n",
      "Epoch 125, Loss: 0.48014338261661127\n",
      "Epoch 126, Loss: 0.47910314809270116\n",
      "Epoch 127, Loss: 0.47880795949683447\n",
      "Epoch 128, Loss: 0.47876177315142754\n",
      "Epoch 129, Loss: 0.47819597116449997\n",
      "Epoch 130, Loss: 0.47743214539542544\n",
      "Epoch 131, Loss: 0.47658476230340185\n",
      "Epoch 132, Loss: 0.4759644680184978\n",
      "Epoch 133, Loss: 0.4755635187712421\n",
      "Epoch 134, Loss: 0.47538264196584307\n",
      "Epoch 135, Loss: 0.4753673450596914\n",
      "Epoch 136, Loss: 0.4753697744168057\n",
      "Epoch 137, Loss: 0.4755973109583155\n",
      "Epoch 138, Loss: 0.4767626826605241\n",
      "Epoch 139, Loss: 0.47683528222287225\n",
      "Epoch 140, Loss: 0.47579445468261566\n",
      "Epoch 141, Loss: 0.47284533919249566\n",
      "Epoch 142, Loss: 0.472809286012082\n",
      "Epoch 143, Loss: 0.47458738111232796\n",
      "Epoch 144, Loss: 0.4733380058918715\n",
      "Epoch 145, Loss: 0.4715229034484008\n",
      "Epoch 146, Loss: 0.47096730966267386\n",
      "Epoch 147, Loss: 0.47161469890467383\n",
      "Epoch 148, Loss: 0.47183531411129226\n",
      "Epoch 149, Loss: 0.470294933688076\n",
      "Epoch 150, Loss: 0.4697114144973765\n",
      "Epoch 151, Loss: 0.47018789053701304\n",
      "Epoch 152, Loss: 0.4700460953124117\n",
      "Epoch 153, Loss: 0.4693645740016508\n",
      "Epoch 154, Loss: 0.4684558104180481\n",
      "Epoch 155, Loss: 0.4680960384979934\n",
      "Epoch 156, Loss: 0.4682454254529793\n",
      "Epoch 157, Loss: 0.4682974379105488\n",
      "Epoch 158, Loss: 0.4681590010361841\n",
      "Epoch 159, Loss: 0.4673225124817593\n",
      "Epoch 160, Loss: 0.4666701428107949\n",
      "Epoch 161, Loss: 0.4663166784414499\n",
      "Epoch 162, Loss: 0.46624854535037\n",
      "Epoch 163, Loss: 0.4664611566834947\n",
      "Epoch 164, Loss: 0.46654532450127006\n",
      "Epoch 165, Loss: 0.46721845607201035\n",
      "Epoch 166, Loss: 0.4668208160006968\n",
      "Epoch 167, Loss: 0.466573749669782\n",
      "Epoch 168, Loss: 0.46506387643568103\n",
      "Epoch 169, Loss: 0.4641220516950567\n",
      "Epoch 170, Loss: 0.4644550172656552\n",
      "Epoch 171, Loss: 0.4654613047116713\n",
      "Epoch 172, Loss: 0.46880176576246846\n",
      "Epoch 173, Loss: 0.4685649824581709\n",
      "Epoch 174, Loss: 0.46606115996454733\n",
      "Epoch 175, Loss: 0.46328263242364126\n",
      "Epoch 176, Loss: 0.4660484187313888\n",
      "Epoch 177, Loss: 0.4667669789465077\n",
      "Epoch 178, Loss: 0.46222975993575777\n",
      "Epoch 179, Loss: 0.46412214801342555\n",
      "Epoch 180, Loss: 0.46867384291927394\n",
      "Epoch 181, Loss: 0.46316269230838986\n",
      "Epoch 182, Loss: 0.4619791093960296\n",
      "Epoch 183, Loss: 0.4650783107301587\n",
      "Epoch 184, Loss: 0.4617210775403838\n",
      "Epoch 185, Loss: 0.4621011464022392\n",
      "Epoch 186, Loss: 0.4641367239628427\n",
      "Epoch 187, Loss: 0.46071131054180564\n",
      "Epoch 188, Loss: 0.46145892957673257\n",
      "Epoch 189, Loss: 0.464304908222076\n",
      "Epoch 190, Loss: 0.4606605441278432\n",
      "Epoch 191, Loss: 0.4601386308130386\n",
      "Epoch 192, Loss: 0.46193430495315263\n",
      "Epoch 193, Loss: 0.45977696912799226\n",
      "Epoch 194, Loss: 0.4597173803227511\n",
      "Epoch 195, Loss: 0.46096589922299847\n",
      "Epoch 196, Loss: 0.4592170612502741\n",
      "Epoch 197, Loss: 0.45864347419013624\n",
      "Epoch 198, Loss: 0.4594650511157848\n",
      "Epoch 199, Loss: 0.45924705942486493\n",
      "Epoch 200, Loss: 0.4584931199827434\n",
      "Epoch 201, Loss: 0.4578552480015059\n",
      "Epoch 202, Loss: 0.458251953125\n",
      "Epoch 203, Loss: 0.4586848316367108\n",
      "Epoch 204, Loss: 0.4577025349799131\n",
      "Epoch 205, Loss: 0.45707273702530193\n",
      "Epoch 206, Loss: 0.4571336158814618\n",
      "Epoch 207, Loss: 0.45746758002181026\n",
      "Epoch 208, Loss: 0.4580334166179955\n",
      "Epoch 209, Loss: 0.457551138813026\n",
      "Epoch 210, Loss: 0.4572461918817771\n",
      "Epoch 211, Loss: 0.45629613394883295\n",
      "Epoch 212, Loss: 0.4557945440718634\n",
      "Epoch 213, Loss: 0.455843155547024\n",
      "Epoch 214, Loss: 0.456219547378948\n",
      "Epoch 215, Loss: 0.4577116995236092\n",
      "Epoch 216, Loss: 0.4589176697652806\n",
      "Epoch 217, Loss: 0.46105034303609993\n",
      "Epoch 218, Loss: 0.4567326442394303\n",
      "Epoch 219, Loss: 0.45493112899589366\n",
      "Epoch 220, Loss: 0.45527199475335134\n",
      "Epoch 221, Loss: 0.45623630281032773\n",
      "Epoch 222, Loss: 0.45802054967026307\n",
      "Epoch 223, Loss: 0.45677224995978094\n",
      "Epoch 224, Loss: 0.45565914342704716\n",
      "Epoch 225, Loss: 0.45401674524485375\n",
      "Epoch 226, Loss: 0.4535057342791329\n",
      "Epoch 227, Loss: 0.45416164205845616\n",
      "Epoch 228, Loss: 0.45450513400796655\n",
      "Epoch 229, Loss: 0.4548238117351997\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2371123607298187\n",
      "Valence RMSE: 0.24427052002036978\n",
      "Arousal RMSE: 0.22973126968419744\n",
      "Test R^2 score: tensor([0.3624, 0.6108], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4865821389598885\n",
      "Num of epochs: 230\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Epoch 121, Loss: 0.4816228295571127\n",
      "Epoch 122, Loss: 0.4807618495484584\n",
      "Epoch 123, Loss: 0.48102030846138955\n",
      "Epoch 124, Loss: 0.4810176288364116\n",
      "Epoch 125, Loss: 0.48014338261661127\n",
      "Epoch 126, Loss: 0.47910314809270116\n",
      "Epoch 127, Loss: 0.47880795949683447\n",
      "Epoch 128, Loss: 0.47876177315142754\n",
      "Epoch 129, Loss: 0.47819597116449997\n",
      "Epoch 130, Loss: 0.47743214539542544\n",
      "Epoch 131, Loss: 0.47658476230340185\n",
      "Epoch 132, Loss: 0.4759644680184978\n",
      "Epoch 133, Loss: 0.4755635187712421\n",
      "Epoch 134, Loss: 0.47538264196584307\n",
      "Epoch 135, Loss: 0.4753673450596914\n",
      "Epoch 136, Loss: 0.4753697744168057\n",
      "Epoch 137, Loss: 0.4755973109583155\n",
      "Epoch 138, Loss: 0.4767626826605241\n",
      "Epoch 139, Loss: 0.47683528222287225\n",
      "Epoch 140, Loss: 0.47579445468261566\n",
      "Epoch 141, Loss: 0.47284533919249566\n",
      "Epoch 142, Loss: 0.472809286012082\n",
      "Epoch 143, Loss: 0.47458738111232796\n",
      "Epoch 144, Loss: 0.4733380058918715\n",
      "Epoch 145, Loss: 0.4715229034484008\n",
      "Epoch 146, Loss: 0.47096730966267386\n",
      "Epoch 147, Loss: 0.47161469890467383\n",
      "Epoch 148, Loss: 0.47183531411129226\n",
      "Epoch 149, Loss: 0.470294933688076\n",
      "Epoch 150, Loss: 0.4697114144973765\n",
      "Epoch 151, Loss: 0.47018789053701304\n",
      "Epoch 152, Loss: 0.4700460953124117\n",
      "Epoch 153, Loss: 0.4693645740016508\n",
      "Epoch 154, Loss: 0.4684558104180481\n",
      "Epoch 155, Loss: 0.4680960384979934\n",
      "Epoch 156, Loss: 0.4682454254529793\n",
      "Epoch 157, Loss: 0.4682974379105488\n",
      "Epoch 158, Loss: 0.4681590010361841\n",
      "Epoch 159, Loss: 0.4673225124817593\n",
      "Epoch 160, Loss: 0.4666701428107949\n",
      "Epoch 161, Loss: 0.4663166784414499\n",
      "Epoch 162, Loss: 0.46624854535037\n",
      "Epoch 163, Loss: 0.4664611566834947\n",
      "Epoch 164, Loss: 0.46654532450127006\n",
      "Epoch 165, Loss: 0.46721845607201035\n",
      "Epoch 166, Loss: 0.4668208160006968\n",
      "Epoch 167, Loss: 0.466573749669782\n",
      "Epoch 168, Loss: 0.46506387643568103\n",
      "Epoch 169, Loss: 0.4641220516950567\n",
      "Epoch 170, Loss: 0.4644550172656552\n",
      "Epoch 171, Loss: 0.4654613047116713\n",
      "Epoch 172, Loss: 0.46880176576246846\n",
      "Epoch 173, Loss: 0.4685649824581709\n",
      "Epoch 174, Loss: 0.46606115996454733\n",
      "Epoch 175, Loss: 0.46328263242364126\n",
      "Epoch 176, Loss: 0.4660484187313888\n",
      "Epoch 177, Loss: 0.4667669789465077\n",
      "Epoch 178, Loss: 0.46222975993575777\n",
      "Epoch 179, Loss: 0.46412214801342555\n",
      "Epoch 180, Loss: 0.46867384291927394\n",
      "Epoch 181, Loss: 0.46316269230838986\n",
      "Epoch 182, Loss: 0.4619791093960296\n",
      "Epoch 183, Loss: 0.4650783107301587\n",
      "Epoch 184, Loss: 0.4617210775403838\n",
      "Epoch 185, Loss: 0.4621011464022392\n",
      "Epoch 186, Loss: 0.4641367239628427\n",
      "Epoch 187, Loss: 0.46071131054180564\n",
      "Epoch 188, Loss: 0.46145892957673257\n",
      "Epoch 189, Loss: 0.464304908222076\n",
      "Epoch 190, Loss: 0.4606605441278432\n",
      "Epoch 191, Loss: 0.4601386308130386\n",
      "Epoch 192, Loss: 0.46193430495315263\n",
      "Epoch 193, Loss: 0.45977696912799226\n",
      "Epoch 194, Loss: 0.4597173803227511\n",
      "Epoch 195, Loss: 0.46096589922299847\n",
      "Epoch 196, Loss: 0.4592170612502741\n",
      "Epoch 197, Loss: 0.45864347419013624\n",
      "Epoch 198, Loss: 0.4594650511157848\n",
      "Epoch 199, Loss: 0.45924705942486493\n",
      "Epoch 200, Loss: 0.4584931199827434\n",
      "Epoch 201, Loss: 0.4578552480015059\n",
      "Epoch 202, Loss: 0.458251953125\n",
      "Epoch 203, Loss: 0.4586848316367108\n",
      "Epoch 204, Loss: 0.4577025349799131\n",
      "Epoch 205, Loss: 0.45707273702530193\n",
      "Epoch 206, Loss: 0.4571336158814618\n",
      "Epoch 207, Loss: 0.45746758002181026\n",
      "Epoch 208, Loss: 0.4580334166179955\n",
      "Epoch 209, Loss: 0.457551138813026\n",
      "Epoch 210, Loss: 0.4572461918817771\n",
      "Epoch 211, Loss: 0.45629613394883295\n",
      "Epoch 212, Loss: 0.4557945440718634\n",
      "Epoch 213, Loss: 0.455843155547024\n",
      "Epoch 214, Loss: 0.456219547378948\n",
      "Epoch 215, Loss: 0.4577116995236092\n",
      "Epoch 216, Loss: 0.4589176697652806\n",
      "Epoch 217, Loss: 0.46105034303609993\n",
      "Epoch 218, Loss: 0.4567326442394303\n",
      "Epoch 219, Loss: 0.45493112899589366\n",
      "Epoch 220, Loss: 0.45527199475335134\n",
      "Epoch 221, Loss: 0.45623630281032773\n",
      "Epoch 222, Loss: 0.45802054967026307\n",
      "Epoch 223, Loss: 0.45677224995978094\n",
      "Epoch 224, Loss: 0.45565914342704716\n",
      "Epoch 225, Loss: 0.45401674524485375\n",
      "Epoch 226, Loss: 0.4535057342791329\n",
      "Epoch 227, Loss: 0.45416164205845616\n",
      "Epoch 228, Loss: 0.45450513400796655\n",
      "Epoch 229, Loss: 0.4548238117351997\n",
      "Epoch 230, Loss: 0.4537990916311975\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23778058133963545\n",
      "Valence RMSE: 0.24726295989762118\n",
      "Arousal RMSE: 0.22790401134490235\n",
      "Test R^2 score: tensor([0.3467, 0.6169], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4818067602413219\n",
      "Num of epochs: 231\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Epoch 121, Loss: 0.4816228295571127\n",
      "Epoch 122, Loss: 0.4807618495484584\n",
      "Epoch 123, Loss: 0.48102030846138955\n",
      "Epoch 124, Loss: 0.4810176288364116\n",
      "Epoch 125, Loss: 0.48014338261661127\n",
      "Epoch 126, Loss: 0.47910314809270116\n",
      "Epoch 127, Loss: 0.47880795949683447\n",
      "Epoch 128, Loss: 0.47876177315142754\n",
      "Epoch 129, Loss: 0.47819597116449997\n",
      "Epoch 130, Loss: 0.47743214539542544\n",
      "Epoch 131, Loss: 0.47658476230340185\n",
      "Epoch 132, Loss: 0.4759644680184978\n",
      "Epoch 133, Loss: 0.4755635187712421\n",
      "Epoch 134, Loss: 0.47538264196584307\n",
      "Epoch 135, Loss: 0.4753673450596914\n",
      "Epoch 136, Loss: 0.4753697744168057\n",
      "Epoch 137, Loss: 0.4755973109583155\n",
      "Epoch 138, Loss: 0.4767626826605241\n",
      "Epoch 139, Loss: 0.47683528222287225\n",
      "Epoch 140, Loss: 0.47579445468261566\n",
      "Epoch 141, Loss: 0.47284533919249566\n",
      "Epoch 142, Loss: 0.472809286012082\n",
      "Epoch 143, Loss: 0.47458738111232796\n",
      "Epoch 144, Loss: 0.4733380058918715\n",
      "Epoch 145, Loss: 0.4715229034484008\n",
      "Epoch 146, Loss: 0.47096730966267386\n",
      "Epoch 147, Loss: 0.47161469890467383\n",
      "Epoch 148, Loss: 0.47183531411129226\n",
      "Epoch 149, Loss: 0.470294933688076\n",
      "Epoch 150, Loss: 0.4697114144973765\n",
      "Epoch 151, Loss: 0.47018789053701304\n",
      "Epoch 152, Loss: 0.4700460953124117\n",
      "Epoch 153, Loss: 0.4693645740016508\n",
      "Epoch 154, Loss: 0.4684558104180481\n",
      "Epoch 155, Loss: 0.4680960384979934\n",
      "Epoch 156, Loss: 0.4682454254529793\n",
      "Epoch 157, Loss: 0.4682974379105488\n",
      "Epoch 158, Loss: 0.4681590010361841\n",
      "Epoch 159, Loss: 0.4673225124817593\n",
      "Epoch 160, Loss: 0.4666701428107949\n",
      "Epoch 161, Loss: 0.4663166784414499\n",
      "Epoch 162, Loss: 0.46624854535037\n",
      "Epoch 163, Loss: 0.4664611566834947\n",
      "Epoch 164, Loss: 0.46654532450127006\n",
      "Epoch 165, Loss: 0.46721845607201035\n",
      "Epoch 166, Loss: 0.4668208160006968\n",
      "Epoch 167, Loss: 0.466573749669782\n",
      "Epoch 168, Loss: 0.46506387643568103\n",
      "Epoch 169, Loss: 0.4641220516950567\n",
      "Epoch 170, Loss: 0.4644550172656552\n",
      "Epoch 171, Loss: 0.4654613047116713\n",
      "Epoch 172, Loss: 0.46880176576246846\n",
      "Epoch 173, Loss: 0.4685649824581709\n",
      "Epoch 174, Loss: 0.46606115996454733\n",
      "Epoch 175, Loss: 0.46328263242364126\n",
      "Epoch 176, Loss: 0.4660484187313888\n",
      "Epoch 177, Loss: 0.4667669789465077\n",
      "Epoch 178, Loss: 0.46222975993575777\n",
      "Epoch 179, Loss: 0.46412214801342555\n",
      "Epoch 180, Loss: 0.46867384291927394\n",
      "Epoch 181, Loss: 0.46316269230838986\n",
      "Epoch 182, Loss: 0.4619791093960296\n",
      "Epoch 183, Loss: 0.4650783107301587\n",
      "Epoch 184, Loss: 0.4617210775403838\n",
      "Epoch 185, Loss: 0.4621011464022392\n",
      "Epoch 186, Loss: 0.4641367239628427\n",
      "Epoch 187, Loss: 0.46071131054180564\n",
      "Epoch 188, Loss: 0.46145892957673257\n",
      "Epoch 189, Loss: 0.464304908222076\n",
      "Epoch 190, Loss: 0.4606605441278432\n",
      "Epoch 191, Loss: 0.4601386308130386\n",
      "Epoch 192, Loss: 0.46193430495315263\n",
      "Epoch 193, Loss: 0.45977696912799226\n",
      "Epoch 194, Loss: 0.4597173803227511\n",
      "Epoch 195, Loss: 0.46096589922299847\n",
      "Epoch 196, Loss: 0.4592170612502741\n",
      "Epoch 197, Loss: 0.45864347419013624\n",
      "Epoch 198, Loss: 0.4594650511157848\n",
      "Epoch 199, Loss: 0.45924705942486493\n",
      "Epoch 200, Loss: 0.4584931199827434\n",
      "Epoch 201, Loss: 0.4578552480015059\n",
      "Epoch 202, Loss: 0.458251953125\n",
      "Epoch 203, Loss: 0.4586848316367108\n",
      "Epoch 204, Loss: 0.4577025349799131\n",
      "Epoch 205, Loss: 0.45707273702530193\n",
      "Epoch 206, Loss: 0.4571336158814618\n",
      "Epoch 207, Loss: 0.45746758002181026\n",
      "Epoch 208, Loss: 0.4580334166179955\n",
      "Epoch 209, Loss: 0.457551138813026\n",
      "Epoch 210, Loss: 0.4572461918817771\n",
      "Epoch 211, Loss: 0.45629613394883295\n",
      "Epoch 212, Loss: 0.4557945440718634\n",
      "Epoch 213, Loss: 0.455843155547024\n",
      "Epoch 214, Loss: 0.456219547378948\n",
      "Epoch 215, Loss: 0.4577116995236092\n",
      "Epoch 216, Loss: 0.4589176697652806\n",
      "Epoch 217, Loss: 0.46105034303609993\n",
      "Epoch 218, Loss: 0.4567326442394303\n",
      "Epoch 219, Loss: 0.45493112899589366\n",
      "Epoch 220, Loss: 0.45527199475335134\n",
      "Epoch 221, Loss: 0.45623630281032773\n",
      "Epoch 222, Loss: 0.45802054967026307\n",
      "Epoch 223, Loss: 0.45677224995978094\n",
      "Epoch 224, Loss: 0.45565914342704716\n",
      "Epoch 225, Loss: 0.45401674524485375\n",
      "Epoch 226, Loss: 0.4535057342791329\n",
      "Epoch 227, Loss: 0.45416164205845616\n",
      "Epoch 228, Loss: 0.45450513400796655\n",
      "Epoch 229, Loss: 0.4548238117351997\n",
      "Epoch 230, Loss: 0.4537990916311975\n",
      "Epoch 231, Loss: 0.4532521496977959\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23712618795955095\n",
      "Valence RMSE: 0.2452827799878229\n",
      "Arousal RMSE: 0.22867884876809627\n",
      "Test R^2 score: tensor([0.3571, 0.6143], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4857133833680443\n",
      "Num of epochs: 232\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Epoch 121, Loss: 0.4816228295571127\n",
      "Epoch 122, Loss: 0.4807618495484584\n",
      "Epoch 123, Loss: 0.48102030846138955\n",
      "Epoch 124, Loss: 0.4810176288364116\n",
      "Epoch 125, Loss: 0.48014338261661127\n",
      "Epoch 126, Loss: 0.47910314809270116\n",
      "Epoch 127, Loss: 0.47880795949683447\n",
      "Epoch 128, Loss: 0.47876177315142754\n",
      "Epoch 129, Loss: 0.47819597116449997\n",
      "Epoch 130, Loss: 0.47743214539542544\n",
      "Epoch 131, Loss: 0.47658476230340185\n",
      "Epoch 132, Loss: 0.4759644680184978\n",
      "Epoch 133, Loss: 0.4755635187712421\n",
      "Epoch 134, Loss: 0.47538264196584307\n",
      "Epoch 135, Loss: 0.4753673450596914\n",
      "Epoch 136, Loss: 0.4753697744168057\n",
      "Epoch 137, Loss: 0.4755973109583155\n",
      "Epoch 138, Loss: 0.4767626826605241\n",
      "Epoch 139, Loss: 0.47683528222287225\n",
      "Epoch 140, Loss: 0.47579445468261566\n",
      "Epoch 141, Loss: 0.47284533919249566\n",
      "Epoch 142, Loss: 0.472809286012082\n",
      "Epoch 143, Loss: 0.47458738111232796\n",
      "Epoch 144, Loss: 0.4733380058918715\n",
      "Epoch 145, Loss: 0.4715229034484008\n",
      "Epoch 146, Loss: 0.47096730966267386\n",
      "Epoch 147, Loss: 0.47161469890467383\n",
      "Epoch 148, Loss: 0.47183531411129226\n",
      "Epoch 149, Loss: 0.470294933688076\n",
      "Epoch 150, Loss: 0.4697114144973765\n",
      "Epoch 151, Loss: 0.47018789053701304\n",
      "Epoch 152, Loss: 0.4700460953124117\n",
      "Epoch 153, Loss: 0.4693645740016508\n",
      "Epoch 154, Loss: 0.4684558104180481\n",
      "Epoch 155, Loss: 0.4680960384979934\n",
      "Epoch 156, Loss: 0.4682454254529793\n",
      "Epoch 157, Loss: 0.4682974379105488\n",
      "Epoch 158, Loss: 0.4681590010361841\n",
      "Epoch 159, Loss: 0.4673225124817593\n",
      "Epoch 160, Loss: 0.4666701428107949\n",
      "Epoch 161, Loss: 0.4663166784414499\n",
      "Epoch 162, Loss: 0.46624854535037\n",
      "Epoch 163, Loss: 0.4664611566834947\n",
      "Epoch 164, Loss: 0.46654532450127006\n",
      "Epoch 165, Loss: 0.46721845607201035\n",
      "Epoch 166, Loss: 0.4668208160006968\n",
      "Epoch 167, Loss: 0.466573749669782\n",
      "Epoch 168, Loss: 0.46506387643568103\n",
      "Epoch 169, Loss: 0.4641220516950567\n",
      "Epoch 170, Loss: 0.4644550172656552\n",
      "Epoch 171, Loss: 0.4654613047116713\n",
      "Epoch 172, Loss: 0.46880176576246846\n",
      "Epoch 173, Loss: 0.4685649824581709\n",
      "Epoch 174, Loss: 0.46606115996454733\n",
      "Epoch 175, Loss: 0.46328263242364126\n",
      "Epoch 176, Loss: 0.4660484187313888\n",
      "Epoch 177, Loss: 0.4667669789465077\n",
      "Epoch 178, Loss: 0.46222975993575777\n",
      "Epoch 179, Loss: 0.46412214801342555\n",
      "Epoch 180, Loss: 0.46867384291927394\n",
      "Epoch 181, Loss: 0.46316269230838986\n",
      "Epoch 182, Loss: 0.4619791093960296\n",
      "Epoch 183, Loss: 0.4650783107301587\n",
      "Epoch 184, Loss: 0.4617210775403838\n",
      "Epoch 185, Loss: 0.4621011464022392\n",
      "Epoch 186, Loss: 0.4641367239628427\n",
      "Epoch 187, Loss: 0.46071131054180564\n",
      "Epoch 188, Loss: 0.46145892957673257\n",
      "Epoch 189, Loss: 0.464304908222076\n",
      "Epoch 190, Loss: 0.4606605441278432\n",
      "Epoch 191, Loss: 0.4601386308130386\n",
      "Epoch 192, Loss: 0.46193430495315263\n",
      "Epoch 193, Loss: 0.45977696912799226\n",
      "Epoch 194, Loss: 0.4597173803227511\n",
      "Epoch 195, Loss: 0.46096589922299847\n",
      "Epoch 196, Loss: 0.4592170612502741\n",
      "Epoch 197, Loss: 0.45864347419013624\n",
      "Epoch 198, Loss: 0.4594650511157848\n",
      "Epoch 199, Loss: 0.45924705942486493\n",
      "Epoch 200, Loss: 0.4584931199827434\n",
      "Epoch 201, Loss: 0.4578552480015059\n",
      "Epoch 202, Loss: 0.458251953125\n",
      "Epoch 203, Loss: 0.4586848316367108\n",
      "Epoch 204, Loss: 0.4577025349799131\n",
      "Epoch 205, Loss: 0.45707273702530193\n",
      "Epoch 206, Loss: 0.4571336158814618\n",
      "Epoch 207, Loss: 0.45746758002181026\n",
      "Epoch 208, Loss: 0.4580334166179955\n",
      "Epoch 209, Loss: 0.457551138813026\n",
      "Epoch 210, Loss: 0.4572461918817771\n",
      "Epoch 211, Loss: 0.45629613394883295\n",
      "Epoch 212, Loss: 0.4557945440718634\n",
      "Epoch 213, Loss: 0.455843155547024\n",
      "Epoch 214, Loss: 0.456219547378948\n",
      "Epoch 215, Loss: 0.4577116995236092\n",
      "Epoch 216, Loss: 0.4589176697652806\n",
      "Epoch 217, Loss: 0.46105034303609993\n",
      "Epoch 218, Loss: 0.4567326442394303\n",
      "Epoch 219, Loss: 0.45493112899589366\n",
      "Epoch 220, Loss: 0.45527199475335134\n",
      "Epoch 221, Loss: 0.45623630281032773\n",
      "Epoch 222, Loss: 0.45802054967026307\n",
      "Epoch 223, Loss: 0.45677224995978094\n",
      "Epoch 224, Loss: 0.45565914342704716\n",
      "Epoch 225, Loss: 0.45401674524485375\n",
      "Epoch 226, Loss: 0.4535057342791329\n",
      "Epoch 227, Loss: 0.45416164205845616\n",
      "Epoch 228, Loss: 0.45450513400796655\n",
      "Epoch 229, Loss: 0.4548238117351997\n",
      "Epoch 230, Loss: 0.4537990916311975\n",
      "Epoch 231, Loss: 0.4532521496977959\n",
      "Epoch 232, Loss: 0.4526084553640119\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23736489039892594\n",
      "Valence RMSE: 0.2465415060664229\n",
      "Arousal RMSE: 0.22781893726090505\n",
      "Test R^2 score: tensor([0.3505, 0.6172], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.48385320149369726\n",
      "Num of epochs: 233\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Epoch 121, Loss: 0.4816228295571127\n",
      "Epoch 122, Loss: 0.4807618495484584\n",
      "Epoch 123, Loss: 0.48102030846138955\n",
      "Epoch 124, Loss: 0.4810176288364116\n",
      "Epoch 125, Loss: 0.48014338261661127\n",
      "Epoch 126, Loss: 0.47910314809270116\n",
      "Epoch 127, Loss: 0.47880795949683447\n",
      "Epoch 128, Loss: 0.47876177315142754\n",
      "Epoch 129, Loss: 0.47819597116449997\n",
      "Epoch 130, Loss: 0.47743214539542544\n",
      "Epoch 131, Loss: 0.47658476230340185\n",
      "Epoch 132, Loss: 0.4759644680184978\n",
      "Epoch 133, Loss: 0.4755635187712421\n",
      "Epoch 134, Loss: 0.47538264196584307\n",
      "Epoch 135, Loss: 0.4753673450596914\n",
      "Epoch 136, Loss: 0.4753697744168057\n",
      "Epoch 137, Loss: 0.4755973109583155\n",
      "Epoch 138, Loss: 0.4767626826605241\n",
      "Epoch 139, Loss: 0.47683528222287225\n",
      "Epoch 140, Loss: 0.47579445468261566\n",
      "Epoch 141, Loss: 0.47284533919249566\n",
      "Epoch 142, Loss: 0.472809286012082\n",
      "Epoch 143, Loss: 0.47458738111232796\n",
      "Epoch 144, Loss: 0.4733380058918715\n",
      "Epoch 145, Loss: 0.4715229034484008\n",
      "Epoch 146, Loss: 0.47096730966267386\n",
      "Epoch 147, Loss: 0.47161469890467383\n",
      "Epoch 148, Loss: 0.47183531411129226\n",
      "Epoch 149, Loss: 0.470294933688076\n",
      "Epoch 150, Loss: 0.4697114144973765\n",
      "Epoch 151, Loss: 0.47018789053701304\n",
      "Epoch 152, Loss: 0.4700460953124117\n",
      "Epoch 153, Loss: 0.4693645740016508\n",
      "Epoch 154, Loss: 0.4684558104180481\n",
      "Epoch 155, Loss: 0.4680960384979934\n",
      "Epoch 156, Loss: 0.4682454254529793\n",
      "Epoch 157, Loss: 0.4682974379105488\n",
      "Epoch 158, Loss: 0.4681590010361841\n",
      "Epoch 159, Loss: 0.4673225124817593\n",
      "Epoch 160, Loss: 0.4666701428107949\n",
      "Epoch 161, Loss: 0.4663166784414499\n",
      "Epoch 162, Loss: 0.46624854535037\n",
      "Epoch 163, Loss: 0.4664611566834947\n",
      "Epoch 164, Loss: 0.46654532450127006\n",
      "Epoch 165, Loss: 0.46721845607201035\n",
      "Epoch 166, Loss: 0.4668208160006968\n",
      "Epoch 167, Loss: 0.466573749669782\n",
      "Epoch 168, Loss: 0.46506387643568103\n",
      "Epoch 169, Loss: 0.4641220516950567\n",
      "Epoch 170, Loss: 0.4644550172656552\n",
      "Epoch 171, Loss: 0.4654613047116713\n",
      "Epoch 172, Loss: 0.46880176576246846\n",
      "Epoch 173, Loss: 0.4685649824581709\n",
      "Epoch 174, Loss: 0.46606115996454733\n",
      "Epoch 175, Loss: 0.46328263242364126\n",
      "Epoch 176, Loss: 0.4660484187313888\n",
      "Epoch 177, Loss: 0.4667669789465077\n",
      "Epoch 178, Loss: 0.46222975993575777\n",
      "Epoch 179, Loss: 0.46412214801342555\n",
      "Epoch 180, Loss: 0.46867384291927394\n",
      "Epoch 181, Loss: 0.46316269230838986\n",
      "Epoch 182, Loss: 0.4619791093960296\n",
      "Epoch 183, Loss: 0.4650783107301587\n",
      "Epoch 184, Loss: 0.4617210775403838\n",
      "Epoch 185, Loss: 0.4621011464022392\n",
      "Epoch 186, Loss: 0.4641367239628427\n",
      "Epoch 187, Loss: 0.46071131054180564\n",
      "Epoch 188, Loss: 0.46145892957673257\n",
      "Epoch 189, Loss: 0.464304908222076\n",
      "Epoch 190, Loss: 0.4606605441278432\n",
      "Epoch 191, Loss: 0.4601386308130386\n",
      "Epoch 192, Loss: 0.46193430495315263\n",
      "Epoch 193, Loss: 0.45977696912799226\n",
      "Epoch 194, Loss: 0.4597173803227511\n",
      "Epoch 195, Loss: 0.46096589922299847\n",
      "Epoch 196, Loss: 0.4592170612502741\n",
      "Epoch 197, Loss: 0.45864347419013624\n",
      "Epoch 198, Loss: 0.4594650511157848\n",
      "Epoch 199, Loss: 0.45924705942486493\n",
      "Epoch 200, Loss: 0.4584931199827434\n",
      "Epoch 201, Loss: 0.4578552480015059\n",
      "Epoch 202, Loss: 0.458251953125\n",
      "Epoch 203, Loss: 0.4586848316367108\n",
      "Epoch 204, Loss: 0.4577025349799131\n",
      "Epoch 205, Loss: 0.45707273702530193\n",
      "Epoch 206, Loss: 0.4571336158814618\n",
      "Epoch 207, Loss: 0.45746758002181026\n",
      "Epoch 208, Loss: 0.4580334166179955\n",
      "Epoch 209, Loss: 0.457551138813026\n",
      "Epoch 210, Loss: 0.4572461918817771\n",
      "Epoch 211, Loss: 0.45629613394883295\n",
      "Epoch 212, Loss: 0.4557945440718634\n",
      "Epoch 213, Loss: 0.455843155547024\n",
      "Epoch 214, Loss: 0.456219547378948\n",
      "Epoch 215, Loss: 0.4577116995236092\n",
      "Epoch 216, Loss: 0.4589176697652806\n",
      "Epoch 217, Loss: 0.46105034303609993\n",
      "Epoch 218, Loss: 0.4567326442394303\n",
      "Epoch 219, Loss: 0.45493112899589366\n",
      "Epoch 220, Loss: 0.45527199475335134\n",
      "Epoch 221, Loss: 0.45623630281032773\n",
      "Epoch 222, Loss: 0.45802054967026307\n",
      "Epoch 223, Loss: 0.45677224995978094\n",
      "Epoch 224, Loss: 0.45565914342704716\n",
      "Epoch 225, Loss: 0.45401674524485375\n",
      "Epoch 226, Loss: 0.4535057342791329\n",
      "Epoch 227, Loss: 0.45416164205845616\n",
      "Epoch 228, Loss: 0.45450513400796655\n",
      "Epoch 229, Loss: 0.4548238117351997\n",
      "Epoch 230, Loss: 0.4537990916311975\n",
      "Epoch 231, Loss: 0.4532521496977959\n",
      "Epoch 232, Loss: 0.4526084553640119\n",
      "Epoch 233, Loss: 0.45228981460773116\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23702990404904098\n",
      "Valence RMSE: 0.24605587527288592\n",
      "Arousal RMSE: 0.22764634209819662\n",
      "Test R^2 score: tensor([0.3530, 0.6178], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.48542122992081876\n",
      "Num of epochs: 234\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Epoch 121, Loss: 0.4816228295571127\n",
      "Epoch 122, Loss: 0.4807618495484584\n",
      "Epoch 123, Loss: 0.48102030846138955\n",
      "Epoch 124, Loss: 0.4810176288364116\n",
      "Epoch 125, Loss: 0.48014338261661127\n",
      "Epoch 126, Loss: 0.47910314809270116\n",
      "Epoch 127, Loss: 0.47880795949683447\n",
      "Epoch 128, Loss: 0.47876177315142754\n",
      "Epoch 129, Loss: 0.47819597116449997\n",
      "Epoch 130, Loss: 0.47743214539542544\n",
      "Epoch 131, Loss: 0.47658476230340185\n",
      "Epoch 132, Loss: 0.4759644680184978\n",
      "Epoch 133, Loss: 0.4755635187712421\n",
      "Epoch 134, Loss: 0.47538264196584307\n",
      "Epoch 135, Loss: 0.4753673450596914\n",
      "Epoch 136, Loss: 0.4753697744168057\n",
      "Epoch 137, Loss: 0.4755973109583155\n",
      "Epoch 138, Loss: 0.4767626826605241\n",
      "Epoch 139, Loss: 0.47683528222287225\n",
      "Epoch 140, Loss: 0.47579445468261566\n",
      "Epoch 141, Loss: 0.47284533919249566\n",
      "Epoch 142, Loss: 0.472809286012082\n",
      "Epoch 143, Loss: 0.47458738111232796\n",
      "Epoch 144, Loss: 0.4733380058918715\n",
      "Epoch 145, Loss: 0.4715229034484008\n",
      "Epoch 146, Loss: 0.47096730966267386\n",
      "Epoch 147, Loss: 0.47161469890467383\n",
      "Epoch 148, Loss: 0.47183531411129226\n",
      "Epoch 149, Loss: 0.470294933688076\n",
      "Epoch 150, Loss: 0.4697114144973765\n",
      "Epoch 151, Loss: 0.47018789053701304\n",
      "Epoch 152, Loss: 0.4700460953124117\n",
      "Epoch 153, Loss: 0.4693645740016508\n",
      "Epoch 154, Loss: 0.4684558104180481\n",
      "Epoch 155, Loss: 0.4680960384979934\n",
      "Epoch 156, Loss: 0.4682454254529793\n",
      "Epoch 157, Loss: 0.4682974379105488\n",
      "Epoch 158, Loss: 0.4681590010361841\n",
      "Epoch 159, Loss: 0.4673225124817593\n",
      "Epoch 160, Loss: 0.4666701428107949\n",
      "Epoch 161, Loss: 0.4663166784414499\n",
      "Epoch 162, Loss: 0.46624854535037\n",
      "Epoch 163, Loss: 0.4664611566834947\n",
      "Epoch 164, Loss: 0.46654532450127006\n",
      "Epoch 165, Loss: 0.46721845607201035\n",
      "Epoch 166, Loss: 0.4668208160006968\n",
      "Epoch 167, Loss: 0.466573749669782\n",
      "Epoch 168, Loss: 0.46506387643568103\n",
      "Epoch 169, Loss: 0.4641220516950567\n",
      "Epoch 170, Loss: 0.4644550172656552\n",
      "Epoch 171, Loss: 0.4654613047116713\n",
      "Epoch 172, Loss: 0.46880176576246846\n",
      "Epoch 173, Loss: 0.4685649824581709\n",
      "Epoch 174, Loss: 0.46606115996454733\n",
      "Epoch 175, Loss: 0.46328263242364126\n",
      "Epoch 176, Loss: 0.4660484187313888\n",
      "Epoch 177, Loss: 0.4667669789465077\n",
      "Epoch 178, Loss: 0.46222975993575777\n",
      "Epoch 179, Loss: 0.46412214801342555\n",
      "Epoch 180, Loss: 0.46867384291927394\n",
      "Epoch 181, Loss: 0.46316269230838986\n",
      "Epoch 182, Loss: 0.4619791093960296\n",
      "Epoch 183, Loss: 0.4650783107301587\n",
      "Epoch 184, Loss: 0.4617210775403838\n",
      "Epoch 185, Loss: 0.4621011464022392\n",
      "Epoch 186, Loss: 0.4641367239628427\n",
      "Epoch 187, Loss: 0.46071131054180564\n",
      "Epoch 188, Loss: 0.46145892957673257\n",
      "Epoch 189, Loss: 0.464304908222076\n",
      "Epoch 190, Loss: 0.4606605441278432\n",
      "Epoch 191, Loss: 0.4601386308130386\n",
      "Epoch 192, Loss: 0.46193430495315263\n",
      "Epoch 193, Loss: 0.45977696912799226\n",
      "Epoch 194, Loss: 0.4597173803227511\n",
      "Epoch 195, Loss: 0.46096589922299847\n",
      "Epoch 196, Loss: 0.4592170612502741\n",
      "Epoch 197, Loss: 0.45864347419013624\n",
      "Epoch 198, Loss: 0.4594650511157848\n",
      "Epoch 199, Loss: 0.45924705942486493\n",
      "Epoch 200, Loss: 0.4584931199827434\n",
      "Epoch 201, Loss: 0.4578552480015059\n",
      "Epoch 202, Loss: 0.458251953125\n",
      "Epoch 203, Loss: 0.4586848316367108\n",
      "Epoch 204, Loss: 0.4577025349799131\n",
      "Epoch 205, Loss: 0.45707273702530193\n",
      "Epoch 206, Loss: 0.4571336158814618\n",
      "Epoch 207, Loss: 0.45746758002181026\n",
      "Epoch 208, Loss: 0.4580334166179955\n",
      "Epoch 209, Loss: 0.457551138813026\n",
      "Epoch 210, Loss: 0.4572461918817771\n",
      "Epoch 211, Loss: 0.45629613394883295\n",
      "Epoch 212, Loss: 0.4557945440718634\n",
      "Epoch 213, Loss: 0.455843155547024\n",
      "Epoch 214, Loss: 0.456219547378948\n",
      "Epoch 215, Loss: 0.4577116995236092\n",
      "Epoch 216, Loss: 0.4589176697652806\n",
      "Epoch 217, Loss: 0.46105034303609993\n",
      "Epoch 218, Loss: 0.4567326442394303\n",
      "Epoch 219, Loss: 0.45493112899589366\n",
      "Epoch 220, Loss: 0.45527199475335134\n",
      "Epoch 221, Loss: 0.45623630281032773\n",
      "Epoch 222, Loss: 0.45802054967026307\n",
      "Epoch 223, Loss: 0.45677224995978094\n",
      "Epoch 224, Loss: 0.45565914342704716\n",
      "Epoch 225, Loss: 0.45401674524485375\n",
      "Epoch 226, Loss: 0.4535057342791329\n",
      "Epoch 227, Loss: 0.45416164205845616\n",
      "Epoch 228, Loss: 0.45450513400796655\n",
      "Epoch 229, Loss: 0.4548238117351997\n",
      "Epoch 230, Loss: 0.4537990916311975\n",
      "Epoch 231, Loss: 0.4532521496977959\n",
      "Epoch 232, Loss: 0.4526084553640119\n",
      "Epoch 233, Loss: 0.45228981460773116\n",
      "Epoch 234, Loss: 0.4520949132230242\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2365416883576327\n",
      "Valence RMSE: 0.2448777059019246\n",
      "Arousal RMSE: 0.2279009649263675\n",
      "Test R^2 score: tensor([0.3592, 0.6170], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4880839078159977\n",
      "Num of epochs: 235\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Epoch 121, Loss: 0.4816228295571127\n",
      "Epoch 122, Loss: 0.4807618495484584\n",
      "Epoch 123, Loss: 0.48102030846138955\n",
      "Epoch 124, Loss: 0.4810176288364116\n",
      "Epoch 125, Loss: 0.48014338261661127\n",
      "Epoch 126, Loss: 0.47910314809270116\n",
      "Epoch 127, Loss: 0.47880795949683447\n",
      "Epoch 128, Loss: 0.47876177315142754\n",
      "Epoch 129, Loss: 0.47819597116449997\n",
      "Epoch 130, Loss: 0.47743214539542544\n",
      "Epoch 131, Loss: 0.47658476230340185\n",
      "Epoch 132, Loss: 0.4759644680184978\n",
      "Epoch 133, Loss: 0.4755635187712421\n",
      "Epoch 134, Loss: 0.47538264196584307\n",
      "Epoch 135, Loss: 0.4753673450596914\n",
      "Epoch 136, Loss: 0.4753697744168057\n",
      "Epoch 137, Loss: 0.4755973109583155\n",
      "Epoch 138, Loss: 0.4767626826605241\n",
      "Epoch 139, Loss: 0.47683528222287225\n",
      "Epoch 140, Loss: 0.47579445468261566\n",
      "Epoch 141, Loss: 0.47284533919249566\n",
      "Epoch 142, Loss: 0.472809286012082\n",
      "Epoch 143, Loss: 0.47458738111232796\n",
      "Epoch 144, Loss: 0.4733380058918715\n",
      "Epoch 145, Loss: 0.4715229034484008\n",
      "Epoch 146, Loss: 0.47096730966267386\n",
      "Epoch 147, Loss: 0.47161469890467383\n",
      "Epoch 148, Loss: 0.47183531411129226\n",
      "Epoch 149, Loss: 0.470294933688076\n",
      "Epoch 150, Loss: 0.4697114144973765\n",
      "Epoch 151, Loss: 0.47018789053701304\n",
      "Epoch 152, Loss: 0.4700460953124117\n",
      "Epoch 153, Loss: 0.4693645740016508\n",
      "Epoch 154, Loss: 0.4684558104180481\n",
      "Epoch 155, Loss: 0.4680960384979934\n",
      "Epoch 156, Loss: 0.4682454254529793\n",
      "Epoch 157, Loss: 0.4682974379105488\n",
      "Epoch 158, Loss: 0.4681590010361841\n",
      "Epoch 159, Loss: 0.4673225124817593\n",
      "Epoch 160, Loss: 0.4666701428107949\n",
      "Epoch 161, Loss: 0.4663166784414499\n",
      "Epoch 162, Loss: 0.46624854535037\n",
      "Epoch 163, Loss: 0.4664611566834947\n",
      "Epoch 164, Loss: 0.46654532450127006\n",
      "Epoch 165, Loss: 0.46721845607201035\n",
      "Epoch 166, Loss: 0.4668208160006968\n",
      "Epoch 167, Loss: 0.466573749669782\n",
      "Epoch 168, Loss: 0.46506387643568103\n",
      "Epoch 169, Loss: 0.4641220516950567\n",
      "Epoch 170, Loss: 0.4644550172656552\n",
      "Epoch 171, Loss: 0.4654613047116713\n",
      "Epoch 172, Loss: 0.46880176576246846\n",
      "Epoch 173, Loss: 0.4685649824581709\n",
      "Epoch 174, Loss: 0.46606115996454733\n",
      "Epoch 175, Loss: 0.46328263242364126\n",
      "Epoch 176, Loss: 0.4660484187313888\n",
      "Epoch 177, Loss: 0.4667669789465077\n",
      "Epoch 178, Loss: 0.46222975993575777\n",
      "Epoch 179, Loss: 0.46412214801342555\n",
      "Epoch 180, Loss: 0.46867384291927394\n",
      "Epoch 181, Loss: 0.46316269230838986\n",
      "Epoch 182, Loss: 0.4619791093960296\n",
      "Epoch 183, Loss: 0.4650783107301587\n",
      "Epoch 184, Loss: 0.4617210775403838\n",
      "Epoch 185, Loss: 0.4621011464022392\n",
      "Epoch 186, Loss: 0.4641367239628427\n",
      "Epoch 187, Loss: 0.46071131054180564\n",
      "Epoch 188, Loss: 0.46145892957673257\n",
      "Epoch 189, Loss: 0.464304908222076\n",
      "Epoch 190, Loss: 0.4606605441278432\n",
      "Epoch 191, Loss: 0.4601386308130386\n",
      "Epoch 192, Loss: 0.46193430495315263\n",
      "Epoch 193, Loss: 0.45977696912799226\n",
      "Epoch 194, Loss: 0.4597173803227511\n",
      "Epoch 195, Loss: 0.46096589922299847\n",
      "Epoch 196, Loss: 0.4592170612502741\n",
      "Epoch 197, Loss: 0.45864347419013624\n",
      "Epoch 198, Loss: 0.4594650511157848\n",
      "Epoch 199, Loss: 0.45924705942486493\n",
      "Epoch 200, Loss: 0.4584931199827434\n",
      "Epoch 201, Loss: 0.4578552480015059\n",
      "Epoch 202, Loss: 0.458251953125\n",
      "Epoch 203, Loss: 0.4586848316367108\n",
      "Epoch 204, Loss: 0.4577025349799131\n",
      "Epoch 205, Loss: 0.45707273702530193\n",
      "Epoch 206, Loss: 0.4571336158814618\n",
      "Epoch 207, Loss: 0.45746758002181026\n",
      "Epoch 208, Loss: 0.4580334166179955\n",
      "Epoch 209, Loss: 0.457551138813026\n",
      "Epoch 210, Loss: 0.4572461918817771\n",
      "Epoch 211, Loss: 0.45629613394883295\n",
      "Epoch 212, Loss: 0.4557945440718634\n",
      "Epoch 213, Loss: 0.455843155547024\n",
      "Epoch 214, Loss: 0.456219547378948\n",
      "Epoch 215, Loss: 0.4577116995236092\n",
      "Epoch 216, Loss: 0.4589176697652806\n",
      "Epoch 217, Loss: 0.46105034303609993\n",
      "Epoch 218, Loss: 0.4567326442394303\n",
      "Epoch 219, Loss: 0.45493112899589366\n",
      "Epoch 220, Loss: 0.45527199475335134\n",
      "Epoch 221, Loss: 0.45623630281032773\n",
      "Epoch 222, Loss: 0.45802054967026307\n",
      "Epoch 223, Loss: 0.45677224995978094\n",
      "Epoch 224, Loss: 0.45565914342704716\n",
      "Epoch 225, Loss: 0.45401674524485375\n",
      "Epoch 226, Loss: 0.4535057342791329\n",
      "Epoch 227, Loss: 0.45416164205845616\n",
      "Epoch 228, Loss: 0.45450513400796655\n",
      "Epoch 229, Loss: 0.4548238117351997\n",
      "Epoch 230, Loss: 0.4537990916311975\n",
      "Epoch 231, Loss: 0.4532521496977959\n",
      "Epoch 232, Loss: 0.4526084553640119\n",
      "Epoch 233, Loss: 0.45228981460773116\n",
      "Epoch 234, Loss: 0.4520949132230242\n",
      "Epoch 235, Loss: 0.45205797975469736\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2375640247322253\n",
      "Valence RMSE: 0.24763087062902986\n",
      "Arousal RMSE: 0.22705127968248803\n",
      "Test R^2 score: tensor([0.3447, 0.6198], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4822645019500271\n",
      "Num of epochs: 236\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Epoch 121, Loss: 0.4816228295571127\n",
      "Epoch 122, Loss: 0.4807618495484584\n",
      "Epoch 123, Loss: 0.48102030846138955\n",
      "Epoch 124, Loss: 0.4810176288364116\n",
      "Epoch 125, Loss: 0.48014338261661127\n",
      "Epoch 126, Loss: 0.47910314809270116\n",
      "Epoch 127, Loss: 0.47880795949683447\n",
      "Epoch 128, Loss: 0.47876177315142754\n",
      "Epoch 129, Loss: 0.47819597116449997\n",
      "Epoch 130, Loss: 0.47743214539542544\n",
      "Epoch 131, Loss: 0.47658476230340185\n",
      "Epoch 132, Loss: 0.4759644680184978\n",
      "Epoch 133, Loss: 0.4755635187712421\n",
      "Epoch 134, Loss: 0.47538264196584307\n",
      "Epoch 135, Loss: 0.4753673450596914\n",
      "Epoch 136, Loss: 0.4753697744168057\n",
      "Epoch 137, Loss: 0.4755973109583155\n",
      "Epoch 138, Loss: 0.4767626826605241\n",
      "Epoch 139, Loss: 0.47683528222287225\n",
      "Epoch 140, Loss: 0.47579445468261566\n",
      "Epoch 141, Loss: 0.47284533919249566\n",
      "Epoch 142, Loss: 0.472809286012082\n",
      "Epoch 143, Loss: 0.47458738111232796\n",
      "Epoch 144, Loss: 0.4733380058918715\n",
      "Epoch 145, Loss: 0.4715229034484008\n",
      "Epoch 146, Loss: 0.47096730966267386\n",
      "Epoch 147, Loss: 0.47161469890467383\n",
      "Epoch 148, Loss: 0.47183531411129226\n",
      "Epoch 149, Loss: 0.470294933688076\n",
      "Epoch 150, Loss: 0.4697114144973765\n",
      "Epoch 151, Loss: 0.47018789053701304\n",
      "Epoch 152, Loss: 0.4700460953124117\n",
      "Epoch 153, Loss: 0.4693645740016508\n",
      "Epoch 154, Loss: 0.4684558104180481\n",
      "Epoch 155, Loss: 0.4680960384979934\n",
      "Epoch 156, Loss: 0.4682454254529793\n",
      "Epoch 157, Loss: 0.4682974379105488\n",
      "Epoch 158, Loss: 0.4681590010361841\n",
      "Epoch 159, Loss: 0.4673225124817593\n",
      "Epoch 160, Loss: 0.4666701428107949\n",
      "Epoch 161, Loss: 0.4663166784414499\n",
      "Epoch 162, Loss: 0.46624854535037\n",
      "Epoch 163, Loss: 0.4664611566834947\n",
      "Epoch 164, Loss: 0.46654532450127006\n",
      "Epoch 165, Loss: 0.46721845607201035\n",
      "Epoch 166, Loss: 0.4668208160006968\n",
      "Epoch 167, Loss: 0.466573749669782\n",
      "Epoch 168, Loss: 0.46506387643568103\n",
      "Epoch 169, Loss: 0.4641220516950567\n",
      "Epoch 170, Loss: 0.4644550172656552\n",
      "Epoch 171, Loss: 0.4654613047116713\n",
      "Epoch 172, Loss: 0.46880176576246846\n",
      "Epoch 173, Loss: 0.4685649824581709\n",
      "Epoch 174, Loss: 0.46606115996454733\n",
      "Epoch 175, Loss: 0.46328263242364126\n",
      "Epoch 176, Loss: 0.4660484187313888\n",
      "Epoch 177, Loss: 0.4667669789465077\n",
      "Epoch 178, Loss: 0.46222975993575777\n",
      "Epoch 179, Loss: 0.46412214801342555\n",
      "Epoch 180, Loss: 0.46867384291927394\n",
      "Epoch 181, Loss: 0.46316269230838986\n",
      "Epoch 182, Loss: 0.4619791093960296\n",
      "Epoch 183, Loss: 0.4650783107301587\n",
      "Epoch 184, Loss: 0.4617210775403838\n",
      "Epoch 185, Loss: 0.4621011464022392\n",
      "Epoch 186, Loss: 0.4641367239628427\n",
      "Epoch 187, Loss: 0.46071131054180564\n",
      "Epoch 188, Loss: 0.46145892957673257\n",
      "Epoch 189, Loss: 0.464304908222076\n",
      "Epoch 190, Loss: 0.4606605441278432\n",
      "Epoch 191, Loss: 0.4601386308130386\n",
      "Epoch 192, Loss: 0.46193430495315263\n",
      "Epoch 193, Loss: 0.45977696912799226\n",
      "Epoch 194, Loss: 0.4597173803227511\n",
      "Epoch 195, Loss: 0.46096589922299847\n",
      "Epoch 196, Loss: 0.4592170612502741\n",
      "Epoch 197, Loss: 0.45864347419013624\n",
      "Epoch 198, Loss: 0.4594650511157848\n",
      "Epoch 199, Loss: 0.45924705942486493\n",
      "Epoch 200, Loss: 0.4584931199827434\n",
      "Epoch 201, Loss: 0.4578552480015059\n",
      "Epoch 202, Loss: 0.458251953125\n",
      "Epoch 203, Loss: 0.4586848316367108\n",
      "Epoch 204, Loss: 0.4577025349799131\n",
      "Epoch 205, Loss: 0.45707273702530193\n",
      "Epoch 206, Loss: 0.4571336158814618\n",
      "Epoch 207, Loss: 0.45746758002181026\n",
      "Epoch 208, Loss: 0.4580334166179955\n",
      "Epoch 209, Loss: 0.457551138813026\n",
      "Epoch 210, Loss: 0.4572461918817771\n",
      "Epoch 211, Loss: 0.45629613394883295\n",
      "Epoch 212, Loss: 0.4557945440718634\n",
      "Epoch 213, Loss: 0.455843155547024\n",
      "Epoch 214, Loss: 0.456219547378948\n",
      "Epoch 215, Loss: 0.4577116995236092\n",
      "Epoch 216, Loss: 0.4589176697652806\n",
      "Epoch 217, Loss: 0.46105034303609993\n",
      "Epoch 218, Loss: 0.4567326442394303\n",
      "Epoch 219, Loss: 0.45493112899589366\n",
      "Epoch 220, Loss: 0.45527199475335134\n",
      "Epoch 221, Loss: 0.45623630281032773\n",
      "Epoch 222, Loss: 0.45802054967026307\n",
      "Epoch 223, Loss: 0.45677224995978094\n",
      "Epoch 224, Loss: 0.45565914342704716\n",
      "Epoch 225, Loss: 0.45401674524485375\n",
      "Epoch 226, Loss: 0.4535057342791329\n",
      "Epoch 227, Loss: 0.45416164205845616\n",
      "Epoch 228, Loss: 0.45450513400796655\n",
      "Epoch 229, Loss: 0.4548238117351997\n",
      "Epoch 230, Loss: 0.4537990916311975\n",
      "Epoch 231, Loss: 0.4532521496977959\n",
      "Epoch 232, Loss: 0.4526084553640119\n",
      "Epoch 233, Loss: 0.45228981460773116\n",
      "Epoch 234, Loss: 0.4520949132230242\n",
      "Epoch 235, Loss: 0.45205797975469736\n",
      "Epoch 236, Loss: 0.45230557901617613\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23668703042679118\n",
      "Valence RMSE: 0.24443640668297975\n",
      "Arousal RMSE: 0.22867519286625396\n",
      "Test R^2 score: tensor([0.3615, 0.6143], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4879341379333784\n",
      "Num of epochs: 237\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Epoch 121, Loss: 0.4816228295571127\n",
      "Epoch 122, Loss: 0.4807618495484584\n",
      "Epoch 123, Loss: 0.48102030846138955\n",
      "Epoch 124, Loss: 0.4810176288364116\n",
      "Epoch 125, Loss: 0.48014338261661127\n",
      "Epoch 126, Loss: 0.47910314809270116\n",
      "Epoch 127, Loss: 0.47880795949683447\n",
      "Epoch 128, Loss: 0.47876177315142754\n",
      "Epoch 129, Loss: 0.47819597116449997\n",
      "Epoch 130, Loss: 0.47743214539542544\n",
      "Epoch 131, Loss: 0.47658476230340185\n",
      "Epoch 132, Loss: 0.4759644680184978\n",
      "Epoch 133, Loss: 0.4755635187712421\n",
      "Epoch 134, Loss: 0.47538264196584307\n",
      "Epoch 135, Loss: 0.4753673450596914\n",
      "Epoch 136, Loss: 0.4753697744168057\n",
      "Epoch 137, Loss: 0.4755973109583155\n",
      "Epoch 138, Loss: 0.4767626826605241\n",
      "Epoch 139, Loss: 0.47683528222287225\n",
      "Epoch 140, Loss: 0.47579445468261566\n",
      "Epoch 141, Loss: 0.47284533919249566\n",
      "Epoch 142, Loss: 0.472809286012082\n",
      "Epoch 143, Loss: 0.47458738111232796\n",
      "Epoch 144, Loss: 0.4733380058918715\n",
      "Epoch 145, Loss: 0.4715229034484008\n",
      "Epoch 146, Loss: 0.47096730966267386\n",
      "Epoch 147, Loss: 0.47161469890467383\n",
      "Epoch 148, Loss: 0.47183531411129226\n",
      "Epoch 149, Loss: 0.470294933688076\n",
      "Epoch 150, Loss: 0.4697114144973765\n",
      "Epoch 151, Loss: 0.47018789053701304\n",
      "Epoch 152, Loss: 0.4700460953124117\n",
      "Epoch 153, Loss: 0.4693645740016508\n",
      "Epoch 154, Loss: 0.4684558104180481\n",
      "Epoch 155, Loss: 0.4680960384979934\n",
      "Epoch 156, Loss: 0.4682454254529793\n",
      "Epoch 157, Loss: 0.4682974379105488\n",
      "Epoch 158, Loss: 0.4681590010361841\n",
      "Epoch 159, Loss: 0.4673225124817593\n",
      "Epoch 160, Loss: 0.4666701428107949\n",
      "Epoch 161, Loss: 0.4663166784414499\n",
      "Epoch 162, Loss: 0.46624854535037\n",
      "Epoch 163, Loss: 0.4664611566834947\n",
      "Epoch 164, Loss: 0.46654532450127006\n",
      "Epoch 165, Loss: 0.46721845607201035\n",
      "Epoch 166, Loss: 0.4668208160006968\n",
      "Epoch 167, Loss: 0.466573749669782\n",
      "Epoch 168, Loss: 0.46506387643568103\n",
      "Epoch 169, Loss: 0.4641220516950567\n",
      "Epoch 170, Loss: 0.4644550172656552\n",
      "Epoch 171, Loss: 0.4654613047116713\n",
      "Epoch 172, Loss: 0.46880176576246846\n",
      "Epoch 173, Loss: 0.4685649824581709\n",
      "Epoch 174, Loss: 0.46606115996454733\n",
      "Epoch 175, Loss: 0.46328263242364126\n",
      "Epoch 176, Loss: 0.4660484187313888\n",
      "Epoch 177, Loss: 0.4667669789465077\n",
      "Epoch 178, Loss: 0.46222975993575777\n",
      "Epoch 179, Loss: 0.46412214801342555\n",
      "Epoch 180, Loss: 0.46867384291927394\n",
      "Epoch 181, Loss: 0.46316269230838986\n",
      "Epoch 182, Loss: 0.4619791093960296\n",
      "Epoch 183, Loss: 0.4650783107301587\n",
      "Epoch 184, Loss: 0.4617210775403838\n",
      "Epoch 185, Loss: 0.4621011464022392\n",
      "Epoch 186, Loss: 0.4641367239628427\n",
      "Epoch 187, Loss: 0.46071131054180564\n",
      "Epoch 188, Loss: 0.46145892957673257\n",
      "Epoch 189, Loss: 0.464304908222076\n",
      "Epoch 190, Loss: 0.4606605441278432\n",
      "Epoch 191, Loss: 0.4601386308130386\n",
      "Epoch 192, Loss: 0.46193430495315263\n",
      "Epoch 193, Loss: 0.45977696912799226\n",
      "Epoch 194, Loss: 0.4597173803227511\n",
      "Epoch 195, Loss: 0.46096589922299847\n",
      "Epoch 196, Loss: 0.4592170612502741\n",
      "Epoch 197, Loss: 0.45864347419013624\n",
      "Epoch 198, Loss: 0.4594650511157848\n",
      "Epoch 199, Loss: 0.45924705942486493\n",
      "Epoch 200, Loss: 0.4584931199827434\n",
      "Epoch 201, Loss: 0.4578552480015059\n",
      "Epoch 202, Loss: 0.458251953125\n",
      "Epoch 203, Loss: 0.4586848316367108\n",
      "Epoch 204, Loss: 0.4577025349799131\n",
      "Epoch 205, Loss: 0.45707273702530193\n",
      "Epoch 206, Loss: 0.4571336158814618\n",
      "Epoch 207, Loss: 0.45746758002181026\n",
      "Epoch 208, Loss: 0.4580334166179955\n",
      "Epoch 209, Loss: 0.457551138813026\n",
      "Epoch 210, Loss: 0.4572461918817771\n",
      "Epoch 211, Loss: 0.45629613394883295\n",
      "Epoch 212, Loss: 0.4557945440718634\n",
      "Epoch 213, Loss: 0.455843155547024\n",
      "Epoch 214, Loss: 0.456219547378948\n",
      "Epoch 215, Loss: 0.4577116995236092\n",
      "Epoch 216, Loss: 0.4589176697652806\n",
      "Epoch 217, Loss: 0.46105034303609993\n",
      "Epoch 218, Loss: 0.4567326442394303\n",
      "Epoch 219, Loss: 0.45493112899589366\n",
      "Epoch 220, Loss: 0.45527199475335134\n",
      "Epoch 221, Loss: 0.45623630281032773\n",
      "Epoch 222, Loss: 0.45802054967026307\n",
      "Epoch 223, Loss: 0.45677224995978094\n",
      "Epoch 224, Loss: 0.45565914342704716\n",
      "Epoch 225, Loss: 0.45401674524485375\n",
      "Epoch 226, Loss: 0.4535057342791329\n",
      "Epoch 227, Loss: 0.45416164205845616\n",
      "Epoch 228, Loss: 0.45450513400796655\n",
      "Epoch 229, Loss: 0.4548238117351997\n",
      "Epoch 230, Loss: 0.4537990916311975\n",
      "Epoch 231, Loss: 0.4532521496977959\n",
      "Epoch 232, Loss: 0.4526084553640119\n",
      "Epoch 233, Loss: 0.45228981460773116\n",
      "Epoch 234, Loss: 0.4520949132230242\n",
      "Epoch 235, Loss: 0.45205797975469736\n",
      "Epoch 236, Loss: 0.45230557901617613\n",
      "Epoch 237, Loss: 0.4528345953247846\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23931225110724805\n",
      "Valence RMSE: 0.25037359831883493\n",
      "Arousal RMSE: 0.22771422512639783\n",
      "Test R^2 score: tensor([0.3301, 0.6176], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4738548483915471\n",
      "Num of epochs: 238\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Epoch 121, Loss: 0.4816228295571127\n",
      "Epoch 122, Loss: 0.4807618495484584\n",
      "Epoch 123, Loss: 0.48102030846138955\n",
      "Epoch 124, Loss: 0.4810176288364116\n",
      "Epoch 125, Loss: 0.48014338261661127\n",
      "Epoch 126, Loss: 0.47910314809270116\n",
      "Epoch 127, Loss: 0.47880795949683447\n",
      "Epoch 128, Loss: 0.47876177315142754\n",
      "Epoch 129, Loss: 0.47819597116449997\n",
      "Epoch 130, Loss: 0.47743214539542544\n",
      "Epoch 131, Loss: 0.47658476230340185\n",
      "Epoch 132, Loss: 0.4759644680184978\n",
      "Epoch 133, Loss: 0.4755635187712421\n",
      "Epoch 134, Loss: 0.47538264196584307\n",
      "Epoch 135, Loss: 0.4753673450596914\n",
      "Epoch 136, Loss: 0.4753697744168057\n",
      "Epoch 137, Loss: 0.4755973109583155\n",
      "Epoch 138, Loss: 0.4767626826605241\n",
      "Epoch 139, Loss: 0.47683528222287225\n",
      "Epoch 140, Loss: 0.47579445468261566\n",
      "Epoch 141, Loss: 0.47284533919249566\n",
      "Epoch 142, Loss: 0.472809286012082\n",
      "Epoch 143, Loss: 0.47458738111232796\n",
      "Epoch 144, Loss: 0.4733380058918715\n",
      "Epoch 145, Loss: 0.4715229034484008\n",
      "Epoch 146, Loss: 0.47096730966267386\n",
      "Epoch 147, Loss: 0.47161469890467383\n",
      "Epoch 148, Loss: 0.47183531411129226\n",
      "Epoch 149, Loss: 0.470294933688076\n",
      "Epoch 150, Loss: 0.4697114144973765\n",
      "Epoch 151, Loss: 0.47018789053701304\n",
      "Epoch 152, Loss: 0.4700460953124117\n",
      "Epoch 153, Loss: 0.4693645740016508\n",
      "Epoch 154, Loss: 0.4684558104180481\n",
      "Epoch 155, Loss: 0.4680960384979934\n",
      "Epoch 156, Loss: 0.4682454254529793\n",
      "Epoch 157, Loss: 0.4682974379105488\n",
      "Epoch 158, Loss: 0.4681590010361841\n",
      "Epoch 159, Loss: 0.4673225124817593\n",
      "Epoch 160, Loss: 0.4666701428107949\n",
      "Epoch 161, Loss: 0.4663166784414499\n",
      "Epoch 162, Loss: 0.46624854535037\n",
      "Epoch 163, Loss: 0.4664611566834947\n",
      "Epoch 164, Loss: 0.46654532450127006\n",
      "Epoch 165, Loss: 0.46721845607201035\n",
      "Epoch 166, Loss: 0.4668208160006968\n",
      "Epoch 167, Loss: 0.466573749669782\n",
      "Epoch 168, Loss: 0.46506387643568103\n",
      "Epoch 169, Loss: 0.4641220516950567\n",
      "Epoch 170, Loss: 0.4644550172656552\n",
      "Epoch 171, Loss: 0.4654613047116713\n",
      "Epoch 172, Loss: 0.46880176576246846\n",
      "Epoch 173, Loss: 0.4685649824581709\n",
      "Epoch 174, Loss: 0.46606115996454733\n",
      "Epoch 175, Loss: 0.46328263242364126\n",
      "Epoch 176, Loss: 0.4660484187313888\n",
      "Epoch 177, Loss: 0.4667669789465077\n",
      "Epoch 178, Loss: 0.46222975993575777\n",
      "Epoch 179, Loss: 0.46412214801342555\n",
      "Epoch 180, Loss: 0.46867384291927394\n",
      "Epoch 181, Loss: 0.46316269230838986\n",
      "Epoch 182, Loss: 0.4619791093960296\n",
      "Epoch 183, Loss: 0.4650783107301587\n",
      "Epoch 184, Loss: 0.4617210775403838\n",
      "Epoch 185, Loss: 0.4621011464022392\n",
      "Epoch 186, Loss: 0.4641367239628427\n",
      "Epoch 187, Loss: 0.46071131054180564\n",
      "Epoch 188, Loss: 0.46145892957673257\n",
      "Epoch 189, Loss: 0.464304908222076\n",
      "Epoch 190, Loss: 0.4606605441278432\n",
      "Epoch 191, Loss: 0.4601386308130386\n",
      "Epoch 192, Loss: 0.46193430495315263\n",
      "Epoch 193, Loss: 0.45977696912799226\n",
      "Epoch 194, Loss: 0.4597173803227511\n",
      "Epoch 195, Loss: 0.46096589922299847\n",
      "Epoch 196, Loss: 0.4592170612502741\n",
      "Epoch 197, Loss: 0.45864347419013624\n",
      "Epoch 198, Loss: 0.4594650511157848\n",
      "Epoch 199, Loss: 0.45924705942486493\n",
      "Epoch 200, Loss: 0.4584931199827434\n",
      "Epoch 201, Loss: 0.4578552480015059\n",
      "Epoch 202, Loss: 0.458251953125\n",
      "Epoch 203, Loss: 0.4586848316367108\n",
      "Epoch 204, Loss: 0.4577025349799131\n",
      "Epoch 205, Loss: 0.45707273702530193\n",
      "Epoch 206, Loss: 0.4571336158814618\n",
      "Epoch 207, Loss: 0.45746758002181026\n",
      "Epoch 208, Loss: 0.4580334166179955\n",
      "Epoch 209, Loss: 0.457551138813026\n",
      "Epoch 210, Loss: 0.4572461918817771\n",
      "Epoch 211, Loss: 0.45629613394883295\n",
      "Epoch 212, Loss: 0.4557945440718634\n",
      "Epoch 213, Loss: 0.455843155547024\n",
      "Epoch 214, Loss: 0.456219547378948\n",
      "Epoch 215, Loss: 0.4577116995236092\n",
      "Epoch 216, Loss: 0.4589176697652806\n",
      "Epoch 217, Loss: 0.46105034303609993\n",
      "Epoch 218, Loss: 0.4567326442394303\n",
      "Epoch 219, Loss: 0.45493112899589366\n",
      "Epoch 220, Loss: 0.45527199475335134\n",
      "Epoch 221, Loss: 0.45623630281032773\n",
      "Epoch 222, Loss: 0.45802054967026307\n",
      "Epoch 223, Loss: 0.45677224995978094\n",
      "Epoch 224, Loss: 0.45565914342704716\n",
      "Epoch 225, Loss: 0.45401674524485375\n",
      "Epoch 226, Loss: 0.4535057342791329\n",
      "Epoch 227, Loss: 0.45416164205845616\n",
      "Epoch 228, Loss: 0.45450513400796655\n",
      "Epoch 229, Loss: 0.4548238117351997\n",
      "Epoch 230, Loss: 0.4537990916311975\n",
      "Epoch 231, Loss: 0.4532521496977959\n",
      "Epoch 232, Loss: 0.4526084553640119\n",
      "Epoch 233, Loss: 0.45228981460773116\n",
      "Epoch 234, Loss: 0.4520949132230242\n",
      "Epoch 235, Loss: 0.45205797975469736\n",
      "Epoch 236, Loss: 0.45230557901617613\n",
      "Epoch 237, Loss: 0.4528345953247846\n",
      "Epoch 238, Loss: 0.4549386133976144\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23757448954045557\n",
      "Valence RMSE: 0.24342005885166834\n",
      "Arousal RMSE: 0.2315814135665174\n",
      "Test R^2 score: tensor([0.3668, 0.6045], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.48565096140352493\n",
      "Num of epochs: 239\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Epoch 121, Loss: 0.4816228295571127\n",
      "Epoch 122, Loss: 0.4807618495484584\n",
      "Epoch 123, Loss: 0.48102030846138955\n",
      "Epoch 124, Loss: 0.4810176288364116\n",
      "Epoch 125, Loss: 0.48014338261661127\n",
      "Epoch 126, Loss: 0.47910314809270116\n",
      "Epoch 127, Loss: 0.47880795949683447\n",
      "Epoch 128, Loss: 0.47876177315142754\n",
      "Epoch 129, Loss: 0.47819597116449997\n",
      "Epoch 130, Loss: 0.47743214539542544\n",
      "Epoch 131, Loss: 0.47658476230340185\n",
      "Epoch 132, Loss: 0.4759644680184978\n",
      "Epoch 133, Loss: 0.4755635187712421\n",
      "Epoch 134, Loss: 0.47538264196584307\n",
      "Epoch 135, Loss: 0.4753673450596914\n",
      "Epoch 136, Loss: 0.4753697744168057\n",
      "Epoch 137, Loss: 0.4755973109583155\n",
      "Epoch 138, Loss: 0.4767626826605241\n",
      "Epoch 139, Loss: 0.47683528222287225\n",
      "Epoch 140, Loss: 0.47579445468261566\n",
      "Epoch 141, Loss: 0.47284533919249566\n",
      "Epoch 142, Loss: 0.472809286012082\n",
      "Epoch 143, Loss: 0.47458738111232796\n",
      "Epoch 144, Loss: 0.4733380058918715\n",
      "Epoch 145, Loss: 0.4715229034484008\n",
      "Epoch 146, Loss: 0.47096730966267386\n",
      "Epoch 147, Loss: 0.47161469890467383\n",
      "Epoch 148, Loss: 0.47183531411129226\n",
      "Epoch 149, Loss: 0.470294933688076\n",
      "Epoch 150, Loss: 0.4697114144973765\n",
      "Epoch 151, Loss: 0.47018789053701304\n",
      "Epoch 152, Loss: 0.4700460953124117\n",
      "Epoch 153, Loss: 0.4693645740016508\n",
      "Epoch 154, Loss: 0.4684558104180481\n",
      "Epoch 155, Loss: 0.4680960384979934\n",
      "Epoch 156, Loss: 0.4682454254529793\n",
      "Epoch 157, Loss: 0.4682974379105488\n",
      "Epoch 158, Loss: 0.4681590010361841\n",
      "Epoch 159, Loss: 0.4673225124817593\n",
      "Epoch 160, Loss: 0.4666701428107949\n",
      "Epoch 161, Loss: 0.4663166784414499\n",
      "Epoch 162, Loss: 0.46624854535037\n",
      "Epoch 163, Loss: 0.4664611566834947\n",
      "Epoch 164, Loss: 0.46654532450127006\n",
      "Epoch 165, Loss: 0.46721845607201035\n",
      "Epoch 166, Loss: 0.4668208160006968\n",
      "Epoch 167, Loss: 0.466573749669782\n",
      "Epoch 168, Loss: 0.46506387643568103\n",
      "Epoch 169, Loss: 0.4641220516950567\n",
      "Epoch 170, Loss: 0.4644550172656552\n",
      "Epoch 171, Loss: 0.4654613047116713\n",
      "Epoch 172, Loss: 0.46880176576246846\n",
      "Epoch 173, Loss: 0.4685649824581709\n",
      "Epoch 174, Loss: 0.46606115996454733\n",
      "Epoch 175, Loss: 0.46328263242364126\n",
      "Epoch 176, Loss: 0.4660484187313888\n",
      "Epoch 177, Loss: 0.4667669789465077\n",
      "Epoch 178, Loss: 0.46222975993575777\n",
      "Epoch 179, Loss: 0.46412214801342555\n",
      "Epoch 180, Loss: 0.46867384291927394\n",
      "Epoch 181, Loss: 0.46316269230838986\n",
      "Epoch 182, Loss: 0.4619791093960296\n",
      "Epoch 183, Loss: 0.4650783107301587\n",
      "Epoch 184, Loss: 0.4617210775403838\n",
      "Epoch 185, Loss: 0.4621011464022392\n",
      "Epoch 186, Loss: 0.4641367239628427\n",
      "Epoch 187, Loss: 0.46071131054180564\n",
      "Epoch 188, Loss: 0.46145892957673257\n",
      "Epoch 189, Loss: 0.464304908222076\n",
      "Epoch 190, Loss: 0.4606605441278432\n",
      "Epoch 191, Loss: 0.4601386308130386\n",
      "Epoch 192, Loss: 0.46193430495315263\n",
      "Epoch 193, Loss: 0.45977696912799226\n",
      "Epoch 194, Loss: 0.4597173803227511\n",
      "Epoch 195, Loss: 0.46096589922299847\n",
      "Epoch 196, Loss: 0.4592170612502741\n",
      "Epoch 197, Loss: 0.45864347419013624\n",
      "Epoch 198, Loss: 0.4594650511157848\n",
      "Epoch 199, Loss: 0.45924705942486493\n",
      "Epoch 200, Loss: 0.4584931199827434\n",
      "Epoch 201, Loss: 0.4578552480015059\n",
      "Epoch 202, Loss: 0.458251953125\n",
      "Epoch 203, Loss: 0.4586848316367108\n",
      "Epoch 204, Loss: 0.4577025349799131\n",
      "Epoch 205, Loss: 0.45707273702530193\n",
      "Epoch 206, Loss: 0.4571336158814618\n",
      "Epoch 207, Loss: 0.45746758002181026\n",
      "Epoch 208, Loss: 0.4580334166179955\n",
      "Epoch 209, Loss: 0.457551138813026\n",
      "Epoch 210, Loss: 0.4572461918817771\n",
      "Epoch 211, Loss: 0.45629613394883295\n",
      "Epoch 212, Loss: 0.4557945440718634\n",
      "Epoch 213, Loss: 0.455843155547024\n",
      "Epoch 214, Loss: 0.456219547378948\n",
      "Epoch 215, Loss: 0.4577116995236092\n",
      "Epoch 216, Loss: 0.4589176697652806\n",
      "Epoch 217, Loss: 0.46105034303609993\n",
      "Epoch 218, Loss: 0.4567326442394303\n",
      "Epoch 219, Loss: 0.45493112899589366\n",
      "Epoch 220, Loss: 0.45527199475335134\n",
      "Epoch 221, Loss: 0.45623630281032773\n",
      "Epoch 222, Loss: 0.45802054967026307\n",
      "Epoch 223, Loss: 0.45677224995978094\n",
      "Epoch 224, Loss: 0.45565914342704716\n",
      "Epoch 225, Loss: 0.45401674524485375\n",
      "Epoch 226, Loss: 0.4535057342791329\n",
      "Epoch 227, Loss: 0.45416164205845616\n",
      "Epoch 228, Loss: 0.45450513400796655\n",
      "Epoch 229, Loss: 0.4548238117351997\n",
      "Epoch 230, Loss: 0.4537990916311975\n",
      "Epoch 231, Loss: 0.4532521496977959\n",
      "Epoch 232, Loss: 0.4526084553640119\n",
      "Epoch 233, Loss: 0.45228981460773116\n",
      "Epoch 234, Loss: 0.4520949132230242\n",
      "Epoch 235, Loss: 0.45205797975469736\n",
      "Epoch 236, Loss: 0.45230557901617613\n",
      "Epoch 237, Loss: 0.4528345953247846\n",
      "Epoch 238, Loss: 0.4549386133976144\n",
      "Epoch 239, Loss: 0.4564022887834084\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.24067127863804422\n",
      "Valence RMSE: 0.2517829284963067\n",
      "Arousal RMSE: 0.2290211467100057\n",
      "Test R^2 score: tensor([0.3226, 0.6132], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4678724735212119\n",
      "Num of epochs: 240\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Epoch 121, Loss: 0.4816228295571127\n",
      "Epoch 122, Loss: 0.4807618495484584\n",
      "Epoch 123, Loss: 0.48102030846138955\n",
      "Epoch 124, Loss: 0.4810176288364116\n",
      "Epoch 125, Loss: 0.48014338261661127\n",
      "Epoch 126, Loss: 0.47910314809270116\n",
      "Epoch 127, Loss: 0.47880795949683447\n",
      "Epoch 128, Loss: 0.47876177315142754\n",
      "Epoch 129, Loss: 0.47819597116449997\n",
      "Epoch 130, Loss: 0.47743214539542544\n",
      "Epoch 131, Loss: 0.47658476230340185\n",
      "Epoch 132, Loss: 0.4759644680184978\n",
      "Epoch 133, Loss: 0.4755635187712421\n",
      "Epoch 134, Loss: 0.47538264196584307\n",
      "Epoch 135, Loss: 0.4753673450596914\n",
      "Epoch 136, Loss: 0.4753697744168057\n",
      "Epoch 137, Loss: 0.4755973109583155\n",
      "Epoch 138, Loss: 0.4767626826605241\n",
      "Epoch 139, Loss: 0.47683528222287225\n",
      "Epoch 140, Loss: 0.47579445468261566\n",
      "Epoch 141, Loss: 0.47284533919249566\n",
      "Epoch 142, Loss: 0.472809286012082\n",
      "Epoch 143, Loss: 0.47458738111232796\n",
      "Epoch 144, Loss: 0.4733380058918715\n",
      "Epoch 145, Loss: 0.4715229034484008\n",
      "Epoch 146, Loss: 0.47096730966267386\n",
      "Epoch 147, Loss: 0.47161469890467383\n",
      "Epoch 148, Loss: 0.47183531411129226\n",
      "Epoch 149, Loss: 0.470294933688076\n",
      "Epoch 150, Loss: 0.4697114144973765\n",
      "Epoch 151, Loss: 0.47018789053701304\n",
      "Epoch 152, Loss: 0.4700460953124117\n",
      "Epoch 153, Loss: 0.4693645740016508\n",
      "Epoch 154, Loss: 0.4684558104180481\n",
      "Epoch 155, Loss: 0.4680960384979934\n",
      "Epoch 156, Loss: 0.4682454254529793\n",
      "Epoch 157, Loss: 0.4682974379105488\n",
      "Epoch 158, Loss: 0.4681590010361841\n",
      "Epoch 159, Loss: 0.4673225124817593\n",
      "Epoch 160, Loss: 0.4666701428107949\n",
      "Epoch 161, Loss: 0.4663166784414499\n",
      "Epoch 162, Loss: 0.46624854535037\n",
      "Epoch 163, Loss: 0.4664611566834947\n",
      "Epoch 164, Loss: 0.46654532450127006\n",
      "Epoch 165, Loss: 0.46721845607201035\n",
      "Epoch 166, Loss: 0.4668208160006968\n",
      "Epoch 167, Loss: 0.466573749669782\n",
      "Epoch 168, Loss: 0.46506387643568103\n",
      "Epoch 169, Loss: 0.4641220516950567\n",
      "Epoch 170, Loss: 0.4644550172656552\n",
      "Epoch 171, Loss: 0.4654613047116713\n",
      "Epoch 172, Loss: 0.46880176576246846\n",
      "Epoch 173, Loss: 0.4685649824581709\n",
      "Epoch 174, Loss: 0.46606115996454733\n",
      "Epoch 175, Loss: 0.46328263242364126\n",
      "Epoch 176, Loss: 0.4660484187313888\n",
      "Epoch 177, Loss: 0.4667669789465077\n",
      "Epoch 178, Loss: 0.46222975993575777\n",
      "Epoch 179, Loss: 0.46412214801342555\n",
      "Epoch 180, Loss: 0.46867384291927394\n",
      "Epoch 181, Loss: 0.46316269230838986\n",
      "Epoch 182, Loss: 0.4619791093960296\n",
      "Epoch 183, Loss: 0.4650783107301587\n",
      "Epoch 184, Loss: 0.4617210775403838\n",
      "Epoch 185, Loss: 0.4621011464022392\n",
      "Epoch 186, Loss: 0.4641367239628427\n",
      "Epoch 187, Loss: 0.46071131054180564\n",
      "Epoch 188, Loss: 0.46145892957673257\n",
      "Epoch 189, Loss: 0.464304908222076\n",
      "Epoch 190, Loss: 0.4606605441278432\n",
      "Epoch 191, Loss: 0.4601386308130386\n",
      "Epoch 192, Loss: 0.46193430495315263\n",
      "Epoch 193, Loss: 0.45977696912799226\n",
      "Epoch 194, Loss: 0.4597173803227511\n",
      "Epoch 195, Loss: 0.46096589922299847\n",
      "Epoch 196, Loss: 0.4592170612502741\n",
      "Epoch 197, Loss: 0.45864347419013624\n",
      "Epoch 198, Loss: 0.4594650511157848\n",
      "Epoch 199, Loss: 0.45924705942486493\n",
      "Epoch 200, Loss: 0.4584931199827434\n",
      "Epoch 201, Loss: 0.4578552480015059\n",
      "Epoch 202, Loss: 0.458251953125\n",
      "Epoch 203, Loss: 0.4586848316367108\n",
      "Epoch 204, Loss: 0.4577025349799131\n",
      "Epoch 205, Loss: 0.45707273702530193\n",
      "Epoch 206, Loss: 0.4571336158814618\n",
      "Epoch 207, Loss: 0.45746758002181026\n",
      "Epoch 208, Loss: 0.4580334166179955\n",
      "Epoch 209, Loss: 0.457551138813026\n",
      "Epoch 210, Loss: 0.4572461918817771\n",
      "Epoch 211, Loss: 0.45629613394883295\n",
      "Epoch 212, Loss: 0.4557945440718634\n",
      "Epoch 213, Loss: 0.455843155547024\n",
      "Epoch 214, Loss: 0.456219547378948\n",
      "Epoch 215, Loss: 0.4577116995236092\n",
      "Epoch 216, Loss: 0.4589176697652806\n",
      "Epoch 217, Loss: 0.46105034303609993\n",
      "Epoch 218, Loss: 0.4567326442394303\n",
      "Epoch 219, Loss: 0.45493112899589366\n",
      "Epoch 220, Loss: 0.45527199475335134\n",
      "Epoch 221, Loss: 0.45623630281032773\n",
      "Epoch 222, Loss: 0.45802054967026307\n",
      "Epoch 223, Loss: 0.45677224995978094\n",
      "Epoch 224, Loss: 0.45565914342704716\n",
      "Epoch 225, Loss: 0.45401674524485375\n",
      "Epoch 226, Loss: 0.4535057342791329\n",
      "Epoch 227, Loss: 0.45416164205845616\n",
      "Epoch 228, Loss: 0.45450513400796655\n",
      "Epoch 229, Loss: 0.4548238117351997\n",
      "Epoch 230, Loss: 0.4537990916311975\n",
      "Epoch 231, Loss: 0.4532521496977959\n",
      "Epoch 232, Loss: 0.4526084553640119\n",
      "Epoch 233, Loss: 0.45228981460773116\n",
      "Epoch 234, Loss: 0.4520949132230242\n",
      "Epoch 235, Loss: 0.45205797975469736\n",
      "Epoch 236, Loss: 0.45230557901617613\n",
      "Epoch 237, Loss: 0.4528345953247846\n",
      "Epoch 238, Loss: 0.4549386133976144\n",
      "Epoch 239, Loss: 0.4564022887834084\n",
      "Epoch 240, Loss: 0.45805755540321125\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2363803229207095\n",
      "Valence RMSE: 0.24241576183425825\n",
      "Arousal RMSE: 0.2301866906285287\n",
      "Test R^2 score: tensor([0.3720, 0.6092], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.49063280870574116\n",
      "Num of epochs: 241\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Epoch 121, Loss: 0.4816228295571127\n",
      "Epoch 122, Loss: 0.4807618495484584\n",
      "Epoch 123, Loss: 0.48102030846138955\n",
      "Epoch 124, Loss: 0.4810176288364116\n",
      "Epoch 125, Loss: 0.48014338261661127\n",
      "Epoch 126, Loss: 0.47910314809270116\n",
      "Epoch 127, Loss: 0.47880795949683447\n",
      "Epoch 128, Loss: 0.47876177315142754\n",
      "Epoch 129, Loss: 0.47819597116449997\n",
      "Epoch 130, Loss: 0.47743214539542544\n",
      "Epoch 131, Loss: 0.47658476230340185\n",
      "Epoch 132, Loss: 0.4759644680184978\n",
      "Epoch 133, Loss: 0.4755635187712421\n",
      "Epoch 134, Loss: 0.47538264196584307\n",
      "Epoch 135, Loss: 0.4753673450596914\n",
      "Epoch 136, Loss: 0.4753697744168057\n",
      "Epoch 137, Loss: 0.4755973109583155\n",
      "Epoch 138, Loss: 0.4767626826605241\n",
      "Epoch 139, Loss: 0.47683528222287225\n",
      "Epoch 140, Loss: 0.47579445468261566\n",
      "Epoch 141, Loss: 0.47284533919249566\n",
      "Epoch 142, Loss: 0.472809286012082\n",
      "Epoch 143, Loss: 0.47458738111232796\n",
      "Epoch 144, Loss: 0.4733380058918715\n",
      "Epoch 145, Loss: 0.4715229034484008\n",
      "Epoch 146, Loss: 0.47096730966267386\n",
      "Epoch 147, Loss: 0.47161469890467383\n",
      "Epoch 148, Loss: 0.47183531411129226\n",
      "Epoch 149, Loss: 0.470294933688076\n",
      "Epoch 150, Loss: 0.4697114144973765\n",
      "Epoch 151, Loss: 0.47018789053701304\n",
      "Epoch 152, Loss: 0.4700460953124117\n",
      "Epoch 153, Loss: 0.4693645740016508\n",
      "Epoch 154, Loss: 0.4684558104180481\n",
      "Epoch 155, Loss: 0.4680960384979934\n",
      "Epoch 156, Loss: 0.4682454254529793\n",
      "Epoch 157, Loss: 0.4682974379105488\n",
      "Epoch 158, Loss: 0.4681590010361841\n",
      "Epoch 159, Loss: 0.4673225124817593\n",
      "Epoch 160, Loss: 0.4666701428107949\n",
      "Epoch 161, Loss: 0.4663166784414499\n",
      "Epoch 162, Loss: 0.46624854535037\n",
      "Epoch 163, Loss: 0.4664611566834947\n",
      "Epoch 164, Loss: 0.46654532450127006\n",
      "Epoch 165, Loss: 0.46721845607201035\n",
      "Epoch 166, Loss: 0.4668208160006968\n",
      "Epoch 167, Loss: 0.466573749669782\n",
      "Epoch 168, Loss: 0.46506387643568103\n",
      "Epoch 169, Loss: 0.4641220516950567\n",
      "Epoch 170, Loss: 0.4644550172656552\n",
      "Epoch 171, Loss: 0.4654613047116713\n",
      "Epoch 172, Loss: 0.46880176576246846\n",
      "Epoch 173, Loss: 0.4685649824581709\n",
      "Epoch 174, Loss: 0.46606115996454733\n",
      "Epoch 175, Loss: 0.46328263242364126\n",
      "Epoch 176, Loss: 0.4660484187313888\n",
      "Epoch 177, Loss: 0.4667669789465077\n",
      "Epoch 178, Loss: 0.46222975993575777\n",
      "Epoch 179, Loss: 0.46412214801342555\n",
      "Epoch 180, Loss: 0.46867384291927394\n",
      "Epoch 181, Loss: 0.46316269230838986\n",
      "Epoch 182, Loss: 0.4619791093960296\n",
      "Epoch 183, Loss: 0.4650783107301587\n",
      "Epoch 184, Loss: 0.4617210775403838\n",
      "Epoch 185, Loss: 0.4621011464022392\n",
      "Epoch 186, Loss: 0.4641367239628427\n",
      "Epoch 187, Loss: 0.46071131054180564\n",
      "Epoch 188, Loss: 0.46145892957673257\n",
      "Epoch 189, Loss: 0.464304908222076\n",
      "Epoch 190, Loss: 0.4606605441278432\n",
      "Epoch 191, Loss: 0.4601386308130386\n",
      "Epoch 192, Loss: 0.46193430495315263\n",
      "Epoch 193, Loss: 0.45977696912799226\n",
      "Epoch 194, Loss: 0.4597173803227511\n",
      "Epoch 195, Loss: 0.46096589922299847\n",
      "Epoch 196, Loss: 0.4592170612502741\n",
      "Epoch 197, Loss: 0.45864347419013624\n",
      "Epoch 198, Loss: 0.4594650511157848\n",
      "Epoch 199, Loss: 0.45924705942486493\n",
      "Epoch 200, Loss: 0.4584931199827434\n",
      "Epoch 201, Loss: 0.4578552480015059\n",
      "Epoch 202, Loss: 0.458251953125\n",
      "Epoch 203, Loss: 0.4586848316367108\n",
      "Epoch 204, Loss: 0.4577025349799131\n",
      "Epoch 205, Loss: 0.45707273702530193\n",
      "Epoch 206, Loss: 0.4571336158814618\n",
      "Epoch 207, Loss: 0.45746758002181026\n",
      "Epoch 208, Loss: 0.4580334166179955\n",
      "Epoch 209, Loss: 0.457551138813026\n",
      "Epoch 210, Loss: 0.4572461918817771\n",
      "Epoch 211, Loss: 0.45629613394883295\n",
      "Epoch 212, Loss: 0.4557945440718634\n",
      "Epoch 213, Loss: 0.455843155547024\n",
      "Epoch 214, Loss: 0.456219547378948\n",
      "Epoch 215, Loss: 0.4577116995236092\n",
      "Epoch 216, Loss: 0.4589176697652806\n",
      "Epoch 217, Loss: 0.46105034303609993\n",
      "Epoch 218, Loss: 0.4567326442394303\n",
      "Epoch 219, Loss: 0.45493112899589366\n",
      "Epoch 220, Loss: 0.45527199475335134\n",
      "Epoch 221, Loss: 0.45623630281032773\n",
      "Epoch 222, Loss: 0.45802054967026307\n",
      "Epoch 223, Loss: 0.45677224995978094\n",
      "Epoch 224, Loss: 0.45565914342704716\n",
      "Epoch 225, Loss: 0.45401674524485375\n",
      "Epoch 226, Loss: 0.4535057342791329\n",
      "Epoch 227, Loss: 0.45416164205845616\n",
      "Epoch 228, Loss: 0.45450513400796655\n",
      "Epoch 229, Loss: 0.4548238117351997\n",
      "Epoch 230, Loss: 0.4537990916311975\n",
      "Epoch 231, Loss: 0.4532521496977959\n",
      "Epoch 232, Loss: 0.4526084553640119\n",
      "Epoch 233, Loss: 0.45228981460773116\n",
      "Epoch 234, Loss: 0.4520949132230242\n",
      "Epoch 235, Loss: 0.45205797975469736\n",
      "Epoch 236, Loss: 0.45230557901617613\n",
      "Epoch 237, Loss: 0.4528345953247846\n",
      "Epoch 238, Loss: 0.4549386133976144\n",
      "Epoch 239, Loss: 0.4564022887834084\n",
      "Epoch 240, Loss: 0.45805755540321125\n",
      "Epoch 241, Loss: 0.4541498302115434\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23600637701207605\n",
      "Valence RMSE: 0.2452873256376612\n",
      "Arousal RMSE: 0.2263451962429428\n",
      "Test R^2 score: tensor([0.3571, 0.6222], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4896170851681628\n",
      "Num of epochs: 242\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Epoch 121, Loss: 0.4816228295571127\n",
      "Epoch 122, Loss: 0.4807618495484584\n",
      "Epoch 123, Loss: 0.48102030846138955\n",
      "Epoch 124, Loss: 0.4810176288364116\n",
      "Epoch 125, Loss: 0.48014338261661127\n",
      "Epoch 126, Loss: 0.47910314809270116\n",
      "Epoch 127, Loss: 0.47880795949683447\n",
      "Epoch 128, Loss: 0.47876177315142754\n",
      "Epoch 129, Loss: 0.47819597116449997\n",
      "Epoch 130, Loss: 0.47743214539542544\n",
      "Epoch 131, Loss: 0.47658476230340185\n",
      "Epoch 132, Loss: 0.4759644680184978\n",
      "Epoch 133, Loss: 0.4755635187712421\n",
      "Epoch 134, Loss: 0.47538264196584307\n",
      "Epoch 135, Loss: 0.4753673450596914\n",
      "Epoch 136, Loss: 0.4753697744168057\n",
      "Epoch 137, Loss: 0.4755973109583155\n",
      "Epoch 138, Loss: 0.4767626826605241\n",
      "Epoch 139, Loss: 0.47683528222287225\n",
      "Epoch 140, Loss: 0.47579445468261566\n",
      "Epoch 141, Loss: 0.47284533919249566\n",
      "Epoch 142, Loss: 0.472809286012082\n",
      "Epoch 143, Loss: 0.47458738111232796\n",
      "Epoch 144, Loss: 0.4733380058918715\n",
      "Epoch 145, Loss: 0.4715229034484008\n",
      "Epoch 146, Loss: 0.47096730966267386\n",
      "Epoch 147, Loss: 0.47161469890467383\n",
      "Epoch 148, Loss: 0.47183531411129226\n",
      "Epoch 149, Loss: 0.470294933688076\n",
      "Epoch 150, Loss: 0.4697114144973765\n",
      "Epoch 151, Loss: 0.47018789053701304\n",
      "Epoch 152, Loss: 0.4700460953124117\n",
      "Epoch 153, Loss: 0.4693645740016508\n",
      "Epoch 154, Loss: 0.4684558104180481\n",
      "Epoch 155, Loss: 0.4680960384979934\n",
      "Epoch 156, Loss: 0.4682454254529793\n",
      "Epoch 157, Loss: 0.4682974379105488\n",
      "Epoch 158, Loss: 0.4681590010361841\n",
      "Epoch 159, Loss: 0.4673225124817593\n",
      "Epoch 160, Loss: 0.4666701428107949\n",
      "Epoch 161, Loss: 0.4663166784414499\n",
      "Epoch 162, Loss: 0.46624854535037\n",
      "Epoch 163, Loss: 0.4664611566834947\n",
      "Epoch 164, Loss: 0.46654532450127006\n",
      "Epoch 165, Loss: 0.46721845607201035\n",
      "Epoch 166, Loss: 0.4668208160006968\n",
      "Epoch 167, Loss: 0.466573749669782\n",
      "Epoch 168, Loss: 0.46506387643568103\n",
      "Epoch 169, Loss: 0.4641220516950567\n",
      "Epoch 170, Loss: 0.4644550172656552\n",
      "Epoch 171, Loss: 0.4654613047116713\n",
      "Epoch 172, Loss: 0.46880176576246846\n",
      "Epoch 173, Loss: 0.4685649824581709\n",
      "Epoch 174, Loss: 0.46606115996454733\n",
      "Epoch 175, Loss: 0.46328263242364126\n",
      "Epoch 176, Loss: 0.4660484187313888\n",
      "Epoch 177, Loss: 0.4667669789465077\n",
      "Epoch 178, Loss: 0.46222975993575777\n",
      "Epoch 179, Loss: 0.46412214801342555\n",
      "Epoch 180, Loss: 0.46867384291927394\n",
      "Epoch 181, Loss: 0.46316269230838986\n",
      "Epoch 182, Loss: 0.4619791093960296\n",
      "Epoch 183, Loss: 0.4650783107301587\n",
      "Epoch 184, Loss: 0.4617210775403838\n",
      "Epoch 185, Loss: 0.4621011464022392\n",
      "Epoch 186, Loss: 0.4641367239628427\n",
      "Epoch 187, Loss: 0.46071131054180564\n",
      "Epoch 188, Loss: 0.46145892957673257\n",
      "Epoch 189, Loss: 0.464304908222076\n",
      "Epoch 190, Loss: 0.4606605441278432\n",
      "Epoch 191, Loss: 0.4601386308130386\n",
      "Epoch 192, Loss: 0.46193430495315263\n",
      "Epoch 193, Loss: 0.45977696912799226\n",
      "Epoch 194, Loss: 0.4597173803227511\n",
      "Epoch 195, Loss: 0.46096589922299847\n",
      "Epoch 196, Loss: 0.4592170612502741\n",
      "Epoch 197, Loss: 0.45864347419013624\n",
      "Epoch 198, Loss: 0.4594650511157848\n",
      "Epoch 199, Loss: 0.45924705942486493\n",
      "Epoch 200, Loss: 0.4584931199827434\n",
      "Epoch 201, Loss: 0.4578552480015059\n",
      "Epoch 202, Loss: 0.458251953125\n",
      "Epoch 203, Loss: 0.4586848316367108\n",
      "Epoch 204, Loss: 0.4577025349799131\n",
      "Epoch 205, Loss: 0.45707273702530193\n",
      "Epoch 206, Loss: 0.4571336158814618\n",
      "Epoch 207, Loss: 0.45746758002181026\n",
      "Epoch 208, Loss: 0.4580334166179955\n",
      "Epoch 209, Loss: 0.457551138813026\n",
      "Epoch 210, Loss: 0.4572461918817771\n",
      "Epoch 211, Loss: 0.45629613394883295\n",
      "Epoch 212, Loss: 0.4557945440718634\n",
      "Epoch 213, Loss: 0.455843155547024\n",
      "Epoch 214, Loss: 0.456219547378948\n",
      "Epoch 215, Loss: 0.4577116995236092\n",
      "Epoch 216, Loss: 0.4589176697652806\n",
      "Epoch 217, Loss: 0.46105034303609993\n",
      "Epoch 218, Loss: 0.4567326442394303\n",
      "Epoch 219, Loss: 0.45493112899589366\n",
      "Epoch 220, Loss: 0.45527199475335134\n",
      "Epoch 221, Loss: 0.45623630281032773\n",
      "Epoch 222, Loss: 0.45802054967026307\n",
      "Epoch 223, Loss: 0.45677224995978094\n",
      "Epoch 224, Loss: 0.45565914342704716\n",
      "Epoch 225, Loss: 0.45401674524485375\n",
      "Epoch 226, Loss: 0.4535057342791329\n",
      "Epoch 227, Loss: 0.45416164205845616\n",
      "Epoch 228, Loss: 0.45450513400796655\n",
      "Epoch 229, Loss: 0.4548238117351997\n",
      "Epoch 230, Loss: 0.4537990916311975\n",
      "Epoch 231, Loss: 0.4532521496977959\n",
      "Epoch 232, Loss: 0.4526084553640119\n",
      "Epoch 233, Loss: 0.45228981460773116\n",
      "Epoch 234, Loss: 0.4520949132230242\n",
      "Epoch 235, Loss: 0.45205797975469736\n",
      "Epoch 236, Loss: 0.45230557901617613\n",
      "Epoch 237, Loss: 0.4528345953247846\n",
      "Epoch 238, Loss: 0.4549386133976144\n",
      "Epoch 239, Loss: 0.4564022887834084\n",
      "Epoch 240, Loss: 0.45805755540321125\n",
      "Epoch 241, Loss: 0.4541498302115434\n",
      "Epoch 242, Loss: 0.45158727109343993\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23576380744538095\n",
      "Valence RMSE: 0.24445725420125788\n",
      "Arousal RMSE: 0.2267372855766491\n",
      "Test R^2 score: tensor([0.3614, 0.6209], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.49113405554589384\n",
      "Num of epochs: 243\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Epoch 121, Loss: 0.4816228295571127\n",
      "Epoch 122, Loss: 0.4807618495484584\n",
      "Epoch 123, Loss: 0.48102030846138955\n",
      "Epoch 124, Loss: 0.4810176288364116\n",
      "Epoch 125, Loss: 0.48014338261661127\n",
      "Epoch 126, Loss: 0.47910314809270116\n",
      "Epoch 127, Loss: 0.47880795949683447\n",
      "Epoch 128, Loss: 0.47876177315142754\n",
      "Epoch 129, Loss: 0.47819597116449997\n",
      "Epoch 130, Loss: 0.47743214539542544\n",
      "Epoch 131, Loss: 0.47658476230340185\n",
      "Epoch 132, Loss: 0.4759644680184978\n",
      "Epoch 133, Loss: 0.4755635187712421\n",
      "Epoch 134, Loss: 0.47538264196584307\n",
      "Epoch 135, Loss: 0.4753673450596914\n",
      "Epoch 136, Loss: 0.4753697744168057\n",
      "Epoch 137, Loss: 0.4755973109583155\n",
      "Epoch 138, Loss: 0.4767626826605241\n",
      "Epoch 139, Loss: 0.47683528222287225\n",
      "Epoch 140, Loss: 0.47579445468261566\n",
      "Epoch 141, Loss: 0.47284533919249566\n",
      "Epoch 142, Loss: 0.472809286012082\n",
      "Epoch 143, Loss: 0.47458738111232796\n",
      "Epoch 144, Loss: 0.4733380058918715\n",
      "Epoch 145, Loss: 0.4715229034484008\n",
      "Epoch 146, Loss: 0.47096730966267386\n",
      "Epoch 147, Loss: 0.47161469890467383\n",
      "Epoch 148, Loss: 0.47183531411129226\n",
      "Epoch 149, Loss: 0.470294933688076\n",
      "Epoch 150, Loss: 0.4697114144973765\n",
      "Epoch 151, Loss: 0.47018789053701304\n",
      "Epoch 152, Loss: 0.4700460953124117\n",
      "Epoch 153, Loss: 0.4693645740016508\n",
      "Epoch 154, Loss: 0.4684558104180481\n",
      "Epoch 155, Loss: 0.4680960384979934\n",
      "Epoch 156, Loss: 0.4682454254529793\n",
      "Epoch 157, Loss: 0.4682974379105488\n",
      "Epoch 158, Loss: 0.4681590010361841\n",
      "Epoch 159, Loss: 0.4673225124817593\n",
      "Epoch 160, Loss: 0.4666701428107949\n",
      "Epoch 161, Loss: 0.4663166784414499\n",
      "Epoch 162, Loss: 0.46624854535037\n",
      "Epoch 163, Loss: 0.4664611566834947\n",
      "Epoch 164, Loss: 0.46654532450127006\n",
      "Epoch 165, Loss: 0.46721845607201035\n",
      "Epoch 166, Loss: 0.4668208160006968\n",
      "Epoch 167, Loss: 0.466573749669782\n",
      "Epoch 168, Loss: 0.46506387643568103\n",
      "Epoch 169, Loss: 0.4641220516950567\n",
      "Epoch 170, Loss: 0.4644550172656552\n",
      "Epoch 171, Loss: 0.4654613047116713\n",
      "Epoch 172, Loss: 0.46880176576246846\n",
      "Epoch 173, Loss: 0.4685649824581709\n",
      "Epoch 174, Loss: 0.46606115996454733\n",
      "Epoch 175, Loss: 0.46328263242364126\n",
      "Epoch 176, Loss: 0.4660484187313888\n",
      "Epoch 177, Loss: 0.4667669789465077\n",
      "Epoch 178, Loss: 0.46222975993575777\n",
      "Epoch 179, Loss: 0.46412214801342555\n",
      "Epoch 180, Loss: 0.46867384291927394\n",
      "Epoch 181, Loss: 0.46316269230838986\n",
      "Epoch 182, Loss: 0.4619791093960296\n",
      "Epoch 183, Loss: 0.4650783107301587\n",
      "Epoch 184, Loss: 0.4617210775403838\n",
      "Epoch 185, Loss: 0.4621011464022392\n",
      "Epoch 186, Loss: 0.4641367239628427\n",
      "Epoch 187, Loss: 0.46071131054180564\n",
      "Epoch 188, Loss: 0.46145892957673257\n",
      "Epoch 189, Loss: 0.464304908222076\n",
      "Epoch 190, Loss: 0.4606605441278432\n",
      "Epoch 191, Loss: 0.4601386308130386\n",
      "Epoch 192, Loss: 0.46193430495315263\n",
      "Epoch 193, Loss: 0.45977696912799226\n",
      "Epoch 194, Loss: 0.4597173803227511\n",
      "Epoch 195, Loss: 0.46096589922299847\n",
      "Epoch 196, Loss: 0.4592170612502741\n",
      "Epoch 197, Loss: 0.45864347419013624\n",
      "Epoch 198, Loss: 0.4594650511157848\n",
      "Epoch 199, Loss: 0.45924705942486493\n",
      "Epoch 200, Loss: 0.4584931199827434\n",
      "Epoch 201, Loss: 0.4578552480015059\n",
      "Epoch 202, Loss: 0.458251953125\n",
      "Epoch 203, Loss: 0.4586848316367108\n",
      "Epoch 204, Loss: 0.4577025349799131\n",
      "Epoch 205, Loss: 0.45707273702530193\n",
      "Epoch 206, Loss: 0.4571336158814618\n",
      "Epoch 207, Loss: 0.45746758002181026\n",
      "Epoch 208, Loss: 0.4580334166179955\n",
      "Epoch 209, Loss: 0.457551138813026\n",
      "Epoch 210, Loss: 0.4572461918817771\n",
      "Epoch 211, Loss: 0.45629613394883295\n",
      "Epoch 212, Loss: 0.4557945440718634\n",
      "Epoch 213, Loss: 0.455843155547024\n",
      "Epoch 214, Loss: 0.456219547378948\n",
      "Epoch 215, Loss: 0.4577116995236092\n",
      "Epoch 216, Loss: 0.4589176697652806\n",
      "Epoch 217, Loss: 0.46105034303609993\n",
      "Epoch 218, Loss: 0.4567326442394303\n",
      "Epoch 219, Loss: 0.45493112899589366\n",
      "Epoch 220, Loss: 0.45527199475335134\n",
      "Epoch 221, Loss: 0.45623630281032773\n",
      "Epoch 222, Loss: 0.45802054967026307\n",
      "Epoch 223, Loss: 0.45677224995978094\n",
      "Epoch 224, Loss: 0.45565914342704716\n",
      "Epoch 225, Loss: 0.45401674524485375\n",
      "Epoch 226, Loss: 0.4535057342791329\n",
      "Epoch 227, Loss: 0.45416164205845616\n",
      "Epoch 228, Loss: 0.45450513400796655\n",
      "Epoch 229, Loss: 0.4548238117351997\n",
      "Epoch 230, Loss: 0.4537990916311975\n",
      "Epoch 231, Loss: 0.4532521496977959\n",
      "Epoch 232, Loss: 0.4526084553640119\n",
      "Epoch 233, Loss: 0.45228981460773116\n",
      "Epoch 234, Loss: 0.4520949132230242\n",
      "Epoch 235, Loss: 0.45205797975469736\n",
      "Epoch 236, Loss: 0.45230557901617613\n",
      "Epoch 237, Loss: 0.4528345953247846\n",
      "Epoch 238, Loss: 0.4549386133976144\n",
      "Epoch 239, Loss: 0.4564022887834084\n",
      "Epoch 240, Loss: 0.45805755540321125\n",
      "Epoch 241, Loss: 0.4541498302115434\n",
      "Epoch 242, Loss: 0.45158727109343993\n",
      "Epoch 243, Loss: 0.45105347173746146\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23618911124375694\n",
      "Valence RMSE: 0.24266174912266072\n",
      "Arousal RMSE: 0.22953402373714119\n",
      "Test R^2 score: tensor([0.3708, 0.6114], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.49110167597258125\n",
      "Num of epochs: 244\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Epoch 121, Loss: 0.4816228295571127\n",
      "Epoch 122, Loss: 0.4807618495484584\n",
      "Epoch 123, Loss: 0.48102030846138955\n",
      "Epoch 124, Loss: 0.4810176288364116\n",
      "Epoch 125, Loss: 0.48014338261661127\n",
      "Epoch 126, Loss: 0.47910314809270116\n",
      "Epoch 127, Loss: 0.47880795949683447\n",
      "Epoch 128, Loss: 0.47876177315142754\n",
      "Epoch 129, Loss: 0.47819597116449997\n",
      "Epoch 130, Loss: 0.47743214539542544\n",
      "Epoch 131, Loss: 0.47658476230340185\n",
      "Epoch 132, Loss: 0.4759644680184978\n",
      "Epoch 133, Loss: 0.4755635187712421\n",
      "Epoch 134, Loss: 0.47538264196584307\n",
      "Epoch 135, Loss: 0.4753673450596914\n",
      "Epoch 136, Loss: 0.4753697744168057\n",
      "Epoch 137, Loss: 0.4755973109583155\n",
      "Epoch 138, Loss: 0.4767626826605241\n",
      "Epoch 139, Loss: 0.47683528222287225\n",
      "Epoch 140, Loss: 0.47579445468261566\n",
      "Epoch 141, Loss: 0.47284533919249566\n",
      "Epoch 142, Loss: 0.472809286012082\n",
      "Epoch 143, Loss: 0.47458738111232796\n",
      "Epoch 144, Loss: 0.4733380058918715\n",
      "Epoch 145, Loss: 0.4715229034484008\n",
      "Epoch 146, Loss: 0.47096730966267386\n",
      "Epoch 147, Loss: 0.47161469890467383\n",
      "Epoch 148, Loss: 0.47183531411129226\n",
      "Epoch 149, Loss: 0.470294933688076\n",
      "Epoch 150, Loss: 0.4697114144973765\n",
      "Epoch 151, Loss: 0.47018789053701304\n",
      "Epoch 152, Loss: 0.4700460953124117\n",
      "Epoch 153, Loss: 0.4693645740016508\n",
      "Epoch 154, Loss: 0.4684558104180481\n",
      "Epoch 155, Loss: 0.4680960384979934\n",
      "Epoch 156, Loss: 0.4682454254529793\n",
      "Epoch 157, Loss: 0.4682974379105488\n",
      "Epoch 158, Loss: 0.4681590010361841\n",
      "Epoch 159, Loss: 0.4673225124817593\n",
      "Epoch 160, Loss: 0.4666701428107949\n",
      "Epoch 161, Loss: 0.4663166784414499\n",
      "Epoch 162, Loss: 0.46624854535037\n",
      "Epoch 163, Loss: 0.4664611566834947\n",
      "Epoch 164, Loss: 0.46654532450127006\n",
      "Epoch 165, Loss: 0.46721845607201035\n",
      "Epoch 166, Loss: 0.4668208160006968\n",
      "Epoch 167, Loss: 0.466573749669782\n",
      "Epoch 168, Loss: 0.46506387643568103\n",
      "Epoch 169, Loss: 0.4641220516950567\n",
      "Epoch 170, Loss: 0.4644550172656552\n",
      "Epoch 171, Loss: 0.4654613047116713\n",
      "Epoch 172, Loss: 0.46880176576246846\n",
      "Epoch 173, Loss: 0.4685649824581709\n",
      "Epoch 174, Loss: 0.46606115996454733\n",
      "Epoch 175, Loss: 0.46328263242364126\n",
      "Epoch 176, Loss: 0.4660484187313888\n",
      "Epoch 177, Loss: 0.4667669789465077\n",
      "Epoch 178, Loss: 0.46222975993575777\n",
      "Epoch 179, Loss: 0.46412214801342555\n",
      "Epoch 180, Loss: 0.46867384291927394\n",
      "Epoch 181, Loss: 0.46316269230838986\n",
      "Epoch 182, Loss: 0.4619791093960296\n",
      "Epoch 183, Loss: 0.4650783107301587\n",
      "Epoch 184, Loss: 0.4617210775403838\n",
      "Epoch 185, Loss: 0.4621011464022392\n",
      "Epoch 186, Loss: 0.4641367239628427\n",
      "Epoch 187, Loss: 0.46071131054180564\n",
      "Epoch 188, Loss: 0.46145892957673257\n",
      "Epoch 189, Loss: 0.464304908222076\n",
      "Epoch 190, Loss: 0.4606605441278432\n",
      "Epoch 191, Loss: 0.4601386308130386\n",
      "Epoch 192, Loss: 0.46193430495315263\n",
      "Epoch 193, Loss: 0.45977696912799226\n",
      "Epoch 194, Loss: 0.4597173803227511\n",
      "Epoch 195, Loss: 0.46096589922299847\n",
      "Epoch 196, Loss: 0.4592170612502741\n",
      "Epoch 197, Loss: 0.45864347419013624\n",
      "Epoch 198, Loss: 0.4594650511157848\n",
      "Epoch 199, Loss: 0.45924705942486493\n",
      "Epoch 200, Loss: 0.4584931199827434\n",
      "Epoch 201, Loss: 0.4578552480015059\n",
      "Epoch 202, Loss: 0.458251953125\n",
      "Epoch 203, Loss: 0.4586848316367108\n",
      "Epoch 204, Loss: 0.4577025349799131\n",
      "Epoch 205, Loss: 0.45707273702530193\n",
      "Epoch 206, Loss: 0.4571336158814618\n",
      "Epoch 207, Loss: 0.45746758002181026\n",
      "Epoch 208, Loss: 0.4580334166179955\n",
      "Epoch 209, Loss: 0.457551138813026\n",
      "Epoch 210, Loss: 0.4572461918817771\n",
      "Epoch 211, Loss: 0.45629613394883295\n",
      "Epoch 212, Loss: 0.4557945440718634\n",
      "Epoch 213, Loss: 0.455843155547024\n",
      "Epoch 214, Loss: 0.456219547378948\n",
      "Epoch 215, Loss: 0.4577116995236092\n",
      "Epoch 216, Loss: 0.4589176697652806\n",
      "Epoch 217, Loss: 0.46105034303609993\n",
      "Epoch 218, Loss: 0.4567326442394303\n",
      "Epoch 219, Loss: 0.45493112899589366\n",
      "Epoch 220, Loss: 0.45527199475335134\n",
      "Epoch 221, Loss: 0.45623630281032773\n",
      "Epoch 222, Loss: 0.45802054967026307\n",
      "Epoch 223, Loss: 0.45677224995978094\n",
      "Epoch 224, Loss: 0.45565914342704716\n",
      "Epoch 225, Loss: 0.45401674524485375\n",
      "Epoch 226, Loss: 0.4535057342791329\n",
      "Epoch 227, Loss: 0.45416164205845616\n",
      "Epoch 228, Loss: 0.45450513400796655\n",
      "Epoch 229, Loss: 0.4548238117351997\n",
      "Epoch 230, Loss: 0.4537990916311975\n",
      "Epoch 231, Loss: 0.4532521496977959\n",
      "Epoch 232, Loss: 0.4526084553640119\n",
      "Epoch 233, Loss: 0.45228981460773116\n",
      "Epoch 234, Loss: 0.4520949132230242\n",
      "Epoch 235, Loss: 0.45205797975469736\n",
      "Epoch 236, Loss: 0.45230557901617613\n",
      "Epoch 237, Loss: 0.4528345953247846\n",
      "Epoch 238, Loss: 0.4549386133976144\n",
      "Epoch 239, Loss: 0.4564022887834084\n",
      "Epoch 240, Loss: 0.45805755540321125\n",
      "Epoch 241, Loss: 0.4541498302115434\n",
      "Epoch 242, Loss: 0.45158727109343993\n",
      "Epoch 243, Loss: 0.45105347173746146\n",
      "Epoch 244, Loss: 0.45263971453254503\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.24020515117519856\n",
      "Valence RMSE: 0.2507290765611776\n",
      "Arousal RMSE: 0.22919851541617606\n",
      "Test R^2 score: tensor([0.3282, 0.6126], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.47040228050479316\n",
      "Num of epochs: 245\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Epoch 121, Loss: 0.4816228295571127\n",
      "Epoch 122, Loss: 0.4807618495484584\n",
      "Epoch 123, Loss: 0.48102030846138955\n",
      "Epoch 124, Loss: 0.4810176288364116\n",
      "Epoch 125, Loss: 0.48014338261661127\n",
      "Epoch 126, Loss: 0.47910314809270116\n",
      "Epoch 127, Loss: 0.47880795949683447\n",
      "Epoch 128, Loss: 0.47876177315142754\n",
      "Epoch 129, Loss: 0.47819597116449997\n",
      "Epoch 130, Loss: 0.47743214539542544\n",
      "Epoch 131, Loss: 0.47658476230340185\n",
      "Epoch 132, Loss: 0.4759644680184978\n",
      "Epoch 133, Loss: 0.4755635187712421\n",
      "Epoch 134, Loss: 0.47538264196584307\n",
      "Epoch 135, Loss: 0.4753673450596914\n",
      "Epoch 136, Loss: 0.4753697744168057\n",
      "Epoch 137, Loss: 0.4755973109583155\n",
      "Epoch 138, Loss: 0.4767626826605241\n",
      "Epoch 139, Loss: 0.47683528222287225\n",
      "Epoch 140, Loss: 0.47579445468261566\n",
      "Epoch 141, Loss: 0.47284533919249566\n",
      "Epoch 142, Loss: 0.472809286012082\n",
      "Epoch 143, Loss: 0.47458738111232796\n",
      "Epoch 144, Loss: 0.4733380058918715\n",
      "Epoch 145, Loss: 0.4715229034484008\n",
      "Epoch 146, Loss: 0.47096730966267386\n",
      "Epoch 147, Loss: 0.47161469890467383\n",
      "Epoch 148, Loss: 0.47183531411129226\n",
      "Epoch 149, Loss: 0.470294933688076\n",
      "Epoch 150, Loss: 0.4697114144973765\n",
      "Epoch 151, Loss: 0.47018789053701304\n",
      "Epoch 152, Loss: 0.4700460953124117\n",
      "Epoch 153, Loss: 0.4693645740016508\n",
      "Epoch 154, Loss: 0.4684558104180481\n",
      "Epoch 155, Loss: 0.4680960384979934\n",
      "Epoch 156, Loss: 0.4682454254529793\n",
      "Epoch 157, Loss: 0.4682974379105488\n",
      "Epoch 158, Loss: 0.4681590010361841\n",
      "Epoch 159, Loss: 0.4673225124817593\n",
      "Epoch 160, Loss: 0.4666701428107949\n",
      "Epoch 161, Loss: 0.4663166784414499\n",
      "Epoch 162, Loss: 0.46624854535037\n",
      "Epoch 163, Loss: 0.4664611566834947\n",
      "Epoch 164, Loss: 0.46654532450127006\n",
      "Epoch 165, Loss: 0.46721845607201035\n",
      "Epoch 166, Loss: 0.4668208160006968\n",
      "Epoch 167, Loss: 0.466573749669782\n",
      "Epoch 168, Loss: 0.46506387643568103\n",
      "Epoch 169, Loss: 0.4641220516950567\n",
      "Epoch 170, Loss: 0.4644550172656552\n",
      "Epoch 171, Loss: 0.4654613047116713\n",
      "Epoch 172, Loss: 0.46880176576246846\n",
      "Epoch 173, Loss: 0.4685649824581709\n",
      "Epoch 174, Loss: 0.46606115996454733\n",
      "Epoch 175, Loss: 0.46328263242364126\n",
      "Epoch 176, Loss: 0.4660484187313888\n",
      "Epoch 177, Loss: 0.4667669789465077\n",
      "Epoch 178, Loss: 0.46222975993575777\n",
      "Epoch 179, Loss: 0.46412214801342555\n",
      "Epoch 180, Loss: 0.46867384291927394\n",
      "Epoch 181, Loss: 0.46316269230838986\n",
      "Epoch 182, Loss: 0.4619791093960296\n",
      "Epoch 183, Loss: 0.4650783107301587\n",
      "Epoch 184, Loss: 0.4617210775403838\n",
      "Epoch 185, Loss: 0.4621011464022392\n",
      "Epoch 186, Loss: 0.4641367239628427\n",
      "Epoch 187, Loss: 0.46071131054180564\n",
      "Epoch 188, Loss: 0.46145892957673257\n",
      "Epoch 189, Loss: 0.464304908222076\n",
      "Epoch 190, Loss: 0.4606605441278432\n",
      "Epoch 191, Loss: 0.4601386308130386\n",
      "Epoch 192, Loss: 0.46193430495315263\n",
      "Epoch 193, Loss: 0.45977696912799226\n",
      "Epoch 194, Loss: 0.4597173803227511\n",
      "Epoch 195, Loss: 0.46096589922299847\n",
      "Epoch 196, Loss: 0.4592170612502741\n",
      "Epoch 197, Loss: 0.45864347419013624\n",
      "Epoch 198, Loss: 0.4594650511157848\n",
      "Epoch 199, Loss: 0.45924705942486493\n",
      "Epoch 200, Loss: 0.4584931199827434\n",
      "Epoch 201, Loss: 0.4578552480015059\n",
      "Epoch 202, Loss: 0.458251953125\n",
      "Epoch 203, Loss: 0.4586848316367108\n",
      "Epoch 204, Loss: 0.4577025349799131\n",
      "Epoch 205, Loss: 0.45707273702530193\n",
      "Epoch 206, Loss: 0.4571336158814618\n",
      "Epoch 207, Loss: 0.45746758002181026\n",
      "Epoch 208, Loss: 0.4580334166179955\n",
      "Epoch 209, Loss: 0.457551138813026\n",
      "Epoch 210, Loss: 0.4572461918817771\n",
      "Epoch 211, Loss: 0.45629613394883295\n",
      "Epoch 212, Loss: 0.4557945440718634\n",
      "Epoch 213, Loss: 0.455843155547024\n",
      "Epoch 214, Loss: 0.456219547378948\n",
      "Epoch 215, Loss: 0.4577116995236092\n",
      "Epoch 216, Loss: 0.4589176697652806\n",
      "Epoch 217, Loss: 0.46105034303609993\n",
      "Epoch 218, Loss: 0.4567326442394303\n",
      "Epoch 219, Loss: 0.45493112899589366\n",
      "Epoch 220, Loss: 0.45527199475335134\n",
      "Epoch 221, Loss: 0.45623630281032773\n",
      "Epoch 222, Loss: 0.45802054967026307\n",
      "Epoch 223, Loss: 0.45677224995978094\n",
      "Epoch 224, Loss: 0.45565914342704716\n",
      "Epoch 225, Loss: 0.45401674524485375\n",
      "Epoch 226, Loss: 0.4535057342791329\n",
      "Epoch 227, Loss: 0.45416164205845616\n",
      "Epoch 228, Loss: 0.45450513400796655\n",
      "Epoch 229, Loss: 0.4548238117351997\n",
      "Epoch 230, Loss: 0.4537990916311975\n",
      "Epoch 231, Loss: 0.4532521496977959\n",
      "Epoch 232, Loss: 0.4526084553640119\n",
      "Epoch 233, Loss: 0.45228981460773116\n",
      "Epoch 234, Loss: 0.4520949132230242\n",
      "Epoch 235, Loss: 0.45205797975469736\n",
      "Epoch 236, Loss: 0.45230557901617613\n",
      "Epoch 237, Loss: 0.4528345953247846\n",
      "Epoch 238, Loss: 0.4549386133976144\n",
      "Epoch 239, Loss: 0.4564022887834084\n",
      "Epoch 240, Loss: 0.45805755540321125\n",
      "Epoch 241, Loss: 0.4541498302115434\n",
      "Epoch 242, Loss: 0.45158727109343993\n",
      "Epoch 243, Loss: 0.45105347173746146\n",
      "Epoch 244, Loss: 0.45263971453254503\n",
      "Epoch 245, Loss: 0.4550743757756552\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23776659098351474\n",
      "Valence RMSE: 0.24360166989151721\n",
      "Arousal RMSE: 0.23178466299975128\n",
      "Test R^2 score: tensor([0.3659, 0.6038], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4848310994408865\n",
      "Num of epochs: 246\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Epoch 121, Loss: 0.4816228295571127\n",
      "Epoch 122, Loss: 0.4807618495484584\n",
      "Epoch 123, Loss: 0.48102030846138955\n",
      "Epoch 124, Loss: 0.4810176288364116\n",
      "Epoch 125, Loss: 0.48014338261661127\n",
      "Epoch 126, Loss: 0.47910314809270116\n",
      "Epoch 127, Loss: 0.47880795949683447\n",
      "Epoch 128, Loss: 0.47876177315142754\n",
      "Epoch 129, Loss: 0.47819597116449997\n",
      "Epoch 130, Loss: 0.47743214539542544\n",
      "Epoch 131, Loss: 0.47658476230340185\n",
      "Epoch 132, Loss: 0.4759644680184978\n",
      "Epoch 133, Loss: 0.4755635187712421\n",
      "Epoch 134, Loss: 0.47538264196584307\n",
      "Epoch 135, Loss: 0.4753673450596914\n",
      "Epoch 136, Loss: 0.4753697744168057\n",
      "Epoch 137, Loss: 0.4755973109583155\n",
      "Epoch 138, Loss: 0.4767626826605241\n",
      "Epoch 139, Loss: 0.47683528222287225\n",
      "Epoch 140, Loss: 0.47579445468261566\n",
      "Epoch 141, Loss: 0.47284533919249566\n",
      "Epoch 142, Loss: 0.472809286012082\n",
      "Epoch 143, Loss: 0.47458738111232796\n",
      "Epoch 144, Loss: 0.4733380058918715\n",
      "Epoch 145, Loss: 0.4715229034484008\n",
      "Epoch 146, Loss: 0.47096730966267386\n",
      "Epoch 147, Loss: 0.47161469890467383\n",
      "Epoch 148, Loss: 0.47183531411129226\n",
      "Epoch 149, Loss: 0.470294933688076\n",
      "Epoch 150, Loss: 0.4697114144973765\n",
      "Epoch 151, Loss: 0.47018789053701304\n",
      "Epoch 152, Loss: 0.4700460953124117\n",
      "Epoch 153, Loss: 0.4693645740016508\n",
      "Epoch 154, Loss: 0.4684558104180481\n",
      "Epoch 155, Loss: 0.4680960384979934\n",
      "Epoch 156, Loss: 0.4682454254529793\n",
      "Epoch 157, Loss: 0.4682974379105488\n",
      "Epoch 158, Loss: 0.4681590010361841\n",
      "Epoch 159, Loss: 0.4673225124817593\n",
      "Epoch 160, Loss: 0.4666701428107949\n",
      "Epoch 161, Loss: 0.4663166784414499\n",
      "Epoch 162, Loss: 0.46624854535037\n",
      "Epoch 163, Loss: 0.4664611566834947\n",
      "Epoch 164, Loss: 0.46654532450127006\n",
      "Epoch 165, Loss: 0.46721845607201035\n",
      "Epoch 166, Loss: 0.4668208160006968\n",
      "Epoch 167, Loss: 0.466573749669782\n",
      "Epoch 168, Loss: 0.46506387643568103\n",
      "Epoch 169, Loss: 0.4641220516950567\n",
      "Epoch 170, Loss: 0.4644550172656552\n",
      "Epoch 171, Loss: 0.4654613047116713\n",
      "Epoch 172, Loss: 0.46880176576246846\n",
      "Epoch 173, Loss: 0.4685649824581709\n",
      "Epoch 174, Loss: 0.46606115996454733\n",
      "Epoch 175, Loss: 0.46328263242364126\n",
      "Epoch 176, Loss: 0.4660484187313888\n",
      "Epoch 177, Loss: 0.4667669789465077\n",
      "Epoch 178, Loss: 0.46222975993575777\n",
      "Epoch 179, Loss: 0.46412214801342555\n",
      "Epoch 180, Loss: 0.46867384291927394\n",
      "Epoch 181, Loss: 0.46316269230838986\n",
      "Epoch 182, Loss: 0.4619791093960296\n",
      "Epoch 183, Loss: 0.4650783107301587\n",
      "Epoch 184, Loss: 0.4617210775403838\n",
      "Epoch 185, Loss: 0.4621011464022392\n",
      "Epoch 186, Loss: 0.4641367239628427\n",
      "Epoch 187, Loss: 0.46071131054180564\n",
      "Epoch 188, Loss: 0.46145892957673257\n",
      "Epoch 189, Loss: 0.464304908222076\n",
      "Epoch 190, Loss: 0.4606605441278432\n",
      "Epoch 191, Loss: 0.4601386308130386\n",
      "Epoch 192, Loss: 0.46193430495315263\n",
      "Epoch 193, Loss: 0.45977696912799226\n",
      "Epoch 194, Loss: 0.4597173803227511\n",
      "Epoch 195, Loss: 0.46096589922299847\n",
      "Epoch 196, Loss: 0.4592170612502741\n",
      "Epoch 197, Loss: 0.45864347419013624\n",
      "Epoch 198, Loss: 0.4594650511157848\n",
      "Epoch 199, Loss: 0.45924705942486493\n",
      "Epoch 200, Loss: 0.4584931199827434\n",
      "Epoch 201, Loss: 0.4578552480015059\n",
      "Epoch 202, Loss: 0.458251953125\n",
      "Epoch 203, Loss: 0.4586848316367108\n",
      "Epoch 204, Loss: 0.4577025349799131\n",
      "Epoch 205, Loss: 0.45707273702530193\n",
      "Epoch 206, Loss: 0.4571336158814618\n",
      "Epoch 207, Loss: 0.45746758002181026\n",
      "Epoch 208, Loss: 0.4580334166179955\n",
      "Epoch 209, Loss: 0.457551138813026\n",
      "Epoch 210, Loss: 0.4572461918817771\n",
      "Epoch 211, Loss: 0.45629613394883295\n",
      "Epoch 212, Loss: 0.4557945440718634\n",
      "Epoch 213, Loss: 0.455843155547024\n",
      "Epoch 214, Loss: 0.456219547378948\n",
      "Epoch 215, Loss: 0.4577116995236092\n",
      "Epoch 216, Loss: 0.4589176697652806\n",
      "Epoch 217, Loss: 0.46105034303609993\n",
      "Epoch 218, Loss: 0.4567326442394303\n",
      "Epoch 219, Loss: 0.45493112899589366\n",
      "Epoch 220, Loss: 0.45527199475335134\n",
      "Epoch 221, Loss: 0.45623630281032773\n",
      "Epoch 222, Loss: 0.45802054967026307\n",
      "Epoch 223, Loss: 0.45677224995978094\n",
      "Epoch 224, Loss: 0.45565914342704716\n",
      "Epoch 225, Loss: 0.45401674524485375\n",
      "Epoch 226, Loss: 0.4535057342791329\n",
      "Epoch 227, Loss: 0.45416164205845616\n",
      "Epoch 228, Loss: 0.45450513400796655\n",
      "Epoch 229, Loss: 0.4548238117351997\n",
      "Epoch 230, Loss: 0.4537990916311975\n",
      "Epoch 231, Loss: 0.4532521496977959\n",
      "Epoch 232, Loss: 0.4526084553640119\n",
      "Epoch 233, Loss: 0.45228981460773116\n",
      "Epoch 234, Loss: 0.4520949132230242\n",
      "Epoch 235, Loss: 0.45205797975469736\n",
      "Epoch 236, Loss: 0.45230557901617613\n",
      "Epoch 237, Loss: 0.4528345953247846\n",
      "Epoch 238, Loss: 0.4549386133976144\n",
      "Epoch 239, Loss: 0.4564022887834084\n",
      "Epoch 240, Loss: 0.45805755540321125\n",
      "Epoch 241, Loss: 0.4541498302115434\n",
      "Epoch 242, Loss: 0.45158727109343993\n",
      "Epoch 243, Loss: 0.45105347173746146\n",
      "Epoch 244, Loss: 0.45263971453254503\n",
      "Epoch 245, Loss: 0.4550743757756552\n",
      "Epoch 246, Loss: 0.4549471294161167\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2389375382316486\n",
      "Valence RMSE: 0.24937098109848285\n",
      "Arousal RMSE: 0.22802720920622868\n",
      "Test R^2 score: tensor([0.3355, 0.6165], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4760059837136018\n",
      "Num of epochs: 247\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Epoch 121, Loss: 0.4816228295571127\n",
      "Epoch 122, Loss: 0.4807618495484584\n",
      "Epoch 123, Loss: 0.48102030846138955\n",
      "Epoch 124, Loss: 0.4810176288364116\n",
      "Epoch 125, Loss: 0.48014338261661127\n",
      "Epoch 126, Loss: 0.47910314809270116\n",
      "Epoch 127, Loss: 0.47880795949683447\n",
      "Epoch 128, Loss: 0.47876177315142754\n",
      "Epoch 129, Loss: 0.47819597116449997\n",
      "Epoch 130, Loss: 0.47743214539542544\n",
      "Epoch 131, Loss: 0.47658476230340185\n",
      "Epoch 132, Loss: 0.4759644680184978\n",
      "Epoch 133, Loss: 0.4755635187712421\n",
      "Epoch 134, Loss: 0.47538264196584307\n",
      "Epoch 135, Loss: 0.4753673450596914\n",
      "Epoch 136, Loss: 0.4753697744168057\n",
      "Epoch 137, Loss: 0.4755973109583155\n",
      "Epoch 138, Loss: 0.4767626826605241\n",
      "Epoch 139, Loss: 0.47683528222287225\n",
      "Epoch 140, Loss: 0.47579445468261566\n",
      "Epoch 141, Loss: 0.47284533919249566\n",
      "Epoch 142, Loss: 0.472809286012082\n",
      "Epoch 143, Loss: 0.47458738111232796\n",
      "Epoch 144, Loss: 0.4733380058918715\n",
      "Epoch 145, Loss: 0.4715229034484008\n",
      "Epoch 146, Loss: 0.47096730966267386\n",
      "Epoch 147, Loss: 0.47161469890467383\n",
      "Epoch 148, Loss: 0.47183531411129226\n",
      "Epoch 149, Loss: 0.470294933688076\n",
      "Epoch 150, Loss: 0.4697114144973765\n",
      "Epoch 151, Loss: 0.47018789053701304\n",
      "Epoch 152, Loss: 0.4700460953124117\n",
      "Epoch 153, Loss: 0.4693645740016508\n",
      "Epoch 154, Loss: 0.4684558104180481\n",
      "Epoch 155, Loss: 0.4680960384979934\n",
      "Epoch 156, Loss: 0.4682454254529793\n",
      "Epoch 157, Loss: 0.4682974379105488\n",
      "Epoch 158, Loss: 0.4681590010361841\n",
      "Epoch 159, Loss: 0.4673225124817593\n",
      "Epoch 160, Loss: 0.4666701428107949\n",
      "Epoch 161, Loss: 0.4663166784414499\n",
      "Epoch 162, Loss: 0.46624854535037\n",
      "Epoch 163, Loss: 0.4664611566834947\n",
      "Epoch 164, Loss: 0.46654532450127006\n",
      "Epoch 165, Loss: 0.46721845607201035\n",
      "Epoch 166, Loss: 0.4668208160006968\n",
      "Epoch 167, Loss: 0.466573749669782\n",
      "Epoch 168, Loss: 0.46506387643568103\n",
      "Epoch 169, Loss: 0.4641220516950567\n",
      "Epoch 170, Loss: 0.4644550172656552\n",
      "Epoch 171, Loss: 0.4654613047116713\n",
      "Epoch 172, Loss: 0.46880176576246846\n",
      "Epoch 173, Loss: 0.4685649824581709\n",
      "Epoch 174, Loss: 0.46606115996454733\n",
      "Epoch 175, Loss: 0.46328263242364126\n",
      "Epoch 176, Loss: 0.4660484187313888\n",
      "Epoch 177, Loss: 0.4667669789465077\n",
      "Epoch 178, Loss: 0.46222975993575777\n",
      "Epoch 179, Loss: 0.46412214801342555\n",
      "Epoch 180, Loss: 0.46867384291927394\n",
      "Epoch 181, Loss: 0.46316269230838986\n",
      "Epoch 182, Loss: 0.4619791093960296\n",
      "Epoch 183, Loss: 0.4650783107301587\n",
      "Epoch 184, Loss: 0.4617210775403838\n",
      "Epoch 185, Loss: 0.4621011464022392\n",
      "Epoch 186, Loss: 0.4641367239628427\n",
      "Epoch 187, Loss: 0.46071131054180564\n",
      "Epoch 188, Loss: 0.46145892957673257\n",
      "Epoch 189, Loss: 0.464304908222076\n",
      "Epoch 190, Loss: 0.4606605441278432\n",
      "Epoch 191, Loss: 0.4601386308130386\n",
      "Epoch 192, Loss: 0.46193430495315263\n",
      "Epoch 193, Loss: 0.45977696912799226\n",
      "Epoch 194, Loss: 0.4597173803227511\n",
      "Epoch 195, Loss: 0.46096589922299847\n",
      "Epoch 196, Loss: 0.4592170612502741\n",
      "Epoch 197, Loss: 0.45864347419013624\n",
      "Epoch 198, Loss: 0.4594650511157848\n",
      "Epoch 199, Loss: 0.45924705942486493\n",
      "Epoch 200, Loss: 0.4584931199827434\n",
      "Epoch 201, Loss: 0.4578552480015059\n",
      "Epoch 202, Loss: 0.458251953125\n",
      "Epoch 203, Loss: 0.4586848316367108\n",
      "Epoch 204, Loss: 0.4577025349799131\n",
      "Epoch 205, Loss: 0.45707273702530193\n",
      "Epoch 206, Loss: 0.4571336158814618\n",
      "Epoch 207, Loss: 0.45746758002181026\n",
      "Epoch 208, Loss: 0.4580334166179955\n",
      "Epoch 209, Loss: 0.457551138813026\n",
      "Epoch 210, Loss: 0.4572461918817771\n",
      "Epoch 211, Loss: 0.45629613394883295\n",
      "Epoch 212, Loss: 0.4557945440718634\n",
      "Epoch 213, Loss: 0.455843155547024\n",
      "Epoch 214, Loss: 0.456219547378948\n",
      "Epoch 215, Loss: 0.4577116995236092\n",
      "Epoch 216, Loss: 0.4589176697652806\n",
      "Epoch 217, Loss: 0.46105034303609993\n",
      "Epoch 218, Loss: 0.4567326442394303\n",
      "Epoch 219, Loss: 0.45493112899589366\n",
      "Epoch 220, Loss: 0.45527199475335134\n",
      "Epoch 221, Loss: 0.45623630281032773\n",
      "Epoch 222, Loss: 0.45802054967026307\n",
      "Epoch 223, Loss: 0.45677224995978094\n",
      "Epoch 224, Loss: 0.45565914342704716\n",
      "Epoch 225, Loss: 0.45401674524485375\n",
      "Epoch 226, Loss: 0.4535057342791329\n",
      "Epoch 227, Loss: 0.45416164205845616\n",
      "Epoch 228, Loss: 0.45450513400796655\n",
      "Epoch 229, Loss: 0.4548238117351997\n",
      "Epoch 230, Loss: 0.4537990916311975\n",
      "Epoch 231, Loss: 0.4532521496977959\n",
      "Epoch 232, Loss: 0.4526084553640119\n",
      "Epoch 233, Loss: 0.45228981460773116\n",
      "Epoch 234, Loss: 0.4520949132230242\n",
      "Epoch 235, Loss: 0.45205797975469736\n",
      "Epoch 236, Loss: 0.45230557901617613\n",
      "Epoch 237, Loss: 0.4528345953247846\n",
      "Epoch 238, Loss: 0.4549386133976144\n",
      "Epoch 239, Loss: 0.4564022887834084\n",
      "Epoch 240, Loss: 0.45805755540321125\n",
      "Epoch 241, Loss: 0.4541498302115434\n",
      "Epoch 242, Loss: 0.45158727109343993\n",
      "Epoch 243, Loss: 0.45105347173746146\n",
      "Epoch 244, Loss: 0.45263971453254503\n",
      "Epoch 245, Loss: 0.4550743757756552\n",
      "Epoch 246, Loss: 0.4549471294161167\n",
      "Epoch 247, Loss: 0.45361945699341477\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2353038044952788\n",
      "Valence RMSE: 0.24318804339234787\n",
      "Arousal RMSE: 0.22714606835890042\n",
      "Test R^2 score: tensor([0.3680, 0.6195], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.493756787749003\n",
      "Num of epochs: 248\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Epoch 121, Loss: 0.4816228295571127\n",
      "Epoch 122, Loss: 0.4807618495484584\n",
      "Epoch 123, Loss: 0.48102030846138955\n",
      "Epoch 124, Loss: 0.4810176288364116\n",
      "Epoch 125, Loss: 0.48014338261661127\n",
      "Epoch 126, Loss: 0.47910314809270116\n",
      "Epoch 127, Loss: 0.47880795949683447\n",
      "Epoch 128, Loss: 0.47876177315142754\n",
      "Epoch 129, Loss: 0.47819597116449997\n",
      "Epoch 130, Loss: 0.47743214539542544\n",
      "Epoch 131, Loss: 0.47658476230340185\n",
      "Epoch 132, Loss: 0.4759644680184978\n",
      "Epoch 133, Loss: 0.4755635187712421\n",
      "Epoch 134, Loss: 0.47538264196584307\n",
      "Epoch 135, Loss: 0.4753673450596914\n",
      "Epoch 136, Loss: 0.4753697744168057\n",
      "Epoch 137, Loss: 0.4755973109583155\n",
      "Epoch 138, Loss: 0.4767626826605241\n",
      "Epoch 139, Loss: 0.47683528222287225\n",
      "Epoch 140, Loss: 0.47579445468261566\n",
      "Epoch 141, Loss: 0.47284533919249566\n",
      "Epoch 142, Loss: 0.472809286012082\n",
      "Epoch 143, Loss: 0.47458738111232796\n",
      "Epoch 144, Loss: 0.4733380058918715\n",
      "Epoch 145, Loss: 0.4715229034484008\n",
      "Epoch 146, Loss: 0.47096730966267386\n",
      "Epoch 147, Loss: 0.47161469890467383\n",
      "Epoch 148, Loss: 0.47183531411129226\n",
      "Epoch 149, Loss: 0.470294933688076\n",
      "Epoch 150, Loss: 0.4697114144973765\n",
      "Epoch 151, Loss: 0.47018789053701304\n",
      "Epoch 152, Loss: 0.4700460953124117\n",
      "Epoch 153, Loss: 0.4693645740016508\n",
      "Epoch 154, Loss: 0.4684558104180481\n",
      "Epoch 155, Loss: 0.4680960384979934\n",
      "Epoch 156, Loss: 0.4682454254529793\n",
      "Epoch 157, Loss: 0.4682974379105488\n",
      "Epoch 158, Loss: 0.4681590010361841\n",
      "Epoch 159, Loss: 0.4673225124817593\n",
      "Epoch 160, Loss: 0.4666701428107949\n",
      "Epoch 161, Loss: 0.4663166784414499\n",
      "Epoch 162, Loss: 0.46624854535037\n",
      "Epoch 163, Loss: 0.4664611566834947\n",
      "Epoch 164, Loss: 0.46654532450127006\n",
      "Epoch 165, Loss: 0.46721845607201035\n",
      "Epoch 166, Loss: 0.4668208160006968\n",
      "Epoch 167, Loss: 0.466573749669782\n",
      "Epoch 168, Loss: 0.46506387643568103\n",
      "Epoch 169, Loss: 0.4641220516950567\n",
      "Epoch 170, Loss: 0.4644550172656552\n",
      "Epoch 171, Loss: 0.4654613047116713\n",
      "Epoch 172, Loss: 0.46880176576246846\n",
      "Epoch 173, Loss: 0.4685649824581709\n",
      "Epoch 174, Loss: 0.46606115996454733\n",
      "Epoch 175, Loss: 0.46328263242364126\n",
      "Epoch 176, Loss: 0.4660484187313888\n",
      "Epoch 177, Loss: 0.4667669789465077\n",
      "Epoch 178, Loss: 0.46222975993575777\n",
      "Epoch 179, Loss: 0.46412214801342555\n",
      "Epoch 180, Loss: 0.46867384291927394\n",
      "Epoch 181, Loss: 0.46316269230838986\n",
      "Epoch 182, Loss: 0.4619791093960296\n",
      "Epoch 183, Loss: 0.4650783107301587\n",
      "Epoch 184, Loss: 0.4617210775403838\n",
      "Epoch 185, Loss: 0.4621011464022392\n",
      "Epoch 186, Loss: 0.4641367239628427\n",
      "Epoch 187, Loss: 0.46071131054180564\n",
      "Epoch 188, Loss: 0.46145892957673257\n",
      "Epoch 189, Loss: 0.464304908222076\n",
      "Epoch 190, Loss: 0.4606605441278432\n",
      "Epoch 191, Loss: 0.4601386308130386\n",
      "Epoch 192, Loss: 0.46193430495315263\n",
      "Epoch 193, Loss: 0.45977696912799226\n",
      "Epoch 194, Loss: 0.4597173803227511\n",
      "Epoch 195, Loss: 0.46096589922299847\n",
      "Epoch 196, Loss: 0.4592170612502741\n",
      "Epoch 197, Loss: 0.45864347419013624\n",
      "Epoch 198, Loss: 0.4594650511157848\n",
      "Epoch 199, Loss: 0.45924705942486493\n",
      "Epoch 200, Loss: 0.4584931199827434\n",
      "Epoch 201, Loss: 0.4578552480015059\n",
      "Epoch 202, Loss: 0.458251953125\n",
      "Epoch 203, Loss: 0.4586848316367108\n",
      "Epoch 204, Loss: 0.4577025349799131\n",
      "Epoch 205, Loss: 0.45707273702530193\n",
      "Epoch 206, Loss: 0.4571336158814618\n",
      "Epoch 207, Loss: 0.45746758002181026\n",
      "Epoch 208, Loss: 0.4580334166179955\n",
      "Epoch 209, Loss: 0.457551138813026\n",
      "Epoch 210, Loss: 0.4572461918817771\n",
      "Epoch 211, Loss: 0.45629613394883295\n",
      "Epoch 212, Loss: 0.4557945440718634\n",
      "Epoch 213, Loss: 0.455843155547024\n",
      "Epoch 214, Loss: 0.456219547378948\n",
      "Epoch 215, Loss: 0.4577116995236092\n",
      "Epoch 216, Loss: 0.4589176697652806\n",
      "Epoch 217, Loss: 0.46105034303609993\n",
      "Epoch 218, Loss: 0.4567326442394303\n",
      "Epoch 219, Loss: 0.45493112899589366\n",
      "Epoch 220, Loss: 0.45527199475335134\n",
      "Epoch 221, Loss: 0.45623630281032773\n",
      "Epoch 222, Loss: 0.45802054967026307\n",
      "Epoch 223, Loss: 0.45677224995978094\n",
      "Epoch 224, Loss: 0.45565914342704716\n",
      "Epoch 225, Loss: 0.45401674524485375\n",
      "Epoch 226, Loss: 0.4535057342791329\n",
      "Epoch 227, Loss: 0.45416164205845616\n",
      "Epoch 228, Loss: 0.45450513400796655\n",
      "Epoch 229, Loss: 0.4548238117351997\n",
      "Epoch 230, Loss: 0.4537990916311975\n",
      "Epoch 231, Loss: 0.4532521496977959\n",
      "Epoch 232, Loss: 0.4526084553640119\n",
      "Epoch 233, Loss: 0.45228981460773116\n",
      "Epoch 234, Loss: 0.4520949132230242\n",
      "Epoch 235, Loss: 0.45205797975469736\n",
      "Epoch 236, Loss: 0.45230557901617613\n",
      "Epoch 237, Loss: 0.4528345953247846\n",
      "Epoch 238, Loss: 0.4549386133976144\n",
      "Epoch 239, Loss: 0.4564022887834084\n",
      "Epoch 240, Loss: 0.45805755540321125\n",
      "Epoch 241, Loss: 0.4541498302115434\n",
      "Epoch 242, Loss: 0.45158727109343993\n",
      "Epoch 243, Loss: 0.45105347173746146\n",
      "Epoch 244, Loss: 0.45263971453254503\n",
      "Epoch 245, Loss: 0.4550743757756552\n",
      "Epoch 246, Loss: 0.4549471294161167\n",
      "Epoch 247, Loss: 0.45361945699341477\n",
      "Epoch 248, Loss: 0.4506913801287277\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2351180030496099\n",
      "Valence RMSE: 0.24388650623368532\n",
      "Arousal RMSE: 0.22600956349942206\n",
      "Test R^2 score: tensor([0.3644, 0.6233], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4938381872586483\n",
      "Num of epochs: 249\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Epoch 121, Loss: 0.4816228295571127\n",
      "Epoch 122, Loss: 0.4807618495484584\n",
      "Epoch 123, Loss: 0.48102030846138955\n",
      "Epoch 124, Loss: 0.4810176288364116\n",
      "Epoch 125, Loss: 0.48014338261661127\n",
      "Epoch 126, Loss: 0.47910314809270116\n",
      "Epoch 127, Loss: 0.47880795949683447\n",
      "Epoch 128, Loss: 0.47876177315142754\n",
      "Epoch 129, Loss: 0.47819597116449997\n",
      "Epoch 130, Loss: 0.47743214539542544\n",
      "Epoch 131, Loss: 0.47658476230340185\n",
      "Epoch 132, Loss: 0.4759644680184978\n",
      "Epoch 133, Loss: 0.4755635187712421\n",
      "Epoch 134, Loss: 0.47538264196584307\n",
      "Epoch 135, Loss: 0.4753673450596914\n",
      "Epoch 136, Loss: 0.4753697744168057\n",
      "Epoch 137, Loss: 0.4755973109583155\n",
      "Epoch 138, Loss: 0.4767626826605241\n",
      "Epoch 139, Loss: 0.47683528222287225\n",
      "Epoch 140, Loss: 0.47579445468261566\n",
      "Epoch 141, Loss: 0.47284533919249566\n",
      "Epoch 142, Loss: 0.472809286012082\n",
      "Epoch 143, Loss: 0.47458738111232796\n",
      "Epoch 144, Loss: 0.4733380058918715\n",
      "Epoch 145, Loss: 0.4715229034484008\n",
      "Epoch 146, Loss: 0.47096730966267386\n",
      "Epoch 147, Loss: 0.47161469890467383\n",
      "Epoch 148, Loss: 0.47183531411129226\n",
      "Epoch 149, Loss: 0.470294933688076\n",
      "Epoch 150, Loss: 0.4697114144973765\n",
      "Epoch 151, Loss: 0.47018789053701304\n",
      "Epoch 152, Loss: 0.4700460953124117\n",
      "Epoch 153, Loss: 0.4693645740016508\n",
      "Epoch 154, Loss: 0.4684558104180481\n",
      "Epoch 155, Loss: 0.4680960384979934\n",
      "Epoch 156, Loss: 0.4682454254529793\n",
      "Epoch 157, Loss: 0.4682974379105488\n",
      "Epoch 158, Loss: 0.4681590010361841\n",
      "Epoch 159, Loss: 0.4673225124817593\n",
      "Epoch 160, Loss: 0.4666701428107949\n",
      "Epoch 161, Loss: 0.4663166784414499\n",
      "Epoch 162, Loss: 0.46624854535037\n",
      "Epoch 163, Loss: 0.4664611566834947\n",
      "Epoch 164, Loss: 0.46654532450127006\n",
      "Epoch 165, Loss: 0.46721845607201035\n",
      "Epoch 166, Loss: 0.4668208160006968\n",
      "Epoch 167, Loss: 0.466573749669782\n",
      "Epoch 168, Loss: 0.46506387643568103\n",
      "Epoch 169, Loss: 0.4641220516950567\n",
      "Epoch 170, Loss: 0.4644550172656552\n",
      "Epoch 171, Loss: 0.4654613047116713\n",
      "Epoch 172, Loss: 0.46880176576246846\n",
      "Epoch 173, Loss: 0.4685649824581709\n",
      "Epoch 174, Loss: 0.46606115996454733\n",
      "Epoch 175, Loss: 0.46328263242364126\n",
      "Epoch 176, Loss: 0.4660484187313888\n",
      "Epoch 177, Loss: 0.4667669789465077\n",
      "Epoch 178, Loss: 0.46222975993575777\n",
      "Epoch 179, Loss: 0.46412214801342555\n",
      "Epoch 180, Loss: 0.46867384291927394\n",
      "Epoch 181, Loss: 0.46316269230838986\n",
      "Epoch 182, Loss: 0.4619791093960296\n",
      "Epoch 183, Loss: 0.4650783107301587\n",
      "Epoch 184, Loss: 0.4617210775403838\n",
      "Epoch 185, Loss: 0.4621011464022392\n",
      "Epoch 186, Loss: 0.4641367239628427\n",
      "Epoch 187, Loss: 0.46071131054180564\n",
      "Epoch 188, Loss: 0.46145892957673257\n",
      "Epoch 189, Loss: 0.464304908222076\n",
      "Epoch 190, Loss: 0.4606605441278432\n",
      "Epoch 191, Loss: 0.4601386308130386\n",
      "Epoch 192, Loss: 0.46193430495315263\n",
      "Epoch 193, Loss: 0.45977696912799226\n",
      "Epoch 194, Loss: 0.4597173803227511\n",
      "Epoch 195, Loss: 0.46096589922299847\n",
      "Epoch 196, Loss: 0.4592170612502741\n",
      "Epoch 197, Loss: 0.45864347419013624\n",
      "Epoch 198, Loss: 0.4594650511157848\n",
      "Epoch 199, Loss: 0.45924705942486493\n",
      "Epoch 200, Loss: 0.4584931199827434\n",
      "Epoch 201, Loss: 0.4578552480015059\n",
      "Epoch 202, Loss: 0.458251953125\n",
      "Epoch 203, Loss: 0.4586848316367108\n",
      "Epoch 204, Loss: 0.4577025349799131\n",
      "Epoch 205, Loss: 0.45707273702530193\n",
      "Epoch 206, Loss: 0.4571336158814618\n",
      "Epoch 207, Loss: 0.45746758002181026\n",
      "Epoch 208, Loss: 0.4580334166179955\n",
      "Epoch 209, Loss: 0.457551138813026\n",
      "Epoch 210, Loss: 0.4572461918817771\n",
      "Epoch 211, Loss: 0.45629613394883295\n",
      "Epoch 212, Loss: 0.4557945440718634\n",
      "Epoch 213, Loss: 0.455843155547024\n",
      "Epoch 214, Loss: 0.456219547378948\n",
      "Epoch 215, Loss: 0.4577116995236092\n",
      "Epoch 216, Loss: 0.4589176697652806\n",
      "Epoch 217, Loss: 0.46105034303609993\n",
      "Epoch 218, Loss: 0.4567326442394303\n",
      "Epoch 219, Loss: 0.45493112899589366\n",
      "Epoch 220, Loss: 0.45527199475335134\n",
      "Epoch 221, Loss: 0.45623630281032773\n",
      "Epoch 222, Loss: 0.45802054967026307\n",
      "Epoch 223, Loss: 0.45677224995978094\n",
      "Epoch 224, Loss: 0.45565914342704716\n",
      "Epoch 225, Loss: 0.45401674524485375\n",
      "Epoch 226, Loss: 0.4535057342791329\n",
      "Epoch 227, Loss: 0.45416164205845616\n",
      "Epoch 228, Loss: 0.45450513400796655\n",
      "Epoch 229, Loss: 0.4548238117351997\n",
      "Epoch 230, Loss: 0.4537990916311975\n",
      "Epoch 231, Loss: 0.4532521496977959\n",
      "Epoch 232, Loss: 0.4526084553640119\n",
      "Epoch 233, Loss: 0.45228981460773116\n",
      "Epoch 234, Loss: 0.4520949132230242\n",
      "Epoch 235, Loss: 0.45205797975469736\n",
      "Epoch 236, Loss: 0.45230557901617613\n",
      "Epoch 237, Loss: 0.4528345953247846\n",
      "Epoch 238, Loss: 0.4549386133976144\n",
      "Epoch 239, Loss: 0.4564022887834084\n",
      "Epoch 240, Loss: 0.45805755540321125\n",
      "Epoch 241, Loss: 0.4541498302115434\n",
      "Epoch 242, Loss: 0.45158727109343993\n",
      "Epoch 243, Loss: 0.45105347173746146\n",
      "Epoch 244, Loss: 0.45263971453254503\n",
      "Epoch 245, Loss: 0.4550743757756552\n",
      "Epoch 246, Loss: 0.4549471294161167\n",
      "Epoch 247, Loss: 0.45361945699341477\n",
      "Epoch 248, Loss: 0.4506913801287277\n",
      "Epoch 249, Loss: 0.44996455105922556\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23629057016489247\n",
      "Valence RMSE: 0.24653689718425187\n",
      "Arousal RMSE: 0.22557931071900536\n",
      "Test R^2 score: tensor([0.3505, 0.6247], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.48760976789381244\n",
      "Num of epochs: 250\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Epoch 121, Loss: 0.4816228295571127\n",
      "Epoch 122, Loss: 0.4807618495484584\n",
      "Epoch 123, Loss: 0.48102030846138955\n",
      "Epoch 124, Loss: 0.4810176288364116\n",
      "Epoch 125, Loss: 0.48014338261661127\n",
      "Epoch 126, Loss: 0.47910314809270116\n",
      "Epoch 127, Loss: 0.47880795949683447\n",
      "Epoch 128, Loss: 0.47876177315142754\n",
      "Epoch 129, Loss: 0.47819597116449997\n",
      "Epoch 130, Loss: 0.47743214539542544\n",
      "Epoch 131, Loss: 0.47658476230340185\n",
      "Epoch 132, Loss: 0.4759644680184978\n",
      "Epoch 133, Loss: 0.4755635187712421\n",
      "Epoch 134, Loss: 0.47538264196584307\n",
      "Epoch 135, Loss: 0.4753673450596914\n",
      "Epoch 136, Loss: 0.4753697744168057\n",
      "Epoch 137, Loss: 0.4755973109583155\n",
      "Epoch 138, Loss: 0.4767626826605241\n",
      "Epoch 139, Loss: 0.47683528222287225\n",
      "Epoch 140, Loss: 0.47579445468261566\n",
      "Epoch 141, Loss: 0.47284533919249566\n",
      "Epoch 142, Loss: 0.472809286012082\n",
      "Epoch 143, Loss: 0.47458738111232796\n",
      "Epoch 144, Loss: 0.4733380058918715\n",
      "Epoch 145, Loss: 0.4715229034484008\n",
      "Epoch 146, Loss: 0.47096730966267386\n",
      "Epoch 147, Loss: 0.47161469890467383\n",
      "Epoch 148, Loss: 0.47183531411129226\n",
      "Epoch 149, Loss: 0.470294933688076\n",
      "Epoch 150, Loss: 0.4697114144973765\n",
      "Epoch 151, Loss: 0.47018789053701304\n",
      "Epoch 152, Loss: 0.4700460953124117\n",
      "Epoch 153, Loss: 0.4693645740016508\n",
      "Epoch 154, Loss: 0.4684558104180481\n",
      "Epoch 155, Loss: 0.4680960384979934\n",
      "Epoch 156, Loss: 0.4682454254529793\n",
      "Epoch 157, Loss: 0.4682974379105488\n",
      "Epoch 158, Loss: 0.4681590010361841\n",
      "Epoch 159, Loss: 0.4673225124817593\n",
      "Epoch 160, Loss: 0.4666701428107949\n",
      "Epoch 161, Loss: 0.4663166784414499\n",
      "Epoch 162, Loss: 0.46624854535037\n",
      "Epoch 163, Loss: 0.4664611566834947\n",
      "Epoch 164, Loss: 0.46654532450127006\n",
      "Epoch 165, Loss: 0.46721845607201035\n",
      "Epoch 166, Loss: 0.4668208160006968\n",
      "Epoch 167, Loss: 0.466573749669782\n",
      "Epoch 168, Loss: 0.46506387643568103\n",
      "Epoch 169, Loss: 0.4641220516950567\n",
      "Epoch 170, Loss: 0.4644550172656552\n",
      "Epoch 171, Loss: 0.4654613047116713\n",
      "Epoch 172, Loss: 0.46880176576246846\n",
      "Epoch 173, Loss: 0.4685649824581709\n",
      "Epoch 174, Loss: 0.46606115996454733\n",
      "Epoch 175, Loss: 0.46328263242364126\n",
      "Epoch 176, Loss: 0.4660484187313888\n",
      "Epoch 177, Loss: 0.4667669789465077\n",
      "Epoch 178, Loss: 0.46222975993575777\n",
      "Epoch 179, Loss: 0.46412214801342555\n",
      "Epoch 180, Loss: 0.46867384291927394\n",
      "Epoch 181, Loss: 0.46316269230838986\n",
      "Epoch 182, Loss: 0.4619791093960296\n",
      "Epoch 183, Loss: 0.4650783107301587\n",
      "Epoch 184, Loss: 0.4617210775403838\n",
      "Epoch 185, Loss: 0.4621011464022392\n",
      "Epoch 186, Loss: 0.4641367239628427\n",
      "Epoch 187, Loss: 0.46071131054180564\n",
      "Epoch 188, Loss: 0.46145892957673257\n",
      "Epoch 189, Loss: 0.464304908222076\n",
      "Epoch 190, Loss: 0.4606605441278432\n",
      "Epoch 191, Loss: 0.4601386308130386\n",
      "Epoch 192, Loss: 0.46193430495315263\n",
      "Epoch 193, Loss: 0.45977696912799226\n",
      "Epoch 194, Loss: 0.4597173803227511\n",
      "Epoch 195, Loss: 0.46096589922299847\n",
      "Epoch 196, Loss: 0.4592170612502741\n",
      "Epoch 197, Loss: 0.45864347419013624\n",
      "Epoch 198, Loss: 0.4594650511157848\n",
      "Epoch 199, Loss: 0.45924705942486493\n",
      "Epoch 200, Loss: 0.4584931199827434\n",
      "Epoch 201, Loss: 0.4578552480015059\n",
      "Epoch 202, Loss: 0.458251953125\n",
      "Epoch 203, Loss: 0.4586848316367108\n",
      "Epoch 204, Loss: 0.4577025349799131\n",
      "Epoch 205, Loss: 0.45707273702530193\n",
      "Epoch 206, Loss: 0.4571336158814618\n",
      "Epoch 207, Loss: 0.45746758002181026\n",
      "Epoch 208, Loss: 0.4580334166179955\n",
      "Epoch 209, Loss: 0.457551138813026\n",
      "Epoch 210, Loss: 0.4572461918817771\n",
      "Epoch 211, Loss: 0.45629613394883295\n",
      "Epoch 212, Loss: 0.4557945440718634\n",
      "Epoch 213, Loss: 0.455843155547024\n",
      "Epoch 214, Loss: 0.456219547378948\n",
      "Epoch 215, Loss: 0.4577116995236092\n",
      "Epoch 216, Loss: 0.4589176697652806\n",
      "Epoch 217, Loss: 0.46105034303609993\n",
      "Epoch 218, Loss: 0.4567326442394303\n",
      "Epoch 219, Loss: 0.45493112899589366\n",
      "Epoch 220, Loss: 0.45527199475335134\n",
      "Epoch 221, Loss: 0.45623630281032773\n",
      "Epoch 222, Loss: 0.45802054967026307\n",
      "Epoch 223, Loss: 0.45677224995978094\n",
      "Epoch 224, Loss: 0.45565914342704716\n",
      "Epoch 225, Loss: 0.45401674524485375\n",
      "Epoch 226, Loss: 0.4535057342791329\n",
      "Epoch 227, Loss: 0.45416164205845616\n",
      "Epoch 228, Loss: 0.45450513400796655\n",
      "Epoch 229, Loss: 0.4548238117351997\n",
      "Epoch 230, Loss: 0.4537990916311975\n",
      "Epoch 231, Loss: 0.4532521496977959\n",
      "Epoch 232, Loss: 0.4526084553640119\n",
      "Epoch 233, Loss: 0.45228981460773116\n",
      "Epoch 234, Loss: 0.4520949132230242\n",
      "Epoch 235, Loss: 0.45205797975469736\n",
      "Epoch 236, Loss: 0.45230557901617613\n",
      "Epoch 237, Loss: 0.4528345953247846\n",
      "Epoch 238, Loss: 0.4549386133976144\n",
      "Epoch 239, Loss: 0.4564022887834084\n",
      "Epoch 240, Loss: 0.45805755540321125\n",
      "Epoch 241, Loss: 0.4541498302115434\n",
      "Epoch 242, Loss: 0.45158727109343993\n",
      "Epoch 243, Loss: 0.45105347173746146\n",
      "Epoch 244, Loss: 0.45263971453254503\n",
      "Epoch 245, Loss: 0.4550743757756552\n",
      "Epoch 246, Loss: 0.4549471294161167\n",
      "Epoch 247, Loss: 0.45361945699341477\n",
      "Epoch 248, Loss: 0.4506913801287277\n",
      "Epoch 249, Loss: 0.44996455105922556\n",
      "Epoch 250, Loss: 0.45070657227233163\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23565258819276172\n",
      "Valence RMSE: 0.24272275835082952\n",
      "Arousal RMSE: 0.22836362937748972\n",
      "Test R^2 score: tensor([0.3704, 0.6154], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.49291965151035666\n",
      "Num of epochs: 251\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Epoch 121, Loss: 0.4816228295571127\n",
      "Epoch 122, Loss: 0.4807618495484584\n",
      "Epoch 123, Loss: 0.48102030846138955\n",
      "Epoch 124, Loss: 0.4810176288364116\n",
      "Epoch 125, Loss: 0.48014338261661127\n",
      "Epoch 126, Loss: 0.47910314809270116\n",
      "Epoch 127, Loss: 0.47880795949683447\n",
      "Epoch 128, Loss: 0.47876177315142754\n",
      "Epoch 129, Loss: 0.47819597116449997\n",
      "Epoch 130, Loss: 0.47743214539542544\n",
      "Epoch 131, Loss: 0.47658476230340185\n",
      "Epoch 132, Loss: 0.4759644680184978\n",
      "Epoch 133, Loss: 0.4755635187712421\n",
      "Epoch 134, Loss: 0.47538264196584307\n",
      "Epoch 135, Loss: 0.4753673450596914\n",
      "Epoch 136, Loss: 0.4753697744168057\n",
      "Epoch 137, Loss: 0.4755973109583155\n",
      "Epoch 138, Loss: 0.4767626826605241\n",
      "Epoch 139, Loss: 0.47683528222287225\n",
      "Epoch 140, Loss: 0.47579445468261566\n",
      "Epoch 141, Loss: 0.47284533919249566\n",
      "Epoch 142, Loss: 0.472809286012082\n",
      "Epoch 143, Loss: 0.47458738111232796\n",
      "Epoch 144, Loss: 0.4733380058918715\n",
      "Epoch 145, Loss: 0.4715229034484008\n",
      "Epoch 146, Loss: 0.47096730966267386\n",
      "Epoch 147, Loss: 0.47161469890467383\n",
      "Epoch 148, Loss: 0.47183531411129226\n",
      "Epoch 149, Loss: 0.470294933688076\n",
      "Epoch 150, Loss: 0.4697114144973765\n",
      "Epoch 151, Loss: 0.47018789053701304\n",
      "Epoch 152, Loss: 0.4700460953124117\n",
      "Epoch 153, Loss: 0.4693645740016508\n",
      "Epoch 154, Loss: 0.4684558104180481\n",
      "Epoch 155, Loss: 0.4680960384979934\n",
      "Epoch 156, Loss: 0.4682454254529793\n",
      "Epoch 157, Loss: 0.4682974379105488\n",
      "Epoch 158, Loss: 0.4681590010361841\n",
      "Epoch 159, Loss: 0.4673225124817593\n",
      "Epoch 160, Loss: 0.4666701428107949\n",
      "Epoch 161, Loss: 0.4663166784414499\n",
      "Epoch 162, Loss: 0.46624854535037\n",
      "Epoch 163, Loss: 0.4664611566834947\n",
      "Epoch 164, Loss: 0.46654532450127006\n",
      "Epoch 165, Loss: 0.46721845607201035\n",
      "Epoch 166, Loss: 0.4668208160006968\n",
      "Epoch 167, Loss: 0.466573749669782\n",
      "Epoch 168, Loss: 0.46506387643568103\n",
      "Epoch 169, Loss: 0.4641220516950567\n",
      "Epoch 170, Loss: 0.4644550172656552\n",
      "Epoch 171, Loss: 0.4654613047116713\n",
      "Epoch 172, Loss: 0.46880176576246846\n",
      "Epoch 173, Loss: 0.4685649824581709\n",
      "Epoch 174, Loss: 0.46606115996454733\n",
      "Epoch 175, Loss: 0.46328263242364126\n",
      "Epoch 176, Loss: 0.4660484187313888\n",
      "Epoch 177, Loss: 0.4667669789465077\n",
      "Epoch 178, Loss: 0.46222975993575777\n",
      "Epoch 179, Loss: 0.46412214801342555\n",
      "Epoch 180, Loss: 0.46867384291927394\n",
      "Epoch 181, Loss: 0.46316269230838986\n",
      "Epoch 182, Loss: 0.4619791093960296\n",
      "Epoch 183, Loss: 0.4650783107301587\n",
      "Epoch 184, Loss: 0.4617210775403838\n",
      "Epoch 185, Loss: 0.4621011464022392\n",
      "Epoch 186, Loss: 0.4641367239628427\n",
      "Epoch 187, Loss: 0.46071131054180564\n",
      "Epoch 188, Loss: 0.46145892957673257\n",
      "Epoch 189, Loss: 0.464304908222076\n",
      "Epoch 190, Loss: 0.4606605441278432\n",
      "Epoch 191, Loss: 0.4601386308130386\n",
      "Epoch 192, Loss: 0.46193430495315263\n",
      "Epoch 193, Loss: 0.45977696912799226\n",
      "Epoch 194, Loss: 0.4597173803227511\n",
      "Epoch 195, Loss: 0.46096589922299847\n",
      "Epoch 196, Loss: 0.4592170612502741\n",
      "Epoch 197, Loss: 0.45864347419013624\n",
      "Epoch 198, Loss: 0.4594650511157848\n",
      "Epoch 199, Loss: 0.45924705942486493\n",
      "Epoch 200, Loss: 0.4584931199827434\n",
      "Epoch 201, Loss: 0.4578552480015059\n",
      "Epoch 202, Loss: 0.458251953125\n",
      "Epoch 203, Loss: 0.4586848316367108\n",
      "Epoch 204, Loss: 0.4577025349799131\n",
      "Epoch 205, Loss: 0.45707273702530193\n",
      "Epoch 206, Loss: 0.4571336158814618\n",
      "Epoch 207, Loss: 0.45746758002181026\n",
      "Epoch 208, Loss: 0.4580334166179955\n",
      "Epoch 209, Loss: 0.457551138813026\n",
      "Epoch 210, Loss: 0.4572461918817771\n",
      "Epoch 211, Loss: 0.45629613394883295\n",
      "Epoch 212, Loss: 0.4557945440718634\n",
      "Epoch 213, Loss: 0.455843155547024\n",
      "Epoch 214, Loss: 0.456219547378948\n",
      "Epoch 215, Loss: 0.4577116995236092\n",
      "Epoch 216, Loss: 0.4589176697652806\n",
      "Epoch 217, Loss: 0.46105034303609993\n",
      "Epoch 218, Loss: 0.4567326442394303\n",
      "Epoch 219, Loss: 0.45493112899589366\n",
      "Epoch 220, Loss: 0.45527199475335134\n",
      "Epoch 221, Loss: 0.45623630281032773\n",
      "Epoch 222, Loss: 0.45802054967026307\n",
      "Epoch 223, Loss: 0.45677224995978094\n",
      "Epoch 224, Loss: 0.45565914342704716\n",
      "Epoch 225, Loss: 0.45401674524485375\n",
      "Epoch 226, Loss: 0.4535057342791329\n",
      "Epoch 227, Loss: 0.45416164205845616\n",
      "Epoch 228, Loss: 0.45450513400796655\n",
      "Epoch 229, Loss: 0.4548238117351997\n",
      "Epoch 230, Loss: 0.4537990916311975\n",
      "Epoch 231, Loss: 0.4532521496977959\n",
      "Epoch 232, Loss: 0.4526084553640119\n",
      "Epoch 233, Loss: 0.45228981460773116\n",
      "Epoch 234, Loss: 0.4520949132230242\n",
      "Epoch 235, Loss: 0.45205797975469736\n",
      "Epoch 236, Loss: 0.45230557901617613\n",
      "Epoch 237, Loss: 0.4528345953247846\n",
      "Epoch 238, Loss: 0.4549386133976144\n",
      "Epoch 239, Loss: 0.4564022887834084\n",
      "Epoch 240, Loss: 0.45805755540321125\n",
      "Epoch 241, Loss: 0.4541498302115434\n",
      "Epoch 242, Loss: 0.45158727109343993\n",
      "Epoch 243, Loss: 0.45105347173746146\n",
      "Epoch 244, Loss: 0.45263971453254503\n",
      "Epoch 245, Loss: 0.4550743757756552\n",
      "Epoch 246, Loss: 0.4549471294161167\n",
      "Epoch 247, Loss: 0.45361945699341477\n",
      "Epoch 248, Loss: 0.4506913801287277\n",
      "Epoch 249, Loss: 0.44996455105922556\n",
      "Epoch 250, Loss: 0.45070657227233163\n",
      "Epoch 251, Loss: 0.45134097806864715\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2380048925229622\n",
      "Valence RMSE: 0.24865541655774379\n",
      "Arousal RMSE: 0.2268548909462357\n",
      "Test R^2 score: tensor([0.3393, 0.6205], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.47987647345094026\n",
      "Num of epochs: 252\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Epoch 121, Loss: 0.4816228295571127\n",
      "Epoch 122, Loss: 0.4807618495484584\n",
      "Epoch 123, Loss: 0.48102030846138955\n",
      "Epoch 124, Loss: 0.4810176288364116\n",
      "Epoch 125, Loss: 0.48014338261661127\n",
      "Epoch 126, Loss: 0.47910314809270116\n",
      "Epoch 127, Loss: 0.47880795949683447\n",
      "Epoch 128, Loss: 0.47876177315142754\n",
      "Epoch 129, Loss: 0.47819597116449997\n",
      "Epoch 130, Loss: 0.47743214539542544\n",
      "Epoch 131, Loss: 0.47658476230340185\n",
      "Epoch 132, Loss: 0.4759644680184978\n",
      "Epoch 133, Loss: 0.4755635187712421\n",
      "Epoch 134, Loss: 0.47538264196584307\n",
      "Epoch 135, Loss: 0.4753673450596914\n",
      "Epoch 136, Loss: 0.4753697744168057\n",
      "Epoch 137, Loss: 0.4755973109583155\n",
      "Epoch 138, Loss: 0.4767626826605241\n",
      "Epoch 139, Loss: 0.47683528222287225\n",
      "Epoch 140, Loss: 0.47579445468261566\n",
      "Epoch 141, Loss: 0.47284533919249566\n",
      "Epoch 142, Loss: 0.472809286012082\n",
      "Epoch 143, Loss: 0.47458738111232796\n",
      "Epoch 144, Loss: 0.4733380058918715\n",
      "Epoch 145, Loss: 0.4715229034484008\n",
      "Epoch 146, Loss: 0.47096730966267386\n",
      "Epoch 147, Loss: 0.47161469890467383\n",
      "Epoch 148, Loss: 0.47183531411129226\n",
      "Epoch 149, Loss: 0.470294933688076\n",
      "Epoch 150, Loss: 0.4697114144973765\n",
      "Epoch 151, Loss: 0.47018789053701304\n",
      "Epoch 152, Loss: 0.4700460953124117\n",
      "Epoch 153, Loss: 0.4693645740016508\n",
      "Epoch 154, Loss: 0.4684558104180481\n",
      "Epoch 155, Loss: 0.4680960384979934\n",
      "Epoch 156, Loss: 0.4682454254529793\n",
      "Epoch 157, Loss: 0.4682974379105488\n",
      "Epoch 158, Loss: 0.4681590010361841\n",
      "Epoch 159, Loss: 0.4673225124817593\n",
      "Epoch 160, Loss: 0.4666701428107949\n",
      "Epoch 161, Loss: 0.4663166784414499\n",
      "Epoch 162, Loss: 0.46624854535037\n",
      "Epoch 163, Loss: 0.4664611566834947\n",
      "Epoch 164, Loss: 0.46654532450127006\n",
      "Epoch 165, Loss: 0.46721845607201035\n",
      "Epoch 166, Loss: 0.4668208160006968\n",
      "Epoch 167, Loss: 0.466573749669782\n",
      "Epoch 168, Loss: 0.46506387643568103\n",
      "Epoch 169, Loss: 0.4641220516950567\n",
      "Epoch 170, Loss: 0.4644550172656552\n",
      "Epoch 171, Loss: 0.4654613047116713\n",
      "Epoch 172, Loss: 0.46880176576246846\n",
      "Epoch 173, Loss: 0.4685649824581709\n",
      "Epoch 174, Loss: 0.46606115996454733\n",
      "Epoch 175, Loss: 0.46328263242364126\n",
      "Epoch 176, Loss: 0.4660484187313888\n",
      "Epoch 177, Loss: 0.4667669789465077\n",
      "Epoch 178, Loss: 0.46222975993575777\n",
      "Epoch 179, Loss: 0.46412214801342555\n",
      "Epoch 180, Loss: 0.46867384291927394\n",
      "Epoch 181, Loss: 0.46316269230838986\n",
      "Epoch 182, Loss: 0.4619791093960296\n",
      "Epoch 183, Loss: 0.4650783107301587\n",
      "Epoch 184, Loss: 0.4617210775403838\n",
      "Epoch 185, Loss: 0.4621011464022392\n",
      "Epoch 186, Loss: 0.4641367239628427\n",
      "Epoch 187, Loss: 0.46071131054180564\n",
      "Epoch 188, Loss: 0.46145892957673257\n",
      "Epoch 189, Loss: 0.464304908222076\n",
      "Epoch 190, Loss: 0.4606605441278432\n",
      "Epoch 191, Loss: 0.4601386308130386\n",
      "Epoch 192, Loss: 0.46193430495315263\n",
      "Epoch 193, Loss: 0.45977696912799226\n",
      "Epoch 194, Loss: 0.4597173803227511\n",
      "Epoch 195, Loss: 0.46096589922299847\n",
      "Epoch 196, Loss: 0.4592170612502741\n",
      "Epoch 197, Loss: 0.45864347419013624\n",
      "Epoch 198, Loss: 0.4594650511157848\n",
      "Epoch 199, Loss: 0.45924705942486493\n",
      "Epoch 200, Loss: 0.4584931199827434\n",
      "Epoch 201, Loss: 0.4578552480015059\n",
      "Epoch 202, Loss: 0.458251953125\n",
      "Epoch 203, Loss: 0.4586848316367108\n",
      "Epoch 204, Loss: 0.4577025349799131\n",
      "Epoch 205, Loss: 0.45707273702530193\n",
      "Epoch 206, Loss: 0.4571336158814618\n",
      "Epoch 207, Loss: 0.45746758002181026\n",
      "Epoch 208, Loss: 0.4580334166179955\n",
      "Epoch 209, Loss: 0.457551138813026\n",
      "Epoch 210, Loss: 0.4572461918817771\n",
      "Epoch 211, Loss: 0.45629613394883295\n",
      "Epoch 212, Loss: 0.4557945440718634\n",
      "Epoch 213, Loss: 0.455843155547024\n",
      "Epoch 214, Loss: 0.456219547378948\n",
      "Epoch 215, Loss: 0.4577116995236092\n",
      "Epoch 216, Loss: 0.4589176697652806\n",
      "Epoch 217, Loss: 0.46105034303609993\n",
      "Epoch 218, Loss: 0.4567326442394303\n",
      "Epoch 219, Loss: 0.45493112899589366\n",
      "Epoch 220, Loss: 0.45527199475335134\n",
      "Epoch 221, Loss: 0.45623630281032773\n",
      "Epoch 222, Loss: 0.45802054967026307\n",
      "Epoch 223, Loss: 0.45677224995978094\n",
      "Epoch 224, Loss: 0.45565914342704716\n",
      "Epoch 225, Loss: 0.45401674524485375\n",
      "Epoch 226, Loss: 0.4535057342791329\n",
      "Epoch 227, Loss: 0.45416164205845616\n",
      "Epoch 228, Loss: 0.45450513400796655\n",
      "Epoch 229, Loss: 0.4548238117351997\n",
      "Epoch 230, Loss: 0.4537990916311975\n",
      "Epoch 231, Loss: 0.4532521496977959\n",
      "Epoch 232, Loss: 0.4526084553640119\n",
      "Epoch 233, Loss: 0.45228981460773116\n",
      "Epoch 234, Loss: 0.4520949132230242\n",
      "Epoch 235, Loss: 0.45205797975469736\n",
      "Epoch 236, Loss: 0.45230557901617613\n",
      "Epoch 237, Loss: 0.4528345953247846\n",
      "Epoch 238, Loss: 0.4549386133976144\n",
      "Epoch 239, Loss: 0.4564022887834084\n",
      "Epoch 240, Loss: 0.45805755540321125\n",
      "Epoch 241, Loss: 0.4541498302115434\n",
      "Epoch 242, Loss: 0.45158727109343993\n",
      "Epoch 243, Loss: 0.45105347173746146\n",
      "Epoch 244, Loss: 0.45263971453254503\n",
      "Epoch 245, Loss: 0.4550743757756552\n",
      "Epoch 246, Loss: 0.4549471294161167\n",
      "Epoch 247, Loss: 0.45361945699341477\n",
      "Epoch 248, Loss: 0.4506913801287277\n",
      "Epoch 249, Loss: 0.44996455105922556\n",
      "Epoch 250, Loss: 0.45070657227233163\n",
      "Epoch 251, Loss: 0.45134097806864715\n",
      "Epoch 252, Loss: 0.4518641160838718\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23625998241073845\n",
      "Valence RMSE: 0.24316754864519566\n",
      "Arousal RMSE: 0.2291442817600546\n",
      "Test R^2 score: tensor([0.3681, 0.6128], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.49044792872492626\n",
      "Num of epochs: 253\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Epoch 121, Loss: 0.4816228295571127\n",
      "Epoch 122, Loss: 0.4807618495484584\n",
      "Epoch 123, Loss: 0.48102030846138955\n",
      "Epoch 124, Loss: 0.4810176288364116\n",
      "Epoch 125, Loss: 0.48014338261661127\n",
      "Epoch 126, Loss: 0.47910314809270116\n",
      "Epoch 127, Loss: 0.47880795949683447\n",
      "Epoch 128, Loss: 0.47876177315142754\n",
      "Epoch 129, Loss: 0.47819597116449997\n",
      "Epoch 130, Loss: 0.47743214539542544\n",
      "Epoch 131, Loss: 0.47658476230340185\n",
      "Epoch 132, Loss: 0.4759644680184978\n",
      "Epoch 133, Loss: 0.4755635187712421\n",
      "Epoch 134, Loss: 0.47538264196584307\n",
      "Epoch 135, Loss: 0.4753673450596914\n",
      "Epoch 136, Loss: 0.4753697744168057\n",
      "Epoch 137, Loss: 0.4755973109583155\n",
      "Epoch 138, Loss: 0.4767626826605241\n",
      "Epoch 139, Loss: 0.47683528222287225\n",
      "Epoch 140, Loss: 0.47579445468261566\n",
      "Epoch 141, Loss: 0.47284533919249566\n",
      "Epoch 142, Loss: 0.472809286012082\n",
      "Epoch 143, Loss: 0.47458738111232796\n",
      "Epoch 144, Loss: 0.4733380058918715\n",
      "Epoch 145, Loss: 0.4715229034484008\n",
      "Epoch 146, Loss: 0.47096730966267386\n",
      "Epoch 147, Loss: 0.47161469890467383\n",
      "Epoch 148, Loss: 0.47183531411129226\n",
      "Epoch 149, Loss: 0.470294933688076\n",
      "Epoch 150, Loss: 0.4697114144973765\n",
      "Epoch 151, Loss: 0.47018789053701304\n",
      "Epoch 152, Loss: 0.4700460953124117\n",
      "Epoch 153, Loss: 0.4693645740016508\n",
      "Epoch 154, Loss: 0.4684558104180481\n",
      "Epoch 155, Loss: 0.4680960384979934\n",
      "Epoch 156, Loss: 0.4682454254529793\n",
      "Epoch 157, Loss: 0.4682974379105488\n",
      "Epoch 158, Loss: 0.4681590010361841\n",
      "Epoch 159, Loss: 0.4673225124817593\n",
      "Epoch 160, Loss: 0.4666701428107949\n",
      "Epoch 161, Loss: 0.4663166784414499\n",
      "Epoch 162, Loss: 0.46624854535037\n",
      "Epoch 163, Loss: 0.4664611566834947\n",
      "Epoch 164, Loss: 0.46654532450127006\n",
      "Epoch 165, Loss: 0.46721845607201035\n",
      "Epoch 166, Loss: 0.4668208160006968\n",
      "Epoch 167, Loss: 0.466573749669782\n",
      "Epoch 168, Loss: 0.46506387643568103\n",
      "Epoch 169, Loss: 0.4641220516950567\n",
      "Epoch 170, Loss: 0.4644550172656552\n",
      "Epoch 171, Loss: 0.4654613047116713\n",
      "Epoch 172, Loss: 0.46880176576246846\n",
      "Epoch 173, Loss: 0.4685649824581709\n",
      "Epoch 174, Loss: 0.46606115996454733\n",
      "Epoch 175, Loss: 0.46328263242364126\n",
      "Epoch 176, Loss: 0.4660484187313888\n",
      "Epoch 177, Loss: 0.4667669789465077\n",
      "Epoch 178, Loss: 0.46222975993575777\n",
      "Epoch 179, Loss: 0.46412214801342555\n",
      "Epoch 180, Loss: 0.46867384291927394\n",
      "Epoch 181, Loss: 0.46316269230838986\n",
      "Epoch 182, Loss: 0.4619791093960296\n",
      "Epoch 183, Loss: 0.4650783107301587\n",
      "Epoch 184, Loss: 0.4617210775403838\n",
      "Epoch 185, Loss: 0.4621011464022392\n",
      "Epoch 186, Loss: 0.4641367239628427\n",
      "Epoch 187, Loss: 0.46071131054180564\n",
      "Epoch 188, Loss: 0.46145892957673257\n",
      "Epoch 189, Loss: 0.464304908222076\n",
      "Epoch 190, Loss: 0.4606605441278432\n",
      "Epoch 191, Loss: 0.4601386308130386\n",
      "Epoch 192, Loss: 0.46193430495315263\n",
      "Epoch 193, Loss: 0.45977696912799226\n",
      "Epoch 194, Loss: 0.4597173803227511\n",
      "Epoch 195, Loss: 0.46096589922299847\n",
      "Epoch 196, Loss: 0.4592170612502741\n",
      "Epoch 197, Loss: 0.45864347419013624\n",
      "Epoch 198, Loss: 0.4594650511157848\n",
      "Epoch 199, Loss: 0.45924705942486493\n",
      "Epoch 200, Loss: 0.4584931199827434\n",
      "Epoch 201, Loss: 0.4578552480015059\n",
      "Epoch 202, Loss: 0.458251953125\n",
      "Epoch 203, Loss: 0.4586848316367108\n",
      "Epoch 204, Loss: 0.4577025349799131\n",
      "Epoch 205, Loss: 0.45707273702530193\n",
      "Epoch 206, Loss: 0.4571336158814618\n",
      "Epoch 207, Loss: 0.45746758002181026\n",
      "Epoch 208, Loss: 0.4580334166179955\n",
      "Epoch 209, Loss: 0.457551138813026\n",
      "Epoch 210, Loss: 0.4572461918817771\n",
      "Epoch 211, Loss: 0.45629613394883295\n",
      "Epoch 212, Loss: 0.4557945440718634\n",
      "Epoch 213, Loss: 0.455843155547024\n",
      "Epoch 214, Loss: 0.456219547378948\n",
      "Epoch 215, Loss: 0.4577116995236092\n",
      "Epoch 216, Loss: 0.4589176697652806\n",
      "Epoch 217, Loss: 0.46105034303609993\n",
      "Epoch 218, Loss: 0.4567326442394303\n",
      "Epoch 219, Loss: 0.45493112899589366\n",
      "Epoch 220, Loss: 0.45527199475335134\n",
      "Epoch 221, Loss: 0.45623630281032773\n",
      "Epoch 222, Loss: 0.45802054967026307\n",
      "Epoch 223, Loss: 0.45677224995978094\n",
      "Epoch 224, Loss: 0.45565914342704716\n",
      "Epoch 225, Loss: 0.45401674524485375\n",
      "Epoch 226, Loss: 0.4535057342791329\n",
      "Epoch 227, Loss: 0.45416164205845616\n",
      "Epoch 228, Loss: 0.45450513400796655\n",
      "Epoch 229, Loss: 0.4548238117351997\n",
      "Epoch 230, Loss: 0.4537990916311975\n",
      "Epoch 231, Loss: 0.4532521496977959\n",
      "Epoch 232, Loss: 0.4526084553640119\n",
      "Epoch 233, Loss: 0.45228981460773116\n",
      "Epoch 234, Loss: 0.4520949132230242\n",
      "Epoch 235, Loss: 0.45205797975469736\n",
      "Epoch 236, Loss: 0.45230557901617613\n",
      "Epoch 237, Loss: 0.4528345953247846\n",
      "Epoch 238, Loss: 0.4549386133976144\n",
      "Epoch 239, Loss: 0.4564022887834084\n",
      "Epoch 240, Loss: 0.45805755540321125\n",
      "Epoch 241, Loss: 0.4541498302115434\n",
      "Epoch 242, Loss: 0.45158727109343993\n",
      "Epoch 243, Loss: 0.45105347173746146\n",
      "Epoch 244, Loss: 0.45263971453254503\n",
      "Epoch 245, Loss: 0.4550743757756552\n",
      "Epoch 246, Loss: 0.4549471294161167\n",
      "Epoch 247, Loss: 0.45361945699341477\n",
      "Epoch 248, Loss: 0.4506913801287277\n",
      "Epoch 249, Loss: 0.44996455105922556\n",
      "Epoch 250, Loss: 0.45070657227233163\n",
      "Epoch 251, Loss: 0.45134097806864715\n",
      "Epoch 252, Loss: 0.4518641160838718\n",
      "Epoch 253, Loss: 0.45124870735997746\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23783771735622017\n",
      "Valence RMSE: 0.24843065142352336\n",
      "Arousal RMSE: 0.22675045981809694\n",
      "Test R^2 score: tensor([0.3405, 0.6208], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.48064811037656185\n",
      "Num of epochs: 254\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Epoch 121, Loss: 0.4816228295571127\n",
      "Epoch 122, Loss: 0.4807618495484584\n",
      "Epoch 123, Loss: 0.48102030846138955\n",
      "Epoch 124, Loss: 0.4810176288364116\n",
      "Epoch 125, Loss: 0.48014338261661127\n",
      "Epoch 126, Loss: 0.47910314809270116\n",
      "Epoch 127, Loss: 0.47880795949683447\n",
      "Epoch 128, Loss: 0.47876177315142754\n",
      "Epoch 129, Loss: 0.47819597116449997\n",
      "Epoch 130, Loss: 0.47743214539542544\n",
      "Epoch 131, Loss: 0.47658476230340185\n",
      "Epoch 132, Loss: 0.4759644680184978\n",
      "Epoch 133, Loss: 0.4755635187712421\n",
      "Epoch 134, Loss: 0.47538264196584307\n",
      "Epoch 135, Loss: 0.4753673450596914\n",
      "Epoch 136, Loss: 0.4753697744168057\n",
      "Epoch 137, Loss: 0.4755973109583155\n",
      "Epoch 138, Loss: 0.4767626826605241\n",
      "Epoch 139, Loss: 0.47683528222287225\n",
      "Epoch 140, Loss: 0.47579445468261566\n",
      "Epoch 141, Loss: 0.47284533919249566\n",
      "Epoch 142, Loss: 0.472809286012082\n",
      "Epoch 143, Loss: 0.47458738111232796\n",
      "Epoch 144, Loss: 0.4733380058918715\n",
      "Epoch 145, Loss: 0.4715229034484008\n",
      "Epoch 146, Loss: 0.47096730966267386\n",
      "Epoch 147, Loss: 0.47161469890467383\n",
      "Epoch 148, Loss: 0.47183531411129226\n",
      "Epoch 149, Loss: 0.470294933688076\n",
      "Epoch 150, Loss: 0.4697114144973765\n",
      "Epoch 151, Loss: 0.47018789053701304\n",
      "Epoch 152, Loss: 0.4700460953124117\n",
      "Epoch 153, Loss: 0.4693645740016508\n",
      "Epoch 154, Loss: 0.4684558104180481\n",
      "Epoch 155, Loss: 0.4680960384979934\n",
      "Epoch 156, Loss: 0.4682454254529793\n",
      "Epoch 157, Loss: 0.4682974379105488\n",
      "Epoch 158, Loss: 0.4681590010361841\n",
      "Epoch 159, Loss: 0.4673225124817593\n",
      "Epoch 160, Loss: 0.4666701428107949\n",
      "Epoch 161, Loss: 0.4663166784414499\n",
      "Epoch 162, Loss: 0.46624854535037\n",
      "Epoch 163, Loss: 0.4664611566834947\n",
      "Epoch 164, Loss: 0.46654532450127006\n",
      "Epoch 165, Loss: 0.46721845607201035\n",
      "Epoch 166, Loss: 0.4668208160006968\n",
      "Epoch 167, Loss: 0.466573749669782\n",
      "Epoch 168, Loss: 0.46506387643568103\n",
      "Epoch 169, Loss: 0.4641220516950567\n",
      "Epoch 170, Loss: 0.4644550172656552\n",
      "Epoch 171, Loss: 0.4654613047116713\n",
      "Epoch 172, Loss: 0.46880176576246846\n",
      "Epoch 173, Loss: 0.4685649824581709\n",
      "Epoch 174, Loss: 0.46606115996454733\n",
      "Epoch 175, Loss: 0.46328263242364126\n",
      "Epoch 176, Loss: 0.4660484187313888\n",
      "Epoch 177, Loss: 0.4667669789465077\n",
      "Epoch 178, Loss: 0.46222975993575777\n",
      "Epoch 179, Loss: 0.46412214801342555\n",
      "Epoch 180, Loss: 0.46867384291927394\n",
      "Epoch 181, Loss: 0.46316269230838986\n",
      "Epoch 182, Loss: 0.4619791093960296\n",
      "Epoch 183, Loss: 0.4650783107301587\n",
      "Epoch 184, Loss: 0.4617210775403838\n",
      "Epoch 185, Loss: 0.4621011464022392\n",
      "Epoch 186, Loss: 0.4641367239628427\n",
      "Epoch 187, Loss: 0.46071131054180564\n",
      "Epoch 188, Loss: 0.46145892957673257\n",
      "Epoch 189, Loss: 0.464304908222076\n",
      "Epoch 190, Loss: 0.4606605441278432\n",
      "Epoch 191, Loss: 0.4601386308130386\n",
      "Epoch 192, Loss: 0.46193430495315263\n",
      "Epoch 193, Loss: 0.45977696912799226\n",
      "Epoch 194, Loss: 0.4597173803227511\n",
      "Epoch 195, Loss: 0.46096589922299847\n",
      "Epoch 196, Loss: 0.4592170612502741\n",
      "Epoch 197, Loss: 0.45864347419013624\n",
      "Epoch 198, Loss: 0.4594650511157848\n",
      "Epoch 199, Loss: 0.45924705942486493\n",
      "Epoch 200, Loss: 0.4584931199827434\n",
      "Epoch 201, Loss: 0.4578552480015059\n",
      "Epoch 202, Loss: 0.458251953125\n",
      "Epoch 203, Loss: 0.4586848316367108\n",
      "Epoch 204, Loss: 0.4577025349799131\n",
      "Epoch 205, Loss: 0.45707273702530193\n",
      "Epoch 206, Loss: 0.4571336158814618\n",
      "Epoch 207, Loss: 0.45746758002181026\n",
      "Epoch 208, Loss: 0.4580334166179955\n",
      "Epoch 209, Loss: 0.457551138813026\n",
      "Epoch 210, Loss: 0.4572461918817771\n",
      "Epoch 211, Loss: 0.45629613394883295\n",
      "Epoch 212, Loss: 0.4557945440718634\n",
      "Epoch 213, Loss: 0.455843155547024\n",
      "Epoch 214, Loss: 0.456219547378948\n",
      "Epoch 215, Loss: 0.4577116995236092\n",
      "Epoch 216, Loss: 0.4589176697652806\n",
      "Epoch 217, Loss: 0.46105034303609993\n",
      "Epoch 218, Loss: 0.4567326442394303\n",
      "Epoch 219, Loss: 0.45493112899589366\n",
      "Epoch 220, Loss: 0.45527199475335134\n",
      "Epoch 221, Loss: 0.45623630281032773\n",
      "Epoch 222, Loss: 0.45802054967026307\n",
      "Epoch 223, Loss: 0.45677224995978094\n",
      "Epoch 224, Loss: 0.45565914342704716\n",
      "Epoch 225, Loss: 0.45401674524485375\n",
      "Epoch 226, Loss: 0.4535057342791329\n",
      "Epoch 227, Loss: 0.45416164205845616\n",
      "Epoch 228, Loss: 0.45450513400796655\n",
      "Epoch 229, Loss: 0.4548238117351997\n",
      "Epoch 230, Loss: 0.4537990916311975\n",
      "Epoch 231, Loss: 0.4532521496977959\n",
      "Epoch 232, Loss: 0.4526084553640119\n",
      "Epoch 233, Loss: 0.45228981460773116\n",
      "Epoch 234, Loss: 0.4520949132230242\n",
      "Epoch 235, Loss: 0.45205797975469736\n",
      "Epoch 236, Loss: 0.45230557901617613\n",
      "Epoch 237, Loss: 0.4528345953247846\n",
      "Epoch 238, Loss: 0.4549386133976144\n",
      "Epoch 239, Loss: 0.4564022887834084\n",
      "Epoch 240, Loss: 0.45805755540321125\n",
      "Epoch 241, Loss: 0.4541498302115434\n",
      "Epoch 242, Loss: 0.45158727109343993\n",
      "Epoch 243, Loss: 0.45105347173746146\n",
      "Epoch 244, Loss: 0.45263971453254503\n",
      "Epoch 245, Loss: 0.4550743757756552\n",
      "Epoch 246, Loss: 0.4549471294161167\n",
      "Epoch 247, Loss: 0.45361945699341477\n",
      "Epoch 248, Loss: 0.4506913801287277\n",
      "Epoch 249, Loss: 0.44996455105922556\n",
      "Epoch 250, Loss: 0.45070657227233163\n",
      "Epoch 251, Loss: 0.45134097806864715\n",
      "Epoch 252, Loss: 0.4518641160838718\n",
      "Epoch 253, Loss: 0.45124870735997746\n",
      "Epoch 254, Loss: 0.4507253179113287\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23562621462715458\n",
      "Valence RMSE: 0.24348642515563843\n",
      "Arousal RMSE: 0.22749458631794195\n",
      "Test R^2 score: tensor([0.3665, 0.6183], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4923966244409657\n",
      "Num of epochs: 255\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Epoch 121, Loss: 0.4816228295571127\n",
      "Epoch 122, Loss: 0.4807618495484584\n",
      "Epoch 123, Loss: 0.48102030846138955\n",
      "Epoch 124, Loss: 0.4810176288364116\n",
      "Epoch 125, Loss: 0.48014338261661127\n",
      "Epoch 126, Loss: 0.47910314809270116\n",
      "Epoch 127, Loss: 0.47880795949683447\n",
      "Epoch 128, Loss: 0.47876177315142754\n",
      "Epoch 129, Loss: 0.47819597116449997\n",
      "Epoch 130, Loss: 0.47743214539542544\n",
      "Epoch 131, Loss: 0.47658476230340185\n",
      "Epoch 132, Loss: 0.4759644680184978\n",
      "Epoch 133, Loss: 0.4755635187712421\n",
      "Epoch 134, Loss: 0.47538264196584307\n",
      "Epoch 135, Loss: 0.4753673450596914\n",
      "Epoch 136, Loss: 0.4753697744168057\n",
      "Epoch 137, Loss: 0.4755973109583155\n",
      "Epoch 138, Loss: 0.4767626826605241\n",
      "Epoch 139, Loss: 0.47683528222287225\n",
      "Epoch 140, Loss: 0.47579445468261566\n",
      "Epoch 141, Loss: 0.47284533919249566\n",
      "Epoch 142, Loss: 0.472809286012082\n",
      "Epoch 143, Loss: 0.47458738111232796\n",
      "Epoch 144, Loss: 0.4733380058918715\n",
      "Epoch 145, Loss: 0.4715229034484008\n",
      "Epoch 146, Loss: 0.47096730966267386\n",
      "Epoch 147, Loss: 0.47161469890467383\n",
      "Epoch 148, Loss: 0.47183531411129226\n",
      "Epoch 149, Loss: 0.470294933688076\n",
      "Epoch 150, Loss: 0.4697114144973765\n",
      "Epoch 151, Loss: 0.47018789053701304\n",
      "Epoch 152, Loss: 0.4700460953124117\n",
      "Epoch 153, Loss: 0.4693645740016508\n",
      "Epoch 154, Loss: 0.4684558104180481\n",
      "Epoch 155, Loss: 0.4680960384979934\n",
      "Epoch 156, Loss: 0.4682454254529793\n",
      "Epoch 157, Loss: 0.4682974379105488\n",
      "Epoch 158, Loss: 0.4681590010361841\n",
      "Epoch 159, Loss: 0.4673225124817593\n",
      "Epoch 160, Loss: 0.4666701428107949\n",
      "Epoch 161, Loss: 0.4663166784414499\n",
      "Epoch 162, Loss: 0.46624854535037\n",
      "Epoch 163, Loss: 0.4664611566834947\n",
      "Epoch 164, Loss: 0.46654532450127006\n",
      "Epoch 165, Loss: 0.46721845607201035\n",
      "Epoch 166, Loss: 0.4668208160006968\n",
      "Epoch 167, Loss: 0.466573749669782\n",
      "Epoch 168, Loss: 0.46506387643568103\n",
      "Epoch 169, Loss: 0.4641220516950567\n",
      "Epoch 170, Loss: 0.4644550172656552\n",
      "Epoch 171, Loss: 0.4654613047116713\n",
      "Epoch 172, Loss: 0.46880176576246846\n",
      "Epoch 173, Loss: 0.4685649824581709\n",
      "Epoch 174, Loss: 0.46606115996454733\n",
      "Epoch 175, Loss: 0.46328263242364126\n",
      "Epoch 176, Loss: 0.4660484187313888\n",
      "Epoch 177, Loss: 0.4667669789465077\n",
      "Epoch 178, Loss: 0.46222975993575777\n",
      "Epoch 179, Loss: 0.46412214801342555\n",
      "Epoch 180, Loss: 0.46867384291927394\n",
      "Epoch 181, Loss: 0.46316269230838986\n",
      "Epoch 182, Loss: 0.4619791093960296\n",
      "Epoch 183, Loss: 0.4650783107301587\n",
      "Epoch 184, Loss: 0.4617210775403838\n",
      "Epoch 185, Loss: 0.4621011464022392\n",
      "Epoch 186, Loss: 0.4641367239628427\n",
      "Epoch 187, Loss: 0.46071131054180564\n",
      "Epoch 188, Loss: 0.46145892957673257\n",
      "Epoch 189, Loss: 0.464304908222076\n",
      "Epoch 190, Loss: 0.4606605441278432\n",
      "Epoch 191, Loss: 0.4601386308130386\n",
      "Epoch 192, Loss: 0.46193430495315263\n",
      "Epoch 193, Loss: 0.45977696912799226\n",
      "Epoch 194, Loss: 0.4597173803227511\n",
      "Epoch 195, Loss: 0.46096589922299847\n",
      "Epoch 196, Loss: 0.4592170612502741\n",
      "Epoch 197, Loss: 0.45864347419013624\n",
      "Epoch 198, Loss: 0.4594650511157848\n",
      "Epoch 199, Loss: 0.45924705942486493\n",
      "Epoch 200, Loss: 0.4584931199827434\n",
      "Epoch 201, Loss: 0.4578552480015059\n",
      "Epoch 202, Loss: 0.458251953125\n",
      "Epoch 203, Loss: 0.4586848316367108\n",
      "Epoch 204, Loss: 0.4577025349799131\n",
      "Epoch 205, Loss: 0.45707273702530193\n",
      "Epoch 206, Loss: 0.4571336158814618\n",
      "Epoch 207, Loss: 0.45746758002181026\n",
      "Epoch 208, Loss: 0.4580334166179955\n",
      "Epoch 209, Loss: 0.457551138813026\n",
      "Epoch 210, Loss: 0.4572461918817771\n",
      "Epoch 211, Loss: 0.45629613394883295\n",
      "Epoch 212, Loss: 0.4557945440718634\n",
      "Epoch 213, Loss: 0.455843155547024\n",
      "Epoch 214, Loss: 0.456219547378948\n",
      "Epoch 215, Loss: 0.4577116995236092\n",
      "Epoch 216, Loss: 0.4589176697652806\n",
      "Epoch 217, Loss: 0.46105034303609993\n",
      "Epoch 218, Loss: 0.4567326442394303\n",
      "Epoch 219, Loss: 0.45493112899589366\n",
      "Epoch 220, Loss: 0.45527199475335134\n",
      "Epoch 221, Loss: 0.45623630281032773\n",
      "Epoch 222, Loss: 0.45802054967026307\n",
      "Epoch 223, Loss: 0.45677224995978094\n",
      "Epoch 224, Loss: 0.45565914342704716\n",
      "Epoch 225, Loss: 0.45401674524485375\n",
      "Epoch 226, Loss: 0.4535057342791329\n",
      "Epoch 227, Loss: 0.45416164205845616\n",
      "Epoch 228, Loss: 0.45450513400796655\n",
      "Epoch 229, Loss: 0.4548238117351997\n",
      "Epoch 230, Loss: 0.4537990916311975\n",
      "Epoch 231, Loss: 0.4532521496977959\n",
      "Epoch 232, Loss: 0.4526084553640119\n",
      "Epoch 233, Loss: 0.45228981460773116\n",
      "Epoch 234, Loss: 0.4520949132230242\n",
      "Epoch 235, Loss: 0.45205797975469736\n",
      "Epoch 236, Loss: 0.45230557901617613\n",
      "Epoch 237, Loss: 0.4528345953247846\n",
      "Epoch 238, Loss: 0.4549386133976144\n",
      "Epoch 239, Loss: 0.4564022887834084\n",
      "Epoch 240, Loss: 0.45805755540321125\n",
      "Epoch 241, Loss: 0.4541498302115434\n",
      "Epoch 242, Loss: 0.45158727109343993\n",
      "Epoch 243, Loss: 0.45105347173746146\n",
      "Epoch 244, Loss: 0.45263971453254503\n",
      "Epoch 245, Loss: 0.4550743757756552\n",
      "Epoch 246, Loss: 0.4549471294161167\n",
      "Epoch 247, Loss: 0.45361945699341477\n",
      "Epoch 248, Loss: 0.4506913801287277\n",
      "Epoch 249, Loss: 0.44996455105922556\n",
      "Epoch 250, Loss: 0.45070657227233163\n",
      "Epoch 251, Loss: 0.45134097806864715\n",
      "Epoch 252, Loss: 0.4518641160838718\n",
      "Epoch 253, Loss: 0.45124870735997746\n",
      "Epoch 254, Loss: 0.4507253179113287\n",
      "Epoch 255, Loss: 0.44965336548788515\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23596816541847251\n",
      "Valence RMSE: 0.2456681710026454\n",
      "Arousal RMSE: 0.22585194251574237\n",
      "Test R^2 score: tensor([0.3551, 0.6238], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.48944054825831185\n",
      "Num of epochs: 256\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Epoch 121, Loss: 0.4816228295571127\n",
      "Epoch 122, Loss: 0.4807618495484584\n",
      "Epoch 123, Loss: 0.48102030846138955\n",
      "Epoch 124, Loss: 0.4810176288364116\n",
      "Epoch 125, Loss: 0.48014338261661127\n",
      "Epoch 126, Loss: 0.47910314809270116\n",
      "Epoch 127, Loss: 0.47880795949683447\n",
      "Epoch 128, Loss: 0.47876177315142754\n",
      "Epoch 129, Loss: 0.47819597116449997\n",
      "Epoch 130, Loss: 0.47743214539542544\n",
      "Epoch 131, Loss: 0.47658476230340185\n",
      "Epoch 132, Loss: 0.4759644680184978\n",
      "Epoch 133, Loss: 0.4755635187712421\n",
      "Epoch 134, Loss: 0.47538264196584307\n",
      "Epoch 135, Loss: 0.4753673450596914\n",
      "Epoch 136, Loss: 0.4753697744168057\n",
      "Epoch 137, Loss: 0.4755973109583155\n",
      "Epoch 138, Loss: 0.4767626826605241\n",
      "Epoch 139, Loss: 0.47683528222287225\n",
      "Epoch 140, Loss: 0.47579445468261566\n",
      "Epoch 141, Loss: 0.47284533919249566\n",
      "Epoch 142, Loss: 0.472809286012082\n",
      "Epoch 143, Loss: 0.47458738111232796\n",
      "Epoch 144, Loss: 0.4733380058918715\n",
      "Epoch 145, Loss: 0.4715229034484008\n",
      "Epoch 146, Loss: 0.47096730966267386\n",
      "Epoch 147, Loss: 0.47161469890467383\n",
      "Epoch 148, Loss: 0.47183531411129226\n",
      "Epoch 149, Loss: 0.470294933688076\n",
      "Epoch 150, Loss: 0.4697114144973765\n",
      "Epoch 151, Loss: 0.47018789053701304\n",
      "Epoch 152, Loss: 0.4700460953124117\n",
      "Epoch 153, Loss: 0.4693645740016508\n",
      "Epoch 154, Loss: 0.4684558104180481\n",
      "Epoch 155, Loss: 0.4680960384979934\n",
      "Epoch 156, Loss: 0.4682454254529793\n",
      "Epoch 157, Loss: 0.4682974379105488\n",
      "Epoch 158, Loss: 0.4681590010361841\n",
      "Epoch 159, Loss: 0.4673225124817593\n",
      "Epoch 160, Loss: 0.4666701428107949\n",
      "Epoch 161, Loss: 0.4663166784414499\n",
      "Epoch 162, Loss: 0.46624854535037\n",
      "Epoch 163, Loss: 0.4664611566834947\n",
      "Epoch 164, Loss: 0.46654532450127006\n",
      "Epoch 165, Loss: 0.46721845607201035\n",
      "Epoch 166, Loss: 0.4668208160006968\n",
      "Epoch 167, Loss: 0.466573749669782\n",
      "Epoch 168, Loss: 0.46506387643568103\n",
      "Epoch 169, Loss: 0.4641220516950567\n",
      "Epoch 170, Loss: 0.4644550172656552\n",
      "Epoch 171, Loss: 0.4654613047116713\n",
      "Epoch 172, Loss: 0.46880176576246846\n",
      "Epoch 173, Loss: 0.4685649824581709\n",
      "Epoch 174, Loss: 0.46606115996454733\n",
      "Epoch 175, Loss: 0.46328263242364126\n",
      "Epoch 176, Loss: 0.4660484187313888\n",
      "Epoch 177, Loss: 0.4667669789465077\n",
      "Epoch 178, Loss: 0.46222975993575777\n",
      "Epoch 179, Loss: 0.46412214801342555\n",
      "Epoch 180, Loss: 0.46867384291927394\n",
      "Epoch 181, Loss: 0.46316269230838986\n",
      "Epoch 182, Loss: 0.4619791093960296\n",
      "Epoch 183, Loss: 0.4650783107301587\n",
      "Epoch 184, Loss: 0.4617210775403838\n",
      "Epoch 185, Loss: 0.4621011464022392\n",
      "Epoch 186, Loss: 0.4641367239628427\n",
      "Epoch 187, Loss: 0.46071131054180564\n",
      "Epoch 188, Loss: 0.46145892957673257\n",
      "Epoch 189, Loss: 0.464304908222076\n",
      "Epoch 190, Loss: 0.4606605441278432\n",
      "Epoch 191, Loss: 0.4601386308130386\n",
      "Epoch 192, Loss: 0.46193430495315263\n",
      "Epoch 193, Loss: 0.45977696912799226\n",
      "Epoch 194, Loss: 0.4597173803227511\n",
      "Epoch 195, Loss: 0.46096589922299847\n",
      "Epoch 196, Loss: 0.4592170612502741\n",
      "Epoch 197, Loss: 0.45864347419013624\n",
      "Epoch 198, Loss: 0.4594650511157848\n",
      "Epoch 199, Loss: 0.45924705942486493\n",
      "Epoch 200, Loss: 0.4584931199827434\n",
      "Epoch 201, Loss: 0.4578552480015059\n",
      "Epoch 202, Loss: 0.458251953125\n",
      "Epoch 203, Loss: 0.4586848316367108\n",
      "Epoch 204, Loss: 0.4577025349799131\n",
      "Epoch 205, Loss: 0.45707273702530193\n",
      "Epoch 206, Loss: 0.4571336158814618\n",
      "Epoch 207, Loss: 0.45746758002181026\n",
      "Epoch 208, Loss: 0.4580334166179955\n",
      "Epoch 209, Loss: 0.457551138813026\n",
      "Epoch 210, Loss: 0.4572461918817771\n",
      "Epoch 211, Loss: 0.45629613394883295\n",
      "Epoch 212, Loss: 0.4557945440718634\n",
      "Epoch 213, Loss: 0.455843155547024\n",
      "Epoch 214, Loss: 0.456219547378948\n",
      "Epoch 215, Loss: 0.4577116995236092\n",
      "Epoch 216, Loss: 0.4589176697652806\n",
      "Epoch 217, Loss: 0.46105034303609993\n",
      "Epoch 218, Loss: 0.4567326442394303\n",
      "Epoch 219, Loss: 0.45493112899589366\n",
      "Epoch 220, Loss: 0.45527199475335134\n",
      "Epoch 221, Loss: 0.45623630281032773\n",
      "Epoch 222, Loss: 0.45802054967026307\n",
      "Epoch 223, Loss: 0.45677224995978094\n",
      "Epoch 224, Loss: 0.45565914342704716\n",
      "Epoch 225, Loss: 0.45401674524485375\n",
      "Epoch 226, Loss: 0.4535057342791329\n",
      "Epoch 227, Loss: 0.45416164205845616\n",
      "Epoch 228, Loss: 0.45450513400796655\n",
      "Epoch 229, Loss: 0.4548238117351997\n",
      "Epoch 230, Loss: 0.4537990916311975\n",
      "Epoch 231, Loss: 0.4532521496977959\n",
      "Epoch 232, Loss: 0.4526084553640119\n",
      "Epoch 233, Loss: 0.45228981460773116\n",
      "Epoch 234, Loss: 0.4520949132230242\n",
      "Epoch 235, Loss: 0.45205797975469736\n",
      "Epoch 236, Loss: 0.45230557901617613\n",
      "Epoch 237, Loss: 0.4528345953247846\n",
      "Epoch 238, Loss: 0.4549386133976144\n",
      "Epoch 239, Loss: 0.4564022887834084\n",
      "Epoch 240, Loss: 0.45805755540321125\n",
      "Epoch 241, Loss: 0.4541498302115434\n",
      "Epoch 242, Loss: 0.45158727109343993\n",
      "Epoch 243, Loss: 0.45105347173746146\n",
      "Epoch 244, Loss: 0.45263971453254503\n",
      "Epoch 245, Loss: 0.4550743757756552\n",
      "Epoch 246, Loss: 0.4549471294161167\n",
      "Epoch 247, Loss: 0.45361945699341477\n",
      "Epoch 248, Loss: 0.4506913801287277\n",
      "Epoch 249, Loss: 0.44996455105922556\n",
      "Epoch 250, Loss: 0.45070657227233163\n",
      "Epoch 251, Loss: 0.45134097806864715\n",
      "Epoch 252, Loss: 0.4518641160838718\n",
      "Epoch 253, Loss: 0.45124870735997746\n",
      "Epoch 254, Loss: 0.4507253179113287\n",
      "Epoch 255, Loss: 0.44965336548788515\n",
      "Epoch 256, Loss: 0.4490457284512466\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23531271345837013\n",
      "Valence RMSE: 0.24446438258097822\n",
      "Arousal RMSE: 0.22579041582756146\n",
      "Test R^2 score: tensor([0.3614, 0.6240], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4926954629196015\n",
      "Num of epochs: 257\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Epoch 121, Loss: 0.4816228295571127\n",
      "Epoch 122, Loss: 0.4807618495484584\n",
      "Epoch 123, Loss: 0.48102030846138955\n",
      "Epoch 124, Loss: 0.4810176288364116\n",
      "Epoch 125, Loss: 0.48014338261661127\n",
      "Epoch 126, Loss: 0.47910314809270116\n",
      "Epoch 127, Loss: 0.47880795949683447\n",
      "Epoch 128, Loss: 0.47876177315142754\n",
      "Epoch 129, Loss: 0.47819597116449997\n",
      "Epoch 130, Loss: 0.47743214539542544\n",
      "Epoch 131, Loss: 0.47658476230340185\n",
      "Epoch 132, Loss: 0.4759644680184978\n",
      "Epoch 133, Loss: 0.4755635187712421\n",
      "Epoch 134, Loss: 0.47538264196584307\n",
      "Epoch 135, Loss: 0.4753673450596914\n",
      "Epoch 136, Loss: 0.4753697744168057\n",
      "Epoch 137, Loss: 0.4755973109583155\n",
      "Epoch 138, Loss: 0.4767626826605241\n",
      "Epoch 139, Loss: 0.47683528222287225\n",
      "Epoch 140, Loss: 0.47579445468261566\n",
      "Epoch 141, Loss: 0.47284533919249566\n",
      "Epoch 142, Loss: 0.472809286012082\n",
      "Epoch 143, Loss: 0.47458738111232796\n",
      "Epoch 144, Loss: 0.4733380058918715\n",
      "Epoch 145, Loss: 0.4715229034484008\n",
      "Epoch 146, Loss: 0.47096730966267386\n",
      "Epoch 147, Loss: 0.47161469890467383\n",
      "Epoch 148, Loss: 0.47183531411129226\n",
      "Epoch 149, Loss: 0.470294933688076\n",
      "Epoch 150, Loss: 0.4697114144973765\n",
      "Epoch 151, Loss: 0.47018789053701304\n",
      "Epoch 152, Loss: 0.4700460953124117\n",
      "Epoch 153, Loss: 0.4693645740016508\n",
      "Epoch 154, Loss: 0.4684558104180481\n",
      "Epoch 155, Loss: 0.4680960384979934\n",
      "Epoch 156, Loss: 0.4682454254529793\n",
      "Epoch 157, Loss: 0.4682974379105488\n",
      "Epoch 158, Loss: 0.4681590010361841\n",
      "Epoch 159, Loss: 0.4673225124817593\n",
      "Epoch 160, Loss: 0.4666701428107949\n",
      "Epoch 161, Loss: 0.4663166784414499\n",
      "Epoch 162, Loss: 0.46624854535037\n",
      "Epoch 163, Loss: 0.4664611566834947\n",
      "Epoch 164, Loss: 0.46654532450127006\n",
      "Epoch 165, Loss: 0.46721845607201035\n",
      "Epoch 166, Loss: 0.4668208160006968\n",
      "Epoch 167, Loss: 0.466573749669782\n",
      "Epoch 168, Loss: 0.46506387643568103\n",
      "Epoch 169, Loss: 0.4641220516950567\n",
      "Epoch 170, Loss: 0.4644550172656552\n",
      "Epoch 171, Loss: 0.4654613047116713\n",
      "Epoch 172, Loss: 0.46880176576246846\n",
      "Epoch 173, Loss: 0.4685649824581709\n",
      "Epoch 174, Loss: 0.46606115996454733\n",
      "Epoch 175, Loss: 0.46328263242364126\n",
      "Epoch 176, Loss: 0.4660484187313888\n",
      "Epoch 177, Loss: 0.4667669789465077\n",
      "Epoch 178, Loss: 0.46222975993575777\n",
      "Epoch 179, Loss: 0.46412214801342555\n",
      "Epoch 180, Loss: 0.46867384291927394\n",
      "Epoch 181, Loss: 0.46316269230838986\n",
      "Epoch 182, Loss: 0.4619791093960296\n",
      "Epoch 183, Loss: 0.4650783107301587\n",
      "Epoch 184, Loss: 0.4617210775403838\n",
      "Epoch 185, Loss: 0.4621011464022392\n",
      "Epoch 186, Loss: 0.4641367239628427\n",
      "Epoch 187, Loss: 0.46071131054180564\n",
      "Epoch 188, Loss: 0.46145892957673257\n",
      "Epoch 189, Loss: 0.464304908222076\n",
      "Epoch 190, Loss: 0.4606605441278432\n",
      "Epoch 191, Loss: 0.4601386308130386\n",
      "Epoch 192, Loss: 0.46193430495315263\n",
      "Epoch 193, Loss: 0.45977696912799226\n",
      "Epoch 194, Loss: 0.4597173803227511\n",
      "Epoch 195, Loss: 0.46096589922299847\n",
      "Epoch 196, Loss: 0.4592170612502741\n",
      "Epoch 197, Loss: 0.45864347419013624\n",
      "Epoch 198, Loss: 0.4594650511157848\n",
      "Epoch 199, Loss: 0.45924705942486493\n",
      "Epoch 200, Loss: 0.4584931199827434\n",
      "Epoch 201, Loss: 0.4578552480015059\n",
      "Epoch 202, Loss: 0.458251953125\n",
      "Epoch 203, Loss: 0.4586848316367108\n",
      "Epoch 204, Loss: 0.4577025349799131\n",
      "Epoch 205, Loss: 0.45707273702530193\n",
      "Epoch 206, Loss: 0.4571336158814618\n",
      "Epoch 207, Loss: 0.45746758002181026\n",
      "Epoch 208, Loss: 0.4580334166179955\n",
      "Epoch 209, Loss: 0.457551138813026\n",
      "Epoch 210, Loss: 0.4572461918817771\n",
      "Epoch 211, Loss: 0.45629613394883295\n",
      "Epoch 212, Loss: 0.4557945440718634\n",
      "Epoch 213, Loss: 0.455843155547024\n",
      "Epoch 214, Loss: 0.456219547378948\n",
      "Epoch 215, Loss: 0.4577116995236092\n",
      "Epoch 216, Loss: 0.4589176697652806\n",
      "Epoch 217, Loss: 0.46105034303609993\n",
      "Epoch 218, Loss: 0.4567326442394303\n",
      "Epoch 219, Loss: 0.45493112899589366\n",
      "Epoch 220, Loss: 0.45527199475335134\n",
      "Epoch 221, Loss: 0.45623630281032773\n",
      "Epoch 222, Loss: 0.45802054967026307\n",
      "Epoch 223, Loss: 0.45677224995978094\n",
      "Epoch 224, Loss: 0.45565914342704716\n",
      "Epoch 225, Loss: 0.45401674524485375\n",
      "Epoch 226, Loss: 0.4535057342791329\n",
      "Epoch 227, Loss: 0.45416164205845616\n",
      "Epoch 228, Loss: 0.45450513400796655\n",
      "Epoch 229, Loss: 0.4548238117351997\n",
      "Epoch 230, Loss: 0.4537990916311975\n",
      "Epoch 231, Loss: 0.4532521496977959\n",
      "Epoch 232, Loss: 0.4526084553640119\n",
      "Epoch 233, Loss: 0.45228981460773116\n",
      "Epoch 234, Loss: 0.4520949132230242\n",
      "Epoch 235, Loss: 0.45205797975469736\n",
      "Epoch 236, Loss: 0.45230557901617613\n",
      "Epoch 237, Loss: 0.4528345953247846\n",
      "Epoch 238, Loss: 0.4549386133976144\n",
      "Epoch 239, Loss: 0.4564022887834084\n",
      "Epoch 240, Loss: 0.45805755540321125\n",
      "Epoch 241, Loss: 0.4541498302115434\n",
      "Epoch 242, Loss: 0.45158727109343993\n",
      "Epoch 243, Loss: 0.45105347173746146\n",
      "Epoch 244, Loss: 0.45263971453254503\n",
      "Epoch 245, Loss: 0.4550743757756552\n",
      "Epoch 246, Loss: 0.4549471294161167\n",
      "Epoch 247, Loss: 0.45361945699341477\n",
      "Epoch 248, Loss: 0.4506913801287277\n",
      "Epoch 249, Loss: 0.44996455105922556\n",
      "Epoch 250, Loss: 0.45070657227233163\n",
      "Epoch 251, Loss: 0.45134097806864715\n",
      "Epoch 252, Loss: 0.4518641160838718\n",
      "Epoch 253, Loss: 0.45124870735997746\n",
      "Epoch 254, Loss: 0.4507253179113287\n",
      "Epoch 255, Loss: 0.44965336548788515\n",
      "Epoch 256, Loss: 0.4490457284512466\n",
      "Epoch 257, Loss: 0.44867439459283065\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23514171957589955\n",
      "Valence RMSE: 0.24413050550384857\n",
      "Arousal RMSE: 0.2257953782801085\n",
      "Test R^2 score: tensor([0.3631, 0.6240], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4935588040970794\n",
      "Num of epochs: 258\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Epoch 121, Loss: 0.4816228295571127\n",
      "Epoch 122, Loss: 0.4807618495484584\n",
      "Epoch 123, Loss: 0.48102030846138955\n",
      "Epoch 124, Loss: 0.4810176288364116\n",
      "Epoch 125, Loss: 0.48014338261661127\n",
      "Epoch 126, Loss: 0.47910314809270116\n",
      "Epoch 127, Loss: 0.47880795949683447\n",
      "Epoch 128, Loss: 0.47876177315142754\n",
      "Epoch 129, Loss: 0.47819597116449997\n",
      "Epoch 130, Loss: 0.47743214539542544\n",
      "Epoch 131, Loss: 0.47658476230340185\n",
      "Epoch 132, Loss: 0.4759644680184978\n",
      "Epoch 133, Loss: 0.4755635187712421\n",
      "Epoch 134, Loss: 0.47538264196584307\n",
      "Epoch 135, Loss: 0.4753673450596914\n",
      "Epoch 136, Loss: 0.4753697744168057\n",
      "Epoch 137, Loss: 0.4755973109583155\n",
      "Epoch 138, Loss: 0.4767626826605241\n",
      "Epoch 139, Loss: 0.47683528222287225\n",
      "Epoch 140, Loss: 0.47579445468261566\n",
      "Epoch 141, Loss: 0.47284533919249566\n",
      "Epoch 142, Loss: 0.472809286012082\n",
      "Epoch 143, Loss: 0.47458738111232796\n",
      "Epoch 144, Loss: 0.4733380058918715\n",
      "Epoch 145, Loss: 0.4715229034484008\n",
      "Epoch 146, Loss: 0.47096730966267386\n",
      "Epoch 147, Loss: 0.47161469890467383\n",
      "Epoch 148, Loss: 0.47183531411129226\n",
      "Epoch 149, Loss: 0.470294933688076\n",
      "Epoch 150, Loss: 0.4697114144973765\n",
      "Epoch 151, Loss: 0.47018789053701304\n",
      "Epoch 152, Loss: 0.4700460953124117\n",
      "Epoch 153, Loss: 0.4693645740016508\n",
      "Epoch 154, Loss: 0.4684558104180481\n",
      "Epoch 155, Loss: 0.4680960384979934\n",
      "Epoch 156, Loss: 0.4682454254529793\n",
      "Epoch 157, Loss: 0.4682974379105488\n",
      "Epoch 158, Loss: 0.4681590010361841\n",
      "Epoch 159, Loss: 0.4673225124817593\n",
      "Epoch 160, Loss: 0.4666701428107949\n",
      "Epoch 161, Loss: 0.4663166784414499\n",
      "Epoch 162, Loss: 0.46624854535037\n",
      "Epoch 163, Loss: 0.4664611566834947\n",
      "Epoch 164, Loss: 0.46654532450127006\n",
      "Epoch 165, Loss: 0.46721845607201035\n",
      "Epoch 166, Loss: 0.4668208160006968\n",
      "Epoch 167, Loss: 0.466573749669782\n",
      "Epoch 168, Loss: 0.46506387643568103\n",
      "Epoch 169, Loss: 0.4641220516950567\n",
      "Epoch 170, Loss: 0.4644550172656552\n",
      "Epoch 171, Loss: 0.4654613047116713\n",
      "Epoch 172, Loss: 0.46880176576246846\n",
      "Epoch 173, Loss: 0.4685649824581709\n",
      "Epoch 174, Loss: 0.46606115996454733\n",
      "Epoch 175, Loss: 0.46328263242364126\n",
      "Epoch 176, Loss: 0.4660484187313888\n",
      "Epoch 177, Loss: 0.4667669789465077\n",
      "Epoch 178, Loss: 0.46222975993575777\n",
      "Epoch 179, Loss: 0.46412214801342555\n",
      "Epoch 180, Loss: 0.46867384291927394\n",
      "Epoch 181, Loss: 0.46316269230838986\n",
      "Epoch 182, Loss: 0.4619791093960296\n",
      "Epoch 183, Loss: 0.4650783107301587\n",
      "Epoch 184, Loss: 0.4617210775403838\n",
      "Epoch 185, Loss: 0.4621011464022392\n",
      "Epoch 186, Loss: 0.4641367239628427\n",
      "Epoch 187, Loss: 0.46071131054180564\n",
      "Epoch 188, Loss: 0.46145892957673257\n",
      "Epoch 189, Loss: 0.464304908222076\n",
      "Epoch 190, Loss: 0.4606605441278432\n",
      "Epoch 191, Loss: 0.4601386308130386\n",
      "Epoch 192, Loss: 0.46193430495315263\n",
      "Epoch 193, Loss: 0.45977696912799226\n",
      "Epoch 194, Loss: 0.4597173803227511\n",
      "Epoch 195, Loss: 0.46096589922299847\n",
      "Epoch 196, Loss: 0.4592170612502741\n",
      "Epoch 197, Loss: 0.45864347419013624\n",
      "Epoch 198, Loss: 0.4594650511157848\n",
      "Epoch 199, Loss: 0.45924705942486493\n",
      "Epoch 200, Loss: 0.4584931199827434\n",
      "Epoch 201, Loss: 0.4578552480015059\n",
      "Epoch 202, Loss: 0.458251953125\n",
      "Epoch 203, Loss: 0.4586848316367108\n",
      "Epoch 204, Loss: 0.4577025349799131\n",
      "Epoch 205, Loss: 0.45707273702530193\n",
      "Epoch 206, Loss: 0.4571336158814618\n",
      "Epoch 207, Loss: 0.45746758002181026\n",
      "Epoch 208, Loss: 0.4580334166179955\n",
      "Epoch 209, Loss: 0.457551138813026\n",
      "Epoch 210, Loss: 0.4572461918817771\n",
      "Epoch 211, Loss: 0.45629613394883295\n",
      "Epoch 212, Loss: 0.4557945440718634\n",
      "Epoch 213, Loss: 0.455843155547024\n",
      "Epoch 214, Loss: 0.456219547378948\n",
      "Epoch 215, Loss: 0.4577116995236092\n",
      "Epoch 216, Loss: 0.4589176697652806\n",
      "Epoch 217, Loss: 0.46105034303609993\n",
      "Epoch 218, Loss: 0.4567326442394303\n",
      "Epoch 219, Loss: 0.45493112899589366\n",
      "Epoch 220, Loss: 0.45527199475335134\n",
      "Epoch 221, Loss: 0.45623630281032773\n",
      "Epoch 222, Loss: 0.45802054967026307\n",
      "Epoch 223, Loss: 0.45677224995978094\n",
      "Epoch 224, Loss: 0.45565914342704716\n",
      "Epoch 225, Loss: 0.45401674524485375\n",
      "Epoch 226, Loss: 0.4535057342791329\n",
      "Epoch 227, Loss: 0.45416164205845616\n",
      "Epoch 228, Loss: 0.45450513400796655\n",
      "Epoch 229, Loss: 0.4548238117351997\n",
      "Epoch 230, Loss: 0.4537990916311975\n",
      "Epoch 231, Loss: 0.4532521496977959\n",
      "Epoch 232, Loss: 0.4526084553640119\n",
      "Epoch 233, Loss: 0.45228981460773116\n",
      "Epoch 234, Loss: 0.4520949132230242\n",
      "Epoch 235, Loss: 0.45205797975469736\n",
      "Epoch 236, Loss: 0.45230557901617613\n",
      "Epoch 237, Loss: 0.4528345953247846\n",
      "Epoch 238, Loss: 0.4549386133976144\n",
      "Epoch 239, Loss: 0.4564022887834084\n",
      "Epoch 240, Loss: 0.45805755540321125\n",
      "Epoch 241, Loss: 0.4541498302115434\n",
      "Epoch 242, Loss: 0.45158727109343993\n",
      "Epoch 243, Loss: 0.45105347173746146\n",
      "Epoch 244, Loss: 0.45263971453254503\n",
      "Epoch 245, Loss: 0.4550743757756552\n",
      "Epoch 246, Loss: 0.4549471294161167\n",
      "Epoch 247, Loss: 0.45361945699341477\n",
      "Epoch 248, Loss: 0.4506913801287277\n",
      "Epoch 249, Loss: 0.44996455105922556\n",
      "Epoch 250, Loss: 0.45070657227233163\n",
      "Epoch 251, Loss: 0.45134097806864715\n",
      "Epoch 252, Loss: 0.4518641160838718\n",
      "Epoch 253, Loss: 0.45124870735997746\n",
      "Epoch 254, Loss: 0.4507253179113287\n",
      "Epoch 255, Loss: 0.44965336548788515\n",
      "Epoch 256, Loss: 0.4490457284512466\n",
      "Epoch 257, Loss: 0.44867439459283065\n",
      "Epoch 258, Loss: 0.4485724236213288\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23580813165434106\n",
      "Valence RMSE: 0.24576645421158758\n",
      "Arousal RMSE: 0.22541029234018062\n",
      "Test R^2 score: tensor([0.3546, 0.6253], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.48991739806020707\n",
      "Num of epochs: 259\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Epoch 121, Loss: 0.4816228295571127\n",
      "Epoch 122, Loss: 0.4807618495484584\n",
      "Epoch 123, Loss: 0.48102030846138955\n",
      "Epoch 124, Loss: 0.4810176288364116\n",
      "Epoch 125, Loss: 0.48014338261661127\n",
      "Epoch 126, Loss: 0.47910314809270116\n",
      "Epoch 127, Loss: 0.47880795949683447\n",
      "Epoch 128, Loss: 0.47876177315142754\n",
      "Epoch 129, Loss: 0.47819597116449997\n",
      "Epoch 130, Loss: 0.47743214539542544\n",
      "Epoch 131, Loss: 0.47658476230340185\n",
      "Epoch 132, Loss: 0.4759644680184978\n",
      "Epoch 133, Loss: 0.4755635187712421\n",
      "Epoch 134, Loss: 0.47538264196584307\n",
      "Epoch 135, Loss: 0.4753673450596914\n",
      "Epoch 136, Loss: 0.4753697744168057\n",
      "Epoch 137, Loss: 0.4755973109583155\n",
      "Epoch 138, Loss: 0.4767626826605241\n",
      "Epoch 139, Loss: 0.47683528222287225\n",
      "Epoch 140, Loss: 0.47579445468261566\n",
      "Epoch 141, Loss: 0.47284533919249566\n",
      "Epoch 142, Loss: 0.472809286012082\n",
      "Epoch 143, Loss: 0.47458738111232796\n",
      "Epoch 144, Loss: 0.4733380058918715\n",
      "Epoch 145, Loss: 0.4715229034484008\n",
      "Epoch 146, Loss: 0.47096730966267386\n",
      "Epoch 147, Loss: 0.47161469890467383\n",
      "Epoch 148, Loss: 0.47183531411129226\n",
      "Epoch 149, Loss: 0.470294933688076\n",
      "Epoch 150, Loss: 0.4697114144973765\n",
      "Epoch 151, Loss: 0.47018789053701304\n",
      "Epoch 152, Loss: 0.4700460953124117\n",
      "Epoch 153, Loss: 0.4693645740016508\n",
      "Epoch 154, Loss: 0.4684558104180481\n",
      "Epoch 155, Loss: 0.4680960384979934\n",
      "Epoch 156, Loss: 0.4682454254529793\n",
      "Epoch 157, Loss: 0.4682974379105488\n",
      "Epoch 158, Loss: 0.4681590010361841\n",
      "Epoch 159, Loss: 0.4673225124817593\n",
      "Epoch 160, Loss: 0.4666701428107949\n",
      "Epoch 161, Loss: 0.4663166784414499\n",
      "Epoch 162, Loss: 0.46624854535037\n",
      "Epoch 163, Loss: 0.4664611566834947\n",
      "Epoch 164, Loss: 0.46654532450127006\n",
      "Epoch 165, Loss: 0.46721845607201035\n",
      "Epoch 166, Loss: 0.4668208160006968\n",
      "Epoch 167, Loss: 0.466573749669782\n",
      "Epoch 168, Loss: 0.46506387643568103\n",
      "Epoch 169, Loss: 0.4641220516950567\n",
      "Epoch 170, Loss: 0.4644550172656552\n",
      "Epoch 171, Loss: 0.4654613047116713\n",
      "Epoch 172, Loss: 0.46880176576246846\n",
      "Epoch 173, Loss: 0.4685649824581709\n",
      "Epoch 174, Loss: 0.46606115996454733\n",
      "Epoch 175, Loss: 0.46328263242364126\n",
      "Epoch 176, Loss: 0.4660484187313888\n",
      "Epoch 177, Loss: 0.4667669789465077\n",
      "Epoch 178, Loss: 0.46222975993575777\n",
      "Epoch 179, Loss: 0.46412214801342555\n",
      "Epoch 180, Loss: 0.46867384291927394\n",
      "Epoch 181, Loss: 0.46316269230838986\n",
      "Epoch 182, Loss: 0.4619791093960296\n",
      "Epoch 183, Loss: 0.4650783107301587\n",
      "Epoch 184, Loss: 0.4617210775403838\n",
      "Epoch 185, Loss: 0.4621011464022392\n",
      "Epoch 186, Loss: 0.4641367239628427\n",
      "Epoch 187, Loss: 0.46071131054180564\n",
      "Epoch 188, Loss: 0.46145892957673257\n",
      "Epoch 189, Loss: 0.464304908222076\n",
      "Epoch 190, Loss: 0.4606605441278432\n",
      "Epoch 191, Loss: 0.4601386308130386\n",
      "Epoch 192, Loss: 0.46193430495315263\n",
      "Epoch 193, Loss: 0.45977696912799226\n",
      "Epoch 194, Loss: 0.4597173803227511\n",
      "Epoch 195, Loss: 0.46096589922299847\n",
      "Epoch 196, Loss: 0.4592170612502741\n",
      "Epoch 197, Loss: 0.45864347419013624\n",
      "Epoch 198, Loss: 0.4594650511157848\n",
      "Epoch 199, Loss: 0.45924705942486493\n",
      "Epoch 200, Loss: 0.4584931199827434\n",
      "Epoch 201, Loss: 0.4578552480015059\n",
      "Epoch 202, Loss: 0.458251953125\n",
      "Epoch 203, Loss: 0.4586848316367108\n",
      "Epoch 204, Loss: 0.4577025349799131\n",
      "Epoch 205, Loss: 0.45707273702530193\n",
      "Epoch 206, Loss: 0.4571336158814618\n",
      "Epoch 207, Loss: 0.45746758002181026\n",
      "Epoch 208, Loss: 0.4580334166179955\n",
      "Epoch 209, Loss: 0.457551138813026\n",
      "Epoch 210, Loss: 0.4572461918817771\n",
      "Epoch 211, Loss: 0.45629613394883295\n",
      "Epoch 212, Loss: 0.4557945440718634\n",
      "Epoch 213, Loss: 0.455843155547024\n",
      "Epoch 214, Loss: 0.456219547378948\n",
      "Epoch 215, Loss: 0.4577116995236092\n",
      "Epoch 216, Loss: 0.4589176697652806\n",
      "Epoch 217, Loss: 0.46105034303609993\n",
      "Epoch 218, Loss: 0.4567326442394303\n",
      "Epoch 219, Loss: 0.45493112899589366\n",
      "Epoch 220, Loss: 0.45527199475335134\n",
      "Epoch 221, Loss: 0.45623630281032773\n",
      "Epoch 222, Loss: 0.45802054967026307\n",
      "Epoch 223, Loss: 0.45677224995978094\n",
      "Epoch 224, Loss: 0.45565914342704716\n",
      "Epoch 225, Loss: 0.45401674524485375\n",
      "Epoch 226, Loss: 0.4535057342791329\n",
      "Epoch 227, Loss: 0.45416164205845616\n",
      "Epoch 228, Loss: 0.45450513400796655\n",
      "Epoch 229, Loss: 0.4548238117351997\n",
      "Epoch 230, Loss: 0.4537990916311975\n",
      "Epoch 231, Loss: 0.4532521496977959\n",
      "Epoch 232, Loss: 0.4526084553640119\n",
      "Epoch 233, Loss: 0.45228981460773116\n",
      "Epoch 234, Loss: 0.4520949132230242\n",
      "Epoch 235, Loss: 0.45205797975469736\n",
      "Epoch 236, Loss: 0.45230557901617613\n",
      "Epoch 237, Loss: 0.4528345953247846\n",
      "Epoch 238, Loss: 0.4549386133976144\n",
      "Epoch 239, Loss: 0.4564022887834084\n",
      "Epoch 240, Loss: 0.45805755540321125\n",
      "Epoch 241, Loss: 0.4541498302115434\n",
      "Epoch 242, Loss: 0.45158727109343993\n",
      "Epoch 243, Loss: 0.45105347173746146\n",
      "Epoch 244, Loss: 0.45263971453254503\n",
      "Epoch 245, Loss: 0.4550743757756552\n",
      "Epoch 246, Loss: 0.4549471294161167\n",
      "Epoch 247, Loss: 0.45361945699341477\n",
      "Epoch 248, Loss: 0.4506913801287277\n",
      "Epoch 249, Loss: 0.44996455105922556\n",
      "Epoch 250, Loss: 0.45070657227233163\n",
      "Epoch 251, Loss: 0.45134097806864715\n",
      "Epoch 252, Loss: 0.4518641160838718\n",
      "Epoch 253, Loss: 0.45124870735997746\n",
      "Epoch 254, Loss: 0.4507253179113287\n",
      "Epoch 255, Loss: 0.44965336548788515\n",
      "Epoch 256, Loss: 0.4490457284512466\n",
      "Epoch 257, Loss: 0.44867439459283065\n",
      "Epoch 258, Loss: 0.4485724236213288\n",
      "Epoch 259, Loss: 0.4487178165615502\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23522385013099475\n",
      "Valence RMSE: 0.2431432720535086\n",
      "Arousal RMSE: 0.22702834315567488\n",
      "Test R^2 score: tensor([0.3683, 0.6199], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.49407028566915806\n",
      "Num of epochs: 260\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Epoch 121, Loss: 0.4816228295571127\n",
      "Epoch 122, Loss: 0.4807618495484584\n",
      "Epoch 123, Loss: 0.48102030846138955\n",
      "Epoch 124, Loss: 0.4810176288364116\n",
      "Epoch 125, Loss: 0.48014338261661127\n",
      "Epoch 126, Loss: 0.47910314809270116\n",
      "Epoch 127, Loss: 0.47880795949683447\n",
      "Epoch 128, Loss: 0.47876177315142754\n",
      "Epoch 129, Loss: 0.47819597116449997\n",
      "Epoch 130, Loss: 0.47743214539542544\n",
      "Epoch 131, Loss: 0.47658476230340185\n",
      "Epoch 132, Loss: 0.4759644680184978\n",
      "Epoch 133, Loss: 0.4755635187712421\n",
      "Epoch 134, Loss: 0.47538264196584307\n",
      "Epoch 135, Loss: 0.4753673450596914\n",
      "Epoch 136, Loss: 0.4753697744168057\n",
      "Epoch 137, Loss: 0.4755973109583155\n",
      "Epoch 138, Loss: 0.4767626826605241\n",
      "Epoch 139, Loss: 0.47683528222287225\n",
      "Epoch 140, Loss: 0.47579445468261566\n",
      "Epoch 141, Loss: 0.47284533919249566\n",
      "Epoch 142, Loss: 0.472809286012082\n",
      "Epoch 143, Loss: 0.47458738111232796\n",
      "Epoch 144, Loss: 0.4733380058918715\n",
      "Epoch 145, Loss: 0.4715229034484008\n",
      "Epoch 146, Loss: 0.47096730966267386\n",
      "Epoch 147, Loss: 0.47161469890467383\n",
      "Epoch 148, Loss: 0.47183531411129226\n",
      "Epoch 149, Loss: 0.470294933688076\n",
      "Epoch 150, Loss: 0.4697114144973765\n",
      "Epoch 151, Loss: 0.47018789053701304\n",
      "Epoch 152, Loss: 0.4700460953124117\n",
      "Epoch 153, Loss: 0.4693645740016508\n",
      "Epoch 154, Loss: 0.4684558104180481\n",
      "Epoch 155, Loss: 0.4680960384979934\n",
      "Epoch 156, Loss: 0.4682454254529793\n",
      "Epoch 157, Loss: 0.4682974379105488\n",
      "Epoch 158, Loss: 0.4681590010361841\n",
      "Epoch 159, Loss: 0.4673225124817593\n",
      "Epoch 160, Loss: 0.4666701428107949\n",
      "Epoch 161, Loss: 0.4663166784414499\n",
      "Epoch 162, Loss: 0.46624854535037\n",
      "Epoch 163, Loss: 0.4664611566834947\n",
      "Epoch 164, Loss: 0.46654532450127006\n",
      "Epoch 165, Loss: 0.46721845607201035\n",
      "Epoch 166, Loss: 0.4668208160006968\n",
      "Epoch 167, Loss: 0.466573749669782\n",
      "Epoch 168, Loss: 0.46506387643568103\n",
      "Epoch 169, Loss: 0.4641220516950567\n",
      "Epoch 170, Loss: 0.4644550172656552\n",
      "Epoch 171, Loss: 0.4654613047116713\n",
      "Epoch 172, Loss: 0.46880176576246846\n",
      "Epoch 173, Loss: 0.4685649824581709\n",
      "Epoch 174, Loss: 0.46606115996454733\n",
      "Epoch 175, Loss: 0.46328263242364126\n",
      "Epoch 176, Loss: 0.4660484187313888\n",
      "Epoch 177, Loss: 0.4667669789465077\n",
      "Epoch 178, Loss: 0.46222975993575777\n",
      "Epoch 179, Loss: 0.46412214801342555\n",
      "Epoch 180, Loss: 0.46867384291927394\n",
      "Epoch 181, Loss: 0.46316269230838986\n",
      "Epoch 182, Loss: 0.4619791093960296\n",
      "Epoch 183, Loss: 0.4650783107301587\n",
      "Epoch 184, Loss: 0.4617210775403838\n",
      "Epoch 185, Loss: 0.4621011464022392\n",
      "Epoch 186, Loss: 0.4641367239628427\n",
      "Epoch 187, Loss: 0.46071131054180564\n",
      "Epoch 188, Loss: 0.46145892957673257\n",
      "Epoch 189, Loss: 0.464304908222076\n",
      "Epoch 190, Loss: 0.4606605441278432\n",
      "Epoch 191, Loss: 0.4601386308130386\n",
      "Epoch 192, Loss: 0.46193430495315263\n",
      "Epoch 193, Loss: 0.45977696912799226\n",
      "Epoch 194, Loss: 0.4597173803227511\n",
      "Epoch 195, Loss: 0.46096589922299847\n",
      "Epoch 196, Loss: 0.4592170612502741\n",
      "Epoch 197, Loss: 0.45864347419013624\n",
      "Epoch 198, Loss: 0.4594650511157848\n",
      "Epoch 199, Loss: 0.45924705942486493\n",
      "Epoch 200, Loss: 0.4584931199827434\n",
      "Epoch 201, Loss: 0.4578552480015059\n",
      "Epoch 202, Loss: 0.458251953125\n",
      "Epoch 203, Loss: 0.4586848316367108\n",
      "Epoch 204, Loss: 0.4577025349799131\n",
      "Epoch 205, Loss: 0.45707273702530193\n",
      "Epoch 206, Loss: 0.4571336158814618\n",
      "Epoch 207, Loss: 0.45746758002181026\n",
      "Epoch 208, Loss: 0.4580334166179955\n",
      "Epoch 209, Loss: 0.457551138813026\n",
      "Epoch 210, Loss: 0.4572461918817771\n",
      "Epoch 211, Loss: 0.45629613394883295\n",
      "Epoch 212, Loss: 0.4557945440718634\n",
      "Epoch 213, Loss: 0.455843155547024\n",
      "Epoch 214, Loss: 0.456219547378948\n",
      "Epoch 215, Loss: 0.4577116995236092\n",
      "Epoch 216, Loss: 0.4589176697652806\n",
      "Epoch 217, Loss: 0.46105034303609993\n",
      "Epoch 218, Loss: 0.4567326442394303\n",
      "Epoch 219, Loss: 0.45493112899589366\n",
      "Epoch 220, Loss: 0.45527199475335134\n",
      "Epoch 221, Loss: 0.45623630281032773\n",
      "Epoch 222, Loss: 0.45802054967026307\n",
      "Epoch 223, Loss: 0.45677224995978094\n",
      "Epoch 224, Loss: 0.45565914342704716\n",
      "Epoch 225, Loss: 0.45401674524485375\n",
      "Epoch 226, Loss: 0.4535057342791329\n",
      "Epoch 227, Loss: 0.45416164205845616\n",
      "Epoch 228, Loss: 0.45450513400796655\n",
      "Epoch 229, Loss: 0.4548238117351997\n",
      "Epoch 230, Loss: 0.4537990916311975\n",
      "Epoch 231, Loss: 0.4532521496977959\n",
      "Epoch 232, Loss: 0.4526084553640119\n",
      "Epoch 233, Loss: 0.45228981460773116\n",
      "Epoch 234, Loss: 0.4520949132230242\n",
      "Epoch 235, Loss: 0.45205797975469736\n",
      "Epoch 236, Loss: 0.45230557901617613\n",
      "Epoch 237, Loss: 0.4528345953247846\n",
      "Epoch 238, Loss: 0.4549386133976144\n",
      "Epoch 239, Loss: 0.4564022887834084\n",
      "Epoch 240, Loss: 0.45805755540321125\n",
      "Epoch 241, Loss: 0.4541498302115434\n",
      "Epoch 242, Loss: 0.45158727109343993\n",
      "Epoch 243, Loss: 0.45105347173746146\n",
      "Epoch 244, Loss: 0.45263971453254503\n",
      "Epoch 245, Loss: 0.4550743757756552\n",
      "Epoch 246, Loss: 0.4549471294161167\n",
      "Epoch 247, Loss: 0.45361945699341477\n",
      "Epoch 248, Loss: 0.4506913801287277\n",
      "Epoch 249, Loss: 0.44996455105922556\n",
      "Epoch 250, Loss: 0.45070657227233163\n",
      "Epoch 251, Loss: 0.45134097806864715\n",
      "Epoch 252, Loss: 0.4518641160838718\n",
      "Epoch 253, Loss: 0.45124870735997746\n",
      "Epoch 254, Loss: 0.4507253179113287\n",
      "Epoch 255, Loss: 0.44965336548788515\n",
      "Epoch 256, Loss: 0.4490457284512466\n",
      "Epoch 257, Loss: 0.44867439459283065\n",
      "Epoch 258, Loss: 0.4485724236213288\n",
      "Epoch 259, Loss: 0.4487178165615502\n",
      "Epoch 260, Loss: 0.44913423821312026\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23794438943661333\n",
      "Valence RMSE: 0.24905969617414775\n",
      "Arousal RMSE: 0.22628374371652651\n",
      "Test R^2 score: tensor([0.3371, 0.6224], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4797557311470989\n",
      "Num of epochs: 261\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Epoch 121, Loss: 0.4816228295571127\n",
      "Epoch 122, Loss: 0.4807618495484584\n",
      "Epoch 123, Loss: 0.48102030846138955\n",
      "Epoch 124, Loss: 0.4810176288364116\n",
      "Epoch 125, Loss: 0.48014338261661127\n",
      "Epoch 126, Loss: 0.47910314809270116\n",
      "Epoch 127, Loss: 0.47880795949683447\n",
      "Epoch 128, Loss: 0.47876177315142754\n",
      "Epoch 129, Loss: 0.47819597116449997\n",
      "Epoch 130, Loss: 0.47743214539542544\n",
      "Epoch 131, Loss: 0.47658476230340185\n",
      "Epoch 132, Loss: 0.4759644680184978\n",
      "Epoch 133, Loss: 0.4755635187712421\n",
      "Epoch 134, Loss: 0.47538264196584307\n",
      "Epoch 135, Loss: 0.4753673450596914\n",
      "Epoch 136, Loss: 0.4753697744168057\n",
      "Epoch 137, Loss: 0.4755973109583155\n",
      "Epoch 138, Loss: 0.4767626826605241\n",
      "Epoch 139, Loss: 0.47683528222287225\n",
      "Epoch 140, Loss: 0.47579445468261566\n",
      "Epoch 141, Loss: 0.47284533919249566\n",
      "Epoch 142, Loss: 0.472809286012082\n",
      "Epoch 143, Loss: 0.47458738111232796\n",
      "Epoch 144, Loss: 0.4733380058918715\n",
      "Epoch 145, Loss: 0.4715229034484008\n",
      "Epoch 146, Loss: 0.47096730966267386\n",
      "Epoch 147, Loss: 0.47161469890467383\n",
      "Epoch 148, Loss: 0.47183531411129226\n",
      "Epoch 149, Loss: 0.470294933688076\n",
      "Epoch 150, Loss: 0.4697114144973765\n",
      "Epoch 151, Loss: 0.47018789053701304\n",
      "Epoch 152, Loss: 0.4700460953124117\n",
      "Epoch 153, Loss: 0.4693645740016508\n",
      "Epoch 154, Loss: 0.4684558104180481\n",
      "Epoch 155, Loss: 0.4680960384979934\n",
      "Epoch 156, Loss: 0.4682454254529793\n",
      "Epoch 157, Loss: 0.4682974379105488\n",
      "Epoch 158, Loss: 0.4681590010361841\n",
      "Epoch 159, Loss: 0.4673225124817593\n",
      "Epoch 160, Loss: 0.4666701428107949\n",
      "Epoch 161, Loss: 0.4663166784414499\n",
      "Epoch 162, Loss: 0.46624854535037\n",
      "Epoch 163, Loss: 0.4664611566834947\n",
      "Epoch 164, Loss: 0.46654532450127006\n",
      "Epoch 165, Loss: 0.46721845607201035\n",
      "Epoch 166, Loss: 0.4668208160006968\n",
      "Epoch 167, Loss: 0.466573749669782\n",
      "Epoch 168, Loss: 0.46506387643568103\n",
      "Epoch 169, Loss: 0.4641220516950567\n",
      "Epoch 170, Loss: 0.4644550172656552\n",
      "Epoch 171, Loss: 0.4654613047116713\n",
      "Epoch 172, Loss: 0.46880176576246846\n",
      "Epoch 173, Loss: 0.4685649824581709\n",
      "Epoch 174, Loss: 0.46606115996454733\n",
      "Epoch 175, Loss: 0.46328263242364126\n",
      "Epoch 176, Loss: 0.4660484187313888\n",
      "Epoch 177, Loss: 0.4667669789465077\n",
      "Epoch 178, Loss: 0.46222975993575777\n",
      "Epoch 179, Loss: 0.46412214801342555\n",
      "Epoch 180, Loss: 0.46867384291927394\n",
      "Epoch 181, Loss: 0.46316269230838986\n",
      "Epoch 182, Loss: 0.4619791093960296\n",
      "Epoch 183, Loss: 0.4650783107301587\n",
      "Epoch 184, Loss: 0.4617210775403838\n",
      "Epoch 185, Loss: 0.4621011464022392\n",
      "Epoch 186, Loss: 0.4641367239628427\n",
      "Epoch 187, Loss: 0.46071131054180564\n",
      "Epoch 188, Loss: 0.46145892957673257\n",
      "Epoch 189, Loss: 0.464304908222076\n",
      "Epoch 190, Loss: 0.4606605441278432\n",
      "Epoch 191, Loss: 0.4601386308130386\n",
      "Epoch 192, Loss: 0.46193430495315263\n",
      "Epoch 193, Loss: 0.45977696912799226\n",
      "Epoch 194, Loss: 0.4597173803227511\n",
      "Epoch 195, Loss: 0.46096589922299847\n",
      "Epoch 196, Loss: 0.4592170612502741\n",
      "Epoch 197, Loss: 0.45864347419013624\n",
      "Epoch 198, Loss: 0.4594650511157848\n",
      "Epoch 199, Loss: 0.45924705942486493\n",
      "Epoch 200, Loss: 0.4584931199827434\n",
      "Epoch 201, Loss: 0.4578552480015059\n",
      "Epoch 202, Loss: 0.458251953125\n",
      "Epoch 203, Loss: 0.4586848316367108\n",
      "Epoch 204, Loss: 0.4577025349799131\n",
      "Epoch 205, Loss: 0.45707273702530193\n",
      "Epoch 206, Loss: 0.4571336158814618\n",
      "Epoch 207, Loss: 0.45746758002181026\n",
      "Epoch 208, Loss: 0.4580334166179955\n",
      "Epoch 209, Loss: 0.457551138813026\n",
      "Epoch 210, Loss: 0.4572461918817771\n",
      "Epoch 211, Loss: 0.45629613394883295\n",
      "Epoch 212, Loss: 0.4557945440718634\n",
      "Epoch 213, Loss: 0.455843155547024\n",
      "Epoch 214, Loss: 0.456219547378948\n",
      "Epoch 215, Loss: 0.4577116995236092\n",
      "Epoch 216, Loss: 0.4589176697652806\n",
      "Epoch 217, Loss: 0.46105034303609993\n",
      "Epoch 218, Loss: 0.4567326442394303\n",
      "Epoch 219, Loss: 0.45493112899589366\n",
      "Epoch 220, Loss: 0.45527199475335134\n",
      "Epoch 221, Loss: 0.45623630281032773\n",
      "Epoch 222, Loss: 0.45802054967026307\n",
      "Epoch 223, Loss: 0.45677224995978094\n",
      "Epoch 224, Loss: 0.45565914342704716\n",
      "Epoch 225, Loss: 0.45401674524485375\n",
      "Epoch 226, Loss: 0.4535057342791329\n",
      "Epoch 227, Loss: 0.45416164205845616\n",
      "Epoch 228, Loss: 0.45450513400796655\n",
      "Epoch 229, Loss: 0.4548238117351997\n",
      "Epoch 230, Loss: 0.4537990916311975\n",
      "Epoch 231, Loss: 0.4532521496977959\n",
      "Epoch 232, Loss: 0.4526084553640119\n",
      "Epoch 233, Loss: 0.45228981460773116\n",
      "Epoch 234, Loss: 0.4520949132230242\n",
      "Epoch 235, Loss: 0.45205797975469736\n",
      "Epoch 236, Loss: 0.45230557901617613\n",
      "Epoch 237, Loss: 0.4528345953247846\n",
      "Epoch 238, Loss: 0.4549386133976144\n",
      "Epoch 239, Loss: 0.4564022887834084\n",
      "Epoch 240, Loss: 0.45805755540321125\n",
      "Epoch 241, Loss: 0.4541498302115434\n",
      "Epoch 242, Loss: 0.45158727109343993\n",
      "Epoch 243, Loss: 0.45105347173746146\n",
      "Epoch 244, Loss: 0.45263971453254503\n",
      "Epoch 245, Loss: 0.4550743757756552\n",
      "Epoch 246, Loss: 0.4549471294161167\n",
      "Epoch 247, Loss: 0.45361945699341477\n",
      "Epoch 248, Loss: 0.4506913801287277\n",
      "Epoch 249, Loss: 0.44996455105922556\n",
      "Epoch 250, Loss: 0.45070657227233163\n",
      "Epoch 251, Loss: 0.45134097806864715\n",
      "Epoch 252, Loss: 0.4518641160838718\n",
      "Epoch 253, Loss: 0.45124870735997746\n",
      "Epoch 254, Loss: 0.4507253179113287\n",
      "Epoch 255, Loss: 0.44965336548788515\n",
      "Epoch 256, Loss: 0.4490457284512466\n",
      "Epoch 257, Loss: 0.44867439459283065\n",
      "Epoch 258, Loss: 0.4485724236213288\n",
      "Epoch 259, Loss: 0.4487178165615502\n",
      "Epoch 260, Loss: 0.44913423821312026\n",
      "Epoch 261, Loss: 0.45068397397968624\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23687640814501862\n",
      "Valence RMSE: 0.2431698507871489\n",
      "Arousal RMSE: 0.23041113067629054\n",
      "Test R^2 score: tensor([0.3681, 0.6085], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4882951430202969\n",
      "Num of epochs: 262\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Epoch 121, Loss: 0.4816228295571127\n",
      "Epoch 122, Loss: 0.4807618495484584\n",
      "Epoch 123, Loss: 0.48102030846138955\n",
      "Epoch 124, Loss: 0.4810176288364116\n",
      "Epoch 125, Loss: 0.48014338261661127\n",
      "Epoch 126, Loss: 0.47910314809270116\n",
      "Epoch 127, Loss: 0.47880795949683447\n",
      "Epoch 128, Loss: 0.47876177315142754\n",
      "Epoch 129, Loss: 0.47819597116449997\n",
      "Epoch 130, Loss: 0.47743214539542544\n",
      "Epoch 131, Loss: 0.47658476230340185\n",
      "Epoch 132, Loss: 0.4759644680184978\n",
      "Epoch 133, Loss: 0.4755635187712421\n",
      "Epoch 134, Loss: 0.47538264196584307\n",
      "Epoch 135, Loss: 0.4753673450596914\n",
      "Epoch 136, Loss: 0.4753697744168057\n",
      "Epoch 137, Loss: 0.4755973109583155\n",
      "Epoch 138, Loss: 0.4767626826605241\n",
      "Epoch 139, Loss: 0.47683528222287225\n",
      "Epoch 140, Loss: 0.47579445468261566\n",
      "Epoch 141, Loss: 0.47284533919249566\n",
      "Epoch 142, Loss: 0.472809286012082\n",
      "Epoch 143, Loss: 0.47458738111232796\n",
      "Epoch 144, Loss: 0.4733380058918715\n",
      "Epoch 145, Loss: 0.4715229034484008\n",
      "Epoch 146, Loss: 0.47096730966267386\n",
      "Epoch 147, Loss: 0.47161469890467383\n",
      "Epoch 148, Loss: 0.47183531411129226\n",
      "Epoch 149, Loss: 0.470294933688076\n",
      "Epoch 150, Loss: 0.4697114144973765\n",
      "Epoch 151, Loss: 0.47018789053701304\n",
      "Epoch 152, Loss: 0.4700460953124117\n",
      "Epoch 153, Loss: 0.4693645740016508\n",
      "Epoch 154, Loss: 0.4684558104180481\n",
      "Epoch 155, Loss: 0.4680960384979934\n",
      "Epoch 156, Loss: 0.4682454254529793\n",
      "Epoch 157, Loss: 0.4682974379105488\n",
      "Epoch 158, Loss: 0.4681590010361841\n",
      "Epoch 159, Loss: 0.4673225124817593\n",
      "Epoch 160, Loss: 0.4666701428107949\n",
      "Epoch 161, Loss: 0.4663166784414499\n",
      "Epoch 162, Loss: 0.46624854535037\n",
      "Epoch 163, Loss: 0.4664611566834947\n",
      "Epoch 164, Loss: 0.46654532450127006\n",
      "Epoch 165, Loss: 0.46721845607201035\n",
      "Epoch 166, Loss: 0.4668208160006968\n",
      "Epoch 167, Loss: 0.466573749669782\n",
      "Epoch 168, Loss: 0.46506387643568103\n",
      "Epoch 169, Loss: 0.4641220516950567\n",
      "Epoch 170, Loss: 0.4644550172656552\n",
      "Epoch 171, Loss: 0.4654613047116713\n",
      "Epoch 172, Loss: 0.46880176576246846\n",
      "Epoch 173, Loss: 0.4685649824581709\n",
      "Epoch 174, Loss: 0.46606115996454733\n",
      "Epoch 175, Loss: 0.46328263242364126\n",
      "Epoch 176, Loss: 0.4660484187313888\n",
      "Epoch 177, Loss: 0.4667669789465077\n",
      "Epoch 178, Loss: 0.46222975993575777\n",
      "Epoch 179, Loss: 0.46412214801342555\n",
      "Epoch 180, Loss: 0.46867384291927394\n",
      "Epoch 181, Loss: 0.46316269230838986\n",
      "Epoch 182, Loss: 0.4619791093960296\n",
      "Epoch 183, Loss: 0.4650783107301587\n",
      "Epoch 184, Loss: 0.4617210775403838\n",
      "Epoch 185, Loss: 0.4621011464022392\n",
      "Epoch 186, Loss: 0.4641367239628427\n",
      "Epoch 187, Loss: 0.46071131054180564\n",
      "Epoch 188, Loss: 0.46145892957673257\n",
      "Epoch 189, Loss: 0.464304908222076\n",
      "Epoch 190, Loss: 0.4606605441278432\n",
      "Epoch 191, Loss: 0.4601386308130386\n",
      "Epoch 192, Loss: 0.46193430495315263\n",
      "Epoch 193, Loss: 0.45977696912799226\n",
      "Epoch 194, Loss: 0.4597173803227511\n",
      "Epoch 195, Loss: 0.46096589922299847\n",
      "Epoch 196, Loss: 0.4592170612502741\n",
      "Epoch 197, Loss: 0.45864347419013624\n",
      "Epoch 198, Loss: 0.4594650511157848\n",
      "Epoch 199, Loss: 0.45924705942486493\n",
      "Epoch 200, Loss: 0.4584931199827434\n",
      "Epoch 201, Loss: 0.4578552480015059\n",
      "Epoch 202, Loss: 0.458251953125\n",
      "Epoch 203, Loss: 0.4586848316367108\n",
      "Epoch 204, Loss: 0.4577025349799131\n",
      "Epoch 205, Loss: 0.45707273702530193\n",
      "Epoch 206, Loss: 0.4571336158814618\n",
      "Epoch 207, Loss: 0.45746758002181026\n",
      "Epoch 208, Loss: 0.4580334166179955\n",
      "Epoch 209, Loss: 0.457551138813026\n",
      "Epoch 210, Loss: 0.4572461918817771\n",
      "Epoch 211, Loss: 0.45629613394883295\n",
      "Epoch 212, Loss: 0.4557945440718634\n",
      "Epoch 213, Loss: 0.455843155547024\n",
      "Epoch 214, Loss: 0.456219547378948\n",
      "Epoch 215, Loss: 0.4577116995236092\n",
      "Epoch 216, Loss: 0.4589176697652806\n",
      "Epoch 217, Loss: 0.46105034303609993\n",
      "Epoch 218, Loss: 0.4567326442394303\n",
      "Epoch 219, Loss: 0.45493112899589366\n",
      "Epoch 220, Loss: 0.45527199475335134\n",
      "Epoch 221, Loss: 0.45623630281032773\n",
      "Epoch 222, Loss: 0.45802054967026307\n",
      "Epoch 223, Loss: 0.45677224995978094\n",
      "Epoch 224, Loss: 0.45565914342704716\n",
      "Epoch 225, Loss: 0.45401674524485375\n",
      "Epoch 226, Loss: 0.4535057342791329\n",
      "Epoch 227, Loss: 0.45416164205845616\n",
      "Epoch 228, Loss: 0.45450513400796655\n",
      "Epoch 229, Loss: 0.4548238117351997\n",
      "Epoch 230, Loss: 0.4537990916311975\n",
      "Epoch 231, Loss: 0.4532521496977959\n",
      "Epoch 232, Loss: 0.4526084553640119\n",
      "Epoch 233, Loss: 0.45228981460773116\n",
      "Epoch 234, Loss: 0.4520949132230242\n",
      "Epoch 235, Loss: 0.45205797975469736\n",
      "Epoch 236, Loss: 0.45230557901617613\n",
      "Epoch 237, Loss: 0.4528345953247846\n",
      "Epoch 238, Loss: 0.4549386133976144\n",
      "Epoch 239, Loss: 0.4564022887834084\n",
      "Epoch 240, Loss: 0.45805755540321125\n",
      "Epoch 241, Loss: 0.4541498302115434\n",
      "Epoch 242, Loss: 0.45158727109343993\n",
      "Epoch 243, Loss: 0.45105347173746146\n",
      "Epoch 244, Loss: 0.45263971453254503\n",
      "Epoch 245, Loss: 0.4550743757756552\n",
      "Epoch 246, Loss: 0.4549471294161167\n",
      "Epoch 247, Loss: 0.45361945699341477\n",
      "Epoch 248, Loss: 0.4506913801287277\n",
      "Epoch 249, Loss: 0.44996455105922556\n",
      "Epoch 250, Loss: 0.45070657227233163\n",
      "Epoch 251, Loss: 0.45134097806864715\n",
      "Epoch 252, Loss: 0.4518641160838718\n",
      "Epoch 253, Loss: 0.45124870735997746\n",
      "Epoch 254, Loss: 0.4507253179113287\n",
      "Epoch 255, Loss: 0.44965336548788515\n",
      "Epoch 256, Loss: 0.4490457284512466\n",
      "Epoch 257, Loss: 0.44867439459283065\n",
      "Epoch 258, Loss: 0.4485724236213288\n",
      "Epoch 259, Loss: 0.4487178165615502\n",
      "Epoch 260, Loss: 0.44913423821312026\n",
      "Epoch 261, Loss: 0.45068397397968624\n",
      "Epoch 262, Loss: 0.4523160224282965\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.24088891400979073\n",
      "Valence RMSE: 0.2525760943388832\n",
      "Arousal RMSE: 0.22860501821733556\n",
      "Test R^2 score: tensor([0.3183, 0.6146], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.46643727491028186\n",
      "Num of epochs: 263\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Epoch 121, Loss: 0.4816228295571127\n",
      "Epoch 122, Loss: 0.4807618495484584\n",
      "Epoch 123, Loss: 0.48102030846138955\n",
      "Epoch 124, Loss: 0.4810176288364116\n",
      "Epoch 125, Loss: 0.48014338261661127\n",
      "Epoch 126, Loss: 0.47910314809270116\n",
      "Epoch 127, Loss: 0.47880795949683447\n",
      "Epoch 128, Loss: 0.47876177315142754\n",
      "Epoch 129, Loss: 0.47819597116449997\n",
      "Epoch 130, Loss: 0.47743214539542544\n",
      "Epoch 131, Loss: 0.47658476230340185\n",
      "Epoch 132, Loss: 0.4759644680184978\n",
      "Epoch 133, Loss: 0.4755635187712421\n",
      "Epoch 134, Loss: 0.47538264196584307\n",
      "Epoch 135, Loss: 0.4753673450596914\n",
      "Epoch 136, Loss: 0.4753697744168057\n",
      "Epoch 137, Loss: 0.4755973109583155\n",
      "Epoch 138, Loss: 0.4767626826605241\n",
      "Epoch 139, Loss: 0.47683528222287225\n",
      "Epoch 140, Loss: 0.47579445468261566\n",
      "Epoch 141, Loss: 0.47284533919249566\n",
      "Epoch 142, Loss: 0.472809286012082\n",
      "Epoch 143, Loss: 0.47458738111232796\n",
      "Epoch 144, Loss: 0.4733380058918715\n",
      "Epoch 145, Loss: 0.4715229034484008\n",
      "Epoch 146, Loss: 0.47096730966267386\n",
      "Epoch 147, Loss: 0.47161469890467383\n",
      "Epoch 148, Loss: 0.47183531411129226\n",
      "Epoch 149, Loss: 0.470294933688076\n",
      "Epoch 150, Loss: 0.4697114144973765\n",
      "Epoch 151, Loss: 0.47018789053701304\n",
      "Epoch 152, Loss: 0.4700460953124117\n",
      "Epoch 153, Loss: 0.4693645740016508\n",
      "Epoch 154, Loss: 0.4684558104180481\n",
      "Epoch 155, Loss: 0.4680960384979934\n",
      "Epoch 156, Loss: 0.4682454254529793\n",
      "Epoch 157, Loss: 0.4682974379105488\n",
      "Epoch 158, Loss: 0.4681590010361841\n",
      "Epoch 159, Loss: 0.4673225124817593\n",
      "Epoch 160, Loss: 0.4666701428107949\n",
      "Epoch 161, Loss: 0.4663166784414499\n",
      "Epoch 162, Loss: 0.46624854535037\n",
      "Epoch 163, Loss: 0.4664611566834947\n",
      "Epoch 164, Loss: 0.46654532450127006\n",
      "Epoch 165, Loss: 0.46721845607201035\n",
      "Epoch 166, Loss: 0.4668208160006968\n",
      "Epoch 167, Loss: 0.466573749669782\n",
      "Epoch 168, Loss: 0.46506387643568103\n",
      "Epoch 169, Loss: 0.4641220516950567\n",
      "Epoch 170, Loss: 0.4644550172656552\n",
      "Epoch 171, Loss: 0.4654613047116713\n",
      "Epoch 172, Loss: 0.46880176576246846\n",
      "Epoch 173, Loss: 0.4685649824581709\n",
      "Epoch 174, Loss: 0.46606115996454733\n",
      "Epoch 175, Loss: 0.46328263242364126\n",
      "Epoch 176, Loss: 0.4660484187313888\n",
      "Epoch 177, Loss: 0.4667669789465077\n",
      "Epoch 178, Loss: 0.46222975993575777\n",
      "Epoch 179, Loss: 0.46412214801342555\n",
      "Epoch 180, Loss: 0.46867384291927394\n",
      "Epoch 181, Loss: 0.46316269230838986\n",
      "Epoch 182, Loss: 0.4619791093960296\n",
      "Epoch 183, Loss: 0.4650783107301587\n",
      "Epoch 184, Loss: 0.4617210775403838\n",
      "Epoch 185, Loss: 0.4621011464022392\n",
      "Epoch 186, Loss: 0.4641367239628427\n",
      "Epoch 187, Loss: 0.46071131054180564\n",
      "Epoch 188, Loss: 0.46145892957673257\n",
      "Epoch 189, Loss: 0.464304908222076\n",
      "Epoch 190, Loss: 0.4606605441278432\n",
      "Epoch 191, Loss: 0.4601386308130386\n",
      "Epoch 192, Loss: 0.46193430495315263\n",
      "Epoch 193, Loss: 0.45977696912799226\n",
      "Epoch 194, Loss: 0.4597173803227511\n",
      "Epoch 195, Loss: 0.46096589922299847\n",
      "Epoch 196, Loss: 0.4592170612502741\n",
      "Epoch 197, Loss: 0.45864347419013624\n",
      "Epoch 198, Loss: 0.4594650511157848\n",
      "Epoch 199, Loss: 0.45924705942486493\n",
      "Epoch 200, Loss: 0.4584931199827434\n",
      "Epoch 201, Loss: 0.4578552480015059\n",
      "Epoch 202, Loss: 0.458251953125\n",
      "Epoch 203, Loss: 0.4586848316367108\n",
      "Epoch 204, Loss: 0.4577025349799131\n",
      "Epoch 205, Loss: 0.45707273702530193\n",
      "Epoch 206, Loss: 0.4571336158814618\n",
      "Epoch 207, Loss: 0.45746758002181026\n",
      "Epoch 208, Loss: 0.4580334166179955\n",
      "Epoch 209, Loss: 0.457551138813026\n",
      "Epoch 210, Loss: 0.4572461918817771\n",
      "Epoch 211, Loss: 0.45629613394883295\n",
      "Epoch 212, Loss: 0.4557945440718634\n",
      "Epoch 213, Loss: 0.455843155547024\n",
      "Epoch 214, Loss: 0.456219547378948\n",
      "Epoch 215, Loss: 0.4577116995236092\n",
      "Epoch 216, Loss: 0.4589176697652806\n",
      "Epoch 217, Loss: 0.46105034303609993\n",
      "Epoch 218, Loss: 0.4567326442394303\n",
      "Epoch 219, Loss: 0.45493112899589366\n",
      "Epoch 220, Loss: 0.45527199475335134\n",
      "Epoch 221, Loss: 0.45623630281032773\n",
      "Epoch 222, Loss: 0.45802054967026307\n",
      "Epoch 223, Loss: 0.45677224995978094\n",
      "Epoch 224, Loss: 0.45565914342704716\n",
      "Epoch 225, Loss: 0.45401674524485375\n",
      "Epoch 226, Loss: 0.4535057342791329\n",
      "Epoch 227, Loss: 0.45416164205845616\n",
      "Epoch 228, Loss: 0.45450513400796655\n",
      "Epoch 229, Loss: 0.4548238117351997\n",
      "Epoch 230, Loss: 0.4537990916311975\n",
      "Epoch 231, Loss: 0.4532521496977959\n",
      "Epoch 232, Loss: 0.4526084553640119\n",
      "Epoch 233, Loss: 0.45228981460773116\n",
      "Epoch 234, Loss: 0.4520949132230242\n",
      "Epoch 235, Loss: 0.45205797975469736\n",
      "Epoch 236, Loss: 0.45230557901617613\n",
      "Epoch 237, Loss: 0.4528345953247846\n",
      "Epoch 238, Loss: 0.4549386133976144\n",
      "Epoch 239, Loss: 0.4564022887834084\n",
      "Epoch 240, Loss: 0.45805755540321125\n",
      "Epoch 241, Loss: 0.4541498302115434\n",
      "Epoch 242, Loss: 0.45158727109343993\n",
      "Epoch 243, Loss: 0.45105347173746146\n",
      "Epoch 244, Loss: 0.45263971453254503\n",
      "Epoch 245, Loss: 0.4550743757756552\n",
      "Epoch 246, Loss: 0.4549471294161167\n",
      "Epoch 247, Loss: 0.45361945699341477\n",
      "Epoch 248, Loss: 0.4506913801287277\n",
      "Epoch 249, Loss: 0.44996455105922556\n",
      "Epoch 250, Loss: 0.45070657227233163\n",
      "Epoch 251, Loss: 0.45134097806864715\n",
      "Epoch 252, Loss: 0.4518641160838718\n",
      "Epoch 253, Loss: 0.45124870735997746\n",
      "Epoch 254, Loss: 0.4507253179113287\n",
      "Epoch 255, Loss: 0.44965336548788515\n",
      "Epoch 256, Loss: 0.4490457284512466\n",
      "Epoch 257, Loss: 0.44867439459283065\n",
      "Epoch 258, Loss: 0.4485724236213288\n",
      "Epoch 259, Loss: 0.4487178165615502\n",
      "Epoch 260, Loss: 0.44913423821312026\n",
      "Epoch 261, Loss: 0.45068397397968624\n",
      "Epoch 262, Loss: 0.4523160224282965\n",
      "Epoch 263, Loss: 0.4554805695192658\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23632397531883986\n",
      "Valence RMSE: 0.24232963602468763\n",
      "Arousal RMSE: 0.23016166084980721\n",
      "Test R^2 score: tensor([0.3725, 0.6093], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4908983621930086\n",
      "Num of epochs: 264\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Epoch 121, Loss: 0.4816228295571127\n",
      "Epoch 122, Loss: 0.4807618495484584\n",
      "Epoch 123, Loss: 0.48102030846138955\n",
      "Epoch 124, Loss: 0.4810176288364116\n",
      "Epoch 125, Loss: 0.48014338261661127\n",
      "Epoch 126, Loss: 0.47910314809270116\n",
      "Epoch 127, Loss: 0.47880795949683447\n",
      "Epoch 128, Loss: 0.47876177315142754\n",
      "Epoch 129, Loss: 0.47819597116449997\n",
      "Epoch 130, Loss: 0.47743214539542544\n",
      "Epoch 131, Loss: 0.47658476230340185\n",
      "Epoch 132, Loss: 0.4759644680184978\n",
      "Epoch 133, Loss: 0.4755635187712421\n",
      "Epoch 134, Loss: 0.47538264196584307\n",
      "Epoch 135, Loss: 0.4753673450596914\n",
      "Epoch 136, Loss: 0.4753697744168057\n",
      "Epoch 137, Loss: 0.4755973109583155\n",
      "Epoch 138, Loss: 0.4767626826605241\n",
      "Epoch 139, Loss: 0.47683528222287225\n",
      "Epoch 140, Loss: 0.47579445468261566\n",
      "Epoch 141, Loss: 0.47284533919249566\n",
      "Epoch 142, Loss: 0.472809286012082\n",
      "Epoch 143, Loss: 0.47458738111232796\n",
      "Epoch 144, Loss: 0.4733380058918715\n",
      "Epoch 145, Loss: 0.4715229034484008\n",
      "Epoch 146, Loss: 0.47096730966267386\n",
      "Epoch 147, Loss: 0.47161469890467383\n",
      "Epoch 148, Loss: 0.47183531411129226\n",
      "Epoch 149, Loss: 0.470294933688076\n",
      "Epoch 150, Loss: 0.4697114144973765\n",
      "Epoch 151, Loss: 0.47018789053701304\n",
      "Epoch 152, Loss: 0.4700460953124117\n",
      "Epoch 153, Loss: 0.4693645740016508\n",
      "Epoch 154, Loss: 0.4684558104180481\n",
      "Epoch 155, Loss: 0.4680960384979934\n",
      "Epoch 156, Loss: 0.4682454254529793\n",
      "Epoch 157, Loss: 0.4682974379105488\n",
      "Epoch 158, Loss: 0.4681590010361841\n",
      "Epoch 159, Loss: 0.4673225124817593\n",
      "Epoch 160, Loss: 0.4666701428107949\n",
      "Epoch 161, Loss: 0.4663166784414499\n",
      "Epoch 162, Loss: 0.46624854535037\n",
      "Epoch 163, Loss: 0.4664611566834947\n",
      "Epoch 164, Loss: 0.46654532450127006\n",
      "Epoch 165, Loss: 0.46721845607201035\n",
      "Epoch 166, Loss: 0.4668208160006968\n",
      "Epoch 167, Loss: 0.466573749669782\n",
      "Epoch 168, Loss: 0.46506387643568103\n",
      "Epoch 169, Loss: 0.4641220516950567\n",
      "Epoch 170, Loss: 0.4644550172656552\n",
      "Epoch 171, Loss: 0.4654613047116713\n",
      "Epoch 172, Loss: 0.46880176576246846\n",
      "Epoch 173, Loss: 0.4685649824581709\n",
      "Epoch 174, Loss: 0.46606115996454733\n",
      "Epoch 175, Loss: 0.46328263242364126\n",
      "Epoch 176, Loss: 0.4660484187313888\n",
      "Epoch 177, Loss: 0.4667669789465077\n",
      "Epoch 178, Loss: 0.46222975993575777\n",
      "Epoch 179, Loss: 0.46412214801342555\n",
      "Epoch 180, Loss: 0.46867384291927394\n",
      "Epoch 181, Loss: 0.46316269230838986\n",
      "Epoch 182, Loss: 0.4619791093960296\n",
      "Epoch 183, Loss: 0.4650783107301587\n",
      "Epoch 184, Loss: 0.4617210775403838\n",
      "Epoch 185, Loss: 0.4621011464022392\n",
      "Epoch 186, Loss: 0.4641367239628427\n",
      "Epoch 187, Loss: 0.46071131054180564\n",
      "Epoch 188, Loss: 0.46145892957673257\n",
      "Epoch 189, Loss: 0.464304908222076\n",
      "Epoch 190, Loss: 0.4606605441278432\n",
      "Epoch 191, Loss: 0.4601386308130386\n",
      "Epoch 192, Loss: 0.46193430495315263\n",
      "Epoch 193, Loss: 0.45977696912799226\n",
      "Epoch 194, Loss: 0.4597173803227511\n",
      "Epoch 195, Loss: 0.46096589922299847\n",
      "Epoch 196, Loss: 0.4592170612502741\n",
      "Epoch 197, Loss: 0.45864347419013624\n",
      "Epoch 198, Loss: 0.4594650511157848\n",
      "Epoch 199, Loss: 0.45924705942486493\n",
      "Epoch 200, Loss: 0.4584931199827434\n",
      "Epoch 201, Loss: 0.4578552480015059\n",
      "Epoch 202, Loss: 0.458251953125\n",
      "Epoch 203, Loss: 0.4586848316367108\n",
      "Epoch 204, Loss: 0.4577025349799131\n",
      "Epoch 205, Loss: 0.45707273702530193\n",
      "Epoch 206, Loss: 0.4571336158814618\n",
      "Epoch 207, Loss: 0.45746758002181026\n",
      "Epoch 208, Loss: 0.4580334166179955\n",
      "Epoch 209, Loss: 0.457551138813026\n",
      "Epoch 210, Loss: 0.4572461918817771\n",
      "Epoch 211, Loss: 0.45629613394883295\n",
      "Epoch 212, Loss: 0.4557945440718634\n",
      "Epoch 213, Loss: 0.455843155547024\n",
      "Epoch 214, Loss: 0.456219547378948\n",
      "Epoch 215, Loss: 0.4577116995236092\n",
      "Epoch 216, Loss: 0.4589176697652806\n",
      "Epoch 217, Loss: 0.46105034303609993\n",
      "Epoch 218, Loss: 0.4567326442394303\n",
      "Epoch 219, Loss: 0.45493112899589366\n",
      "Epoch 220, Loss: 0.45527199475335134\n",
      "Epoch 221, Loss: 0.45623630281032773\n",
      "Epoch 222, Loss: 0.45802054967026307\n",
      "Epoch 223, Loss: 0.45677224995978094\n",
      "Epoch 224, Loss: 0.45565914342704716\n",
      "Epoch 225, Loss: 0.45401674524485375\n",
      "Epoch 226, Loss: 0.4535057342791329\n",
      "Epoch 227, Loss: 0.45416164205845616\n",
      "Epoch 228, Loss: 0.45450513400796655\n",
      "Epoch 229, Loss: 0.4548238117351997\n",
      "Epoch 230, Loss: 0.4537990916311975\n",
      "Epoch 231, Loss: 0.4532521496977959\n",
      "Epoch 232, Loss: 0.4526084553640119\n",
      "Epoch 233, Loss: 0.45228981460773116\n",
      "Epoch 234, Loss: 0.4520949132230242\n",
      "Epoch 235, Loss: 0.45205797975469736\n",
      "Epoch 236, Loss: 0.45230557901617613\n",
      "Epoch 237, Loss: 0.4528345953247846\n",
      "Epoch 238, Loss: 0.4549386133976144\n",
      "Epoch 239, Loss: 0.4564022887834084\n",
      "Epoch 240, Loss: 0.45805755540321125\n",
      "Epoch 241, Loss: 0.4541498302115434\n",
      "Epoch 242, Loss: 0.45158727109343993\n",
      "Epoch 243, Loss: 0.45105347173746146\n",
      "Epoch 244, Loss: 0.45263971453254503\n",
      "Epoch 245, Loss: 0.4550743757756552\n",
      "Epoch 246, Loss: 0.4549471294161167\n",
      "Epoch 247, Loss: 0.45361945699341477\n",
      "Epoch 248, Loss: 0.4506913801287277\n",
      "Epoch 249, Loss: 0.44996455105922556\n",
      "Epoch 250, Loss: 0.45070657227233163\n",
      "Epoch 251, Loss: 0.45134097806864715\n",
      "Epoch 252, Loss: 0.4518641160838718\n",
      "Epoch 253, Loss: 0.45124870735997746\n",
      "Epoch 254, Loss: 0.4507253179113287\n",
      "Epoch 255, Loss: 0.44965336548788515\n",
      "Epoch 256, Loss: 0.4490457284512466\n",
      "Epoch 257, Loss: 0.44867439459283065\n",
      "Epoch 258, Loss: 0.4485724236213288\n",
      "Epoch 259, Loss: 0.4487178165615502\n",
      "Epoch 260, Loss: 0.44913423821312026\n",
      "Epoch 261, Loss: 0.45068397397968624\n",
      "Epoch 262, Loss: 0.4523160224282965\n",
      "Epoch 263, Loss: 0.4554805695192658\n",
      "Epoch 264, Loss: 0.4521284819909996\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23550822305581948\n",
      "Valence RMSE: 0.24571410126889076\n",
      "Arousal RMSE: 0.22483955766599556\n",
      "Test R^2 score: tensor([0.3548, 0.6272], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.49100245891023925\n",
      "Num of epochs: 265\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Epoch 121, Loss: 0.4816228295571127\n",
      "Epoch 122, Loss: 0.4807618495484584\n",
      "Epoch 123, Loss: 0.48102030846138955\n",
      "Epoch 124, Loss: 0.4810176288364116\n",
      "Epoch 125, Loss: 0.48014338261661127\n",
      "Epoch 126, Loss: 0.47910314809270116\n",
      "Epoch 127, Loss: 0.47880795949683447\n",
      "Epoch 128, Loss: 0.47876177315142754\n",
      "Epoch 129, Loss: 0.47819597116449997\n",
      "Epoch 130, Loss: 0.47743214539542544\n",
      "Epoch 131, Loss: 0.47658476230340185\n",
      "Epoch 132, Loss: 0.4759644680184978\n",
      "Epoch 133, Loss: 0.4755635187712421\n",
      "Epoch 134, Loss: 0.47538264196584307\n",
      "Epoch 135, Loss: 0.4753673450596914\n",
      "Epoch 136, Loss: 0.4753697744168057\n",
      "Epoch 137, Loss: 0.4755973109583155\n",
      "Epoch 138, Loss: 0.4767626826605241\n",
      "Epoch 139, Loss: 0.47683528222287225\n",
      "Epoch 140, Loss: 0.47579445468261566\n",
      "Epoch 141, Loss: 0.47284533919249566\n",
      "Epoch 142, Loss: 0.472809286012082\n",
      "Epoch 143, Loss: 0.47458738111232796\n",
      "Epoch 144, Loss: 0.4733380058918715\n",
      "Epoch 145, Loss: 0.4715229034484008\n",
      "Epoch 146, Loss: 0.47096730966267386\n",
      "Epoch 147, Loss: 0.47161469890467383\n",
      "Epoch 148, Loss: 0.47183531411129226\n",
      "Epoch 149, Loss: 0.470294933688076\n",
      "Epoch 150, Loss: 0.4697114144973765\n",
      "Epoch 151, Loss: 0.47018789053701304\n",
      "Epoch 152, Loss: 0.4700460953124117\n",
      "Epoch 153, Loss: 0.4693645740016508\n",
      "Epoch 154, Loss: 0.4684558104180481\n",
      "Epoch 155, Loss: 0.4680960384979934\n",
      "Epoch 156, Loss: 0.4682454254529793\n",
      "Epoch 157, Loss: 0.4682974379105488\n",
      "Epoch 158, Loss: 0.4681590010361841\n",
      "Epoch 159, Loss: 0.4673225124817593\n",
      "Epoch 160, Loss: 0.4666701428107949\n",
      "Epoch 161, Loss: 0.4663166784414499\n",
      "Epoch 162, Loss: 0.46624854535037\n",
      "Epoch 163, Loss: 0.4664611566834947\n",
      "Epoch 164, Loss: 0.46654532450127006\n",
      "Epoch 165, Loss: 0.46721845607201035\n",
      "Epoch 166, Loss: 0.4668208160006968\n",
      "Epoch 167, Loss: 0.466573749669782\n",
      "Epoch 168, Loss: 0.46506387643568103\n",
      "Epoch 169, Loss: 0.4641220516950567\n",
      "Epoch 170, Loss: 0.4644550172656552\n",
      "Epoch 171, Loss: 0.4654613047116713\n",
      "Epoch 172, Loss: 0.46880176576246846\n",
      "Epoch 173, Loss: 0.4685649824581709\n",
      "Epoch 174, Loss: 0.46606115996454733\n",
      "Epoch 175, Loss: 0.46328263242364126\n",
      "Epoch 176, Loss: 0.4660484187313888\n",
      "Epoch 177, Loss: 0.4667669789465077\n",
      "Epoch 178, Loss: 0.46222975993575777\n",
      "Epoch 179, Loss: 0.46412214801342555\n",
      "Epoch 180, Loss: 0.46867384291927394\n",
      "Epoch 181, Loss: 0.46316269230838986\n",
      "Epoch 182, Loss: 0.4619791093960296\n",
      "Epoch 183, Loss: 0.4650783107301587\n",
      "Epoch 184, Loss: 0.4617210775403838\n",
      "Epoch 185, Loss: 0.4621011464022392\n",
      "Epoch 186, Loss: 0.4641367239628427\n",
      "Epoch 187, Loss: 0.46071131054180564\n",
      "Epoch 188, Loss: 0.46145892957673257\n",
      "Epoch 189, Loss: 0.464304908222076\n",
      "Epoch 190, Loss: 0.4606605441278432\n",
      "Epoch 191, Loss: 0.4601386308130386\n",
      "Epoch 192, Loss: 0.46193430495315263\n",
      "Epoch 193, Loss: 0.45977696912799226\n",
      "Epoch 194, Loss: 0.4597173803227511\n",
      "Epoch 195, Loss: 0.46096589922299847\n",
      "Epoch 196, Loss: 0.4592170612502741\n",
      "Epoch 197, Loss: 0.45864347419013624\n",
      "Epoch 198, Loss: 0.4594650511157848\n",
      "Epoch 199, Loss: 0.45924705942486493\n",
      "Epoch 200, Loss: 0.4584931199827434\n",
      "Epoch 201, Loss: 0.4578552480015059\n",
      "Epoch 202, Loss: 0.458251953125\n",
      "Epoch 203, Loss: 0.4586848316367108\n",
      "Epoch 204, Loss: 0.4577025349799131\n",
      "Epoch 205, Loss: 0.45707273702530193\n",
      "Epoch 206, Loss: 0.4571336158814618\n",
      "Epoch 207, Loss: 0.45746758002181026\n",
      "Epoch 208, Loss: 0.4580334166179955\n",
      "Epoch 209, Loss: 0.457551138813026\n",
      "Epoch 210, Loss: 0.4572461918817771\n",
      "Epoch 211, Loss: 0.45629613394883295\n",
      "Epoch 212, Loss: 0.4557945440718634\n",
      "Epoch 213, Loss: 0.455843155547024\n",
      "Epoch 214, Loss: 0.456219547378948\n",
      "Epoch 215, Loss: 0.4577116995236092\n",
      "Epoch 216, Loss: 0.4589176697652806\n",
      "Epoch 217, Loss: 0.46105034303609993\n",
      "Epoch 218, Loss: 0.4567326442394303\n",
      "Epoch 219, Loss: 0.45493112899589366\n",
      "Epoch 220, Loss: 0.45527199475335134\n",
      "Epoch 221, Loss: 0.45623630281032773\n",
      "Epoch 222, Loss: 0.45802054967026307\n",
      "Epoch 223, Loss: 0.45677224995978094\n",
      "Epoch 224, Loss: 0.45565914342704716\n",
      "Epoch 225, Loss: 0.45401674524485375\n",
      "Epoch 226, Loss: 0.4535057342791329\n",
      "Epoch 227, Loss: 0.45416164205845616\n",
      "Epoch 228, Loss: 0.45450513400796655\n",
      "Epoch 229, Loss: 0.4548238117351997\n",
      "Epoch 230, Loss: 0.4537990916311975\n",
      "Epoch 231, Loss: 0.4532521496977959\n",
      "Epoch 232, Loss: 0.4526084553640119\n",
      "Epoch 233, Loss: 0.45228981460773116\n",
      "Epoch 234, Loss: 0.4520949132230242\n",
      "Epoch 235, Loss: 0.45205797975469736\n",
      "Epoch 236, Loss: 0.45230557901617613\n",
      "Epoch 237, Loss: 0.4528345953247846\n",
      "Epoch 238, Loss: 0.4549386133976144\n",
      "Epoch 239, Loss: 0.4564022887834084\n",
      "Epoch 240, Loss: 0.45805755540321125\n",
      "Epoch 241, Loss: 0.4541498302115434\n",
      "Epoch 242, Loss: 0.45158727109343993\n",
      "Epoch 243, Loss: 0.45105347173746146\n",
      "Epoch 244, Loss: 0.45263971453254503\n",
      "Epoch 245, Loss: 0.4550743757756552\n",
      "Epoch 246, Loss: 0.4549471294161167\n",
      "Epoch 247, Loss: 0.45361945699341477\n",
      "Epoch 248, Loss: 0.4506913801287277\n",
      "Epoch 249, Loss: 0.44996455105922556\n",
      "Epoch 250, Loss: 0.45070657227233163\n",
      "Epoch 251, Loss: 0.45134097806864715\n",
      "Epoch 252, Loss: 0.4518641160838718\n",
      "Epoch 253, Loss: 0.45124870735997746\n",
      "Epoch 254, Loss: 0.4507253179113287\n",
      "Epoch 255, Loss: 0.44965336548788515\n",
      "Epoch 256, Loss: 0.4490457284512466\n",
      "Epoch 257, Loss: 0.44867439459283065\n",
      "Epoch 258, Loss: 0.4485724236213288\n",
      "Epoch 259, Loss: 0.4487178165615502\n",
      "Epoch 260, Loss: 0.44913423821312026\n",
      "Epoch 261, Loss: 0.45068397397968624\n",
      "Epoch 262, Loss: 0.4523160224282965\n",
      "Epoch 263, Loss: 0.4554805695192658\n",
      "Epoch 264, Loss: 0.4521284819909996\n",
      "Epoch 265, Loss: 0.4491460824324\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23431830825728248\n",
      "Valence RMSE: 0.2431877438052919\n",
      "Arousal RMSE: 0.22509966777408047\n",
      "Test R^2 score: tensor([0.3680, 0.6263], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.49717023976881025\n",
      "Num of epochs: 266\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Epoch 121, Loss: 0.4816228295571127\n",
      "Epoch 122, Loss: 0.4807618495484584\n",
      "Epoch 123, Loss: 0.48102030846138955\n",
      "Epoch 124, Loss: 0.4810176288364116\n",
      "Epoch 125, Loss: 0.48014338261661127\n",
      "Epoch 126, Loss: 0.47910314809270116\n",
      "Epoch 127, Loss: 0.47880795949683447\n",
      "Epoch 128, Loss: 0.47876177315142754\n",
      "Epoch 129, Loss: 0.47819597116449997\n",
      "Epoch 130, Loss: 0.47743214539542544\n",
      "Epoch 131, Loss: 0.47658476230340185\n",
      "Epoch 132, Loss: 0.4759644680184978\n",
      "Epoch 133, Loss: 0.4755635187712421\n",
      "Epoch 134, Loss: 0.47538264196584307\n",
      "Epoch 135, Loss: 0.4753673450596914\n",
      "Epoch 136, Loss: 0.4753697744168057\n",
      "Epoch 137, Loss: 0.4755973109583155\n",
      "Epoch 138, Loss: 0.4767626826605241\n",
      "Epoch 139, Loss: 0.47683528222287225\n",
      "Epoch 140, Loss: 0.47579445468261566\n",
      "Epoch 141, Loss: 0.47284533919249566\n",
      "Epoch 142, Loss: 0.472809286012082\n",
      "Epoch 143, Loss: 0.47458738111232796\n",
      "Epoch 144, Loss: 0.4733380058918715\n",
      "Epoch 145, Loss: 0.4715229034484008\n",
      "Epoch 146, Loss: 0.47096730966267386\n",
      "Epoch 147, Loss: 0.47161469890467383\n",
      "Epoch 148, Loss: 0.47183531411129226\n",
      "Epoch 149, Loss: 0.470294933688076\n",
      "Epoch 150, Loss: 0.4697114144973765\n",
      "Epoch 151, Loss: 0.47018789053701304\n",
      "Epoch 152, Loss: 0.4700460953124117\n",
      "Epoch 153, Loss: 0.4693645740016508\n",
      "Epoch 154, Loss: 0.4684558104180481\n",
      "Epoch 155, Loss: 0.4680960384979934\n",
      "Epoch 156, Loss: 0.4682454254529793\n",
      "Epoch 157, Loss: 0.4682974379105488\n",
      "Epoch 158, Loss: 0.4681590010361841\n",
      "Epoch 159, Loss: 0.4673225124817593\n",
      "Epoch 160, Loss: 0.4666701428107949\n",
      "Epoch 161, Loss: 0.4663166784414499\n",
      "Epoch 162, Loss: 0.46624854535037\n",
      "Epoch 163, Loss: 0.4664611566834947\n",
      "Epoch 164, Loss: 0.46654532450127006\n",
      "Epoch 165, Loss: 0.46721845607201035\n",
      "Epoch 166, Loss: 0.4668208160006968\n",
      "Epoch 167, Loss: 0.466573749669782\n",
      "Epoch 168, Loss: 0.46506387643568103\n",
      "Epoch 169, Loss: 0.4641220516950567\n",
      "Epoch 170, Loss: 0.4644550172656552\n",
      "Epoch 171, Loss: 0.4654613047116713\n",
      "Epoch 172, Loss: 0.46880176576246846\n",
      "Epoch 173, Loss: 0.4685649824581709\n",
      "Epoch 174, Loss: 0.46606115996454733\n",
      "Epoch 175, Loss: 0.46328263242364126\n",
      "Epoch 176, Loss: 0.4660484187313888\n",
      "Epoch 177, Loss: 0.4667669789465077\n",
      "Epoch 178, Loss: 0.46222975993575777\n",
      "Epoch 179, Loss: 0.46412214801342555\n",
      "Epoch 180, Loss: 0.46867384291927394\n",
      "Epoch 181, Loss: 0.46316269230838986\n",
      "Epoch 182, Loss: 0.4619791093960296\n",
      "Epoch 183, Loss: 0.4650783107301587\n",
      "Epoch 184, Loss: 0.4617210775403838\n",
      "Epoch 185, Loss: 0.4621011464022392\n",
      "Epoch 186, Loss: 0.4641367239628427\n",
      "Epoch 187, Loss: 0.46071131054180564\n",
      "Epoch 188, Loss: 0.46145892957673257\n",
      "Epoch 189, Loss: 0.464304908222076\n",
      "Epoch 190, Loss: 0.4606605441278432\n",
      "Epoch 191, Loss: 0.4601386308130386\n",
      "Epoch 192, Loss: 0.46193430495315263\n",
      "Epoch 193, Loss: 0.45977696912799226\n",
      "Epoch 194, Loss: 0.4597173803227511\n",
      "Epoch 195, Loss: 0.46096589922299847\n",
      "Epoch 196, Loss: 0.4592170612502741\n",
      "Epoch 197, Loss: 0.45864347419013624\n",
      "Epoch 198, Loss: 0.4594650511157848\n",
      "Epoch 199, Loss: 0.45924705942486493\n",
      "Epoch 200, Loss: 0.4584931199827434\n",
      "Epoch 201, Loss: 0.4578552480015059\n",
      "Epoch 202, Loss: 0.458251953125\n",
      "Epoch 203, Loss: 0.4586848316367108\n",
      "Epoch 204, Loss: 0.4577025349799131\n",
      "Epoch 205, Loss: 0.45707273702530193\n",
      "Epoch 206, Loss: 0.4571336158814618\n",
      "Epoch 207, Loss: 0.45746758002181026\n",
      "Epoch 208, Loss: 0.4580334166179955\n",
      "Epoch 209, Loss: 0.457551138813026\n",
      "Epoch 210, Loss: 0.4572461918817771\n",
      "Epoch 211, Loss: 0.45629613394883295\n",
      "Epoch 212, Loss: 0.4557945440718634\n",
      "Epoch 213, Loss: 0.455843155547024\n",
      "Epoch 214, Loss: 0.456219547378948\n",
      "Epoch 215, Loss: 0.4577116995236092\n",
      "Epoch 216, Loss: 0.4589176697652806\n",
      "Epoch 217, Loss: 0.46105034303609993\n",
      "Epoch 218, Loss: 0.4567326442394303\n",
      "Epoch 219, Loss: 0.45493112899589366\n",
      "Epoch 220, Loss: 0.45527199475335134\n",
      "Epoch 221, Loss: 0.45623630281032773\n",
      "Epoch 222, Loss: 0.45802054967026307\n",
      "Epoch 223, Loss: 0.45677224995978094\n",
      "Epoch 224, Loss: 0.45565914342704716\n",
      "Epoch 225, Loss: 0.45401674524485375\n",
      "Epoch 226, Loss: 0.4535057342791329\n",
      "Epoch 227, Loss: 0.45416164205845616\n",
      "Epoch 228, Loss: 0.45450513400796655\n",
      "Epoch 229, Loss: 0.4548238117351997\n",
      "Epoch 230, Loss: 0.4537990916311975\n",
      "Epoch 231, Loss: 0.4532521496977959\n",
      "Epoch 232, Loss: 0.4526084553640119\n",
      "Epoch 233, Loss: 0.45228981460773116\n",
      "Epoch 234, Loss: 0.4520949132230242\n",
      "Epoch 235, Loss: 0.45205797975469736\n",
      "Epoch 236, Loss: 0.45230557901617613\n",
      "Epoch 237, Loss: 0.4528345953247846\n",
      "Epoch 238, Loss: 0.4549386133976144\n",
      "Epoch 239, Loss: 0.4564022887834084\n",
      "Epoch 240, Loss: 0.45805755540321125\n",
      "Epoch 241, Loss: 0.4541498302115434\n",
      "Epoch 242, Loss: 0.45158727109343993\n",
      "Epoch 243, Loss: 0.45105347173746146\n",
      "Epoch 244, Loss: 0.45263971453254503\n",
      "Epoch 245, Loss: 0.4550743757756552\n",
      "Epoch 246, Loss: 0.4549471294161167\n",
      "Epoch 247, Loss: 0.45361945699341477\n",
      "Epoch 248, Loss: 0.4506913801287277\n",
      "Epoch 249, Loss: 0.44996455105922556\n",
      "Epoch 250, Loss: 0.45070657227233163\n",
      "Epoch 251, Loss: 0.45134097806864715\n",
      "Epoch 252, Loss: 0.4518641160838718\n",
      "Epoch 253, Loss: 0.45124870735997746\n",
      "Epoch 254, Loss: 0.4507253179113287\n",
      "Epoch 255, Loss: 0.44965336548788515\n",
      "Epoch 256, Loss: 0.4490457284512466\n",
      "Epoch 257, Loss: 0.44867439459283065\n",
      "Epoch 258, Loss: 0.4485724236213288\n",
      "Epoch 259, Loss: 0.4487178165615502\n",
      "Epoch 260, Loss: 0.44913423821312026\n",
      "Epoch 261, Loss: 0.45068397397968624\n",
      "Epoch 262, Loss: 0.4523160224282965\n",
      "Epoch 263, Loss: 0.4554805695192658\n",
      "Epoch 264, Loss: 0.4521284819909996\n",
      "Epoch 265, Loss: 0.4491460824324\n",
      "Epoch 266, Loss: 0.4475917876267889\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2351203990781496\n",
      "Valence RMSE: 0.2422528929005424\n",
      "Arousal RMSE: 0.22776465925743392\n",
      "Test R^2 score: tensor([0.3729, 0.6174], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4951446214065342\n",
      "Num of epochs: 267\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Epoch 121, Loss: 0.4816228295571127\n",
      "Epoch 122, Loss: 0.4807618495484584\n",
      "Epoch 123, Loss: 0.48102030846138955\n",
      "Epoch 124, Loss: 0.4810176288364116\n",
      "Epoch 125, Loss: 0.48014338261661127\n",
      "Epoch 126, Loss: 0.47910314809270116\n",
      "Epoch 127, Loss: 0.47880795949683447\n",
      "Epoch 128, Loss: 0.47876177315142754\n",
      "Epoch 129, Loss: 0.47819597116449997\n",
      "Epoch 130, Loss: 0.47743214539542544\n",
      "Epoch 131, Loss: 0.47658476230340185\n",
      "Epoch 132, Loss: 0.4759644680184978\n",
      "Epoch 133, Loss: 0.4755635187712421\n",
      "Epoch 134, Loss: 0.47538264196584307\n",
      "Epoch 135, Loss: 0.4753673450596914\n",
      "Epoch 136, Loss: 0.4753697744168057\n",
      "Epoch 137, Loss: 0.4755973109583155\n",
      "Epoch 138, Loss: 0.4767626826605241\n",
      "Epoch 139, Loss: 0.47683528222287225\n",
      "Epoch 140, Loss: 0.47579445468261566\n",
      "Epoch 141, Loss: 0.47284533919249566\n",
      "Epoch 142, Loss: 0.472809286012082\n",
      "Epoch 143, Loss: 0.47458738111232796\n",
      "Epoch 144, Loss: 0.4733380058918715\n",
      "Epoch 145, Loss: 0.4715229034484008\n",
      "Epoch 146, Loss: 0.47096730966267386\n",
      "Epoch 147, Loss: 0.47161469890467383\n",
      "Epoch 148, Loss: 0.47183531411129226\n",
      "Epoch 149, Loss: 0.470294933688076\n",
      "Epoch 150, Loss: 0.4697114144973765\n",
      "Epoch 151, Loss: 0.47018789053701304\n",
      "Epoch 152, Loss: 0.4700460953124117\n",
      "Epoch 153, Loss: 0.4693645740016508\n",
      "Epoch 154, Loss: 0.4684558104180481\n",
      "Epoch 155, Loss: 0.4680960384979934\n",
      "Epoch 156, Loss: 0.4682454254529793\n",
      "Epoch 157, Loss: 0.4682974379105488\n",
      "Epoch 158, Loss: 0.4681590010361841\n",
      "Epoch 159, Loss: 0.4673225124817593\n",
      "Epoch 160, Loss: 0.4666701428107949\n",
      "Epoch 161, Loss: 0.4663166784414499\n",
      "Epoch 162, Loss: 0.46624854535037\n",
      "Epoch 163, Loss: 0.4664611566834947\n",
      "Epoch 164, Loss: 0.46654532450127006\n",
      "Epoch 165, Loss: 0.46721845607201035\n",
      "Epoch 166, Loss: 0.4668208160006968\n",
      "Epoch 167, Loss: 0.466573749669782\n",
      "Epoch 168, Loss: 0.46506387643568103\n",
      "Epoch 169, Loss: 0.4641220516950567\n",
      "Epoch 170, Loss: 0.4644550172656552\n",
      "Epoch 171, Loss: 0.4654613047116713\n",
      "Epoch 172, Loss: 0.46880176576246846\n",
      "Epoch 173, Loss: 0.4685649824581709\n",
      "Epoch 174, Loss: 0.46606115996454733\n",
      "Epoch 175, Loss: 0.46328263242364126\n",
      "Epoch 176, Loss: 0.4660484187313888\n",
      "Epoch 177, Loss: 0.4667669789465077\n",
      "Epoch 178, Loss: 0.46222975993575777\n",
      "Epoch 179, Loss: 0.46412214801342555\n",
      "Epoch 180, Loss: 0.46867384291927394\n",
      "Epoch 181, Loss: 0.46316269230838986\n",
      "Epoch 182, Loss: 0.4619791093960296\n",
      "Epoch 183, Loss: 0.4650783107301587\n",
      "Epoch 184, Loss: 0.4617210775403838\n",
      "Epoch 185, Loss: 0.4621011464022392\n",
      "Epoch 186, Loss: 0.4641367239628427\n",
      "Epoch 187, Loss: 0.46071131054180564\n",
      "Epoch 188, Loss: 0.46145892957673257\n",
      "Epoch 189, Loss: 0.464304908222076\n",
      "Epoch 190, Loss: 0.4606605441278432\n",
      "Epoch 191, Loss: 0.4601386308130386\n",
      "Epoch 192, Loss: 0.46193430495315263\n",
      "Epoch 193, Loss: 0.45977696912799226\n",
      "Epoch 194, Loss: 0.4597173803227511\n",
      "Epoch 195, Loss: 0.46096589922299847\n",
      "Epoch 196, Loss: 0.4592170612502741\n",
      "Epoch 197, Loss: 0.45864347419013624\n",
      "Epoch 198, Loss: 0.4594650511157848\n",
      "Epoch 199, Loss: 0.45924705942486493\n",
      "Epoch 200, Loss: 0.4584931199827434\n",
      "Epoch 201, Loss: 0.4578552480015059\n",
      "Epoch 202, Loss: 0.458251953125\n",
      "Epoch 203, Loss: 0.4586848316367108\n",
      "Epoch 204, Loss: 0.4577025349799131\n",
      "Epoch 205, Loss: 0.45707273702530193\n",
      "Epoch 206, Loss: 0.4571336158814618\n",
      "Epoch 207, Loss: 0.45746758002181026\n",
      "Epoch 208, Loss: 0.4580334166179955\n",
      "Epoch 209, Loss: 0.457551138813026\n",
      "Epoch 210, Loss: 0.4572461918817771\n",
      "Epoch 211, Loss: 0.45629613394883295\n",
      "Epoch 212, Loss: 0.4557945440718634\n",
      "Epoch 213, Loss: 0.455843155547024\n",
      "Epoch 214, Loss: 0.456219547378948\n",
      "Epoch 215, Loss: 0.4577116995236092\n",
      "Epoch 216, Loss: 0.4589176697652806\n",
      "Epoch 217, Loss: 0.46105034303609993\n",
      "Epoch 218, Loss: 0.4567326442394303\n",
      "Epoch 219, Loss: 0.45493112899589366\n",
      "Epoch 220, Loss: 0.45527199475335134\n",
      "Epoch 221, Loss: 0.45623630281032773\n",
      "Epoch 222, Loss: 0.45802054967026307\n",
      "Epoch 223, Loss: 0.45677224995978094\n",
      "Epoch 224, Loss: 0.45565914342704716\n",
      "Epoch 225, Loss: 0.45401674524485375\n",
      "Epoch 226, Loss: 0.4535057342791329\n",
      "Epoch 227, Loss: 0.45416164205845616\n",
      "Epoch 228, Loss: 0.45450513400796655\n",
      "Epoch 229, Loss: 0.4548238117351997\n",
      "Epoch 230, Loss: 0.4537990916311975\n",
      "Epoch 231, Loss: 0.4532521496977959\n",
      "Epoch 232, Loss: 0.4526084553640119\n",
      "Epoch 233, Loss: 0.45228981460773116\n",
      "Epoch 234, Loss: 0.4520949132230242\n",
      "Epoch 235, Loss: 0.45205797975469736\n",
      "Epoch 236, Loss: 0.45230557901617613\n",
      "Epoch 237, Loss: 0.4528345953247846\n",
      "Epoch 238, Loss: 0.4549386133976144\n",
      "Epoch 239, Loss: 0.4564022887834084\n",
      "Epoch 240, Loss: 0.45805755540321125\n",
      "Epoch 241, Loss: 0.4541498302115434\n",
      "Epoch 242, Loss: 0.45158727109343993\n",
      "Epoch 243, Loss: 0.45105347173746146\n",
      "Epoch 244, Loss: 0.45263971453254503\n",
      "Epoch 245, Loss: 0.4550743757756552\n",
      "Epoch 246, Loss: 0.4549471294161167\n",
      "Epoch 247, Loss: 0.45361945699341477\n",
      "Epoch 248, Loss: 0.4506913801287277\n",
      "Epoch 249, Loss: 0.44996455105922556\n",
      "Epoch 250, Loss: 0.45070657227233163\n",
      "Epoch 251, Loss: 0.45134097806864715\n",
      "Epoch 252, Loss: 0.4518641160838718\n",
      "Epoch 253, Loss: 0.45124870735997746\n",
      "Epoch 254, Loss: 0.4507253179113287\n",
      "Epoch 255, Loss: 0.44965336548788515\n",
      "Epoch 256, Loss: 0.4490457284512466\n",
      "Epoch 257, Loss: 0.44867439459283065\n",
      "Epoch 258, Loss: 0.4485724236213288\n",
      "Epoch 259, Loss: 0.4487178165615502\n",
      "Epoch 260, Loss: 0.44913423821312026\n",
      "Epoch 261, Loss: 0.45068397397968624\n",
      "Epoch 262, Loss: 0.4523160224282965\n",
      "Epoch 263, Loss: 0.4554805695192658\n",
      "Epoch 264, Loss: 0.4521284819909996\n",
      "Epoch 265, Loss: 0.4491460824324\n",
      "Epoch 266, Loss: 0.4475917876267889\n",
      "Epoch 267, Loss: 0.4489836201844808\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23916273970480775\n",
      "Valence RMSE: 0.24982627987096548\n",
      "Arousal RMSE: 0.2280010131820762\n",
      "Test R^2 score: tensor([0.3331, 0.6166], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.47483566006954286\n",
      "Num of epochs: 268\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Epoch 121, Loss: 0.4816228295571127\n",
      "Epoch 122, Loss: 0.4807618495484584\n",
      "Epoch 123, Loss: 0.48102030846138955\n",
      "Epoch 124, Loss: 0.4810176288364116\n",
      "Epoch 125, Loss: 0.48014338261661127\n",
      "Epoch 126, Loss: 0.47910314809270116\n",
      "Epoch 127, Loss: 0.47880795949683447\n",
      "Epoch 128, Loss: 0.47876177315142754\n",
      "Epoch 129, Loss: 0.47819597116449997\n",
      "Epoch 130, Loss: 0.47743214539542544\n",
      "Epoch 131, Loss: 0.47658476230340185\n",
      "Epoch 132, Loss: 0.4759644680184978\n",
      "Epoch 133, Loss: 0.4755635187712421\n",
      "Epoch 134, Loss: 0.47538264196584307\n",
      "Epoch 135, Loss: 0.4753673450596914\n",
      "Epoch 136, Loss: 0.4753697744168057\n",
      "Epoch 137, Loss: 0.4755973109583155\n",
      "Epoch 138, Loss: 0.4767626826605241\n",
      "Epoch 139, Loss: 0.47683528222287225\n",
      "Epoch 140, Loss: 0.47579445468261566\n",
      "Epoch 141, Loss: 0.47284533919249566\n",
      "Epoch 142, Loss: 0.472809286012082\n",
      "Epoch 143, Loss: 0.47458738111232796\n",
      "Epoch 144, Loss: 0.4733380058918715\n",
      "Epoch 145, Loss: 0.4715229034484008\n",
      "Epoch 146, Loss: 0.47096730966267386\n",
      "Epoch 147, Loss: 0.47161469890467383\n",
      "Epoch 148, Loss: 0.47183531411129226\n",
      "Epoch 149, Loss: 0.470294933688076\n",
      "Epoch 150, Loss: 0.4697114144973765\n",
      "Epoch 151, Loss: 0.47018789053701304\n",
      "Epoch 152, Loss: 0.4700460953124117\n",
      "Epoch 153, Loss: 0.4693645740016508\n",
      "Epoch 154, Loss: 0.4684558104180481\n",
      "Epoch 155, Loss: 0.4680960384979934\n",
      "Epoch 156, Loss: 0.4682454254529793\n",
      "Epoch 157, Loss: 0.4682974379105488\n",
      "Epoch 158, Loss: 0.4681590010361841\n",
      "Epoch 159, Loss: 0.4673225124817593\n",
      "Epoch 160, Loss: 0.4666701428107949\n",
      "Epoch 161, Loss: 0.4663166784414499\n",
      "Epoch 162, Loss: 0.46624854535037\n",
      "Epoch 163, Loss: 0.4664611566834947\n",
      "Epoch 164, Loss: 0.46654532450127006\n",
      "Epoch 165, Loss: 0.46721845607201035\n",
      "Epoch 166, Loss: 0.4668208160006968\n",
      "Epoch 167, Loss: 0.466573749669782\n",
      "Epoch 168, Loss: 0.46506387643568103\n",
      "Epoch 169, Loss: 0.4641220516950567\n",
      "Epoch 170, Loss: 0.4644550172656552\n",
      "Epoch 171, Loss: 0.4654613047116713\n",
      "Epoch 172, Loss: 0.46880176576246846\n",
      "Epoch 173, Loss: 0.4685649824581709\n",
      "Epoch 174, Loss: 0.46606115996454733\n",
      "Epoch 175, Loss: 0.46328263242364126\n",
      "Epoch 176, Loss: 0.4660484187313888\n",
      "Epoch 177, Loss: 0.4667669789465077\n",
      "Epoch 178, Loss: 0.46222975993575777\n",
      "Epoch 179, Loss: 0.46412214801342555\n",
      "Epoch 180, Loss: 0.46867384291927394\n",
      "Epoch 181, Loss: 0.46316269230838986\n",
      "Epoch 182, Loss: 0.4619791093960296\n",
      "Epoch 183, Loss: 0.4650783107301587\n",
      "Epoch 184, Loss: 0.4617210775403838\n",
      "Epoch 185, Loss: 0.4621011464022392\n",
      "Epoch 186, Loss: 0.4641367239628427\n",
      "Epoch 187, Loss: 0.46071131054180564\n",
      "Epoch 188, Loss: 0.46145892957673257\n",
      "Epoch 189, Loss: 0.464304908222076\n",
      "Epoch 190, Loss: 0.4606605441278432\n",
      "Epoch 191, Loss: 0.4601386308130386\n",
      "Epoch 192, Loss: 0.46193430495315263\n",
      "Epoch 193, Loss: 0.45977696912799226\n",
      "Epoch 194, Loss: 0.4597173803227511\n",
      "Epoch 195, Loss: 0.46096589922299847\n",
      "Epoch 196, Loss: 0.4592170612502741\n",
      "Epoch 197, Loss: 0.45864347419013624\n",
      "Epoch 198, Loss: 0.4594650511157848\n",
      "Epoch 199, Loss: 0.45924705942486493\n",
      "Epoch 200, Loss: 0.4584931199827434\n",
      "Epoch 201, Loss: 0.4578552480015059\n",
      "Epoch 202, Loss: 0.458251953125\n",
      "Epoch 203, Loss: 0.4586848316367108\n",
      "Epoch 204, Loss: 0.4577025349799131\n",
      "Epoch 205, Loss: 0.45707273702530193\n",
      "Epoch 206, Loss: 0.4571336158814618\n",
      "Epoch 207, Loss: 0.45746758002181026\n",
      "Epoch 208, Loss: 0.4580334166179955\n",
      "Epoch 209, Loss: 0.457551138813026\n",
      "Epoch 210, Loss: 0.4572461918817771\n",
      "Epoch 211, Loss: 0.45629613394883295\n",
      "Epoch 212, Loss: 0.4557945440718634\n",
      "Epoch 213, Loss: 0.455843155547024\n",
      "Epoch 214, Loss: 0.456219547378948\n",
      "Epoch 215, Loss: 0.4577116995236092\n",
      "Epoch 216, Loss: 0.4589176697652806\n",
      "Epoch 217, Loss: 0.46105034303609993\n",
      "Epoch 218, Loss: 0.4567326442394303\n",
      "Epoch 219, Loss: 0.45493112899589366\n",
      "Epoch 220, Loss: 0.45527199475335134\n",
      "Epoch 221, Loss: 0.45623630281032773\n",
      "Epoch 222, Loss: 0.45802054967026307\n",
      "Epoch 223, Loss: 0.45677224995978094\n",
      "Epoch 224, Loss: 0.45565914342704716\n",
      "Epoch 225, Loss: 0.45401674524485375\n",
      "Epoch 226, Loss: 0.4535057342791329\n",
      "Epoch 227, Loss: 0.45416164205845616\n",
      "Epoch 228, Loss: 0.45450513400796655\n",
      "Epoch 229, Loss: 0.4548238117351997\n",
      "Epoch 230, Loss: 0.4537990916311975\n",
      "Epoch 231, Loss: 0.4532521496977959\n",
      "Epoch 232, Loss: 0.4526084553640119\n",
      "Epoch 233, Loss: 0.45228981460773116\n",
      "Epoch 234, Loss: 0.4520949132230242\n",
      "Epoch 235, Loss: 0.45205797975469736\n",
      "Epoch 236, Loss: 0.45230557901617613\n",
      "Epoch 237, Loss: 0.4528345953247846\n",
      "Epoch 238, Loss: 0.4549386133976144\n",
      "Epoch 239, Loss: 0.4564022887834084\n",
      "Epoch 240, Loss: 0.45805755540321125\n",
      "Epoch 241, Loss: 0.4541498302115434\n",
      "Epoch 242, Loss: 0.45158727109343993\n",
      "Epoch 243, Loss: 0.45105347173746146\n",
      "Epoch 244, Loss: 0.45263971453254503\n",
      "Epoch 245, Loss: 0.4550743757756552\n",
      "Epoch 246, Loss: 0.4549471294161167\n",
      "Epoch 247, Loss: 0.45361945699341477\n",
      "Epoch 248, Loss: 0.4506913801287277\n",
      "Epoch 249, Loss: 0.44996455105922556\n",
      "Epoch 250, Loss: 0.45070657227233163\n",
      "Epoch 251, Loss: 0.45134097806864715\n",
      "Epoch 252, Loss: 0.4518641160838718\n",
      "Epoch 253, Loss: 0.45124870735997746\n",
      "Epoch 254, Loss: 0.4507253179113287\n",
      "Epoch 255, Loss: 0.44965336548788515\n",
      "Epoch 256, Loss: 0.4490457284512466\n",
      "Epoch 257, Loss: 0.44867439459283065\n",
      "Epoch 258, Loss: 0.4485724236213288\n",
      "Epoch 259, Loss: 0.4487178165615502\n",
      "Epoch 260, Loss: 0.44913423821312026\n",
      "Epoch 261, Loss: 0.45068397397968624\n",
      "Epoch 262, Loss: 0.4523160224282965\n",
      "Epoch 263, Loss: 0.4554805695192658\n",
      "Epoch 264, Loss: 0.4521284819909996\n",
      "Epoch 265, Loss: 0.4491460824324\n",
      "Epoch 266, Loss: 0.4475917876267889\n",
      "Epoch 267, Loss: 0.4489836201844808\n",
      "Epoch 268, Loss: 0.4522696841254308\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23766147985724076\n",
      "Valence RMSE: 0.24352386208813165\n",
      "Arousal RMSE: 0.231650785903152\n",
      "Test R^2 score: tensor([0.3663, 0.6042], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.48526239393172954\n",
      "Num of epochs: 269\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Epoch 121, Loss: 0.4816228295571127\n",
      "Epoch 122, Loss: 0.4807618495484584\n",
      "Epoch 123, Loss: 0.48102030846138955\n",
      "Epoch 124, Loss: 0.4810176288364116\n",
      "Epoch 125, Loss: 0.48014338261661127\n",
      "Epoch 126, Loss: 0.47910314809270116\n",
      "Epoch 127, Loss: 0.47880795949683447\n",
      "Epoch 128, Loss: 0.47876177315142754\n",
      "Epoch 129, Loss: 0.47819597116449997\n",
      "Epoch 130, Loss: 0.47743214539542544\n",
      "Epoch 131, Loss: 0.47658476230340185\n",
      "Epoch 132, Loss: 0.4759644680184978\n",
      "Epoch 133, Loss: 0.4755635187712421\n",
      "Epoch 134, Loss: 0.47538264196584307\n",
      "Epoch 135, Loss: 0.4753673450596914\n",
      "Epoch 136, Loss: 0.4753697744168057\n",
      "Epoch 137, Loss: 0.4755973109583155\n",
      "Epoch 138, Loss: 0.4767626826605241\n",
      "Epoch 139, Loss: 0.47683528222287225\n",
      "Epoch 140, Loss: 0.47579445468261566\n",
      "Epoch 141, Loss: 0.47284533919249566\n",
      "Epoch 142, Loss: 0.472809286012082\n",
      "Epoch 143, Loss: 0.47458738111232796\n",
      "Epoch 144, Loss: 0.4733380058918715\n",
      "Epoch 145, Loss: 0.4715229034484008\n",
      "Epoch 146, Loss: 0.47096730966267386\n",
      "Epoch 147, Loss: 0.47161469890467383\n",
      "Epoch 148, Loss: 0.47183531411129226\n",
      "Epoch 149, Loss: 0.470294933688076\n",
      "Epoch 150, Loss: 0.4697114144973765\n",
      "Epoch 151, Loss: 0.47018789053701304\n",
      "Epoch 152, Loss: 0.4700460953124117\n",
      "Epoch 153, Loss: 0.4693645740016508\n",
      "Epoch 154, Loss: 0.4684558104180481\n",
      "Epoch 155, Loss: 0.4680960384979934\n",
      "Epoch 156, Loss: 0.4682454254529793\n",
      "Epoch 157, Loss: 0.4682974379105488\n",
      "Epoch 158, Loss: 0.4681590010361841\n",
      "Epoch 159, Loss: 0.4673225124817593\n",
      "Epoch 160, Loss: 0.4666701428107949\n",
      "Epoch 161, Loss: 0.4663166784414499\n",
      "Epoch 162, Loss: 0.46624854535037\n",
      "Epoch 163, Loss: 0.4664611566834947\n",
      "Epoch 164, Loss: 0.46654532450127006\n",
      "Epoch 165, Loss: 0.46721845607201035\n",
      "Epoch 166, Loss: 0.4668208160006968\n",
      "Epoch 167, Loss: 0.466573749669782\n",
      "Epoch 168, Loss: 0.46506387643568103\n",
      "Epoch 169, Loss: 0.4641220516950567\n",
      "Epoch 170, Loss: 0.4644550172656552\n",
      "Epoch 171, Loss: 0.4654613047116713\n",
      "Epoch 172, Loss: 0.46880176576246846\n",
      "Epoch 173, Loss: 0.4685649824581709\n",
      "Epoch 174, Loss: 0.46606115996454733\n",
      "Epoch 175, Loss: 0.46328263242364126\n",
      "Epoch 176, Loss: 0.4660484187313888\n",
      "Epoch 177, Loss: 0.4667669789465077\n",
      "Epoch 178, Loss: 0.46222975993575777\n",
      "Epoch 179, Loss: 0.46412214801342555\n",
      "Epoch 180, Loss: 0.46867384291927394\n",
      "Epoch 181, Loss: 0.46316269230838986\n",
      "Epoch 182, Loss: 0.4619791093960296\n",
      "Epoch 183, Loss: 0.4650783107301587\n",
      "Epoch 184, Loss: 0.4617210775403838\n",
      "Epoch 185, Loss: 0.4621011464022392\n",
      "Epoch 186, Loss: 0.4641367239628427\n",
      "Epoch 187, Loss: 0.46071131054180564\n",
      "Epoch 188, Loss: 0.46145892957673257\n",
      "Epoch 189, Loss: 0.464304908222076\n",
      "Epoch 190, Loss: 0.4606605441278432\n",
      "Epoch 191, Loss: 0.4601386308130386\n",
      "Epoch 192, Loss: 0.46193430495315263\n",
      "Epoch 193, Loss: 0.45977696912799226\n",
      "Epoch 194, Loss: 0.4597173803227511\n",
      "Epoch 195, Loss: 0.46096589922299847\n",
      "Epoch 196, Loss: 0.4592170612502741\n",
      "Epoch 197, Loss: 0.45864347419013624\n",
      "Epoch 198, Loss: 0.4594650511157848\n",
      "Epoch 199, Loss: 0.45924705942486493\n",
      "Epoch 200, Loss: 0.4584931199827434\n",
      "Epoch 201, Loss: 0.4578552480015059\n",
      "Epoch 202, Loss: 0.458251953125\n",
      "Epoch 203, Loss: 0.4586848316367108\n",
      "Epoch 204, Loss: 0.4577025349799131\n",
      "Epoch 205, Loss: 0.45707273702530193\n",
      "Epoch 206, Loss: 0.4571336158814618\n",
      "Epoch 207, Loss: 0.45746758002181026\n",
      "Epoch 208, Loss: 0.4580334166179955\n",
      "Epoch 209, Loss: 0.457551138813026\n",
      "Epoch 210, Loss: 0.4572461918817771\n",
      "Epoch 211, Loss: 0.45629613394883295\n",
      "Epoch 212, Loss: 0.4557945440718634\n",
      "Epoch 213, Loss: 0.455843155547024\n",
      "Epoch 214, Loss: 0.456219547378948\n",
      "Epoch 215, Loss: 0.4577116995236092\n",
      "Epoch 216, Loss: 0.4589176697652806\n",
      "Epoch 217, Loss: 0.46105034303609993\n",
      "Epoch 218, Loss: 0.4567326442394303\n",
      "Epoch 219, Loss: 0.45493112899589366\n",
      "Epoch 220, Loss: 0.45527199475335134\n",
      "Epoch 221, Loss: 0.45623630281032773\n",
      "Epoch 222, Loss: 0.45802054967026307\n",
      "Epoch 223, Loss: 0.45677224995978094\n",
      "Epoch 224, Loss: 0.45565914342704716\n",
      "Epoch 225, Loss: 0.45401674524485375\n",
      "Epoch 226, Loss: 0.4535057342791329\n",
      "Epoch 227, Loss: 0.45416164205845616\n",
      "Epoch 228, Loss: 0.45450513400796655\n",
      "Epoch 229, Loss: 0.4548238117351997\n",
      "Epoch 230, Loss: 0.4537990916311975\n",
      "Epoch 231, Loss: 0.4532521496977959\n",
      "Epoch 232, Loss: 0.4526084553640119\n",
      "Epoch 233, Loss: 0.45228981460773116\n",
      "Epoch 234, Loss: 0.4520949132230242\n",
      "Epoch 235, Loss: 0.45205797975469736\n",
      "Epoch 236, Loss: 0.45230557901617613\n",
      "Epoch 237, Loss: 0.4528345953247846\n",
      "Epoch 238, Loss: 0.4549386133976144\n",
      "Epoch 239, Loss: 0.4564022887834084\n",
      "Epoch 240, Loss: 0.45805755540321125\n",
      "Epoch 241, Loss: 0.4541498302115434\n",
      "Epoch 242, Loss: 0.45158727109343993\n",
      "Epoch 243, Loss: 0.45105347173746146\n",
      "Epoch 244, Loss: 0.45263971453254503\n",
      "Epoch 245, Loss: 0.4550743757756552\n",
      "Epoch 246, Loss: 0.4549471294161167\n",
      "Epoch 247, Loss: 0.45361945699341477\n",
      "Epoch 248, Loss: 0.4506913801287277\n",
      "Epoch 249, Loss: 0.44996455105922556\n",
      "Epoch 250, Loss: 0.45070657227233163\n",
      "Epoch 251, Loss: 0.45134097806864715\n",
      "Epoch 252, Loss: 0.4518641160838718\n",
      "Epoch 253, Loss: 0.45124870735997746\n",
      "Epoch 254, Loss: 0.4507253179113287\n",
      "Epoch 255, Loss: 0.44965336548788515\n",
      "Epoch 256, Loss: 0.4490457284512466\n",
      "Epoch 257, Loss: 0.44867439459283065\n",
      "Epoch 258, Loss: 0.4485724236213288\n",
      "Epoch 259, Loss: 0.4487178165615502\n",
      "Epoch 260, Loss: 0.44913423821312026\n",
      "Epoch 261, Loss: 0.45068397397968624\n",
      "Epoch 262, Loss: 0.4523160224282965\n",
      "Epoch 263, Loss: 0.4554805695192658\n",
      "Epoch 264, Loss: 0.4521284819909996\n",
      "Epoch 265, Loss: 0.4491460824324\n",
      "Epoch 266, Loss: 0.4475917876267889\n",
      "Epoch 267, Loss: 0.4489836201844808\n",
      "Epoch 268, Loss: 0.4522696841254308\n",
      "Epoch 269, Loss: 0.45230492011773493\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23810760580991866\n",
      "Valence RMSE: 0.2486246648894959\n",
      "Arousal RMSE: 0.22710402880100805\n",
      "Test R^2 score: tensor([0.3395, 0.6196], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4795411315781748\n",
      "Num of epochs: 270\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Epoch 121, Loss: 0.4816228295571127\n",
      "Epoch 122, Loss: 0.4807618495484584\n",
      "Epoch 123, Loss: 0.48102030846138955\n",
      "Epoch 124, Loss: 0.4810176288364116\n",
      "Epoch 125, Loss: 0.48014338261661127\n",
      "Epoch 126, Loss: 0.47910314809270116\n",
      "Epoch 127, Loss: 0.47880795949683447\n",
      "Epoch 128, Loss: 0.47876177315142754\n",
      "Epoch 129, Loss: 0.47819597116449997\n",
      "Epoch 130, Loss: 0.47743214539542544\n",
      "Epoch 131, Loss: 0.47658476230340185\n",
      "Epoch 132, Loss: 0.4759644680184978\n",
      "Epoch 133, Loss: 0.4755635187712421\n",
      "Epoch 134, Loss: 0.47538264196584307\n",
      "Epoch 135, Loss: 0.4753673450596914\n",
      "Epoch 136, Loss: 0.4753697744168057\n",
      "Epoch 137, Loss: 0.4755973109583155\n",
      "Epoch 138, Loss: 0.4767626826605241\n",
      "Epoch 139, Loss: 0.47683528222287225\n",
      "Epoch 140, Loss: 0.47579445468261566\n",
      "Epoch 141, Loss: 0.47284533919249566\n",
      "Epoch 142, Loss: 0.472809286012082\n",
      "Epoch 143, Loss: 0.47458738111232796\n",
      "Epoch 144, Loss: 0.4733380058918715\n",
      "Epoch 145, Loss: 0.4715229034484008\n",
      "Epoch 146, Loss: 0.47096730966267386\n",
      "Epoch 147, Loss: 0.47161469890467383\n",
      "Epoch 148, Loss: 0.47183531411129226\n",
      "Epoch 149, Loss: 0.470294933688076\n",
      "Epoch 150, Loss: 0.4697114144973765\n",
      "Epoch 151, Loss: 0.47018789053701304\n",
      "Epoch 152, Loss: 0.4700460953124117\n",
      "Epoch 153, Loss: 0.4693645740016508\n",
      "Epoch 154, Loss: 0.4684558104180481\n",
      "Epoch 155, Loss: 0.4680960384979934\n",
      "Epoch 156, Loss: 0.4682454254529793\n",
      "Epoch 157, Loss: 0.4682974379105488\n",
      "Epoch 158, Loss: 0.4681590010361841\n",
      "Epoch 159, Loss: 0.4673225124817593\n",
      "Epoch 160, Loss: 0.4666701428107949\n",
      "Epoch 161, Loss: 0.4663166784414499\n",
      "Epoch 162, Loss: 0.46624854535037\n",
      "Epoch 163, Loss: 0.4664611566834947\n",
      "Epoch 164, Loss: 0.46654532450127006\n",
      "Epoch 165, Loss: 0.46721845607201035\n",
      "Epoch 166, Loss: 0.4668208160006968\n",
      "Epoch 167, Loss: 0.466573749669782\n",
      "Epoch 168, Loss: 0.46506387643568103\n",
      "Epoch 169, Loss: 0.4641220516950567\n",
      "Epoch 170, Loss: 0.4644550172656552\n",
      "Epoch 171, Loss: 0.4654613047116713\n",
      "Epoch 172, Loss: 0.46880176576246846\n",
      "Epoch 173, Loss: 0.4685649824581709\n",
      "Epoch 174, Loss: 0.46606115996454733\n",
      "Epoch 175, Loss: 0.46328263242364126\n",
      "Epoch 176, Loss: 0.4660484187313888\n",
      "Epoch 177, Loss: 0.4667669789465077\n",
      "Epoch 178, Loss: 0.46222975993575777\n",
      "Epoch 179, Loss: 0.46412214801342555\n",
      "Epoch 180, Loss: 0.46867384291927394\n",
      "Epoch 181, Loss: 0.46316269230838986\n",
      "Epoch 182, Loss: 0.4619791093960296\n",
      "Epoch 183, Loss: 0.4650783107301587\n",
      "Epoch 184, Loss: 0.4617210775403838\n",
      "Epoch 185, Loss: 0.4621011464022392\n",
      "Epoch 186, Loss: 0.4641367239628427\n",
      "Epoch 187, Loss: 0.46071131054180564\n",
      "Epoch 188, Loss: 0.46145892957673257\n",
      "Epoch 189, Loss: 0.464304908222076\n",
      "Epoch 190, Loss: 0.4606605441278432\n",
      "Epoch 191, Loss: 0.4601386308130386\n",
      "Epoch 192, Loss: 0.46193430495315263\n",
      "Epoch 193, Loss: 0.45977696912799226\n",
      "Epoch 194, Loss: 0.4597173803227511\n",
      "Epoch 195, Loss: 0.46096589922299847\n",
      "Epoch 196, Loss: 0.4592170612502741\n",
      "Epoch 197, Loss: 0.45864347419013624\n",
      "Epoch 198, Loss: 0.4594650511157848\n",
      "Epoch 199, Loss: 0.45924705942486493\n",
      "Epoch 200, Loss: 0.4584931199827434\n",
      "Epoch 201, Loss: 0.4578552480015059\n",
      "Epoch 202, Loss: 0.458251953125\n",
      "Epoch 203, Loss: 0.4586848316367108\n",
      "Epoch 204, Loss: 0.4577025349799131\n",
      "Epoch 205, Loss: 0.45707273702530193\n",
      "Epoch 206, Loss: 0.4571336158814618\n",
      "Epoch 207, Loss: 0.45746758002181026\n",
      "Epoch 208, Loss: 0.4580334166179955\n",
      "Epoch 209, Loss: 0.457551138813026\n",
      "Epoch 210, Loss: 0.4572461918817771\n",
      "Epoch 211, Loss: 0.45629613394883295\n",
      "Epoch 212, Loss: 0.4557945440718634\n",
      "Epoch 213, Loss: 0.455843155547024\n",
      "Epoch 214, Loss: 0.456219547378948\n",
      "Epoch 215, Loss: 0.4577116995236092\n",
      "Epoch 216, Loss: 0.4589176697652806\n",
      "Epoch 217, Loss: 0.46105034303609993\n",
      "Epoch 218, Loss: 0.4567326442394303\n",
      "Epoch 219, Loss: 0.45493112899589366\n",
      "Epoch 220, Loss: 0.45527199475335134\n",
      "Epoch 221, Loss: 0.45623630281032773\n",
      "Epoch 222, Loss: 0.45802054967026307\n",
      "Epoch 223, Loss: 0.45677224995978094\n",
      "Epoch 224, Loss: 0.45565914342704716\n",
      "Epoch 225, Loss: 0.45401674524485375\n",
      "Epoch 226, Loss: 0.4535057342791329\n",
      "Epoch 227, Loss: 0.45416164205845616\n",
      "Epoch 228, Loss: 0.45450513400796655\n",
      "Epoch 229, Loss: 0.4548238117351997\n",
      "Epoch 230, Loss: 0.4537990916311975\n",
      "Epoch 231, Loss: 0.4532521496977959\n",
      "Epoch 232, Loss: 0.4526084553640119\n",
      "Epoch 233, Loss: 0.45228981460773116\n",
      "Epoch 234, Loss: 0.4520949132230242\n",
      "Epoch 235, Loss: 0.45205797975469736\n",
      "Epoch 236, Loss: 0.45230557901617613\n",
      "Epoch 237, Loss: 0.4528345953247846\n",
      "Epoch 238, Loss: 0.4549386133976144\n",
      "Epoch 239, Loss: 0.4564022887834084\n",
      "Epoch 240, Loss: 0.45805755540321125\n",
      "Epoch 241, Loss: 0.4541498302115434\n",
      "Epoch 242, Loss: 0.45158727109343993\n",
      "Epoch 243, Loss: 0.45105347173746146\n",
      "Epoch 244, Loss: 0.45263971453254503\n",
      "Epoch 245, Loss: 0.4550743757756552\n",
      "Epoch 246, Loss: 0.4549471294161167\n",
      "Epoch 247, Loss: 0.45361945699341477\n",
      "Epoch 248, Loss: 0.4506913801287277\n",
      "Epoch 249, Loss: 0.44996455105922556\n",
      "Epoch 250, Loss: 0.45070657227233163\n",
      "Epoch 251, Loss: 0.45134097806864715\n",
      "Epoch 252, Loss: 0.4518641160838718\n",
      "Epoch 253, Loss: 0.45124870735997746\n",
      "Epoch 254, Loss: 0.4507253179113287\n",
      "Epoch 255, Loss: 0.44965336548788515\n",
      "Epoch 256, Loss: 0.4490457284512466\n",
      "Epoch 257, Loss: 0.44867439459283065\n",
      "Epoch 258, Loss: 0.4485724236213288\n",
      "Epoch 259, Loss: 0.4487178165615502\n",
      "Epoch 260, Loss: 0.44913423821312026\n",
      "Epoch 261, Loss: 0.45068397397968624\n",
      "Epoch 262, Loss: 0.4523160224282965\n",
      "Epoch 263, Loss: 0.4554805695192658\n",
      "Epoch 264, Loss: 0.4521284819909996\n",
      "Epoch 265, Loss: 0.4491460824324\n",
      "Epoch 266, Loss: 0.4475917876267889\n",
      "Epoch 267, Loss: 0.4489836201844808\n",
      "Epoch 268, Loss: 0.4522696841254308\n",
      "Epoch 269, Loss: 0.45230492011773493\n",
      "Epoch 270, Loss: 0.45030259118174343\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23492207513263078\n",
      "Valence RMSE: 0.2429583469625883\n",
      "Arousal RMSE: 0.2266009806034594\n",
      "Test R^2 score: tensor([0.3692, 0.6213], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4952654496419405\n",
      "Num of epochs: 271\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Epoch 121, Loss: 0.4816228295571127\n",
      "Epoch 122, Loss: 0.4807618495484584\n",
      "Epoch 123, Loss: 0.48102030846138955\n",
      "Epoch 124, Loss: 0.4810176288364116\n",
      "Epoch 125, Loss: 0.48014338261661127\n",
      "Epoch 126, Loss: 0.47910314809270116\n",
      "Epoch 127, Loss: 0.47880795949683447\n",
      "Epoch 128, Loss: 0.47876177315142754\n",
      "Epoch 129, Loss: 0.47819597116449997\n",
      "Epoch 130, Loss: 0.47743214539542544\n",
      "Epoch 131, Loss: 0.47658476230340185\n",
      "Epoch 132, Loss: 0.4759644680184978\n",
      "Epoch 133, Loss: 0.4755635187712421\n",
      "Epoch 134, Loss: 0.47538264196584307\n",
      "Epoch 135, Loss: 0.4753673450596914\n",
      "Epoch 136, Loss: 0.4753697744168057\n",
      "Epoch 137, Loss: 0.4755973109583155\n",
      "Epoch 138, Loss: 0.4767626826605241\n",
      "Epoch 139, Loss: 0.47683528222287225\n",
      "Epoch 140, Loss: 0.47579445468261566\n",
      "Epoch 141, Loss: 0.47284533919249566\n",
      "Epoch 142, Loss: 0.472809286012082\n",
      "Epoch 143, Loss: 0.47458738111232796\n",
      "Epoch 144, Loss: 0.4733380058918715\n",
      "Epoch 145, Loss: 0.4715229034484008\n",
      "Epoch 146, Loss: 0.47096730966267386\n",
      "Epoch 147, Loss: 0.47161469890467383\n",
      "Epoch 148, Loss: 0.47183531411129226\n",
      "Epoch 149, Loss: 0.470294933688076\n",
      "Epoch 150, Loss: 0.4697114144973765\n",
      "Epoch 151, Loss: 0.47018789053701304\n",
      "Epoch 152, Loss: 0.4700460953124117\n",
      "Epoch 153, Loss: 0.4693645740016508\n",
      "Epoch 154, Loss: 0.4684558104180481\n",
      "Epoch 155, Loss: 0.4680960384979934\n",
      "Epoch 156, Loss: 0.4682454254529793\n",
      "Epoch 157, Loss: 0.4682974379105488\n",
      "Epoch 158, Loss: 0.4681590010361841\n",
      "Epoch 159, Loss: 0.4673225124817593\n",
      "Epoch 160, Loss: 0.4666701428107949\n",
      "Epoch 161, Loss: 0.4663166784414499\n",
      "Epoch 162, Loss: 0.46624854535037\n",
      "Epoch 163, Loss: 0.4664611566834947\n",
      "Epoch 164, Loss: 0.46654532450127006\n",
      "Epoch 165, Loss: 0.46721845607201035\n",
      "Epoch 166, Loss: 0.4668208160006968\n",
      "Epoch 167, Loss: 0.466573749669782\n",
      "Epoch 168, Loss: 0.46506387643568103\n",
      "Epoch 169, Loss: 0.4641220516950567\n",
      "Epoch 170, Loss: 0.4644550172656552\n",
      "Epoch 171, Loss: 0.4654613047116713\n",
      "Epoch 172, Loss: 0.46880176576246846\n",
      "Epoch 173, Loss: 0.4685649824581709\n",
      "Epoch 174, Loss: 0.46606115996454733\n",
      "Epoch 175, Loss: 0.46328263242364126\n",
      "Epoch 176, Loss: 0.4660484187313888\n",
      "Epoch 177, Loss: 0.4667669789465077\n",
      "Epoch 178, Loss: 0.46222975993575777\n",
      "Epoch 179, Loss: 0.46412214801342555\n",
      "Epoch 180, Loss: 0.46867384291927394\n",
      "Epoch 181, Loss: 0.46316269230838986\n",
      "Epoch 182, Loss: 0.4619791093960296\n",
      "Epoch 183, Loss: 0.4650783107301587\n",
      "Epoch 184, Loss: 0.4617210775403838\n",
      "Epoch 185, Loss: 0.4621011464022392\n",
      "Epoch 186, Loss: 0.4641367239628427\n",
      "Epoch 187, Loss: 0.46071131054180564\n",
      "Epoch 188, Loss: 0.46145892957673257\n",
      "Epoch 189, Loss: 0.464304908222076\n",
      "Epoch 190, Loss: 0.4606605441278432\n",
      "Epoch 191, Loss: 0.4601386308130386\n",
      "Epoch 192, Loss: 0.46193430495315263\n",
      "Epoch 193, Loss: 0.45977696912799226\n",
      "Epoch 194, Loss: 0.4597173803227511\n",
      "Epoch 195, Loss: 0.46096589922299847\n",
      "Epoch 196, Loss: 0.4592170612502741\n",
      "Epoch 197, Loss: 0.45864347419013624\n",
      "Epoch 198, Loss: 0.4594650511157848\n",
      "Epoch 199, Loss: 0.45924705942486493\n",
      "Epoch 200, Loss: 0.4584931199827434\n",
      "Epoch 201, Loss: 0.4578552480015059\n",
      "Epoch 202, Loss: 0.458251953125\n",
      "Epoch 203, Loss: 0.4586848316367108\n",
      "Epoch 204, Loss: 0.4577025349799131\n",
      "Epoch 205, Loss: 0.45707273702530193\n",
      "Epoch 206, Loss: 0.4571336158814618\n",
      "Epoch 207, Loss: 0.45746758002181026\n",
      "Epoch 208, Loss: 0.4580334166179955\n",
      "Epoch 209, Loss: 0.457551138813026\n",
      "Epoch 210, Loss: 0.4572461918817771\n",
      "Epoch 211, Loss: 0.45629613394883295\n",
      "Epoch 212, Loss: 0.4557945440718634\n",
      "Epoch 213, Loss: 0.455843155547024\n",
      "Epoch 214, Loss: 0.456219547378948\n",
      "Epoch 215, Loss: 0.4577116995236092\n",
      "Epoch 216, Loss: 0.4589176697652806\n",
      "Epoch 217, Loss: 0.46105034303609993\n",
      "Epoch 218, Loss: 0.4567326442394303\n",
      "Epoch 219, Loss: 0.45493112899589366\n",
      "Epoch 220, Loss: 0.45527199475335134\n",
      "Epoch 221, Loss: 0.45623630281032773\n",
      "Epoch 222, Loss: 0.45802054967026307\n",
      "Epoch 223, Loss: 0.45677224995978094\n",
      "Epoch 224, Loss: 0.45565914342704716\n",
      "Epoch 225, Loss: 0.45401674524485375\n",
      "Epoch 226, Loss: 0.4535057342791329\n",
      "Epoch 227, Loss: 0.45416164205845616\n",
      "Epoch 228, Loss: 0.45450513400796655\n",
      "Epoch 229, Loss: 0.4548238117351997\n",
      "Epoch 230, Loss: 0.4537990916311975\n",
      "Epoch 231, Loss: 0.4532521496977959\n",
      "Epoch 232, Loss: 0.4526084553640119\n",
      "Epoch 233, Loss: 0.45228981460773116\n",
      "Epoch 234, Loss: 0.4520949132230242\n",
      "Epoch 235, Loss: 0.45205797975469736\n",
      "Epoch 236, Loss: 0.45230557901617613\n",
      "Epoch 237, Loss: 0.4528345953247846\n",
      "Epoch 238, Loss: 0.4549386133976144\n",
      "Epoch 239, Loss: 0.4564022887834084\n",
      "Epoch 240, Loss: 0.45805755540321125\n",
      "Epoch 241, Loss: 0.4541498302115434\n",
      "Epoch 242, Loss: 0.45158727109343993\n",
      "Epoch 243, Loss: 0.45105347173746146\n",
      "Epoch 244, Loss: 0.45263971453254503\n",
      "Epoch 245, Loss: 0.4550743757756552\n",
      "Epoch 246, Loss: 0.4549471294161167\n",
      "Epoch 247, Loss: 0.45361945699341477\n",
      "Epoch 248, Loss: 0.4506913801287277\n",
      "Epoch 249, Loss: 0.44996455105922556\n",
      "Epoch 250, Loss: 0.45070657227233163\n",
      "Epoch 251, Loss: 0.45134097806864715\n",
      "Epoch 252, Loss: 0.4518641160838718\n",
      "Epoch 253, Loss: 0.45124870735997746\n",
      "Epoch 254, Loss: 0.4507253179113287\n",
      "Epoch 255, Loss: 0.44965336548788515\n",
      "Epoch 256, Loss: 0.4490457284512466\n",
      "Epoch 257, Loss: 0.44867439459283065\n",
      "Epoch 258, Loss: 0.4485724236213288\n",
      "Epoch 259, Loss: 0.4487178165615502\n",
      "Epoch 260, Loss: 0.44913423821312026\n",
      "Epoch 261, Loss: 0.45068397397968624\n",
      "Epoch 262, Loss: 0.4523160224282965\n",
      "Epoch 263, Loss: 0.4554805695192658\n",
      "Epoch 264, Loss: 0.4521284819909996\n",
      "Epoch 265, Loss: 0.4491460824324\n",
      "Epoch 266, Loss: 0.4475917876267889\n",
      "Epoch 267, Loss: 0.4489836201844808\n",
      "Epoch 268, Loss: 0.4522696841254308\n",
      "Epoch 269, Loss: 0.45230492011773493\n",
      "Epoch 270, Loss: 0.45030259118174343\n",
      "Epoch 271, Loss: 0.44750189058874124\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23429076815215394\n",
      "Valence RMSE: 0.2430220273282606\n",
      "Arousal RMSE: 0.22522127411928666\n",
      "Test R^2 score: tensor([0.3689, 0.6259], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4973988067163652\n",
      "Num of epochs: 272\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Epoch 121, Loss: 0.4816228295571127\n",
      "Epoch 122, Loss: 0.4807618495484584\n",
      "Epoch 123, Loss: 0.48102030846138955\n",
      "Epoch 124, Loss: 0.4810176288364116\n",
      "Epoch 125, Loss: 0.48014338261661127\n",
      "Epoch 126, Loss: 0.47910314809270116\n",
      "Epoch 127, Loss: 0.47880795949683447\n",
      "Epoch 128, Loss: 0.47876177315142754\n",
      "Epoch 129, Loss: 0.47819597116449997\n",
      "Epoch 130, Loss: 0.47743214539542544\n",
      "Epoch 131, Loss: 0.47658476230340185\n",
      "Epoch 132, Loss: 0.4759644680184978\n",
      "Epoch 133, Loss: 0.4755635187712421\n",
      "Epoch 134, Loss: 0.47538264196584307\n",
      "Epoch 135, Loss: 0.4753673450596914\n",
      "Epoch 136, Loss: 0.4753697744168057\n",
      "Epoch 137, Loss: 0.4755973109583155\n",
      "Epoch 138, Loss: 0.4767626826605241\n",
      "Epoch 139, Loss: 0.47683528222287225\n",
      "Epoch 140, Loss: 0.47579445468261566\n",
      "Epoch 141, Loss: 0.47284533919249566\n",
      "Epoch 142, Loss: 0.472809286012082\n",
      "Epoch 143, Loss: 0.47458738111232796\n",
      "Epoch 144, Loss: 0.4733380058918715\n",
      "Epoch 145, Loss: 0.4715229034484008\n",
      "Epoch 146, Loss: 0.47096730966267386\n",
      "Epoch 147, Loss: 0.47161469890467383\n",
      "Epoch 148, Loss: 0.47183531411129226\n",
      "Epoch 149, Loss: 0.470294933688076\n",
      "Epoch 150, Loss: 0.4697114144973765\n",
      "Epoch 151, Loss: 0.47018789053701304\n",
      "Epoch 152, Loss: 0.4700460953124117\n",
      "Epoch 153, Loss: 0.4693645740016508\n",
      "Epoch 154, Loss: 0.4684558104180481\n",
      "Epoch 155, Loss: 0.4680960384979934\n",
      "Epoch 156, Loss: 0.4682454254529793\n",
      "Epoch 157, Loss: 0.4682974379105488\n",
      "Epoch 158, Loss: 0.4681590010361841\n",
      "Epoch 159, Loss: 0.4673225124817593\n",
      "Epoch 160, Loss: 0.4666701428107949\n",
      "Epoch 161, Loss: 0.4663166784414499\n",
      "Epoch 162, Loss: 0.46624854535037\n",
      "Epoch 163, Loss: 0.4664611566834947\n",
      "Epoch 164, Loss: 0.46654532450127006\n",
      "Epoch 165, Loss: 0.46721845607201035\n",
      "Epoch 166, Loss: 0.4668208160006968\n",
      "Epoch 167, Loss: 0.466573749669782\n",
      "Epoch 168, Loss: 0.46506387643568103\n",
      "Epoch 169, Loss: 0.4641220516950567\n",
      "Epoch 170, Loss: 0.4644550172656552\n",
      "Epoch 171, Loss: 0.4654613047116713\n",
      "Epoch 172, Loss: 0.46880176576246846\n",
      "Epoch 173, Loss: 0.4685649824581709\n",
      "Epoch 174, Loss: 0.46606115996454733\n",
      "Epoch 175, Loss: 0.46328263242364126\n",
      "Epoch 176, Loss: 0.4660484187313888\n",
      "Epoch 177, Loss: 0.4667669789465077\n",
      "Epoch 178, Loss: 0.46222975993575777\n",
      "Epoch 179, Loss: 0.46412214801342555\n",
      "Epoch 180, Loss: 0.46867384291927394\n",
      "Epoch 181, Loss: 0.46316269230838986\n",
      "Epoch 182, Loss: 0.4619791093960296\n",
      "Epoch 183, Loss: 0.4650783107301587\n",
      "Epoch 184, Loss: 0.4617210775403838\n",
      "Epoch 185, Loss: 0.4621011464022392\n",
      "Epoch 186, Loss: 0.4641367239628427\n",
      "Epoch 187, Loss: 0.46071131054180564\n",
      "Epoch 188, Loss: 0.46145892957673257\n",
      "Epoch 189, Loss: 0.464304908222076\n",
      "Epoch 190, Loss: 0.4606605441278432\n",
      "Epoch 191, Loss: 0.4601386308130386\n",
      "Epoch 192, Loss: 0.46193430495315263\n",
      "Epoch 193, Loss: 0.45977696912799226\n",
      "Epoch 194, Loss: 0.4597173803227511\n",
      "Epoch 195, Loss: 0.46096589922299847\n",
      "Epoch 196, Loss: 0.4592170612502741\n",
      "Epoch 197, Loss: 0.45864347419013624\n",
      "Epoch 198, Loss: 0.4594650511157848\n",
      "Epoch 199, Loss: 0.45924705942486493\n",
      "Epoch 200, Loss: 0.4584931199827434\n",
      "Epoch 201, Loss: 0.4578552480015059\n",
      "Epoch 202, Loss: 0.458251953125\n",
      "Epoch 203, Loss: 0.4586848316367108\n",
      "Epoch 204, Loss: 0.4577025349799131\n",
      "Epoch 205, Loss: 0.45707273702530193\n",
      "Epoch 206, Loss: 0.4571336158814618\n",
      "Epoch 207, Loss: 0.45746758002181026\n",
      "Epoch 208, Loss: 0.4580334166179955\n",
      "Epoch 209, Loss: 0.457551138813026\n",
      "Epoch 210, Loss: 0.4572461918817771\n",
      "Epoch 211, Loss: 0.45629613394883295\n",
      "Epoch 212, Loss: 0.4557945440718634\n",
      "Epoch 213, Loss: 0.455843155547024\n",
      "Epoch 214, Loss: 0.456219547378948\n",
      "Epoch 215, Loss: 0.4577116995236092\n",
      "Epoch 216, Loss: 0.4589176697652806\n",
      "Epoch 217, Loss: 0.46105034303609993\n",
      "Epoch 218, Loss: 0.4567326442394303\n",
      "Epoch 219, Loss: 0.45493112899589366\n",
      "Epoch 220, Loss: 0.45527199475335134\n",
      "Epoch 221, Loss: 0.45623630281032773\n",
      "Epoch 222, Loss: 0.45802054967026307\n",
      "Epoch 223, Loss: 0.45677224995978094\n",
      "Epoch 224, Loss: 0.45565914342704716\n",
      "Epoch 225, Loss: 0.45401674524485375\n",
      "Epoch 226, Loss: 0.4535057342791329\n",
      "Epoch 227, Loss: 0.45416164205845616\n",
      "Epoch 228, Loss: 0.45450513400796655\n",
      "Epoch 229, Loss: 0.4548238117351997\n",
      "Epoch 230, Loss: 0.4537990916311975\n",
      "Epoch 231, Loss: 0.4532521496977959\n",
      "Epoch 232, Loss: 0.4526084553640119\n",
      "Epoch 233, Loss: 0.45228981460773116\n",
      "Epoch 234, Loss: 0.4520949132230242\n",
      "Epoch 235, Loss: 0.45205797975469736\n",
      "Epoch 236, Loss: 0.45230557901617613\n",
      "Epoch 237, Loss: 0.4528345953247846\n",
      "Epoch 238, Loss: 0.4549386133976144\n",
      "Epoch 239, Loss: 0.4564022887834084\n",
      "Epoch 240, Loss: 0.45805755540321125\n",
      "Epoch 241, Loss: 0.4541498302115434\n",
      "Epoch 242, Loss: 0.45158727109343993\n",
      "Epoch 243, Loss: 0.45105347173746146\n",
      "Epoch 244, Loss: 0.45263971453254503\n",
      "Epoch 245, Loss: 0.4550743757756552\n",
      "Epoch 246, Loss: 0.4549471294161167\n",
      "Epoch 247, Loss: 0.45361945699341477\n",
      "Epoch 248, Loss: 0.4506913801287277\n",
      "Epoch 249, Loss: 0.44996455105922556\n",
      "Epoch 250, Loss: 0.45070657227233163\n",
      "Epoch 251, Loss: 0.45134097806864715\n",
      "Epoch 252, Loss: 0.4518641160838718\n",
      "Epoch 253, Loss: 0.45124870735997746\n",
      "Epoch 254, Loss: 0.4507253179113287\n",
      "Epoch 255, Loss: 0.44965336548788515\n",
      "Epoch 256, Loss: 0.4490457284512466\n",
      "Epoch 257, Loss: 0.44867439459283065\n",
      "Epoch 258, Loss: 0.4485724236213288\n",
      "Epoch 259, Loss: 0.4487178165615502\n",
      "Epoch 260, Loss: 0.44913423821312026\n",
      "Epoch 261, Loss: 0.45068397397968624\n",
      "Epoch 262, Loss: 0.4523160224282965\n",
      "Epoch 263, Loss: 0.4554805695192658\n",
      "Epoch 264, Loss: 0.4521284819909996\n",
      "Epoch 265, Loss: 0.4491460824324\n",
      "Epoch 266, Loss: 0.4475917876267889\n",
      "Epoch 267, Loss: 0.4489836201844808\n",
      "Epoch 268, Loss: 0.4522696841254308\n",
      "Epoch 269, Loss: 0.45230492011773493\n",
      "Epoch 270, Loss: 0.45030259118174343\n",
      "Epoch 271, Loss: 0.44750189058874124\n",
      "Epoch 272, Loss: 0.4466950387656147\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2353481721955594\n",
      "Valence RMSE: 0.24553361067878218\n",
      "Arousal RMSE: 0.2247015138770133\n",
      "Test R^2 score: tensor([0.3558, 0.6276], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4917050294540243\n",
      "Num of epochs: 273\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Epoch 121, Loss: 0.4816228295571127\n",
      "Epoch 122, Loss: 0.4807618495484584\n",
      "Epoch 123, Loss: 0.48102030846138955\n",
      "Epoch 124, Loss: 0.4810176288364116\n",
      "Epoch 125, Loss: 0.48014338261661127\n",
      "Epoch 126, Loss: 0.47910314809270116\n",
      "Epoch 127, Loss: 0.47880795949683447\n",
      "Epoch 128, Loss: 0.47876177315142754\n",
      "Epoch 129, Loss: 0.47819597116449997\n",
      "Epoch 130, Loss: 0.47743214539542544\n",
      "Epoch 131, Loss: 0.47658476230340185\n",
      "Epoch 132, Loss: 0.4759644680184978\n",
      "Epoch 133, Loss: 0.4755635187712421\n",
      "Epoch 134, Loss: 0.47538264196584307\n",
      "Epoch 135, Loss: 0.4753673450596914\n",
      "Epoch 136, Loss: 0.4753697744168057\n",
      "Epoch 137, Loss: 0.4755973109583155\n",
      "Epoch 138, Loss: 0.4767626826605241\n",
      "Epoch 139, Loss: 0.47683528222287225\n",
      "Epoch 140, Loss: 0.47579445468261566\n",
      "Epoch 141, Loss: 0.47284533919249566\n",
      "Epoch 142, Loss: 0.472809286012082\n",
      "Epoch 143, Loss: 0.47458738111232796\n",
      "Epoch 144, Loss: 0.4733380058918715\n",
      "Epoch 145, Loss: 0.4715229034484008\n",
      "Epoch 146, Loss: 0.47096730966267386\n",
      "Epoch 147, Loss: 0.47161469890467383\n",
      "Epoch 148, Loss: 0.47183531411129226\n",
      "Epoch 149, Loss: 0.470294933688076\n",
      "Epoch 150, Loss: 0.4697114144973765\n",
      "Epoch 151, Loss: 0.47018789053701304\n",
      "Epoch 152, Loss: 0.4700460953124117\n",
      "Epoch 153, Loss: 0.4693645740016508\n",
      "Epoch 154, Loss: 0.4684558104180481\n",
      "Epoch 155, Loss: 0.4680960384979934\n",
      "Epoch 156, Loss: 0.4682454254529793\n",
      "Epoch 157, Loss: 0.4682974379105488\n",
      "Epoch 158, Loss: 0.4681590010361841\n",
      "Epoch 159, Loss: 0.4673225124817593\n",
      "Epoch 160, Loss: 0.4666701428107949\n",
      "Epoch 161, Loss: 0.4663166784414499\n",
      "Epoch 162, Loss: 0.46624854535037\n",
      "Epoch 163, Loss: 0.4664611566834947\n",
      "Epoch 164, Loss: 0.46654532450127006\n",
      "Epoch 165, Loss: 0.46721845607201035\n",
      "Epoch 166, Loss: 0.4668208160006968\n",
      "Epoch 167, Loss: 0.466573749669782\n",
      "Epoch 168, Loss: 0.46506387643568103\n",
      "Epoch 169, Loss: 0.4641220516950567\n",
      "Epoch 170, Loss: 0.4644550172656552\n",
      "Epoch 171, Loss: 0.4654613047116713\n",
      "Epoch 172, Loss: 0.46880176576246846\n",
      "Epoch 173, Loss: 0.4685649824581709\n",
      "Epoch 174, Loss: 0.46606115996454733\n",
      "Epoch 175, Loss: 0.46328263242364126\n",
      "Epoch 176, Loss: 0.4660484187313888\n",
      "Epoch 177, Loss: 0.4667669789465077\n",
      "Epoch 178, Loss: 0.46222975993575777\n",
      "Epoch 179, Loss: 0.46412214801342555\n",
      "Epoch 180, Loss: 0.46867384291927394\n",
      "Epoch 181, Loss: 0.46316269230838986\n",
      "Epoch 182, Loss: 0.4619791093960296\n",
      "Epoch 183, Loss: 0.4650783107301587\n",
      "Epoch 184, Loss: 0.4617210775403838\n",
      "Epoch 185, Loss: 0.4621011464022392\n",
      "Epoch 186, Loss: 0.4641367239628427\n",
      "Epoch 187, Loss: 0.46071131054180564\n",
      "Epoch 188, Loss: 0.46145892957673257\n",
      "Epoch 189, Loss: 0.464304908222076\n",
      "Epoch 190, Loss: 0.4606605441278432\n",
      "Epoch 191, Loss: 0.4601386308130386\n",
      "Epoch 192, Loss: 0.46193430495315263\n",
      "Epoch 193, Loss: 0.45977696912799226\n",
      "Epoch 194, Loss: 0.4597173803227511\n",
      "Epoch 195, Loss: 0.46096589922299847\n",
      "Epoch 196, Loss: 0.4592170612502741\n",
      "Epoch 197, Loss: 0.45864347419013624\n",
      "Epoch 198, Loss: 0.4594650511157848\n",
      "Epoch 199, Loss: 0.45924705942486493\n",
      "Epoch 200, Loss: 0.4584931199827434\n",
      "Epoch 201, Loss: 0.4578552480015059\n",
      "Epoch 202, Loss: 0.458251953125\n",
      "Epoch 203, Loss: 0.4586848316367108\n",
      "Epoch 204, Loss: 0.4577025349799131\n",
      "Epoch 205, Loss: 0.45707273702530193\n",
      "Epoch 206, Loss: 0.4571336158814618\n",
      "Epoch 207, Loss: 0.45746758002181026\n",
      "Epoch 208, Loss: 0.4580334166179955\n",
      "Epoch 209, Loss: 0.457551138813026\n",
      "Epoch 210, Loss: 0.4572461918817771\n",
      "Epoch 211, Loss: 0.45629613394883295\n",
      "Epoch 212, Loss: 0.4557945440718634\n",
      "Epoch 213, Loss: 0.455843155547024\n",
      "Epoch 214, Loss: 0.456219547378948\n",
      "Epoch 215, Loss: 0.4577116995236092\n",
      "Epoch 216, Loss: 0.4589176697652806\n",
      "Epoch 217, Loss: 0.46105034303609993\n",
      "Epoch 218, Loss: 0.4567326442394303\n",
      "Epoch 219, Loss: 0.45493112899589366\n",
      "Epoch 220, Loss: 0.45527199475335134\n",
      "Epoch 221, Loss: 0.45623630281032773\n",
      "Epoch 222, Loss: 0.45802054967026307\n",
      "Epoch 223, Loss: 0.45677224995978094\n",
      "Epoch 224, Loss: 0.45565914342704716\n",
      "Epoch 225, Loss: 0.45401674524485375\n",
      "Epoch 226, Loss: 0.4535057342791329\n",
      "Epoch 227, Loss: 0.45416164205845616\n",
      "Epoch 228, Loss: 0.45450513400796655\n",
      "Epoch 229, Loss: 0.4548238117351997\n",
      "Epoch 230, Loss: 0.4537990916311975\n",
      "Epoch 231, Loss: 0.4532521496977959\n",
      "Epoch 232, Loss: 0.4526084553640119\n",
      "Epoch 233, Loss: 0.45228981460773116\n",
      "Epoch 234, Loss: 0.4520949132230242\n",
      "Epoch 235, Loss: 0.45205797975469736\n",
      "Epoch 236, Loss: 0.45230557901617613\n",
      "Epoch 237, Loss: 0.4528345953247846\n",
      "Epoch 238, Loss: 0.4549386133976144\n",
      "Epoch 239, Loss: 0.4564022887834084\n",
      "Epoch 240, Loss: 0.45805755540321125\n",
      "Epoch 241, Loss: 0.4541498302115434\n",
      "Epoch 242, Loss: 0.45158727109343993\n",
      "Epoch 243, Loss: 0.45105347173746146\n",
      "Epoch 244, Loss: 0.45263971453254503\n",
      "Epoch 245, Loss: 0.4550743757756552\n",
      "Epoch 246, Loss: 0.4549471294161167\n",
      "Epoch 247, Loss: 0.45361945699341477\n",
      "Epoch 248, Loss: 0.4506913801287277\n",
      "Epoch 249, Loss: 0.44996455105922556\n",
      "Epoch 250, Loss: 0.45070657227233163\n",
      "Epoch 251, Loss: 0.45134097806864715\n",
      "Epoch 252, Loss: 0.4518641160838718\n",
      "Epoch 253, Loss: 0.45124870735997746\n",
      "Epoch 254, Loss: 0.4507253179113287\n",
      "Epoch 255, Loss: 0.44965336548788515\n",
      "Epoch 256, Loss: 0.4490457284512466\n",
      "Epoch 257, Loss: 0.44867439459283065\n",
      "Epoch 258, Loss: 0.4485724236213288\n",
      "Epoch 259, Loss: 0.4487178165615502\n",
      "Epoch 260, Loss: 0.44913423821312026\n",
      "Epoch 261, Loss: 0.45068397397968624\n",
      "Epoch 262, Loss: 0.4523160224282965\n",
      "Epoch 263, Loss: 0.4554805695192658\n",
      "Epoch 264, Loss: 0.4521284819909996\n",
      "Epoch 265, Loss: 0.4491460824324\n",
      "Epoch 266, Loss: 0.4475917876267889\n",
      "Epoch 267, Loss: 0.4489836201844808\n",
      "Epoch 268, Loss: 0.4522696841254308\n",
      "Epoch 269, Loss: 0.45230492011773493\n",
      "Epoch 270, Loss: 0.45030259118174343\n",
      "Epoch 271, Loss: 0.44750189058874124\n",
      "Epoch 272, Loss: 0.4466950387656147\n",
      "Epoch 273, Loss: 0.44782488644087964\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23544197811781656\n",
      "Valence RMSE: 0.2424591752628466\n",
      "Arousal RMSE: 0.2282091112355531\n",
      "Test R^2 score: tensor([0.3718, 0.6159], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.49386309046195703\n",
      "Num of epochs: 274\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Epoch 121, Loss: 0.4816228295571127\n",
      "Epoch 122, Loss: 0.4807618495484584\n",
      "Epoch 123, Loss: 0.48102030846138955\n",
      "Epoch 124, Loss: 0.4810176288364116\n",
      "Epoch 125, Loss: 0.48014338261661127\n",
      "Epoch 126, Loss: 0.47910314809270116\n",
      "Epoch 127, Loss: 0.47880795949683447\n",
      "Epoch 128, Loss: 0.47876177315142754\n",
      "Epoch 129, Loss: 0.47819597116449997\n",
      "Epoch 130, Loss: 0.47743214539542544\n",
      "Epoch 131, Loss: 0.47658476230340185\n",
      "Epoch 132, Loss: 0.4759644680184978\n",
      "Epoch 133, Loss: 0.4755635187712421\n",
      "Epoch 134, Loss: 0.47538264196584307\n",
      "Epoch 135, Loss: 0.4753673450596914\n",
      "Epoch 136, Loss: 0.4753697744168057\n",
      "Epoch 137, Loss: 0.4755973109583155\n",
      "Epoch 138, Loss: 0.4767626826605241\n",
      "Epoch 139, Loss: 0.47683528222287225\n",
      "Epoch 140, Loss: 0.47579445468261566\n",
      "Epoch 141, Loss: 0.47284533919249566\n",
      "Epoch 142, Loss: 0.472809286012082\n",
      "Epoch 143, Loss: 0.47458738111232796\n",
      "Epoch 144, Loss: 0.4733380058918715\n",
      "Epoch 145, Loss: 0.4715229034484008\n",
      "Epoch 146, Loss: 0.47096730966267386\n",
      "Epoch 147, Loss: 0.47161469890467383\n",
      "Epoch 148, Loss: 0.47183531411129226\n",
      "Epoch 149, Loss: 0.470294933688076\n",
      "Epoch 150, Loss: 0.4697114144973765\n",
      "Epoch 151, Loss: 0.47018789053701304\n",
      "Epoch 152, Loss: 0.4700460953124117\n",
      "Epoch 153, Loss: 0.4693645740016508\n",
      "Epoch 154, Loss: 0.4684558104180481\n",
      "Epoch 155, Loss: 0.4680960384979934\n",
      "Epoch 156, Loss: 0.4682454254529793\n",
      "Epoch 157, Loss: 0.4682974379105488\n",
      "Epoch 158, Loss: 0.4681590010361841\n",
      "Epoch 159, Loss: 0.4673225124817593\n",
      "Epoch 160, Loss: 0.4666701428107949\n",
      "Epoch 161, Loss: 0.4663166784414499\n",
      "Epoch 162, Loss: 0.46624854535037\n",
      "Epoch 163, Loss: 0.4664611566834947\n",
      "Epoch 164, Loss: 0.46654532450127006\n",
      "Epoch 165, Loss: 0.46721845607201035\n",
      "Epoch 166, Loss: 0.4668208160006968\n",
      "Epoch 167, Loss: 0.466573749669782\n",
      "Epoch 168, Loss: 0.46506387643568103\n",
      "Epoch 169, Loss: 0.4641220516950567\n",
      "Epoch 170, Loss: 0.4644550172656552\n",
      "Epoch 171, Loss: 0.4654613047116713\n",
      "Epoch 172, Loss: 0.46880176576246846\n",
      "Epoch 173, Loss: 0.4685649824581709\n",
      "Epoch 174, Loss: 0.46606115996454733\n",
      "Epoch 175, Loss: 0.46328263242364126\n",
      "Epoch 176, Loss: 0.4660484187313888\n",
      "Epoch 177, Loss: 0.4667669789465077\n",
      "Epoch 178, Loss: 0.46222975993575777\n",
      "Epoch 179, Loss: 0.46412214801342555\n",
      "Epoch 180, Loss: 0.46867384291927394\n",
      "Epoch 181, Loss: 0.46316269230838986\n",
      "Epoch 182, Loss: 0.4619791093960296\n",
      "Epoch 183, Loss: 0.4650783107301587\n",
      "Epoch 184, Loss: 0.4617210775403838\n",
      "Epoch 185, Loss: 0.4621011464022392\n",
      "Epoch 186, Loss: 0.4641367239628427\n",
      "Epoch 187, Loss: 0.46071131054180564\n",
      "Epoch 188, Loss: 0.46145892957673257\n",
      "Epoch 189, Loss: 0.464304908222076\n",
      "Epoch 190, Loss: 0.4606605441278432\n",
      "Epoch 191, Loss: 0.4601386308130386\n",
      "Epoch 192, Loss: 0.46193430495315263\n",
      "Epoch 193, Loss: 0.45977696912799226\n",
      "Epoch 194, Loss: 0.4597173803227511\n",
      "Epoch 195, Loss: 0.46096589922299847\n",
      "Epoch 196, Loss: 0.4592170612502741\n",
      "Epoch 197, Loss: 0.45864347419013624\n",
      "Epoch 198, Loss: 0.4594650511157848\n",
      "Epoch 199, Loss: 0.45924705942486493\n",
      "Epoch 200, Loss: 0.4584931199827434\n",
      "Epoch 201, Loss: 0.4578552480015059\n",
      "Epoch 202, Loss: 0.458251953125\n",
      "Epoch 203, Loss: 0.4586848316367108\n",
      "Epoch 204, Loss: 0.4577025349799131\n",
      "Epoch 205, Loss: 0.45707273702530193\n",
      "Epoch 206, Loss: 0.4571336158814618\n",
      "Epoch 207, Loss: 0.45746758002181026\n",
      "Epoch 208, Loss: 0.4580334166179955\n",
      "Epoch 209, Loss: 0.457551138813026\n",
      "Epoch 210, Loss: 0.4572461918817771\n",
      "Epoch 211, Loss: 0.45629613394883295\n",
      "Epoch 212, Loss: 0.4557945440718634\n",
      "Epoch 213, Loss: 0.455843155547024\n",
      "Epoch 214, Loss: 0.456219547378948\n",
      "Epoch 215, Loss: 0.4577116995236092\n",
      "Epoch 216, Loss: 0.4589176697652806\n",
      "Epoch 217, Loss: 0.46105034303609993\n",
      "Epoch 218, Loss: 0.4567326442394303\n",
      "Epoch 219, Loss: 0.45493112899589366\n",
      "Epoch 220, Loss: 0.45527199475335134\n",
      "Epoch 221, Loss: 0.45623630281032773\n",
      "Epoch 222, Loss: 0.45802054967026307\n",
      "Epoch 223, Loss: 0.45677224995978094\n",
      "Epoch 224, Loss: 0.45565914342704716\n",
      "Epoch 225, Loss: 0.45401674524485375\n",
      "Epoch 226, Loss: 0.4535057342791329\n",
      "Epoch 227, Loss: 0.45416164205845616\n",
      "Epoch 228, Loss: 0.45450513400796655\n",
      "Epoch 229, Loss: 0.4548238117351997\n",
      "Epoch 230, Loss: 0.4537990916311975\n",
      "Epoch 231, Loss: 0.4532521496977959\n",
      "Epoch 232, Loss: 0.4526084553640119\n",
      "Epoch 233, Loss: 0.45228981460773116\n",
      "Epoch 234, Loss: 0.4520949132230242\n",
      "Epoch 235, Loss: 0.45205797975469736\n",
      "Epoch 236, Loss: 0.45230557901617613\n",
      "Epoch 237, Loss: 0.4528345953247846\n",
      "Epoch 238, Loss: 0.4549386133976144\n",
      "Epoch 239, Loss: 0.4564022887834084\n",
      "Epoch 240, Loss: 0.45805755540321125\n",
      "Epoch 241, Loss: 0.4541498302115434\n",
      "Epoch 242, Loss: 0.45158727109343993\n",
      "Epoch 243, Loss: 0.45105347173746146\n",
      "Epoch 244, Loss: 0.45263971453254503\n",
      "Epoch 245, Loss: 0.4550743757756552\n",
      "Epoch 246, Loss: 0.4549471294161167\n",
      "Epoch 247, Loss: 0.45361945699341477\n",
      "Epoch 248, Loss: 0.4506913801287277\n",
      "Epoch 249, Loss: 0.44996455105922556\n",
      "Epoch 250, Loss: 0.45070657227233163\n",
      "Epoch 251, Loss: 0.45134097806864715\n",
      "Epoch 252, Loss: 0.4518641160838718\n",
      "Epoch 253, Loss: 0.45124870735997746\n",
      "Epoch 254, Loss: 0.4507253179113287\n",
      "Epoch 255, Loss: 0.44965336548788515\n",
      "Epoch 256, Loss: 0.4490457284512466\n",
      "Epoch 257, Loss: 0.44867439459283065\n",
      "Epoch 258, Loss: 0.4485724236213288\n",
      "Epoch 259, Loss: 0.4487178165615502\n",
      "Epoch 260, Loss: 0.44913423821312026\n",
      "Epoch 261, Loss: 0.45068397397968624\n",
      "Epoch 262, Loss: 0.4523160224282965\n",
      "Epoch 263, Loss: 0.4554805695192658\n",
      "Epoch 264, Loss: 0.4521284819909996\n",
      "Epoch 265, Loss: 0.4491460824324\n",
      "Epoch 266, Loss: 0.4475917876267889\n",
      "Epoch 267, Loss: 0.4489836201844808\n",
      "Epoch 268, Loss: 0.4522696841254308\n",
      "Epoch 269, Loss: 0.45230492011773493\n",
      "Epoch 270, Loss: 0.45030259118174343\n",
      "Epoch 271, Loss: 0.44750189058874124\n",
      "Epoch 272, Loss: 0.4466950387656147\n",
      "Epoch 273, Loss: 0.44782488644087964\n",
      "Epoch 274, Loss: 0.4490013591686314\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2368931638569563\n",
      "Valence RMSE: 0.24786037230071342\n",
      "Arousal RMSE: 0.22539294134304483\n",
      "Test R^2 score: tensor([0.3435, 0.6253], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.48442365502233214\n",
      "Num of epochs: 275\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Epoch 121, Loss: 0.4816228295571127\n",
      "Epoch 122, Loss: 0.4807618495484584\n",
      "Epoch 123, Loss: 0.48102030846138955\n",
      "Epoch 124, Loss: 0.4810176288364116\n",
      "Epoch 125, Loss: 0.48014338261661127\n",
      "Epoch 126, Loss: 0.47910314809270116\n",
      "Epoch 127, Loss: 0.47880795949683447\n",
      "Epoch 128, Loss: 0.47876177315142754\n",
      "Epoch 129, Loss: 0.47819597116449997\n",
      "Epoch 130, Loss: 0.47743214539542544\n",
      "Epoch 131, Loss: 0.47658476230340185\n",
      "Epoch 132, Loss: 0.4759644680184978\n",
      "Epoch 133, Loss: 0.4755635187712421\n",
      "Epoch 134, Loss: 0.47538264196584307\n",
      "Epoch 135, Loss: 0.4753673450596914\n",
      "Epoch 136, Loss: 0.4753697744168057\n",
      "Epoch 137, Loss: 0.4755973109583155\n",
      "Epoch 138, Loss: 0.4767626826605241\n",
      "Epoch 139, Loss: 0.47683528222287225\n",
      "Epoch 140, Loss: 0.47579445468261566\n",
      "Epoch 141, Loss: 0.47284533919249566\n",
      "Epoch 142, Loss: 0.472809286012082\n",
      "Epoch 143, Loss: 0.47458738111232796\n",
      "Epoch 144, Loss: 0.4733380058918715\n",
      "Epoch 145, Loss: 0.4715229034484008\n",
      "Epoch 146, Loss: 0.47096730966267386\n",
      "Epoch 147, Loss: 0.47161469890467383\n",
      "Epoch 148, Loss: 0.47183531411129226\n",
      "Epoch 149, Loss: 0.470294933688076\n",
      "Epoch 150, Loss: 0.4697114144973765\n",
      "Epoch 151, Loss: 0.47018789053701304\n",
      "Epoch 152, Loss: 0.4700460953124117\n",
      "Epoch 153, Loss: 0.4693645740016508\n",
      "Epoch 154, Loss: 0.4684558104180481\n",
      "Epoch 155, Loss: 0.4680960384979934\n",
      "Epoch 156, Loss: 0.4682454254529793\n",
      "Epoch 157, Loss: 0.4682974379105488\n",
      "Epoch 158, Loss: 0.4681590010361841\n",
      "Epoch 159, Loss: 0.4673225124817593\n",
      "Epoch 160, Loss: 0.4666701428107949\n",
      "Epoch 161, Loss: 0.4663166784414499\n",
      "Epoch 162, Loss: 0.46624854535037\n",
      "Epoch 163, Loss: 0.4664611566834947\n",
      "Epoch 164, Loss: 0.46654532450127006\n",
      "Epoch 165, Loss: 0.46721845607201035\n",
      "Epoch 166, Loss: 0.4668208160006968\n",
      "Epoch 167, Loss: 0.466573749669782\n",
      "Epoch 168, Loss: 0.46506387643568103\n",
      "Epoch 169, Loss: 0.4641220516950567\n",
      "Epoch 170, Loss: 0.4644550172656552\n",
      "Epoch 171, Loss: 0.4654613047116713\n",
      "Epoch 172, Loss: 0.46880176576246846\n",
      "Epoch 173, Loss: 0.4685649824581709\n",
      "Epoch 174, Loss: 0.46606115996454733\n",
      "Epoch 175, Loss: 0.46328263242364126\n",
      "Epoch 176, Loss: 0.4660484187313888\n",
      "Epoch 177, Loss: 0.4667669789465077\n",
      "Epoch 178, Loss: 0.46222975993575777\n",
      "Epoch 179, Loss: 0.46412214801342555\n",
      "Epoch 180, Loss: 0.46867384291927394\n",
      "Epoch 181, Loss: 0.46316269230838986\n",
      "Epoch 182, Loss: 0.4619791093960296\n",
      "Epoch 183, Loss: 0.4650783107301587\n",
      "Epoch 184, Loss: 0.4617210775403838\n",
      "Epoch 185, Loss: 0.4621011464022392\n",
      "Epoch 186, Loss: 0.4641367239628427\n",
      "Epoch 187, Loss: 0.46071131054180564\n",
      "Epoch 188, Loss: 0.46145892957673257\n",
      "Epoch 189, Loss: 0.464304908222076\n",
      "Epoch 190, Loss: 0.4606605441278432\n",
      "Epoch 191, Loss: 0.4601386308130386\n",
      "Epoch 192, Loss: 0.46193430495315263\n",
      "Epoch 193, Loss: 0.45977696912799226\n",
      "Epoch 194, Loss: 0.4597173803227511\n",
      "Epoch 195, Loss: 0.46096589922299847\n",
      "Epoch 196, Loss: 0.4592170612502741\n",
      "Epoch 197, Loss: 0.45864347419013624\n",
      "Epoch 198, Loss: 0.4594650511157848\n",
      "Epoch 199, Loss: 0.45924705942486493\n",
      "Epoch 200, Loss: 0.4584931199827434\n",
      "Epoch 201, Loss: 0.4578552480015059\n",
      "Epoch 202, Loss: 0.458251953125\n",
      "Epoch 203, Loss: 0.4586848316367108\n",
      "Epoch 204, Loss: 0.4577025349799131\n",
      "Epoch 205, Loss: 0.45707273702530193\n",
      "Epoch 206, Loss: 0.4571336158814618\n",
      "Epoch 207, Loss: 0.45746758002181026\n",
      "Epoch 208, Loss: 0.4580334166179955\n",
      "Epoch 209, Loss: 0.457551138813026\n",
      "Epoch 210, Loss: 0.4572461918817771\n",
      "Epoch 211, Loss: 0.45629613394883295\n",
      "Epoch 212, Loss: 0.4557945440718634\n",
      "Epoch 213, Loss: 0.455843155547024\n",
      "Epoch 214, Loss: 0.456219547378948\n",
      "Epoch 215, Loss: 0.4577116995236092\n",
      "Epoch 216, Loss: 0.4589176697652806\n",
      "Epoch 217, Loss: 0.46105034303609993\n",
      "Epoch 218, Loss: 0.4567326442394303\n",
      "Epoch 219, Loss: 0.45493112899589366\n",
      "Epoch 220, Loss: 0.45527199475335134\n",
      "Epoch 221, Loss: 0.45623630281032773\n",
      "Epoch 222, Loss: 0.45802054967026307\n",
      "Epoch 223, Loss: 0.45677224995978094\n",
      "Epoch 224, Loss: 0.45565914342704716\n",
      "Epoch 225, Loss: 0.45401674524485375\n",
      "Epoch 226, Loss: 0.4535057342791329\n",
      "Epoch 227, Loss: 0.45416164205845616\n",
      "Epoch 228, Loss: 0.45450513400796655\n",
      "Epoch 229, Loss: 0.4548238117351997\n",
      "Epoch 230, Loss: 0.4537990916311975\n",
      "Epoch 231, Loss: 0.4532521496977959\n",
      "Epoch 232, Loss: 0.4526084553640119\n",
      "Epoch 233, Loss: 0.45228981460773116\n",
      "Epoch 234, Loss: 0.4520949132230242\n",
      "Epoch 235, Loss: 0.45205797975469736\n",
      "Epoch 236, Loss: 0.45230557901617613\n",
      "Epoch 237, Loss: 0.4528345953247846\n",
      "Epoch 238, Loss: 0.4549386133976144\n",
      "Epoch 239, Loss: 0.4564022887834084\n",
      "Epoch 240, Loss: 0.45805755540321125\n",
      "Epoch 241, Loss: 0.4541498302115434\n",
      "Epoch 242, Loss: 0.45158727109343993\n",
      "Epoch 243, Loss: 0.45105347173746146\n",
      "Epoch 244, Loss: 0.45263971453254503\n",
      "Epoch 245, Loss: 0.4550743757756552\n",
      "Epoch 246, Loss: 0.4549471294161167\n",
      "Epoch 247, Loss: 0.45361945699341477\n",
      "Epoch 248, Loss: 0.4506913801287277\n",
      "Epoch 249, Loss: 0.44996455105922556\n",
      "Epoch 250, Loss: 0.45070657227233163\n",
      "Epoch 251, Loss: 0.45134097806864715\n",
      "Epoch 252, Loss: 0.4518641160838718\n",
      "Epoch 253, Loss: 0.45124870735997746\n",
      "Epoch 254, Loss: 0.4507253179113287\n",
      "Epoch 255, Loss: 0.44965336548788515\n",
      "Epoch 256, Loss: 0.4490457284512466\n",
      "Epoch 257, Loss: 0.44867439459283065\n",
      "Epoch 258, Loss: 0.4485724236213288\n",
      "Epoch 259, Loss: 0.4487178165615502\n",
      "Epoch 260, Loss: 0.44913423821312026\n",
      "Epoch 261, Loss: 0.45068397397968624\n",
      "Epoch 262, Loss: 0.4523160224282965\n",
      "Epoch 263, Loss: 0.4554805695192658\n",
      "Epoch 264, Loss: 0.4521284819909996\n",
      "Epoch 265, Loss: 0.4491460824324\n",
      "Epoch 266, Loss: 0.4475917876267889\n",
      "Epoch 267, Loss: 0.4489836201844808\n",
      "Epoch 268, Loss: 0.4522696841254308\n",
      "Epoch 269, Loss: 0.45230492011773493\n",
      "Epoch 270, Loss: 0.45030259118174343\n",
      "Epoch 271, Loss: 0.44750189058874124\n",
      "Epoch 272, Loss: 0.4466950387656147\n",
      "Epoch 273, Loss: 0.44782488644087964\n",
      "Epoch 274, Loss: 0.4490013591686314\n",
      "Epoch 275, Loss: 0.44932258069431275\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23494351579064224\n",
      "Valence RMSE: 0.24237608496222365\n",
      "Arousal RMSE: 0.22726800184478338\n",
      "Test R^2 score: tensor([0.3722, 0.6191], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.49565898526026364\n",
      "Num of epochs: 276\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Epoch 121, Loss: 0.4816228295571127\n",
      "Epoch 122, Loss: 0.4807618495484584\n",
      "Epoch 123, Loss: 0.48102030846138955\n",
      "Epoch 124, Loss: 0.4810176288364116\n",
      "Epoch 125, Loss: 0.48014338261661127\n",
      "Epoch 126, Loss: 0.47910314809270116\n",
      "Epoch 127, Loss: 0.47880795949683447\n",
      "Epoch 128, Loss: 0.47876177315142754\n",
      "Epoch 129, Loss: 0.47819597116449997\n",
      "Epoch 130, Loss: 0.47743214539542544\n",
      "Epoch 131, Loss: 0.47658476230340185\n",
      "Epoch 132, Loss: 0.4759644680184978\n",
      "Epoch 133, Loss: 0.4755635187712421\n",
      "Epoch 134, Loss: 0.47538264196584307\n",
      "Epoch 135, Loss: 0.4753673450596914\n",
      "Epoch 136, Loss: 0.4753697744168057\n",
      "Epoch 137, Loss: 0.4755973109583155\n",
      "Epoch 138, Loss: 0.4767626826605241\n",
      "Epoch 139, Loss: 0.47683528222287225\n",
      "Epoch 140, Loss: 0.47579445468261566\n",
      "Epoch 141, Loss: 0.47284533919249566\n",
      "Epoch 142, Loss: 0.472809286012082\n",
      "Epoch 143, Loss: 0.47458738111232796\n",
      "Epoch 144, Loss: 0.4733380058918715\n",
      "Epoch 145, Loss: 0.4715229034484008\n",
      "Epoch 146, Loss: 0.47096730966267386\n",
      "Epoch 147, Loss: 0.47161469890467383\n",
      "Epoch 148, Loss: 0.47183531411129226\n",
      "Epoch 149, Loss: 0.470294933688076\n",
      "Epoch 150, Loss: 0.4697114144973765\n",
      "Epoch 151, Loss: 0.47018789053701304\n",
      "Epoch 152, Loss: 0.4700460953124117\n",
      "Epoch 153, Loss: 0.4693645740016508\n",
      "Epoch 154, Loss: 0.4684558104180481\n",
      "Epoch 155, Loss: 0.4680960384979934\n",
      "Epoch 156, Loss: 0.4682454254529793\n",
      "Epoch 157, Loss: 0.4682974379105488\n",
      "Epoch 158, Loss: 0.4681590010361841\n",
      "Epoch 159, Loss: 0.4673225124817593\n",
      "Epoch 160, Loss: 0.4666701428107949\n",
      "Epoch 161, Loss: 0.4663166784414499\n",
      "Epoch 162, Loss: 0.46624854535037\n",
      "Epoch 163, Loss: 0.4664611566834947\n",
      "Epoch 164, Loss: 0.46654532450127006\n",
      "Epoch 165, Loss: 0.46721845607201035\n",
      "Epoch 166, Loss: 0.4668208160006968\n",
      "Epoch 167, Loss: 0.466573749669782\n",
      "Epoch 168, Loss: 0.46506387643568103\n",
      "Epoch 169, Loss: 0.4641220516950567\n",
      "Epoch 170, Loss: 0.4644550172656552\n",
      "Epoch 171, Loss: 0.4654613047116713\n",
      "Epoch 172, Loss: 0.46880176576246846\n",
      "Epoch 173, Loss: 0.4685649824581709\n",
      "Epoch 174, Loss: 0.46606115996454733\n",
      "Epoch 175, Loss: 0.46328263242364126\n",
      "Epoch 176, Loss: 0.4660484187313888\n",
      "Epoch 177, Loss: 0.4667669789465077\n",
      "Epoch 178, Loss: 0.46222975993575777\n",
      "Epoch 179, Loss: 0.46412214801342555\n",
      "Epoch 180, Loss: 0.46867384291927394\n",
      "Epoch 181, Loss: 0.46316269230838986\n",
      "Epoch 182, Loss: 0.4619791093960296\n",
      "Epoch 183, Loss: 0.4650783107301587\n",
      "Epoch 184, Loss: 0.4617210775403838\n",
      "Epoch 185, Loss: 0.4621011464022392\n",
      "Epoch 186, Loss: 0.4641367239628427\n",
      "Epoch 187, Loss: 0.46071131054180564\n",
      "Epoch 188, Loss: 0.46145892957673257\n",
      "Epoch 189, Loss: 0.464304908222076\n",
      "Epoch 190, Loss: 0.4606605441278432\n",
      "Epoch 191, Loss: 0.4601386308130386\n",
      "Epoch 192, Loss: 0.46193430495315263\n",
      "Epoch 193, Loss: 0.45977696912799226\n",
      "Epoch 194, Loss: 0.4597173803227511\n",
      "Epoch 195, Loss: 0.46096589922299847\n",
      "Epoch 196, Loss: 0.4592170612502741\n",
      "Epoch 197, Loss: 0.45864347419013624\n",
      "Epoch 198, Loss: 0.4594650511157848\n",
      "Epoch 199, Loss: 0.45924705942486493\n",
      "Epoch 200, Loss: 0.4584931199827434\n",
      "Epoch 201, Loss: 0.4578552480015059\n",
      "Epoch 202, Loss: 0.458251953125\n",
      "Epoch 203, Loss: 0.4586848316367108\n",
      "Epoch 204, Loss: 0.4577025349799131\n",
      "Epoch 205, Loss: 0.45707273702530193\n",
      "Epoch 206, Loss: 0.4571336158814618\n",
      "Epoch 207, Loss: 0.45746758002181026\n",
      "Epoch 208, Loss: 0.4580334166179955\n",
      "Epoch 209, Loss: 0.457551138813026\n",
      "Epoch 210, Loss: 0.4572461918817771\n",
      "Epoch 211, Loss: 0.45629613394883295\n",
      "Epoch 212, Loss: 0.4557945440718634\n",
      "Epoch 213, Loss: 0.455843155547024\n",
      "Epoch 214, Loss: 0.456219547378948\n",
      "Epoch 215, Loss: 0.4577116995236092\n",
      "Epoch 216, Loss: 0.4589176697652806\n",
      "Epoch 217, Loss: 0.46105034303609993\n",
      "Epoch 218, Loss: 0.4567326442394303\n",
      "Epoch 219, Loss: 0.45493112899589366\n",
      "Epoch 220, Loss: 0.45527199475335134\n",
      "Epoch 221, Loss: 0.45623630281032773\n",
      "Epoch 222, Loss: 0.45802054967026307\n",
      "Epoch 223, Loss: 0.45677224995978094\n",
      "Epoch 224, Loss: 0.45565914342704716\n",
      "Epoch 225, Loss: 0.45401674524485375\n",
      "Epoch 226, Loss: 0.4535057342791329\n",
      "Epoch 227, Loss: 0.45416164205845616\n",
      "Epoch 228, Loss: 0.45450513400796655\n",
      "Epoch 229, Loss: 0.4548238117351997\n",
      "Epoch 230, Loss: 0.4537990916311975\n",
      "Epoch 231, Loss: 0.4532521496977959\n",
      "Epoch 232, Loss: 0.4526084553640119\n",
      "Epoch 233, Loss: 0.45228981460773116\n",
      "Epoch 234, Loss: 0.4520949132230242\n",
      "Epoch 235, Loss: 0.45205797975469736\n",
      "Epoch 236, Loss: 0.45230557901617613\n",
      "Epoch 237, Loss: 0.4528345953247846\n",
      "Epoch 238, Loss: 0.4549386133976144\n",
      "Epoch 239, Loss: 0.4564022887834084\n",
      "Epoch 240, Loss: 0.45805755540321125\n",
      "Epoch 241, Loss: 0.4541498302115434\n",
      "Epoch 242, Loss: 0.45158727109343993\n",
      "Epoch 243, Loss: 0.45105347173746146\n",
      "Epoch 244, Loss: 0.45263971453254503\n",
      "Epoch 245, Loss: 0.4550743757756552\n",
      "Epoch 246, Loss: 0.4549471294161167\n",
      "Epoch 247, Loss: 0.45361945699341477\n",
      "Epoch 248, Loss: 0.4506913801287277\n",
      "Epoch 249, Loss: 0.44996455105922556\n",
      "Epoch 250, Loss: 0.45070657227233163\n",
      "Epoch 251, Loss: 0.45134097806864715\n",
      "Epoch 252, Loss: 0.4518641160838718\n",
      "Epoch 253, Loss: 0.45124870735997746\n",
      "Epoch 254, Loss: 0.4507253179113287\n",
      "Epoch 255, Loss: 0.44965336548788515\n",
      "Epoch 256, Loss: 0.4490457284512466\n",
      "Epoch 257, Loss: 0.44867439459283065\n",
      "Epoch 258, Loss: 0.4485724236213288\n",
      "Epoch 259, Loss: 0.4487178165615502\n",
      "Epoch 260, Loss: 0.44913423821312026\n",
      "Epoch 261, Loss: 0.45068397397968624\n",
      "Epoch 262, Loss: 0.4523160224282965\n",
      "Epoch 263, Loss: 0.4554805695192658\n",
      "Epoch 264, Loss: 0.4521284819909996\n",
      "Epoch 265, Loss: 0.4491460824324\n",
      "Epoch 266, Loss: 0.4475917876267889\n",
      "Epoch 267, Loss: 0.4489836201844808\n",
      "Epoch 268, Loss: 0.4522696841254308\n",
      "Epoch 269, Loss: 0.45230492011773493\n",
      "Epoch 270, Loss: 0.45030259118174343\n",
      "Epoch 271, Loss: 0.44750189058874124\n",
      "Epoch 272, Loss: 0.4466950387656147\n",
      "Epoch 273, Loss: 0.44782488644087964\n",
      "Epoch 274, Loss: 0.4490013591686314\n",
      "Epoch 275, Loss: 0.44932258069431275\n",
      "Epoch 276, Loss: 0.4477355520631983\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23525775245511324\n",
      "Valence RMSE: 0.24537732460236167\n",
      "Arousal RMSE: 0.2246828626118377\n",
      "Test R^2 score: tensor([0.3566, 0.6277], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4921458638768425\n",
      "Num of epochs: 277\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Epoch 121, Loss: 0.4816228295571127\n",
      "Epoch 122, Loss: 0.4807618495484584\n",
      "Epoch 123, Loss: 0.48102030846138955\n",
      "Epoch 124, Loss: 0.4810176288364116\n",
      "Epoch 125, Loss: 0.48014338261661127\n",
      "Epoch 126, Loss: 0.47910314809270116\n",
      "Epoch 127, Loss: 0.47880795949683447\n",
      "Epoch 128, Loss: 0.47876177315142754\n",
      "Epoch 129, Loss: 0.47819597116449997\n",
      "Epoch 130, Loss: 0.47743214539542544\n",
      "Epoch 131, Loss: 0.47658476230340185\n",
      "Epoch 132, Loss: 0.4759644680184978\n",
      "Epoch 133, Loss: 0.4755635187712421\n",
      "Epoch 134, Loss: 0.47538264196584307\n",
      "Epoch 135, Loss: 0.4753673450596914\n",
      "Epoch 136, Loss: 0.4753697744168057\n",
      "Epoch 137, Loss: 0.4755973109583155\n",
      "Epoch 138, Loss: 0.4767626826605241\n",
      "Epoch 139, Loss: 0.47683528222287225\n",
      "Epoch 140, Loss: 0.47579445468261566\n",
      "Epoch 141, Loss: 0.47284533919249566\n",
      "Epoch 142, Loss: 0.472809286012082\n",
      "Epoch 143, Loss: 0.47458738111232796\n",
      "Epoch 144, Loss: 0.4733380058918715\n",
      "Epoch 145, Loss: 0.4715229034484008\n",
      "Epoch 146, Loss: 0.47096730966267386\n",
      "Epoch 147, Loss: 0.47161469890467383\n",
      "Epoch 148, Loss: 0.47183531411129226\n",
      "Epoch 149, Loss: 0.470294933688076\n",
      "Epoch 150, Loss: 0.4697114144973765\n",
      "Epoch 151, Loss: 0.47018789053701304\n",
      "Epoch 152, Loss: 0.4700460953124117\n",
      "Epoch 153, Loss: 0.4693645740016508\n",
      "Epoch 154, Loss: 0.4684558104180481\n",
      "Epoch 155, Loss: 0.4680960384979934\n",
      "Epoch 156, Loss: 0.4682454254529793\n",
      "Epoch 157, Loss: 0.4682974379105488\n",
      "Epoch 158, Loss: 0.4681590010361841\n",
      "Epoch 159, Loss: 0.4673225124817593\n",
      "Epoch 160, Loss: 0.4666701428107949\n",
      "Epoch 161, Loss: 0.4663166784414499\n",
      "Epoch 162, Loss: 0.46624854535037\n",
      "Epoch 163, Loss: 0.4664611566834947\n",
      "Epoch 164, Loss: 0.46654532450127006\n",
      "Epoch 165, Loss: 0.46721845607201035\n",
      "Epoch 166, Loss: 0.4668208160006968\n",
      "Epoch 167, Loss: 0.466573749669782\n",
      "Epoch 168, Loss: 0.46506387643568103\n",
      "Epoch 169, Loss: 0.4641220516950567\n",
      "Epoch 170, Loss: 0.4644550172656552\n",
      "Epoch 171, Loss: 0.4654613047116713\n",
      "Epoch 172, Loss: 0.46880176576246846\n",
      "Epoch 173, Loss: 0.4685649824581709\n",
      "Epoch 174, Loss: 0.46606115996454733\n",
      "Epoch 175, Loss: 0.46328263242364126\n",
      "Epoch 176, Loss: 0.4660484187313888\n",
      "Epoch 177, Loss: 0.4667669789465077\n",
      "Epoch 178, Loss: 0.46222975993575777\n",
      "Epoch 179, Loss: 0.46412214801342555\n",
      "Epoch 180, Loss: 0.46867384291927394\n",
      "Epoch 181, Loss: 0.46316269230838986\n",
      "Epoch 182, Loss: 0.4619791093960296\n",
      "Epoch 183, Loss: 0.4650783107301587\n",
      "Epoch 184, Loss: 0.4617210775403838\n",
      "Epoch 185, Loss: 0.4621011464022392\n",
      "Epoch 186, Loss: 0.4641367239628427\n",
      "Epoch 187, Loss: 0.46071131054180564\n",
      "Epoch 188, Loss: 0.46145892957673257\n",
      "Epoch 189, Loss: 0.464304908222076\n",
      "Epoch 190, Loss: 0.4606605441278432\n",
      "Epoch 191, Loss: 0.4601386308130386\n",
      "Epoch 192, Loss: 0.46193430495315263\n",
      "Epoch 193, Loss: 0.45977696912799226\n",
      "Epoch 194, Loss: 0.4597173803227511\n",
      "Epoch 195, Loss: 0.46096589922299847\n",
      "Epoch 196, Loss: 0.4592170612502741\n",
      "Epoch 197, Loss: 0.45864347419013624\n",
      "Epoch 198, Loss: 0.4594650511157848\n",
      "Epoch 199, Loss: 0.45924705942486493\n",
      "Epoch 200, Loss: 0.4584931199827434\n",
      "Epoch 201, Loss: 0.4578552480015059\n",
      "Epoch 202, Loss: 0.458251953125\n",
      "Epoch 203, Loss: 0.4586848316367108\n",
      "Epoch 204, Loss: 0.4577025349799131\n",
      "Epoch 205, Loss: 0.45707273702530193\n",
      "Epoch 206, Loss: 0.4571336158814618\n",
      "Epoch 207, Loss: 0.45746758002181026\n",
      "Epoch 208, Loss: 0.4580334166179955\n",
      "Epoch 209, Loss: 0.457551138813026\n",
      "Epoch 210, Loss: 0.4572461918817771\n",
      "Epoch 211, Loss: 0.45629613394883295\n",
      "Epoch 212, Loss: 0.4557945440718634\n",
      "Epoch 213, Loss: 0.455843155547024\n",
      "Epoch 214, Loss: 0.456219547378948\n",
      "Epoch 215, Loss: 0.4577116995236092\n",
      "Epoch 216, Loss: 0.4589176697652806\n",
      "Epoch 217, Loss: 0.46105034303609993\n",
      "Epoch 218, Loss: 0.4567326442394303\n",
      "Epoch 219, Loss: 0.45493112899589366\n",
      "Epoch 220, Loss: 0.45527199475335134\n",
      "Epoch 221, Loss: 0.45623630281032773\n",
      "Epoch 222, Loss: 0.45802054967026307\n",
      "Epoch 223, Loss: 0.45677224995978094\n",
      "Epoch 224, Loss: 0.45565914342704716\n",
      "Epoch 225, Loss: 0.45401674524485375\n",
      "Epoch 226, Loss: 0.4535057342791329\n",
      "Epoch 227, Loss: 0.45416164205845616\n",
      "Epoch 228, Loss: 0.45450513400796655\n",
      "Epoch 229, Loss: 0.4548238117351997\n",
      "Epoch 230, Loss: 0.4537990916311975\n",
      "Epoch 231, Loss: 0.4532521496977959\n",
      "Epoch 232, Loss: 0.4526084553640119\n",
      "Epoch 233, Loss: 0.45228981460773116\n",
      "Epoch 234, Loss: 0.4520949132230242\n",
      "Epoch 235, Loss: 0.45205797975469736\n",
      "Epoch 236, Loss: 0.45230557901617613\n",
      "Epoch 237, Loss: 0.4528345953247846\n",
      "Epoch 238, Loss: 0.4549386133976144\n",
      "Epoch 239, Loss: 0.4564022887834084\n",
      "Epoch 240, Loss: 0.45805755540321125\n",
      "Epoch 241, Loss: 0.4541498302115434\n",
      "Epoch 242, Loss: 0.45158727109343993\n",
      "Epoch 243, Loss: 0.45105347173746146\n",
      "Epoch 244, Loss: 0.45263971453254503\n",
      "Epoch 245, Loss: 0.4550743757756552\n",
      "Epoch 246, Loss: 0.4549471294161167\n",
      "Epoch 247, Loss: 0.45361945699341477\n",
      "Epoch 248, Loss: 0.4506913801287277\n",
      "Epoch 249, Loss: 0.44996455105922556\n",
      "Epoch 250, Loss: 0.45070657227233163\n",
      "Epoch 251, Loss: 0.45134097806864715\n",
      "Epoch 252, Loss: 0.4518641160838718\n",
      "Epoch 253, Loss: 0.45124870735997746\n",
      "Epoch 254, Loss: 0.4507253179113287\n",
      "Epoch 255, Loss: 0.44965336548788515\n",
      "Epoch 256, Loss: 0.4490457284512466\n",
      "Epoch 257, Loss: 0.44867439459283065\n",
      "Epoch 258, Loss: 0.4485724236213288\n",
      "Epoch 259, Loss: 0.4487178165615502\n",
      "Epoch 260, Loss: 0.44913423821312026\n",
      "Epoch 261, Loss: 0.45068397397968624\n",
      "Epoch 262, Loss: 0.4523160224282965\n",
      "Epoch 263, Loss: 0.4554805695192658\n",
      "Epoch 264, Loss: 0.4521284819909996\n",
      "Epoch 265, Loss: 0.4491460824324\n",
      "Epoch 266, Loss: 0.4475917876267889\n",
      "Epoch 267, Loss: 0.4489836201844808\n",
      "Epoch 268, Loss: 0.4522696841254308\n",
      "Epoch 269, Loss: 0.45230492011773493\n",
      "Epoch 270, Loss: 0.45030259118174343\n",
      "Epoch 271, Loss: 0.44750189058874124\n",
      "Epoch 272, Loss: 0.4466950387656147\n",
      "Epoch 273, Loss: 0.44782488644087964\n",
      "Epoch 274, Loss: 0.4490013591686314\n",
      "Epoch 275, Loss: 0.44932258069431275\n",
      "Epoch 276, Loss: 0.4477355520631983\n",
      "Epoch 277, Loss: 0.4464409066948383\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23485946048663028\n",
      "Valence RMSE: 0.24409898359282395\n",
      "Arousal RMSE: 0.22524124526625305\n",
      "Test R^2 score: tensor([0.3633, 0.6258], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.49456265979016145\n",
      "Num of epochs: 278\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Epoch 121, Loss: 0.4816228295571127\n",
      "Epoch 122, Loss: 0.4807618495484584\n",
      "Epoch 123, Loss: 0.48102030846138955\n",
      "Epoch 124, Loss: 0.4810176288364116\n",
      "Epoch 125, Loss: 0.48014338261661127\n",
      "Epoch 126, Loss: 0.47910314809270116\n",
      "Epoch 127, Loss: 0.47880795949683447\n",
      "Epoch 128, Loss: 0.47876177315142754\n",
      "Epoch 129, Loss: 0.47819597116449997\n",
      "Epoch 130, Loss: 0.47743214539542544\n",
      "Epoch 131, Loss: 0.47658476230340185\n",
      "Epoch 132, Loss: 0.4759644680184978\n",
      "Epoch 133, Loss: 0.4755635187712421\n",
      "Epoch 134, Loss: 0.47538264196584307\n",
      "Epoch 135, Loss: 0.4753673450596914\n",
      "Epoch 136, Loss: 0.4753697744168057\n",
      "Epoch 137, Loss: 0.4755973109583155\n",
      "Epoch 138, Loss: 0.4767626826605241\n",
      "Epoch 139, Loss: 0.47683528222287225\n",
      "Epoch 140, Loss: 0.47579445468261566\n",
      "Epoch 141, Loss: 0.47284533919249566\n",
      "Epoch 142, Loss: 0.472809286012082\n",
      "Epoch 143, Loss: 0.47458738111232796\n",
      "Epoch 144, Loss: 0.4733380058918715\n",
      "Epoch 145, Loss: 0.4715229034484008\n",
      "Epoch 146, Loss: 0.47096730966267386\n",
      "Epoch 147, Loss: 0.47161469890467383\n",
      "Epoch 148, Loss: 0.47183531411129226\n",
      "Epoch 149, Loss: 0.470294933688076\n",
      "Epoch 150, Loss: 0.4697114144973765\n",
      "Epoch 151, Loss: 0.47018789053701304\n",
      "Epoch 152, Loss: 0.4700460953124117\n",
      "Epoch 153, Loss: 0.4693645740016508\n",
      "Epoch 154, Loss: 0.4684558104180481\n",
      "Epoch 155, Loss: 0.4680960384979934\n",
      "Epoch 156, Loss: 0.4682454254529793\n",
      "Epoch 157, Loss: 0.4682974379105488\n",
      "Epoch 158, Loss: 0.4681590010361841\n",
      "Epoch 159, Loss: 0.4673225124817593\n",
      "Epoch 160, Loss: 0.4666701428107949\n",
      "Epoch 161, Loss: 0.4663166784414499\n",
      "Epoch 162, Loss: 0.46624854535037\n",
      "Epoch 163, Loss: 0.4664611566834947\n",
      "Epoch 164, Loss: 0.46654532450127006\n",
      "Epoch 165, Loss: 0.46721845607201035\n",
      "Epoch 166, Loss: 0.4668208160006968\n",
      "Epoch 167, Loss: 0.466573749669782\n",
      "Epoch 168, Loss: 0.46506387643568103\n",
      "Epoch 169, Loss: 0.4641220516950567\n",
      "Epoch 170, Loss: 0.4644550172656552\n",
      "Epoch 171, Loss: 0.4654613047116713\n",
      "Epoch 172, Loss: 0.46880176576246846\n",
      "Epoch 173, Loss: 0.4685649824581709\n",
      "Epoch 174, Loss: 0.46606115996454733\n",
      "Epoch 175, Loss: 0.46328263242364126\n",
      "Epoch 176, Loss: 0.4660484187313888\n",
      "Epoch 177, Loss: 0.4667669789465077\n",
      "Epoch 178, Loss: 0.46222975993575777\n",
      "Epoch 179, Loss: 0.46412214801342555\n",
      "Epoch 180, Loss: 0.46867384291927394\n",
      "Epoch 181, Loss: 0.46316269230838986\n",
      "Epoch 182, Loss: 0.4619791093960296\n",
      "Epoch 183, Loss: 0.4650783107301587\n",
      "Epoch 184, Loss: 0.4617210775403838\n",
      "Epoch 185, Loss: 0.4621011464022392\n",
      "Epoch 186, Loss: 0.4641367239628427\n",
      "Epoch 187, Loss: 0.46071131054180564\n",
      "Epoch 188, Loss: 0.46145892957673257\n",
      "Epoch 189, Loss: 0.464304908222076\n",
      "Epoch 190, Loss: 0.4606605441278432\n",
      "Epoch 191, Loss: 0.4601386308130386\n",
      "Epoch 192, Loss: 0.46193430495315263\n",
      "Epoch 193, Loss: 0.45977696912799226\n",
      "Epoch 194, Loss: 0.4597173803227511\n",
      "Epoch 195, Loss: 0.46096589922299847\n",
      "Epoch 196, Loss: 0.4592170612502741\n",
      "Epoch 197, Loss: 0.45864347419013624\n",
      "Epoch 198, Loss: 0.4594650511157848\n",
      "Epoch 199, Loss: 0.45924705942486493\n",
      "Epoch 200, Loss: 0.4584931199827434\n",
      "Epoch 201, Loss: 0.4578552480015059\n",
      "Epoch 202, Loss: 0.458251953125\n",
      "Epoch 203, Loss: 0.4586848316367108\n",
      "Epoch 204, Loss: 0.4577025349799131\n",
      "Epoch 205, Loss: 0.45707273702530193\n",
      "Epoch 206, Loss: 0.4571336158814618\n",
      "Epoch 207, Loss: 0.45746758002181026\n",
      "Epoch 208, Loss: 0.4580334166179955\n",
      "Epoch 209, Loss: 0.457551138813026\n",
      "Epoch 210, Loss: 0.4572461918817771\n",
      "Epoch 211, Loss: 0.45629613394883295\n",
      "Epoch 212, Loss: 0.4557945440718634\n",
      "Epoch 213, Loss: 0.455843155547024\n",
      "Epoch 214, Loss: 0.456219547378948\n",
      "Epoch 215, Loss: 0.4577116995236092\n",
      "Epoch 216, Loss: 0.4589176697652806\n",
      "Epoch 217, Loss: 0.46105034303609993\n",
      "Epoch 218, Loss: 0.4567326442394303\n",
      "Epoch 219, Loss: 0.45493112899589366\n",
      "Epoch 220, Loss: 0.45527199475335134\n",
      "Epoch 221, Loss: 0.45623630281032773\n",
      "Epoch 222, Loss: 0.45802054967026307\n",
      "Epoch 223, Loss: 0.45677224995978094\n",
      "Epoch 224, Loss: 0.45565914342704716\n",
      "Epoch 225, Loss: 0.45401674524485375\n",
      "Epoch 226, Loss: 0.4535057342791329\n",
      "Epoch 227, Loss: 0.45416164205845616\n",
      "Epoch 228, Loss: 0.45450513400796655\n",
      "Epoch 229, Loss: 0.4548238117351997\n",
      "Epoch 230, Loss: 0.4537990916311975\n",
      "Epoch 231, Loss: 0.4532521496977959\n",
      "Epoch 232, Loss: 0.4526084553640119\n",
      "Epoch 233, Loss: 0.45228981460773116\n",
      "Epoch 234, Loss: 0.4520949132230242\n",
      "Epoch 235, Loss: 0.45205797975469736\n",
      "Epoch 236, Loss: 0.45230557901617613\n",
      "Epoch 237, Loss: 0.4528345953247846\n",
      "Epoch 238, Loss: 0.4549386133976144\n",
      "Epoch 239, Loss: 0.4564022887834084\n",
      "Epoch 240, Loss: 0.45805755540321125\n",
      "Epoch 241, Loss: 0.4541498302115434\n",
      "Epoch 242, Loss: 0.45158727109343993\n",
      "Epoch 243, Loss: 0.45105347173746146\n",
      "Epoch 244, Loss: 0.45263971453254503\n",
      "Epoch 245, Loss: 0.4550743757756552\n",
      "Epoch 246, Loss: 0.4549471294161167\n",
      "Epoch 247, Loss: 0.45361945699341477\n",
      "Epoch 248, Loss: 0.4506913801287277\n",
      "Epoch 249, Loss: 0.44996455105922556\n",
      "Epoch 250, Loss: 0.45070657227233163\n",
      "Epoch 251, Loss: 0.45134097806864715\n",
      "Epoch 252, Loss: 0.4518641160838718\n",
      "Epoch 253, Loss: 0.45124870735997746\n",
      "Epoch 254, Loss: 0.4507253179113287\n",
      "Epoch 255, Loss: 0.44965336548788515\n",
      "Epoch 256, Loss: 0.4490457284512466\n",
      "Epoch 257, Loss: 0.44867439459283065\n",
      "Epoch 258, Loss: 0.4485724236213288\n",
      "Epoch 259, Loss: 0.4487178165615502\n",
      "Epoch 260, Loss: 0.44913423821312026\n",
      "Epoch 261, Loss: 0.45068397397968624\n",
      "Epoch 262, Loss: 0.4523160224282965\n",
      "Epoch 263, Loss: 0.4554805695192658\n",
      "Epoch 264, Loss: 0.4521284819909996\n",
      "Epoch 265, Loss: 0.4491460824324\n",
      "Epoch 266, Loss: 0.4475917876267889\n",
      "Epoch 267, Loss: 0.4489836201844808\n",
      "Epoch 268, Loss: 0.4522696841254308\n",
      "Epoch 269, Loss: 0.45230492011773493\n",
      "Epoch 270, Loss: 0.45030259118174343\n",
      "Epoch 271, Loss: 0.44750189058874124\n",
      "Epoch 272, Loss: 0.4466950387656147\n",
      "Epoch 273, Loss: 0.44782488644087964\n",
      "Epoch 274, Loss: 0.4490013591686314\n",
      "Epoch 275, Loss: 0.44932258069431275\n",
      "Epoch 276, Loss: 0.4477355520631983\n",
      "Epoch 277, Loss: 0.4464409066948383\n",
      "Epoch 278, Loss: 0.4458417591594104\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23486635423579963\n",
      "Valence RMSE: 0.24406804072324909\n",
      "Arousal RMSE: 0.22528914798886038\n",
      "Test R^2 score: tensor([0.3634, 0.6257], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.49456378531268036\n",
      "Num of epochs: 279\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Epoch 121, Loss: 0.4816228295571127\n",
      "Epoch 122, Loss: 0.4807618495484584\n",
      "Epoch 123, Loss: 0.48102030846138955\n",
      "Epoch 124, Loss: 0.4810176288364116\n",
      "Epoch 125, Loss: 0.48014338261661127\n",
      "Epoch 126, Loss: 0.47910314809270116\n",
      "Epoch 127, Loss: 0.47880795949683447\n",
      "Epoch 128, Loss: 0.47876177315142754\n",
      "Epoch 129, Loss: 0.47819597116449997\n",
      "Epoch 130, Loss: 0.47743214539542544\n",
      "Epoch 131, Loss: 0.47658476230340185\n",
      "Epoch 132, Loss: 0.4759644680184978\n",
      "Epoch 133, Loss: 0.4755635187712421\n",
      "Epoch 134, Loss: 0.47538264196584307\n",
      "Epoch 135, Loss: 0.4753673450596914\n",
      "Epoch 136, Loss: 0.4753697744168057\n",
      "Epoch 137, Loss: 0.4755973109583155\n",
      "Epoch 138, Loss: 0.4767626826605241\n",
      "Epoch 139, Loss: 0.47683528222287225\n",
      "Epoch 140, Loss: 0.47579445468261566\n",
      "Epoch 141, Loss: 0.47284533919249566\n",
      "Epoch 142, Loss: 0.472809286012082\n",
      "Epoch 143, Loss: 0.47458738111232796\n",
      "Epoch 144, Loss: 0.4733380058918715\n",
      "Epoch 145, Loss: 0.4715229034484008\n",
      "Epoch 146, Loss: 0.47096730966267386\n",
      "Epoch 147, Loss: 0.47161469890467383\n",
      "Epoch 148, Loss: 0.47183531411129226\n",
      "Epoch 149, Loss: 0.470294933688076\n",
      "Epoch 150, Loss: 0.4697114144973765\n",
      "Epoch 151, Loss: 0.47018789053701304\n",
      "Epoch 152, Loss: 0.4700460953124117\n",
      "Epoch 153, Loss: 0.4693645740016508\n",
      "Epoch 154, Loss: 0.4684558104180481\n",
      "Epoch 155, Loss: 0.4680960384979934\n",
      "Epoch 156, Loss: 0.4682454254529793\n",
      "Epoch 157, Loss: 0.4682974379105488\n",
      "Epoch 158, Loss: 0.4681590010361841\n",
      "Epoch 159, Loss: 0.4673225124817593\n",
      "Epoch 160, Loss: 0.4666701428107949\n",
      "Epoch 161, Loss: 0.4663166784414499\n",
      "Epoch 162, Loss: 0.46624854535037\n",
      "Epoch 163, Loss: 0.4664611566834947\n",
      "Epoch 164, Loss: 0.46654532450127006\n",
      "Epoch 165, Loss: 0.46721845607201035\n",
      "Epoch 166, Loss: 0.4668208160006968\n",
      "Epoch 167, Loss: 0.466573749669782\n",
      "Epoch 168, Loss: 0.46506387643568103\n",
      "Epoch 169, Loss: 0.4641220516950567\n",
      "Epoch 170, Loss: 0.4644550172656552\n",
      "Epoch 171, Loss: 0.4654613047116713\n",
      "Epoch 172, Loss: 0.46880176576246846\n",
      "Epoch 173, Loss: 0.4685649824581709\n",
      "Epoch 174, Loss: 0.46606115996454733\n",
      "Epoch 175, Loss: 0.46328263242364126\n",
      "Epoch 176, Loss: 0.4660484187313888\n",
      "Epoch 177, Loss: 0.4667669789465077\n",
      "Epoch 178, Loss: 0.46222975993575777\n",
      "Epoch 179, Loss: 0.46412214801342555\n",
      "Epoch 180, Loss: 0.46867384291927394\n",
      "Epoch 181, Loss: 0.46316269230838986\n",
      "Epoch 182, Loss: 0.4619791093960296\n",
      "Epoch 183, Loss: 0.4650783107301587\n",
      "Epoch 184, Loss: 0.4617210775403838\n",
      "Epoch 185, Loss: 0.4621011464022392\n",
      "Epoch 186, Loss: 0.4641367239628427\n",
      "Epoch 187, Loss: 0.46071131054180564\n",
      "Epoch 188, Loss: 0.46145892957673257\n",
      "Epoch 189, Loss: 0.464304908222076\n",
      "Epoch 190, Loss: 0.4606605441278432\n",
      "Epoch 191, Loss: 0.4601386308130386\n",
      "Epoch 192, Loss: 0.46193430495315263\n",
      "Epoch 193, Loss: 0.45977696912799226\n",
      "Epoch 194, Loss: 0.4597173803227511\n",
      "Epoch 195, Loss: 0.46096589922299847\n",
      "Epoch 196, Loss: 0.4592170612502741\n",
      "Epoch 197, Loss: 0.45864347419013624\n",
      "Epoch 198, Loss: 0.4594650511157848\n",
      "Epoch 199, Loss: 0.45924705942486493\n",
      "Epoch 200, Loss: 0.4584931199827434\n",
      "Epoch 201, Loss: 0.4578552480015059\n",
      "Epoch 202, Loss: 0.458251953125\n",
      "Epoch 203, Loss: 0.4586848316367108\n",
      "Epoch 204, Loss: 0.4577025349799131\n",
      "Epoch 205, Loss: 0.45707273702530193\n",
      "Epoch 206, Loss: 0.4571336158814618\n",
      "Epoch 207, Loss: 0.45746758002181026\n",
      "Epoch 208, Loss: 0.4580334166179955\n",
      "Epoch 209, Loss: 0.457551138813026\n",
      "Epoch 210, Loss: 0.4572461918817771\n",
      "Epoch 211, Loss: 0.45629613394883295\n",
      "Epoch 212, Loss: 0.4557945440718634\n",
      "Epoch 213, Loss: 0.455843155547024\n",
      "Epoch 214, Loss: 0.456219547378948\n",
      "Epoch 215, Loss: 0.4577116995236092\n",
      "Epoch 216, Loss: 0.4589176697652806\n",
      "Epoch 217, Loss: 0.46105034303609993\n",
      "Epoch 218, Loss: 0.4567326442394303\n",
      "Epoch 219, Loss: 0.45493112899589366\n",
      "Epoch 220, Loss: 0.45527199475335134\n",
      "Epoch 221, Loss: 0.45623630281032773\n",
      "Epoch 222, Loss: 0.45802054967026307\n",
      "Epoch 223, Loss: 0.45677224995978094\n",
      "Epoch 224, Loss: 0.45565914342704716\n",
      "Epoch 225, Loss: 0.45401674524485375\n",
      "Epoch 226, Loss: 0.4535057342791329\n",
      "Epoch 227, Loss: 0.45416164205845616\n",
      "Epoch 228, Loss: 0.45450513400796655\n",
      "Epoch 229, Loss: 0.4548238117351997\n",
      "Epoch 230, Loss: 0.4537990916311975\n",
      "Epoch 231, Loss: 0.4532521496977959\n",
      "Epoch 232, Loss: 0.4526084553640119\n",
      "Epoch 233, Loss: 0.45228981460773116\n",
      "Epoch 234, Loss: 0.4520949132230242\n",
      "Epoch 235, Loss: 0.45205797975469736\n",
      "Epoch 236, Loss: 0.45230557901617613\n",
      "Epoch 237, Loss: 0.4528345953247846\n",
      "Epoch 238, Loss: 0.4549386133976144\n",
      "Epoch 239, Loss: 0.4564022887834084\n",
      "Epoch 240, Loss: 0.45805755540321125\n",
      "Epoch 241, Loss: 0.4541498302115434\n",
      "Epoch 242, Loss: 0.45158727109343993\n",
      "Epoch 243, Loss: 0.45105347173746146\n",
      "Epoch 244, Loss: 0.45263971453254503\n",
      "Epoch 245, Loss: 0.4550743757756552\n",
      "Epoch 246, Loss: 0.4549471294161167\n",
      "Epoch 247, Loss: 0.45361945699341477\n",
      "Epoch 248, Loss: 0.4506913801287277\n",
      "Epoch 249, Loss: 0.44996455105922556\n",
      "Epoch 250, Loss: 0.45070657227233163\n",
      "Epoch 251, Loss: 0.45134097806864715\n",
      "Epoch 252, Loss: 0.4518641160838718\n",
      "Epoch 253, Loss: 0.45124870735997746\n",
      "Epoch 254, Loss: 0.4507253179113287\n",
      "Epoch 255, Loss: 0.44965336548788515\n",
      "Epoch 256, Loss: 0.4490457284512466\n",
      "Epoch 257, Loss: 0.44867439459283065\n",
      "Epoch 258, Loss: 0.4485724236213288\n",
      "Epoch 259, Loss: 0.4487178165615502\n",
      "Epoch 260, Loss: 0.44913423821312026\n",
      "Epoch 261, Loss: 0.45068397397968624\n",
      "Epoch 262, Loss: 0.4523160224282965\n",
      "Epoch 263, Loss: 0.4554805695192658\n",
      "Epoch 264, Loss: 0.4521284819909996\n",
      "Epoch 265, Loss: 0.4491460824324\n",
      "Epoch 266, Loss: 0.4475917876267889\n",
      "Epoch 267, Loss: 0.4489836201844808\n",
      "Epoch 268, Loss: 0.4522696841254308\n",
      "Epoch 269, Loss: 0.45230492011773493\n",
      "Epoch 270, Loss: 0.45030259118174343\n",
      "Epoch 271, Loss: 0.44750189058874124\n",
      "Epoch 272, Loss: 0.4466950387656147\n",
      "Epoch 273, Loss: 0.44782488644087964\n",
      "Epoch 274, Loss: 0.4490013591686314\n",
      "Epoch 275, Loss: 0.44932258069431275\n",
      "Epoch 276, Loss: 0.4477355520631983\n",
      "Epoch 277, Loss: 0.4464409066948383\n",
      "Epoch 278, Loss: 0.4458417591594104\n",
      "Epoch 279, Loss: 0.4457414301360274\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2355669813989774\n",
      "Valence RMSE: 0.24598435943471608\n",
      "Arousal RMSE: 0.22466708785299355\n",
      "Test R^2 score: tensor([0.3534, 0.6277], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4905783301263843\n",
      "Num of epochs: 280\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Epoch 121, Loss: 0.4816228295571127\n",
      "Epoch 122, Loss: 0.4807618495484584\n",
      "Epoch 123, Loss: 0.48102030846138955\n",
      "Epoch 124, Loss: 0.4810176288364116\n",
      "Epoch 125, Loss: 0.48014338261661127\n",
      "Epoch 126, Loss: 0.47910314809270116\n",
      "Epoch 127, Loss: 0.47880795949683447\n",
      "Epoch 128, Loss: 0.47876177315142754\n",
      "Epoch 129, Loss: 0.47819597116449997\n",
      "Epoch 130, Loss: 0.47743214539542544\n",
      "Epoch 131, Loss: 0.47658476230340185\n",
      "Epoch 132, Loss: 0.4759644680184978\n",
      "Epoch 133, Loss: 0.4755635187712421\n",
      "Epoch 134, Loss: 0.47538264196584307\n",
      "Epoch 135, Loss: 0.4753673450596914\n",
      "Epoch 136, Loss: 0.4753697744168057\n",
      "Epoch 137, Loss: 0.4755973109583155\n",
      "Epoch 138, Loss: 0.4767626826605241\n",
      "Epoch 139, Loss: 0.47683528222287225\n",
      "Epoch 140, Loss: 0.47579445468261566\n",
      "Epoch 141, Loss: 0.47284533919249566\n",
      "Epoch 142, Loss: 0.472809286012082\n",
      "Epoch 143, Loss: 0.47458738111232796\n",
      "Epoch 144, Loss: 0.4733380058918715\n",
      "Epoch 145, Loss: 0.4715229034484008\n",
      "Epoch 146, Loss: 0.47096730966267386\n",
      "Epoch 147, Loss: 0.47161469890467383\n",
      "Epoch 148, Loss: 0.47183531411129226\n",
      "Epoch 149, Loss: 0.470294933688076\n",
      "Epoch 150, Loss: 0.4697114144973765\n",
      "Epoch 151, Loss: 0.47018789053701304\n",
      "Epoch 152, Loss: 0.4700460953124117\n",
      "Epoch 153, Loss: 0.4693645740016508\n",
      "Epoch 154, Loss: 0.4684558104180481\n",
      "Epoch 155, Loss: 0.4680960384979934\n",
      "Epoch 156, Loss: 0.4682454254529793\n",
      "Epoch 157, Loss: 0.4682974379105488\n",
      "Epoch 158, Loss: 0.4681590010361841\n",
      "Epoch 159, Loss: 0.4673225124817593\n",
      "Epoch 160, Loss: 0.4666701428107949\n",
      "Epoch 161, Loss: 0.4663166784414499\n",
      "Epoch 162, Loss: 0.46624854535037\n",
      "Epoch 163, Loss: 0.4664611566834947\n",
      "Epoch 164, Loss: 0.46654532450127006\n",
      "Epoch 165, Loss: 0.46721845607201035\n",
      "Epoch 166, Loss: 0.4668208160006968\n",
      "Epoch 167, Loss: 0.466573749669782\n",
      "Epoch 168, Loss: 0.46506387643568103\n",
      "Epoch 169, Loss: 0.4641220516950567\n",
      "Epoch 170, Loss: 0.4644550172656552\n",
      "Epoch 171, Loss: 0.4654613047116713\n",
      "Epoch 172, Loss: 0.46880176576246846\n",
      "Epoch 173, Loss: 0.4685649824581709\n",
      "Epoch 174, Loss: 0.46606115996454733\n",
      "Epoch 175, Loss: 0.46328263242364126\n",
      "Epoch 176, Loss: 0.4660484187313888\n",
      "Epoch 177, Loss: 0.4667669789465077\n",
      "Epoch 178, Loss: 0.46222975993575777\n",
      "Epoch 179, Loss: 0.46412214801342555\n",
      "Epoch 180, Loss: 0.46867384291927394\n",
      "Epoch 181, Loss: 0.46316269230838986\n",
      "Epoch 182, Loss: 0.4619791093960296\n",
      "Epoch 183, Loss: 0.4650783107301587\n",
      "Epoch 184, Loss: 0.4617210775403838\n",
      "Epoch 185, Loss: 0.4621011464022392\n",
      "Epoch 186, Loss: 0.4641367239628427\n",
      "Epoch 187, Loss: 0.46071131054180564\n",
      "Epoch 188, Loss: 0.46145892957673257\n",
      "Epoch 189, Loss: 0.464304908222076\n",
      "Epoch 190, Loss: 0.4606605441278432\n",
      "Epoch 191, Loss: 0.4601386308130386\n",
      "Epoch 192, Loss: 0.46193430495315263\n",
      "Epoch 193, Loss: 0.45977696912799226\n",
      "Epoch 194, Loss: 0.4597173803227511\n",
      "Epoch 195, Loss: 0.46096589922299847\n",
      "Epoch 196, Loss: 0.4592170612502741\n",
      "Epoch 197, Loss: 0.45864347419013624\n",
      "Epoch 198, Loss: 0.4594650511157848\n",
      "Epoch 199, Loss: 0.45924705942486493\n",
      "Epoch 200, Loss: 0.4584931199827434\n",
      "Epoch 201, Loss: 0.4578552480015059\n",
      "Epoch 202, Loss: 0.458251953125\n",
      "Epoch 203, Loss: 0.4586848316367108\n",
      "Epoch 204, Loss: 0.4577025349799131\n",
      "Epoch 205, Loss: 0.45707273702530193\n",
      "Epoch 206, Loss: 0.4571336158814618\n",
      "Epoch 207, Loss: 0.45746758002181026\n",
      "Epoch 208, Loss: 0.4580334166179955\n",
      "Epoch 209, Loss: 0.457551138813026\n",
      "Epoch 210, Loss: 0.4572461918817771\n",
      "Epoch 211, Loss: 0.45629613394883295\n",
      "Epoch 212, Loss: 0.4557945440718634\n",
      "Epoch 213, Loss: 0.455843155547024\n",
      "Epoch 214, Loss: 0.456219547378948\n",
      "Epoch 215, Loss: 0.4577116995236092\n",
      "Epoch 216, Loss: 0.4589176697652806\n",
      "Epoch 217, Loss: 0.46105034303609993\n",
      "Epoch 218, Loss: 0.4567326442394303\n",
      "Epoch 219, Loss: 0.45493112899589366\n",
      "Epoch 220, Loss: 0.45527199475335134\n",
      "Epoch 221, Loss: 0.45623630281032773\n",
      "Epoch 222, Loss: 0.45802054967026307\n",
      "Epoch 223, Loss: 0.45677224995978094\n",
      "Epoch 224, Loss: 0.45565914342704716\n",
      "Epoch 225, Loss: 0.45401674524485375\n",
      "Epoch 226, Loss: 0.4535057342791329\n",
      "Epoch 227, Loss: 0.45416164205845616\n",
      "Epoch 228, Loss: 0.45450513400796655\n",
      "Epoch 229, Loss: 0.4548238117351997\n",
      "Epoch 230, Loss: 0.4537990916311975\n",
      "Epoch 231, Loss: 0.4532521496977959\n",
      "Epoch 232, Loss: 0.4526084553640119\n",
      "Epoch 233, Loss: 0.45228981460773116\n",
      "Epoch 234, Loss: 0.4520949132230242\n",
      "Epoch 235, Loss: 0.45205797975469736\n",
      "Epoch 236, Loss: 0.45230557901617613\n",
      "Epoch 237, Loss: 0.4528345953247846\n",
      "Epoch 238, Loss: 0.4549386133976144\n",
      "Epoch 239, Loss: 0.4564022887834084\n",
      "Epoch 240, Loss: 0.45805755540321125\n",
      "Epoch 241, Loss: 0.4541498302115434\n",
      "Epoch 242, Loss: 0.45158727109343993\n",
      "Epoch 243, Loss: 0.45105347173746146\n",
      "Epoch 244, Loss: 0.45263971453254503\n",
      "Epoch 245, Loss: 0.4550743757756552\n",
      "Epoch 246, Loss: 0.4549471294161167\n",
      "Epoch 247, Loss: 0.45361945699341477\n",
      "Epoch 248, Loss: 0.4506913801287277\n",
      "Epoch 249, Loss: 0.44996455105922556\n",
      "Epoch 250, Loss: 0.45070657227233163\n",
      "Epoch 251, Loss: 0.45134097806864715\n",
      "Epoch 252, Loss: 0.4518641160838718\n",
      "Epoch 253, Loss: 0.45124870735997746\n",
      "Epoch 254, Loss: 0.4507253179113287\n",
      "Epoch 255, Loss: 0.44965336548788515\n",
      "Epoch 256, Loss: 0.4490457284512466\n",
      "Epoch 257, Loss: 0.44867439459283065\n",
      "Epoch 258, Loss: 0.4485724236213288\n",
      "Epoch 259, Loss: 0.4487178165615502\n",
      "Epoch 260, Loss: 0.44913423821312026\n",
      "Epoch 261, Loss: 0.45068397397968624\n",
      "Epoch 262, Loss: 0.4523160224282965\n",
      "Epoch 263, Loss: 0.4554805695192658\n",
      "Epoch 264, Loss: 0.4521284819909996\n",
      "Epoch 265, Loss: 0.4491460824324\n",
      "Epoch 266, Loss: 0.4475917876267889\n",
      "Epoch 267, Loss: 0.4489836201844808\n",
      "Epoch 268, Loss: 0.4522696841254308\n",
      "Epoch 269, Loss: 0.45230492011773493\n",
      "Epoch 270, Loss: 0.45030259118174343\n",
      "Epoch 271, Loss: 0.44750189058874124\n",
      "Epoch 272, Loss: 0.4466950387656147\n",
      "Epoch 273, Loss: 0.44782488644087964\n",
      "Epoch 274, Loss: 0.4490013591686314\n",
      "Epoch 275, Loss: 0.44932258069431275\n",
      "Epoch 276, Loss: 0.4477355520631983\n",
      "Epoch 277, Loss: 0.4464409066948383\n",
      "Epoch 278, Loss: 0.4458417591594104\n",
      "Epoch 279, Loss: 0.4457414301360274\n",
      "Epoch 280, Loss: 0.4460887501130773\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23527502676469766\n",
      "Valence RMSE: 0.24344628635545354\n",
      "Arousal RMSE: 0.22680957232444335\n",
      "Test R^2 score: tensor([0.3667, 0.6206], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4936486134942685\n",
      "Num of epochs: 281\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Epoch 121, Loss: 0.4816228295571127\n",
      "Epoch 122, Loss: 0.4807618495484584\n",
      "Epoch 123, Loss: 0.48102030846138955\n",
      "Epoch 124, Loss: 0.4810176288364116\n",
      "Epoch 125, Loss: 0.48014338261661127\n",
      "Epoch 126, Loss: 0.47910314809270116\n",
      "Epoch 127, Loss: 0.47880795949683447\n",
      "Epoch 128, Loss: 0.47876177315142754\n",
      "Epoch 129, Loss: 0.47819597116449997\n",
      "Epoch 130, Loss: 0.47743214539542544\n",
      "Epoch 131, Loss: 0.47658476230340185\n",
      "Epoch 132, Loss: 0.4759644680184978\n",
      "Epoch 133, Loss: 0.4755635187712421\n",
      "Epoch 134, Loss: 0.47538264196584307\n",
      "Epoch 135, Loss: 0.4753673450596914\n",
      "Epoch 136, Loss: 0.4753697744168057\n",
      "Epoch 137, Loss: 0.4755973109583155\n",
      "Epoch 138, Loss: 0.4767626826605241\n",
      "Epoch 139, Loss: 0.47683528222287225\n",
      "Epoch 140, Loss: 0.47579445468261566\n",
      "Epoch 141, Loss: 0.47284533919249566\n",
      "Epoch 142, Loss: 0.472809286012082\n",
      "Epoch 143, Loss: 0.47458738111232796\n",
      "Epoch 144, Loss: 0.4733380058918715\n",
      "Epoch 145, Loss: 0.4715229034484008\n",
      "Epoch 146, Loss: 0.47096730966267386\n",
      "Epoch 147, Loss: 0.47161469890467383\n",
      "Epoch 148, Loss: 0.47183531411129226\n",
      "Epoch 149, Loss: 0.470294933688076\n",
      "Epoch 150, Loss: 0.4697114144973765\n",
      "Epoch 151, Loss: 0.47018789053701304\n",
      "Epoch 152, Loss: 0.4700460953124117\n",
      "Epoch 153, Loss: 0.4693645740016508\n",
      "Epoch 154, Loss: 0.4684558104180481\n",
      "Epoch 155, Loss: 0.4680960384979934\n",
      "Epoch 156, Loss: 0.4682454254529793\n",
      "Epoch 157, Loss: 0.4682974379105488\n",
      "Epoch 158, Loss: 0.4681590010361841\n",
      "Epoch 159, Loss: 0.4673225124817593\n",
      "Epoch 160, Loss: 0.4666701428107949\n",
      "Epoch 161, Loss: 0.4663166784414499\n",
      "Epoch 162, Loss: 0.46624854535037\n",
      "Epoch 163, Loss: 0.4664611566834947\n",
      "Epoch 164, Loss: 0.46654532450127006\n",
      "Epoch 165, Loss: 0.46721845607201035\n",
      "Epoch 166, Loss: 0.4668208160006968\n",
      "Epoch 167, Loss: 0.466573749669782\n",
      "Epoch 168, Loss: 0.46506387643568103\n",
      "Epoch 169, Loss: 0.4641220516950567\n",
      "Epoch 170, Loss: 0.4644550172656552\n",
      "Epoch 171, Loss: 0.4654613047116713\n",
      "Epoch 172, Loss: 0.46880176576246846\n",
      "Epoch 173, Loss: 0.4685649824581709\n",
      "Epoch 174, Loss: 0.46606115996454733\n",
      "Epoch 175, Loss: 0.46328263242364126\n",
      "Epoch 176, Loss: 0.4660484187313888\n",
      "Epoch 177, Loss: 0.4667669789465077\n",
      "Epoch 178, Loss: 0.46222975993575777\n",
      "Epoch 179, Loss: 0.46412214801342555\n",
      "Epoch 180, Loss: 0.46867384291927394\n",
      "Epoch 181, Loss: 0.46316269230838986\n",
      "Epoch 182, Loss: 0.4619791093960296\n",
      "Epoch 183, Loss: 0.4650783107301587\n",
      "Epoch 184, Loss: 0.4617210775403838\n",
      "Epoch 185, Loss: 0.4621011464022392\n",
      "Epoch 186, Loss: 0.4641367239628427\n",
      "Epoch 187, Loss: 0.46071131054180564\n",
      "Epoch 188, Loss: 0.46145892957673257\n",
      "Epoch 189, Loss: 0.464304908222076\n",
      "Epoch 190, Loss: 0.4606605441278432\n",
      "Epoch 191, Loss: 0.4601386308130386\n",
      "Epoch 192, Loss: 0.46193430495315263\n",
      "Epoch 193, Loss: 0.45977696912799226\n",
      "Epoch 194, Loss: 0.4597173803227511\n",
      "Epoch 195, Loss: 0.46096589922299847\n",
      "Epoch 196, Loss: 0.4592170612502741\n",
      "Epoch 197, Loss: 0.45864347419013624\n",
      "Epoch 198, Loss: 0.4594650511157848\n",
      "Epoch 199, Loss: 0.45924705942486493\n",
      "Epoch 200, Loss: 0.4584931199827434\n",
      "Epoch 201, Loss: 0.4578552480015059\n",
      "Epoch 202, Loss: 0.458251953125\n",
      "Epoch 203, Loss: 0.4586848316367108\n",
      "Epoch 204, Loss: 0.4577025349799131\n",
      "Epoch 205, Loss: 0.45707273702530193\n",
      "Epoch 206, Loss: 0.4571336158814618\n",
      "Epoch 207, Loss: 0.45746758002181026\n",
      "Epoch 208, Loss: 0.4580334166179955\n",
      "Epoch 209, Loss: 0.457551138813026\n",
      "Epoch 210, Loss: 0.4572461918817771\n",
      "Epoch 211, Loss: 0.45629613394883295\n",
      "Epoch 212, Loss: 0.4557945440718634\n",
      "Epoch 213, Loss: 0.455843155547024\n",
      "Epoch 214, Loss: 0.456219547378948\n",
      "Epoch 215, Loss: 0.4577116995236092\n",
      "Epoch 216, Loss: 0.4589176697652806\n",
      "Epoch 217, Loss: 0.46105034303609993\n",
      "Epoch 218, Loss: 0.4567326442394303\n",
      "Epoch 219, Loss: 0.45493112899589366\n",
      "Epoch 220, Loss: 0.45527199475335134\n",
      "Epoch 221, Loss: 0.45623630281032773\n",
      "Epoch 222, Loss: 0.45802054967026307\n",
      "Epoch 223, Loss: 0.45677224995978094\n",
      "Epoch 224, Loss: 0.45565914342704716\n",
      "Epoch 225, Loss: 0.45401674524485375\n",
      "Epoch 226, Loss: 0.4535057342791329\n",
      "Epoch 227, Loss: 0.45416164205845616\n",
      "Epoch 228, Loss: 0.45450513400796655\n",
      "Epoch 229, Loss: 0.4548238117351997\n",
      "Epoch 230, Loss: 0.4537990916311975\n",
      "Epoch 231, Loss: 0.4532521496977959\n",
      "Epoch 232, Loss: 0.4526084553640119\n",
      "Epoch 233, Loss: 0.45228981460773116\n",
      "Epoch 234, Loss: 0.4520949132230242\n",
      "Epoch 235, Loss: 0.45205797975469736\n",
      "Epoch 236, Loss: 0.45230557901617613\n",
      "Epoch 237, Loss: 0.4528345953247846\n",
      "Epoch 238, Loss: 0.4549386133976144\n",
      "Epoch 239, Loss: 0.4564022887834084\n",
      "Epoch 240, Loss: 0.45805755540321125\n",
      "Epoch 241, Loss: 0.4541498302115434\n",
      "Epoch 242, Loss: 0.45158727109343993\n",
      "Epoch 243, Loss: 0.45105347173746146\n",
      "Epoch 244, Loss: 0.45263971453254503\n",
      "Epoch 245, Loss: 0.4550743757756552\n",
      "Epoch 246, Loss: 0.4549471294161167\n",
      "Epoch 247, Loss: 0.45361945699341477\n",
      "Epoch 248, Loss: 0.4506913801287277\n",
      "Epoch 249, Loss: 0.44996455105922556\n",
      "Epoch 250, Loss: 0.45070657227233163\n",
      "Epoch 251, Loss: 0.45134097806864715\n",
      "Epoch 252, Loss: 0.4518641160838718\n",
      "Epoch 253, Loss: 0.45124870735997746\n",
      "Epoch 254, Loss: 0.4507253179113287\n",
      "Epoch 255, Loss: 0.44965336548788515\n",
      "Epoch 256, Loss: 0.4490457284512466\n",
      "Epoch 257, Loss: 0.44867439459283065\n",
      "Epoch 258, Loss: 0.4485724236213288\n",
      "Epoch 259, Loss: 0.4487178165615502\n",
      "Epoch 260, Loss: 0.44913423821312026\n",
      "Epoch 261, Loss: 0.45068397397968624\n",
      "Epoch 262, Loss: 0.4523160224282965\n",
      "Epoch 263, Loss: 0.4554805695192658\n",
      "Epoch 264, Loss: 0.4521284819909996\n",
      "Epoch 265, Loss: 0.4491460824324\n",
      "Epoch 266, Loss: 0.4475917876267889\n",
      "Epoch 267, Loss: 0.4489836201844808\n",
      "Epoch 268, Loss: 0.4522696841254308\n",
      "Epoch 269, Loss: 0.45230492011773493\n",
      "Epoch 270, Loss: 0.45030259118174343\n",
      "Epoch 271, Loss: 0.44750189058874124\n",
      "Epoch 272, Loss: 0.4466950387656147\n",
      "Epoch 273, Loss: 0.44782488644087964\n",
      "Epoch 274, Loss: 0.4490013591686314\n",
      "Epoch 275, Loss: 0.44932258069431275\n",
      "Epoch 276, Loss: 0.4477355520631983\n",
      "Epoch 277, Loss: 0.4464409066948383\n",
      "Epoch 278, Loss: 0.4458417591594104\n",
      "Epoch 279, Loss: 0.4457414301360274\n",
      "Epoch 280, Loss: 0.4460887501130773\n",
      "Epoch 281, Loss: 0.44690611604251484\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23720583677525983\n",
      "Valence RMSE: 0.2488635590661654\n",
      "Arousal RMSE: 0.22494476426319335\n",
      "Test R^2 score: tensor([0.3382, 0.6268], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.48250545607670453\n",
      "Num of epochs: 282\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Epoch 121, Loss: 0.4816228295571127\n",
      "Epoch 122, Loss: 0.4807618495484584\n",
      "Epoch 123, Loss: 0.48102030846138955\n",
      "Epoch 124, Loss: 0.4810176288364116\n",
      "Epoch 125, Loss: 0.48014338261661127\n",
      "Epoch 126, Loss: 0.47910314809270116\n",
      "Epoch 127, Loss: 0.47880795949683447\n",
      "Epoch 128, Loss: 0.47876177315142754\n",
      "Epoch 129, Loss: 0.47819597116449997\n",
      "Epoch 130, Loss: 0.47743214539542544\n",
      "Epoch 131, Loss: 0.47658476230340185\n",
      "Epoch 132, Loss: 0.4759644680184978\n",
      "Epoch 133, Loss: 0.4755635187712421\n",
      "Epoch 134, Loss: 0.47538264196584307\n",
      "Epoch 135, Loss: 0.4753673450596914\n",
      "Epoch 136, Loss: 0.4753697744168057\n",
      "Epoch 137, Loss: 0.4755973109583155\n",
      "Epoch 138, Loss: 0.4767626826605241\n",
      "Epoch 139, Loss: 0.47683528222287225\n",
      "Epoch 140, Loss: 0.47579445468261566\n",
      "Epoch 141, Loss: 0.47284533919249566\n",
      "Epoch 142, Loss: 0.472809286012082\n",
      "Epoch 143, Loss: 0.47458738111232796\n",
      "Epoch 144, Loss: 0.4733380058918715\n",
      "Epoch 145, Loss: 0.4715229034484008\n",
      "Epoch 146, Loss: 0.47096730966267386\n",
      "Epoch 147, Loss: 0.47161469890467383\n",
      "Epoch 148, Loss: 0.47183531411129226\n",
      "Epoch 149, Loss: 0.470294933688076\n",
      "Epoch 150, Loss: 0.4697114144973765\n",
      "Epoch 151, Loss: 0.47018789053701304\n",
      "Epoch 152, Loss: 0.4700460953124117\n",
      "Epoch 153, Loss: 0.4693645740016508\n",
      "Epoch 154, Loss: 0.4684558104180481\n",
      "Epoch 155, Loss: 0.4680960384979934\n",
      "Epoch 156, Loss: 0.4682454254529793\n",
      "Epoch 157, Loss: 0.4682974379105488\n",
      "Epoch 158, Loss: 0.4681590010361841\n",
      "Epoch 159, Loss: 0.4673225124817593\n",
      "Epoch 160, Loss: 0.4666701428107949\n",
      "Epoch 161, Loss: 0.4663166784414499\n",
      "Epoch 162, Loss: 0.46624854535037\n",
      "Epoch 163, Loss: 0.4664611566834947\n",
      "Epoch 164, Loss: 0.46654532450127006\n",
      "Epoch 165, Loss: 0.46721845607201035\n",
      "Epoch 166, Loss: 0.4668208160006968\n",
      "Epoch 167, Loss: 0.466573749669782\n",
      "Epoch 168, Loss: 0.46506387643568103\n",
      "Epoch 169, Loss: 0.4641220516950567\n",
      "Epoch 170, Loss: 0.4644550172656552\n",
      "Epoch 171, Loss: 0.4654613047116713\n",
      "Epoch 172, Loss: 0.46880176576246846\n",
      "Epoch 173, Loss: 0.4685649824581709\n",
      "Epoch 174, Loss: 0.46606115996454733\n",
      "Epoch 175, Loss: 0.46328263242364126\n",
      "Epoch 176, Loss: 0.4660484187313888\n",
      "Epoch 177, Loss: 0.4667669789465077\n",
      "Epoch 178, Loss: 0.46222975993575777\n",
      "Epoch 179, Loss: 0.46412214801342555\n",
      "Epoch 180, Loss: 0.46867384291927394\n",
      "Epoch 181, Loss: 0.46316269230838986\n",
      "Epoch 182, Loss: 0.4619791093960296\n",
      "Epoch 183, Loss: 0.4650783107301587\n",
      "Epoch 184, Loss: 0.4617210775403838\n",
      "Epoch 185, Loss: 0.4621011464022392\n",
      "Epoch 186, Loss: 0.4641367239628427\n",
      "Epoch 187, Loss: 0.46071131054180564\n",
      "Epoch 188, Loss: 0.46145892957673257\n",
      "Epoch 189, Loss: 0.464304908222076\n",
      "Epoch 190, Loss: 0.4606605441278432\n",
      "Epoch 191, Loss: 0.4601386308130386\n",
      "Epoch 192, Loss: 0.46193430495315263\n",
      "Epoch 193, Loss: 0.45977696912799226\n",
      "Epoch 194, Loss: 0.4597173803227511\n",
      "Epoch 195, Loss: 0.46096589922299847\n",
      "Epoch 196, Loss: 0.4592170612502741\n",
      "Epoch 197, Loss: 0.45864347419013624\n",
      "Epoch 198, Loss: 0.4594650511157848\n",
      "Epoch 199, Loss: 0.45924705942486493\n",
      "Epoch 200, Loss: 0.4584931199827434\n",
      "Epoch 201, Loss: 0.4578552480015059\n",
      "Epoch 202, Loss: 0.458251953125\n",
      "Epoch 203, Loss: 0.4586848316367108\n",
      "Epoch 204, Loss: 0.4577025349799131\n",
      "Epoch 205, Loss: 0.45707273702530193\n",
      "Epoch 206, Loss: 0.4571336158814618\n",
      "Epoch 207, Loss: 0.45746758002181026\n",
      "Epoch 208, Loss: 0.4580334166179955\n",
      "Epoch 209, Loss: 0.457551138813026\n",
      "Epoch 210, Loss: 0.4572461918817771\n",
      "Epoch 211, Loss: 0.45629613394883295\n",
      "Epoch 212, Loss: 0.4557945440718634\n",
      "Epoch 213, Loss: 0.455843155547024\n",
      "Epoch 214, Loss: 0.456219547378948\n",
      "Epoch 215, Loss: 0.4577116995236092\n",
      "Epoch 216, Loss: 0.4589176697652806\n",
      "Epoch 217, Loss: 0.46105034303609993\n",
      "Epoch 218, Loss: 0.4567326442394303\n",
      "Epoch 219, Loss: 0.45493112899589366\n",
      "Epoch 220, Loss: 0.45527199475335134\n",
      "Epoch 221, Loss: 0.45623630281032773\n",
      "Epoch 222, Loss: 0.45802054967026307\n",
      "Epoch 223, Loss: 0.45677224995978094\n",
      "Epoch 224, Loss: 0.45565914342704716\n",
      "Epoch 225, Loss: 0.45401674524485375\n",
      "Epoch 226, Loss: 0.4535057342791329\n",
      "Epoch 227, Loss: 0.45416164205845616\n",
      "Epoch 228, Loss: 0.45450513400796655\n",
      "Epoch 229, Loss: 0.4548238117351997\n",
      "Epoch 230, Loss: 0.4537990916311975\n",
      "Epoch 231, Loss: 0.4532521496977959\n",
      "Epoch 232, Loss: 0.4526084553640119\n",
      "Epoch 233, Loss: 0.45228981460773116\n",
      "Epoch 234, Loss: 0.4520949132230242\n",
      "Epoch 235, Loss: 0.45205797975469736\n",
      "Epoch 236, Loss: 0.45230557901617613\n",
      "Epoch 237, Loss: 0.4528345953247846\n",
      "Epoch 238, Loss: 0.4549386133976144\n",
      "Epoch 239, Loss: 0.4564022887834084\n",
      "Epoch 240, Loss: 0.45805755540321125\n",
      "Epoch 241, Loss: 0.4541498302115434\n",
      "Epoch 242, Loss: 0.45158727109343993\n",
      "Epoch 243, Loss: 0.45105347173746146\n",
      "Epoch 244, Loss: 0.45263971453254503\n",
      "Epoch 245, Loss: 0.4550743757756552\n",
      "Epoch 246, Loss: 0.4549471294161167\n",
      "Epoch 247, Loss: 0.45361945699341477\n",
      "Epoch 248, Loss: 0.4506913801287277\n",
      "Epoch 249, Loss: 0.44996455105922556\n",
      "Epoch 250, Loss: 0.45070657227233163\n",
      "Epoch 251, Loss: 0.45134097806864715\n",
      "Epoch 252, Loss: 0.4518641160838718\n",
      "Epoch 253, Loss: 0.45124870735997746\n",
      "Epoch 254, Loss: 0.4507253179113287\n",
      "Epoch 255, Loss: 0.44965336548788515\n",
      "Epoch 256, Loss: 0.4490457284512466\n",
      "Epoch 257, Loss: 0.44867439459283065\n",
      "Epoch 258, Loss: 0.4485724236213288\n",
      "Epoch 259, Loss: 0.4487178165615502\n",
      "Epoch 260, Loss: 0.44913423821312026\n",
      "Epoch 261, Loss: 0.45068397397968624\n",
      "Epoch 262, Loss: 0.4523160224282965\n",
      "Epoch 263, Loss: 0.4554805695192658\n",
      "Epoch 264, Loss: 0.4521284819909996\n",
      "Epoch 265, Loss: 0.4491460824324\n",
      "Epoch 266, Loss: 0.4475917876267889\n",
      "Epoch 267, Loss: 0.4489836201844808\n",
      "Epoch 268, Loss: 0.4522696841254308\n",
      "Epoch 269, Loss: 0.45230492011773493\n",
      "Epoch 270, Loss: 0.45030259118174343\n",
      "Epoch 271, Loss: 0.44750189058874124\n",
      "Epoch 272, Loss: 0.4466950387656147\n",
      "Epoch 273, Loss: 0.44782488644087964\n",
      "Epoch 274, Loss: 0.4490013591686314\n",
      "Epoch 275, Loss: 0.44932258069431275\n",
      "Epoch 276, Loss: 0.4477355520631983\n",
      "Epoch 277, Loss: 0.4464409066948383\n",
      "Epoch 278, Loss: 0.4458417591594104\n",
      "Epoch 279, Loss: 0.4457414301360274\n",
      "Epoch 280, Loss: 0.4460887501130773\n",
      "Epoch 281, Loss: 0.44690611604251484\n",
      "Epoch 282, Loss: 0.4483696750030082\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23600889231409572\n",
      "Valence RMSE: 0.24306949254037205\n",
      "Arousal RMSE: 0.2287304446260245\n",
      "Test R^2 score: tensor([0.3686, 0.6142], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4914013984672911\n",
      "Num of epochs: 283\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Epoch 121, Loss: 0.4816228295571127\n",
      "Epoch 122, Loss: 0.4807618495484584\n",
      "Epoch 123, Loss: 0.48102030846138955\n",
      "Epoch 124, Loss: 0.4810176288364116\n",
      "Epoch 125, Loss: 0.48014338261661127\n",
      "Epoch 126, Loss: 0.47910314809270116\n",
      "Epoch 127, Loss: 0.47880795949683447\n",
      "Epoch 128, Loss: 0.47876177315142754\n",
      "Epoch 129, Loss: 0.47819597116449997\n",
      "Epoch 130, Loss: 0.47743214539542544\n",
      "Epoch 131, Loss: 0.47658476230340185\n",
      "Epoch 132, Loss: 0.4759644680184978\n",
      "Epoch 133, Loss: 0.4755635187712421\n",
      "Epoch 134, Loss: 0.47538264196584307\n",
      "Epoch 135, Loss: 0.4753673450596914\n",
      "Epoch 136, Loss: 0.4753697744168057\n",
      "Epoch 137, Loss: 0.4755973109583155\n",
      "Epoch 138, Loss: 0.4767626826605241\n",
      "Epoch 139, Loss: 0.47683528222287225\n",
      "Epoch 140, Loss: 0.47579445468261566\n",
      "Epoch 141, Loss: 0.47284533919249566\n",
      "Epoch 142, Loss: 0.472809286012082\n",
      "Epoch 143, Loss: 0.47458738111232796\n",
      "Epoch 144, Loss: 0.4733380058918715\n",
      "Epoch 145, Loss: 0.4715229034484008\n",
      "Epoch 146, Loss: 0.47096730966267386\n",
      "Epoch 147, Loss: 0.47161469890467383\n",
      "Epoch 148, Loss: 0.47183531411129226\n",
      "Epoch 149, Loss: 0.470294933688076\n",
      "Epoch 150, Loss: 0.4697114144973765\n",
      "Epoch 151, Loss: 0.47018789053701304\n",
      "Epoch 152, Loss: 0.4700460953124117\n",
      "Epoch 153, Loss: 0.4693645740016508\n",
      "Epoch 154, Loss: 0.4684558104180481\n",
      "Epoch 155, Loss: 0.4680960384979934\n",
      "Epoch 156, Loss: 0.4682454254529793\n",
      "Epoch 157, Loss: 0.4682974379105488\n",
      "Epoch 158, Loss: 0.4681590010361841\n",
      "Epoch 159, Loss: 0.4673225124817593\n",
      "Epoch 160, Loss: 0.4666701428107949\n",
      "Epoch 161, Loss: 0.4663166784414499\n",
      "Epoch 162, Loss: 0.46624854535037\n",
      "Epoch 163, Loss: 0.4664611566834947\n",
      "Epoch 164, Loss: 0.46654532450127006\n",
      "Epoch 165, Loss: 0.46721845607201035\n",
      "Epoch 166, Loss: 0.4668208160006968\n",
      "Epoch 167, Loss: 0.466573749669782\n",
      "Epoch 168, Loss: 0.46506387643568103\n",
      "Epoch 169, Loss: 0.4641220516950567\n",
      "Epoch 170, Loss: 0.4644550172656552\n",
      "Epoch 171, Loss: 0.4654613047116713\n",
      "Epoch 172, Loss: 0.46880176576246846\n",
      "Epoch 173, Loss: 0.4685649824581709\n",
      "Epoch 174, Loss: 0.46606115996454733\n",
      "Epoch 175, Loss: 0.46328263242364126\n",
      "Epoch 176, Loss: 0.4660484187313888\n",
      "Epoch 177, Loss: 0.4667669789465077\n",
      "Epoch 178, Loss: 0.46222975993575777\n",
      "Epoch 179, Loss: 0.46412214801342555\n",
      "Epoch 180, Loss: 0.46867384291927394\n",
      "Epoch 181, Loss: 0.46316269230838986\n",
      "Epoch 182, Loss: 0.4619791093960296\n",
      "Epoch 183, Loss: 0.4650783107301587\n",
      "Epoch 184, Loss: 0.4617210775403838\n",
      "Epoch 185, Loss: 0.4621011464022392\n",
      "Epoch 186, Loss: 0.4641367239628427\n",
      "Epoch 187, Loss: 0.46071131054180564\n",
      "Epoch 188, Loss: 0.46145892957673257\n",
      "Epoch 189, Loss: 0.464304908222076\n",
      "Epoch 190, Loss: 0.4606605441278432\n",
      "Epoch 191, Loss: 0.4601386308130386\n",
      "Epoch 192, Loss: 0.46193430495315263\n",
      "Epoch 193, Loss: 0.45977696912799226\n",
      "Epoch 194, Loss: 0.4597173803227511\n",
      "Epoch 195, Loss: 0.46096589922299847\n",
      "Epoch 196, Loss: 0.4592170612502741\n",
      "Epoch 197, Loss: 0.45864347419013624\n",
      "Epoch 198, Loss: 0.4594650511157848\n",
      "Epoch 199, Loss: 0.45924705942486493\n",
      "Epoch 200, Loss: 0.4584931199827434\n",
      "Epoch 201, Loss: 0.4578552480015059\n",
      "Epoch 202, Loss: 0.458251953125\n",
      "Epoch 203, Loss: 0.4586848316367108\n",
      "Epoch 204, Loss: 0.4577025349799131\n",
      "Epoch 205, Loss: 0.45707273702530193\n",
      "Epoch 206, Loss: 0.4571336158814618\n",
      "Epoch 207, Loss: 0.45746758002181026\n",
      "Epoch 208, Loss: 0.4580334166179955\n",
      "Epoch 209, Loss: 0.457551138813026\n",
      "Epoch 210, Loss: 0.4572461918817771\n",
      "Epoch 211, Loss: 0.45629613394883295\n",
      "Epoch 212, Loss: 0.4557945440718634\n",
      "Epoch 213, Loss: 0.455843155547024\n",
      "Epoch 214, Loss: 0.456219547378948\n",
      "Epoch 215, Loss: 0.4577116995236092\n",
      "Epoch 216, Loss: 0.4589176697652806\n",
      "Epoch 217, Loss: 0.46105034303609993\n",
      "Epoch 218, Loss: 0.4567326442394303\n",
      "Epoch 219, Loss: 0.45493112899589366\n",
      "Epoch 220, Loss: 0.45527199475335134\n",
      "Epoch 221, Loss: 0.45623630281032773\n",
      "Epoch 222, Loss: 0.45802054967026307\n",
      "Epoch 223, Loss: 0.45677224995978094\n",
      "Epoch 224, Loss: 0.45565914342704716\n",
      "Epoch 225, Loss: 0.45401674524485375\n",
      "Epoch 226, Loss: 0.4535057342791329\n",
      "Epoch 227, Loss: 0.45416164205845616\n",
      "Epoch 228, Loss: 0.45450513400796655\n",
      "Epoch 229, Loss: 0.4548238117351997\n",
      "Epoch 230, Loss: 0.4537990916311975\n",
      "Epoch 231, Loss: 0.4532521496977959\n",
      "Epoch 232, Loss: 0.4526084553640119\n",
      "Epoch 233, Loss: 0.45228981460773116\n",
      "Epoch 234, Loss: 0.4520949132230242\n",
      "Epoch 235, Loss: 0.45205797975469736\n",
      "Epoch 236, Loss: 0.45230557901617613\n",
      "Epoch 237, Loss: 0.4528345953247846\n",
      "Epoch 238, Loss: 0.4549386133976144\n",
      "Epoch 239, Loss: 0.4564022887834084\n",
      "Epoch 240, Loss: 0.45805755540321125\n",
      "Epoch 241, Loss: 0.4541498302115434\n",
      "Epoch 242, Loss: 0.45158727109343993\n",
      "Epoch 243, Loss: 0.45105347173746146\n",
      "Epoch 244, Loss: 0.45263971453254503\n",
      "Epoch 245, Loss: 0.4550743757756552\n",
      "Epoch 246, Loss: 0.4549471294161167\n",
      "Epoch 247, Loss: 0.45361945699341477\n",
      "Epoch 248, Loss: 0.4506913801287277\n",
      "Epoch 249, Loss: 0.44996455105922556\n",
      "Epoch 250, Loss: 0.45070657227233163\n",
      "Epoch 251, Loss: 0.45134097806864715\n",
      "Epoch 252, Loss: 0.4518641160838718\n",
      "Epoch 253, Loss: 0.45124870735997746\n",
      "Epoch 254, Loss: 0.4507253179113287\n",
      "Epoch 255, Loss: 0.44965336548788515\n",
      "Epoch 256, Loss: 0.4490457284512466\n",
      "Epoch 257, Loss: 0.44867439459283065\n",
      "Epoch 258, Loss: 0.4485724236213288\n",
      "Epoch 259, Loss: 0.4487178165615502\n",
      "Epoch 260, Loss: 0.44913423821312026\n",
      "Epoch 261, Loss: 0.45068397397968624\n",
      "Epoch 262, Loss: 0.4523160224282965\n",
      "Epoch 263, Loss: 0.4554805695192658\n",
      "Epoch 264, Loss: 0.4521284819909996\n",
      "Epoch 265, Loss: 0.4491460824324\n",
      "Epoch 266, Loss: 0.4475917876267889\n",
      "Epoch 267, Loss: 0.4489836201844808\n",
      "Epoch 268, Loss: 0.4522696841254308\n",
      "Epoch 269, Loss: 0.45230492011773493\n",
      "Epoch 270, Loss: 0.45030259118174343\n",
      "Epoch 271, Loss: 0.44750189058874124\n",
      "Epoch 272, Loss: 0.4466950387656147\n",
      "Epoch 273, Loss: 0.44782488644087964\n",
      "Epoch 274, Loss: 0.4490013591686314\n",
      "Epoch 275, Loss: 0.44932258069431275\n",
      "Epoch 276, Loss: 0.4477355520631983\n",
      "Epoch 277, Loss: 0.4464409066948383\n",
      "Epoch 278, Loss: 0.4458417591594104\n",
      "Epoch 279, Loss: 0.4457414301360274\n",
      "Epoch 280, Loss: 0.4460887501130773\n",
      "Epoch 281, Loss: 0.44690611604251484\n",
      "Epoch 282, Loss: 0.4483696750030082\n",
      "Epoch 283, Loss: 0.4491296928695683\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23750246396204655\n",
      "Valence RMSE: 0.2491315594019\n",
      "Arousal RMSE: 0.2252738486510674\n",
      "Test R^2 score: tensor([0.3368, 0.6257], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.48124602922059057\n",
      "Num of epochs: 284\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Epoch 121, Loss: 0.4816228295571127\n",
      "Epoch 122, Loss: 0.4807618495484584\n",
      "Epoch 123, Loss: 0.48102030846138955\n",
      "Epoch 124, Loss: 0.4810176288364116\n",
      "Epoch 125, Loss: 0.48014338261661127\n",
      "Epoch 126, Loss: 0.47910314809270116\n",
      "Epoch 127, Loss: 0.47880795949683447\n",
      "Epoch 128, Loss: 0.47876177315142754\n",
      "Epoch 129, Loss: 0.47819597116449997\n",
      "Epoch 130, Loss: 0.47743214539542544\n",
      "Epoch 131, Loss: 0.47658476230340185\n",
      "Epoch 132, Loss: 0.4759644680184978\n",
      "Epoch 133, Loss: 0.4755635187712421\n",
      "Epoch 134, Loss: 0.47538264196584307\n",
      "Epoch 135, Loss: 0.4753673450596914\n",
      "Epoch 136, Loss: 0.4753697744168057\n",
      "Epoch 137, Loss: 0.4755973109583155\n",
      "Epoch 138, Loss: 0.4767626826605241\n",
      "Epoch 139, Loss: 0.47683528222287225\n",
      "Epoch 140, Loss: 0.47579445468261566\n",
      "Epoch 141, Loss: 0.47284533919249566\n",
      "Epoch 142, Loss: 0.472809286012082\n",
      "Epoch 143, Loss: 0.47458738111232796\n",
      "Epoch 144, Loss: 0.4733380058918715\n",
      "Epoch 145, Loss: 0.4715229034484008\n",
      "Epoch 146, Loss: 0.47096730966267386\n",
      "Epoch 147, Loss: 0.47161469890467383\n",
      "Epoch 148, Loss: 0.47183531411129226\n",
      "Epoch 149, Loss: 0.470294933688076\n",
      "Epoch 150, Loss: 0.4697114144973765\n",
      "Epoch 151, Loss: 0.47018789053701304\n",
      "Epoch 152, Loss: 0.4700460953124117\n",
      "Epoch 153, Loss: 0.4693645740016508\n",
      "Epoch 154, Loss: 0.4684558104180481\n",
      "Epoch 155, Loss: 0.4680960384979934\n",
      "Epoch 156, Loss: 0.4682454254529793\n",
      "Epoch 157, Loss: 0.4682974379105488\n",
      "Epoch 158, Loss: 0.4681590010361841\n",
      "Epoch 159, Loss: 0.4673225124817593\n",
      "Epoch 160, Loss: 0.4666701428107949\n",
      "Epoch 161, Loss: 0.4663166784414499\n",
      "Epoch 162, Loss: 0.46624854535037\n",
      "Epoch 163, Loss: 0.4664611566834947\n",
      "Epoch 164, Loss: 0.46654532450127006\n",
      "Epoch 165, Loss: 0.46721845607201035\n",
      "Epoch 166, Loss: 0.4668208160006968\n",
      "Epoch 167, Loss: 0.466573749669782\n",
      "Epoch 168, Loss: 0.46506387643568103\n",
      "Epoch 169, Loss: 0.4641220516950567\n",
      "Epoch 170, Loss: 0.4644550172656552\n",
      "Epoch 171, Loss: 0.4654613047116713\n",
      "Epoch 172, Loss: 0.46880176576246846\n",
      "Epoch 173, Loss: 0.4685649824581709\n",
      "Epoch 174, Loss: 0.46606115996454733\n",
      "Epoch 175, Loss: 0.46328263242364126\n",
      "Epoch 176, Loss: 0.4660484187313888\n",
      "Epoch 177, Loss: 0.4667669789465077\n",
      "Epoch 178, Loss: 0.46222975993575777\n",
      "Epoch 179, Loss: 0.46412214801342555\n",
      "Epoch 180, Loss: 0.46867384291927394\n",
      "Epoch 181, Loss: 0.46316269230838986\n",
      "Epoch 182, Loss: 0.4619791093960296\n",
      "Epoch 183, Loss: 0.4650783107301587\n",
      "Epoch 184, Loss: 0.4617210775403838\n",
      "Epoch 185, Loss: 0.4621011464022392\n",
      "Epoch 186, Loss: 0.4641367239628427\n",
      "Epoch 187, Loss: 0.46071131054180564\n",
      "Epoch 188, Loss: 0.46145892957673257\n",
      "Epoch 189, Loss: 0.464304908222076\n",
      "Epoch 190, Loss: 0.4606605441278432\n",
      "Epoch 191, Loss: 0.4601386308130386\n",
      "Epoch 192, Loss: 0.46193430495315263\n",
      "Epoch 193, Loss: 0.45977696912799226\n",
      "Epoch 194, Loss: 0.4597173803227511\n",
      "Epoch 195, Loss: 0.46096589922299847\n",
      "Epoch 196, Loss: 0.4592170612502741\n",
      "Epoch 197, Loss: 0.45864347419013624\n",
      "Epoch 198, Loss: 0.4594650511157848\n",
      "Epoch 199, Loss: 0.45924705942486493\n",
      "Epoch 200, Loss: 0.4584931199827434\n",
      "Epoch 201, Loss: 0.4578552480015059\n",
      "Epoch 202, Loss: 0.458251953125\n",
      "Epoch 203, Loss: 0.4586848316367108\n",
      "Epoch 204, Loss: 0.4577025349799131\n",
      "Epoch 205, Loss: 0.45707273702530193\n",
      "Epoch 206, Loss: 0.4571336158814618\n",
      "Epoch 207, Loss: 0.45746758002181026\n",
      "Epoch 208, Loss: 0.4580334166179955\n",
      "Epoch 209, Loss: 0.457551138813026\n",
      "Epoch 210, Loss: 0.4572461918817771\n",
      "Epoch 211, Loss: 0.45629613394883295\n",
      "Epoch 212, Loss: 0.4557945440718634\n",
      "Epoch 213, Loss: 0.455843155547024\n",
      "Epoch 214, Loss: 0.456219547378948\n",
      "Epoch 215, Loss: 0.4577116995236092\n",
      "Epoch 216, Loss: 0.4589176697652806\n",
      "Epoch 217, Loss: 0.46105034303609993\n",
      "Epoch 218, Loss: 0.4567326442394303\n",
      "Epoch 219, Loss: 0.45493112899589366\n",
      "Epoch 220, Loss: 0.45527199475335134\n",
      "Epoch 221, Loss: 0.45623630281032773\n",
      "Epoch 222, Loss: 0.45802054967026307\n",
      "Epoch 223, Loss: 0.45677224995978094\n",
      "Epoch 224, Loss: 0.45565914342704716\n",
      "Epoch 225, Loss: 0.45401674524485375\n",
      "Epoch 226, Loss: 0.4535057342791329\n",
      "Epoch 227, Loss: 0.45416164205845616\n",
      "Epoch 228, Loss: 0.45450513400796655\n",
      "Epoch 229, Loss: 0.4548238117351997\n",
      "Epoch 230, Loss: 0.4537990916311975\n",
      "Epoch 231, Loss: 0.4532521496977959\n",
      "Epoch 232, Loss: 0.4526084553640119\n",
      "Epoch 233, Loss: 0.45228981460773116\n",
      "Epoch 234, Loss: 0.4520949132230242\n",
      "Epoch 235, Loss: 0.45205797975469736\n",
      "Epoch 236, Loss: 0.45230557901617613\n",
      "Epoch 237, Loss: 0.4528345953247846\n",
      "Epoch 238, Loss: 0.4549386133976144\n",
      "Epoch 239, Loss: 0.4564022887834084\n",
      "Epoch 240, Loss: 0.45805755540321125\n",
      "Epoch 241, Loss: 0.4541498302115434\n",
      "Epoch 242, Loss: 0.45158727109343993\n",
      "Epoch 243, Loss: 0.45105347173746146\n",
      "Epoch 244, Loss: 0.45263971453254503\n",
      "Epoch 245, Loss: 0.4550743757756552\n",
      "Epoch 246, Loss: 0.4549471294161167\n",
      "Epoch 247, Loss: 0.45361945699341477\n",
      "Epoch 248, Loss: 0.4506913801287277\n",
      "Epoch 249, Loss: 0.44996455105922556\n",
      "Epoch 250, Loss: 0.45070657227233163\n",
      "Epoch 251, Loss: 0.45134097806864715\n",
      "Epoch 252, Loss: 0.4518641160838718\n",
      "Epoch 253, Loss: 0.45124870735997746\n",
      "Epoch 254, Loss: 0.4507253179113287\n",
      "Epoch 255, Loss: 0.44965336548788515\n",
      "Epoch 256, Loss: 0.4490457284512466\n",
      "Epoch 257, Loss: 0.44867439459283065\n",
      "Epoch 258, Loss: 0.4485724236213288\n",
      "Epoch 259, Loss: 0.4487178165615502\n",
      "Epoch 260, Loss: 0.44913423821312026\n",
      "Epoch 261, Loss: 0.45068397397968624\n",
      "Epoch 262, Loss: 0.4523160224282965\n",
      "Epoch 263, Loss: 0.4554805695192658\n",
      "Epoch 264, Loss: 0.4521284819909996\n",
      "Epoch 265, Loss: 0.4491460824324\n",
      "Epoch 266, Loss: 0.4475917876267889\n",
      "Epoch 267, Loss: 0.4489836201844808\n",
      "Epoch 268, Loss: 0.4522696841254308\n",
      "Epoch 269, Loss: 0.45230492011773493\n",
      "Epoch 270, Loss: 0.45030259118174343\n",
      "Epoch 271, Loss: 0.44750189058874124\n",
      "Epoch 272, Loss: 0.4466950387656147\n",
      "Epoch 273, Loss: 0.44782488644087964\n",
      "Epoch 274, Loss: 0.4490013591686314\n",
      "Epoch 275, Loss: 0.44932258069431275\n",
      "Epoch 276, Loss: 0.4477355520631983\n",
      "Epoch 277, Loss: 0.4464409066948383\n",
      "Epoch 278, Loss: 0.4458417591594104\n",
      "Epoch 279, Loss: 0.4457414301360274\n",
      "Epoch 280, Loss: 0.4460887501130773\n",
      "Epoch 281, Loss: 0.44690611604251484\n",
      "Epoch 282, Loss: 0.4483696750030082\n",
      "Epoch 283, Loss: 0.4491296928695683\n",
      "Epoch 284, Loss: 0.4498625247292945\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23505388741151745\n",
      "Valence RMSE: 0.24219393477818155\n",
      "Arousal RMSE: 0.22769004794060407\n",
      "Test R^2 score: tensor([0.3732, 0.6177], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4954225363846636\n",
      "Num of epochs: 285\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Epoch 121, Loss: 0.4816228295571127\n",
      "Epoch 122, Loss: 0.4807618495484584\n",
      "Epoch 123, Loss: 0.48102030846138955\n",
      "Epoch 124, Loss: 0.4810176288364116\n",
      "Epoch 125, Loss: 0.48014338261661127\n",
      "Epoch 126, Loss: 0.47910314809270116\n",
      "Epoch 127, Loss: 0.47880795949683447\n",
      "Epoch 128, Loss: 0.47876177315142754\n",
      "Epoch 129, Loss: 0.47819597116449997\n",
      "Epoch 130, Loss: 0.47743214539542544\n",
      "Epoch 131, Loss: 0.47658476230340185\n",
      "Epoch 132, Loss: 0.4759644680184978\n",
      "Epoch 133, Loss: 0.4755635187712421\n",
      "Epoch 134, Loss: 0.47538264196584307\n",
      "Epoch 135, Loss: 0.4753673450596914\n",
      "Epoch 136, Loss: 0.4753697744168057\n",
      "Epoch 137, Loss: 0.4755973109583155\n",
      "Epoch 138, Loss: 0.4767626826605241\n",
      "Epoch 139, Loss: 0.47683528222287225\n",
      "Epoch 140, Loss: 0.47579445468261566\n",
      "Epoch 141, Loss: 0.47284533919249566\n",
      "Epoch 142, Loss: 0.472809286012082\n",
      "Epoch 143, Loss: 0.47458738111232796\n",
      "Epoch 144, Loss: 0.4733380058918715\n",
      "Epoch 145, Loss: 0.4715229034484008\n",
      "Epoch 146, Loss: 0.47096730966267386\n",
      "Epoch 147, Loss: 0.47161469890467383\n",
      "Epoch 148, Loss: 0.47183531411129226\n",
      "Epoch 149, Loss: 0.470294933688076\n",
      "Epoch 150, Loss: 0.4697114144973765\n",
      "Epoch 151, Loss: 0.47018789053701304\n",
      "Epoch 152, Loss: 0.4700460953124117\n",
      "Epoch 153, Loss: 0.4693645740016508\n",
      "Epoch 154, Loss: 0.4684558104180481\n",
      "Epoch 155, Loss: 0.4680960384979934\n",
      "Epoch 156, Loss: 0.4682454254529793\n",
      "Epoch 157, Loss: 0.4682974379105488\n",
      "Epoch 158, Loss: 0.4681590010361841\n",
      "Epoch 159, Loss: 0.4673225124817593\n",
      "Epoch 160, Loss: 0.4666701428107949\n",
      "Epoch 161, Loss: 0.4663166784414499\n",
      "Epoch 162, Loss: 0.46624854535037\n",
      "Epoch 163, Loss: 0.4664611566834947\n",
      "Epoch 164, Loss: 0.46654532450127006\n",
      "Epoch 165, Loss: 0.46721845607201035\n",
      "Epoch 166, Loss: 0.4668208160006968\n",
      "Epoch 167, Loss: 0.466573749669782\n",
      "Epoch 168, Loss: 0.46506387643568103\n",
      "Epoch 169, Loss: 0.4641220516950567\n",
      "Epoch 170, Loss: 0.4644550172656552\n",
      "Epoch 171, Loss: 0.4654613047116713\n",
      "Epoch 172, Loss: 0.46880176576246846\n",
      "Epoch 173, Loss: 0.4685649824581709\n",
      "Epoch 174, Loss: 0.46606115996454733\n",
      "Epoch 175, Loss: 0.46328263242364126\n",
      "Epoch 176, Loss: 0.4660484187313888\n",
      "Epoch 177, Loss: 0.4667669789465077\n",
      "Epoch 178, Loss: 0.46222975993575777\n",
      "Epoch 179, Loss: 0.46412214801342555\n",
      "Epoch 180, Loss: 0.46867384291927394\n",
      "Epoch 181, Loss: 0.46316269230838986\n",
      "Epoch 182, Loss: 0.4619791093960296\n",
      "Epoch 183, Loss: 0.4650783107301587\n",
      "Epoch 184, Loss: 0.4617210775403838\n",
      "Epoch 185, Loss: 0.4621011464022392\n",
      "Epoch 186, Loss: 0.4641367239628427\n",
      "Epoch 187, Loss: 0.46071131054180564\n",
      "Epoch 188, Loss: 0.46145892957673257\n",
      "Epoch 189, Loss: 0.464304908222076\n",
      "Epoch 190, Loss: 0.4606605441278432\n",
      "Epoch 191, Loss: 0.4601386308130386\n",
      "Epoch 192, Loss: 0.46193430495315263\n",
      "Epoch 193, Loss: 0.45977696912799226\n",
      "Epoch 194, Loss: 0.4597173803227511\n",
      "Epoch 195, Loss: 0.46096589922299847\n",
      "Epoch 196, Loss: 0.4592170612502741\n",
      "Epoch 197, Loss: 0.45864347419013624\n",
      "Epoch 198, Loss: 0.4594650511157848\n",
      "Epoch 199, Loss: 0.45924705942486493\n",
      "Epoch 200, Loss: 0.4584931199827434\n",
      "Epoch 201, Loss: 0.4578552480015059\n",
      "Epoch 202, Loss: 0.458251953125\n",
      "Epoch 203, Loss: 0.4586848316367108\n",
      "Epoch 204, Loss: 0.4577025349799131\n",
      "Epoch 205, Loss: 0.45707273702530193\n",
      "Epoch 206, Loss: 0.4571336158814618\n",
      "Epoch 207, Loss: 0.45746758002181026\n",
      "Epoch 208, Loss: 0.4580334166179955\n",
      "Epoch 209, Loss: 0.457551138813026\n",
      "Epoch 210, Loss: 0.4572461918817771\n",
      "Epoch 211, Loss: 0.45629613394883295\n",
      "Epoch 212, Loss: 0.4557945440718634\n",
      "Epoch 213, Loss: 0.455843155547024\n",
      "Epoch 214, Loss: 0.456219547378948\n",
      "Epoch 215, Loss: 0.4577116995236092\n",
      "Epoch 216, Loss: 0.4589176697652806\n",
      "Epoch 217, Loss: 0.46105034303609993\n",
      "Epoch 218, Loss: 0.4567326442394303\n",
      "Epoch 219, Loss: 0.45493112899589366\n",
      "Epoch 220, Loss: 0.45527199475335134\n",
      "Epoch 221, Loss: 0.45623630281032773\n",
      "Epoch 222, Loss: 0.45802054967026307\n",
      "Epoch 223, Loss: 0.45677224995978094\n",
      "Epoch 224, Loss: 0.45565914342704716\n",
      "Epoch 225, Loss: 0.45401674524485375\n",
      "Epoch 226, Loss: 0.4535057342791329\n",
      "Epoch 227, Loss: 0.45416164205845616\n",
      "Epoch 228, Loss: 0.45450513400796655\n",
      "Epoch 229, Loss: 0.4548238117351997\n",
      "Epoch 230, Loss: 0.4537990916311975\n",
      "Epoch 231, Loss: 0.4532521496977959\n",
      "Epoch 232, Loss: 0.4526084553640119\n",
      "Epoch 233, Loss: 0.45228981460773116\n",
      "Epoch 234, Loss: 0.4520949132230242\n",
      "Epoch 235, Loss: 0.45205797975469736\n",
      "Epoch 236, Loss: 0.45230557901617613\n",
      "Epoch 237, Loss: 0.4528345953247846\n",
      "Epoch 238, Loss: 0.4549386133976144\n",
      "Epoch 239, Loss: 0.4564022887834084\n",
      "Epoch 240, Loss: 0.45805755540321125\n",
      "Epoch 241, Loss: 0.4541498302115434\n",
      "Epoch 242, Loss: 0.45158727109343993\n",
      "Epoch 243, Loss: 0.45105347173746146\n",
      "Epoch 244, Loss: 0.45263971453254503\n",
      "Epoch 245, Loss: 0.4550743757756552\n",
      "Epoch 246, Loss: 0.4549471294161167\n",
      "Epoch 247, Loss: 0.45361945699341477\n",
      "Epoch 248, Loss: 0.4506913801287277\n",
      "Epoch 249, Loss: 0.44996455105922556\n",
      "Epoch 250, Loss: 0.45070657227233163\n",
      "Epoch 251, Loss: 0.45134097806864715\n",
      "Epoch 252, Loss: 0.4518641160838718\n",
      "Epoch 253, Loss: 0.45124870735997746\n",
      "Epoch 254, Loss: 0.4507253179113287\n",
      "Epoch 255, Loss: 0.44965336548788515\n",
      "Epoch 256, Loss: 0.4490457284512466\n",
      "Epoch 257, Loss: 0.44867439459283065\n",
      "Epoch 258, Loss: 0.4485724236213288\n",
      "Epoch 259, Loss: 0.4487178165615502\n",
      "Epoch 260, Loss: 0.44913423821312026\n",
      "Epoch 261, Loss: 0.45068397397968624\n",
      "Epoch 262, Loss: 0.4523160224282965\n",
      "Epoch 263, Loss: 0.4554805695192658\n",
      "Epoch 264, Loss: 0.4521284819909996\n",
      "Epoch 265, Loss: 0.4491460824324\n",
      "Epoch 266, Loss: 0.4475917876267889\n",
      "Epoch 267, Loss: 0.4489836201844808\n",
      "Epoch 268, Loss: 0.4522696841254308\n",
      "Epoch 269, Loss: 0.45230492011773493\n",
      "Epoch 270, Loss: 0.45030259118174343\n",
      "Epoch 271, Loss: 0.44750189058874124\n",
      "Epoch 272, Loss: 0.4466950387656147\n",
      "Epoch 273, Loss: 0.44782488644087964\n",
      "Epoch 274, Loss: 0.4490013591686314\n",
      "Epoch 275, Loss: 0.44932258069431275\n",
      "Epoch 276, Loss: 0.4477355520631983\n",
      "Epoch 277, Loss: 0.4464409066948383\n",
      "Epoch 278, Loss: 0.4458417591594104\n",
      "Epoch 279, Loss: 0.4457414301360274\n",
      "Epoch 280, Loss: 0.4460887501130773\n",
      "Epoch 281, Loss: 0.44690611604251484\n",
      "Epoch 282, Loss: 0.4483696750030082\n",
      "Epoch 283, Loss: 0.4491296928695683\n",
      "Epoch 284, Loss: 0.4498625247292945\n",
      "Epoch 285, Loss: 0.44791530084032105\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23470403829531467\n",
      "Valence RMSE: 0.24524561915588972\n",
      "Arousal RMSE: 0.22366617417280912\n",
      "Test R^2 score: tensor([0.3573, 0.6311], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4941719839044473\n",
      "Num of epochs: 286\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Epoch 121, Loss: 0.4816228295571127\n",
      "Epoch 122, Loss: 0.4807618495484584\n",
      "Epoch 123, Loss: 0.48102030846138955\n",
      "Epoch 124, Loss: 0.4810176288364116\n",
      "Epoch 125, Loss: 0.48014338261661127\n",
      "Epoch 126, Loss: 0.47910314809270116\n",
      "Epoch 127, Loss: 0.47880795949683447\n",
      "Epoch 128, Loss: 0.47876177315142754\n",
      "Epoch 129, Loss: 0.47819597116449997\n",
      "Epoch 130, Loss: 0.47743214539542544\n",
      "Epoch 131, Loss: 0.47658476230340185\n",
      "Epoch 132, Loss: 0.4759644680184978\n",
      "Epoch 133, Loss: 0.4755635187712421\n",
      "Epoch 134, Loss: 0.47538264196584307\n",
      "Epoch 135, Loss: 0.4753673450596914\n",
      "Epoch 136, Loss: 0.4753697744168057\n",
      "Epoch 137, Loss: 0.4755973109583155\n",
      "Epoch 138, Loss: 0.4767626826605241\n",
      "Epoch 139, Loss: 0.47683528222287225\n",
      "Epoch 140, Loss: 0.47579445468261566\n",
      "Epoch 141, Loss: 0.47284533919249566\n",
      "Epoch 142, Loss: 0.472809286012082\n",
      "Epoch 143, Loss: 0.47458738111232796\n",
      "Epoch 144, Loss: 0.4733380058918715\n",
      "Epoch 145, Loss: 0.4715229034484008\n",
      "Epoch 146, Loss: 0.47096730966267386\n",
      "Epoch 147, Loss: 0.47161469890467383\n",
      "Epoch 148, Loss: 0.47183531411129226\n",
      "Epoch 149, Loss: 0.470294933688076\n",
      "Epoch 150, Loss: 0.4697114144973765\n",
      "Epoch 151, Loss: 0.47018789053701304\n",
      "Epoch 152, Loss: 0.4700460953124117\n",
      "Epoch 153, Loss: 0.4693645740016508\n",
      "Epoch 154, Loss: 0.4684558104180481\n",
      "Epoch 155, Loss: 0.4680960384979934\n",
      "Epoch 156, Loss: 0.4682454254529793\n",
      "Epoch 157, Loss: 0.4682974379105488\n",
      "Epoch 158, Loss: 0.4681590010361841\n",
      "Epoch 159, Loss: 0.4673225124817593\n",
      "Epoch 160, Loss: 0.4666701428107949\n",
      "Epoch 161, Loss: 0.4663166784414499\n",
      "Epoch 162, Loss: 0.46624854535037\n",
      "Epoch 163, Loss: 0.4664611566834947\n",
      "Epoch 164, Loss: 0.46654532450127006\n",
      "Epoch 165, Loss: 0.46721845607201035\n",
      "Epoch 166, Loss: 0.4668208160006968\n",
      "Epoch 167, Loss: 0.466573749669782\n",
      "Epoch 168, Loss: 0.46506387643568103\n",
      "Epoch 169, Loss: 0.4641220516950567\n",
      "Epoch 170, Loss: 0.4644550172656552\n",
      "Epoch 171, Loss: 0.4654613047116713\n",
      "Epoch 172, Loss: 0.46880176576246846\n",
      "Epoch 173, Loss: 0.4685649824581709\n",
      "Epoch 174, Loss: 0.46606115996454733\n",
      "Epoch 175, Loss: 0.46328263242364126\n",
      "Epoch 176, Loss: 0.4660484187313888\n",
      "Epoch 177, Loss: 0.4667669789465077\n",
      "Epoch 178, Loss: 0.46222975993575777\n",
      "Epoch 179, Loss: 0.46412214801342555\n",
      "Epoch 180, Loss: 0.46867384291927394\n",
      "Epoch 181, Loss: 0.46316269230838986\n",
      "Epoch 182, Loss: 0.4619791093960296\n",
      "Epoch 183, Loss: 0.4650783107301587\n",
      "Epoch 184, Loss: 0.4617210775403838\n",
      "Epoch 185, Loss: 0.4621011464022392\n",
      "Epoch 186, Loss: 0.4641367239628427\n",
      "Epoch 187, Loss: 0.46071131054180564\n",
      "Epoch 188, Loss: 0.46145892957673257\n",
      "Epoch 189, Loss: 0.464304908222076\n",
      "Epoch 190, Loss: 0.4606605441278432\n",
      "Epoch 191, Loss: 0.4601386308130386\n",
      "Epoch 192, Loss: 0.46193430495315263\n",
      "Epoch 193, Loss: 0.45977696912799226\n",
      "Epoch 194, Loss: 0.4597173803227511\n",
      "Epoch 195, Loss: 0.46096589922299847\n",
      "Epoch 196, Loss: 0.4592170612502741\n",
      "Epoch 197, Loss: 0.45864347419013624\n",
      "Epoch 198, Loss: 0.4594650511157848\n",
      "Epoch 199, Loss: 0.45924705942486493\n",
      "Epoch 200, Loss: 0.4584931199827434\n",
      "Epoch 201, Loss: 0.4578552480015059\n",
      "Epoch 202, Loss: 0.458251953125\n",
      "Epoch 203, Loss: 0.4586848316367108\n",
      "Epoch 204, Loss: 0.4577025349799131\n",
      "Epoch 205, Loss: 0.45707273702530193\n",
      "Epoch 206, Loss: 0.4571336158814618\n",
      "Epoch 207, Loss: 0.45746758002181026\n",
      "Epoch 208, Loss: 0.4580334166179955\n",
      "Epoch 209, Loss: 0.457551138813026\n",
      "Epoch 210, Loss: 0.4572461918817771\n",
      "Epoch 211, Loss: 0.45629613394883295\n",
      "Epoch 212, Loss: 0.4557945440718634\n",
      "Epoch 213, Loss: 0.455843155547024\n",
      "Epoch 214, Loss: 0.456219547378948\n",
      "Epoch 215, Loss: 0.4577116995236092\n",
      "Epoch 216, Loss: 0.4589176697652806\n",
      "Epoch 217, Loss: 0.46105034303609993\n",
      "Epoch 218, Loss: 0.4567326442394303\n",
      "Epoch 219, Loss: 0.45493112899589366\n",
      "Epoch 220, Loss: 0.45527199475335134\n",
      "Epoch 221, Loss: 0.45623630281032773\n",
      "Epoch 222, Loss: 0.45802054967026307\n",
      "Epoch 223, Loss: 0.45677224995978094\n",
      "Epoch 224, Loss: 0.45565914342704716\n",
      "Epoch 225, Loss: 0.45401674524485375\n",
      "Epoch 226, Loss: 0.4535057342791329\n",
      "Epoch 227, Loss: 0.45416164205845616\n",
      "Epoch 228, Loss: 0.45450513400796655\n",
      "Epoch 229, Loss: 0.4548238117351997\n",
      "Epoch 230, Loss: 0.4537990916311975\n",
      "Epoch 231, Loss: 0.4532521496977959\n",
      "Epoch 232, Loss: 0.4526084553640119\n",
      "Epoch 233, Loss: 0.45228981460773116\n",
      "Epoch 234, Loss: 0.4520949132230242\n",
      "Epoch 235, Loss: 0.45205797975469736\n",
      "Epoch 236, Loss: 0.45230557901617613\n",
      "Epoch 237, Loss: 0.4528345953247846\n",
      "Epoch 238, Loss: 0.4549386133976144\n",
      "Epoch 239, Loss: 0.4564022887834084\n",
      "Epoch 240, Loss: 0.45805755540321125\n",
      "Epoch 241, Loss: 0.4541498302115434\n",
      "Epoch 242, Loss: 0.45158727109343993\n",
      "Epoch 243, Loss: 0.45105347173746146\n",
      "Epoch 244, Loss: 0.45263971453254503\n",
      "Epoch 245, Loss: 0.4550743757756552\n",
      "Epoch 246, Loss: 0.4549471294161167\n",
      "Epoch 247, Loss: 0.45361945699341477\n",
      "Epoch 248, Loss: 0.4506913801287277\n",
      "Epoch 249, Loss: 0.44996455105922556\n",
      "Epoch 250, Loss: 0.45070657227233163\n",
      "Epoch 251, Loss: 0.45134097806864715\n",
      "Epoch 252, Loss: 0.4518641160838718\n",
      "Epoch 253, Loss: 0.45124870735997746\n",
      "Epoch 254, Loss: 0.4507253179113287\n",
      "Epoch 255, Loss: 0.44965336548788515\n",
      "Epoch 256, Loss: 0.4490457284512466\n",
      "Epoch 257, Loss: 0.44867439459283065\n",
      "Epoch 258, Loss: 0.4485724236213288\n",
      "Epoch 259, Loss: 0.4487178165615502\n",
      "Epoch 260, Loss: 0.44913423821312026\n",
      "Epoch 261, Loss: 0.45068397397968624\n",
      "Epoch 262, Loss: 0.4523160224282965\n",
      "Epoch 263, Loss: 0.4554805695192658\n",
      "Epoch 264, Loss: 0.4521284819909996\n",
      "Epoch 265, Loss: 0.4491460824324\n",
      "Epoch 266, Loss: 0.4475917876267889\n",
      "Epoch 267, Loss: 0.4489836201844808\n",
      "Epoch 268, Loss: 0.4522696841254308\n",
      "Epoch 269, Loss: 0.45230492011773493\n",
      "Epoch 270, Loss: 0.45030259118174343\n",
      "Epoch 271, Loss: 0.44750189058874124\n",
      "Epoch 272, Loss: 0.4466950387656147\n",
      "Epoch 273, Loss: 0.44782488644087964\n",
      "Epoch 274, Loss: 0.4490013591686314\n",
      "Epoch 275, Loss: 0.44932258069431275\n",
      "Epoch 276, Loss: 0.4477355520631983\n",
      "Epoch 277, Loss: 0.4464409066948383\n",
      "Epoch 278, Loss: 0.4458417591594104\n",
      "Epoch 279, Loss: 0.4457414301360274\n",
      "Epoch 280, Loss: 0.4460887501130773\n",
      "Epoch 281, Loss: 0.44690611604251484\n",
      "Epoch 282, Loss: 0.4483696750030082\n",
      "Epoch 283, Loss: 0.4491296928695683\n",
      "Epoch 284, Loss: 0.4498625247292945\n",
      "Epoch 285, Loss: 0.44791530084032105\n",
      "Epoch 286, Loss: 0.4460872803334146\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23367778732083896\n",
      "Valence RMSE: 0.24253249045211256\n",
      "Arousal RMSE: 0.22447406899110292\n",
      "Test R^2 score: tensor([0.3714, 0.6284], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4999078648283185\n",
      "Num of epochs: 287\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Epoch 121, Loss: 0.4816228295571127\n",
      "Epoch 122, Loss: 0.4807618495484584\n",
      "Epoch 123, Loss: 0.48102030846138955\n",
      "Epoch 124, Loss: 0.4810176288364116\n",
      "Epoch 125, Loss: 0.48014338261661127\n",
      "Epoch 126, Loss: 0.47910314809270116\n",
      "Epoch 127, Loss: 0.47880795949683447\n",
      "Epoch 128, Loss: 0.47876177315142754\n",
      "Epoch 129, Loss: 0.47819597116449997\n",
      "Epoch 130, Loss: 0.47743214539542544\n",
      "Epoch 131, Loss: 0.47658476230340185\n",
      "Epoch 132, Loss: 0.4759644680184978\n",
      "Epoch 133, Loss: 0.4755635187712421\n",
      "Epoch 134, Loss: 0.47538264196584307\n",
      "Epoch 135, Loss: 0.4753673450596914\n",
      "Epoch 136, Loss: 0.4753697744168057\n",
      "Epoch 137, Loss: 0.4755973109583155\n",
      "Epoch 138, Loss: 0.4767626826605241\n",
      "Epoch 139, Loss: 0.47683528222287225\n",
      "Epoch 140, Loss: 0.47579445468261566\n",
      "Epoch 141, Loss: 0.47284533919249566\n",
      "Epoch 142, Loss: 0.472809286012082\n",
      "Epoch 143, Loss: 0.47458738111232796\n",
      "Epoch 144, Loss: 0.4733380058918715\n",
      "Epoch 145, Loss: 0.4715229034484008\n",
      "Epoch 146, Loss: 0.47096730966267386\n",
      "Epoch 147, Loss: 0.47161469890467383\n",
      "Epoch 148, Loss: 0.47183531411129226\n",
      "Epoch 149, Loss: 0.470294933688076\n",
      "Epoch 150, Loss: 0.4697114144973765\n",
      "Epoch 151, Loss: 0.47018789053701304\n",
      "Epoch 152, Loss: 0.4700460953124117\n",
      "Epoch 153, Loss: 0.4693645740016508\n",
      "Epoch 154, Loss: 0.4684558104180481\n",
      "Epoch 155, Loss: 0.4680960384979934\n",
      "Epoch 156, Loss: 0.4682454254529793\n",
      "Epoch 157, Loss: 0.4682974379105488\n",
      "Epoch 158, Loss: 0.4681590010361841\n",
      "Epoch 159, Loss: 0.4673225124817593\n",
      "Epoch 160, Loss: 0.4666701428107949\n",
      "Epoch 161, Loss: 0.4663166784414499\n",
      "Epoch 162, Loss: 0.46624854535037\n",
      "Epoch 163, Loss: 0.4664611566834947\n",
      "Epoch 164, Loss: 0.46654532450127006\n",
      "Epoch 165, Loss: 0.46721845607201035\n",
      "Epoch 166, Loss: 0.4668208160006968\n",
      "Epoch 167, Loss: 0.466573749669782\n",
      "Epoch 168, Loss: 0.46506387643568103\n",
      "Epoch 169, Loss: 0.4641220516950567\n",
      "Epoch 170, Loss: 0.4644550172656552\n",
      "Epoch 171, Loss: 0.4654613047116713\n",
      "Epoch 172, Loss: 0.46880176576246846\n",
      "Epoch 173, Loss: 0.4685649824581709\n",
      "Epoch 174, Loss: 0.46606115996454733\n",
      "Epoch 175, Loss: 0.46328263242364126\n",
      "Epoch 176, Loss: 0.4660484187313888\n",
      "Epoch 177, Loss: 0.4667669789465077\n",
      "Epoch 178, Loss: 0.46222975993575777\n",
      "Epoch 179, Loss: 0.46412214801342555\n",
      "Epoch 180, Loss: 0.46867384291927394\n",
      "Epoch 181, Loss: 0.46316269230838986\n",
      "Epoch 182, Loss: 0.4619791093960296\n",
      "Epoch 183, Loss: 0.4650783107301587\n",
      "Epoch 184, Loss: 0.4617210775403838\n",
      "Epoch 185, Loss: 0.4621011464022392\n",
      "Epoch 186, Loss: 0.4641367239628427\n",
      "Epoch 187, Loss: 0.46071131054180564\n",
      "Epoch 188, Loss: 0.46145892957673257\n",
      "Epoch 189, Loss: 0.464304908222076\n",
      "Epoch 190, Loss: 0.4606605441278432\n",
      "Epoch 191, Loss: 0.4601386308130386\n",
      "Epoch 192, Loss: 0.46193430495315263\n",
      "Epoch 193, Loss: 0.45977696912799226\n",
      "Epoch 194, Loss: 0.4597173803227511\n",
      "Epoch 195, Loss: 0.46096589922299847\n",
      "Epoch 196, Loss: 0.4592170612502741\n",
      "Epoch 197, Loss: 0.45864347419013624\n",
      "Epoch 198, Loss: 0.4594650511157848\n",
      "Epoch 199, Loss: 0.45924705942486493\n",
      "Epoch 200, Loss: 0.4584931199827434\n",
      "Epoch 201, Loss: 0.4578552480015059\n",
      "Epoch 202, Loss: 0.458251953125\n",
      "Epoch 203, Loss: 0.4586848316367108\n",
      "Epoch 204, Loss: 0.4577025349799131\n",
      "Epoch 205, Loss: 0.45707273702530193\n",
      "Epoch 206, Loss: 0.4571336158814618\n",
      "Epoch 207, Loss: 0.45746758002181026\n",
      "Epoch 208, Loss: 0.4580334166179955\n",
      "Epoch 209, Loss: 0.457551138813026\n",
      "Epoch 210, Loss: 0.4572461918817771\n",
      "Epoch 211, Loss: 0.45629613394883295\n",
      "Epoch 212, Loss: 0.4557945440718634\n",
      "Epoch 213, Loss: 0.455843155547024\n",
      "Epoch 214, Loss: 0.456219547378948\n",
      "Epoch 215, Loss: 0.4577116995236092\n",
      "Epoch 216, Loss: 0.4589176697652806\n",
      "Epoch 217, Loss: 0.46105034303609993\n",
      "Epoch 218, Loss: 0.4567326442394303\n",
      "Epoch 219, Loss: 0.45493112899589366\n",
      "Epoch 220, Loss: 0.45527199475335134\n",
      "Epoch 221, Loss: 0.45623630281032773\n",
      "Epoch 222, Loss: 0.45802054967026307\n",
      "Epoch 223, Loss: 0.45677224995978094\n",
      "Epoch 224, Loss: 0.45565914342704716\n",
      "Epoch 225, Loss: 0.45401674524485375\n",
      "Epoch 226, Loss: 0.4535057342791329\n",
      "Epoch 227, Loss: 0.45416164205845616\n",
      "Epoch 228, Loss: 0.45450513400796655\n",
      "Epoch 229, Loss: 0.4548238117351997\n",
      "Epoch 230, Loss: 0.4537990916311975\n",
      "Epoch 231, Loss: 0.4532521496977959\n",
      "Epoch 232, Loss: 0.4526084553640119\n",
      "Epoch 233, Loss: 0.45228981460773116\n",
      "Epoch 234, Loss: 0.4520949132230242\n",
      "Epoch 235, Loss: 0.45205797975469736\n",
      "Epoch 236, Loss: 0.45230557901617613\n",
      "Epoch 237, Loss: 0.4528345953247846\n",
      "Epoch 238, Loss: 0.4549386133976144\n",
      "Epoch 239, Loss: 0.4564022887834084\n",
      "Epoch 240, Loss: 0.45805755540321125\n",
      "Epoch 241, Loss: 0.4541498302115434\n",
      "Epoch 242, Loss: 0.45158727109343993\n",
      "Epoch 243, Loss: 0.45105347173746146\n",
      "Epoch 244, Loss: 0.45263971453254503\n",
      "Epoch 245, Loss: 0.4550743757756552\n",
      "Epoch 246, Loss: 0.4549471294161167\n",
      "Epoch 247, Loss: 0.45361945699341477\n",
      "Epoch 248, Loss: 0.4506913801287277\n",
      "Epoch 249, Loss: 0.44996455105922556\n",
      "Epoch 250, Loss: 0.45070657227233163\n",
      "Epoch 251, Loss: 0.45134097806864715\n",
      "Epoch 252, Loss: 0.4518641160838718\n",
      "Epoch 253, Loss: 0.45124870735997746\n",
      "Epoch 254, Loss: 0.4507253179113287\n",
      "Epoch 255, Loss: 0.44965336548788515\n",
      "Epoch 256, Loss: 0.4490457284512466\n",
      "Epoch 257, Loss: 0.44867439459283065\n",
      "Epoch 258, Loss: 0.4485724236213288\n",
      "Epoch 259, Loss: 0.4487178165615502\n",
      "Epoch 260, Loss: 0.44913423821312026\n",
      "Epoch 261, Loss: 0.45068397397968624\n",
      "Epoch 262, Loss: 0.4523160224282965\n",
      "Epoch 263, Loss: 0.4554805695192658\n",
      "Epoch 264, Loss: 0.4521284819909996\n",
      "Epoch 265, Loss: 0.4491460824324\n",
      "Epoch 266, Loss: 0.4475917876267889\n",
      "Epoch 267, Loss: 0.4489836201844808\n",
      "Epoch 268, Loss: 0.4522696841254308\n",
      "Epoch 269, Loss: 0.45230492011773493\n",
      "Epoch 270, Loss: 0.45030259118174343\n",
      "Epoch 271, Loss: 0.44750189058874124\n",
      "Epoch 272, Loss: 0.4466950387656147\n",
      "Epoch 273, Loss: 0.44782488644087964\n",
      "Epoch 274, Loss: 0.4490013591686314\n",
      "Epoch 275, Loss: 0.44932258069431275\n",
      "Epoch 276, Loss: 0.4477355520631983\n",
      "Epoch 277, Loss: 0.4464409066948383\n",
      "Epoch 278, Loss: 0.4458417591594104\n",
      "Epoch 279, Loss: 0.4457414301360274\n",
      "Epoch 280, Loss: 0.4460887501130773\n",
      "Epoch 281, Loss: 0.44690611604251484\n",
      "Epoch 282, Loss: 0.4483696750030082\n",
      "Epoch 283, Loss: 0.4491296928695683\n",
      "Epoch 284, Loss: 0.4498625247292945\n",
      "Epoch 285, Loss: 0.44791530084032105\n",
      "Epoch 286, Loss: 0.4460872803334146\n",
      "Epoch 287, Loss: 0.44485744850497494\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2341061037598244\n",
      "Valence RMSE: 0.24283497905637927\n",
      "Arousal RMSE: 0.2250389045962924\n",
      "Test R^2 score: tensor([0.3699, 0.6265], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.49818716313163486\n",
      "Num of epochs: 288\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Epoch 121, Loss: 0.4816228295571127\n",
      "Epoch 122, Loss: 0.4807618495484584\n",
      "Epoch 123, Loss: 0.48102030846138955\n",
      "Epoch 124, Loss: 0.4810176288364116\n",
      "Epoch 125, Loss: 0.48014338261661127\n",
      "Epoch 126, Loss: 0.47910314809270116\n",
      "Epoch 127, Loss: 0.47880795949683447\n",
      "Epoch 128, Loss: 0.47876177315142754\n",
      "Epoch 129, Loss: 0.47819597116449997\n",
      "Epoch 130, Loss: 0.47743214539542544\n",
      "Epoch 131, Loss: 0.47658476230340185\n",
      "Epoch 132, Loss: 0.4759644680184978\n",
      "Epoch 133, Loss: 0.4755635187712421\n",
      "Epoch 134, Loss: 0.47538264196584307\n",
      "Epoch 135, Loss: 0.4753673450596914\n",
      "Epoch 136, Loss: 0.4753697744168057\n",
      "Epoch 137, Loss: 0.4755973109583155\n",
      "Epoch 138, Loss: 0.4767626826605241\n",
      "Epoch 139, Loss: 0.47683528222287225\n",
      "Epoch 140, Loss: 0.47579445468261566\n",
      "Epoch 141, Loss: 0.47284533919249566\n",
      "Epoch 142, Loss: 0.472809286012082\n",
      "Epoch 143, Loss: 0.47458738111232796\n",
      "Epoch 144, Loss: 0.4733380058918715\n",
      "Epoch 145, Loss: 0.4715229034484008\n",
      "Epoch 146, Loss: 0.47096730966267386\n",
      "Epoch 147, Loss: 0.47161469890467383\n",
      "Epoch 148, Loss: 0.47183531411129226\n",
      "Epoch 149, Loss: 0.470294933688076\n",
      "Epoch 150, Loss: 0.4697114144973765\n",
      "Epoch 151, Loss: 0.47018789053701304\n",
      "Epoch 152, Loss: 0.4700460953124117\n",
      "Epoch 153, Loss: 0.4693645740016508\n",
      "Epoch 154, Loss: 0.4684558104180481\n",
      "Epoch 155, Loss: 0.4680960384979934\n",
      "Epoch 156, Loss: 0.4682454254529793\n",
      "Epoch 157, Loss: 0.4682974379105488\n",
      "Epoch 158, Loss: 0.4681590010361841\n",
      "Epoch 159, Loss: 0.4673225124817593\n",
      "Epoch 160, Loss: 0.4666701428107949\n",
      "Epoch 161, Loss: 0.4663166784414499\n",
      "Epoch 162, Loss: 0.46624854535037\n",
      "Epoch 163, Loss: 0.4664611566834947\n",
      "Epoch 164, Loss: 0.46654532450127006\n",
      "Epoch 165, Loss: 0.46721845607201035\n",
      "Epoch 166, Loss: 0.4668208160006968\n",
      "Epoch 167, Loss: 0.466573749669782\n",
      "Epoch 168, Loss: 0.46506387643568103\n",
      "Epoch 169, Loss: 0.4641220516950567\n",
      "Epoch 170, Loss: 0.4644550172656552\n",
      "Epoch 171, Loss: 0.4654613047116713\n",
      "Epoch 172, Loss: 0.46880176576246846\n",
      "Epoch 173, Loss: 0.4685649824581709\n",
      "Epoch 174, Loss: 0.46606115996454733\n",
      "Epoch 175, Loss: 0.46328263242364126\n",
      "Epoch 176, Loss: 0.4660484187313888\n",
      "Epoch 177, Loss: 0.4667669789465077\n",
      "Epoch 178, Loss: 0.46222975993575777\n",
      "Epoch 179, Loss: 0.46412214801342555\n",
      "Epoch 180, Loss: 0.46867384291927394\n",
      "Epoch 181, Loss: 0.46316269230838986\n",
      "Epoch 182, Loss: 0.4619791093960296\n",
      "Epoch 183, Loss: 0.4650783107301587\n",
      "Epoch 184, Loss: 0.4617210775403838\n",
      "Epoch 185, Loss: 0.4621011464022392\n",
      "Epoch 186, Loss: 0.4641367239628427\n",
      "Epoch 187, Loss: 0.46071131054180564\n",
      "Epoch 188, Loss: 0.46145892957673257\n",
      "Epoch 189, Loss: 0.464304908222076\n",
      "Epoch 190, Loss: 0.4606605441278432\n",
      "Epoch 191, Loss: 0.4601386308130386\n",
      "Epoch 192, Loss: 0.46193430495315263\n",
      "Epoch 193, Loss: 0.45977696912799226\n",
      "Epoch 194, Loss: 0.4597173803227511\n",
      "Epoch 195, Loss: 0.46096589922299847\n",
      "Epoch 196, Loss: 0.4592170612502741\n",
      "Epoch 197, Loss: 0.45864347419013624\n",
      "Epoch 198, Loss: 0.4594650511157848\n",
      "Epoch 199, Loss: 0.45924705942486493\n",
      "Epoch 200, Loss: 0.4584931199827434\n",
      "Epoch 201, Loss: 0.4578552480015059\n",
      "Epoch 202, Loss: 0.458251953125\n",
      "Epoch 203, Loss: 0.4586848316367108\n",
      "Epoch 204, Loss: 0.4577025349799131\n",
      "Epoch 205, Loss: 0.45707273702530193\n",
      "Epoch 206, Loss: 0.4571336158814618\n",
      "Epoch 207, Loss: 0.45746758002181026\n",
      "Epoch 208, Loss: 0.4580334166179955\n",
      "Epoch 209, Loss: 0.457551138813026\n",
      "Epoch 210, Loss: 0.4572461918817771\n",
      "Epoch 211, Loss: 0.45629613394883295\n",
      "Epoch 212, Loss: 0.4557945440718634\n",
      "Epoch 213, Loss: 0.455843155547024\n",
      "Epoch 214, Loss: 0.456219547378948\n",
      "Epoch 215, Loss: 0.4577116995236092\n",
      "Epoch 216, Loss: 0.4589176697652806\n",
      "Epoch 217, Loss: 0.46105034303609993\n",
      "Epoch 218, Loss: 0.4567326442394303\n",
      "Epoch 219, Loss: 0.45493112899589366\n",
      "Epoch 220, Loss: 0.45527199475335134\n",
      "Epoch 221, Loss: 0.45623630281032773\n",
      "Epoch 222, Loss: 0.45802054967026307\n",
      "Epoch 223, Loss: 0.45677224995978094\n",
      "Epoch 224, Loss: 0.45565914342704716\n",
      "Epoch 225, Loss: 0.45401674524485375\n",
      "Epoch 226, Loss: 0.4535057342791329\n",
      "Epoch 227, Loss: 0.45416164205845616\n",
      "Epoch 228, Loss: 0.45450513400796655\n",
      "Epoch 229, Loss: 0.4548238117351997\n",
      "Epoch 230, Loss: 0.4537990916311975\n",
      "Epoch 231, Loss: 0.4532521496977959\n",
      "Epoch 232, Loss: 0.4526084553640119\n",
      "Epoch 233, Loss: 0.45228981460773116\n",
      "Epoch 234, Loss: 0.4520949132230242\n",
      "Epoch 235, Loss: 0.45205797975469736\n",
      "Epoch 236, Loss: 0.45230557901617613\n",
      "Epoch 237, Loss: 0.4528345953247846\n",
      "Epoch 238, Loss: 0.4549386133976144\n",
      "Epoch 239, Loss: 0.4564022887834084\n",
      "Epoch 240, Loss: 0.45805755540321125\n",
      "Epoch 241, Loss: 0.4541498302115434\n",
      "Epoch 242, Loss: 0.45158727109343993\n",
      "Epoch 243, Loss: 0.45105347173746146\n",
      "Epoch 244, Loss: 0.45263971453254503\n",
      "Epoch 245, Loss: 0.4550743757756552\n",
      "Epoch 246, Loss: 0.4549471294161167\n",
      "Epoch 247, Loss: 0.45361945699341477\n",
      "Epoch 248, Loss: 0.4506913801287277\n",
      "Epoch 249, Loss: 0.44996455105922556\n",
      "Epoch 250, Loss: 0.45070657227233163\n",
      "Epoch 251, Loss: 0.45134097806864715\n",
      "Epoch 252, Loss: 0.4518641160838718\n",
      "Epoch 253, Loss: 0.45124870735997746\n",
      "Epoch 254, Loss: 0.4507253179113287\n",
      "Epoch 255, Loss: 0.44965336548788515\n",
      "Epoch 256, Loss: 0.4490457284512466\n",
      "Epoch 257, Loss: 0.44867439459283065\n",
      "Epoch 258, Loss: 0.4485724236213288\n",
      "Epoch 259, Loss: 0.4487178165615502\n",
      "Epoch 260, Loss: 0.44913423821312026\n",
      "Epoch 261, Loss: 0.45068397397968624\n",
      "Epoch 262, Loss: 0.4523160224282965\n",
      "Epoch 263, Loss: 0.4554805695192658\n",
      "Epoch 264, Loss: 0.4521284819909996\n",
      "Epoch 265, Loss: 0.4491460824324\n",
      "Epoch 266, Loss: 0.4475917876267889\n",
      "Epoch 267, Loss: 0.4489836201844808\n",
      "Epoch 268, Loss: 0.4522696841254308\n",
      "Epoch 269, Loss: 0.45230492011773493\n",
      "Epoch 270, Loss: 0.45030259118174343\n",
      "Epoch 271, Loss: 0.44750189058874124\n",
      "Epoch 272, Loss: 0.4466950387656147\n",
      "Epoch 273, Loss: 0.44782488644087964\n",
      "Epoch 274, Loss: 0.4490013591686314\n",
      "Epoch 275, Loss: 0.44932258069431275\n",
      "Epoch 276, Loss: 0.4477355520631983\n",
      "Epoch 277, Loss: 0.4464409066948383\n",
      "Epoch 278, Loss: 0.4458417591594104\n",
      "Epoch 279, Loss: 0.4457414301360274\n",
      "Epoch 280, Loss: 0.4460887501130773\n",
      "Epoch 281, Loss: 0.44690611604251484\n",
      "Epoch 282, Loss: 0.4483696750030082\n",
      "Epoch 283, Loss: 0.4491296928695683\n",
      "Epoch 284, Loss: 0.4498625247292945\n",
      "Epoch 285, Loss: 0.44791530084032105\n",
      "Epoch 286, Loss: 0.4460872803334146\n",
      "Epoch 287, Loss: 0.44485744850497494\n",
      "Epoch 288, Loss: 0.4448821514773518\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.236010229273761\n",
      "Valence RMSE: 0.24698113646816905\n",
      "Arousal RMSE: 0.22450384155421071\n",
      "Test R^2 score: tensor([0.3482, 0.6283], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.48822329752989735\n",
      "Num of epochs: 289\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Epoch 121, Loss: 0.4816228295571127\n",
      "Epoch 122, Loss: 0.4807618495484584\n",
      "Epoch 123, Loss: 0.48102030846138955\n",
      "Epoch 124, Loss: 0.4810176288364116\n",
      "Epoch 125, Loss: 0.48014338261661127\n",
      "Epoch 126, Loss: 0.47910314809270116\n",
      "Epoch 127, Loss: 0.47880795949683447\n",
      "Epoch 128, Loss: 0.47876177315142754\n",
      "Epoch 129, Loss: 0.47819597116449997\n",
      "Epoch 130, Loss: 0.47743214539542544\n",
      "Epoch 131, Loss: 0.47658476230340185\n",
      "Epoch 132, Loss: 0.4759644680184978\n",
      "Epoch 133, Loss: 0.4755635187712421\n",
      "Epoch 134, Loss: 0.47538264196584307\n",
      "Epoch 135, Loss: 0.4753673450596914\n",
      "Epoch 136, Loss: 0.4753697744168057\n",
      "Epoch 137, Loss: 0.4755973109583155\n",
      "Epoch 138, Loss: 0.4767626826605241\n",
      "Epoch 139, Loss: 0.47683528222287225\n",
      "Epoch 140, Loss: 0.47579445468261566\n",
      "Epoch 141, Loss: 0.47284533919249566\n",
      "Epoch 142, Loss: 0.472809286012082\n",
      "Epoch 143, Loss: 0.47458738111232796\n",
      "Epoch 144, Loss: 0.4733380058918715\n",
      "Epoch 145, Loss: 0.4715229034484008\n",
      "Epoch 146, Loss: 0.47096730966267386\n",
      "Epoch 147, Loss: 0.47161469890467383\n",
      "Epoch 148, Loss: 0.47183531411129226\n",
      "Epoch 149, Loss: 0.470294933688076\n",
      "Epoch 150, Loss: 0.4697114144973765\n",
      "Epoch 151, Loss: 0.47018789053701304\n",
      "Epoch 152, Loss: 0.4700460953124117\n",
      "Epoch 153, Loss: 0.4693645740016508\n",
      "Epoch 154, Loss: 0.4684558104180481\n",
      "Epoch 155, Loss: 0.4680960384979934\n",
      "Epoch 156, Loss: 0.4682454254529793\n",
      "Epoch 157, Loss: 0.4682974379105488\n",
      "Epoch 158, Loss: 0.4681590010361841\n",
      "Epoch 159, Loss: 0.4673225124817593\n",
      "Epoch 160, Loss: 0.4666701428107949\n",
      "Epoch 161, Loss: 0.4663166784414499\n",
      "Epoch 162, Loss: 0.46624854535037\n",
      "Epoch 163, Loss: 0.4664611566834947\n",
      "Epoch 164, Loss: 0.46654532450127006\n",
      "Epoch 165, Loss: 0.46721845607201035\n",
      "Epoch 166, Loss: 0.4668208160006968\n",
      "Epoch 167, Loss: 0.466573749669782\n",
      "Epoch 168, Loss: 0.46506387643568103\n",
      "Epoch 169, Loss: 0.4641220516950567\n",
      "Epoch 170, Loss: 0.4644550172656552\n",
      "Epoch 171, Loss: 0.4654613047116713\n",
      "Epoch 172, Loss: 0.46880176576246846\n",
      "Epoch 173, Loss: 0.4685649824581709\n",
      "Epoch 174, Loss: 0.46606115996454733\n",
      "Epoch 175, Loss: 0.46328263242364126\n",
      "Epoch 176, Loss: 0.4660484187313888\n",
      "Epoch 177, Loss: 0.4667669789465077\n",
      "Epoch 178, Loss: 0.46222975993575777\n",
      "Epoch 179, Loss: 0.46412214801342555\n",
      "Epoch 180, Loss: 0.46867384291927394\n",
      "Epoch 181, Loss: 0.46316269230838986\n",
      "Epoch 182, Loss: 0.4619791093960296\n",
      "Epoch 183, Loss: 0.4650783107301587\n",
      "Epoch 184, Loss: 0.4617210775403838\n",
      "Epoch 185, Loss: 0.4621011464022392\n",
      "Epoch 186, Loss: 0.4641367239628427\n",
      "Epoch 187, Loss: 0.46071131054180564\n",
      "Epoch 188, Loss: 0.46145892957673257\n",
      "Epoch 189, Loss: 0.464304908222076\n",
      "Epoch 190, Loss: 0.4606605441278432\n",
      "Epoch 191, Loss: 0.4601386308130386\n",
      "Epoch 192, Loss: 0.46193430495315263\n",
      "Epoch 193, Loss: 0.45977696912799226\n",
      "Epoch 194, Loss: 0.4597173803227511\n",
      "Epoch 195, Loss: 0.46096589922299847\n",
      "Epoch 196, Loss: 0.4592170612502741\n",
      "Epoch 197, Loss: 0.45864347419013624\n",
      "Epoch 198, Loss: 0.4594650511157848\n",
      "Epoch 199, Loss: 0.45924705942486493\n",
      "Epoch 200, Loss: 0.4584931199827434\n",
      "Epoch 201, Loss: 0.4578552480015059\n",
      "Epoch 202, Loss: 0.458251953125\n",
      "Epoch 203, Loss: 0.4586848316367108\n",
      "Epoch 204, Loss: 0.4577025349799131\n",
      "Epoch 205, Loss: 0.45707273702530193\n",
      "Epoch 206, Loss: 0.4571336158814618\n",
      "Epoch 207, Loss: 0.45746758002181026\n",
      "Epoch 208, Loss: 0.4580334166179955\n",
      "Epoch 209, Loss: 0.457551138813026\n",
      "Epoch 210, Loss: 0.4572461918817771\n",
      "Epoch 211, Loss: 0.45629613394883295\n",
      "Epoch 212, Loss: 0.4557945440718634\n",
      "Epoch 213, Loss: 0.455843155547024\n",
      "Epoch 214, Loss: 0.456219547378948\n",
      "Epoch 215, Loss: 0.4577116995236092\n",
      "Epoch 216, Loss: 0.4589176697652806\n",
      "Epoch 217, Loss: 0.46105034303609993\n",
      "Epoch 218, Loss: 0.4567326442394303\n",
      "Epoch 219, Loss: 0.45493112899589366\n",
      "Epoch 220, Loss: 0.45527199475335134\n",
      "Epoch 221, Loss: 0.45623630281032773\n",
      "Epoch 222, Loss: 0.45802054967026307\n",
      "Epoch 223, Loss: 0.45677224995978094\n",
      "Epoch 224, Loss: 0.45565914342704716\n",
      "Epoch 225, Loss: 0.45401674524485375\n",
      "Epoch 226, Loss: 0.4535057342791329\n",
      "Epoch 227, Loss: 0.45416164205845616\n",
      "Epoch 228, Loss: 0.45450513400796655\n",
      "Epoch 229, Loss: 0.4548238117351997\n",
      "Epoch 230, Loss: 0.4537990916311975\n",
      "Epoch 231, Loss: 0.4532521496977959\n",
      "Epoch 232, Loss: 0.4526084553640119\n",
      "Epoch 233, Loss: 0.45228981460773116\n",
      "Epoch 234, Loss: 0.4520949132230242\n",
      "Epoch 235, Loss: 0.45205797975469736\n",
      "Epoch 236, Loss: 0.45230557901617613\n",
      "Epoch 237, Loss: 0.4528345953247846\n",
      "Epoch 238, Loss: 0.4549386133976144\n",
      "Epoch 239, Loss: 0.4564022887834084\n",
      "Epoch 240, Loss: 0.45805755540321125\n",
      "Epoch 241, Loss: 0.4541498302115434\n",
      "Epoch 242, Loss: 0.45158727109343993\n",
      "Epoch 243, Loss: 0.45105347173746146\n",
      "Epoch 244, Loss: 0.45263971453254503\n",
      "Epoch 245, Loss: 0.4550743757756552\n",
      "Epoch 246, Loss: 0.4549471294161167\n",
      "Epoch 247, Loss: 0.45361945699341477\n",
      "Epoch 248, Loss: 0.4506913801287277\n",
      "Epoch 249, Loss: 0.44996455105922556\n",
      "Epoch 250, Loss: 0.45070657227233163\n",
      "Epoch 251, Loss: 0.45134097806864715\n",
      "Epoch 252, Loss: 0.4518641160838718\n",
      "Epoch 253, Loss: 0.45124870735997746\n",
      "Epoch 254, Loss: 0.4507253179113287\n",
      "Epoch 255, Loss: 0.44965336548788515\n",
      "Epoch 256, Loss: 0.4490457284512466\n",
      "Epoch 257, Loss: 0.44867439459283065\n",
      "Epoch 258, Loss: 0.4485724236213288\n",
      "Epoch 259, Loss: 0.4487178165615502\n",
      "Epoch 260, Loss: 0.44913423821312026\n",
      "Epoch 261, Loss: 0.45068397397968624\n",
      "Epoch 262, Loss: 0.4523160224282965\n",
      "Epoch 263, Loss: 0.4554805695192658\n",
      "Epoch 264, Loss: 0.4521284819909996\n",
      "Epoch 265, Loss: 0.4491460824324\n",
      "Epoch 266, Loss: 0.4475917876267889\n",
      "Epoch 267, Loss: 0.4489836201844808\n",
      "Epoch 268, Loss: 0.4522696841254308\n",
      "Epoch 269, Loss: 0.45230492011773493\n",
      "Epoch 270, Loss: 0.45030259118174343\n",
      "Epoch 271, Loss: 0.44750189058874124\n",
      "Epoch 272, Loss: 0.4466950387656147\n",
      "Epoch 273, Loss: 0.44782488644087964\n",
      "Epoch 274, Loss: 0.4490013591686314\n",
      "Epoch 275, Loss: 0.44932258069431275\n",
      "Epoch 276, Loss: 0.4477355520631983\n",
      "Epoch 277, Loss: 0.4464409066948383\n",
      "Epoch 278, Loss: 0.4458417591594104\n",
      "Epoch 279, Loss: 0.4457414301360274\n",
      "Epoch 280, Loss: 0.4460887501130773\n",
      "Epoch 281, Loss: 0.44690611604251484\n",
      "Epoch 282, Loss: 0.4483696750030082\n",
      "Epoch 283, Loss: 0.4491296928695683\n",
      "Epoch 284, Loss: 0.4498625247292945\n",
      "Epoch 285, Loss: 0.44791530084032105\n",
      "Epoch 286, Loss: 0.4460872803334146\n",
      "Epoch 287, Loss: 0.44485744850497494\n",
      "Epoch 288, Loss: 0.4448821514773518\n",
      "Epoch 289, Loss: 0.4460718473545706\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23571272946871935\n",
      "Valence RMSE: 0.2430988113274512\n",
      "Arousal RMSE: 0.22808759194302441\n",
      "Test R^2 score: tensor([0.3685, 0.6163], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.492408130179323\n",
      "Num of epochs: 290\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Epoch 121, Loss: 0.4816228295571127\n",
      "Epoch 122, Loss: 0.4807618495484584\n",
      "Epoch 123, Loss: 0.48102030846138955\n",
      "Epoch 124, Loss: 0.4810176288364116\n",
      "Epoch 125, Loss: 0.48014338261661127\n",
      "Epoch 126, Loss: 0.47910314809270116\n",
      "Epoch 127, Loss: 0.47880795949683447\n",
      "Epoch 128, Loss: 0.47876177315142754\n",
      "Epoch 129, Loss: 0.47819597116449997\n",
      "Epoch 130, Loss: 0.47743214539542544\n",
      "Epoch 131, Loss: 0.47658476230340185\n",
      "Epoch 132, Loss: 0.4759644680184978\n",
      "Epoch 133, Loss: 0.4755635187712421\n",
      "Epoch 134, Loss: 0.47538264196584307\n",
      "Epoch 135, Loss: 0.4753673450596914\n",
      "Epoch 136, Loss: 0.4753697744168057\n",
      "Epoch 137, Loss: 0.4755973109583155\n",
      "Epoch 138, Loss: 0.4767626826605241\n",
      "Epoch 139, Loss: 0.47683528222287225\n",
      "Epoch 140, Loss: 0.47579445468261566\n",
      "Epoch 141, Loss: 0.47284533919249566\n",
      "Epoch 142, Loss: 0.472809286012082\n",
      "Epoch 143, Loss: 0.47458738111232796\n",
      "Epoch 144, Loss: 0.4733380058918715\n",
      "Epoch 145, Loss: 0.4715229034484008\n",
      "Epoch 146, Loss: 0.47096730966267386\n",
      "Epoch 147, Loss: 0.47161469890467383\n",
      "Epoch 148, Loss: 0.47183531411129226\n",
      "Epoch 149, Loss: 0.470294933688076\n",
      "Epoch 150, Loss: 0.4697114144973765\n",
      "Epoch 151, Loss: 0.47018789053701304\n",
      "Epoch 152, Loss: 0.4700460953124117\n",
      "Epoch 153, Loss: 0.4693645740016508\n",
      "Epoch 154, Loss: 0.4684558104180481\n",
      "Epoch 155, Loss: 0.4680960384979934\n",
      "Epoch 156, Loss: 0.4682454254529793\n",
      "Epoch 157, Loss: 0.4682974379105488\n",
      "Epoch 158, Loss: 0.4681590010361841\n",
      "Epoch 159, Loss: 0.4673225124817593\n",
      "Epoch 160, Loss: 0.4666701428107949\n",
      "Epoch 161, Loss: 0.4663166784414499\n",
      "Epoch 162, Loss: 0.46624854535037\n",
      "Epoch 163, Loss: 0.4664611566834947\n",
      "Epoch 164, Loss: 0.46654532450127006\n",
      "Epoch 165, Loss: 0.46721845607201035\n",
      "Epoch 166, Loss: 0.4668208160006968\n",
      "Epoch 167, Loss: 0.466573749669782\n",
      "Epoch 168, Loss: 0.46506387643568103\n",
      "Epoch 169, Loss: 0.4641220516950567\n",
      "Epoch 170, Loss: 0.4644550172656552\n",
      "Epoch 171, Loss: 0.4654613047116713\n",
      "Epoch 172, Loss: 0.46880176576246846\n",
      "Epoch 173, Loss: 0.4685649824581709\n",
      "Epoch 174, Loss: 0.46606115996454733\n",
      "Epoch 175, Loss: 0.46328263242364126\n",
      "Epoch 176, Loss: 0.4660484187313888\n",
      "Epoch 177, Loss: 0.4667669789465077\n",
      "Epoch 178, Loss: 0.46222975993575777\n",
      "Epoch 179, Loss: 0.46412214801342555\n",
      "Epoch 180, Loss: 0.46867384291927394\n",
      "Epoch 181, Loss: 0.46316269230838986\n",
      "Epoch 182, Loss: 0.4619791093960296\n",
      "Epoch 183, Loss: 0.4650783107301587\n",
      "Epoch 184, Loss: 0.4617210775403838\n",
      "Epoch 185, Loss: 0.4621011464022392\n",
      "Epoch 186, Loss: 0.4641367239628427\n",
      "Epoch 187, Loss: 0.46071131054180564\n",
      "Epoch 188, Loss: 0.46145892957673257\n",
      "Epoch 189, Loss: 0.464304908222076\n",
      "Epoch 190, Loss: 0.4606605441278432\n",
      "Epoch 191, Loss: 0.4601386308130386\n",
      "Epoch 192, Loss: 0.46193430495315263\n",
      "Epoch 193, Loss: 0.45977696912799226\n",
      "Epoch 194, Loss: 0.4597173803227511\n",
      "Epoch 195, Loss: 0.46096589922299847\n",
      "Epoch 196, Loss: 0.4592170612502741\n",
      "Epoch 197, Loss: 0.45864347419013624\n",
      "Epoch 198, Loss: 0.4594650511157848\n",
      "Epoch 199, Loss: 0.45924705942486493\n",
      "Epoch 200, Loss: 0.4584931199827434\n",
      "Epoch 201, Loss: 0.4578552480015059\n",
      "Epoch 202, Loss: 0.458251953125\n",
      "Epoch 203, Loss: 0.4586848316367108\n",
      "Epoch 204, Loss: 0.4577025349799131\n",
      "Epoch 205, Loss: 0.45707273702530193\n",
      "Epoch 206, Loss: 0.4571336158814618\n",
      "Epoch 207, Loss: 0.45746758002181026\n",
      "Epoch 208, Loss: 0.4580334166179955\n",
      "Epoch 209, Loss: 0.457551138813026\n",
      "Epoch 210, Loss: 0.4572461918817771\n",
      "Epoch 211, Loss: 0.45629613394883295\n",
      "Epoch 212, Loss: 0.4557945440718634\n",
      "Epoch 213, Loss: 0.455843155547024\n",
      "Epoch 214, Loss: 0.456219547378948\n",
      "Epoch 215, Loss: 0.4577116995236092\n",
      "Epoch 216, Loss: 0.4589176697652806\n",
      "Epoch 217, Loss: 0.46105034303609993\n",
      "Epoch 218, Loss: 0.4567326442394303\n",
      "Epoch 219, Loss: 0.45493112899589366\n",
      "Epoch 220, Loss: 0.45527199475335134\n",
      "Epoch 221, Loss: 0.45623630281032773\n",
      "Epoch 222, Loss: 0.45802054967026307\n",
      "Epoch 223, Loss: 0.45677224995978094\n",
      "Epoch 224, Loss: 0.45565914342704716\n",
      "Epoch 225, Loss: 0.45401674524485375\n",
      "Epoch 226, Loss: 0.4535057342791329\n",
      "Epoch 227, Loss: 0.45416164205845616\n",
      "Epoch 228, Loss: 0.45450513400796655\n",
      "Epoch 229, Loss: 0.4548238117351997\n",
      "Epoch 230, Loss: 0.4537990916311975\n",
      "Epoch 231, Loss: 0.4532521496977959\n",
      "Epoch 232, Loss: 0.4526084553640119\n",
      "Epoch 233, Loss: 0.45228981460773116\n",
      "Epoch 234, Loss: 0.4520949132230242\n",
      "Epoch 235, Loss: 0.45205797975469736\n",
      "Epoch 236, Loss: 0.45230557901617613\n",
      "Epoch 237, Loss: 0.4528345953247846\n",
      "Epoch 238, Loss: 0.4549386133976144\n",
      "Epoch 239, Loss: 0.4564022887834084\n",
      "Epoch 240, Loss: 0.45805755540321125\n",
      "Epoch 241, Loss: 0.4541498302115434\n",
      "Epoch 242, Loss: 0.45158727109343993\n",
      "Epoch 243, Loss: 0.45105347173746146\n",
      "Epoch 244, Loss: 0.45263971453254503\n",
      "Epoch 245, Loss: 0.4550743757756552\n",
      "Epoch 246, Loss: 0.4549471294161167\n",
      "Epoch 247, Loss: 0.45361945699341477\n",
      "Epoch 248, Loss: 0.4506913801287277\n",
      "Epoch 249, Loss: 0.44996455105922556\n",
      "Epoch 250, Loss: 0.45070657227233163\n",
      "Epoch 251, Loss: 0.45134097806864715\n",
      "Epoch 252, Loss: 0.4518641160838718\n",
      "Epoch 253, Loss: 0.45124870735997746\n",
      "Epoch 254, Loss: 0.4507253179113287\n",
      "Epoch 255, Loss: 0.44965336548788515\n",
      "Epoch 256, Loss: 0.4490457284512466\n",
      "Epoch 257, Loss: 0.44867439459283065\n",
      "Epoch 258, Loss: 0.4485724236213288\n",
      "Epoch 259, Loss: 0.4487178165615502\n",
      "Epoch 260, Loss: 0.44913423821312026\n",
      "Epoch 261, Loss: 0.45068397397968624\n",
      "Epoch 262, Loss: 0.4523160224282965\n",
      "Epoch 263, Loss: 0.4554805695192658\n",
      "Epoch 264, Loss: 0.4521284819909996\n",
      "Epoch 265, Loss: 0.4491460824324\n",
      "Epoch 266, Loss: 0.4475917876267889\n",
      "Epoch 267, Loss: 0.4489836201844808\n",
      "Epoch 268, Loss: 0.4522696841254308\n",
      "Epoch 269, Loss: 0.45230492011773493\n",
      "Epoch 270, Loss: 0.45030259118174343\n",
      "Epoch 271, Loss: 0.44750189058874124\n",
      "Epoch 272, Loss: 0.4466950387656147\n",
      "Epoch 273, Loss: 0.44782488644087964\n",
      "Epoch 274, Loss: 0.4490013591686314\n",
      "Epoch 275, Loss: 0.44932258069431275\n",
      "Epoch 276, Loss: 0.4477355520631983\n",
      "Epoch 277, Loss: 0.4464409066948383\n",
      "Epoch 278, Loss: 0.4458417591594104\n",
      "Epoch 279, Loss: 0.4457414301360274\n",
      "Epoch 280, Loss: 0.4460887501130773\n",
      "Epoch 281, Loss: 0.44690611604251484\n",
      "Epoch 282, Loss: 0.4483696750030082\n",
      "Epoch 283, Loss: 0.4491296928695683\n",
      "Epoch 284, Loss: 0.4498625247292945\n",
      "Epoch 285, Loss: 0.44791530084032105\n",
      "Epoch 286, Loss: 0.4460872803334146\n",
      "Epoch 287, Loss: 0.44485744850497494\n",
      "Epoch 288, Loss: 0.4448821514773518\n",
      "Epoch 289, Loss: 0.4460718473545706\n",
      "Epoch 290, Loss: 0.4475995445619453\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23828894792863137\n",
      "Valence RMSE: 0.2500455829791231\n",
      "Arousal RMSE: 0.22592133994489863\n",
      "Test R^2 score: tensor([0.3319, 0.6236], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4777309615506324\n",
      "Num of epochs: 291\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Epoch 121, Loss: 0.4816228295571127\n",
      "Epoch 122, Loss: 0.4807618495484584\n",
      "Epoch 123, Loss: 0.48102030846138955\n",
      "Epoch 124, Loss: 0.4810176288364116\n",
      "Epoch 125, Loss: 0.48014338261661127\n",
      "Epoch 126, Loss: 0.47910314809270116\n",
      "Epoch 127, Loss: 0.47880795949683447\n",
      "Epoch 128, Loss: 0.47876177315142754\n",
      "Epoch 129, Loss: 0.47819597116449997\n",
      "Epoch 130, Loss: 0.47743214539542544\n",
      "Epoch 131, Loss: 0.47658476230340185\n",
      "Epoch 132, Loss: 0.4759644680184978\n",
      "Epoch 133, Loss: 0.4755635187712421\n",
      "Epoch 134, Loss: 0.47538264196584307\n",
      "Epoch 135, Loss: 0.4753673450596914\n",
      "Epoch 136, Loss: 0.4753697744168057\n",
      "Epoch 137, Loss: 0.4755973109583155\n",
      "Epoch 138, Loss: 0.4767626826605241\n",
      "Epoch 139, Loss: 0.47683528222287225\n",
      "Epoch 140, Loss: 0.47579445468261566\n",
      "Epoch 141, Loss: 0.47284533919249566\n",
      "Epoch 142, Loss: 0.472809286012082\n",
      "Epoch 143, Loss: 0.47458738111232796\n",
      "Epoch 144, Loss: 0.4733380058918715\n",
      "Epoch 145, Loss: 0.4715229034484008\n",
      "Epoch 146, Loss: 0.47096730966267386\n",
      "Epoch 147, Loss: 0.47161469890467383\n",
      "Epoch 148, Loss: 0.47183531411129226\n",
      "Epoch 149, Loss: 0.470294933688076\n",
      "Epoch 150, Loss: 0.4697114144973765\n",
      "Epoch 151, Loss: 0.47018789053701304\n",
      "Epoch 152, Loss: 0.4700460953124117\n",
      "Epoch 153, Loss: 0.4693645740016508\n",
      "Epoch 154, Loss: 0.4684558104180481\n",
      "Epoch 155, Loss: 0.4680960384979934\n",
      "Epoch 156, Loss: 0.4682454254529793\n",
      "Epoch 157, Loss: 0.4682974379105488\n",
      "Epoch 158, Loss: 0.4681590010361841\n",
      "Epoch 159, Loss: 0.4673225124817593\n",
      "Epoch 160, Loss: 0.4666701428107949\n",
      "Epoch 161, Loss: 0.4663166784414499\n",
      "Epoch 162, Loss: 0.46624854535037\n",
      "Epoch 163, Loss: 0.4664611566834947\n",
      "Epoch 164, Loss: 0.46654532450127006\n",
      "Epoch 165, Loss: 0.46721845607201035\n",
      "Epoch 166, Loss: 0.4668208160006968\n",
      "Epoch 167, Loss: 0.466573749669782\n",
      "Epoch 168, Loss: 0.46506387643568103\n",
      "Epoch 169, Loss: 0.4641220516950567\n",
      "Epoch 170, Loss: 0.4644550172656552\n",
      "Epoch 171, Loss: 0.4654613047116713\n",
      "Epoch 172, Loss: 0.46880176576246846\n",
      "Epoch 173, Loss: 0.4685649824581709\n",
      "Epoch 174, Loss: 0.46606115996454733\n",
      "Epoch 175, Loss: 0.46328263242364126\n",
      "Epoch 176, Loss: 0.4660484187313888\n",
      "Epoch 177, Loss: 0.4667669789465077\n",
      "Epoch 178, Loss: 0.46222975993575777\n",
      "Epoch 179, Loss: 0.46412214801342555\n",
      "Epoch 180, Loss: 0.46867384291927394\n",
      "Epoch 181, Loss: 0.46316269230838986\n",
      "Epoch 182, Loss: 0.4619791093960296\n",
      "Epoch 183, Loss: 0.4650783107301587\n",
      "Epoch 184, Loss: 0.4617210775403838\n",
      "Epoch 185, Loss: 0.4621011464022392\n",
      "Epoch 186, Loss: 0.4641367239628427\n",
      "Epoch 187, Loss: 0.46071131054180564\n",
      "Epoch 188, Loss: 0.46145892957673257\n",
      "Epoch 189, Loss: 0.464304908222076\n",
      "Epoch 190, Loss: 0.4606605441278432\n",
      "Epoch 191, Loss: 0.4601386308130386\n",
      "Epoch 192, Loss: 0.46193430495315263\n",
      "Epoch 193, Loss: 0.45977696912799226\n",
      "Epoch 194, Loss: 0.4597173803227511\n",
      "Epoch 195, Loss: 0.46096589922299847\n",
      "Epoch 196, Loss: 0.4592170612502741\n",
      "Epoch 197, Loss: 0.45864347419013624\n",
      "Epoch 198, Loss: 0.4594650511157848\n",
      "Epoch 199, Loss: 0.45924705942486493\n",
      "Epoch 200, Loss: 0.4584931199827434\n",
      "Epoch 201, Loss: 0.4578552480015059\n",
      "Epoch 202, Loss: 0.458251953125\n",
      "Epoch 203, Loss: 0.4586848316367108\n",
      "Epoch 204, Loss: 0.4577025349799131\n",
      "Epoch 205, Loss: 0.45707273702530193\n",
      "Epoch 206, Loss: 0.4571336158814618\n",
      "Epoch 207, Loss: 0.45746758002181026\n",
      "Epoch 208, Loss: 0.4580334166179955\n",
      "Epoch 209, Loss: 0.457551138813026\n",
      "Epoch 210, Loss: 0.4572461918817771\n",
      "Epoch 211, Loss: 0.45629613394883295\n",
      "Epoch 212, Loss: 0.4557945440718634\n",
      "Epoch 213, Loss: 0.455843155547024\n",
      "Epoch 214, Loss: 0.456219547378948\n",
      "Epoch 215, Loss: 0.4577116995236092\n",
      "Epoch 216, Loss: 0.4589176697652806\n",
      "Epoch 217, Loss: 0.46105034303609993\n",
      "Epoch 218, Loss: 0.4567326442394303\n",
      "Epoch 219, Loss: 0.45493112899589366\n",
      "Epoch 220, Loss: 0.45527199475335134\n",
      "Epoch 221, Loss: 0.45623630281032773\n",
      "Epoch 222, Loss: 0.45802054967026307\n",
      "Epoch 223, Loss: 0.45677224995978094\n",
      "Epoch 224, Loss: 0.45565914342704716\n",
      "Epoch 225, Loss: 0.45401674524485375\n",
      "Epoch 226, Loss: 0.4535057342791329\n",
      "Epoch 227, Loss: 0.45416164205845616\n",
      "Epoch 228, Loss: 0.45450513400796655\n",
      "Epoch 229, Loss: 0.4548238117351997\n",
      "Epoch 230, Loss: 0.4537990916311975\n",
      "Epoch 231, Loss: 0.4532521496977959\n",
      "Epoch 232, Loss: 0.4526084553640119\n",
      "Epoch 233, Loss: 0.45228981460773116\n",
      "Epoch 234, Loss: 0.4520949132230242\n",
      "Epoch 235, Loss: 0.45205797975469736\n",
      "Epoch 236, Loss: 0.45230557901617613\n",
      "Epoch 237, Loss: 0.4528345953247846\n",
      "Epoch 238, Loss: 0.4549386133976144\n",
      "Epoch 239, Loss: 0.4564022887834084\n",
      "Epoch 240, Loss: 0.45805755540321125\n",
      "Epoch 241, Loss: 0.4541498302115434\n",
      "Epoch 242, Loss: 0.45158727109343993\n",
      "Epoch 243, Loss: 0.45105347173746146\n",
      "Epoch 244, Loss: 0.45263971453254503\n",
      "Epoch 245, Loss: 0.4550743757756552\n",
      "Epoch 246, Loss: 0.4549471294161167\n",
      "Epoch 247, Loss: 0.45361945699341477\n",
      "Epoch 248, Loss: 0.4506913801287277\n",
      "Epoch 249, Loss: 0.44996455105922556\n",
      "Epoch 250, Loss: 0.45070657227233163\n",
      "Epoch 251, Loss: 0.45134097806864715\n",
      "Epoch 252, Loss: 0.4518641160838718\n",
      "Epoch 253, Loss: 0.45124870735997746\n",
      "Epoch 254, Loss: 0.4507253179113287\n",
      "Epoch 255, Loss: 0.44965336548788515\n",
      "Epoch 256, Loss: 0.4490457284512466\n",
      "Epoch 257, Loss: 0.44867439459283065\n",
      "Epoch 258, Loss: 0.4485724236213288\n",
      "Epoch 259, Loss: 0.4487178165615502\n",
      "Epoch 260, Loss: 0.44913423821312026\n",
      "Epoch 261, Loss: 0.45068397397968624\n",
      "Epoch 262, Loss: 0.4523160224282965\n",
      "Epoch 263, Loss: 0.4554805695192658\n",
      "Epoch 264, Loss: 0.4521284819909996\n",
      "Epoch 265, Loss: 0.4491460824324\n",
      "Epoch 266, Loss: 0.4475917876267889\n",
      "Epoch 267, Loss: 0.4489836201844808\n",
      "Epoch 268, Loss: 0.4522696841254308\n",
      "Epoch 269, Loss: 0.45230492011773493\n",
      "Epoch 270, Loss: 0.45030259118174343\n",
      "Epoch 271, Loss: 0.44750189058874124\n",
      "Epoch 272, Loss: 0.4466950387656147\n",
      "Epoch 273, Loss: 0.44782488644087964\n",
      "Epoch 274, Loss: 0.4490013591686314\n",
      "Epoch 275, Loss: 0.44932258069431275\n",
      "Epoch 276, Loss: 0.4477355520631983\n",
      "Epoch 277, Loss: 0.4464409066948383\n",
      "Epoch 278, Loss: 0.4458417591594104\n",
      "Epoch 279, Loss: 0.4457414301360274\n",
      "Epoch 280, Loss: 0.4460887501130773\n",
      "Epoch 281, Loss: 0.44690611604251484\n",
      "Epoch 282, Loss: 0.4483696750030082\n",
      "Epoch 283, Loss: 0.4491296928695683\n",
      "Epoch 284, Loss: 0.4498625247292945\n",
      "Epoch 285, Loss: 0.44791530084032105\n",
      "Epoch 286, Loss: 0.4460872803334146\n",
      "Epoch 287, Loss: 0.44485744850497494\n",
      "Epoch 288, Loss: 0.4448821514773518\n",
      "Epoch 289, Loss: 0.4460718473545706\n",
      "Epoch 290, Loss: 0.4475995445619453\n",
      "Epoch 291, Loss: 0.44939647973391844\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2364931697653789\n",
      "Valence RMSE: 0.2433712894846816\n",
      "Arousal RMSE: 0.22940892342259914\n",
      "Test R^2 score: tensor([0.3671, 0.6119], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.48947080604615106\n",
      "Num of epochs: 292\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Epoch 121, Loss: 0.4816228295571127\n",
      "Epoch 122, Loss: 0.4807618495484584\n",
      "Epoch 123, Loss: 0.48102030846138955\n",
      "Epoch 124, Loss: 0.4810176288364116\n",
      "Epoch 125, Loss: 0.48014338261661127\n",
      "Epoch 126, Loss: 0.47910314809270116\n",
      "Epoch 127, Loss: 0.47880795949683447\n",
      "Epoch 128, Loss: 0.47876177315142754\n",
      "Epoch 129, Loss: 0.47819597116449997\n",
      "Epoch 130, Loss: 0.47743214539542544\n",
      "Epoch 131, Loss: 0.47658476230340185\n",
      "Epoch 132, Loss: 0.4759644680184978\n",
      "Epoch 133, Loss: 0.4755635187712421\n",
      "Epoch 134, Loss: 0.47538264196584307\n",
      "Epoch 135, Loss: 0.4753673450596914\n",
      "Epoch 136, Loss: 0.4753697744168057\n",
      "Epoch 137, Loss: 0.4755973109583155\n",
      "Epoch 138, Loss: 0.4767626826605241\n",
      "Epoch 139, Loss: 0.47683528222287225\n",
      "Epoch 140, Loss: 0.47579445468261566\n",
      "Epoch 141, Loss: 0.47284533919249566\n",
      "Epoch 142, Loss: 0.472809286012082\n",
      "Epoch 143, Loss: 0.47458738111232796\n",
      "Epoch 144, Loss: 0.4733380058918715\n",
      "Epoch 145, Loss: 0.4715229034484008\n",
      "Epoch 146, Loss: 0.47096730966267386\n",
      "Epoch 147, Loss: 0.47161469890467383\n",
      "Epoch 148, Loss: 0.47183531411129226\n",
      "Epoch 149, Loss: 0.470294933688076\n",
      "Epoch 150, Loss: 0.4697114144973765\n",
      "Epoch 151, Loss: 0.47018789053701304\n",
      "Epoch 152, Loss: 0.4700460953124117\n",
      "Epoch 153, Loss: 0.4693645740016508\n",
      "Epoch 154, Loss: 0.4684558104180481\n",
      "Epoch 155, Loss: 0.4680960384979934\n",
      "Epoch 156, Loss: 0.4682454254529793\n",
      "Epoch 157, Loss: 0.4682974379105488\n",
      "Epoch 158, Loss: 0.4681590010361841\n",
      "Epoch 159, Loss: 0.4673225124817593\n",
      "Epoch 160, Loss: 0.4666701428107949\n",
      "Epoch 161, Loss: 0.4663166784414499\n",
      "Epoch 162, Loss: 0.46624854535037\n",
      "Epoch 163, Loss: 0.4664611566834947\n",
      "Epoch 164, Loss: 0.46654532450127006\n",
      "Epoch 165, Loss: 0.46721845607201035\n",
      "Epoch 166, Loss: 0.4668208160006968\n",
      "Epoch 167, Loss: 0.466573749669782\n",
      "Epoch 168, Loss: 0.46506387643568103\n",
      "Epoch 169, Loss: 0.4641220516950567\n",
      "Epoch 170, Loss: 0.4644550172656552\n",
      "Epoch 171, Loss: 0.4654613047116713\n",
      "Epoch 172, Loss: 0.46880176576246846\n",
      "Epoch 173, Loss: 0.4685649824581709\n",
      "Epoch 174, Loss: 0.46606115996454733\n",
      "Epoch 175, Loss: 0.46328263242364126\n",
      "Epoch 176, Loss: 0.4660484187313888\n",
      "Epoch 177, Loss: 0.4667669789465077\n",
      "Epoch 178, Loss: 0.46222975993575777\n",
      "Epoch 179, Loss: 0.46412214801342555\n",
      "Epoch 180, Loss: 0.46867384291927394\n",
      "Epoch 181, Loss: 0.46316269230838986\n",
      "Epoch 182, Loss: 0.4619791093960296\n",
      "Epoch 183, Loss: 0.4650783107301587\n",
      "Epoch 184, Loss: 0.4617210775403838\n",
      "Epoch 185, Loss: 0.4621011464022392\n",
      "Epoch 186, Loss: 0.4641367239628427\n",
      "Epoch 187, Loss: 0.46071131054180564\n",
      "Epoch 188, Loss: 0.46145892957673257\n",
      "Epoch 189, Loss: 0.464304908222076\n",
      "Epoch 190, Loss: 0.4606605441278432\n",
      "Epoch 191, Loss: 0.4601386308130386\n",
      "Epoch 192, Loss: 0.46193430495315263\n",
      "Epoch 193, Loss: 0.45977696912799226\n",
      "Epoch 194, Loss: 0.4597173803227511\n",
      "Epoch 195, Loss: 0.46096589922299847\n",
      "Epoch 196, Loss: 0.4592170612502741\n",
      "Epoch 197, Loss: 0.45864347419013624\n",
      "Epoch 198, Loss: 0.4594650511157848\n",
      "Epoch 199, Loss: 0.45924705942486493\n",
      "Epoch 200, Loss: 0.4584931199827434\n",
      "Epoch 201, Loss: 0.4578552480015059\n",
      "Epoch 202, Loss: 0.458251953125\n",
      "Epoch 203, Loss: 0.4586848316367108\n",
      "Epoch 204, Loss: 0.4577025349799131\n",
      "Epoch 205, Loss: 0.45707273702530193\n",
      "Epoch 206, Loss: 0.4571336158814618\n",
      "Epoch 207, Loss: 0.45746758002181026\n",
      "Epoch 208, Loss: 0.4580334166179955\n",
      "Epoch 209, Loss: 0.457551138813026\n",
      "Epoch 210, Loss: 0.4572461918817771\n",
      "Epoch 211, Loss: 0.45629613394883295\n",
      "Epoch 212, Loss: 0.4557945440718634\n",
      "Epoch 213, Loss: 0.455843155547024\n",
      "Epoch 214, Loss: 0.456219547378948\n",
      "Epoch 215, Loss: 0.4577116995236092\n",
      "Epoch 216, Loss: 0.4589176697652806\n",
      "Epoch 217, Loss: 0.46105034303609993\n",
      "Epoch 218, Loss: 0.4567326442394303\n",
      "Epoch 219, Loss: 0.45493112899589366\n",
      "Epoch 220, Loss: 0.45527199475335134\n",
      "Epoch 221, Loss: 0.45623630281032773\n",
      "Epoch 222, Loss: 0.45802054967026307\n",
      "Epoch 223, Loss: 0.45677224995978094\n",
      "Epoch 224, Loss: 0.45565914342704716\n",
      "Epoch 225, Loss: 0.45401674524485375\n",
      "Epoch 226, Loss: 0.4535057342791329\n",
      "Epoch 227, Loss: 0.45416164205845616\n",
      "Epoch 228, Loss: 0.45450513400796655\n",
      "Epoch 229, Loss: 0.4548238117351997\n",
      "Epoch 230, Loss: 0.4537990916311975\n",
      "Epoch 231, Loss: 0.4532521496977959\n",
      "Epoch 232, Loss: 0.4526084553640119\n",
      "Epoch 233, Loss: 0.45228981460773116\n",
      "Epoch 234, Loss: 0.4520949132230242\n",
      "Epoch 235, Loss: 0.45205797975469736\n",
      "Epoch 236, Loss: 0.45230557901617613\n",
      "Epoch 237, Loss: 0.4528345953247846\n",
      "Epoch 238, Loss: 0.4549386133976144\n",
      "Epoch 239, Loss: 0.4564022887834084\n",
      "Epoch 240, Loss: 0.45805755540321125\n",
      "Epoch 241, Loss: 0.4541498302115434\n",
      "Epoch 242, Loss: 0.45158727109343993\n",
      "Epoch 243, Loss: 0.45105347173746146\n",
      "Epoch 244, Loss: 0.45263971453254503\n",
      "Epoch 245, Loss: 0.4550743757756552\n",
      "Epoch 246, Loss: 0.4549471294161167\n",
      "Epoch 247, Loss: 0.45361945699341477\n",
      "Epoch 248, Loss: 0.4506913801287277\n",
      "Epoch 249, Loss: 0.44996455105922556\n",
      "Epoch 250, Loss: 0.45070657227233163\n",
      "Epoch 251, Loss: 0.45134097806864715\n",
      "Epoch 252, Loss: 0.4518641160838718\n",
      "Epoch 253, Loss: 0.45124870735997746\n",
      "Epoch 254, Loss: 0.4507253179113287\n",
      "Epoch 255, Loss: 0.44965336548788515\n",
      "Epoch 256, Loss: 0.4490457284512466\n",
      "Epoch 257, Loss: 0.44867439459283065\n",
      "Epoch 258, Loss: 0.4485724236213288\n",
      "Epoch 259, Loss: 0.4487178165615502\n",
      "Epoch 260, Loss: 0.44913423821312026\n",
      "Epoch 261, Loss: 0.45068397397968624\n",
      "Epoch 262, Loss: 0.4523160224282965\n",
      "Epoch 263, Loss: 0.4554805695192658\n",
      "Epoch 264, Loss: 0.4521284819909996\n",
      "Epoch 265, Loss: 0.4491460824324\n",
      "Epoch 266, Loss: 0.4475917876267889\n",
      "Epoch 267, Loss: 0.4489836201844808\n",
      "Epoch 268, Loss: 0.4522696841254308\n",
      "Epoch 269, Loss: 0.45230492011773493\n",
      "Epoch 270, Loss: 0.45030259118174343\n",
      "Epoch 271, Loss: 0.44750189058874124\n",
      "Epoch 272, Loss: 0.4466950387656147\n",
      "Epoch 273, Loss: 0.44782488644087964\n",
      "Epoch 274, Loss: 0.4490013591686314\n",
      "Epoch 275, Loss: 0.44932258069431275\n",
      "Epoch 276, Loss: 0.4477355520631983\n",
      "Epoch 277, Loss: 0.4464409066948383\n",
      "Epoch 278, Loss: 0.4458417591594104\n",
      "Epoch 279, Loss: 0.4457414301360274\n",
      "Epoch 280, Loss: 0.4460887501130773\n",
      "Epoch 281, Loss: 0.44690611604251484\n",
      "Epoch 282, Loss: 0.4483696750030082\n",
      "Epoch 283, Loss: 0.4491296928695683\n",
      "Epoch 284, Loss: 0.4498625247292945\n",
      "Epoch 285, Loss: 0.44791530084032105\n",
      "Epoch 286, Loss: 0.4460872803334146\n",
      "Epoch 287, Loss: 0.44485744850497494\n",
      "Epoch 288, Loss: 0.4448821514773518\n",
      "Epoch 289, Loss: 0.4460718473545706\n",
      "Epoch 290, Loss: 0.4475995445619453\n",
      "Epoch 291, Loss: 0.44939647973391844\n",
      "Epoch 292, Loss: 0.44907386765397006\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23640228132856053\n",
      "Valence RMSE: 0.24825174239178566\n",
      "Arousal RMSE: 0.22392666128475686\n",
      "Test R^2 score: tensor([0.3414, 0.6302], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.48581565849619135\n",
      "Num of epochs: 293\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Epoch 121, Loss: 0.4816228295571127\n",
      "Epoch 122, Loss: 0.4807618495484584\n",
      "Epoch 123, Loss: 0.48102030846138955\n",
      "Epoch 124, Loss: 0.4810176288364116\n",
      "Epoch 125, Loss: 0.48014338261661127\n",
      "Epoch 126, Loss: 0.47910314809270116\n",
      "Epoch 127, Loss: 0.47880795949683447\n",
      "Epoch 128, Loss: 0.47876177315142754\n",
      "Epoch 129, Loss: 0.47819597116449997\n",
      "Epoch 130, Loss: 0.47743214539542544\n",
      "Epoch 131, Loss: 0.47658476230340185\n",
      "Epoch 132, Loss: 0.4759644680184978\n",
      "Epoch 133, Loss: 0.4755635187712421\n",
      "Epoch 134, Loss: 0.47538264196584307\n",
      "Epoch 135, Loss: 0.4753673450596914\n",
      "Epoch 136, Loss: 0.4753697744168057\n",
      "Epoch 137, Loss: 0.4755973109583155\n",
      "Epoch 138, Loss: 0.4767626826605241\n",
      "Epoch 139, Loss: 0.47683528222287225\n",
      "Epoch 140, Loss: 0.47579445468261566\n",
      "Epoch 141, Loss: 0.47284533919249566\n",
      "Epoch 142, Loss: 0.472809286012082\n",
      "Epoch 143, Loss: 0.47458738111232796\n",
      "Epoch 144, Loss: 0.4733380058918715\n",
      "Epoch 145, Loss: 0.4715229034484008\n",
      "Epoch 146, Loss: 0.47096730966267386\n",
      "Epoch 147, Loss: 0.47161469890467383\n",
      "Epoch 148, Loss: 0.47183531411129226\n",
      "Epoch 149, Loss: 0.470294933688076\n",
      "Epoch 150, Loss: 0.4697114144973765\n",
      "Epoch 151, Loss: 0.47018789053701304\n",
      "Epoch 152, Loss: 0.4700460953124117\n",
      "Epoch 153, Loss: 0.4693645740016508\n",
      "Epoch 154, Loss: 0.4684558104180481\n",
      "Epoch 155, Loss: 0.4680960384979934\n",
      "Epoch 156, Loss: 0.4682454254529793\n",
      "Epoch 157, Loss: 0.4682974379105488\n",
      "Epoch 158, Loss: 0.4681590010361841\n",
      "Epoch 159, Loss: 0.4673225124817593\n",
      "Epoch 160, Loss: 0.4666701428107949\n",
      "Epoch 161, Loss: 0.4663166784414499\n",
      "Epoch 162, Loss: 0.46624854535037\n",
      "Epoch 163, Loss: 0.4664611566834947\n",
      "Epoch 164, Loss: 0.46654532450127006\n",
      "Epoch 165, Loss: 0.46721845607201035\n",
      "Epoch 166, Loss: 0.4668208160006968\n",
      "Epoch 167, Loss: 0.466573749669782\n",
      "Epoch 168, Loss: 0.46506387643568103\n",
      "Epoch 169, Loss: 0.4641220516950567\n",
      "Epoch 170, Loss: 0.4644550172656552\n",
      "Epoch 171, Loss: 0.4654613047116713\n",
      "Epoch 172, Loss: 0.46880176576246846\n",
      "Epoch 173, Loss: 0.4685649824581709\n",
      "Epoch 174, Loss: 0.46606115996454733\n",
      "Epoch 175, Loss: 0.46328263242364126\n",
      "Epoch 176, Loss: 0.4660484187313888\n",
      "Epoch 177, Loss: 0.4667669789465077\n",
      "Epoch 178, Loss: 0.46222975993575777\n",
      "Epoch 179, Loss: 0.46412214801342555\n",
      "Epoch 180, Loss: 0.46867384291927394\n",
      "Epoch 181, Loss: 0.46316269230838986\n",
      "Epoch 182, Loss: 0.4619791093960296\n",
      "Epoch 183, Loss: 0.4650783107301587\n",
      "Epoch 184, Loss: 0.4617210775403838\n",
      "Epoch 185, Loss: 0.4621011464022392\n",
      "Epoch 186, Loss: 0.4641367239628427\n",
      "Epoch 187, Loss: 0.46071131054180564\n",
      "Epoch 188, Loss: 0.46145892957673257\n",
      "Epoch 189, Loss: 0.464304908222076\n",
      "Epoch 190, Loss: 0.4606605441278432\n",
      "Epoch 191, Loss: 0.4601386308130386\n",
      "Epoch 192, Loss: 0.46193430495315263\n",
      "Epoch 193, Loss: 0.45977696912799226\n",
      "Epoch 194, Loss: 0.4597173803227511\n",
      "Epoch 195, Loss: 0.46096589922299847\n",
      "Epoch 196, Loss: 0.4592170612502741\n",
      "Epoch 197, Loss: 0.45864347419013624\n",
      "Epoch 198, Loss: 0.4594650511157848\n",
      "Epoch 199, Loss: 0.45924705942486493\n",
      "Epoch 200, Loss: 0.4584931199827434\n",
      "Epoch 201, Loss: 0.4578552480015059\n",
      "Epoch 202, Loss: 0.458251953125\n",
      "Epoch 203, Loss: 0.4586848316367108\n",
      "Epoch 204, Loss: 0.4577025349799131\n",
      "Epoch 205, Loss: 0.45707273702530193\n",
      "Epoch 206, Loss: 0.4571336158814618\n",
      "Epoch 207, Loss: 0.45746758002181026\n",
      "Epoch 208, Loss: 0.4580334166179955\n",
      "Epoch 209, Loss: 0.457551138813026\n",
      "Epoch 210, Loss: 0.4572461918817771\n",
      "Epoch 211, Loss: 0.45629613394883295\n",
      "Epoch 212, Loss: 0.4557945440718634\n",
      "Epoch 213, Loss: 0.455843155547024\n",
      "Epoch 214, Loss: 0.456219547378948\n",
      "Epoch 215, Loss: 0.4577116995236092\n",
      "Epoch 216, Loss: 0.4589176697652806\n",
      "Epoch 217, Loss: 0.46105034303609993\n",
      "Epoch 218, Loss: 0.4567326442394303\n",
      "Epoch 219, Loss: 0.45493112899589366\n",
      "Epoch 220, Loss: 0.45527199475335134\n",
      "Epoch 221, Loss: 0.45623630281032773\n",
      "Epoch 222, Loss: 0.45802054967026307\n",
      "Epoch 223, Loss: 0.45677224995978094\n",
      "Epoch 224, Loss: 0.45565914342704716\n",
      "Epoch 225, Loss: 0.45401674524485375\n",
      "Epoch 226, Loss: 0.4535057342791329\n",
      "Epoch 227, Loss: 0.45416164205845616\n",
      "Epoch 228, Loss: 0.45450513400796655\n",
      "Epoch 229, Loss: 0.4548238117351997\n",
      "Epoch 230, Loss: 0.4537990916311975\n",
      "Epoch 231, Loss: 0.4532521496977959\n",
      "Epoch 232, Loss: 0.4526084553640119\n",
      "Epoch 233, Loss: 0.45228981460773116\n",
      "Epoch 234, Loss: 0.4520949132230242\n",
      "Epoch 235, Loss: 0.45205797975469736\n",
      "Epoch 236, Loss: 0.45230557901617613\n",
      "Epoch 237, Loss: 0.4528345953247846\n",
      "Epoch 238, Loss: 0.4549386133976144\n",
      "Epoch 239, Loss: 0.4564022887834084\n",
      "Epoch 240, Loss: 0.45805755540321125\n",
      "Epoch 241, Loss: 0.4541498302115434\n",
      "Epoch 242, Loss: 0.45158727109343993\n",
      "Epoch 243, Loss: 0.45105347173746146\n",
      "Epoch 244, Loss: 0.45263971453254503\n",
      "Epoch 245, Loss: 0.4550743757756552\n",
      "Epoch 246, Loss: 0.4549471294161167\n",
      "Epoch 247, Loss: 0.45361945699341477\n",
      "Epoch 248, Loss: 0.4506913801287277\n",
      "Epoch 249, Loss: 0.44996455105922556\n",
      "Epoch 250, Loss: 0.45070657227233163\n",
      "Epoch 251, Loss: 0.45134097806864715\n",
      "Epoch 252, Loss: 0.4518641160838718\n",
      "Epoch 253, Loss: 0.45124870735997746\n",
      "Epoch 254, Loss: 0.4507253179113287\n",
      "Epoch 255, Loss: 0.44965336548788515\n",
      "Epoch 256, Loss: 0.4490457284512466\n",
      "Epoch 257, Loss: 0.44867439459283065\n",
      "Epoch 258, Loss: 0.4485724236213288\n",
      "Epoch 259, Loss: 0.4487178165615502\n",
      "Epoch 260, Loss: 0.44913423821312026\n",
      "Epoch 261, Loss: 0.45068397397968624\n",
      "Epoch 262, Loss: 0.4523160224282965\n",
      "Epoch 263, Loss: 0.4554805695192658\n",
      "Epoch 264, Loss: 0.4521284819909996\n",
      "Epoch 265, Loss: 0.4491460824324\n",
      "Epoch 266, Loss: 0.4475917876267889\n",
      "Epoch 267, Loss: 0.4489836201844808\n",
      "Epoch 268, Loss: 0.4522696841254308\n",
      "Epoch 269, Loss: 0.45230492011773493\n",
      "Epoch 270, Loss: 0.45030259118174343\n",
      "Epoch 271, Loss: 0.44750189058874124\n",
      "Epoch 272, Loss: 0.4466950387656147\n",
      "Epoch 273, Loss: 0.44782488644087964\n",
      "Epoch 274, Loss: 0.4490013591686314\n",
      "Epoch 275, Loss: 0.44932258069431275\n",
      "Epoch 276, Loss: 0.4477355520631983\n",
      "Epoch 277, Loss: 0.4464409066948383\n",
      "Epoch 278, Loss: 0.4458417591594104\n",
      "Epoch 279, Loss: 0.4457414301360274\n",
      "Epoch 280, Loss: 0.4460887501130773\n",
      "Epoch 281, Loss: 0.44690611604251484\n",
      "Epoch 282, Loss: 0.4483696750030082\n",
      "Epoch 283, Loss: 0.4491296928695683\n",
      "Epoch 284, Loss: 0.4498625247292945\n",
      "Epoch 285, Loss: 0.44791530084032105\n",
      "Epoch 286, Loss: 0.4460872803334146\n",
      "Epoch 287, Loss: 0.44485744850497494\n",
      "Epoch 288, Loss: 0.4448821514773518\n",
      "Epoch 289, Loss: 0.4460718473545706\n",
      "Epoch 290, Loss: 0.4475995445619453\n",
      "Epoch 291, Loss: 0.44939647973391844\n",
      "Epoch 292, Loss: 0.44907386765397006\n",
      "Epoch 293, Loss: 0.4480021879332198\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23377592008050743\n",
      "Valence RMSE: 0.24190282334452098\n",
      "Arousal RMSE: 0.2253561307728838\n",
      "Test R^2 score: tensor([0.3747, 0.6255], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.5000745410296419\n",
      "Num of epochs: 294\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Epoch 121, Loss: 0.4816228295571127\n",
      "Epoch 122, Loss: 0.4807618495484584\n",
      "Epoch 123, Loss: 0.48102030846138955\n",
      "Epoch 124, Loss: 0.4810176288364116\n",
      "Epoch 125, Loss: 0.48014338261661127\n",
      "Epoch 126, Loss: 0.47910314809270116\n",
      "Epoch 127, Loss: 0.47880795949683447\n",
      "Epoch 128, Loss: 0.47876177315142754\n",
      "Epoch 129, Loss: 0.47819597116449997\n",
      "Epoch 130, Loss: 0.47743214539542544\n",
      "Epoch 131, Loss: 0.47658476230340185\n",
      "Epoch 132, Loss: 0.4759644680184978\n",
      "Epoch 133, Loss: 0.4755635187712421\n",
      "Epoch 134, Loss: 0.47538264196584307\n",
      "Epoch 135, Loss: 0.4753673450596914\n",
      "Epoch 136, Loss: 0.4753697744168057\n",
      "Epoch 137, Loss: 0.4755973109583155\n",
      "Epoch 138, Loss: 0.4767626826605241\n",
      "Epoch 139, Loss: 0.47683528222287225\n",
      "Epoch 140, Loss: 0.47579445468261566\n",
      "Epoch 141, Loss: 0.47284533919249566\n",
      "Epoch 142, Loss: 0.472809286012082\n",
      "Epoch 143, Loss: 0.47458738111232796\n",
      "Epoch 144, Loss: 0.4733380058918715\n",
      "Epoch 145, Loss: 0.4715229034484008\n",
      "Epoch 146, Loss: 0.47096730966267386\n",
      "Epoch 147, Loss: 0.47161469890467383\n",
      "Epoch 148, Loss: 0.47183531411129226\n",
      "Epoch 149, Loss: 0.470294933688076\n",
      "Epoch 150, Loss: 0.4697114144973765\n",
      "Epoch 151, Loss: 0.47018789053701304\n",
      "Epoch 152, Loss: 0.4700460953124117\n",
      "Epoch 153, Loss: 0.4693645740016508\n",
      "Epoch 154, Loss: 0.4684558104180481\n",
      "Epoch 155, Loss: 0.4680960384979934\n",
      "Epoch 156, Loss: 0.4682454254529793\n",
      "Epoch 157, Loss: 0.4682974379105488\n",
      "Epoch 158, Loss: 0.4681590010361841\n",
      "Epoch 159, Loss: 0.4673225124817593\n",
      "Epoch 160, Loss: 0.4666701428107949\n",
      "Epoch 161, Loss: 0.4663166784414499\n",
      "Epoch 162, Loss: 0.46624854535037\n",
      "Epoch 163, Loss: 0.4664611566834947\n",
      "Epoch 164, Loss: 0.46654532450127006\n",
      "Epoch 165, Loss: 0.46721845607201035\n",
      "Epoch 166, Loss: 0.4668208160006968\n",
      "Epoch 167, Loss: 0.466573749669782\n",
      "Epoch 168, Loss: 0.46506387643568103\n",
      "Epoch 169, Loss: 0.4641220516950567\n",
      "Epoch 170, Loss: 0.4644550172656552\n",
      "Epoch 171, Loss: 0.4654613047116713\n",
      "Epoch 172, Loss: 0.46880176576246846\n",
      "Epoch 173, Loss: 0.4685649824581709\n",
      "Epoch 174, Loss: 0.46606115996454733\n",
      "Epoch 175, Loss: 0.46328263242364126\n",
      "Epoch 176, Loss: 0.4660484187313888\n",
      "Epoch 177, Loss: 0.4667669789465077\n",
      "Epoch 178, Loss: 0.46222975993575777\n",
      "Epoch 179, Loss: 0.46412214801342555\n",
      "Epoch 180, Loss: 0.46867384291927394\n",
      "Epoch 181, Loss: 0.46316269230838986\n",
      "Epoch 182, Loss: 0.4619791093960296\n",
      "Epoch 183, Loss: 0.4650783107301587\n",
      "Epoch 184, Loss: 0.4617210775403838\n",
      "Epoch 185, Loss: 0.4621011464022392\n",
      "Epoch 186, Loss: 0.4641367239628427\n",
      "Epoch 187, Loss: 0.46071131054180564\n",
      "Epoch 188, Loss: 0.46145892957673257\n",
      "Epoch 189, Loss: 0.464304908222076\n",
      "Epoch 190, Loss: 0.4606605441278432\n",
      "Epoch 191, Loss: 0.4601386308130386\n",
      "Epoch 192, Loss: 0.46193430495315263\n",
      "Epoch 193, Loss: 0.45977696912799226\n",
      "Epoch 194, Loss: 0.4597173803227511\n",
      "Epoch 195, Loss: 0.46096589922299847\n",
      "Epoch 196, Loss: 0.4592170612502741\n",
      "Epoch 197, Loss: 0.45864347419013624\n",
      "Epoch 198, Loss: 0.4594650511157848\n",
      "Epoch 199, Loss: 0.45924705942486493\n",
      "Epoch 200, Loss: 0.4584931199827434\n",
      "Epoch 201, Loss: 0.4578552480015059\n",
      "Epoch 202, Loss: 0.458251953125\n",
      "Epoch 203, Loss: 0.4586848316367108\n",
      "Epoch 204, Loss: 0.4577025349799131\n",
      "Epoch 205, Loss: 0.45707273702530193\n",
      "Epoch 206, Loss: 0.4571336158814618\n",
      "Epoch 207, Loss: 0.45746758002181026\n",
      "Epoch 208, Loss: 0.4580334166179955\n",
      "Epoch 209, Loss: 0.457551138813026\n",
      "Epoch 210, Loss: 0.4572461918817771\n",
      "Epoch 211, Loss: 0.45629613394883295\n",
      "Epoch 212, Loss: 0.4557945440718634\n",
      "Epoch 213, Loss: 0.455843155547024\n",
      "Epoch 214, Loss: 0.456219547378948\n",
      "Epoch 215, Loss: 0.4577116995236092\n",
      "Epoch 216, Loss: 0.4589176697652806\n",
      "Epoch 217, Loss: 0.46105034303609993\n",
      "Epoch 218, Loss: 0.4567326442394303\n",
      "Epoch 219, Loss: 0.45493112899589366\n",
      "Epoch 220, Loss: 0.45527199475335134\n",
      "Epoch 221, Loss: 0.45623630281032773\n",
      "Epoch 222, Loss: 0.45802054967026307\n",
      "Epoch 223, Loss: 0.45677224995978094\n",
      "Epoch 224, Loss: 0.45565914342704716\n",
      "Epoch 225, Loss: 0.45401674524485375\n",
      "Epoch 226, Loss: 0.4535057342791329\n",
      "Epoch 227, Loss: 0.45416164205845616\n",
      "Epoch 228, Loss: 0.45450513400796655\n",
      "Epoch 229, Loss: 0.4548238117351997\n",
      "Epoch 230, Loss: 0.4537990916311975\n",
      "Epoch 231, Loss: 0.4532521496977959\n",
      "Epoch 232, Loss: 0.4526084553640119\n",
      "Epoch 233, Loss: 0.45228981460773116\n",
      "Epoch 234, Loss: 0.4520949132230242\n",
      "Epoch 235, Loss: 0.45205797975469736\n",
      "Epoch 236, Loss: 0.45230557901617613\n",
      "Epoch 237, Loss: 0.4528345953247846\n",
      "Epoch 238, Loss: 0.4549386133976144\n",
      "Epoch 239, Loss: 0.4564022887834084\n",
      "Epoch 240, Loss: 0.45805755540321125\n",
      "Epoch 241, Loss: 0.4541498302115434\n",
      "Epoch 242, Loss: 0.45158727109343993\n",
      "Epoch 243, Loss: 0.45105347173746146\n",
      "Epoch 244, Loss: 0.45263971453254503\n",
      "Epoch 245, Loss: 0.4550743757756552\n",
      "Epoch 246, Loss: 0.4549471294161167\n",
      "Epoch 247, Loss: 0.45361945699341477\n",
      "Epoch 248, Loss: 0.4506913801287277\n",
      "Epoch 249, Loss: 0.44996455105922556\n",
      "Epoch 250, Loss: 0.45070657227233163\n",
      "Epoch 251, Loss: 0.45134097806864715\n",
      "Epoch 252, Loss: 0.4518641160838718\n",
      "Epoch 253, Loss: 0.45124870735997746\n",
      "Epoch 254, Loss: 0.4507253179113287\n",
      "Epoch 255, Loss: 0.44965336548788515\n",
      "Epoch 256, Loss: 0.4490457284512466\n",
      "Epoch 257, Loss: 0.44867439459283065\n",
      "Epoch 258, Loss: 0.4485724236213288\n",
      "Epoch 259, Loss: 0.4487178165615502\n",
      "Epoch 260, Loss: 0.44913423821312026\n",
      "Epoch 261, Loss: 0.45068397397968624\n",
      "Epoch 262, Loss: 0.4523160224282965\n",
      "Epoch 263, Loss: 0.4554805695192658\n",
      "Epoch 264, Loss: 0.4521284819909996\n",
      "Epoch 265, Loss: 0.4491460824324\n",
      "Epoch 266, Loss: 0.4475917876267889\n",
      "Epoch 267, Loss: 0.4489836201844808\n",
      "Epoch 268, Loss: 0.4522696841254308\n",
      "Epoch 269, Loss: 0.45230492011773493\n",
      "Epoch 270, Loss: 0.45030259118174343\n",
      "Epoch 271, Loss: 0.44750189058874124\n",
      "Epoch 272, Loss: 0.4466950387656147\n",
      "Epoch 273, Loss: 0.44782488644087964\n",
      "Epoch 274, Loss: 0.4490013591686314\n",
      "Epoch 275, Loss: 0.44932258069431275\n",
      "Epoch 276, Loss: 0.4477355520631983\n",
      "Epoch 277, Loss: 0.4464409066948383\n",
      "Epoch 278, Loss: 0.4458417591594104\n",
      "Epoch 279, Loss: 0.4457414301360274\n",
      "Epoch 280, Loss: 0.4460887501130773\n",
      "Epoch 281, Loss: 0.44690611604251484\n",
      "Epoch 282, Loss: 0.4483696750030082\n",
      "Epoch 283, Loss: 0.4491296928695683\n",
      "Epoch 284, Loss: 0.4498625247292945\n",
      "Epoch 285, Loss: 0.44791530084032105\n",
      "Epoch 286, Loss: 0.4460872803334146\n",
      "Epoch 287, Loss: 0.44485744850497494\n",
      "Epoch 288, Loss: 0.4448821514773518\n",
      "Epoch 289, Loss: 0.4460718473545706\n",
      "Epoch 290, Loss: 0.4475995445619453\n",
      "Epoch 291, Loss: 0.44939647973391844\n",
      "Epoch 292, Loss: 0.44907386765397006\n",
      "Epoch 293, Loss: 0.4480021879332198\n",
      "Epoch 294, Loss: 0.44517745931865826\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2332381147157868\n",
      "Valence RMSE: 0.24267499277215512\n",
      "Arousal RMSE: 0.2234029637121752\n",
      "Test R^2 score: tensor([0.3707, 0.6319], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.5013074040747145\n",
      "Num of epochs: 295\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Epoch 121, Loss: 0.4816228295571127\n",
      "Epoch 122, Loss: 0.4807618495484584\n",
      "Epoch 123, Loss: 0.48102030846138955\n",
      "Epoch 124, Loss: 0.4810176288364116\n",
      "Epoch 125, Loss: 0.48014338261661127\n",
      "Epoch 126, Loss: 0.47910314809270116\n",
      "Epoch 127, Loss: 0.47880795949683447\n",
      "Epoch 128, Loss: 0.47876177315142754\n",
      "Epoch 129, Loss: 0.47819597116449997\n",
      "Epoch 130, Loss: 0.47743214539542544\n",
      "Epoch 131, Loss: 0.47658476230340185\n",
      "Epoch 132, Loss: 0.4759644680184978\n",
      "Epoch 133, Loss: 0.4755635187712421\n",
      "Epoch 134, Loss: 0.47538264196584307\n",
      "Epoch 135, Loss: 0.4753673450596914\n",
      "Epoch 136, Loss: 0.4753697744168057\n",
      "Epoch 137, Loss: 0.4755973109583155\n",
      "Epoch 138, Loss: 0.4767626826605241\n",
      "Epoch 139, Loss: 0.47683528222287225\n",
      "Epoch 140, Loss: 0.47579445468261566\n",
      "Epoch 141, Loss: 0.47284533919249566\n",
      "Epoch 142, Loss: 0.472809286012082\n",
      "Epoch 143, Loss: 0.47458738111232796\n",
      "Epoch 144, Loss: 0.4733380058918715\n",
      "Epoch 145, Loss: 0.4715229034484008\n",
      "Epoch 146, Loss: 0.47096730966267386\n",
      "Epoch 147, Loss: 0.47161469890467383\n",
      "Epoch 148, Loss: 0.47183531411129226\n",
      "Epoch 149, Loss: 0.470294933688076\n",
      "Epoch 150, Loss: 0.4697114144973765\n",
      "Epoch 151, Loss: 0.47018789053701304\n",
      "Epoch 152, Loss: 0.4700460953124117\n",
      "Epoch 153, Loss: 0.4693645740016508\n",
      "Epoch 154, Loss: 0.4684558104180481\n",
      "Epoch 155, Loss: 0.4680960384979934\n",
      "Epoch 156, Loss: 0.4682454254529793\n",
      "Epoch 157, Loss: 0.4682974379105488\n",
      "Epoch 158, Loss: 0.4681590010361841\n",
      "Epoch 159, Loss: 0.4673225124817593\n",
      "Epoch 160, Loss: 0.4666701428107949\n",
      "Epoch 161, Loss: 0.4663166784414499\n",
      "Epoch 162, Loss: 0.46624854535037\n",
      "Epoch 163, Loss: 0.4664611566834947\n",
      "Epoch 164, Loss: 0.46654532450127006\n",
      "Epoch 165, Loss: 0.46721845607201035\n",
      "Epoch 166, Loss: 0.4668208160006968\n",
      "Epoch 167, Loss: 0.466573749669782\n",
      "Epoch 168, Loss: 0.46506387643568103\n",
      "Epoch 169, Loss: 0.4641220516950567\n",
      "Epoch 170, Loss: 0.4644550172656552\n",
      "Epoch 171, Loss: 0.4654613047116713\n",
      "Epoch 172, Loss: 0.46880176576246846\n",
      "Epoch 173, Loss: 0.4685649824581709\n",
      "Epoch 174, Loss: 0.46606115996454733\n",
      "Epoch 175, Loss: 0.46328263242364126\n",
      "Epoch 176, Loss: 0.4660484187313888\n",
      "Epoch 177, Loss: 0.4667669789465077\n",
      "Epoch 178, Loss: 0.46222975993575777\n",
      "Epoch 179, Loss: 0.46412214801342555\n",
      "Epoch 180, Loss: 0.46867384291927394\n",
      "Epoch 181, Loss: 0.46316269230838986\n",
      "Epoch 182, Loss: 0.4619791093960296\n",
      "Epoch 183, Loss: 0.4650783107301587\n",
      "Epoch 184, Loss: 0.4617210775403838\n",
      "Epoch 185, Loss: 0.4621011464022392\n",
      "Epoch 186, Loss: 0.4641367239628427\n",
      "Epoch 187, Loss: 0.46071131054180564\n",
      "Epoch 188, Loss: 0.46145892957673257\n",
      "Epoch 189, Loss: 0.464304908222076\n",
      "Epoch 190, Loss: 0.4606605441278432\n",
      "Epoch 191, Loss: 0.4601386308130386\n",
      "Epoch 192, Loss: 0.46193430495315263\n",
      "Epoch 193, Loss: 0.45977696912799226\n",
      "Epoch 194, Loss: 0.4597173803227511\n",
      "Epoch 195, Loss: 0.46096589922299847\n",
      "Epoch 196, Loss: 0.4592170612502741\n",
      "Epoch 197, Loss: 0.45864347419013624\n",
      "Epoch 198, Loss: 0.4594650511157848\n",
      "Epoch 199, Loss: 0.45924705942486493\n",
      "Epoch 200, Loss: 0.4584931199827434\n",
      "Epoch 201, Loss: 0.4578552480015059\n",
      "Epoch 202, Loss: 0.458251953125\n",
      "Epoch 203, Loss: 0.4586848316367108\n",
      "Epoch 204, Loss: 0.4577025349799131\n",
      "Epoch 205, Loss: 0.45707273702530193\n",
      "Epoch 206, Loss: 0.4571336158814618\n",
      "Epoch 207, Loss: 0.45746758002181026\n",
      "Epoch 208, Loss: 0.4580334166179955\n",
      "Epoch 209, Loss: 0.457551138813026\n",
      "Epoch 210, Loss: 0.4572461918817771\n",
      "Epoch 211, Loss: 0.45629613394883295\n",
      "Epoch 212, Loss: 0.4557945440718634\n",
      "Epoch 213, Loss: 0.455843155547024\n",
      "Epoch 214, Loss: 0.456219547378948\n",
      "Epoch 215, Loss: 0.4577116995236092\n",
      "Epoch 216, Loss: 0.4589176697652806\n",
      "Epoch 217, Loss: 0.46105034303609993\n",
      "Epoch 218, Loss: 0.4567326442394303\n",
      "Epoch 219, Loss: 0.45493112899589366\n",
      "Epoch 220, Loss: 0.45527199475335134\n",
      "Epoch 221, Loss: 0.45623630281032773\n",
      "Epoch 222, Loss: 0.45802054967026307\n",
      "Epoch 223, Loss: 0.45677224995978094\n",
      "Epoch 224, Loss: 0.45565914342704716\n",
      "Epoch 225, Loss: 0.45401674524485375\n",
      "Epoch 226, Loss: 0.4535057342791329\n",
      "Epoch 227, Loss: 0.45416164205845616\n",
      "Epoch 228, Loss: 0.45450513400796655\n",
      "Epoch 229, Loss: 0.4548238117351997\n",
      "Epoch 230, Loss: 0.4537990916311975\n",
      "Epoch 231, Loss: 0.4532521496977959\n",
      "Epoch 232, Loss: 0.4526084553640119\n",
      "Epoch 233, Loss: 0.45228981460773116\n",
      "Epoch 234, Loss: 0.4520949132230242\n",
      "Epoch 235, Loss: 0.45205797975469736\n",
      "Epoch 236, Loss: 0.45230557901617613\n",
      "Epoch 237, Loss: 0.4528345953247846\n",
      "Epoch 238, Loss: 0.4549386133976144\n",
      "Epoch 239, Loss: 0.4564022887834084\n",
      "Epoch 240, Loss: 0.45805755540321125\n",
      "Epoch 241, Loss: 0.4541498302115434\n",
      "Epoch 242, Loss: 0.45158727109343993\n",
      "Epoch 243, Loss: 0.45105347173746146\n",
      "Epoch 244, Loss: 0.45263971453254503\n",
      "Epoch 245, Loss: 0.4550743757756552\n",
      "Epoch 246, Loss: 0.4549471294161167\n",
      "Epoch 247, Loss: 0.45361945699341477\n",
      "Epoch 248, Loss: 0.4506913801287277\n",
      "Epoch 249, Loss: 0.44996455105922556\n",
      "Epoch 250, Loss: 0.45070657227233163\n",
      "Epoch 251, Loss: 0.45134097806864715\n",
      "Epoch 252, Loss: 0.4518641160838718\n",
      "Epoch 253, Loss: 0.45124870735997746\n",
      "Epoch 254, Loss: 0.4507253179113287\n",
      "Epoch 255, Loss: 0.44965336548788515\n",
      "Epoch 256, Loss: 0.4490457284512466\n",
      "Epoch 257, Loss: 0.44867439459283065\n",
      "Epoch 258, Loss: 0.4485724236213288\n",
      "Epoch 259, Loss: 0.4487178165615502\n",
      "Epoch 260, Loss: 0.44913423821312026\n",
      "Epoch 261, Loss: 0.45068397397968624\n",
      "Epoch 262, Loss: 0.4523160224282965\n",
      "Epoch 263, Loss: 0.4554805695192658\n",
      "Epoch 264, Loss: 0.4521284819909996\n",
      "Epoch 265, Loss: 0.4491460824324\n",
      "Epoch 266, Loss: 0.4475917876267889\n",
      "Epoch 267, Loss: 0.4489836201844808\n",
      "Epoch 268, Loss: 0.4522696841254308\n",
      "Epoch 269, Loss: 0.45230492011773493\n",
      "Epoch 270, Loss: 0.45030259118174343\n",
      "Epoch 271, Loss: 0.44750189058874124\n",
      "Epoch 272, Loss: 0.4466950387656147\n",
      "Epoch 273, Loss: 0.44782488644087964\n",
      "Epoch 274, Loss: 0.4490013591686314\n",
      "Epoch 275, Loss: 0.44932258069431275\n",
      "Epoch 276, Loss: 0.4477355520631983\n",
      "Epoch 277, Loss: 0.4464409066948383\n",
      "Epoch 278, Loss: 0.4458417591594104\n",
      "Epoch 279, Loss: 0.4457414301360274\n",
      "Epoch 280, Loss: 0.4460887501130773\n",
      "Epoch 281, Loss: 0.44690611604251484\n",
      "Epoch 282, Loss: 0.4483696750030082\n",
      "Epoch 283, Loss: 0.4491296928695683\n",
      "Epoch 284, Loss: 0.4498625247292945\n",
      "Epoch 285, Loss: 0.44791530084032105\n",
      "Epoch 286, Loss: 0.4460872803334146\n",
      "Epoch 287, Loss: 0.44485744850497494\n",
      "Epoch 288, Loss: 0.4448821514773518\n",
      "Epoch 289, Loss: 0.4460718473545706\n",
      "Epoch 290, Loss: 0.4475995445619453\n",
      "Epoch 291, Loss: 0.44939647973391844\n",
      "Epoch 292, Loss: 0.44907386765397006\n",
      "Epoch 293, Loss: 0.4480021879332198\n",
      "Epoch 294, Loss: 0.44517745931865826\n",
      "Epoch 295, Loss: 0.44400204700028173\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.234155670140704\n",
      "Valence RMSE: 0.24445760031073221\n",
      "Arousal RMSE: 0.22337913369068885\n",
      "Test R^2 score: tensor([0.3614, 0.6320], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.49670699524312517\n",
      "Num of epochs: 296\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Epoch 121, Loss: 0.4816228295571127\n",
      "Epoch 122, Loss: 0.4807618495484584\n",
      "Epoch 123, Loss: 0.48102030846138955\n",
      "Epoch 124, Loss: 0.4810176288364116\n",
      "Epoch 125, Loss: 0.48014338261661127\n",
      "Epoch 126, Loss: 0.47910314809270116\n",
      "Epoch 127, Loss: 0.47880795949683447\n",
      "Epoch 128, Loss: 0.47876177315142754\n",
      "Epoch 129, Loss: 0.47819597116449997\n",
      "Epoch 130, Loss: 0.47743214539542544\n",
      "Epoch 131, Loss: 0.47658476230340185\n",
      "Epoch 132, Loss: 0.4759644680184978\n",
      "Epoch 133, Loss: 0.4755635187712421\n",
      "Epoch 134, Loss: 0.47538264196584307\n",
      "Epoch 135, Loss: 0.4753673450596914\n",
      "Epoch 136, Loss: 0.4753697744168057\n",
      "Epoch 137, Loss: 0.4755973109583155\n",
      "Epoch 138, Loss: 0.4767626826605241\n",
      "Epoch 139, Loss: 0.47683528222287225\n",
      "Epoch 140, Loss: 0.47579445468261566\n",
      "Epoch 141, Loss: 0.47284533919249566\n",
      "Epoch 142, Loss: 0.472809286012082\n",
      "Epoch 143, Loss: 0.47458738111232796\n",
      "Epoch 144, Loss: 0.4733380058918715\n",
      "Epoch 145, Loss: 0.4715229034484008\n",
      "Epoch 146, Loss: 0.47096730966267386\n",
      "Epoch 147, Loss: 0.47161469890467383\n",
      "Epoch 148, Loss: 0.47183531411129226\n",
      "Epoch 149, Loss: 0.470294933688076\n",
      "Epoch 150, Loss: 0.4697114144973765\n",
      "Epoch 151, Loss: 0.47018789053701304\n",
      "Epoch 152, Loss: 0.4700460953124117\n",
      "Epoch 153, Loss: 0.4693645740016508\n",
      "Epoch 154, Loss: 0.4684558104180481\n",
      "Epoch 155, Loss: 0.4680960384979934\n",
      "Epoch 156, Loss: 0.4682454254529793\n",
      "Epoch 157, Loss: 0.4682974379105488\n",
      "Epoch 158, Loss: 0.4681590010361841\n",
      "Epoch 159, Loss: 0.4673225124817593\n",
      "Epoch 160, Loss: 0.4666701428107949\n",
      "Epoch 161, Loss: 0.4663166784414499\n",
      "Epoch 162, Loss: 0.46624854535037\n",
      "Epoch 163, Loss: 0.4664611566834947\n",
      "Epoch 164, Loss: 0.46654532450127006\n",
      "Epoch 165, Loss: 0.46721845607201035\n",
      "Epoch 166, Loss: 0.4668208160006968\n",
      "Epoch 167, Loss: 0.466573749669782\n",
      "Epoch 168, Loss: 0.46506387643568103\n",
      "Epoch 169, Loss: 0.4641220516950567\n",
      "Epoch 170, Loss: 0.4644550172656552\n",
      "Epoch 171, Loss: 0.4654613047116713\n",
      "Epoch 172, Loss: 0.46880176576246846\n",
      "Epoch 173, Loss: 0.4685649824581709\n",
      "Epoch 174, Loss: 0.46606115996454733\n",
      "Epoch 175, Loss: 0.46328263242364126\n",
      "Epoch 176, Loss: 0.4660484187313888\n",
      "Epoch 177, Loss: 0.4667669789465077\n",
      "Epoch 178, Loss: 0.46222975993575777\n",
      "Epoch 179, Loss: 0.46412214801342555\n",
      "Epoch 180, Loss: 0.46867384291927394\n",
      "Epoch 181, Loss: 0.46316269230838986\n",
      "Epoch 182, Loss: 0.4619791093960296\n",
      "Epoch 183, Loss: 0.4650783107301587\n",
      "Epoch 184, Loss: 0.4617210775403838\n",
      "Epoch 185, Loss: 0.4621011464022392\n",
      "Epoch 186, Loss: 0.4641367239628427\n",
      "Epoch 187, Loss: 0.46071131054180564\n",
      "Epoch 188, Loss: 0.46145892957673257\n",
      "Epoch 189, Loss: 0.464304908222076\n",
      "Epoch 190, Loss: 0.4606605441278432\n",
      "Epoch 191, Loss: 0.4601386308130386\n",
      "Epoch 192, Loss: 0.46193430495315263\n",
      "Epoch 193, Loss: 0.45977696912799226\n",
      "Epoch 194, Loss: 0.4597173803227511\n",
      "Epoch 195, Loss: 0.46096589922299847\n",
      "Epoch 196, Loss: 0.4592170612502741\n",
      "Epoch 197, Loss: 0.45864347419013624\n",
      "Epoch 198, Loss: 0.4594650511157848\n",
      "Epoch 199, Loss: 0.45924705942486493\n",
      "Epoch 200, Loss: 0.4584931199827434\n",
      "Epoch 201, Loss: 0.4578552480015059\n",
      "Epoch 202, Loss: 0.458251953125\n",
      "Epoch 203, Loss: 0.4586848316367108\n",
      "Epoch 204, Loss: 0.4577025349799131\n",
      "Epoch 205, Loss: 0.45707273702530193\n",
      "Epoch 206, Loss: 0.4571336158814618\n",
      "Epoch 207, Loss: 0.45746758002181026\n",
      "Epoch 208, Loss: 0.4580334166179955\n",
      "Epoch 209, Loss: 0.457551138813026\n",
      "Epoch 210, Loss: 0.4572461918817771\n",
      "Epoch 211, Loss: 0.45629613394883295\n",
      "Epoch 212, Loss: 0.4557945440718634\n",
      "Epoch 213, Loss: 0.455843155547024\n",
      "Epoch 214, Loss: 0.456219547378948\n",
      "Epoch 215, Loss: 0.4577116995236092\n",
      "Epoch 216, Loss: 0.4589176697652806\n",
      "Epoch 217, Loss: 0.46105034303609993\n",
      "Epoch 218, Loss: 0.4567326442394303\n",
      "Epoch 219, Loss: 0.45493112899589366\n",
      "Epoch 220, Loss: 0.45527199475335134\n",
      "Epoch 221, Loss: 0.45623630281032773\n",
      "Epoch 222, Loss: 0.45802054967026307\n",
      "Epoch 223, Loss: 0.45677224995978094\n",
      "Epoch 224, Loss: 0.45565914342704716\n",
      "Epoch 225, Loss: 0.45401674524485375\n",
      "Epoch 226, Loss: 0.4535057342791329\n",
      "Epoch 227, Loss: 0.45416164205845616\n",
      "Epoch 228, Loss: 0.45450513400796655\n",
      "Epoch 229, Loss: 0.4548238117351997\n",
      "Epoch 230, Loss: 0.4537990916311975\n",
      "Epoch 231, Loss: 0.4532521496977959\n",
      "Epoch 232, Loss: 0.4526084553640119\n",
      "Epoch 233, Loss: 0.45228981460773116\n",
      "Epoch 234, Loss: 0.4520949132230242\n",
      "Epoch 235, Loss: 0.45205797975469736\n",
      "Epoch 236, Loss: 0.45230557901617613\n",
      "Epoch 237, Loss: 0.4528345953247846\n",
      "Epoch 238, Loss: 0.4549386133976144\n",
      "Epoch 239, Loss: 0.4564022887834084\n",
      "Epoch 240, Loss: 0.45805755540321125\n",
      "Epoch 241, Loss: 0.4541498302115434\n",
      "Epoch 242, Loss: 0.45158727109343993\n",
      "Epoch 243, Loss: 0.45105347173746146\n",
      "Epoch 244, Loss: 0.45263971453254503\n",
      "Epoch 245, Loss: 0.4550743757756552\n",
      "Epoch 246, Loss: 0.4549471294161167\n",
      "Epoch 247, Loss: 0.45361945699341477\n",
      "Epoch 248, Loss: 0.4506913801287277\n",
      "Epoch 249, Loss: 0.44996455105922556\n",
      "Epoch 250, Loss: 0.45070657227233163\n",
      "Epoch 251, Loss: 0.45134097806864715\n",
      "Epoch 252, Loss: 0.4518641160838718\n",
      "Epoch 253, Loss: 0.45124870735997746\n",
      "Epoch 254, Loss: 0.4507253179113287\n",
      "Epoch 255, Loss: 0.44965336548788515\n",
      "Epoch 256, Loss: 0.4490457284512466\n",
      "Epoch 257, Loss: 0.44867439459283065\n",
      "Epoch 258, Loss: 0.4485724236213288\n",
      "Epoch 259, Loss: 0.4487178165615502\n",
      "Epoch 260, Loss: 0.44913423821312026\n",
      "Epoch 261, Loss: 0.45068397397968624\n",
      "Epoch 262, Loss: 0.4523160224282965\n",
      "Epoch 263, Loss: 0.4554805695192658\n",
      "Epoch 264, Loss: 0.4521284819909996\n",
      "Epoch 265, Loss: 0.4491460824324\n",
      "Epoch 266, Loss: 0.4475917876267889\n",
      "Epoch 267, Loss: 0.4489836201844808\n",
      "Epoch 268, Loss: 0.4522696841254308\n",
      "Epoch 269, Loss: 0.45230492011773493\n",
      "Epoch 270, Loss: 0.45030259118174343\n",
      "Epoch 271, Loss: 0.44750189058874124\n",
      "Epoch 272, Loss: 0.4466950387656147\n",
      "Epoch 273, Loss: 0.44782488644087964\n",
      "Epoch 274, Loss: 0.4490013591686314\n",
      "Epoch 275, Loss: 0.44932258069431275\n",
      "Epoch 276, Loss: 0.4477355520631983\n",
      "Epoch 277, Loss: 0.4464409066948383\n",
      "Epoch 278, Loss: 0.4458417591594104\n",
      "Epoch 279, Loss: 0.4457414301360274\n",
      "Epoch 280, Loss: 0.4460887501130773\n",
      "Epoch 281, Loss: 0.44690611604251484\n",
      "Epoch 282, Loss: 0.4483696750030082\n",
      "Epoch 283, Loss: 0.4491296928695683\n",
      "Epoch 284, Loss: 0.4498625247292945\n",
      "Epoch 285, Loss: 0.44791530084032105\n",
      "Epoch 286, Loss: 0.4460872803334146\n",
      "Epoch 287, Loss: 0.44485744850497494\n",
      "Epoch 288, Loss: 0.4448821514773518\n",
      "Epoch 289, Loss: 0.4460718473545706\n",
      "Epoch 290, Loss: 0.4475995445619453\n",
      "Epoch 291, Loss: 0.44939647973391844\n",
      "Epoch 292, Loss: 0.44907386765397006\n",
      "Epoch 293, Loss: 0.4480021879332198\n",
      "Epoch 294, Loss: 0.44517745931865826\n",
      "Epoch 295, Loss: 0.44400204700028173\n",
      "Epoch 296, Loss: 0.4442791745325936\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.2348061704748228\n",
      "Valence RMSE: 0.24255248532584267\n",
      "Arousal RMSE: 0.22679543039567615\n",
      "Test R^2 score: tensor([0.3713, 0.6207], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4959931876003816\n",
      "Num of epochs: 297\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Epoch 121, Loss: 0.4816228295571127\n",
      "Epoch 122, Loss: 0.4807618495484584\n",
      "Epoch 123, Loss: 0.48102030846138955\n",
      "Epoch 124, Loss: 0.4810176288364116\n",
      "Epoch 125, Loss: 0.48014338261661127\n",
      "Epoch 126, Loss: 0.47910314809270116\n",
      "Epoch 127, Loss: 0.47880795949683447\n",
      "Epoch 128, Loss: 0.47876177315142754\n",
      "Epoch 129, Loss: 0.47819597116449997\n",
      "Epoch 130, Loss: 0.47743214539542544\n",
      "Epoch 131, Loss: 0.47658476230340185\n",
      "Epoch 132, Loss: 0.4759644680184978\n",
      "Epoch 133, Loss: 0.4755635187712421\n",
      "Epoch 134, Loss: 0.47538264196584307\n",
      "Epoch 135, Loss: 0.4753673450596914\n",
      "Epoch 136, Loss: 0.4753697744168057\n",
      "Epoch 137, Loss: 0.4755973109583155\n",
      "Epoch 138, Loss: 0.4767626826605241\n",
      "Epoch 139, Loss: 0.47683528222287225\n",
      "Epoch 140, Loss: 0.47579445468261566\n",
      "Epoch 141, Loss: 0.47284533919249566\n",
      "Epoch 142, Loss: 0.472809286012082\n",
      "Epoch 143, Loss: 0.47458738111232796\n",
      "Epoch 144, Loss: 0.4733380058918715\n",
      "Epoch 145, Loss: 0.4715229034484008\n",
      "Epoch 146, Loss: 0.47096730966267386\n",
      "Epoch 147, Loss: 0.47161469890467383\n",
      "Epoch 148, Loss: 0.47183531411129226\n",
      "Epoch 149, Loss: 0.470294933688076\n",
      "Epoch 150, Loss: 0.4697114144973765\n",
      "Epoch 151, Loss: 0.47018789053701304\n",
      "Epoch 152, Loss: 0.4700460953124117\n",
      "Epoch 153, Loss: 0.4693645740016508\n",
      "Epoch 154, Loss: 0.4684558104180481\n",
      "Epoch 155, Loss: 0.4680960384979934\n",
      "Epoch 156, Loss: 0.4682454254529793\n",
      "Epoch 157, Loss: 0.4682974379105488\n",
      "Epoch 158, Loss: 0.4681590010361841\n",
      "Epoch 159, Loss: 0.4673225124817593\n",
      "Epoch 160, Loss: 0.4666701428107949\n",
      "Epoch 161, Loss: 0.4663166784414499\n",
      "Epoch 162, Loss: 0.46624854535037\n",
      "Epoch 163, Loss: 0.4664611566834947\n",
      "Epoch 164, Loss: 0.46654532450127006\n",
      "Epoch 165, Loss: 0.46721845607201035\n",
      "Epoch 166, Loss: 0.4668208160006968\n",
      "Epoch 167, Loss: 0.466573749669782\n",
      "Epoch 168, Loss: 0.46506387643568103\n",
      "Epoch 169, Loss: 0.4641220516950567\n",
      "Epoch 170, Loss: 0.4644550172656552\n",
      "Epoch 171, Loss: 0.4654613047116713\n",
      "Epoch 172, Loss: 0.46880176576246846\n",
      "Epoch 173, Loss: 0.4685649824581709\n",
      "Epoch 174, Loss: 0.46606115996454733\n",
      "Epoch 175, Loss: 0.46328263242364126\n",
      "Epoch 176, Loss: 0.4660484187313888\n",
      "Epoch 177, Loss: 0.4667669789465077\n",
      "Epoch 178, Loss: 0.46222975993575777\n",
      "Epoch 179, Loss: 0.46412214801342555\n",
      "Epoch 180, Loss: 0.46867384291927394\n",
      "Epoch 181, Loss: 0.46316269230838986\n",
      "Epoch 182, Loss: 0.4619791093960296\n",
      "Epoch 183, Loss: 0.4650783107301587\n",
      "Epoch 184, Loss: 0.4617210775403838\n",
      "Epoch 185, Loss: 0.4621011464022392\n",
      "Epoch 186, Loss: 0.4641367239628427\n",
      "Epoch 187, Loss: 0.46071131054180564\n",
      "Epoch 188, Loss: 0.46145892957673257\n",
      "Epoch 189, Loss: 0.464304908222076\n",
      "Epoch 190, Loss: 0.4606605441278432\n",
      "Epoch 191, Loss: 0.4601386308130386\n",
      "Epoch 192, Loss: 0.46193430495315263\n",
      "Epoch 193, Loss: 0.45977696912799226\n",
      "Epoch 194, Loss: 0.4597173803227511\n",
      "Epoch 195, Loss: 0.46096589922299847\n",
      "Epoch 196, Loss: 0.4592170612502741\n",
      "Epoch 197, Loss: 0.45864347419013624\n",
      "Epoch 198, Loss: 0.4594650511157848\n",
      "Epoch 199, Loss: 0.45924705942486493\n",
      "Epoch 200, Loss: 0.4584931199827434\n",
      "Epoch 201, Loss: 0.4578552480015059\n",
      "Epoch 202, Loss: 0.458251953125\n",
      "Epoch 203, Loss: 0.4586848316367108\n",
      "Epoch 204, Loss: 0.4577025349799131\n",
      "Epoch 205, Loss: 0.45707273702530193\n",
      "Epoch 206, Loss: 0.4571336158814618\n",
      "Epoch 207, Loss: 0.45746758002181026\n",
      "Epoch 208, Loss: 0.4580334166179955\n",
      "Epoch 209, Loss: 0.457551138813026\n",
      "Epoch 210, Loss: 0.4572461918817771\n",
      "Epoch 211, Loss: 0.45629613394883295\n",
      "Epoch 212, Loss: 0.4557945440718634\n",
      "Epoch 213, Loss: 0.455843155547024\n",
      "Epoch 214, Loss: 0.456219547378948\n",
      "Epoch 215, Loss: 0.4577116995236092\n",
      "Epoch 216, Loss: 0.4589176697652806\n",
      "Epoch 217, Loss: 0.46105034303609993\n",
      "Epoch 218, Loss: 0.4567326442394303\n",
      "Epoch 219, Loss: 0.45493112899589366\n",
      "Epoch 220, Loss: 0.45527199475335134\n",
      "Epoch 221, Loss: 0.45623630281032773\n",
      "Epoch 222, Loss: 0.45802054967026307\n",
      "Epoch 223, Loss: 0.45677224995978094\n",
      "Epoch 224, Loss: 0.45565914342704716\n",
      "Epoch 225, Loss: 0.45401674524485375\n",
      "Epoch 226, Loss: 0.4535057342791329\n",
      "Epoch 227, Loss: 0.45416164205845616\n",
      "Epoch 228, Loss: 0.45450513400796655\n",
      "Epoch 229, Loss: 0.4548238117351997\n",
      "Epoch 230, Loss: 0.4537990916311975\n",
      "Epoch 231, Loss: 0.4532521496977959\n",
      "Epoch 232, Loss: 0.4526084553640119\n",
      "Epoch 233, Loss: 0.45228981460773116\n",
      "Epoch 234, Loss: 0.4520949132230242\n",
      "Epoch 235, Loss: 0.45205797975469736\n",
      "Epoch 236, Loss: 0.45230557901617613\n",
      "Epoch 237, Loss: 0.4528345953247846\n",
      "Epoch 238, Loss: 0.4549386133976144\n",
      "Epoch 239, Loss: 0.4564022887834084\n",
      "Epoch 240, Loss: 0.45805755540321125\n",
      "Epoch 241, Loss: 0.4541498302115434\n",
      "Epoch 242, Loss: 0.45158727109343993\n",
      "Epoch 243, Loss: 0.45105347173746146\n",
      "Epoch 244, Loss: 0.45263971453254503\n",
      "Epoch 245, Loss: 0.4550743757756552\n",
      "Epoch 246, Loss: 0.4549471294161167\n",
      "Epoch 247, Loss: 0.45361945699341477\n",
      "Epoch 248, Loss: 0.4506913801287277\n",
      "Epoch 249, Loss: 0.44996455105922556\n",
      "Epoch 250, Loss: 0.45070657227233163\n",
      "Epoch 251, Loss: 0.45134097806864715\n",
      "Epoch 252, Loss: 0.4518641160838718\n",
      "Epoch 253, Loss: 0.45124870735997746\n",
      "Epoch 254, Loss: 0.4507253179113287\n",
      "Epoch 255, Loss: 0.44965336548788515\n",
      "Epoch 256, Loss: 0.4490457284512466\n",
      "Epoch 257, Loss: 0.44867439459283065\n",
      "Epoch 258, Loss: 0.4485724236213288\n",
      "Epoch 259, Loss: 0.4487178165615502\n",
      "Epoch 260, Loss: 0.44913423821312026\n",
      "Epoch 261, Loss: 0.45068397397968624\n",
      "Epoch 262, Loss: 0.4523160224282965\n",
      "Epoch 263, Loss: 0.4554805695192658\n",
      "Epoch 264, Loss: 0.4521284819909996\n",
      "Epoch 265, Loss: 0.4491460824324\n",
      "Epoch 266, Loss: 0.4475917876267889\n",
      "Epoch 267, Loss: 0.4489836201844808\n",
      "Epoch 268, Loss: 0.4522696841254308\n",
      "Epoch 269, Loss: 0.45230492011773493\n",
      "Epoch 270, Loss: 0.45030259118174343\n",
      "Epoch 271, Loss: 0.44750189058874124\n",
      "Epoch 272, Loss: 0.4466950387656147\n",
      "Epoch 273, Loss: 0.44782488644087964\n",
      "Epoch 274, Loss: 0.4490013591686314\n",
      "Epoch 275, Loss: 0.44932258069431275\n",
      "Epoch 276, Loss: 0.4477355520631983\n",
      "Epoch 277, Loss: 0.4464409066948383\n",
      "Epoch 278, Loss: 0.4458417591594104\n",
      "Epoch 279, Loss: 0.4457414301360274\n",
      "Epoch 280, Loss: 0.4460887501130773\n",
      "Epoch 281, Loss: 0.44690611604251484\n",
      "Epoch 282, Loss: 0.4483696750030082\n",
      "Epoch 283, Loss: 0.4491296928695683\n",
      "Epoch 284, Loss: 0.4498625247292945\n",
      "Epoch 285, Loss: 0.44791530084032105\n",
      "Epoch 286, Loss: 0.4460872803334146\n",
      "Epoch 287, Loss: 0.44485744850497494\n",
      "Epoch 288, Loss: 0.4448821514773518\n",
      "Epoch 289, Loss: 0.4460718473545706\n",
      "Epoch 290, Loss: 0.4475995445619453\n",
      "Epoch 291, Loss: 0.44939647973391844\n",
      "Epoch 292, Loss: 0.44907386765397006\n",
      "Epoch 293, Loss: 0.4480021879332198\n",
      "Epoch 294, Loss: 0.44517745931865826\n",
      "Epoch 295, Loss: 0.44400204700028173\n",
      "Epoch 296, Loss: 0.4442791745325936\n",
      "Epoch 297, Loss: 0.4453458104307112\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23750492828623487\n",
      "Valence RMSE: 0.24895564847176324\n",
      "Arousal RMSE: 0.22547342862187328\n",
      "Test R^2 score: tensor([0.3377, 0.6251], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4813824501192954\n",
      "Num of epochs: 298\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Epoch 121, Loss: 0.4816228295571127\n",
      "Epoch 122, Loss: 0.4807618495484584\n",
      "Epoch 123, Loss: 0.48102030846138955\n",
      "Epoch 124, Loss: 0.4810176288364116\n",
      "Epoch 125, Loss: 0.48014338261661127\n",
      "Epoch 126, Loss: 0.47910314809270116\n",
      "Epoch 127, Loss: 0.47880795949683447\n",
      "Epoch 128, Loss: 0.47876177315142754\n",
      "Epoch 129, Loss: 0.47819597116449997\n",
      "Epoch 130, Loss: 0.47743214539542544\n",
      "Epoch 131, Loss: 0.47658476230340185\n",
      "Epoch 132, Loss: 0.4759644680184978\n",
      "Epoch 133, Loss: 0.4755635187712421\n",
      "Epoch 134, Loss: 0.47538264196584307\n",
      "Epoch 135, Loss: 0.4753673450596914\n",
      "Epoch 136, Loss: 0.4753697744168057\n",
      "Epoch 137, Loss: 0.4755973109583155\n",
      "Epoch 138, Loss: 0.4767626826605241\n",
      "Epoch 139, Loss: 0.47683528222287225\n",
      "Epoch 140, Loss: 0.47579445468261566\n",
      "Epoch 141, Loss: 0.47284533919249566\n",
      "Epoch 142, Loss: 0.472809286012082\n",
      "Epoch 143, Loss: 0.47458738111232796\n",
      "Epoch 144, Loss: 0.4733380058918715\n",
      "Epoch 145, Loss: 0.4715229034484008\n",
      "Epoch 146, Loss: 0.47096730966267386\n",
      "Epoch 147, Loss: 0.47161469890467383\n",
      "Epoch 148, Loss: 0.47183531411129226\n",
      "Epoch 149, Loss: 0.470294933688076\n",
      "Epoch 150, Loss: 0.4697114144973765\n",
      "Epoch 151, Loss: 0.47018789053701304\n",
      "Epoch 152, Loss: 0.4700460953124117\n",
      "Epoch 153, Loss: 0.4693645740016508\n",
      "Epoch 154, Loss: 0.4684558104180481\n",
      "Epoch 155, Loss: 0.4680960384979934\n",
      "Epoch 156, Loss: 0.4682454254529793\n",
      "Epoch 157, Loss: 0.4682974379105488\n",
      "Epoch 158, Loss: 0.4681590010361841\n",
      "Epoch 159, Loss: 0.4673225124817593\n",
      "Epoch 160, Loss: 0.4666701428107949\n",
      "Epoch 161, Loss: 0.4663166784414499\n",
      "Epoch 162, Loss: 0.46624854535037\n",
      "Epoch 163, Loss: 0.4664611566834947\n",
      "Epoch 164, Loss: 0.46654532450127006\n",
      "Epoch 165, Loss: 0.46721845607201035\n",
      "Epoch 166, Loss: 0.4668208160006968\n",
      "Epoch 167, Loss: 0.466573749669782\n",
      "Epoch 168, Loss: 0.46506387643568103\n",
      "Epoch 169, Loss: 0.4641220516950567\n",
      "Epoch 170, Loss: 0.4644550172656552\n",
      "Epoch 171, Loss: 0.4654613047116713\n",
      "Epoch 172, Loss: 0.46880176576246846\n",
      "Epoch 173, Loss: 0.4685649824581709\n",
      "Epoch 174, Loss: 0.46606115996454733\n",
      "Epoch 175, Loss: 0.46328263242364126\n",
      "Epoch 176, Loss: 0.4660484187313888\n",
      "Epoch 177, Loss: 0.4667669789465077\n",
      "Epoch 178, Loss: 0.46222975993575777\n",
      "Epoch 179, Loss: 0.46412214801342555\n",
      "Epoch 180, Loss: 0.46867384291927394\n",
      "Epoch 181, Loss: 0.46316269230838986\n",
      "Epoch 182, Loss: 0.4619791093960296\n",
      "Epoch 183, Loss: 0.4650783107301587\n",
      "Epoch 184, Loss: 0.4617210775403838\n",
      "Epoch 185, Loss: 0.4621011464022392\n",
      "Epoch 186, Loss: 0.4641367239628427\n",
      "Epoch 187, Loss: 0.46071131054180564\n",
      "Epoch 188, Loss: 0.46145892957673257\n",
      "Epoch 189, Loss: 0.464304908222076\n",
      "Epoch 190, Loss: 0.4606605441278432\n",
      "Epoch 191, Loss: 0.4601386308130386\n",
      "Epoch 192, Loss: 0.46193430495315263\n",
      "Epoch 193, Loss: 0.45977696912799226\n",
      "Epoch 194, Loss: 0.4597173803227511\n",
      "Epoch 195, Loss: 0.46096589922299847\n",
      "Epoch 196, Loss: 0.4592170612502741\n",
      "Epoch 197, Loss: 0.45864347419013624\n",
      "Epoch 198, Loss: 0.4594650511157848\n",
      "Epoch 199, Loss: 0.45924705942486493\n",
      "Epoch 200, Loss: 0.4584931199827434\n",
      "Epoch 201, Loss: 0.4578552480015059\n",
      "Epoch 202, Loss: 0.458251953125\n",
      "Epoch 203, Loss: 0.4586848316367108\n",
      "Epoch 204, Loss: 0.4577025349799131\n",
      "Epoch 205, Loss: 0.45707273702530193\n",
      "Epoch 206, Loss: 0.4571336158814618\n",
      "Epoch 207, Loss: 0.45746758002181026\n",
      "Epoch 208, Loss: 0.4580334166179955\n",
      "Epoch 209, Loss: 0.457551138813026\n",
      "Epoch 210, Loss: 0.4572461918817771\n",
      "Epoch 211, Loss: 0.45629613394883295\n",
      "Epoch 212, Loss: 0.4557945440718634\n",
      "Epoch 213, Loss: 0.455843155547024\n",
      "Epoch 214, Loss: 0.456219547378948\n",
      "Epoch 215, Loss: 0.4577116995236092\n",
      "Epoch 216, Loss: 0.4589176697652806\n",
      "Epoch 217, Loss: 0.46105034303609993\n",
      "Epoch 218, Loss: 0.4567326442394303\n",
      "Epoch 219, Loss: 0.45493112899589366\n",
      "Epoch 220, Loss: 0.45527199475335134\n",
      "Epoch 221, Loss: 0.45623630281032773\n",
      "Epoch 222, Loss: 0.45802054967026307\n",
      "Epoch 223, Loss: 0.45677224995978094\n",
      "Epoch 224, Loss: 0.45565914342704716\n",
      "Epoch 225, Loss: 0.45401674524485375\n",
      "Epoch 226, Loss: 0.4535057342791329\n",
      "Epoch 227, Loss: 0.45416164205845616\n",
      "Epoch 228, Loss: 0.45450513400796655\n",
      "Epoch 229, Loss: 0.4548238117351997\n",
      "Epoch 230, Loss: 0.4537990916311975\n",
      "Epoch 231, Loss: 0.4532521496977959\n",
      "Epoch 232, Loss: 0.4526084553640119\n",
      "Epoch 233, Loss: 0.45228981460773116\n",
      "Epoch 234, Loss: 0.4520949132230242\n",
      "Epoch 235, Loss: 0.45205797975469736\n",
      "Epoch 236, Loss: 0.45230557901617613\n",
      "Epoch 237, Loss: 0.4528345953247846\n",
      "Epoch 238, Loss: 0.4549386133976144\n",
      "Epoch 239, Loss: 0.4564022887834084\n",
      "Epoch 240, Loss: 0.45805755540321125\n",
      "Epoch 241, Loss: 0.4541498302115434\n",
      "Epoch 242, Loss: 0.45158727109343993\n",
      "Epoch 243, Loss: 0.45105347173746146\n",
      "Epoch 244, Loss: 0.45263971453254503\n",
      "Epoch 245, Loss: 0.4550743757756552\n",
      "Epoch 246, Loss: 0.4549471294161167\n",
      "Epoch 247, Loss: 0.45361945699341477\n",
      "Epoch 248, Loss: 0.4506913801287277\n",
      "Epoch 249, Loss: 0.44996455105922556\n",
      "Epoch 250, Loss: 0.45070657227233163\n",
      "Epoch 251, Loss: 0.45134097806864715\n",
      "Epoch 252, Loss: 0.4518641160838718\n",
      "Epoch 253, Loss: 0.45124870735997746\n",
      "Epoch 254, Loss: 0.4507253179113287\n",
      "Epoch 255, Loss: 0.44965336548788515\n",
      "Epoch 256, Loss: 0.4490457284512466\n",
      "Epoch 257, Loss: 0.44867439459283065\n",
      "Epoch 258, Loss: 0.4485724236213288\n",
      "Epoch 259, Loss: 0.4487178165615502\n",
      "Epoch 260, Loss: 0.44913423821312026\n",
      "Epoch 261, Loss: 0.45068397397968624\n",
      "Epoch 262, Loss: 0.4523160224282965\n",
      "Epoch 263, Loss: 0.4554805695192658\n",
      "Epoch 264, Loss: 0.4521284819909996\n",
      "Epoch 265, Loss: 0.4491460824324\n",
      "Epoch 266, Loss: 0.4475917876267889\n",
      "Epoch 267, Loss: 0.4489836201844808\n",
      "Epoch 268, Loss: 0.4522696841254308\n",
      "Epoch 269, Loss: 0.45230492011773493\n",
      "Epoch 270, Loss: 0.45030259118174343\n",
      "Epoch 271, Loss: 0.44750189058874124\n",
      "Epoch 272, Loss: 0.4466950387656147\n",
      "Epoch 273, Loss: 0.44782488644087964\n",
      "Epoch 274, Loss: 0.4490013591686314\n",
      "Epoch 275, Loss: 0.44932258069431275\n",
      "Epoch 276, Loss: 0.4477355520631983\n",
      "Epoch 277, Loss: 0.4464409066948383\n",
      "Epoch 278, Loss: 0.4458417591594104\n",
      "Epoch 279, Loss: 0.4457414301360274\n",
      "Epoch 280, Loss: 0.4460887501130773\n",
      "Epoch 281, Loss: 0.44690611604251484\n",
      "Epoch 282, Loss: 0.4483696750030082\n",
      "Epoch 283, Loss: 0.4491296928695683\n",
      "Epoch 284, Loss: 0.4498625247292945\n",
      "Epoch 285, Loss: 0.44791530084032105\n",
      "Epoch 286, Loss: 0.4460872803334146\n",
      "Epoch 287, Loss: 0.44485744850497494\n",
      "Epoch 288, Loss: 0.4448821514773518\n",
      "Epoch 289, Loss: 0.4460718473545706\n",
      "Epoch 290, Loss: 0.4475995445619453\n",
      "Epoch 291, Loss: 0.44939647973391844\n",
      "Epoch 292, Loss: 0.44907386765397006\n",
      "Epoch 293, Loss: 0.4480021879332198\n",
      "Epoch 294, Loss: 0.44517745931865826\n",
      "Epoch 295, Loss: 0.44400204700028173\n",
      "Epoch 296, Loss: 0.4442791745325936\n",
      "Epoch 297, Loss: 0.4453458104307112\n",
      "Epoch 298, Loss: 0.4473529547178379\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23656397566248571\n",
      "Valence RMSE: 0.24400486120782394\n",
      "Arousal RMSE: 0.22888131612133134\n",
      "Test R^2 score: tensor([0.3638, 0.6137], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.48871257985790256\n",
      "Num of epochs: 299\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Epoch 121, Loss: 0.4816228295571127\n",
      "Epoch 122, Loss: 0.4807618495484584\n",
      "Epoch 123, Loss: 0.48102030846138955\n",
      "Epoch 124, Loss: 0.4810176288364116\n",
      "Epoch 125, Loss: 0.48014338261661127\n",
      "Epoch 126, Loss: 0.47910314809270116\n",
      "Epoch 127, Loss: 0.47880795949683447\n",
      "Epoch 128, Loss: 0.47876177315142754\n",
      "Epoch 129, Loss: 0.47819597116449997\n",
      "Epoch 130, Loss: 0.47743214539542544\n",
      "Epoch 131, Loss: 0.47658476230340185\n",
      "Epoch 132, Loss: 0.4759644680184978\n",
      "Epoch 133, Loss: 0.4755635187712421\n",
      "Epoch 134, Loss: 0.47538264196584307\n",
      "Epoch 135, Loss: 0.4753673450596914\n",
      "Epoch 136, Loss: 0.4753697744168057\n",
      "Epoch 137, Loss: 0.4755973109583155\n",
      "Epoch 138, Loss: 0.4767626826605241\n",
      "Epoch 139, Loss: 0.47683528222287225\n",
      "Epoch 140, Loss: 0.47579445468261566\n",
      "Epoch 141, Loss: 0.47284533919249566\n",
      "Epoch 142, Loss: 0.472809286012082\n",
      "Epoch 143, Loss: 0.47458738111232796\n",
      "Epoch 144, Loss: 0.4733380058918715\n",
      "Epoch 145, Loss: 0.4715229034484008\n",
      "Epoch 146, Loss: 0.47096730966267386\n",
      "Epoch 147, Loss: 0.47161469890467383\n",
      "Epoch 148, Loss: 0.47183531411129226\n",
      "Epoch 149, Loss: 0.470294933688076\n",
      "Epoch 150, Loss: 0.4697114144973765\n",
      "Epoch 151, Loss: 0.47018789053701304\n",
      "Epoch 152, Loss: 0.4700460953124117\n",
      "Epoch 153, Loss: 0.4693645740016508\n",
      "Epoch 154, Loss: 0.4684558104180481\n",
      "Epoch 155, Loss: 0.4680960384979934\n",
      "Epoch 156, Loss: 0.4682454254529793\n",
      "Epoch 157, Loss: 0.4682974379105488\n",
      "Epoch 158, Loss: 0.4681590010361841\n",
      "Epoch 159, Loss: 0.4673225124817593\n",
      "Epoch 160, Loss: 0.4666701428107949\n",
      "Epoch 161, Loss: 0.4663166784414499\n",
      "Epoch 162, Loss: 0.46624854535037\n",
      "Epoch 163, Loss: 0.4664611566834947\n",
      "Epoch 164, Loss: 0.46654532450127006\n",
      "Epoch 165, Loss: 0.46721845607201035\n",
      "Epoch 166, Loss: 0.4668208160006968\n",
      "Epoch 167, Loss: 0.466573749669782\n",
      "Epoch 168, Loss: 0.46506387643568103\n",
      "Epoch 169, Loss: 0.4641220516950567\n",
      "Epoch 170, Loss: 0.4644550172656552\n",
      "Epoch 171, Loss: 0.4654613047116713\n",
      "Epoch 172, Loss: 0.46880176576246846\n",
      "Epoch 173, Loss: 0.4685649824581709\n",
      "Epoch 174, Loss: 0.46606115996454733\n",
      "Epoch 175, Loss: 0.46328263242364126\n",
      "Epoch 176, Loss: 0.4660484187313888\n",
      "Epoch 177, Loss: 0.4667669789465077\n",
      "Epoch 178, Loss: 0.46222975993575777\n",
      "Epoch 179, Loss: 0.46412214801342555\n",
      "Epoch 180, Loss: 0.46867384291927394\n",
      "Epoch 181, Loss: 0.46316269230838986\n",
      "Epoch 182, Loss: 0.4619791093960296\n",
      "Epoch 183, Loss: 0.4650783107301587\n",
      "Epoch 184, Loss: 0.4617210775403838\n",
      "Epoch 185, Loss: 0.4621011464022392\n",
      "Epoch 186, Loss: 0.4641367239628427\n",
      "Epoch 187, Loss: 0.46071131054180564\n",
      "Epoch 188, Loss: 0.46145892957673257\n",
      "Epoch 189, Loss: 0.464304908222076\n",
      "Epoch 190, Loss: 0.4606605441278432\n",
      "Epoch 191, Loss: 0.4601386308130386\n",
      "Epoch 192, Loss: 0.46193430495315263\n",
      "Epoch 193, Loss: 0.45977696912799226\n",
      "Epoch 194, Loss: 0.4597173803227511\n",
      "Epoch 195, Loss: 0.46096589922299847\n",
      "Epoch 196, Loss: 0.4592170612502741\n",
      "Epoch 197, Loss: 0.45864347419013624\n",
      "Epoch 198, Loss: 0.4594650511157848\n",
      "Epoch 199, Loss: 0.45924705942486493\n",
      "Epoch 200, Loss: 0.4584931199827434\n",
      "Epoch 201, Loss: 0.4578552480015059\n",
      "Epoch 202, Loss: 0.458251953125\n",
      "Epoch 203, Loss: 0.4586848316367108\n",
      "Epoch 204, Loss: 0.4577025349799131\n",
      "Epoch 205, Loss: 0.45707273702530193\n",
      "Epoch 206, Loss: 0.4571336158814618\n",
      "Epoch 207, Loss: 0.45746758002181026\n",
      "Epoch 208, Loss: 0.4580334166179955\n",
      "Epoch 209, Loss: 0.457551138813026\n",
      "Epoch 210, Loss: 0.4572461918817771\n",
      "Epoch 211, Loss: 0.45629613394883295\n",
      "Epoch 212, Loss: 0.4557945440718634\n",
      "Epoch 213, Loss: 0.455843155547024\n",
      "Epoch 214, Loss: 0.456219547378948\n",
      "Epoch 215, Loss: 0.4577116995236092\n",
      "Epoch 216, Loss: 0.4589176697652806\n",
      "Epoch 217, Loss: 0.46105034303609993\n",
      "Epoch 218, Loss: 0.4567326442394303\n",
      "Epoch 219, Loss: 0.45493112899589366\n",
      "Epoch 220, Loss: 0.45527199475335134\n",
      "Epoch 221, Loss: 0.45623630281032773\n",
      "Epoch 222, Loss: 0.45802054967026307\n",
      "Epoch 223, Loss: 0.45677224995978094\n",
      "Epoch 224, Loss: 0.45565914342704716\n",
      "Epoch 225, Loss: 0.45401674524485375\n",
      "Epoch 226, Loss: 0.4535057342791329\n",
      "Epoch 227, Loss: 0.45416164205845616\n",
      "Epoch 228, Loss: 0.45450513400796655\n",
      "Epoch 229, Loss: 0.4548238117351997\n",
      "Epoch 230, Loss: 0.4537990916311975\n",
      "Epoch 231, Loss: 0.4532521496977959\n",
      "Epoch 232, Loss: 0.4526084553640119\n",
      "Epoch 233, Loss: 0.45228981460773116\n",
      "Epoch 234, Loss: 0.4520949132230242\n",
      "Epoch 235, Loss: 0.45205797975469736\n",
      "Epoch 236, Loss: 0.45230557901617613\n",
      "Epoch 237, Loss: 0.4528345953247846\n",
      "Epoch 238, Loss: 0.4549386133976144\n",
      "Epoch 239, Loss: 0.4564022887834084\n",
      "Epoch 240, Loss: 0.45805755540321125\n",
      "Epoch 241, Loss: 0.4541498302115434\n",
      "Epoch 242, Loss: 0.45158727109343993\n",
      "Epoch 243, Loss: 0.45105347173746146\n",
      "Epoch 244, Loss: 0.45263971453254503\n",
      "Epoch 245, Loss: 0.4550743757756552\n",
      "Epoch 246, Loss: 0.4549471294161167\n",
      "Epoch 247, Loss: 0.45361945699341477\n",
      "Epoch 248, Loss: 0.4506913801287277\n",
      "Epoch 249, Loss: 0.44996455105922556\n",
      "Epoch 250, Loss: 0.45070657227233163\n",
      "Epoch 251, Loss: 0.45134097806864715\n",
      "Epoch 252, Loss: 0.4518641160838718\n",
      "Epoch 253, Loss: 0.45124870735997746\n",
      "Epoch 254, Loss: 0.4507253179113287\n",
      "Epoch 255, Loss: 0.44965336548788515\n",
      "Epoch 256, Loss: 0.4490457284512466\n",
      "Epoch 257, Loss: 0.44867439459283065\n",
      "Epoch 258, Loss: 0.4485724236213288\n",
      "Epoch 259, Loss: 0.4487178165615502\n",
      "Epoch 260, Loss: 0.44913423821312026\n",
      "Epoch 261, Loss: 0.45068397397968624\n",
      "Epoch 262, Loss: 0.4523160224282965\n",
      "Epoch 263, Loss: 0.4554805695192658\n",
      "Epoch 264, Loss: 0.4521284819909996\n",
      "Epoch 265, Loss: 0.4491460824324\n",
      "Epoch 266, Loss: 0.4475917876267889\n",
      "Epoch 267, Loss: 0.4489836201844808\n",
      "Epoch 268, Loss: 0.4522696841254308\n",
      "Epoch 269, Loss: 0.45230492011773493\n",
      "Epoch 270, Loss: 0.45030259118174343\n",
      "Epoch 271, Loss: 0.44750189058874124\n",
      "Epoch 272, Loss: 0.4466950387656147\n",
      "Epoch 273, Loss: 0.44782488644087964\n",
      "Epoch 274, Loss: 0.4490013591686314\n",
      "Epoch 275, Loss: 0.44932258069431275\n",
      "Epoch 276, Loss: 0.4477355520631983\n",
      "Epoch 277, Loss: 0.4464409066948383\n",
      "Epoch 278, Loss: 0.4458417591594104\n",
      "Epoch 279, Loss: 0.4457414301360274\n",
      "Epoch 280, Loss: 0.4460887501130773\n",
      "Epoch 281, Loss: 0.44690611604251484\n",
      "Epoch 282, Loss: 0.4483696750030082\n",
      "Epoch 283, Loss: 0.4491296928695683\n",
      "Epoch 284, Loss: 0.4498625247292945\n",
      "Epoch 285, Loss: 0.44791530084032105\n",
      "Epoch 286, Loss: 0.4460872803334146\n",
      "Epoch 287, Loss: 0.44485744850497494\n",
      "Epoch 288, Loss: 0.4448821514773518\n",
      "Epoch 289, Loss: 0.4460718473545706\n",
      "Epoch 290, Loss: 0.4475995445619453\n",
      "Epoch 291, Loss: 0.44939647973391844\n",
      "Epoch 292, Loss: 0.44907386765397006\n",
      "Epoch 293, Loss: 0.4480021879332198\n",
      "Epoch 294, Loss: 0.44517745931865826\n",
      "Epoch 295, Loss: 0.44400204700028173\n",
      "Epoch 296, Loss: 0.4442791745325936\n",
      "Epoch 297, Loss: 0.4453458104307112\n",
      "Epoch 298, Loss: 0.4473529547178379\n",
      "Epoch 299, Loss: 0.44795630153980215\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23763864316494954\n",
      "Valence RMSE: 0.24994094997020008\n",
      "Arousal RMSE: 0.2246636841560077\n",
      "Test R^2 score: tensor([0.3324, 0.6278], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4801001044089925\n",
      "Num of epochs: 300\n",
      "Epoch 1, Loss: 0.650410030407706\n",
      "Epoch 2, Loss: 0.6459746411279922\n",
      "Epoch 3, Loss: 0.6416126871523545\n",
      "Epoch 4, Loss: 0.6375532707695886\n",
      "Epoch 5, Loss: 0.6336942581251322\n",
      "Epoch 6, Loss: 0.630001914422593\n",
      "Epoch 7, Loss: 0.6265504441088404\n",
      "Epoch 8, Loss: 0.6231732613977309\n",
      "Epoch 9, Loss: 0.6198640321644744\n",
      "Epoch 10, Loss: 0.6166051217460674\n",
      "Epoch 11, Loss: 0.6133670868301936\n",
      "Epoch 12, Loss: 0.610256535571278\n",
      "Epoch 13, Loss: 0.607290272086774\n",
      "Epoch 14, Loss: 0.6043625355158097\n",
      "Epoch 15, Loss: 0.6015415931140213\n",
      "Epoch 16, Loss: 0.5987990927828234\n",
      "Epoch 17, Loss: 0.59612246902898\n",
      "Epoch 18, Loss: 0.5937126549470603\n",
      "Epoch 19, Loss: 0.5921507428922907\n",
      "Epoch 20, Loss: 0.5922823133030879\n",
      "Epoch 21, Loss: 0.5936999550860341\n",
      "Epoch 22, Loss: 0.5942395852042492\n",
      "Epoch 23, Loss: 0.5937741927187319\n",
      "Epoch 24, Loss: 0.5928925546938177\n",
      "Epoch 25, Loss: 0.5920854123173221\n",
      "Epoch 26, Loss: 0.5915439178401749\n",
      "Epoch 27, Loss: 0.591233945713277\n",
      "Epoch 28, Loss: 0.5910767810391497\n",
      "Epoch 29, Loss: 0.5910590832169714\n",
      "Epoch 30, Loss: 0.5911071585441002\n",
      "Epoch 31, Loss: 0.5911900900072121\n",
      "Epoch 32, Loss: 0.5912496220787082\n",
      "Epoch 33, Loss: 0.5912913564851844\n",
      "Epoch 34, Loss: 0.5913086693509318\n",
      "Epoch 35, Loss: 0.5912952878354282\n",
      "Epoch 36, Loss: 0.5912479334869725\n",
      "Epoch 37, Loss: 0.5911991386635072\n",
      "Epoch 38, Loss: 0.5911447942335321\n",
      "Epoch 39, Loss: 0.5911045116036836\n",
      "Epoch 40, Loss: 0.5910844953086264\n",
      "Epoch 41, Loss: 0.5910561587394972\n",
      "Epoch 42, Loss: 0.5910303924591438\n",
      "Epoch 43, Loss: 0.5910160465579348\n",
      "Epoch 44, Loss: 0.5910078775587659\n",
      "Epoch 45, Loss: 0.5910038434434545\n",
      "Epoch 46, Loss: 0.5910011960403165\n",
      "Epoch 47, Loss: 0.5909968845298225\n",
      "Epoch 48, Loss: 0.5909877571348259\n",
      "Epoch 49, Loss: 0.5909722251110124\n",
      "Epoch 50, Loss: 0.5909492288975793\n",
      "Epoch 51, Loss: 0.5909180867654118\n",
      "Epoch 52, Loss: 0.590880159231653\n",
      "Epoch 53, Loss: 0.5908393289327294\n",
      "Epoch 54, Loss: 0.590796805930882\n",
      "Epoch 55, Loss: 0.5907521862759616\n",
      "Epoch 56, Loss: 0.590702190098239\n",
      "Epoch 57, Loss: 0.5906453275462308\n",
      "Epoch 58, Loss: 0.5905771811367462\n",
      "Epoch 59, Loss: 0.5904953244062136\n",
      "Epoch 60, Loss: 0.5903964453199938\n",
      "Epoch 61, Loss: 0.5902762942825325\n",
      "Epoch 62, Loss: 0.5901333432729927\n",
      "Epoch 63, Loss: 0.5899618422162785\n",
      "Epoch 64, Loss: 0.5897478442506414\n",
      "Epoch 65, Loss: 0.5894854638472858\n",
      "Epoch 66, Loss: 0.5891686422305009\n",
      "Epoch 67, Loss: 0.58877494431154\n",
      "Epoch 68, Loss: 0.588286079903643\n",
      "Epoch 69, Loss: 0.5876927597279505\n",
      "Epoch 70, Loss: 0.5869446300601179\n",
      "Epoch 71, Loss: 0.5859754168639927\n",
      "Epoch 72, Loss: 0.5847472754321611\n",
      "Epoch 73, Loss: 0.5831537112070423\n",
      "Epoch 74, Loss: 0.5811095909017117\n",
      "Epoch 75, Loss: 0.5784774170080423\n",
      "Epoch 76, Loss: 0.5751381086262156\n",
      "Epoch 77, Loss: 0.5710167390637915\n",
      "Epoch 78, Loss: 0.566328503764738\n",
      "Epoch 79, Loss: 0.5616536130529656\n",
      "Epoch 80, Loss: 0.5579018742958061\n",
      "Epoch 81, Loss: 0.5549427036402748\n",
      "Epoch 82, Loss: 0.5518646676898905\n",
      "Epoch 83, Loss: 0.5476968312159111\n",
      "Epoch 84, Loss: 0.5429894971657256\n",
      "Epoch 85, Loss: 0.5390425139329584\n",
      "Epoch 86, Loss: 0.5365251172417508\n",
      "Epoch 87, Loss: 0.5343228275640965\n",
      "Epoch 88, Loss: 0.5311807699534568\n",
      "Epoch 89, Loss: 0.5274364248717509\n",
      "Epoch 90, Loss: 0.5242798134362933\n",
      "Epoch 91, Loss: 0.522407429174375\n",
      "Epoch 92, Loss: 0.5208363882611009\n",
      "Epoch 93, Loss: 0.5189016155069271\n",
      "Epoch 94, Loss: 0.5191248114181606\n",
      "Epoch 95, Loss: 0.5163470905253383\n",
      "Epoch 96, Loss: 0.5132102197187762\n",
      "Epoch 97, Loss: 0.5133656216143967\n",
      "Epoch 98, Loss: 0.5098711394219722\n",
      "Epoch 99, Loss: 0.508205203287389\n",
      "Epoch 100, Loss: 0.5074778407760361\n",
      "Epoch 101, Loss: 0.5046792127088278\n",
      "Epoch 102, Loss: 0.5040967418583524\n",
      "Epoch 103, Loss: 0.5024414655650171\n",
      "Epoch 104, Loss: 0.4986953253882061\n",
      "Epoch 105, Loss: 0.49904762403547265\n",
      "Epoch 106, Loss: 0.49668242603406576\n",
      "Epoch 107, Loss: 0.49308007157704853\n",
      "Epoch 108, Loss: 0.493359351441299\n",
      "Epoch 109, Loss: 0.4929205113672413\n",
      "Epoch 110, Loss: 0.4887589978465458\n",
      "Epoch 111, Loss: 0.48742935634716184\n",
      "Epoch 112, Loss: 0.48900126966650953\n",
      "Epoch 113, Loss: 0.48742606996277066\n",
      "Epoch 114, Loss: 0.48564702333861426\n",
      "Epoch 115, Loss: 0.4861451383064925\n",
      "Epoch 116, Loss: 0.4882300235579893\n",
      "Epoch 117, Loss: 0.4861607550824061\n",
      "Epoch 118, Loss: 0.4835380275588832\n",
      "Epoch 119, Loss: 0.48308312507255524\n",
      "Epoch 120, Loss: 0.482821171877119\n",
      "Epoch 121, Loss: 0.4816228295571127\n",
      "Epoch 122, Loss: 0.4807618495484584\n",
      "Epoch 123, Loss: 0.48102030846138955\n",
      "Epoch 124, Loss: 0.4810176288364116\n",
      "Epoch 125, Loss: 0.48014338261661127\n",
      "Epoch 126, Loss: 0.47910314809270116\n",
      "Epoch 127, Loss: 0.47880795949683447\n",
      "Epoch 128, Loss: 0.47876177315142754\n",
      "Epoch 129, Loss: 0.47819597116449997\n",
      "Epoch 130, Loss: 0.47743214539542544\n",
      "Epoch 131, Loss: 0.47658476230340185\n",
      "Epoch 132, Loss: 0.4759644680184978\n",
      "Epoch 133, Loss: 0.4755635187712421\n",
      "Epoch 134, Loss: 0.47538264196584307\n",
      "Epoch 135, Loss: 0.4753673450596914\n",
      "Epoch 136, Loss: 0.4753697744168057\n",
      "Epoch 137, Loss: 0.4755973109583155\n",
      "Epoch 138, Loss: 0.4767626826605241\n",
      "Epoch 139, Loss: 0.47683528222287225\n",
      "Epoch 140, Loss: 0.47579445468261566\n",
      "Epoch 141, Loss: 0.47284533919249566\n",
      "Epoch 142, Loss: 0.472809286012082\n",
      "Epoch 143, Loss: 0.47458738111232796\n",
      "Epoch 144, Loss: 0.4733380058918715\n",
      "Epoch 145, Loss: 0.4715229034484008\n",
      "Epoch 146, Loss: 0.47096730966267386\n",
      "Epoch 147, Loss: 0.47161469890467383\n",
      "Epoch 148, Loss: 0.47183531411129226\n",
      "Epoch 149, Loss: 0.470294933688076\n",
      "Epoch 150, Loss: 0.4697114144973765\n",
      "Epoch 151, Loss: 0.47018789053701304\n",
      "Epoch 152, Loss: 0.4700460953124117\n",
      "Epoch 153, Loss: 0.4693645740016508\n",
      "Epoch 154, Loss: 0.4684558104180481\n",
      "Epoch 155, Loss: 0.4680960384979934\n",
      "Epoch 156, Loss: 0.4682454254529793\n",
      "Epoch 157, Loss: 0.4682974379105488\n",
      "Epoch 158, Loss: 0.4681590010361841\n",
      "Epoch 159, Loss: 0.4673225124817593\n",
      "Epoch 160, Loss: 0.4666701428107949\n",
      "Epoch 161, Loss: 0.4663166784414499\n",
      "Epoch 162, Loss: 0.46624854535037\n",
      "Epoch 163, Loss: 0.4664611566834947\n",
      "Epoch 164, Loss: 0.46654532450127006\n",
      "Epoch 165, Loss: 0.46721845607201035\n",
      "Epoch 166, Loss: 0.4668208160006968\n",
      "Epoch 167, Loss: 0.466573749669782\n",
      "Epoch 168, Loss: 0.46506387643568103\n",
      "Epoch 169, Loss: 0.4641220516950567\n",
      "Epoch 170, Loss: 0.4644550172656552\n",
      "Epoch 171, Loss: 0.4654613047116713\n",
      "Epoch 172, Loss: 0.46880176576246846\n",
      "Epoch 173, Loss: 0.4685649824581709\n",
      "Epoch 174, Loss: 0.46606115996454733\n",
      "Epoch 175, Loss: 0.46328263242364126\n",
      "Epoch 176, Loss: 0.4660484187313888\n",
      "Epoch 177, Loss: 0.4667669789465077\n",
      "Epoch 178, Loss: 0.46222975993575777\n",
      "Epoch 179, Loss: 0.46412214801342555\n",
      "Epoch 180, Loss: 0.46867384291927394\n",
      "Epoch 181, Loss: 0.46316269230838986\n",
      "Epoch 182, Loss: 0.4619791093960296\n",
      "Epoch 183, Loss: 0.4650783107301587\n",
      "Epoch 184, Loss: 0.4617210775403838\n",
      "Epoch 185, Loss: 0.4621011464022392\n",
      "Epoch 186, Loss: 0.4641367239628427\n",
      "Epoch 187, Loss: 0.46071131054180564\n",
      "Epoch 188, Loss: 0.46145892957673257\n",
      "Epoch 189, Loss: 0.464304908222076\n",
      "Epoch 190, Loss: 0.4606605441278432\n",
      "Epoch 191, Loss: 0.4601386308130386\n",
      "Epoch 192, Loss: 0.46193430495315263\n",
      "Epoch 193, Loss: 0.45977696912799226\n",
      "Epoch 194, Loss: 0.4597173803227511\n",
      "Epoch 195, Loss: 0.46096589922299847\n",
      "Epoch 196, Loss: 0.4592170612502741\n",
      "Epoch 197, Loss: 0.45864347419013624\n",
      "Epoch 198, Loss: 0.4594650511157848\n",
      "Epoch 199, Loss: 0.45924705942486493\n",
      "Epoch 200, Loss: 0.4584931199827434\n",
      "Epoch 201, Loss: 0.4578552480015059\n",
      "Epoch 202, Loss: 0.458251953125\n",
      "Epoch 203, Loss: 0.4586848316367108\n",
      "Epoch 204, Loss: 0.4577025349799131\n",
      "Epoch 205, Loss: 0.45707273702530193\n",
      "Epoch 206, Loss: 0.4571336158814618\n",
      "Epoch 207, Loss: 0.45746758002181026\n",
      "Epoch 208, Loss: 0.4580334166179955\n",
      "Epoch 209, Loss: 0.457551138813026\n",
      "Epoch 210, Loss: 0.4572461918817771\n",
      "Epoch 211, Loss: 0.45629613394883295\n",
      "Epoch 212, Loss: 0.4557945440718634\n",
      "Epoch 213, Loss: 0.455843155547024\n",
      "Epoch 214, Loss: 0.456219547378948\n",
      "Epoch 215, Loss: 0.4577116995236092\n",
      "Epoch 216, Loss: 0.4589176697652806\n",
      "Epoch 217, Loss: 0.46105034303609993\n",
      "Epoch 218, Loss: 0.4567326442394303\n",
      "Epoch 219, Loss: 0.45493112899589366\n",
      "Epoch 220, Loss: 0.45527199475335134\n",
      "Epoch 221, Loss: 0.45623630281032773\n",
      "Epoch 222, Loss: 0.45802054967026307\n",
      "Epoch 223, Loss: 0.45677224995978094\n",
      "Epoch 224, Loss: 0.45565914342704716\n",
      "Epoch 225, Loss: 0.45401674524485375\n",
      "Epoch 226, Loss: 0.4535057342791329\n",
      "Epoch 227, Loss: 0.45416164205845616\n",
      "Epoch 228, Loss: 0.45450513400796655\n",
      "Epoch 229, Loss: 0.4548238117351997\n",
      "Epoch 230, Loss: 0.4537990916311975\n",
      "Epoch 231, Loss: 0.4532521496977959\n",
      "Epoch 232, Loss: 0.4526084553640119\n",
      "Epoch 233, Loss: 0.45228981460773116\n",
      "Epoch 234, Loss: 0.4520949132230242\n",
      "Epoch 235, Loss: 0.45205797975469736\n",
      "Epoch 236, Loss: 0.45230557901617613\n",
      "Epoch 237, Loss: 0.4528345953247846\n",
      "Epoch 238, Loss: 0.4549386133976144\n",
      "Epoch 239, Loss: 0.4564022887834084\n",
      "Epoch 240, Loss: 0.45805755540321125\n",
      "Epoch 241, Loss: 0.4541498302115434\n",
      "Epoch 242, Loss: 0.45158727109343993\n",
      "Epoch 243, Loss: 0.45105347173746146\n",
      "Epoch 244, Loss: 0.45263971453254503\n",
      "Epoch 245, Loss: 0.4550743757756552\n",
      "Epoch 246, Loss: 0.4549471294161167\n",
      "Epoch 247, Loss: 0.45361945699341477\n",
      "Epoch 248, Loss: 0.4506913801287277\n",
      "Epoch 249, Loss: 0.44996455105922556\n",
      "Epoch 250, Loss: 0.45070657227233163\n",
      "Epoch 251, Loss: 0.45134097806864715\n",
      "Epoch 252, Loss: 0.4518641160838718\n",
      "Epoch 253, Loss: 0.45124870735997746\n",
      "Epoch 254, Loss: 0.4507253179113287\n",
      "Epoch 255, Loss: 0.44965336548788515\n",
      "Epoch 256, Loss: 0.4490457284512466\n",
      "Epoch 257, Loss: 0.44867439459283065\n",
      "Epoch 258, Loss: 0.4485724236213288\n",
      "Epoch 259, Loss: 0.4487178165615502\n",
      "Epoch 260, Loss: 0.44913423821312026\n",
      "Epoch 261, Loss: 0.45068397397968624\n",
      "Epoch 262, Loss: 0.4523160224282965\n",
      "Epoch 263, Loss: 0.4554805695192658\n",
      "Epoch 264, Loss: 0.4521284819909996\n",
      "Epoch 265, Loss: 0.4491460824324\n",
      "Epoch 266, Loss: 0.4475917876267889\n",
      "Epoch 267, Loss: 0.4489836201844808\n",
      "Epoch 268, Loss: 0.4522696841254308\n",
      "Epoch 269, Loss: 0.45230492011773493\n",
      "Epoch 270, Loss: 0.45030259118174343\n",
      "Epoch 271, Loss: 0.44750189058874124\n",
      "Epoch 272, Loss: 0.4466950387656147\n",
      "Epoch 273, Loss: 0.44782488644087964\n",
      "Epoch 274, Loss: 0.4490013591686314\n",
      "Epoch 275, Loss: 0.44932258069431275\n",
      "Epoch 276, Loss: 0.4477355520631983\n",
      "Epoch 277, Loss: 0.4464409066948383\n",
      "Epoch 278, Loss: 0.4458417591594104\n",
      "Epoch 279, Loss: 0.4457414301360274\n",
      "Epoch 280, Loss: 0.4460887501130773\n",
      "Epoch 281, Loss: 0.44690611604251484\n",
      "Epoch 282, Loss: 0.4483696750030082\n",
      "Epoch 283, Loss: 0.4491296928695683\n",
      "Epoch 284, Loss: 0.4498625247292945\n",
      "Epoch 285, Loss: 0.44791530084032105\n",
      "Epoch 286, Loss: 0.4460872803334146\n",
      "Epoch 287, Loss: 0.44485744850497494\n",
      "Epoch 288, Loss: 0.4448821514773518\n",
      "Epoch 289, Loss: 0.4460718473545706\n",
      "Epoch 290, Loss: 0.4475995445619453\n",
      "Epoch 291, Loss: 0.44939647973391844\n",
      "Epoch 292, Loss: 0.44907386765397006\n",
      "Epoch 293, Loss: 0.4480021879332198\n",
      "Epoch 294, Loss: 0.44517745931865826\n",
      "Epoch 295, Loss: 0.44400204700028173\n",
      "Epoch 296, Loss: 0.4442791745325936\n",
      "Epoch 297, Loss: 0.4453458104307112\n",
      "Epoch 298, Loss: 0.4473529547178379\n",
      "Epoch 299, Loss: 0.44795630153980215\n",
      "Epoch 300, Loss: 0.44784109084226464\n",
      "Training completed.\n",
      "Training completed.\n",
      "Testing model...\n",
      "Test RMSE: 0.23443511729541908\n",
      "Valence RMSE: 0.24262136432704848\n",
      "Arousal RMSE: 0.22595247733697327\n",
      "Test R^2 score: tensor([0.3710, 0.6235], dtype=torch.float64)\n",
      "Test R^2 score (overall): 0.4972219430843419\n",
      "Completed.\n"
     ]
    }
   ],
   "source": [
    "for num_epochs in num_epochs_list:\n",
    "  # Set the seed\n",
    "  torch.manual_seed(seed)\n",
    "\n",
    "  print(f'Num of epochs: {num_epochs}')\n",
    "  \n",
    "  model = train_model(num_epochs)\n",
    "\n",
    "  print(\"Training completed.\")\n",
    "  print(\"Testing model...\")\n",
    "\n",
    "  test_pred, rmse, adjusted_r2_score, r2_score = test_model(model)\n",
    "  adjusted_r2_scores_valence_list.append(adjusted_r2_score[0])\n",
    "  adjusted_r2_scores_arousal_list.append(adjusted_r2_score[1])\n",
    "  r2_scores_list.append(r2_score)\n",
    "  rmse_list.append(rmse)\n",
    "\n",
    "print(\"Completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the graph to visualise the relationship between the evaluation metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import relevant libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RMSE vs. num_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(num_epochs_list, rmse_list, color='b', linestyle='-')\n",
    "plt.title('num_epochs vs. Test RMSE')\n",
    "plt.xlabel('num_epochs')\n",
    "plt.ylabel('Tets RMSE') \n",
    "plt.grid(True)\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min RMSE score: 0.2332381147157868\n",
      "Corresponding R^2 SCore: 0.5013074040747145\n",
      "Corresponding num_epochs: 294\n"
     ]
    }
   ],
   "source": [
    "min_rmse = min(rmse_list)\n",
    "corresponding_r2_score = r2_scores_list[rmse_list.index(min_rmse)]\n",
    "corresponding_num_epochs = num_epochs_list[rmse_list.index(min_rmse)]\n",
    "\n",
    "print(f'Min RMSE score: {min_rmse}')\n",
    "print(f'Corresponding R^2 SCore: {corresponding_r2_score}')\n",
    "print(f'Corresponding num_epochs: {corresponding_num_epochs}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test R^2 Score vs. num_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkIAAAHHCAYAAABTMjf2AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAABiXUlEQVR4nO3deXgT1f4G8DfdW7pRu0OhlH0t0EpFdim7yKKyKquoF9ArIFeQK1AQyyKKCsJPEUGvCAoCylJogYJgZeeyyC5QEEqF0oUW2rQ5vz/OTdrQLYGkkzTv53n6TDIzmXxzkjZvz5yZUQkhBIiIiIhskJ3SBRAREREphUGIiIiIbBaDEBEREdksBiEiIiKyWQxCREREZLMYhIiIiMhmMQgRERGRzWIQIiIiIpvFIEREREQ2i0GIiEwiNDQUzz77rNJlEBEZhUGIiKgEHTt2hEqlKvdn5syZJnm+zz//HCtXrjR4/Yfr8PT0RIcOHbBly5ZyH7tt2zY4OjrC1dUV+/btK3W9nTt3YtSoUahXrx7c3NwQFhaGV155BTdv3jS4zl9++QUdOnSAv7+/bhsDBgxAXFycwdsgMicVrzVGRKYQGhqKJk2aYPPmzUqXYhLx8fG4deuW7v6hQ4fw6aef4t1330XDhg1185s1a4ZmzZo99vM1adIEvr6+SExMNGh9lUqFLl26YNiwYRBC4OrVq1i6dClu3ryJbdu2oVu3biU+7siRI+jYsSNq1qyJ+/fvIz09Hfv370eDBg2KrRsZGYm0tDS8+OKLqFu3Lv78808sXrwYbm5uOH78OAIDA8us8cMPP8TkyZPRoUMH9OnTB25ubrh48SISEhIQHh5uVPAjMhtBRGQCNWvWFL169VK6DLP58ccfBQCxe/dus2y/cePGokOHDgavD0CMGzdOb94ff/whAIgePXqU+JjLly+LwMBA0aRJE5GamiquXr0qwsLCRGhoqEhJSSm2/p49e0RBQUGxeQDEtGnTyqxPrVYLT09P0aVLlxKX37p1q8zHm1JBQYG4f/9+hT0fWRfuGiObM3PmTKhUKly8eBEjRoyAt7c3vLy8MHLkSOTk5OjWu3LlClQqVYn/tT68S0S7zfPnz+Oll16Cl5cX/Pz88N5770EIgWvXrqFPnz7w9PREYGAgFi5c+Ei1b9u2De3atUOVKlXg4eGBXr164fTp03rrjBgxAu7u7vjzzz/RrVs3VKlSBcHBwZg1axbEQx3A2dnZmDRpEkJCQuDs7Iz69evjww8/LLYeAPznP/9Bq1at4ObmhqpVq6J9+/bYsWNHsfX27duHVq1awcXFBWFhYfjmm2/0lqvVasTExKBu3bpwcXHBE088gbZt2yI+Pr7U13348GGoVCqsWrWq2LLt27dDpVLpeqKysrLw1ltvITQ0FM7OzvD390eXLl1w9OjR0hv2MRjynqSkpGDkyJGoXr06nJ2dERQUhD59+uDKlSsAZG/a6dOnsWfPHt2uro4dOxpdS8OGDeHr64tLly4VW5aWloYePXrAz88Pu3btgp+fH2rUqIHExETY2dmhV69eyM7O1ntM+/btYWdnV2yej48Pzpw5U2Ytt2/fRmZmJtq0aVPicn9/f737Dx48wMyZM1GvXj24uLggKCgI/fv313sthn5eVSoVxo8fj++++w6NGzeGs7OzblfcX3/9hVGjRiEgIADOzs5o3LgxVqxYUeZrocqNQYhs1oABA5CVlYXY2FgMGDAAK1euRExMzGNtc+DAgdBoNJg7dy6ioqLw/vvvY9GiRejSpQuqVauGefPmoU6dOnj77bexd+9eo7b97bffolevXnB3d8e8efPw3nvv4Y8//kDbtm11X6haBQUF6N69OwICAjB//nxERERgxowZmDFjhm4dIQSee+45fPzxx+jevTs++ugj1K9fH5MnT8bEiRP1thcTE4OXX34Zjo6OmDVrFmJiYhASEoJdu3bprXfx4kW88MIL6NKlCxYuXIiqVatixIgResFg5syZiImJQadOnbB48WJMmzYNNWrUKDOoREZGIiwsDD/88EOxZWvXrkXVqlV1u4Jef/11LF26FM8//zw+//xzvP3223B1dS33i/tRGPqePP/889iwYQNGjhyJzz//HG+++SaysrKQnJwMAFi0aBGqV6+OBg0a4Ntvv8W3336LadOmGV1PRkYG7t69i6pVq+rNz83NRZ8+feDk5KQLQVohISFITExEeno6XnzxReTn55f5HPfu3cO9e/fg6+tb5nr+/v5wdXXFL7/8grS0tDLXLSgowLPPPouYmBhERERg4cKF+Oc//4mMjAycOnUKgHGfVwDYtWsXJkyYgIEDB+KTTz5BaGgobt26haeeegoJCQkYP348PvnkE9SpUwejR4/GokWLyqyRKjElu6OIlDBjxgwBQIwaNUpvfr9+/cQTTzyhu3/58mUBQHz99dfFtgFAzJgxo9g2X331Vd28/Px8Ub16daFSqcTcuXN18+/evStcXV3F8OHDDa45KytLeHt7izFjxujNT0lJEV5eXnrzhw8fLgCIN954QzdPo9GIXr16CScnJ/H3338LIYTYuHGjACDef/99vW2+8MILQqVSiYsXLwohhLhw4YKws7MT/fr1K7abRKPR6G7XrFlTABB79+7VzUtNTRXOzs5i0qRJunnh4eGPtAtt6tSpwtHRUaSlpenm5ebmCm9vb7330svLq9guI1N4eNeYoe/J3bt3BQCxYMGCMrf/KLvGRo8eLf7++2+RmpoqDh8+LLp3727Qcz2O2bNnCwBi586d5a47ffp0AUBUqVJF9OjRQ8yZM0ccOXKk2HorVqwQAMRHH31UbJn2M2bo51UI2TZ2dnbi9OnTeuuOHj1aBAUFidu3b+vNHzRokPDy8hI5OTnlviaqfNgjRDbr9ddf17vfrl073LlzB5mZmY+8zVdeeUV3297eHpGRkRBCYPTo0br53t7eqF+/Pv7880+DtxsfH4/09HQMHjwYt2/f1v3Y29sjKioKu3fvLvaY8ePH625rdxXk5eUhISEBALB161bY29vjzTff1HvcpEmTIITAtm3bAAAbN26ERqPB9OnTi+0mUalUevcbNWqEdu3a6e77+fkVe63e3t44ffo0Lly4YPDrB2Rvm1qtxk8//aSbt2PHDqSnp2PgwIF62z9w4ABu3Lhh1PaNZeh74urqCicnJyQmJuLu3bsmreGrr76Cn58f/P39ERkZiZ07d+Jf//pXiT0kprB3717ExMRgwIABeOaZZ8pdPyYmBqtXr0aLFi2wfft2TJs2DREREWjZsqVeD9369evh6+uLN954o9g2tJ8xQz+vWh06dECjRo1094UQWL9+PXr37g0hhN571q1bN2RkZJht9ylZNgYhslk1atTQu6/dnfA4X1YPb9PLywsuLi7FdiN4eXkZ9Tza0PDMM8/Az89P72fHjh1ITU3VW9/Ozg5hYWF68+rVqwcAul02V69eRXBwMDw8PPTW0x4RdfXqVQDApUuXYGdnp/elUpqHXz8g27Xoa501axbS09NRr149NG3aFJMnT8aJEyfK3XZ4eDgaNGiAtWvX6uatXbsWvr6+el/K8+fPx6lTpxASEoJWrVph5syZRoVOQxn6njg7O2PevHnYtm0bAgIC0L59e8yfPx8pKSmPXUOfPn0QHx+PLVu26Map5eTkFAuspnD27Fn069cPTZo0wfLlyw1+3ODBg/Hrr7/i7t272LFjB4YMGYJjx46hd+/eePDgAQD5Gatfvz4cHBxK3Y6hn1etWrVq6d3/+++/kZ6eji+++KLY+zVy5EgAKPZ7RLah9E8dUSVnb29f4nzxv4GXD/d2aBUUFBi1zfKexxAajQaAHJNS0iHLZX2BVCRDXmv79u1x6dIlbNq0CTt27MDy5cvx8ccfY9myZXo9aiUZOHAg5syZg9u3b8PDwwM///wzBg8erPf6BwwYgHbt2mHDhg3YsWMHFixYgHnz5uGnn35Cjx49TPNCYdx78tZbb6F3797YuHEjtm/fjvfeew+xsbHYtWsXWrRo8cg1VK9eHdHR0QCAnj17wtfXF+PHj0enTp3Qv3//R97uw65du4auXbvCy8sLW7duLRZGDOHp6YkuXbqgS5cucHR0xKpVq3DgwAF06NDBZHUW5erqqndf+3699NJLGD58eImPMcVpEMj6WMZfTyILpO0hSk9P15v/8H+eFaF27doA5ABU7RdfWTQaDf78809dLxAAnD9/HoA8QgkAatasiYSEBGRlZel9sZ09e1a3XPvcGo0Gf/zxB5o3b26KlwMfHx+MHDkSI0eOxL1799C+fXvMnDnToCAUExOD9evXIyAgAJmZmRg0aFCx9YKCgjB27FiMHTsWqampaNmyJebMmWPSIGTse1K7dm1MmjQJkyZNwoULF9C8eXMsXLgQ//nPfwCUHryN8dprr+Hjjz/Gv//9b/Tr188k27xz5w66du2K3Nxc7Ny5E0FBQY+9zcjISKxatUp3YsbatWvjwIEDUKvVcHR0LPExhn5eS+Pn5wcPDw8UFBQY9H6R7eCuMaJSeHp6wtfXt9jRXZ9//nmF19KtWzd4enrigw8+gFqtLrb877//LjZv8eLFuttCCCxevBiOjo7o3LkzANmDUFBQoLceAHz88cdQqVS60NC3b1/Y2dlh1qxZuv+qi27XWHfu3NG77+7ujjp16iA3N7fcxzZs2BBNmzbF2rVrsXbtWgQFBaF9+/a65QUFBcjIyNB7jL+/P4KDg/W2f/v2bZw9e1bvdAnGMvQ9ycnJ0e0C0qpduzY8PDz0aqpSpUqx0G0sBwcHTJo0CWfOnMGmTZsea1uAPFy9Z8+e+Ouvv7B161bUrVvX4Mfm5OQgKSmpxGXa8Tz169cHII+qu337drHPIlD4GTP081oae3t7PP/881i/fr3uSLSiSvodItvAHiGiMrzyyiuYO3cuXnnlFURGRmLv3r26npWK5OnpiaVLl+Lll19Gy5YtMWjQIPj5+SE5ORlbtmxBmzZt9L4gXFxcEBcXh+HDhyMqKgrbtm3Dli1b8O677+oOne7duzc6deqEadOm4cqVKwgPD8eOHTuwadMmvPXWW7oejzp16mDatGmYPXs22rVrh/79+8PZ2RmHDh1CcHAwYmNjjXotjRo1QseOHREREQEfHx8cPnwY69at0xvcXZaBAwdi+vTpcHFxwejRo/XGw2RlZaF69ep44YUXEB4eDnd3dyQkJODQoUN6525avHgxYmJisHv37kc6Xw9g+Hty/vx5dO7cGQMGDECjRo3g4OCADRs24NatW3q9WREREVi6dCnef/991KlTB/7+/gYNSH7YiBEjMH36dMybNw99+/Z9pNemNXToUBw8eBCjRo3CmTNn9AY4u7u7l7n9nJwcPP3003jqqafQvXt3hISEID09HRs3bsSvv/6Kvn376nYLDhs2DN988w0mTpyIgwcPol27dsjOzkZCQgLGjh2LPn36GPx5LcvcuXOxe/duREVFYcyYMWjUqBHS0tJw9OhRJCQklHuYP1VSCh2tRqQY7aHu2sPItb7++msBQFy+fFk3LycnR4wePVp4eXkJDw8PMWDAAJGamlrq4fMPb3P48OGiSpUqxWro0KGDaNy4sdG17969W3Tr1k14eXkJFxcXUbt2bTFixAhx+PDhYs956dIl0bVrV+Hm5iYCAgLEjBkzih3+npWVJSZMmCCCg4OFo6OjqFu3rliwYIHeYfFaK1asEC1atBDOzs6iatWqokOHDiI+Pl63vLQzS3fo0EHvsPD3339ftGrVSnh7ewtXV1fRoEEDMWfOHJGXl2dQG1y4cEEAEADEvn379Jbl5uaKyZMni/DwcOHh4SGqVKkiwsPDxeeff663nvb9MuYs0aWdWbq89+T27dti3LhxokGDBqJKlSrCy8tLREVFiR9++EFvOykpKaJXr17Cw8NDACj3UHqUcGZprZkzZ5rkLNjaUyKU9FOzZs0yH6tWq8WXX34p+vbtK2rWrCmcnZ2Fm5ubaNGihViwYIHIzc3VWz8nJ0dMmzZN1KpVSzg6OorAwEDxwgsviEuXLunWMfTzWlbb3Lp1S4wbN06EhITonqdz587iiy++eLRGIqvHa40RVTIjRozAunXrcO/ePaVLISKyeBwjRERERDaLY4SIFPb333+XeUi+k5MTfHx8KrAiIiLbwSBEpLAnn3yyzEPyO3TogMTExIoriIjIhnCMEJHC9u/fj/v375e6vGrVqoiIiKjAioiIbAeDEBEREdksDpYmIiIim8UxQuXQaDS4ceMGPDw8THK6eiIiIjI/IQSysrIQHBxc5oWIGYTKcePGDYSEhChdBhERET2Ca9euoXr16qUuZxAqh/bifteuXYOnp6dJtqlWq7Fjxw507dq11AsMksS2Mg7by3BsK8OxrYzD9jKcOdsqMzMTISEhehfpLQmDUDm0u8M8PT1NGoTc3Nzg6enJX5JysK2Mw/YyHNvKcGwr47C9DFcRbVXesBYOliYiIiKbxSBERERENotBiIiIiGwWgxARERHZLAYhIiIislkMQkRERGSzGISIiIjIZjEIERERkc1iECIiIiKbxSBERERENotBiIiIiGwWgxARERHZLAYhIiIiMrlr14ALF5SuonwMQkRERFSuDz4AXnwR2LABKCgofb3kZKBFC6BGDaB+feCbb0pfNz/f9HUai0GIiIjICmzfDrRtC8ybB2RkFF+ekgJMnAg0bAj8/LP+MrUa2LMHePDAuOdMT5e9OitWANOmAevWAf37A2+/Xfpjli8Hjh+Xt4UARo4Exo4Ffvih+Lovv2yPN9/shPh4lXGFmRCDEBER0SP69VdgwACgTx/g22/lvP37Vbh1y7XYumlpQG6uvH3sGLBwIRATA9y4AWzaJLdx5UrJz3PtGjBoELB/PzBlCvDMM4BGU7j8wQOgdWvg44+Bs2eBIUOAJUtkCDl/Hpg/H+jYEXj6aeCdd4CAAMDDA2jXTj7n8uXAF1/I4KKl0QBt2gD16gGjR8t50dFyunQpkJoq63q4V2fTJjldsQIYNUpuZ+lSYOBAYO9e/e0nJqqQnOwJd3dDWts8HJR7aiIiIstTUAAsWADY2wPu7sDmzbJnJDgY+OorwNOzcN233gKOHpW3t2wBDh0CPvvMAcHBT2PECBmO6tYFgoKA5s0BJyegRw9g1arCbSxZAty5I4NBaCjwySeFy06eBGbPls+Rng40aSJ7aI4elQGnQQO53t69MtD4+gJ16gC//w6MHy+XXb4MXLokbx87Jn+09u2T9WnDTJMmwE8/AdevA2+8AfzxR+G6vXrJnqannpKvMzpa1vfGG8Cnn8pdYmo1cOKEbLvnngOGDQO6dwcWLQJ++03uVmvfvvC13bmjgotLPiIjiySwiiaoTBkZGQKAyMjIMNk28/LyxMaNG0VeXp7JtllZsa2MY8ntpVYL8fnnQowYIcStW6bZ5p07Qpw8KafGsuS2sjQV2VZXrwrxySdCXL5c9noFBUJs3y7Ehg1CaDRC5OUJkZ4ul927J8TNmyU/7uhRIYYNE2LzZiHu3hViwgQhdu/WX2fDBiFk30jxn6++Klzv7l0hVCo5v3Pn4utOn54vACG8vISYOLH48mefFaJBA/15oaFC7Nkjl125IsTgwYXL3N2FuHBBiLZt5f2VKwtrmTBBzhs9WoiUFCHCwuTzOjgUPt7HR4j27YVo3lyI9etlWzRsqP/8LVoU3tYu69NHiAMHhMjNlc+1erX+Y3x9ZXsCQnh7y2mHDvptun69nF+7thDvvCOf5+235byWLVPM8tky9PubQagcDELKYlsZx9La6/vvhRg0SIhGjYTw9y/8w/nyy0JcuybEf/4jxIMHj7btjAwhnnhCbs/OTn55GaOi2qqgQH5RlycpSYiFCw1rD23JGo0QixcLsWqVEPfvG17TlStCvP++EDNnCvH774Y8n2yrmzfzxL17ct7580JcvFj+Y8+dE+Kll4R45RUhVqyQ7VGa+fOFcHSU76mnpxCLFgmxb58Q+fn66128KETduoWfp759hQgMlF/C588LEREht7N+vQwG4eFCzJkjxNCh8rMCCOHiIkSbNvJ23boyPHTsKMSyZUK8+66cHxIiRLt2QsydK8TAgXLeq68KcfiwbL916woff/euXB8QwtlZIwAhVCpNsfDTvbsMItrPa0aGEG+8IcQHH8iaACGqVpXTd96RvzuAEFOmyN8ZIYSYNEnOe/11OX/atMLQ8sMPcp0HD+TnZNiwwud+663ibX7njhBLlgjx3Xelh7///Ofhz4MMWvK1ymmNGvqP+egj/cdkZQnh5FTy9keMOMkgZMkYhJTFtjJORbfXvXvyi/TiRfmHfe5cIb74Qv5Xn5BQ/A+ej4/43xeE/C8SEGLIECHS0oRYulSI3r2FGD9e/nfZpo0Qr71W+nNv366/7UGDZBjYubMwTDx4IP+D1/Ya/OtfQgwfLsTx44VtlZZW2FZ37+qHll275JedRiO/yDt3ll+0Qsh5168Xrl9SEDlwQH65hYUJMXt28S90LY1GiJo15evo2VNuKztbiDFjhPjxR/11164VwsNDiFGjhNi6tfD1V68uvygvXxZi+XL5eK3162V7+vkJceyYED16FD6ualX5fBcvCnHmTOHrSUmRIevaNSG+/lotQkIyBCBErVry/XFykqFi1CghUlPlF/r27bLn79o1IWbMEOLsWSFattR/n6KjZe/IihX6r+vevcKQEhio/5h33tFf96WXhK6nRfsY7U/9+qV/oWt/qlUrPu/55wuXde8ub3/+uX4bAjLENG0qb2uD+Jgxch1t23/7rbrE5/X01H9fHta7t/76bdsKYW8vb1+/Xrjejz/KeR4e+uvb2cnfpaJOnSpcfvJk6c+t0RSGLgeHwoDj4FB8m0IIceOG3F7RHitAiDp1ZM+VNrQV1a1bye/Hhx/uZhCyZAxCymJbGaci2ysnR4jIyJL/sNWuLbvTAfmlGxcnA1NWVvE/nEBhL0BJP4mJMsxs367//DNnFgYAQIjgYBl0APnlO2eO3M2g3Y52fe3PkiX54rXXjgt7e4147TUh3ntPzg8NFWLWLCH++9/CuoYOLXyct7e836RJ4ZfgunVy3ZkzZW9L585C/PRT8de6bFnJbXnsmP56r74qd3toQ6M2DH31VeE6zs4yNBZ93Jw5QjzzjLxdr54MfCdO6K/Tr1/h7hLtboxp0wr/Ww8Nle/V00/L+9ov4rJ+AgKECAoqDLZPPqn/vnp7y/dG2+Oh/Tl+XAbLkydlj5g2BKnVQixYIMRTTxXWdPKk3J0yf35hrQcPys9WRIQMuEW3rQ0pnp7y/ezWTYh//EOG09u3C3tvAgKKvx5tnUlJhe/RtWuF78fD6z/cY5KZmSdcXGQYateueGAqzZdflty+Pj76AT05ueT1nn665O2uXq2/G600X3whtzNunAy4gPwsl6XobrKnnpJhPyur5HWXLCl8b7S9cd7eGrF+vXn+ZjEImQiDkLLYVsYxZXvl5so/uCXRaOQXnvYL2dlZfvEOHVrY01P0y66oP/+U/3U//bTcHaBdr2lT2WuiDVe1ahV+CQBCVKkie3h++EGI+PjC/y4XLCj8wn34P2SgeI+Btkvf1VUjnJxK/s9d+3wPz/PzK3ldV1c59fWVPUfaurVfqNog5esr633/ff0vNm1I0/YKeXsLMXJk4fadnGSPjbYn4uHnjYoqnBYdE9K4ceGX68PvS6NGheHv4R83N/0vfQcHjRg8+A+xd69a18a1askQ0rhx+UHpyy/l6zx1Sog33ywch9Krl+w9cHOTbQLI91Xr3r3C0PNw6G7VqvjnUjtOp0oVGVz+7//krrmSpKXJMTL/938l12xnp997o9EUhr2Hfx7u/cjLyxO9e18U9vYakZAgxDffyHCu7U0sTWqq7KGrV09/N1LHjvrraTQy+GuXa1/3w7ujjKXRyN9XtVr2CL76qgzpZUlLKwzLpQV9rfv3hZg8WYhffxXiyBH5+/rmm/lm+xvPIGQiDELKYlsZx1Tt9ddf8o+xg4Pc/VJURobsVZBfkLLHpqii/9U++WTJ29cOuhRC7mZJTCwMBhqN/I/9yhX9L3Wg8EvL0VF+gQJyvIa29wKQPQC9e8svhy++kLtntIHEw0N+2XTqVLh+nToaXVhauFD+Mdf+YffwkLtCADkOJDNT7ppasEC+zjffLP6l+HCPQcOGckzFw4NiExKEWLNGBift+I4vv5T1Fw0j2i/EFSsKw5m2p0T7s22b/v2iX5IDBsjpxIn64zimTZPhSnvfwUG2ZdFdSx98IMT+/UKcOFH4udq9W+5GOnFCvl/378ueqAULhJg+vfCxixfL9nn77eLjghITi7ebtubJk/XX7dKl5PCxalXxz9Xhw3I7CxeW+fHWc/Nm4Xum3R2kDYoP0/ZyAoU9Gg0aFF8vLy9PrF+/UaSkGP97ePu2/B3T9qoBsh0fpv0d7NhRtu/hw6XvejW3adOE6NpV/n4YQ6027994BiETYRBSFtvKOI/TXnfvyh6Xo0f1v7SDggqPytJoCne9ODkJ8e23xbeTn1/YS1D0CJtHod31ox08+vBuDFdXGTLeeadw3ssvF9/OrFmFX+xCCHHpkhDu7rJH6NSpPHHwoDxSR2v9etn78sMPslestP+MHzyQPRO+vvpfkkV3Ac2dK9dNSND/L3/BAv2xMHZ2MqQ9PE7kxRfldMyYwqBVdBdZrVryfSk6GH3s2ML3UNtb9p//6LfT4cOyrg4dCh8jhOy18fKS76F23JOhn6v8fBm4pk4te4C4RlN6T9LDu5k+/rhwWXi43P6AAY8+yL4kPXvK92b+/MLneuml4usV7cG8cEEG2ZLG3Zji79brrxc+1/LlxZfv2iV7mQwZ7G7JGISsAIOQsthWximvvbKz5Rfz6dP687dskT0C9vaF/x2HhBQemaMd2/Dtt4UBpOj4iYclJ8uxA2UdIWQItVruTli6tOQvzfbtC+vXzvv55+Lb0WjkF1fRL+ezZ/PE55/HP/ZnS62WgeHgwcIapk+X/yH7+clBpVr37gkREyPX0QYQ7Y9298eiRfq9Eh9+KG9re3NCQmQvhnadESPk47Q9P4AQmzYVHzNz6pR83x0dZQ+Uti0uXJBf8EXHdaSnyzFgWub4PVyzRtb1cI+PtqdJ69y5wmWffWayp9dz757cvXXvXmEvZEm9StpxTC1alL09U7TX8uWFr/vgwUfejMWzhCDEM0sT2ZB//UuelfbVVwvnXbsGvPyyPKFaQYH80ztkiDz52ZIlcp1Nm+Qp/bWn1X/vPXlStdKEhACDBwN2j/kXxsFBnuytY0f9+fXqyenTT8tp27byRHIhIUDXrsW3o1LJk8ypipzFPywMCA7OfrwC/1ejiwsQGSnbxMtLnoV361Z5yYOgoMJ1q1SR12AC5OUOAKBRI2DNmsLrMT3zTOH6bdrI1w/Ik9Vp6w4MBMLD5f1OnfSnDg7ydlRU4XacneU1nxo1kifT27WrsC3q1AGmToXemX29vADX4idGNqmBA4GcHHnZBe3nxMmp8ASBWnXrAl26ALVqAUOHmqeWKlWA6tXltHt3eTLAzp2Lr/fUU0BCArBxo3nqKCoyUk7t7IDGjc3/fLaMZ5YmshGHDgGffy5v//ab/JJ2cZHXDUpLAyIi5HWEHBzklwIgw4WjozyV/oIFwK1b8otz0qSKrb1+fXlJgFu3ZABYswb47DNgwgS53NNTns3WwUF+6StBpQJ27pSXOvDxKX09bYDRatNGhgKtxo0BPz/g779lwKtTR3/9sDA5/eoree2pIUPk/b595UUxu3WTl04oGoSaNpVto92+pXB1lT8tWgBHjsig5uiov45KBezYIQO6qgIuR/Xdd/Lz/nC7a5UUkMyhWTP5j0twMODmVjHPaasYhIhsxFtvyS8TQE6/+07+N374MPDEE8DatfL0/kW5uMiA9Pvv8hT6gPzidXKqyMrlF2DnzsDq1fJ6TA0aFPZWaRXteVGKm1v5X1ohIYC3t7xcAgC0aqW/3M5OXhdqyxYZUh0c5OvXvnfaIBQRIX+0AgMLe40AGX5cXGQwa978MV5UBYiOlkGoZcvS16mIEATIUF30EhpKUankxVXJ/LhrjMgGnD8ve4EcHAqvPzR5MnDwoOy92LkTqF275Me2bi2nWVly2q2b+estyYIF8iKVU6Yo8/ymolLpB5Mnnyy+zogRwI8/yt1VLi4yPGlpg1B5HB0Lt11WwLAEU6cCM2fKC5ASVTQGIaJK7NIleTHG77+X97t0AV5/Xd4WQu5G2rat+O6aorTjcAA5duThHoyKEhwMTJxo/rErFUHb3q6uhu2qKrqbxtAgBMgrkU+eLIOVJfPyAmbMKNwlS1SRuGuMqJIqKJADb5OT5SBQQO7WatRI7jY5eRL44ovyg422RwiQQcqBfzUem3YgbKtWhrVnnTpygDNgXBB6ePcZERXHP2lEldSePYVjRrKz5S6WPn3krpm4OODGjcIv5LJUqwbUqCG3pdRuscpm8GA5ILd7d8PW1x45VqWKHEhNRKbDIERUSa1eLafBwTL0DBokjybSzgsONnxbCxfKwbuDB5u+Tltkby938xmqfn05rVu34gYNE9kKBiGiSujBA3koPCAD0RNPlD4Y2hAvvCB/SBndu8tDqdkjR2R6DEJEldCOHSpkZMjdWu3aPf6JDUlZjo48lJrIXPjnkagS2rlT7j/p148hiIioLPwTSVQJ7d0rf7UfvjQFERHpYxAiqmQyM51w+rTsEWrfXuFiiIgsHMcIEVUSZ84AH35ojzt35LHW2mtWERFR6RiEiCqBNWuAV14BsrPtAMjTEHfooGxNRETWgLvGiKyUEMDFi8Abb8jz+2RnA15eQrec44OIiMrHIERkhYQAevWSJ9hbvFjOe/dd4MKFfDRseAfVqwt06aJsjURE1oBBiMgKHTwoL5ZqZwe0aQNs3gzMmQN4ewMffLAPFy7kw9tb6SqJiCwfxwgRWaFly+T05ZeBlSv1l6lU8hIORERUPqvrEVqyZAlCQ0Ph4uKCqKgoHDx40KDHrVmzBiqVCn379jVvgURmlJcHHD4sB0cDwGuvKVsPEZG1s6ogtHbtWkycOBEzZszA0aNHER4ejm7duiE1NbXMx125cgVvv/022rVrV0GVEpmeEHI32JNPymuJNWsGPPWU0lUREVk3qwpCH330EcaMGYORI0eiUaNGWLZsGdzc3LBixYpSH1NQUIChQ4ciJiYGYWFhFVgtkWn99ZfsDVKpgObNgQ8/5JXIiYgel9UEoby8PBw5cgTR0dG6eXZ2doiOjkZSUlKpj5s1axb8/f0xevToiiiTyGwOHZLT8HDg2DHwqDAiIhOwmsHSt2/fRkFBAQICAvTmBwQE4OzZsyU+Zt++ffjqq69w/Phxg58nNzcXubm5uvuZmZkAALVaDbVabXzhJdBux1Tbq8zYVoWSkuwA2CMiQgO1uqDEddhehmNbGY5tZRy2l+HM2VaGbtNqgpCxsrKy8PLLL+PLL7+Er6+vwY+LjY1FTExMsfk7duyAm5ubKUtEfHy8SbdXmbGtgLi4pwH4wcXlv9i6NbnMddlehmNbGY5tZRy2l+HM0VY5OTkGracSQojyV1NeXl4e3NzcsG7dOr0jv4YPH4709HRs2rRJb/3jx4+jRYsWsC9yHLFGowEgd6mdO3cOtWvXLvY8JfUIhYSE4Pbt2/D09DTJa1Gr1YiPj0eXLl3g6Ohokm1WVmwrSaMB/P0dkJmpwuHDajRrVvJ6bC/Dsa0Mx7YyDtvLcOZsq8zMTPj6+iIjI6PM72+r6RFycnJCREQEdu7cqQtCGo0GO3fuxPjx44ut36BBA5w8eVJv3r///W9kZWXhk08+QUhISInP4+zsDGdn52LzHR0dTf4mmWOblZWtt9XZs0BmJuDqCoSHO8KhnN9cW28vY7CtDMe2Mg7by3Dm+o41hNUEIQCYOHEihg8fjsjISLRq1QqLFi1CdnY2Ro4cCQAYNmwYqlWrhtjYWLi4uKBJkyZ6j/f+36l2H55PZMnu3gUWLJC3IyJQbggiIiLDWdWf1IEDB+Lvv//G9OnTkZKSgubNmyMuLk43gDo5ORl2dlZzIBxRufLzgbZtgT/+kPcHDlS2HiKiysaqghAAjB8/vsRdYQCQmJhY5mNXPnwtAiILt3WrDEFVq8pLafTurXRFRESVi9UFISJb8sUXcvrKK8BzzylbCxFRZcT9SEQWKjlZXmEekEGIiIhMj0GIyAJt3Qq0aycPm+/UCahXT+mKiIgqJ+4aI7Iwe/bI3WAFBUCNGsDChUpXRERUebFHiMiC3LgBDBggQ9CAAcCZM0CLFkpXRURUeTEIEVkIIYARI4DUVKBZM+DrrwETX9WFiIgewiBEZCGWLgXi4+XZo3/4gSGIiKgiMAgRWYDUVOBf/5K3584F6tdXth4iIlvBIERkAWJjgexsIDISKOV8oUREZAYMQkQKu35d7hYDgDlzAF4lhoio4vBPLpHCli4FcnOB9u2BLl2UroaIyLYwCBEpbMcOOR09GlCplK2FiMjWMAgRKejuXeDIEXm7c2dlayEiskUMQkQKSkyU5w9q0ACoVk3paoiIbA+DEJGCEhLklL1BRETKYBAiUtDOnXLKIEREpAwGISKFnDsnf+ztgY4dla6GiMg2MQgRKeTbb+W0WzegalVlayEislUMQkQK0GiA//xH3h42TNlaiIhsGYMQkQJ+/RW4ehXw9ASee07paoiIbBeDEJECtLvFXnhBXm2eiIiUwSBEVMHu3wd+/FHefvllZWshIrJ1DEJEFeyXX4DMTKBGDXl9MSIiUg6DEFEF0+4WGzqUV5onIlIa/wwTVaDkZCAuTt7mbjEiIuUxCBFVoAULgPx8oFMnoGFDpashIiIGIaIKcusWsHy5vD1tmrK1EBGRxCBEVEG+/BJ48ABo1Qp45hmlqyEiIoBBiKjC7Nsnp8OHAyqVsrUQEZHEIERUAYQADh+Wt1u1UrYWIiIqxCBEVAGuXAHu3AEcHYGmTZWuhoiItBiEiCqAtjeoWTPA2VnZWoiIqBCDEFEF0AahJ59Utg4iItLHIERUAQ4dktPISGXrICIifQxCRGam0QBHjsjbDEJERJaFQYjIzC5flhdZdXYGGjdWuhoiIiqKQYjIzE6elNNGjQAHB2VrISIifQxCRGZ24oSc8rB5IiLLwyBEZGbaHqFmzZStg4iIimMQIjIzbRBijxARkeVhECIyo/v3gQsX5G0GISIiy8MgRGRGf/whD5/39QUCA5WuhoiIHsYgRGRGRXeL8YrzRESWh0GIyIx4xBgRkWVjECIyIx4xRkRk2RiEiMyIR4wREVk2BiEiM0lNBW7dkmODeGkNIiLLxCBEZCba3qDatYEqVZSthYiISsYgRGQm3C1GRGT5GISIzIRHjBERWT4GISIz4RFjRESWj0GIyAwKCoDTp+Vt9ggREVkuBiEiMzh5Ul5nzN1dDpYmIiLLxCBEZAa7d8tpu3aAvb2ytRARUekYhIjMQBuEOnVStg4iIiobgxCRiRUUAHv3ytsMQkRElo1BiMjEjh0DMjIALy+gRQulqyEiorIwCBGZmHa3WPv2HB9ERGTprC4ILVmyBKGhoXBxcUFUVBQOHjxY6rpffvkl2rVrh6pVq6Jq1aqIjo4uc30iU+D4ICIi62FVQWjt2rWYOHEiZsyYgaNHjyI8PBzdunVDampqiesnJiZi8ODB2L17N5KSkhASEoKuXbvir7/+quDKyVao1cCvv8rbDEJERJbPqoLQRx99hDFjxmDkyJFo1KgRli1bBjc3N6xYsaLE9b/77juMHTsWzZs3R4MGDbB8+XJoNBrs3LmzgisnW3HkCHDvHuDjwzNKExFZAwelCzBUXl4ejhw5gqlTp+rm2dnZITo6GklJSQZtIycnB2q1Gj4+PqWuk5ubi9zcXN39zMxMAIBarYZarX7E6vVpt2Oq7VVm1tZWCQl2AOzRrp0GBQUFKCio2Oe3tvZSEtvKcGwr47C9DGfOtjJ0m1YThG7fvo2CggIEBATozQ8ICMDZs2cN2sY777yD4OBgREdHl7pObGwsYmJiis3fsWMH3NzcjCu6HPHx8SbdXmVmLW21bl1rAP7w8zuFrVsvK1aHtbSXJWBbGY5tZRy2l+HM0VY5OTkGrWc1QehxzZ07F2vWrEFiYiJcXFxKXW/q1KmYOHGi7n5mZqZubJGnp6dJalGr1YiPj0eXLl3g6Ohokm1WVtbUVnl5wJAh8ldq7NiGaNKkYYXXYE3tpTS2leHYVsZhexnOnG2l3aNTHqsJQr6+vrC3t8etW7f05t+6dQuBgYFlPvbDDz/E3LlzkZCQgGblDNxwdnaGs7NzsfmOjo4mf5PMsc3Kyhra6sABICcH8PMDmjd3hEqlXC3W0F6Wgm1lOLaVcdhehjPXd6whrGawtJOTEyIiIvQGOmsHPrdu3brUx82fPx+zZ89GXFwcIiMjK6JUslHaw+Y7doSiIYiIiAxnNT1CADBx4kQMHz4ckZGRaNWqFRYtWoTs7GyMHDkSADBs2DBUq1YNsbGxAIB58+Zh+vTpWL16NUJDQ5GSkgIAcHd3h7u7u2Kvgyonnj+IiMj6WFUQGjhwIP7++29Mnz4dKSkpaN68OeLi4nQDqJOTk2FnV9jJtXTpUuTl5eGFF17Q286MGTMwc+bMiiydKrkHD4DffpO3GYSIiKyHVQUhABg/fjzGjx9f4rLExES9+1euXDF/QUQAfv8dyM0FAgOB+vWVroaIiAxlNWOEiCxZ0d1iHB9ERGQ9GISITODoUTl9+mll6yAiIuMwCBGZwOnTctqkibJ1EBGRcRiEiB5TdjZw+X8nkW7cWNlaiIjIOAxCRI/pzBk59fOTP0REZD0YhIgek3a3GHuDiIiszyMHoYsXL2L79u24f/8+AEAIYbKiiKwJxwcREVkvo4PQnTt3EB0djXr16qFnz564efMmAGD06NGYNGmSyQsksnTsESIisl5GB6EJEybAwcEBycnJcHNz080fOHAg4uLiTFockTVgECIisl5Gn1l6x44d2L59O6pXr643v27durh69arJCiOyBllZgPZjzyBERGR9jO4Rys7O1usJ0kpLS4Ozs7NJiiKyFtreoKAgwMdH2VqIiMh4Rgehdu3a4ZtvvtHdV6lU0Gg0mD9/PjrxapNkY06ckNNmzZStg4iIHo3Ru8bmz5+Pzp074/Dhw8jLy8O//vUvnD59Gmlpadi/f785aiSyWAxCRETWzegeoSZNmuD8+fNo27Yt+vTpg+zsbPTv3x/Hjh1D7dq1zVEjkcViECIism5G9Qip1Wp0794dy5Ytw7Rp08xVE5FVEAI4eVLeZhAiIrJORvUIOTo64oT2X2AiG3f9OpCeDjg4AA0aKF0NERE9CqN3jb300kv46quvzFELkVXR/k/QoAHg5KRsLURE9GiMHiydn5+PFStWICEhAREREahSpYre8o8++shkxRFZMo4PIiKyfkYHoVOnTqFly5YAgPPnz+stU6lUpqmKyAowCBERWT+jg9Du3bvNUQeR1eFAaSIi6/fIV58HgOvXr+P69eumqoXIauTmAmfPytsMQkRE1svoIKTRaDBr1ix4eXmhZs2aqFmzJry9vTF79mxoNBpz1Ehkcc6cAQoKgKpVgeBgpashIqJHZfSusWnTpuGrr77C3Llz0aZNGwDAvn37MHPmTDx48ABz5swxeZFElqbo+CAOjSMisl5GB6FVq1Zh+fLleO6553TzmjVrhmrVqmHs2LEMQmQTOFCaiKhyMHrXWFpaGhqUcPa4Bg0aIC0tzSRFEVk6DpQmIqocjA5C4eHhWLx4cbH5ixcvRnh4uEmKIrJ02h6hpk2VrYOIiB7PI119vlevXkhISEDr1q0BAElJSbh27Rq2bt1q8gKJLE1qKpCSIscGNW6sdDVERPQ4jO4R6tChA86fP49+/fohPT0d6enp6N+/P86dO4d27dqZo0Yii3LwoJzWrw+4uytbCxERPR6je4QAIDg4mIOiyWYdOCCnTz2lbB1ERPT4DO4RunDhAgYPHozMzMxiyzIyMjBkyBD8+eefJi2OyBJpg1BUlLJ1EBHR4zM4CC1YsAAhISHw9PQstszLywshISFYsGCBSYsjsjQaDYMQEVFlYnAQ2rNnD1588cVSlw8YMAC7du0ySVFElurcOSAzE3B15RFjRESVgcFBKDk5Gf7+/qUu9/X1xbVr10xSFJGl+v13OY2MBBweaYQdERFZEoODkJeXFy5dulTq8osXL5a424yoMklKklMOlCYiqhwMDkLt27fHZ599VuryTz/9lIfPU6W3c6ec8qNORFQ5GByEpk6dim3btuGFF17AwYMHkZGRgYyMDBw4cADPP/88tm/fjqlTp5qzViJF/fmn/HFwADp2VLoaIiIyBYNHObRo0QLr1q3DqFGjsGHDBr1lTzzxBH744Qe0bNnS5AUSWYr4eDl96inAw0PZWoiIyDSMGu757LPP4urVq4iLi8PFixchhEC9evXQtWtXuLm5matGIouQkCCnXbooWwcREZmO0ce9uLq6ol+/fuaohchiFRQUjg9iECIiqjwMHiOUlJSEzZs368375ptvUKtWLfj7++PVV19Fbm6uyQsksgRJScDdu4CXF/Dkk0pXQ0REpmJwEJo1axZOnz6tu3/y5EmMHj0a0dHRmDJlCn755RfExsaapUgipa1dK6d9+vD8QURElYnBQej48ePo3Lmz7v6aNWsQFRWFL7/8EhMnTsSnn36KH374wSxFEimpoABYt07eHjhQ2VqIiMi0DA5Cd+/eRUBAgO7+nj170KNHD939J598kmeWpkpp714gJQWoWhWIjla6GiIiMiWDg1BAQAAuX74MAMjLy8PRo0fxVJHT62ZlZcHR0dH0FRIpbPVqOe3fH3ByUrYWIiIyLYODUM+ePTFlyhT8+uuvmDp1Ktzc3PTOJH3ixAnUrl3bLEUSKeXuXeC77+TtYcOUrYWIiEzP4GGfs2fPRv/+/dGhQwe4u7tj1apVcCry7/GKFSvQtWtXsxRJpJTly4H794HwcF5Wg4ioMjI4CPn6+mLv3r3IyMiAu7s77O3t9Zb/+OOPcHd3N3mBRErJzweWLJG333wTUKmUrYeIiEzP6AOBvby8Spzv4+Pz2MUQWZKVK4GrV4EnngAGD1a6GiIiMgeDxwgR2ZLsbGD6dHl72jTA1VXZeoiIyDwYhIhKMH8+cPMmUKsWMHas0tUQEZG5MAgRPeTIEeCDD+TtefMAZ2dl6yEiIvNhECIq4t494OWX5UDpF16QP0REVHkZHYSuX7+Oe/fuFZuvVquxd+9ekxRFpAS1GnjxReDMGSAwEFi2jEeKERFVdgYHoZs3b6JVq1aoWbMmvL29MWzYML1AlJaWhk6dOpmlSCJzy80Fhg4F4uLkwOiNG+XRYkREVLkZHISmTJkCOzs7HDhwAHFxcfjjjz/QqVMn3L17V7eOEMIsRRKZ061bQLduwI8/Ao6OwA8/AFFRSldFREQVweAglJCQgE8//RSRkZGIjo7G/v37ERQUhGeeeQZpaWkAABX3I5AVEQL4+Wd51ug9ewAPD2DbNuDZZ5WujIiIKorBQSgjIwNVq1bV3Xd2dsZPP/2E0NBQdOrUCampqWYpkMjUhJDBp0cPoE8f2SPUpAnw++9A585KV0dERBXJ4CAUFhaGEydO6M1zcHDAjz/+iLCwMDzLf6PJwt25I88WHRkJdOwIbN8ud4W98w5w8CDQqJHSFRIRUUUzOAj16NEDX3zxRbH52jDUvHlzU9ZVqiVLliA0NBQuLi6IiorCwYMHy1z/xx9/RIMGDeDi4oKmTZti69atFVInKe/2bRl23n8faNsW8PcHRo4Ejh6VA6Jfew04fRqYO5dnjiYislUGX2tszpw5yMnJKXkjDg5Yv349/vrrL5MVVpK1a9di4sSJWLZsGaKiorBo0SJ069YN586dg7+/f7H1f/vtNwwePBixsbF49tlnsXr1avTt2xdHjx5FkyZNzFormVdBgQw6KSny5+ZN+XP5MnDxInD+PFDSx7FZM2DgQBmCeFQYEREZHIQcHBzg6elZ5vKaNWuapKjSfPTRRxgzZgxGjhwJAFi2bBm2bNmCFStWYMqUKcXW/+STT9C9e3dMnjwZADB79mzEx8dj8eLFWLZsmVlrtXb37wN//y3H0/j6AlWqGL8NIYAHD4CsLLk9tVqeqDA/v/C2Wi1/8vL0pzk5QFoa8Pffdjh+vBm++84e6emF4Sc1VYah8tStC0REAO3bA716ATVqGP86iIio8jL66vO3b9+Gr6+vOWopU15eHo4cOYKpU6fq5tnZ2SE6OhpJSUklPiYpKQkTJ07Um9etWzds3Lix1OfJzc1Fbm6u7n5mZiYAecJItVr9GK+gkHY7ptqeqeTnA6tWqfDtt3ZISlJBiMKjAF1dBfz8AB8fwNdXwN0dsLeXP2q1PCNzVhaQlaX631T+5Oc/7pGE9gBqlbhEpZI1BQYCgYEC/v5AzZoCtWsL1KkDNGwo4OWl/xgLa3KTs9TPliViWxmObWUctpfhzNlWhm7TqCB05coV3a6oinb79m0UFBQgICBAb35AQADOnj1b4mNSUlJKXD8lJaXU54mNjUVMTEyx+Tt27ICbm9sjVF66+Ph4k26vJOfOVcWDB/Zo2vQ27MoYEXbtmgcWLozAlSuFycHBQXa55Ofb4/59FZKTgeRkADA+3Dg5FcDeXgN7e1FkKm87OAg4OGjg4CDnOzho4OxcAHf3PLi7q+HhUXSaBx+fXHh7P4CXVx7s7Us+d1VaGrB/v9FlVhoV8dmqLNhWhmNbGYftZThztFVpw3keZnAQOnXqFLp3746xlfxS3FOnTtXrRcrMzERISAi6du1a5q5BY6jVasTHx6NLly5wdHQ0yTZLcvUq8OKLDlCrVahXT2DDhnzUrVt8vbNngVdfdUBqqgpVqwq8844GL76oQfXqcvm9exr8/Tdw544Kt2/Lo6+ys1UoKJC7p+ztAQ8PAQ8PeS4eT0/A3V3A01Ped3dHkRCmguzlMUxFtVVlwfYyHNvKcGwr47C9DGfOttLu0SmPQUHot99+w7PPPovXX38d77777mMV9qh8fX1hb2+PW7du6c2/desWAgMDS3xMYGCgUesD8vxIziVcbtzR0dHkb5I5tlnU2rWFu4LOn1fh/fcdsXq1/jp37wI9e8oxN82bA9u3q+Dvb4+iYcXHR/4oydxtVdmwvQzHtjIc28o4bC/Dmes71hAGHT7ftWtXvPzyy/jggw8eq6jH4eTkhIiICOzcuVM3T6PRYOfOnWjdunWJj2ndurXe+oDsfitt/cpECOCbb+Tt11+X002bgOxs/fXeegu4fl0OKo6Pl4eYExER2QqDglCVKlVw8+ZNxa8lNnHiRHz55ZdYtWoVzpw5g3/84x/Izs7WHUU2bNgwvcHU//znPxEXF4eFCxfi7NmzmDlzJg4fPozx48cr9RIqzMGD8hByV1dg3jwgLEweibV5c+E6W7bIsKRSAatWyaPDiIiIbIlBQWj//v04fPgwRo0aZe56yjRw4EB8+OGHmD59Opo3b47jx48jLi5ONyA6OTkZN2/e1K3/9NNPY/Xq1fjiiy8QHh6OdevWYePGjTZxDqG1a+W0f385ZmfQIHl/zRo5zciQ59IBgAkTABvoJCMiIirGoDFCderUwb59+9C9e3eMGzcOS5YsMXddpRo/fnypPTqJiYnF5r344ot48cUXzVyV5dmzR061Vz4ZNAj44APZI7RvH/Dll/KEg3XqALNnK1cnERGRkgy+xEZwcDD27NmD48ePm7EcMoWsLED7NrVtK6dNm8ozKufnAx06FO4SW7ECMPFZAYiIiKyGwUEIAKpWrYqEhARz1UImcuAAoNEANWtCdwg8ACxfLq+yrtHIM0WvWAG0a6dcnUREREoz+szSrrw6pcXbt09Otb1BWu7u8iKkq1YBAwYAtWtXfG1ERESWxKgeobLcvHnTJo7GsgbaINSmTfFlwcHA1KkMQURERICRPUKnT5/G7t274eTkhAEDBsDb2xu3b9/GnDlzsGzZMoSFhZmrTjJQfj7w++/y9sM9QkRERKTP4B6hn3/+GS1atMCbb76J119/HZGRkdi9ezcaNmyIM2fOYMOGDTh9+rQ5ayUDnDolT5ro6Qk0bqx0NURERJbN4CD0/vvvY9y4ccjMzMRHH32EP//8E2+++Sa2bt2KuLg4dO/e3Zx1koEOHZLTyEiUeZFVIiIiMiIInTt3DuPGjYO7uzveeOMN2NnZ4eOPP8aTTz5pzvrISNogxLeFiIiofAYHoaysLN3V1+3t7eHq6soxQRaIQYiIiMhwRg2W3r59O7y8vAAUXvD01KlTeus899xzpquOjHL/PnDypLzNIERERFQ+o4LQ8OHD9e6/pr1Y1f+oVCoUFBQ8flX0SI4fBwoK5BXkQ0KUroaIiMjyGRyENBqNOesgEyi6W0ylUrYWIiIia8DjiiqRw4fllLvFiIiIDMMgVIkcOSKnkZHK1kFERGQtGIQqiexs4OxZebtlS2VrISIishYMQpXEiRPyqvKBgUBQkNLVEBERWQcGoUpCu1ssIkLZOoiIiKyJ0UEoLCwMd+7cKTY/PT2dJ1hU0NGjcsrdYkRERIYzOghduXKlxHMF5ebm4q+//jJJUWQ8BiEiIiLjGXweoZ9//ll3u+gZpgGgoKAAO3fuRGhoqEmLI8M8eACcPi1vMwgREREZzuAg1LdvXwDy7NEPn2Ha0dERoaGhWLhwoUmLI8OcPAnk5wO+vjyjNBERkTGMPrN0rVq1cOjQIfj6+pqtKDKOdqB0y5Y8ozQREZExjLrWGABcvny52Lz09HR4e3uboh56BBwfRERE9GiMHiw9b948rF27Vnf/xRdfhI+PD6pVq4b//ve/Ji2ODKMNQjx0noiIyDhGB6Fly5Yh5H8DUeLj45GQkIC4uDj06NEDkydPNnmBVLa8PDlGCGCPEBERkbGM3jWWkpKiC0KbN2/GgAED0LVrV4SGhiIqKsrkBVLZTp+WYcjbG6hVS+lqiIiIrIvRPUJVq1bFtWvXAABxcXGIjo4GAAghSjy/EJlX0fFBHChNRERkHKN7hPr3748hQ4agbt26uHPnDnr06AEAOHbsGOrUqWPyAqlsHChNRET06IwOQh9//DFCQ0Nx7do1zJ8/H+7u7gCAmzdvYuzYsSYvkMp2/LicNm+uZBVERETWyegg5OjoiLfffrvY/AkTJpikIDKcRlM4UDo8XNlaiIiIrNEjXX3+22+/Rdu2bREcHIyrV68CABYtWoRNmzaZtDgq29WrQFYW4OQE1K+vdDVERETWx+ggtHTpUkycOBE9evRAenq6boC0t7c3Fi1aZOr6qAza0zY1agQ4OipbCxERkTUyOgh99tln+PLLLzFt2jTY29vr5kdGRuKkdj8NVYgTJ+S0WTNl6yAiIrJWRgehy5cvo0WLFsXmOzs7Izs72yRFkWEYhIiIiB6P0UGoVq1aOK49VKmIuLg4NGzY0BQ1kYG0u8Y4UJqIiOjRGHzU2KxZs/D2229j4sSJGDduHB48eAAhBA4ePIjvv/8esbGxWL58uTlrpSLu3QMuXZK32SNERET0aAwOQjExMXj99dfxyiuvwNXVFf/+97+Rk5ODIUOGIDg4GJ988gkGDRpkzlqpiNOnASGAwEDA31/paoiIiKyTwUFICKG7PXToUAwdOhQ5OTm4d+8e/PlNXOG0u8XYG0RERPTojDqhouqhi1m5ubnBzc3NpAWRYbQDpTk+iIiI6NEZFYTq1atXLAw9LC0t7bEKIsPwiDEiIqLHZ1QQiomJgZeXl7lqIQMJwR4hIiIiUzAqCA0aNIjjgSxAcjKQkSHPJs1LaxARET06g88jVN4uMao42t6ghg3ldcaIiIjo0RgchIoeNUbK4okUiYiITMPgXWMajcacdZARtJd0a9pU2TqIiIisndGX2CDl/fGHnDZpomwdRERE1o5ByMrk5wPnzsnbvLQbERHR42EQsjJ//gmo1YCbG1CjhtLVEBERWTcGISuj3S3WsCFgx3ePiIjosfCr1MoUDUJERET0eBiErMyZM3LaqJGydRAREVUGDEJWRtsjxCBERET0+BiErIhGU9gjxF1jREREj49ByIpcvQrcvy8vqxEWpnQ1RERE1o9ByIpoe4Pq1wccjLpcLhEREZWEQciK8IgxIiIi02IQsiIcKE1ERGRaVhOE0tLSMHToUHh6esLb2xujR4/GvXv3ylz/jTfeQP369eHq6ooaNWrgzTffREZGRgVWbVo8dJ6IiMi0rCYIDR06FKdPn0Z8fDw2b96MvXv34tVXXy11/Rs3buDGjRv48MMPcerUKaxcuRJxcXEYPXp0BVZtOkKwR4iIiMjUrGLI7ZkzZxAXF4dDhw4hMjISAPDZZ5+hZ8+e+PDDDxEcHFzsMU2aNMH69et192vXro05c+bgpZdeQn5+PhysbLTxjRtAZiZgbw/Urat0NURERJWDVaSBpKQkeHt760IQAERHR8POzg4HDhxAv379DNpORkYGPD09ywxBubm5yM3N1d3PzMwEAKjVaqjV6kd8Bfq02zFmeydOqAA4oHZtAZUqHyYqxeI9SlvZMraX4dhWhmNbGYftZThztpWh27SKIJSSkgJ/f3+9eQ4ODvDx8UFKSopB27h9+zZmz55d5u40AIiNjUVMTEyx+Tt27ICbm5vhRRsgPj7e4HU3bw4D0BQ+Pjexdeshk9ZhDYxpK2J7GYNtZTi2lXHYXoYzR1vl5OQYtJ6iQWjKlCmYN29emeuc0Y4QfgyZmZno1asXGjVqhJkzZ5a57tSpUzFx4kS9x4aEhKBr167w9PR87FoAmVLj4+PRpUsXODo6GvSYLVvkcK4OHQLQs2dPk9RhDR6lrWwZ28twbCvDsa2Mw/YynDnbSrtHpzyKBqFJkyZhxIgRZa4TFhaGwMBApKam6s3Pz89HWloaAgMDy3x8VlYWunfvDg8PD2zYsKHchnZ2doazs3Ox+Y6OjiZ/k4zZ5vnzctqkiT0cHe1NWoc1MEf7V2ZsL8OxrQzHtjIO28tw5vqONYSiQcjPzw9+fn7lrte6dWukp6fjyJEjiIiIAADs2rULGo0GUVFRpT4uMzMT3bp1g7OzM37++We4uLiYrPaKdu6cnNavr2wdRERElYlVHD7fsGFDdO/eHWPGjMHBgwexf/9+jB8/HoMGDdIdMfbXX3+hQYMGOHjwIAAZgrp27Yrs7Gx89dVXyMzMREpKClJSUlBQUKDkyzFaRgagHQrFIERERGQ6VjFYGgC+++47jB8/Hp07d4adnR2ef/55fPrpp7rlarUa586d0w2OOnr0KA4cOAAAqFOnjt62Ll++jNDQ0Aqr/XFpe4MCAwETDVMiIiIiWFEQ8vHxwerVq0tdHhoaCiGE7n7Hjh317lsz7hYjIiIyD6vYNWbrGISIiIjMg0HICjAIERERmQeDkBVgECIiIjIPBiELp9EAFy7I2wxCREREpsUgZOGSk4EHDwBHR8CKDnQjIiKyCgxCFk67W6xOHaCMa8USERHRI2AQsnAcH0RERGQ+DEIWjkGIiIjIfBiELJw2CDVooGwdRERElRGDkIVjjxAREZH5MAhZsHv3gOvX5W0GISIiItNjELJg58/Lqa8v4OOjbC1ERESVEYOQBeNuMSIiIvNiELJgDEJERETmxSBkwbS7xhiEiIiIzINByIJprzFWr56ydRAREVVWDEIWSojCIFSnjrK1EBERVVYMQhYqLQ3IyJC3w8KUrYWIiKiyYhCyUBcvymm1aoCbm7K1EBERVVYMQhZKG4S4W4yIiMh8GIQslHZ8UN26ytZBRERUmTEIWSj2CBEREZkfg5CFYhAiIiIyPwYhC8UgREREZH4MQhbo7l3gzh15u3ZtZWshIiKqzBiELNClS3IaFAS4uytbCxERUWXGIGSBuFuMiIioYjAIWSBeWoOIiKhiMAhZIPYIERERVQwGIQvEIERERFQxGIQskDYI8azSRERE5sUgZGEyM4HUVHmbh84TERGZF4OQhdEeOu/vD3h6KlsLERFRZccgZGE4PoiIiKjiMAhZGB46T0REVHEYhCwMB0oTERFVHAYhC/Pnn3IaFqZsHURERLaAQcjCXL4sp7VqKVsHERGRLWAQsiBqNXD9urzNIERERGR+DEIW5No1QKMBXFyAgAClqyEiIqr8GIQsyJUrchoaCqhUSlZCRERkGxiELIh2fFBoqKJlEBER2QwGIQvCIERERFSxGIQsiHbXGAdKExERVQwGIQvCHiEiIqKKxSBkQdgjREREVLEYhCzEgwfAjRvyNnuEiIiIKgaDkIVITpbTKlUAX19layEiIrIVDEIW4upVOa1Rg+cQIiIiqigMQhbi2jU5rVlT2TqIiIhsCYOQhdDuGgsJUbYOIiIiW8IgZCG0QahGDWXrICIisiUMQhZCu2uMQYiIiKjiMAhZCO4aIyIiqngMQhZACPYIERERKYFByALcuQPcvy9vV6+ubC1ERES2hEHIAmh3iwUEAM7OytZCRERkSxiELAB3ixERESnDaoJQWloahg4dCk9PT3h7e2P06NG4d++eQY8VQqBHjx5QqVTYuHGjeQt9BBwoTUREpAyrCUJDhw7F6dOnER8fj82bN2Pv3r149dVXDXrsokWLoLLg61awR4iIiEgZDkoXYIgzZ84gLi4Ohw4dQmRkJADgs88+Q8+ePfHhhx8iODi41MceP34cCxcuxOHDhxEUFFRRJRuFPUJERETKsIoglJSUBG9vb10IAoDo6GjY2dnhwIED6NevX4mPy8nJwZAhQ7BkyRIEBgYa9Fy5ubnIzc3V3c/MzAQAqNVqqNXqx3gVhbTb0U6Tk+0B2CE4OB9qtTDJc1QWD7cVlY3tZTi2leHYVsZhexnOnG1l6DatIgilpKTA399fb56DgwN8fHyQkpJS6uMmTJiAp59+Gn369DH4uWJjYxETE1Ns/o4dO+Dm5mZ40QaIj48HAJw/3xWAK65d24+tW9NN+hyVhbatyDBsL8OxrQzHtjIO28tw5mirnJwcg9ZTNAhNmTIF8+bNK3OdM2fOPNK2f/75Z+zatQvHjh0z6nFTp07FxIkTdfczMzMREhKCrl27wtPT85FqeZharUZ8fDy6dOkClcoRd+/Kt2HgwKdhoXvvFFO0rRwdHZUux+KxvQzHtjIc28o4bC/DmbOttHt0yqNoEJo0aRJGjBhR5jphYWEIDAxEamqq3vz8/HykpaWVustr165duHTpEry9vfXmP//882jXrh0SExNLfJyzszOcSziZj6Ojo8nfJEdHR9y86QiNBnB0BKpXd4Sd1Qxfr1jmaP/KjO1lOLaV4dhWxmF7Gc5c37GGUDQI+fn5wc/Pr9z1WrdujfT0dBw5cgQREREAZNDRaDSIiooq8TFTpkzBK6+8ojevadOm+Pjjj9G7d+/HL95Eig6UZggiIiKqWFYxRqhhw4bo3r07xowZg2XLlkGtVmP8+PEYNGiQ7oixv/76C507d8Y333yDVq1aITAwsMTeoho1aqBWrVoV/RJKpT10nkeMERERVTyr6YP47rvv0KBBA3Tu3Bk9e/ZE27Zt8cUXX+iWq9VqnDt3zuDBUZZC2yPEcwgRERFVPKvoEQIAHx8frF69utTloaGhEKLsQ8/LW64EnkOIiIhIOVbTI1RZ8azSREREymEQUhh3jRERESmHQUhhHCxNRESkHAYhBd27B6SlydvsESIiIqp4DEIK0vYGeXrKHyIiIqpYDEIKunFDBQCoXl3hQoiIiGwUg5CCbtyQ02rVlK2DiIjIVjEIKejmTdkj9L+TYxMREVEFYxBSkLZHiEGIiIhIGQxCCtKOEeKuMSIiImUwCCno5k05ZY8QERGRMhiEFMQxQkRERMpiEFKIRsMeISIiIqUxCCkkM9MJarXsEQoMVLgYIiIiG8UgpJC0NBcAgL8/4OiocDFEREQ2ikFIIdogxCPGiIiIlMMgpJC7d2UQ4vggIiIi5TAIKUTbI8QgREREpBwGIYXcueMKgEGIiIhISQxCCmGPEBERkfIYhBTyxBP3Ua+eQM2aSldCRERkuxyULsBW/eMfJ9CzZ3U48th5IiIixbBHiIiIiGwWgxARERHZLAYhIiIislkMQkRERGSzGISIiIjIZjEIERERkc1iECIiIiKbxSBERERENotBiIiIiGwWgxARERHZLAYhIiIislkMQkRERGSzGISIiIjIZjEIERERkc1yULoASyeEAABkZmaabJtqtRo5OTnIzMyEo6OjybZbGbGtjMP2MhzbynBsK+OwvQxnzrbSfm9rv8dLwyBUjqysLABASEiIwpUQERGRsbKysuDl5VXqcpUoLyrZOI1Ggxs3bsDDwwMqlcok28zMzERISAiuXbsGT09Pk2yzsmJbGYftZTi2leHYVsZhexnOnG0lhEBWVhaCg4NhZ1f6SCD2CJXDzs4O1atXN8u2PT09+UtiILaVcdhehmNbGY5tZRy2l+HM1VZl9QRpcbA0ERER2SwGISIiIrJZDEIKcHZ2xowZM+Ds7Kx0KRaPbWUctpfh2FaGY1sZh+1lOEtoKw6WJiIiIpvFHiEiIiKyWQxCREREZLMYhIiIiMhmMQgRERGRzWIQqmBLlixBaGgoXFxcEBUVhYMHDypdkkWYOXMmVCqV3k+DBg10yx88eIBx48bhiSeegLu7O55//nncunVLwYorzt69e9G7d28EBwdDpVJh48aNesuFEJg+fTqCgoLg6uqK6OhoXLhwQW+dtLQ0DB06FJ6envD29sbo0aNx7969CnwVFaO8thoxYkSxz1n37t311rGVtoqNjcWTTz4JDw8P+Pv7o2/fvjh37pzeOob83iUnJ6NXr15wc3ODv78/Jk+ejPz8/Ip8KRXCkPbq2LFjsc/X66+/rreOLbTX0qVL0axZM91JElu3bo1t27bpllva54pBqAKtXbsWEydOxIwZM3D06FGEh4ejW7duSE1NVbo0i9C4cWPcvHlT97Nv3z7dsgkTJuCXX37Bjz/+iD179uDGjRvo37+/gtVWnOzsbISHh2PJkiUlLp8/fz4+/fRTLFu2DAcOHECVKlXQrVs3PHjwQLfO0KFDcfr0acTHx2Pz5s3Yu3cvXn311Yp6CRWmvLYCgO7du+t9zr7//nu95bbSVnv27MG4cePw+++/Iz4+Hmq1Gl27dkV2drZunfJ+7woKCtCrVy/k5eXht99+w6pVq7By5UpMnz5diZdkVoa0FwCMGTNG7/M1f/583TJbaa/q1atj7ty5OHLkCA4fPoxnnnkGffr0wenTpwFY4OdKUIVp1aqVGDdunO5+QUGBCA4OFrGxsQpWZRlmzJghwsPDS1yWnp4uHB0dxY8//qibd+bMGQFAJCUlVVCFlgGA2LBhg+6+RqMRgYGBYsGCBbp56enpwtnZWXz//fdCCCH++OMPAUAcOnRIt862bduESqUSf/31V4XVXtEebishhBg+fLjo06dPqY+x1bYSQojU1FQBQOzZs0cIYdjv3datW4WdnZ1ISUnRrbN06VLh6ekpcnNzK/YFVLCH20sIITp06CD++c9/lvoYW26vqlWriuXLl1vk54o9QhUkLy8PR44cQXR0tG6enZ0doqOjkZSUpGBlluPChQsIDg5GWFgYhg4diuTkZADAkSNHoFar9dquQYMGqFGjhs233eXLl5GSkqLXNl5eXoiKitK1TVJSEry9vREZGalbJzo6GnZ2djhw4ECF16y0xMRE+Pv7o379+vjHP/6BO3fu6JbZcltlZGQAAHx8fAAY9nuXlJSEpk2bIiAgQLdOt27dkJmZqfvvv7J6uL20vvvuO/j6+qJJkyaYOnUqcnJydMtssb0KCgqwZs0aZGdno3Xr1hb5ueJFVyvI7du3UVBQoPfGAkBAQADOnj2rUFWWIyoqCitXrkT9+vVx8+ZNxMTEoF27djh16hRSUlLg5OQEb29vvccEBAQgJSVFmYIthPb1l/S50i5LSUmBv7+/3nIHBwf4+PjYXPt1794d/fv3R61atXDp0iW8++676NGjB5KSkmBvb2+zbaXRaPDWW2+hTZs2aNKkCQAY9HuXkpJS4mdPu6yyKqm9AGDIkCGoWbMmgoODceLECbzzzjs4d+4cfvrpJwC21V4nT55E69at8eDBA7i7u2PDhg1o1KgRjh8/bnGfKwYhsgg9evTQ3W7WrBmioqJQs2ZN/PDDD3B1dVWwMqpMBg0apLvdtGlTNGvWDLVr10ZiYiI6d+6sYGXKGjduHE6dOqU3Lo9KV1p7FR1L1rRpUwQFBaFz5864dOkSateuXdFlKqp+/fo4fvw4MjIysG7dOgwfPhx79uxRuqwScddYBfH19YW9vX2xkfG3bt1CYGCgQlVZLm9vb9SrVw8XL15EYGAg8vLykJ6errcO2w6611/W5yowMLDYgPz8/HykpaXZfPuFhYXB19cXFy9eBGCbbTV+/Hhs3rwZu3fvRvXq1XXzDfm9CwwMLPGzp11WGZXWXiWJiooCAL3Pl620l5OTE+rUqYOIiAjExsYiPDwcn3zyiUV+rhiEKoiTkxMiIiKwc+dO3TyNRoOdO3eidevWClZmme7du4dLly4hKCgIERERcHR01Gu7c+fOITk52ebbrlatWggMDNRrm8zMTBw4cEDXNq1bt0Z6ejqOHDmiW2fXrl3QaDS6P9S26vr167hz5w6CgoIA2FZbCSEwfvx4bNiwAbt27UKtWrX0lhvye9e6dWucPHlSLzzGx8fD09MTjRo1qpgXUkHKa6+SHD9+HAD0Pl+20l4P02g0yM3NtczPlcmHX1Op1qxZI5ydncXKlSvFH3/8IV599VXh7e2tNzLeVk2aNEkkJiaKy5cvi/3794vo6Gjh6+srUlNThRBCvP7666JGjRpi165d4vDhw6J169aidevWClddMbKyssSxY8fEsWPHBADx0UcfiWPHjomrV68KIYSYO3eu8Pb2Fps2bRInTpwQffr0EbVq1RL379/XbaN79+6iRYsW4sCBA2Lfvn2ibt26YvDgwUq9JLMpq62ysrLE22+/LZKSksTly5dFQkKCaNmypahbt6548OCBbhu20lb/+Mc/hJeXl0hMTBQ3b97U/eTk5OjWKe/3Lj8/XzRp0kR07dpVHD9+XMTFxQk/Pz8xdepUJV6SWZXXXhcvXhSzZs0Shw8fFpcvXxabNm0SYWFhon379rpt2Ep7TZkyRezZs0dcvnxZnDhxQkyZMkWoVCqxY8cOIYTlfa4YhCrYZ599JmrUqCGcnJxEq1atxO+//650SRZh4MCBIigoSDg5OYlq1aqJgQMHiosXL+qW379/X4wdO1ZUrVpVuLm5iX79+ombN28qWHHF2b17twBQ7Gf48OFCCHkI/XvvvScCAgKEs7Oz6Ny5szh37pzeNu7cuSMGDx4s3N3dhaenpxg5cqTIyspS4NWYV1ltlZOTI7p27Sr8/PyEo6OjqFmzphgzZkyxf0Rspa1KaicA4uuvv9atY8jv3ZUrV0SPHj2Eq6ur8PX1FZMmTRJqtbqCX435lddeycnJon379sLHx0c4OzuLOnXqiMmTJ4uMjAy97dhCe40aNUrUrFlTODk5CT8/P9G5c2ddCBLC8j5XKiGEMH0/ExEREZHl4xghIiIislkMQkRERGSzGISIiIjIZjEIERERkc1iECIiIiKbxSBERERENotBiIiIiGwWgxARUQVJTEyESqUqdp0lIlIOgxARERHZLAYhIiIislkMQkRkch07dsSbb76Jf/3rX/Dx8UFgYCBmzpwJALhy5QpUKpXuytwAkJ6eDpVKhcTERACFu5C2b9+OFi1awNXVFc888wxSU1Oxbds2NGzYEJ6enhgyZAhycnIMqkmj0SA2Nha1atWCq6srwsPDsW7dOt1y7XNu2bIFzZo1g4uLC5566imcOnVKbzvr169H48aN4ezsjNDQUCxcuFBveW5uLt555x2EhITA2dkZderUwVdffaW3zpEjRxAZGQk3Nzc8/fTTOHfunG7Zf//7X3Tq1AkeHh7w9PREREQEDh8+bNBrJCLjMQgRkVmsWrUKVapUwYEDBzB//nzMmjUL8fHxRm1j5syZWLx4MX777Tdcu3YNAwYMwKJFi7B69Wps2bIFO3bswGeffWbQtmJjY/HNN99g2bJlOH36NCZMmICXXnoJe/bs0Vtv8uTJWLhwIQ4dOgQ/Pz/07t0barUagAwwAwYMwKBBg3Dy5EnMnDkT7733HlauXKl7/LBhw/D999/j008/xZkzZ/B///d/cHd313uOadOmYeHChTh8+DAcHBwwatQo3bKhQ4eievXqOHToEI4cOYIpU6bA0dHRqHYjIiOY5VKuRGTTOnToINq2bas378knnxTvvPOOuHz5sgAgjh07plt29+5dAUDs3r1bCFF4FfmEhATdOrGxsQKAuHTpkm7ea6+9Jrp161ZuPQ8ePBBubm7it99+05s/evRoMXjwYL3nXLNmjW75nTt3hKurq1i7dq0QQoghQ4aILl266G1j8uTJolGjRkIIIc6dOycAiPj4+BLrKOl1bdmyRQAQ9+/fF0II4eHhIVauXFnuayIi02CPEBGZRbNmzfTuBwUFITU19ZG3ERAQADc3N4SFhenNM2SbFy9eRE5ODrp06QJ3d3fdzzfffINLly7prdu6dWvdbR8fH9SvXx9nzpwBAJw5cwZt2rTRW79Nmza4cOECCgoKcPz4cdjb26NDhw4Gv66goCAA0L2OiRMn4pVXXkF0dDTmzp1brD4iMi0HpQsgosrp4d05KpUKGo0Gdnby/y8hhG6ZdtdTWdtQqVSlbrM89+7dAwBs2bIF1apV01vm7Oxc7uMN5erqatB6D78uALrXMXPmTAwZMgRbtmzBtm3bMGPGDKxZswb9+vUzWZ1EVIg9QkRUofz8/AAAN2/e1M0rOnDaHBo1agRnZ2ckJyejTp06ej8hISF66/7++++623fv3sX58+fRsGFDAEDDhg2xf/9+vfX379+PevXqwd7eHk2bNoVGoyk27shY9erVw4QJE7Bjxw70798fX3/99WNtj4hKxx4hIqpQrq6ueOqppzB37lzUqlULqamp+Pe//23W5/Tw8MDbb7+NCRMmQKPRoG3btsjIyMD+/fvh6emJ4cOH69adNWsWnnjiCQQEBGDatGnw9fVF3759AQCTJk3Ck08+idmzZ2PgwIFISkrC4sWL8fnnnwMAQkNDMXz4cIwaNQqffvopwsPDcfXqVaSmpmLAgAHl1nn//n1MnjwZL7zwAmrVqoXr16/j0KFDeP75583SLkTEIEREClixYgVGjx6NiIgI1K9fH/Pnz0fXrl3N+pyzZ8+Gn58fYmNj8eeff8Lb2xstW7bEu+++q7fe3Llz8c9//hMXLlxA8+bN8csvv8DJyQkA0LJlS/zwww+YPn06Zs+ejaCgIMyaNQsjRozQPX7p0qV49913MXbsWNy5cwc1atQo9hylsbe3x507dzBs2DDcunULvr6+6N+/P2JiYkzWDkSkTyWK7qgnIrJRiYmJ6NSpE+7evQtvb2+lyyGiCsIxQkRERGSzGISIyOolJyfrHRb/8E9ycrLSJRKRheKuMSKyevn5+bhy5Uqpy0NDQ+HgwCGRRFQcgxARERHZLO4aIyIiIpvFIEREREQ2i0GIiIiIbBaDEBEREdksBiEiIiKyWQxCREREZLMYhIiIiMhmMQgRERGRzfp/pQO05CgFVU8AAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(num_epochs_list, r2_scores_list, color='b', linestyle='-')\n",
    "plt.title('num_epochs vs. Test R^2 Score')\n",
    "plt.xlabel('num_epochs')\n",
    "plt.ylabel('Test R^2 SCore') \n",
    "plt.grid(True)\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max R^2 score: 0.5013074040747145\n",
      "Corresponding RMSE: 0.2332381147157868\n",
      "Corresponding num_epochs: 294\n"
     ]
    }
   ],
   "source": [
    "max_r2_score = max(r2_scores_list)\n",
    "corresponding_rmse = rmse_list[r2_scores_list.index(max_r2_score)]\n",
    "corresponding_num_epochs = num_epochs_list[r2_scores_list.index(max_r2_score)]\n",
    "\n",
    "print(f'Max R^2 score: {max_r2_score}')\n",
    "print(f'Corresponding RMSE: {corresponding_rmse}')\n",
    "print(f'Corresponding num_epochs: {corresponding_num_epochs}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Adjusted R^2 Score (Valence) vs. num_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(num_epochs_list, adjusted_r2_scores_valence_list, color='b', linestyle='-')\n",
    "plt.title('num_epochs vs. Test Adjusted R^2 Score (Valence)')\n",
    "plt.xlabel('num_epochs')\n",
    "plt.ylabel('Test Adjusted R^2 Score (Valence)') \n",
    "plt.grid(True)\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max R^2 score: 0.4121616574107988\n",
      "Corresponding RMSE: 0.24472369433258123\n",
      "Corresponding num_epochs: 106\n"
     ]
    }
   ],
   "source": [
    "max_r2_score_valence = max(adjusted_r2_scores_valence_list)\n",
    "corresponding_rmse = rmse_list[adjusted_r2_scores_valence_list.index(max_r2_score_valence)]\n",
    "corresponding_num_epochs = num_epochs_list[adjusted_r2_scores_valence_list.index(max_r2_score_valence)]\n",
    "\n",
    "print(f'Max R^2 score: {max_r2_score_valence}')\n",
    "print(f'Corresponding RMSE: {corresponding_rmse}')\n",
    "print(f'Corresponding num_epochs: {corresponding_num_epochs}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Adjusted R^2 Score (Arousal) vs. num_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(num_epochs_list, adjusted_r2_scores_arousal_list, color='b', linestyle='-')\n",
    "plt.title('num_epochs vs. Test Adjusted R^2 Score (Arousal)')\n",
    "plt.xlabel('num_epochs')\n",
    "plt.ylabel('Test Adjusted R^2 SCore (Arousal)') \n",
    "plt.grid(True)\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max R^2 score: 0.6320025527047093\n",
      "Corresponding RMSE: 0.234155670140704\n",
      "Corresponding num_epochs: 295\n"
     ]
    }
   ],
   "source": [
    "max_r2_score_arousal = max(adjusted_r2_scores_arousal_list)\n",
    "corresponding_rmse = rmse_list[adjusted_r2_scores_arousal_list.index(max_r2_score_arousal)]\n",
    "corresponding_num_epochs = num_epochs_list[adjusted_r2_scores_arousal_list.index(max_r2_score_arousal)]\n",
    "\n",
    "print(f'Max R^2 score: {max_r2_score_arousal}')\n",
    "print(f'Corresponding RMSE: {corresponding_rmse}')\n",
    "print(f'Corresponding num_epochs: {corresponding_num_epochs}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
